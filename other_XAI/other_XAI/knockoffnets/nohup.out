result path:  /home/gpu2/jbw/other_XAI/knockoffnets/ms_elastictrainer_result_resnet18_vgg16_mobilenetv2.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.022280	Acc: 70.9% (7089/10000)
[Test]  Epoch: 2	Loss: 0.022285	Acc: 70.9% (7088/10000)
[Test]  Epoch: 3	Loss: 0.022082	Acc: 70.8% (7083/10000)
[Test]  Epoch: 4	Loss: 0.022156	Acc: 70.7% (7071/10000)
[Test]  Epoch: 5	Loss: 0.022166	Acc: 70.9% (7093/10000)
[Test]  Epoch: 6	Loss: 0.022125	Acc: 70.9% (7088/10000)
[Test]  Epoch: 7	Loss: 0.021714	Acc: 71.0% (7099/10000)
[Test]  Epoch: 8	Loss: 0.021890	Acc: 71.1% (7109/10000)
[Test]  Epoch: 9	Loss: 0.021911	Acc: 71.3% (7130/10000)
[Test]  Epoch: 10	Loss: 0.021826	Acc: 71.0% (7100/10000)
[Test]  Epoch: 11	Loss: 0.022084	Acc: 71.0% (7099/10000)
[Test]  Epoch: 12	Loss: 0.022047	Acc: 70.6% (7062/10000)
[Test]  Epoch: 13	Loss: 0.022131	Acc: 70.4% (7042/10000)
[Test]  Epoch: 14	Loss: 0.022022	Acc: 70.8% (7078/10000)
[Test]  Epoch: 15	Loss: 0.022235	Acc: 70.6% (7057/10000)
[Test]  Epoch: 16	Loss: 0.022212	Acc: 70.6% (7058/10000)
[Test]  Epoch: 17	Loss: 0.021914	Acc: 70.8% (7080/10000)
[Test]  Epoch: 18	Loss: 0.021989	Acc: 70.8% (7077/10000)
[Test]  Epoch: 19	Loss: 0.022344	Acc: 70.8% (7078/10000)
[Test]  Epoch: 20	Loss: 0.021931	Acc: 70.9% (7087/10000)
[Test]  Epoch: 21	Loss: 0.022090	Acc: 70.3% (7035/10000)
[Test]  Epoch: 22	Loss: 0.022260	Acc: 70.6% (7059/10000)
[Test]  Epoch: 23	Loss: 0.021851	Acc: 70.4% (7044/10000)
[Test]  Epoch: 24	Loss: 0.022148	Acc: 70.8% (7076/10000)
[Test]  Epoch: 25	Loss: 0.022061	Acc: 70.5% (7047/10000)
[Test]  Epoch: 26	Loss: 0.021967	Acc: 70.7% (7065/10000)
[Test]  Epoch: 27	Loss: 0.022130	Acc: 70.7% (7070/10000)
[Test]  Epoch: 28	Loss: 0.022184	Acc: 70.5% (7052/10000)
[Test]  Epoch: 29	Loss: 0.021868	Acc: 70.3% (7032/10000)
[Test]  Epoch: 30	Loss: 0.022224	Acc: 70.7% (7074/10000)
[Test]  Epoch: 31	Loss: 0.022174	Acc: 70.5% (7051/10000)
[Test]  Epoch: 32	Loss: 0.021939	Acc: 70.7% (7073/10000)
[Test]  Epoch: 33	Loss: 0.022069	Acc: 71.0% (7099/10000)
[Test]  Epoch: 34	Loss: 0.021808	Acc: 71.2% (7117/10000)
[Test]  Epoch: 35	Loss: 0.022104	Acc: 70.7% (7065/10000)
[Test]  Epoch: 36	Loss: 0.021970	Acc: 70.7% (7069/10000)
[Test]  Epoch: 37	Loss: 0.021862	Acc: 70.8% (7077/10000)
[Test]  Epoch: 38	Loss: 0.022327	Acc: 70.0% (7005/10000)
[Test]  Epoch: 39	Loss: 0.022033	Acc: 70.5% (7045/10000)
[Test]  Epoch: 40	Loss: 0.021956	Acc: 70.7% (7074/10000)
[Test]  Epoch: 41	Loss: 0.022088	Acc: 70.5% (7050/10000)
[Test]  Epoch: 42	Loss: 0.021922	Acc: 70.4% (7039/10000)
[Test]  Epoch: 43	Loss: 0.021995	Acc: 70.3% (7032/10000)
[Test]  Epoch: 44	Loss: 0.021878	Acc: 71.0% (7102/10000)
[Test]  Epoch: 45	Loss: 0.021753	Acc: 70.9% (7086/10000)
[Test]  Epoch: 46	Loss: 0.021963	Acc: 70.8% (7080/10000)
[Test]  Epoch: 47	Loss: 0.021906	Acc: 70.6% (7057/10000)
[Test]  Epoch: 48	Loss: 0.021696	Acc: 70.8% (7075/10000)
[Test]  Epoch: 49	Loss: 0.021501	Acc: 71.2% (7120/10000)
[Test]  Epoch: 50	Loss: 0.021719	Acc: 70.5% (7053/10000)
[Test]  Epoch: 51	Loss: 0.021754	Acc: 71.0% (7095/10000)
[Test]  Epoch: 52	Loss: 0.021765	Acc: 70.6% (7060/10000)
[Test]  Epoch: 53	Loss: 0.021905	Acc: 70.2% (7025/10000)
[Test]  Epoch: 54	Loss: 0.021983	Acc: 70.6% (7063/10000)
[Test]  Epoch: 55	Loss: 0.021939	Acc: 70.8% (7075/10000)
[Test]  Epoch: 56	Loss: 0.021784	Acc: 70.6% (7059/10000)
[Test]  Epoch: 57	Loss: 0.021727	Acc: 70.8% (7075/10000)
[Test]  Epoch: 58	Loss: 0.022228	Acc: 70.4% (7044/10000)
[Test]  Epoch: 59	Loss: 0.021813	Acc: 71.1% (7111/10000)
[Test]  Epoch: 60	Loss: 0.022042	Acc: 70.5% (7050/10000)
[Test]  Epoch: 61	Loss: 0.021825	Acc: 70.5% (7046/10000)
[Test]  Epoch: 62	Loss: 0.021893	Acc: 70.6% (7062/10000)
[Test]  Epoch: 63	Loss: 0.021962	Acc: 70.5% (7055/10000)
[Test]  Epoch: 64	Loss: 0.021779	Acc: 70.5% (7053/10000)
[Test]  Epoch: 65	Loss: 0.022044	Acc: 70.3% (7032/10000)
[Test]  Epoch: 66	Loss: 0.021836	Acc: 70.8% (7084/10000)
[Test]  Epoch: 67	Loss: 0.021975	Acc: 70.9% (7088/10000)
[Test]  Epoch: 68	Loss: 0.021857	Acc: 71.0% (7099/10000)
[Test]  Epoch: 69	Loss: 0.021860	Acc: 70.7% (7072/10000)
[Test]  Epoch: 70	Loss: 0.022090	Acc: 70.4% (7041/10000)
[Test]  Epoch: 71	Loss: 0.021661	Acc: 70.2% (7024/10000)
[Test]  Epoch: 72	Loss: 0.022004	Acc: 70.8% (7078/10000)
[Test]  Epoch: 73	Loss: 0.021860	Acc: 70.5% (7049/10000)
[Test]  Epoch: 74	Loss: 0.021677	Acc: 70.9% (7086/10000)
[Test]  Epoch: 75	Loss: 0.021587	Acc: 71.3% (7131/10000)
[Test]  Epoch: 76	Loss: 0.022096	Acc: 70.0% (6995/10000)
[Test]  Epoch: 77	Loss: 0.021891	Acc: 70.6% (7062/10000)
[Test]  Epoch: 78	Loss: 0.022243	Acc: 70.1% (7008/10000)
[Test]  Epoch: 79	Loss: 0.021605	Acc: 71.0% (7099/10000)
[Test]  Epoch: 80	Loss: 0.021898	Acc: 70.6% (7056/10000)
[Test]  Epoch: 81	Loss: 0.021774	Acc: 70.4% (7038/10000)
[Test]  Epoch: 82	Loss: 0.021783	Acc: 70.8% (7084/10000)
[Test]  Epoch: 83	Loss: 0.021772	Acc: 70.8% (7078/10000)
[Test]  Epoch: 84	Loss: 0.021576	Acc: 70.8% (7084/10000)
[Test]  Epoch: 85	Loss: 0.021850	Acc: 70.9% (7093/10000)
[Test]  Epoch: 86	Loss: 0.021667	Acc: 70.5% (7054/10000)
[Test]  Epoch: 87	Loss: 0.021727	Acc: 70.6% (7058/10000)
[Test]  Epoch: 88	Loss: 0.021800	Acc: 70.8% (7075/10000)
[Test]  Epoch: 89	Loss: 0.021492	Acc: 70.8% (7076/10000)
[Test]  Epoch: 90	Loss: 0.021755	Acc: 70.7% (7072/10000)
[Test]  Epoch: 91	Loss: 0.021977	Acc: 70.6% (7059/10000)
[Test]  Epoch: 92	Loss: 0.021710	Acc: 70.7% (7071/10000)
[Test]  Epoch: 93	Loss: 0.021878	Acc: 70.7% (7073/10000)
[Test]  Epoch: 94	Loss: 0.021715	Acc: 70.7% (7073/10000)
[Test]  Epoch: 95	Loss: 0.021794	Acc: 70.4% (7039/10000)
[Test]  Epoch: 96	Loss: 0.021739	Acc: 70.7% (7070/10000)
[Test]  Epoch: 97	Loss: 0.021840	Acc: 70.7% (7069/10000)
[Test]  Epoch: 98	Loss: 0.021847	Acc: 70.5% (7053/10000)
[Test]  Epoch: 99	Loss: 0.021728	Acc: 71.1% (7107/10000)
[Test]  Epoch: 100	Loss: 0.021930	Acc: 70.6% (7056/10000)
===========finish==========
['2024-08-19', '00:22:55.839764', '100', 'test', '0.02193025636076927', '70.56', '71.31']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.41.weight', 'features.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.045589	Acc: 66.5% (6651/10000)
[Test]  Epoch: 2	Loss: 0.042963	Acc: 68.0% (6800/10000)
[Test]  Epoch: 3	Loss: 0.042394	Acc: 67.3% (6734/10000)
[Test]  Epoch: 4	Loss: 0.044607	Acc: 66.4% (6643/10000)
[Test]  Epoch: 5	Loss: 0.046297	Acc: 65.8% (6585/10000)
[Test]  Epoch: 6	Loss: 0.043618	Acc: 66.2% (6622/10000)
[Test]  Epoch: 7	Loss: 0.043335	Acc: 66.7% (6666/10000)
[Test]  Epoch: 8	Loss: 0.044125	Acc: 66.4% (6637/10000)
[Test]  Epoch: 9	Loss: 0.042522	Acc: 66.8% (6678/10000)
[Test]  Epoch: 10	Loss: 0.041047	Acc: 66.7% (6671/10000)
[Test]  Epoch: 11	Loss: 0.042130	Acc: 65.6% (6557/10000)
[Test]  Epoch: 12	Loss: 0.043308	Acc: 65.1% (6511/10000)
[Test]  Epoch: 13	Loss: 0.042303	Acc: 65.5% (6548/10000)
[Test]  Epoch: 14	Loss: 0.042379	Acc: 65.6% (6557/10000)
[Test]  Epoch: 15	Loss: 0.041529	Acc: 65.9% (6589/10000)
[Test]  Epoch: 16	Loss: 0.042552	Acc: 65.2% (6515/10000)
[Test]  Epoch: 17	Loss: 0.041821	Acc: 65.5% (6550/10000)
[Test]  Epoch: 18	Loss: 0.041686	Acc: 65.5% (6554/10000)
[Test]  Epoch: 19	Loss: 0.042936	Acc: 64.9% (6486/10000)
[Test]  Epoch: 20	Loss: 0.041428	Acc: 65.5% (6550/10000)
[Test]  Epoch: 21	Loss: 0.041797	Acc: 65.5% (6550/10000)
[Test]  Epoch: 22	Loss: 0.041258	Acc: 65.8% (6585/10000)
[Test]  Epoch: 23	Loss: 0.041175	Acc: 65.6% (6560/10000)
[Test]  Epoch: 24	Loss: 0.040947	Acc: 66.1% (6607/10000)
[Test]  Epoch: 25	Loss: 0.042018	Acc: 65.2% (6521/10000)
[Test]  Epoch: 26	Loss: 0.041655	Acc: 65.5% (6545/10000)
[Test]  Epoch: 27	Loss: 0.041483	Acc: 65.4% (6544/10000)
[Test]  Epoch: 28	Loss: 0.040988	Acc: 65.7% (6569/10000)
[Test]  Epoch: 29	Loss: 0.041155	Acc: 65.5% (6553/10000)
[Test]  Epoch: 30	Loss: 0.040979	Acc: 66.0% (6600/10000)
[Test]  Epoch: 31	Loss: 0.040602	Acc: 65.8% (6575/10000)
[Test]  Epoch: 32	Loss: 0.039634	Acc: 66.0% (6598/10000)
[Test]  Epoch: 33	Loss: 0.039589	Acc: 66.3% (6626/10000)
[Test]  Epoch: 34	Loss: 0.039793	Acc: 66.0% (6596/10000)
[Test]  Epoch: 35	Loss: 0.039725	Acc: 66.3% (6628/10000)
[Test]  Epoch: 36	Loss: 0.039217	Acc: 65.9% (6586/10000)
[Test]  Epoch: 37	Loss: 0.039668	Acc: 66.0% (6601/10000)
[Test]  Epoch: 38	Loss: 0.040916	Acc: 65.7% (6573/10000)
[Test]  Epoch: 39	Loss: 0.038958	Acc: 66.3% (6630/10000)
[Test]  Epoch: 40	Loss: 0.038830	Acc: 66.1% (6610/10000)
[Test]  Epoch: 41	Loss: 0.039156	Acc: 65.9% (6592/10000)
[Test]  Epoch: 42	Loss: 0.038985	Acc: 66.0% (6602/10000)
[Test]  Epoch: 43	Loss: 0.039224	Acc: 66.0% (6601/10000)
[Test]  Epoch: 44	Loss: 0.039309	Acc: 66.2% (6619/10000)
[Test]  Epoch: 45	Loss: 0.038584	Acc: 66.2% (6619/10000)
[Test]  Epoch: 46	Loss: 0.039280	Acc: 66.1% (6607/10000)
[Test]  Epoch: 47	Loss: 0.039871	Acc: 65.7% (6565/10000)
[Test]  Epoch: 48	Loss: 0.039169	Acc: 65.8% (6576/10000)
[Test]  Epoch: 49	Loss: 0.038236	Acc: 66.1% (6611/10000)
[Test]  Epoch: 50	Loss: 0.038510	Acc: 66.2% (6615/10000)
[Test]  Epoch: 51	Loss: 0.038420	Acc: 66.2% (6620/10000)
[Test]  Epoch: 52	Loss: 0.038692	Acc: 65.8% (6582/10000)
[Test]  Epoch: 53	Loss: 0.038085	Acc: 66.0% (6602/10000)
[Test]  Epoch: 54	Loss: 0.038458	Acc: 66.1% (6610/10000)
[Test]  Epoch: 55	Loss: 0.038187	Acc: 66.0% (6605/10000)
[Test]  Epoch: 56	Loss: 0.038094	Acc: 66.0% (6598/10000)
[Test]  Epoch: 57	Loss: 0.038546	Acc: 65.8% (6583/10000)
[Test]  Epoch: 58	Loss: 0.038802	Acc: 65.8% (6577/10000)
[Test]  Epoch: 59	Loss: 0.038126	Acc: 66.3% (6633/10000)
[Test]  Epoch: 60	Loss: 0.038649	Acc: 65.7% (6570/10000)
[Test]  Epoch: 61	Loss: 0.038082	Acc: 65.7% (6567/10000)
[Test]  Epoch: 62	Loss: 0.037866	Acc: 66.3% (6629/10000)
[Test]  Epoch: 63	Loss: 0.038516	Acc: 66.2% (6618/10000)
[Test]  Epoch: 64	Loss: 0.037928	Acc: 65.7% (6570/10000)
[Test]  Epoch: 65	Loss: 0.038623	Acc: 65.9% (6591/10000)
[Test]  Epoch: 66	Loss: 0.038059	Acc: 65.9% (6591/10000)
[Test]  Epoch: 67	Loss: 0.037909	Acc: 66.2% (6618/10000)
[Test]  Epoch: 68	Loss: 0.037591	Acc: 65.7% (6572/10000)
[Test]  Epoch: 69	Loss: 0.038312	Acc: 65.9% (6591/10000)
[Test]  Epoch: 70	Loss: 0.037985	Acc: 65.8% (6584/10000)
[Test]  Epoch: 71	Loss: 0.037195	Acc: 66.4% (6639/10000)
[Test]  Epoch: 72	Loss: 0.037903	Acc: 65.9% (6590/10000)
[Test]  Epoch: 73	Loss: 0.037788	Acc: 65.8% (6583/10000)
[Test]  Epoch: 74	Loss: 0.037685	Acc: 66.2% (6623/10000)
[Test]  Epoch: 75	Loss: 0.037644	Acc: 66.6% (6664/10000)
[Test]  Epoch: 76	Loss: 0.037849	Acc: 65.9% (6592/10000)
[Test]  Epoch: 77	Loss: 0.037825	Acc: 66.1% (6613/10000)
[Test]  Epoch: 78	Loss: 0.037921	Acc: 66.1% (6608/10000)
[Test]  Epoch: 79	Loss: 0.037623	Acc: 66.3% (6630/10000)
[Test]  Epoch: 80	Loss: 0.037924	Acc: 65.6% (6564/10000)
[Test]  Epoch: 81	Loss: 0.037802	Acc: 65.9% (6588/10000)
[Test]  Epoch: 82	Loss: 0.038491	Acc: 66.0% (6595/10000)
[Test]  Epoch: 83	Loss: 0.037696	Acc: 66.1% (6613/10000)
[Test]  Epoch: 84	Loss: 0.037545	Acc: 66.0% (6602/10000)
[Test]  Epoch: 85	Loss: 0.037919	Acc: 66.0% (6595/10000)
[Test]  Epoch: 86	Loss: 0.037294	Acc: 65.9% (6593/10000)
[Test]  Epoch: 87	Loss: 0.037461	Acc: 66.2% (6622/10000)
[Test]  Epoch: 88	Loss: 0.037760	Acc: 66.1% (6608/10000)
[Test]  Epoch: 89	Loss: 0.037176	Acc: 65.9% (6588/10000)
[Test]  Epoch: 90	Loss: 0.037770	Acc: 66.1% (6611/10000)
[Test]  Epoch: 91	Loss: 0.037986	Acc: 66.1% (6612/10000)
[Test]  Epoch: 92	Loss: 0.037136	Acc: 66.5% (6645/10000)
[Test]  Epoch: 93	Loss: 0.037695	Acc: 65.8% (6577/10000)
[Test]  Epoch: 94	Loss: 0.037510	Acc: 66.2% (6618/10000)
[Test]  Epoch: 95	Loss: 0.037473	Acc: 65.9% (6586/10000)
[Test]  Epoch: 96	Loss: 0.037496	Acc: 66.0% (6601/10000)
[Test]  Epoch: 97	Loss: 0.037757	Acc: 66.1% (6606/10000)
[Test]  Epoch: 98	Loss: 0.037533	Acc: 66.6% (6662/10000)
[Test]  Epoch: 99	Loss: 0.037458	Acc: 66.0% (6598/10000)
[Test]  Epoch: 100	Loss: 0.037661	Acc: 66.1% (6611/10000)
===========finish==========
['2024-08-19', '00:25:21.364346', '100', 'test', '0.03766085258722305', '66.11', '68.0']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.055173	Acc: 60.6% (6064/10000)
[Test]  Epoch: 2	Loss: 0.048251	Acc: 64.0% (6397/10000)
[Test]  Epoch: 3	Loss: 0.046202	Acc: 64.7% (6466/10000)
[Test]  Epoch: 4	Loss: 0.046884	Acc: 64.3% (6434/10000)
[Test]  Epoch: 5	Loss: 0.045541	Acc: 65.1% (6509/10000)
[Test]  Epoch: 6	Loss: 0.044884	Acc: 64.8% (6478/10000)
[Test]  Epoch: 7	Loss: 0.044091	Acc: 64.7% (6473/10000)
[Test]  Epoch: 8	Loss: 0.043199	Acc: 65.6% (6560/10000)
[Test]  Epoch: 9	Loss: 0.043969	Acc: 65.1% (6509/10000)
[Test]  Epoch: 10	Loss: 0.043272	Acc: 65.2% (6521/10000)
[Test]  Epoch: 11	Loss: 0.042696	Acc: 65.2% (6524/10000)
[Test]  Epoch: 12	Loss: 0.042567	Acc: 65.3% (6531/10000)
[Test]  Epoch: 13	Loss: 0.042636	Acc: 65.0% (6497/10000)
[Test]  Epoch: 14	Loss: 0.042342	Acc: 65.3% (6527/10000)
[Test]  Epoch: 15	Loss: 0.043104	Acc: 64.6% (6457/10000)
[Test]  Epoch: 16	Loss: 0.042051	Acc: 65.6% (6556/10000)
[Test]  Epoch: 17	Loss: 0.041204	Acc: 65.8% (6585/10000)
[Test]  Epoch: 18	Loss: 0.042522	Acc: 65.0% (6500/10000)
[Test]  Epoch: 19	Loss: 0.041922	Acc: 65.7% (6574/10000)
[Test]  Epoch: 20	Loss: 0.041691	Acc: 65.3% (6529/10000)
[Test]  Epoch: 21	Loss: 0.041800	Acc: 65.3% (6532/10000)
[Test]  Epoch: 22	Loss: 0.041991	Acc: 65.3% (6532/10000)
[Test]  Epoch: 23	Loss: 0.041764	Acc: 65.1% (6513/10000)
[Test]  Epoch: 24	Loss: 0.041781	Acc: 65.2% (6520/10000)
[Test]  Epoch: 25	Loss: 0.040898	Acc: 65.0% (6504/10000)
[Test]  Epoch: 26	Loss: 0.041068	Acc: 65.0% (6503/10000)
[Test]  Epoch: 27	Loss: 0.041750	Acc: 64.6% (6463/10000)
[Test]  Epoch: 28	Loss: 0.040683	Acc: 64.9% (6488/10000)
[Test]  Epoch: 29	Loss: 0.041396	Acc: 64.4% (6444/10000)
[Test]  Epoch: 30	Loss: 0.041898	Acc: 64.7% (6471/10000)
[Test]  Epoch: 31	Loss: 0.041574	Acc: 64.8% (6481/10000)
[Test]  Epoch: 32	Loss: 0.041014	Acc: 64.8% (6484/10000)
[Test]  Epoch: 33	Loss: 0.041142	Acc: 65.2% (6518/10000)
[Test]  Epoch: 34	Loss: 0.041430	Acc: 65.1% (6507/10000)
[Test]  Epoch: 35	Loss: 0.041520	Acc: 64.9% (6487/10000)
[Test]  Epoch: 36	Loss: 0.040429	Acc: 65.1% (6511/10000)
[Test]  Epoch: 37	Loss: 0.040364	Acc: 65.2% (6520/10000)
[Test]  Epoch: 38	Loss: 0.041543	Acc: 64.3% (6432/10000)
[Test]  Epoch: 39	Loss: 0.041239	Acc: 64.6% (6456/10000)
[Test]  Epoch: 40	Loss: 0.040060	Acc: 64.9% (6489/10000)
[Test]  Epoch: 41	Loss: 0.040199	Acc: 65.2% (6515/10000)
[Test]  Epoch: 42	Loss: 0.039717	Acc: 64.8% (6483/10000)
[Test]  Epoch: 43	Loss: 0.039800	Acc: 65.5% (6550/10000)
[Test]  Epoch: 44	Loss: 0.039297	Acc: 66.4% (6637/10000)
[Test]  Epoch: 45	Loss: 0.039149	Acc: 65.6% (6561/10000)
[Test]  Epoch: 46	Loss: 0.040472	Acc: 64.8% (6483/10000)
[Test]  Epoch: 47	Loss: 0.040186	Acc: 65.2% (6521/10000)
[Test]  Epoch: 48	Loss: 0.038632	Acc: 65.4% (6542/10000)
[Test]  Epoch: 49	Loss: 0.038607	Acc: 65.5% (6545/10000)
[Test]  Epoch: 50	Loss: 0.039025	Acc: 65.8% (6575/10000)
[Test]  Epoch: 51	Loss: 0.039104	Acc: 65.5% (6545/10000)
[Test]  Epoch: 52	Loss: 0.039144	Acc: 64.9% (6492/10000)
[Test]  Epoch: 53	Loss: 0.038818	Acc: 65.3% (6531/10000)
[Test]  Epoch: 54	Loss: 0.038958	Acc: 65.3% (6528/10000)
[Test]  Epoch: 55	Loss: 0.038698	Acc: 65.4% (6541/10000)
[Test]  Epoch: 56	Loss: 0.038929	Acc: 65.0% (6500/10000)
[Test]  Epoch: 57	Loss: 0.038163	Acc: 65.3% (6534/10000)
[Test]  Epoch: 58	Loss: 0.039011	Acc: 65.5% (6555/10000)
[Test]  Epoch: 59	Loss: 0.038626	Acc: 65.6% (6563/10000)
[Test]  Epoch: 60	Loss: 0.038500	Acc: 65.3% (6533/10000)
[Test]  Epoch: 61	Loss: 0.037889	Acc: 65.5% (6552/10000)
[Test]  Epoch: 62	Loss: 0.038493	Acc: 65.1% (6510/10000)
[Test]  Epoch: 63	Loss: 0.038488	Acc: 65.0% (6496/10000)
[Test]  Epoch: 64	Loss: 0.038286	Acc: 65.1% (6508/10000)
[Test]  Epoch: 65	Loss: 0.038763	Acc: 65.1% (6513/10000)
[Test]  Epoch: 66	Loss: 0.038235	Acc: 65.5% (6552/10000)
[Test]  Epoch: 67	Loss: 0.038291	Acc: 65.3% (6531/10000)
[Test]  Epoch: 68	Loss: 0.038228	Acc: 65.4% (6539/10000)
[Test]  Epoch: 69	Loss: 0.038026	Acc: 65.1% (6510/10000)
[Test]  Epoch: 70	Loss: 0.038819	Acc: 65.1% (6507/10000)
[Test]  Epoch: 71	Loss: 0.037743	Acc: 65.3% (6534/10000)
[Test]  Epoch: 72	Loss: 0.037946	Acc: 65.3% (6534/10000)
[Test]  Epoch: 73	Loss: 0.037955	Acc: 65.3% (6534/10000)
[Test]  Epoch: 74	Loss: 0.037651	Acc: 65.4% (6540/10000)
[Test]  Epoch: 75	Loss: 0.038083	Acc: 65.5% (6552/10000)
[Test]  Epoch: 76	Loss: 0.038732	Acc: 65.2% (6519/10000)
[Test]  Epoch: 77	Loss: 0.037856	Acc: 65.6% (6562/10000)
[Test]  Epoch: 78	Loss: 0.038272	Acc: 65.0% (6503/10000)
[Test]  Epoch: 79	Loss: 0.038102	Acc: 65.5% (6546/10000)
[Test]  Epoch: 80	Loss: 0.038456	Acc: 65.1% (6509/10000)
[Test]  Epoch: 81	Loss: 0.038214	Acc: 65.2% (6515/10000)
[Test]  Epoch: 82	Loss: 0.038177	Acc: 65.6% (6559/10000)
[Test]  Epoch: 83	Loss: 0.037767	Acc: 65.5% (6553/10000)
[Test]  Epoch: 84	Loss: 0.038024	Acc: 65.3% (6531/10000)
[Test]  Epoch: 85	Loss: 0.038089	Acc: 65.5% (6545/10000)
[Test]  Epoch: 86	Loss: 0.037798	Acc: 65.6% (6564/10000)
[Test]  Epoch: 87	Loss: 0.037704	Acc: 65.4% (6542/10000)
[Test]  Epoch: 88	Loss: 0.037863	Acc: 65.6% (6556/10000)
[Test]  Epoch: 89	Loss: 0.037143	Acc: 65.6% (6557/10000)
[Test]  Epoch: 90	Loss: 0.038078	Acc: 65.5% (6548/10000)
[Test]  Epoch: 91	Loss: 0.038249	Acc: 65.3% (6532/10000)
[Test]  Epoch: 92	Loss: 0.037255	Acc: 65.8% (6575/10000)
[Test]  Epoch: 93	Loss: 0.037762	Acc: 65.3% (6532/10000)
[Test]  Epoch: 94	Loss: 0.037817	Acc: 65.5% (6546/10000)
[Test]  Epoch: 95	Loss: 0.037564	Acc: 65.4% (6536/10000)
[Test]  Epoch: 96	Loss: 0.038108	Acc: 65.1% (6514/10000)
[Test]  Epoch: 97	Loss: 0.037996	Acc: 65.0% (6503/10000)
[Test]  Epoch: 98	Loss: 0.038044	Acc: 65.2% (6521/10000)
[Test]  Epoch: 99	Loss: 0.037645	Acc: 65.5% (6551/10000)
[Test]  Epoch: 100	Loss: 0.037756	Acc: 65.7% (6566/10000)
===========finish==========
['2024-08-19', '00:27:45.645238', '100', 'test', '0.03775613124370575', '65.66', '66.37']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight', 'features.15.weight', 'features.35.weight', 'features.28.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.038431	Acc: 60.6% (6059/10000)
[Test]  Epoch: 2	Loss: 0.038886	Acc: 65.5% (6552/10000)
[Test]  Epoch: 3	Loss: 0.038137	Acc: 66.2% (6616/10000)
[Test]  Epoch: 4	Loss: 0.039049	Acc: 65.5% (6553/10000)
[Test]  Epoch: 5	Loss: 0.038445	Acc: 65.5% (6545/10000)
[Test]  Epoch: 6	Loss: 0.039160	Acc: 65.0% (6505/10000)
[Test]  Epoch: 7	Loss: 0.037989	Acc: 65.9% (6590/10000)
[Test]  Epoch: 8	Loss: 0.037625	Acc: 65.8% (6583/10000)
[Test]  Epoch: 9	Loss: 0.037770	Acc: 65.8% (6580/10000)
[Test]  Epoch: 10	Loss: 0.037127	Acc: 66.7% (6672/10000)
[Test]  Epoch: 11	Loss: 0.037009	Acc: 65.8% (6581/10000)
[Test]  Epoch: 12	Loss: 0.037577	Acc: 65.6% (6560/10000)
[Test]  Epoch: 13	Loss: 0.038228	Acc: 65.6% (6563/10000)
[Test]  Epoch: 14	Loss: 0.037733	Acc: 65.9% (6588/10000)
[Test]  Epoch: 15	Loss: 0.037865	Acc: 65.4% (6538/10000)
[Test]  Epoch: 16	Loss: 0.037485	Acc: 65.8% (6582/10000)
[Test]  Epoch: 17	Loss: 0.036305	Acc: 66.2% (6622/10000)
[Test]  Epoch: 18	Loss: 0.036699	Acc: 65.9% (6592/10000)
[Test]  Epoch: 19	Loss: 0.036349	Acc: 66.1% (6611/10000)
[Test]  Epoch: 20	Loss: 0.036647	Acc: 65.8% (6576/10000)
[Test]  Epoch: 21	Loss: 0.036896	Acc: 65.8% (6578/10000)
[Test]  Epoch: 22	Loss: 0.036955	Acc: 65.9% (6586/10000)
[Test]  Epoch: 23	Loss: 0.036187	Acc: 65.8% (6584/10000)
[Test]  Epoch: 24	Loss: 0.037100	Acc: 65.6% (6563/10000)
[Test]  Epoch: 25	Loss: 0.036346	Acc: 65.9% (6590/10000)
[Test]  Epoch: 26	Loss: 0.036185	Acc: 66.0% (6599/10000)
[Test]  Epoch: 27	Loss: 0.035636	Acc: 66.4% (6638/10000)
[Test]  Epoch: 28	Loss: 0.035783	Acc: 66.2% (6625/10000)
[Test]  Epoch: 29	Loss: 0.035753	Acc: 65.8% (6582/10000)
[Test]  Epoch: 30	Loss: 0.035919	Acc: 65.9% (6591/10000)
[Test]  Epoch: 31	Loss: 0.035704	Acc: 66.0% (6600/10000)
[Test]  Epoch: 32	Loss: 0.035317	Acc: 66.2% (6617/10000)
[Test]  Epoch: 33	Loss: 0.035354	Acc: 66.0% (6597/10000)
[Test]  Epoch: 34	Loss: 0.035141	Acc: 66.3% (6628/10000)
[Test]  Epoch: 35	Loss: 0.035128	Acc: 65.9% (6593/10000)
[Test]  Epoch: 36	Loss: 0.035084	Acc: 66.1% (6608/10000)
[Test]  Epoch: 37	Loss: 0.034985	Acc: 65.9% (6594/10000)
[Test]  Epoch: 38	Loss: 0.035619	Acc: 65.8% (6579/10000)
[Test]  Epoch: 39	Loss: 0.034955	Acc: 65.9% (6594/10000)
[Test]  Epoch: 40	Loss: 0.034633	Acc: 66.2% (6620/10000)
[Test]  Epoch: 41	Loss: 0.034874	Acc: 66.2% (6618/10000)
[Test]  Epoch: 42	Loss: 0.035144	Acc: 65.7% (6571/10000)
[Test]  Epoch: 43	Loss: 0.035309	Acc: 66.3% (6626/10000)
[Test]  Epoch: 44	Loss: 0.034608	Acc: 66.1% (6614/10000)
[Test]  Epoch: 45	Loss: 0.034373	Acc: 66.2% (6617/10000)
[Test]  Epoch: 46	Loss: 0.034901	Acc: 65.7% (6571/10000)
[Test]  Epoch: 47	Loss: 0.034306	Acc: 66.0% (6603/10000)
[Test]  Epoch: 48	Loss: 0.034143	Acc: 65.8% (6577/10000)
[Test]  Epoch: 49	Loss: 0.033790	Acc: 65.8% (6576/10000)
[Test]  Epoch: 50	Loss: 0.034259	Acc: 65.8% (6577/10000)
[Test]  Epoch: 51	Loss: 0.034050	Acc: 66.1% (6614/10000)
[Test]  Epoch: 52	Loss: 0.034565	Acc: 65.3% (6534/10000)
[Test]  Epoch: 53	Loss: 0.034141	Acc: 65.6% (6562/10000)
[Test]  Epoch: 54	Loss: 0.034494	Acc: 65.7% (6565/10000)
[Test]  Epoch: 55	Loss: 0.034038	Acc: 65.8% (6585/10000)
[Test]  Epoch: 56	Loss: 0.034100	Acc: 65.7% (6571/10000)
[Test]  Epoch: 57	Loss: 0.033814	Acc: 65.8% (6584/10000)
[Test]  Epoch: 58	Loss: 0.034610	Acc: 65.5% (6555/10000)
[Test]  Epoch: 59	Loss: 0.033764	Acc: 66.4% (6638/10000)
[Test]  Epoch: 60	Loss: 0.033812	Acc: 65.8% (6576/10000)
[Test]  Epoch: 61	Loss: 0.033663	Acc: 65.9% (6586/10000)
[Test]  Epoch: 62	Loss: 0.034271	Acc: 65.6% (6561/10000)
[Test]  Epoch: 63	Loss: 0.033911	Acc: 65.9% (6594/10000)
[Test]  Epoch: 64	Loss: 0.034111	Acc: 65.9% (6591/10000)
[Test]  Epoch: 65	Loss: 0.033882	Acc: 65.8% (6579/10000)
[Test]  Epoch: 66	Loss: 0.033887	Acc: 66.0% (6601/10000)
[Test]  Epoch: 67	Loss: 0.034158	Acc: 66.3% (6626/10000)
[Test]  Epoch: 68	Loss: 0.034163	Acc: 65.9% (6593/10000)
[Test]  Epoch: 69	Loss: 0.033956	Acc: 65.8% (6584/10000)
[Test]  Epoch: 70	Loss: 0.033837	Acc: 66.0% (6599/10000)
[Test]  Epoch: 71	Loss: 0.033612	Acc: 65.9% (6593/10000)
[Test]  Epoch: 72	Loss: 0.033751	Acc: 65.8% (6576/10000)
[Test]  Epoch: 73	Loss: 0.033956	Acc: 65.6% (6557/10000)
[Test]  Epoch: 74	Loss: 0.033487	Acc: 65.8% (6582/10000)
[Test]  Epoch: 75	Loss: 0.033818	Acc: 66.2% (6617/10000)
[Test]  Epoch: 76	Loss: 0.033774	Acc: 65.7% (6568/10000)
[Test]  Epoch: 77	Loss: 0.033730	Acc: 65.6% (6559/10000)
[Test]  Epoch: 78	Loss: 0.034078	Acc: 65.8% (6575/10000)
[Test]  Epoch: 79	Loss: 0.033520	Acc: 65.8% (6584/10000)
[Test]  Epoch: 80	Loss: 0.033744	Acc: 65.7% (6569/10000)
[Test]  Epoch: 81	Loss: 0.033767	Acc: 65.8% (6584/10000)
[Test]  Epoch: 82	Loss: 0.033564	Acc: 65.8% (6585/10000)
[Test]  Epoch: 83	Loss: 0.033474	Acc: 65.8% (6582/10000)
[Test]  Epoch: 84	Loss: 0.033517	Acc: 65.9% (6594/10000)
[Test]  Epoch: 85	Loss: 0.033786	Acc: 66.0% (6598/10000)
[Test]  Epoch: 86	Loss: 0.033600	Acc: 65.7% (6573/10000)
[Test]  Epoch: 87	Loss: 0.033097	Acc: 66.1% (6608/10000)
[Test]  Epoch: 88	Loss: 0.033255	Acc: 66.0% (6596/10000)
[Test]  Epoch: 89	Loss: 0.033026	Acc: 66.5% (6653/10000)
[Test]  Epoch: 90	Loss: 0.033706	Acc: 65.7% (6569/10000)
[Test]  Epoch: 91	Loss: 0.034316	Acc: 65.8% (6575/10000)
[Test]  Epoch: 92	Loss: 0.033004	Acc: 66.0% (6604/10000)
[Test]  Epoch: 93	Loss: 0.033615	Acc: 65.8% (6577/10000)
[Test]  Epoch: 94	Loss: 0.033611	Acc: 66.1% (6611/10000)
[Test]  Epoch: 95	Loss: 0.033455	Acc: 65.8% (6578/10000)
[Test]  Epoch: 96	Loss: 0.033910	Acc: 65.7% (6572/10000)
[Test]  Epoch: 97	Loss: 0.033852	Acc: 65.7% (6574/10000)
[Test]  Epoch: 98	Loss: 0.033457	Acc: 66.0% (6599/10000)
[Test]  Epoch: 99	Loss: 0.033435	Acc: 66.0% (6597/10000)
[Test]  Epoch: 100	Loss: 0.033370	Acc: 65.5% (6554/10000)
===========finish==========
['2024-08-19', '00:30:20.135583', '100', 'test', '0.03337006937861443', '65.54', '66.72']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight', 'features.15.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.041490	Acc: 56.7% (5667/10000)
[Test]  Epoch: 2	Loss: 0.039736	Acc: 64.1% (6408/10000)
[Test]  Epoch: 3	Loss: 0.039443	Acc: 64.8% (6475/10000)
[Test]  Epoch: 4	Loss: 0.039450	Acc: 64.7% (6467/10000)
[Test]  Epoch: 5	Loss: 0.039705	Acc: 64.2% (6424/10000)
[Test]  Epoch: 6	Loss: 0.038194	Acc: 65.0% (6497/10000)
[Test]  Epoch: 7	Loss: 0.037538	Acc: 65.3% (6528/10000)
[Test]  Epoch: 8	Loss: 0.037545	Acc: 65.3% (6530/10000)
[Test]  Epoch: 9	Loss: 0.037001	Acc: 65.5% (6545/10000)
[Test]  Epoch: 10	Loss: 0.037147	Acc: 65.2% (6520/10000)
[Test]  Epoch: 11	Loss: 0.036871	Acc: 65.4% (6544/10000)
[Test]  Epoch: 12	Loss: 0.037416	Acc: 65.3% (6529/10000)
[Test]  Epoch: 13	Loss: 0.037475	Acc: 65.1% (6506/10000)
[Test]  Epoch: 14	Loss: 0.037859	Acc: 64.9% (6488/10000)
[Test]  Epoch: 15	Loss: 0.037861	Acc: 64.4% (6438/10000)
[Test]  Epoch: 16	Loss: 0.037750	Acc: 64.7% (6474/10000)
[Test]  Epoch: 17	Loss: 0.036757	Acc: 65.6% (6559/10000)
[Test]  Epoch: 18	Loss: 0.037389	Acc: 64.6% (6464/10000)
[Test]  Epoch: 19	Loss: 0.037322	Acc: 64.9% (6490/10000)
[Test]  Epoch: 20	Loss: 0.037245	Acc: 64.6% (6458/10000)
[Test]  Epoch: 21	Loss: 0.036909	Acc: 65.1% (6506/10000)
[Test]  Epoch: 22	Loss: 0.036869	Acc: 65.3% (6534/10000)
[Test]  Epoch: 23	Loss: 0.036602	Acc: 65.0% (6504/10000)
[Test]  Epoch: 24	Loss: 0.036981	Acc: 64.7% (6470/10000)
[Test]  Epoch: 25	Loss: 0.036219	Acc: 65.2% (6519/10000)
[Test]  Epoch: 26	Loss: 0.035790	Acc: 65.5% (6550/10000)
[Test]  Epoch: 27	Loss: 0.035632	Acc: 65.4% (6537/10000)
[Test]  Epoch: 28	Loss: 0.035824	Acc: 65.2% (6522/10000)
[Test]  Epoch: 29	Loss: 0.036758	Acc: 64.5% (6452/10000)
[Test]  Epoch: 30	Loss: 0.035926	Acc: 65.3% (6526/10000)
[Test]  Epoch: 31	Loss: 0.036028	Acc: 65.1% (6512/10000)
[Test]  Epoch: 32	Loss: 0.035776	Acc: 65.0% (6496/10000)
[Test]  Epoch: 33	Loss: 0.035762	Acc: 65.3% (6534/10000)
[Test]  Epoch: 34	Loss: 0.034941	Acc: 65.4% (6542/10000)
[Test]  Epoch: 35	Loss: 0.035480	Acc: 65.2% (6516/10000)
[Test]  Epoch: 36	Loss: 0.034912	Acc: 65.3% (6528/10000)
[Test]  Epoch: 37	Loss: 0.035109	Acc: 65.1% (6509/10000)
[Test]  Epoch: 38	Loss: 0.035520	Acc: 64.9% (6490/10000)
[Test]  Epoch: 39	Loss: 0.035368	Acc: 64.9% (6490/10000)
[Test]  Epoch: 40	Loss: 0.035124	Acc: 65.3% (6533/10000)
[Test]  Epoch: 41	Loss: 0.034628	Acc: 65.6% (6558/10000)
[Test]  Epoch: 42	Loss: 0.034677	Acc: 65.1% (6513/10000)
[Test]  Epoch: 43	Loss: 0.035052	Acc: 65.5% (6552/10000)
[Test]  Epoch: 44	Loss: 0.034227	Acc: 65.2% (6522/10000)
[Test]  Epoch: 45	Loss: 0.034194	Acc: 65.8% (6577/10000)
[Test]  Epoch: 46	Loss: 0.035023	Acc: 65.0% (6502/10000)
[Test]  Epoch: 47	Loss: 0.034357	Acc: 65.3% (6534/10000)
[Test]  Epoch: 48	Loss: 0.033771	Acc: 65.4% (6541/10000)
[Test]  Epoch: 49	Loss: 0.034192	Acc: 65.4% (6540/10000)
[Test]  Epoch: 50	Loss: 0.034100	Acc: 65.1% (6514/10000)
[Test]  Epoch: 51	Loss: 0.034498	Acc: 65.4% (6536/10000)
[Test]  Epoch: 52	Loss: 0.034331	Acc: 64.8% (6484/10000)
[Test]  Epoch: 53	Loss: 0.034115	Acc: 64.8% (6477/10000)
[Test]  Epoch: 54	Loss: 0.034412	Acc: 65.0% (6502/10000)
[Test]  Epoch: 55	Loss: 0.033885	Acc: 65.8% (6575/10000)
[Test]  Epoch: 56	Loss: 0.034073	Acc: 65.5% (6548/10000)
[Test]  Epoch: 57	Loss: 0.033767	Acc: 65.4% (6540/10000)
[Test]  Epoch: 58	Loss: 0.034477	Acc: 65.0% (6504/10000)
[Test]  Epoch: 59	Loss: 0.033868	Acc: 65.5% (6549/10000)
[Test]  Epoch: 60	Loss: 0.033551	Acc: 65.6% (6558/10000)
[Test]  Epoch: 61	Loss: 0.033745	Acc: 65.0% (6496/10000)
[Test]  Epoch: 62	Loss: 0.034429	Acc: 64.9% (6491/10000)
[Test]  Epoch: 63	Loss: 0.033890	Acc: 65.3% (6532/10000)
[Test]  Epoch: 64	Loss: 0.033770	Acc: 65.2% (6524/10000)
[Test]  Epoch: 65	Loss: 0.033599	Acc: 65.5% (6545/10000)
[Test]  Epoch: 66	Loss: 0.033694	Acc: 65.5% (6545/10000)
[Test]  Epoch: 67	Loss: 0.033878	Acc: 65.3% (6534/10000)
[Test]  Epoch: 68	Loss: 0.033854	Acc: 65.7% (6574/10000)
[Test]  Epoch: 69	Loss: 0.033716	Acc: 65.6% (6556/10000)
[Test]  Epoch: 70	Loss: 0.034042	Acc: 65.0% (6500/10000)
[Test]  Epoch: 71	Loss: 0.033512	Acc: 65.5% (6546/10000)
[Test]  Epoch: 72	Loss: 0.033635	Acc: 65.5% (6546/10000)
[Test]  Epoch: 73	Loss: 0.034237	Acc: 64.9% (6489/10000)
[Test]  Epoch: 74	Loss: 0.033396	Acc: 65.8% (6580/10000)
[Test]  Epoch: 75	Loss: 0.033699	Acc: 65.5% (6549/10000)
[Test]  Epoch: 76	Loss: 0.033836	Acc: 65.4% (6543/10000)
[Test]  Epoch: 77	Loss: 0.033586	Acc: 65.9% (6591/10000)
[Test]  Epoch: 78	Loss: 0.033940	Acc: 65.2% (6515/10000)
[Test]  Epoch: 79	Loss: 0.033660	Acc: 65.6% (6560/10000)
[Test]  Epoch: 80	Loss: 0.033875	Acc: 65.1% (6509/10000)
[Test]  Epoch: 81	Loss: 0.033858	Acc: 65.6% (6563/10000)
[Test]  Epoch: 82	Loss: 0.033474	Acc: 65.9% (6589/10000)
[Test]  Epoch: 83	Loss: 0.033681	Acc: 65.2% (6518/10000)
[Test]  Epoch: 84	Loss: 0.033692	Acc: 65.4% (6541/10000)
[Test]  Epoch: 85	Loss: 0.033318	Acc: 65.4% (6542/10000)
[Test]  Epoch: 86	Loss: 0.033351	Acc: 65.4% (6538/10000)
[Test]  Epoch: 87	Loss: 0.033398	Acc: 65.6% (6563/10000)
[Test]  Epoch: 88	Loss: 0.033193	Acc: 65.6% (6562/10000)
[Test]  Epoch: 89	Loss: 0.033342	Acc: 65.8% (6578/10000)
[Test]  Epoch: 90	Loss: 0.033779	Acc: 65.0% (6499/10000)
[Test]  Epoch: 91	Loss: 0.034024	Acc: 65.1% (6510/10000)
[Test]  Epoch: 92	Loss: 0.033198	Acc: 65.7% (6566/10000)
[Test]  Epoch: 93	Loss: 0.033790	Acc: 65.2% (6522/10000)
[Test]  Epoch: 94	Loss: 0.033600	Acc: 65.5% (6549/10000)
[Test]  Epoch: 95	Loss: 0.033484	Acc: 64.9% (6492/10000)
[Test]  Epoch: 96	Loss: 0.033701	Acc: 65.2% (6520/10000)
[Test]  Epoch: 97	Loss: 0.033451	Acc: 65.3% (6531/10000)
[Test]  Epoch: 98	Loss: 0.033553	Acc: 65.7% (6567/10000)
[Test]  Epoch: 99	Loss: 0.033669	Acc: 65.4% (6544/10000)
[Test]  Epoch: 100	Loss: 0.033247	Acc: 65.7% (6570/10000)
===========finish==========
['2024-08-19', '00:32:48.130868', '100', 'test', '0.033246937358379365', '65.7', '65.91']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight', 'features.15.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.11.weight', 'features.31.weight', 'features.21.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.055561	Acc: 53.1% (5308/10000)
[Test]  Epoch: 2	Loss: 0.038981	Acc: 62.5% (6247/10000)
[Test]  Epoch: 3	Loss: 0.036706	Acc: 63.9% (6387/10000)
[Test]  Epoch: 4	Loss: 0.036017	Acc: 64.2% (6415/10000)
[Test]  Epoch: 5	Loss: 0.036321	Acc: 63.9% (6392/10000)
[Test]  Epoch: 6	Loss: 0.035220	Acc: 65.0% (6501/10000)
[Test]  Epoch: 7	Loss: 0.034725	Acc: 65.2% (6521/10000)
[Test]  Epoch: 8	Loss: 0.035175	Acc: 64.5% (6455/10000)
[Test]  Epoch: 9	Loss: 0.034635	Acc: 65.3% (6534/10000)
[Test]  Epoch: 10	Loss: 0.034798	Acc: 64.6% (6458/10000)
[Test]  Epoch: 11	Loss: 0.034146	Acc: 64.9% (6493/10000)
[Test]  Epoch: 12	Loss: 0.034509	Acc: 64.9% (6490/10000)
[Test]  Epoch: 13	Loss: 0.034837	Acc: 64.8% (6476/10000)
[Test]  Epoch: 14	Loss: 0.034387	Acc: 65.4% (6541/10000)
[Test]  Epoch: 15	Loss: 0.034610	Acc: 65.0% (6496/10000)
[Test]  Epoch: 16	Loss: 0.034651	Acc: 64.5% (6451/10000)
[Test]  Epoch: 17	Loss: 0.033900	Acc: 65.1% (6508/10000)
[Test]  Epoch: 18	Loss: 0.034096	Acc: 65.3% (6527/10000)
[Test]  Epoch: 19	Loss: 0.034033	Acc: 65.6% (6563/10000)
[Test]  Epoch: 20	Loss: 0.034138	Acc: 65.2% (6521/10000)
[Test]  Epoch: 21	Loss: 0.034241	Acc: 65.2% (6520/10000)
[Test]  Epoch: 22	Loss: 0.034188	Acc: 65.1% (6508/10000)
[Test]  Epoch: 23	Loss: 0.033628	Acc: 65.3% (6532/10000)
[Test]  Epoch: 24	Loss: 0.034585	Acc: 64.8% (6475/10000)
[Test]  Epoch: 25	Loss: 0.033898	Acc: 65.0% (6502/10000)
[Test]  Epoch: 26	Loss: 0.033525	Acc: 64.7% (6465/10000)
[Test]  Epoch: 27	Loss: 0.033578	Acc: 65.4% (6536/10000)
[Test]  Epoch: 28	Loss: 0.033259	Acc: 64.8% (6480/10000)
[Test]  Epoch: 29	Loss: 0.033862	Acc: 64.4% (6437/10000)
[Test]  Epoch: 30	Loss: 0.033396	Acc: 65.2% (6524/10000)
[Test]  Epoch: 31	Loss: 0.033398	Acc: 65.1% (6511/10000)
[Test]  Epoch: 32	Loss: 0.033391	Acc: 65.2% (6518/10000)
[Test]  Epoch: 33	Loss: 0.033254	Acc: 65.3% (6526/10000)
[Test]  Epoch: 34	Loss: 0.033130	Acc: 65.1% (6510/10000)
[Test]  Epoch: 35	Loss: 0.032930	Acc: 65.3% (6535/10000)
[Test]  Epoch: 36	Loss: 0.032769	Acc: 65.0% (6503/10000)
[Test]  Epoch: 37	Loss: 0.032925	Acc: 64.9% (6494/10000)
[Test]  Epoch: 38	Loss: 0.033018	Acc: 64.9% (6492/10000)
[Test]  Epoch: 39	Loss: 0.032917	Acc: 65.3% (6535/10000)
[Test]  Epoch: 40	Loss: 0.032680	Acc: 65.1% (6513/10000)
[Test]  Epoch: 41	Loss: 0.032894	Acc: 65.0% (6501/10000)
[Test]  Epoch: 42	Loss: 0.032643	Acc: 65.3% (6531/10000)
[Test]  Epoch: 43	Loss: 0.032802	Acc: 65.3% (6529/10000)
[Test]  Epoch: 44	Loss: 0.032243	Acc: 65.4% (6541/10000)
[Test]  Epoch: 45	Loss: 0.032496	Acc: 65.5% (6550/10000)
[Test]  Epoch: 46	Loss: 0.032869	Acc: 65.1% (6509/10000)
[Test]  Epoch: 47	Loss: 0.032478	Acc: 65.1% (6508/10000)
[Test]  Epoch: 48	Loss: 0.032329	Acc: 65.2% (6525/10000)
[Test]  Epoch: 49	Loss: 0.032401	Acc: 65.5% (6555/10000)
[Test]  Epoch: 50	Loss: 0.032472	Acc: 64.7% (6469/10000)
[Test]  Epoch: 51	Loss: 0.032377	Acc: 65.1% (6506/10000)
[Test]  Epoch: 52	Loss: 0.032356	Acc: 64.7% (6468/10000)
[Test]  Epoch: 53	Loss: 0.031973	Acc: 65.0% (6502/10000)
[Test]  Epoch: 54	Loss: 0.032487	Acc: 64.6% (6460/10000)
[Test]  Epoch: 55	Loss: 0.032161	Acc: 65.3% (6527/10000)
[Test]  Epoch: 56	Loss: 0.032147	Acc: 65.2% (6522/10000)
[Test]  Epoch: 57	Loss: 0.032005	Acc: 65.0% (6503/10000)
[Test]  Epoch: 58	Loss: 0.032173	Acc: 65.2% (6520/10000)
[Test]  Epoch: 59	Loss: 0.031862	Acc: 65.0% (6499/10000)
[Test]  Epoch: 60	Loss: 0.031800	Acc: 65.1% (6514/10000)
[Test]  Epoch: 61	Loss: 0.031753	Acc: 65.2% (6519/10000)
[Test]  Epoch: 62	Loss: 0.031964	Acc: 65.1% (6509/10000)
[Test]  Epoch: 63	Loss: 0.031937	Acc: 64.9% (6489/10000)
[Test]  Epoch: 64	Loss: 0.032142	Acc: 64.5% (6455/10000)
[Test]  Epoch: 65	Loss: 0.031837	Acc: 64.9% (6494/10000)
[Test]  Epoch: 66	Loss: 0.031794	Acc: 64.9% (6494/10000)
[Test]  Epoch: 67	Loss: 0.032216	Acc: 65.5% (6554/10000)
[Test]  Epoch: 68	Loss: 0.031968	Acc: 65.2% (6516/10000)
[Test]  Epoch: 69	Loss: 0.031906	Acc: 64.8% (6485/10000)
[Test]  Epoch: 70	Loss: 0.032042	Acc: 64.9% (6486/10000)
[Test]  Epoch: 71	Loss: 0.031881	Acc: 65.1% (6514/10000)
[Test]  Epoch: 72	Loss: 0.031623	Acc: 65.4% (6537/10000)
[Test]  Epoch: 73	Loss: 0.031826	Acc: 64.7% (6473/10000)
[Test]  Epoch: 74	Loss: 0.031396	Acc: 65.3% (6530/10000)
[Test]  Epoch: 75	Loss: 0.031732	Acc: 65.6% (6559/10000)
[Test]  Epoch: 76	Loss: 0.031906	Acc: 64.8% (6485/10000)
[Test]  Epoch: 77	Loss: 0.031777	Acc: 65.2% (6519/10000)
[Test]  Epoch: 78	Loss: 0.031904	Acc: 64.6% (6462/10000)
[Test]  Epoch: 79	Loss: 0.031517	Acc: 65.3% (6533/10000)
[Test]  Epoch: 80	Loss: 0.031856	Acc: 65.0% (6496/10000)
[Test]  Epoch: 81	Loss: 0.031731	Acc: 64.5% (6449/10000)
[Test]  Epoch: 82	Loss: 0.031581	Acc: 64.8% (6485/10000)
[Test]  Epoch: 83	Loss: 0.031590	Acc: 65.3% (6526/10000)
[Test]  Epoch: 84	Loss: 0.031953	Acc: 64.9% (6487/10000)
[Test]  Epoch: 85	Loss: 0.031494	Acc: 64.5% (6448/10000)
[Test]  Epoch: 86	Loss: 0.031488	Acc: 65.1% (6510/10000)
[Test]  Epoch: 87	Loss: 0.031503	Acc: 65.1% (6511/10000)
[Test]  Epoch: 88	Loss: 0.031334	Acc: 65.3% (6527/10000)
[Test]  Epoch: 89	Loss: 0.031724	Acc: 65.0% (6497/10000)
[Test]  Epoch: 90	Loss: 0.031862	Acc: 64.8% (6480/10000)
[Test]  Epoch: 91	Loss: 0.031853	Acc: 64.8% (6480/10000)
[Test]  Epoch: 92	Loss: 0.031291	Acc: 65.2% (6524/10000)
[Test]  Epoch: 93	Loss: 0.031668	Acc: 65.4% (6541/10000)
[Test]  Epoch: 94	Loss: 0.031684	Acc: 64.7% (6471/10000)
[Test]  Epoch: 95	Loss: 0.031467	Acc: 64.6% (6463/10000)
[Test]  Epoch: 96	Loss: 0.031781	Acc: 64.9% (6486/10000)
[Test]  Epoch: 97	Loss: 0.031625	Acc: 64.6% (6464/10000)
[Test]  Epoch: 98	Loss: 0.031664	Acc: 65.2% (6519/10000)
[Test]  Epoch: 99	Loss: 0.031374	Acc: 65.1% (6510/10000)
[Test]  Epoch: 100	Loss: 0.031221	Acc: 65.3% (6530/10000)
===========finish==========
['2024-08-19', '00:35:17.690990', '100', 'test', '0.031220611536502837', '65.3', '65.63']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight', 'features.15.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.11.weight', 'features.31.weight', 'features.21.weight', 'features.38.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.071680	Acc: 2.3% (233/10000)
[Test]  Epoch: 2	Loss: 0.068411	Acc: 5.9% (593/10000)
[Test]  Epoch: 3	Loss: 0.066630	Acc: 6.5% (645/10000)
[Test]  Epoch: 4	Loss: 0.064414	Acc: 8.4% (843/10000)
[Test]  Epoch: 5	Loss: 0.063260	Acc: 9.8% (980/10000)
[Test]  Epoch: 6	Loss: 0.061914	Acc: 11.5% (1149/10000)
[Test]  Epoch: 7	Loss: 0.061137	Acc: 11.6% (1155/10000)
[Test]  Epoch: 8	Loss: 0.060021	Acc: 13.2% (1317/10000)
[Test]  Epoch: 9	Loss: 0.059142	Acc: 13.8% (1381/10000)
[Test]  Epoch: 10	Loss: 0.058255	Acc: 14.8% (1479/10000)
[Test]  Epoch: 11	Loss: 0.058694	Acc: 15.0% (1502/10000)
[Test]  Epoch: 12	Loss: 0.056369	Acc: 16.8% (1682/10000)
[Test]  Epoch: 13	Loss: 0.057153	Acc: 16.2% (1618/10000)
[Test]  Epoch: 14	Loss: 0.055762	Acc: 17.7% (1767/10000)
[Test]  Epoch: 15	Loss: 0.055880	Acc: 17.5% (1746/10000)
[Test]  Epoch: 16	Loss: 0.055144	Acc: 18.4% (1844/10000)
[Test]  Epoch: 17	Loss: 0.054350	Acc: 19.6% (1957/10000)
[Test]  Epoch: 18	Loss: 0.054506	Acc: 20.3% (2029/10000)
[Test]  Epoch: 19	Loss: 0.054558	Acc: 19.6% (1961/10000)
[Test]  Epoch: 20	Loss: 0.054808	Acc: 19.8% (1981/10000)
[Test]  Epoch: 21	Loss: 0.053509	Acc: 21.5% (2148/10000)
[Test]  Epoch: 22	Loss: 0.053389	Acc: 22.3% (2233/10000)
[Test]  Epoch: 23	Loss: 0.053104	Acc: 22.3% (2229/10000)
[Test]  Epoch: 24	Loss: 0.052963	Acc: 23.1% (2305/10000)
[Test]  Epoch: 25	Loss: 0.053082	Acc: 22.7% (2270/10000)
[Test]  Epoch: 26	Loss: 0.053767	Acc: 22.4% (2245/10000)
[Test]  Epoch: 27	Loss: 0.052623	Acc: 23.8% (2379/10000)
[Test]  Epoch: 28	Loss: 0.053281	Acc: 23.7% (2372/10000)
[Test]  Epoch: 29	Loss: 0.052740	Acc: 24.5% (2449/10000)
[Test]  Epoch: 30	Loss: 0.052546	Acc: 24.9% (2491/10000)
[Test]  Epoch: 31	Loss: 0.052786	Acc: 24.9% (2486/10000)
[Test]  Epoch: 32	Loss: 0.053289	Acc: 24.7% (2470/10000)
[Test]  Epoch: 33	Loss: 0.053194	Acc: 25.2% (2521/10000)
[Test]  Epoch: 34	Loss: 0.053370	Acc: 25.6% (2564/10000)
[Test]  Epoch: 35	Loss: 0.052773	Acc: 25.9% (2592/10000)
[Test]  Epoch: 36	Loss: 0.053516	Acc: 25.4% (2542/10000)
[Test]  Epoch: 37	Loss: 0.053156	Acc: 25.4% (2545/10000)
[Test]  Epoch: 38	Loss: 0.053003	Acc: 25.6% (2564/10000)
[Test]  Epoch: 39	Loss: 0.053510	Acc: 26.0% (2598/10000)
[Test]  Epoch: 40	Loss: 0.053453	Acc: 26.3% (2628/10000)
[Test]  Epoch: 41	Loss: 0.054587	Acc: 25.7% (2569/10000)
[Test]  Epoch: 42	Loss: 0.053577	Acc: 26.4% (2637/10000)
[Test]  Epoch: 43	Loss: 0.053477	Acc: 26.4% (2638/10000)
[Test]  Epoch: 44	Loss: 0.053430	Acc: 26.7% (2667/10000)
[Test]  Epoch: 45	Loss: 0.053697	Acc: 26.6% (2663/10000)
[Test]  Epoch: 46	Loss: 0.053709	Acc: 26.9% (2688/10000)
[Test]  Epoch: 47	Loss: 0.053885	Acc: 26.2% (2619/10000)
[Test]  Epoch: 48	Loss: 0.054073	Acc: 26.5% (2646/10000)
[Test]  Epoch: 49	Loss: 0.054326	Acc: 26.2% (2617/10000)
[Test]  Epoch: 50	Loss: 0.053750	Acc: 27.3% (2727/10000)
[Test]  Epoch: 51	Loss: 0.054246	Acc: 26.9% (2693/10000)
[Test]  Epoch: 52	Loss: 0.054451	Acc: 26.5% (2647/10000)
[Test]  Epoch: 53	Loss: 0.054825	Acc: 26.9% (2687/10000)
[Test]  Epoch: 54	Loss: 0.054074	Acc: 27.1% (2711/10000)
[Test]  Epoch: 55	Loss: 0.054096	Acc: 27.3% (2733/10000)
[Test]  Epoch: 56	Loss: 0.054349	Acc: 26.8% (2684/10000)
[Test]  Epoch: 57	Loss: 0.054408	Acc: 27.4% (2735/10000)
[Test]  Epoch: 58	Loss: 0.054458	Acc: 27.3% (2729/10000)
[Test]  Epoch: 59	Loss: 0.054387	Acc: 27.4% (2736/10000)
[Test]  Epoch: 60	Loss: 0.054307	Acc: 27.5% (2754/10000)
[Test]  Epoch: 61	Loss: 0.054403	Acc: 27.2% (2717/10000)
[Test]  Epoch: 62	Loss: 0.054296	Acc: 27.8% (2780/10000)
[Test]  Epoch: 63	Loss: 0.054434	Acc: 27.4% (2744/10000)
[Test]  Epoch: 64	Loss: 0.054462	Acc: 28.0% (2799/10000)
[Test]  Epoch: 65	Loss: 0.054552	Acc: 28.1% (2811/10000)
[Test]  Epoch: 66	Loss: 0.054373	Acc: 28.0% (2802/10000)
[Test]  Epoch: 67	Loss: 0.054071	Acc: 27.9% (2791/10000)
[Test]  Epoch: 68	Loss: 0.054180	Acc: 27.8% (2779/10000)
[Test]  Epoch: 69	Loss: 0.054376	Acc: 28.0% (2796/10000)
[Test]  Epoch: 70	Loss: 0.054244	Acc: 27.8% (2784/10000)
[Test]  Epoch: 71	Loss: 0.053919	Acc: 28.1% (2814/10000)
[Test]  Epoch: 72	Loss: 0.054333	Acc: 27.6% (2763/10000)
[Test]  Epoch: 73	Loss: 0.054362	Acc: 28.1% (2807/10000)
[Test]  Epoch: 74	Loss: 0.054213	Acc: 27.9% (2788/10000)
[Test]  Epoch: 75	Loss: 0.054288	Acc: 27.8% (2783/10000)
[Test]  Epoch: 76	Loss: 0.054428	Acc: 27.7% (2767/10000)
[Test]  Epoch: 77	Loss: 0.054427	Acc: 27.7% (2771/10000)
[Test]  Epoch: 78	Loss: 0.054375	Acc: 28.4% (2842/10000)
[Test]  Epoch: 79	Loss: 0.054216	Acc: 27.8% (2784/10000)
[Test]  Epoch: 80	Loss: 0.054272	Acc: 28.1% (2806/10000)
[Test]  Epoch: 81	Loss: 0.054397	Acc: 27.8% (2783/10000)
[Test]  Epoch: 82	Loss: 0.054104	Acc: 28.3% (2826/10000)
[Test]  Epoch: 83	Loss: 0.054567	Acc: 27.8% (2784/10000)
[Test]  Epoch: 84	Loss: 0.054360	Acc: 28.0% (2802/10000)
[Test]  Epoch: 85	Loss: 0.054277	Acc: 28.0% (2797/10000)
[Test]  Epoch: 86	Loss: 0.054122	Acc: 27.6% (2762/10000)
[Test]  Epoch: 87	Loss: 0.054290	Acc: 27.8% (2778/10000)
[Test]  Epoch: 88	Loss: 0.054228	Acc: 27.9% (2791/10000)
[Test]  Epoch: 89	Loss: 0.054238	Acc: 28.5% (2850/10000)
[Test]  Epoch: 90	Loss: 0.054243	Acc: 28.1% (2812/10000)
[Test]  Epoch: 91	Loss: 0.054611	Acc: 27.6% (2758/10000)
[Test]  Epoch: 92	Loss: 0.054333	Acc: 27.8% (2784/10000)
[Test]  Epoch: 93	Loss: 0.054159	Acc: 28.3% (2826/10000)
[Test]  Epoch: 94	Loss: 0.054256	Acc: 27.9% (2788/10000)
[Test]  Epoch: 95	Loss: 0.054414	Acc: 27.8% (2782/10000)
[Test]  Epoch: 96	Loss: 0.054516	Acc: 27.5% (2750/10000)
[Test]  Epoch: 97	Loss: 0.054324	Acc: 27.8% (2777/10000)
[Test]  Epoch: 98	Loss: 0.054285	Acc: 27.9% (2789/10000)
[Test]  Epoch: 99	Loss: 0.054311	Acc: 27.8% (2779/10000)
[Test]  Epoch: 100	Loss: 0.054157	Acc: 27.9% (2790/10000)
===========finish==========
['2024-08-19', '00:37:45.348973', '100', 'test', '0.05415698471069336', '27.9', '28.5']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight', 'features.15.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.11.weight', 'features.31.weight', 'features.21.weight', 'features.38.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight', 'features.7.weight', 'features.40.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.072410	Acc: 1.3% (132/10000)
[Test]  Epoch: 2	Loss: 0.071048	Acc: 4.0% (405/10000)
[Test]  Epoch: 3	Loss: 0.069836	Acc: 4.5% (454/10000)
[Test]  Epoch: 4	Loss: 0.068519	Acc: 5.5% (554/10000)
[Test]  Epoch: 5	Loss: 0.067517	Acc: 6.0% (600/10000)
[Test]  Epoch: 6	Loss: 0.066244	Acc: 7.5% (749/10000)
[Test]  Epoch: 7	Loss: 0.065340	Acc: 7.8% (782/10000)
[Test]  Epoch: 8	Loss: 0.064587	Acc: 8.2% (816/10000)
[Test]  Epoch: 9	Loss: 0.064067	Acc: 8.7% (873/10000)
[Test]  Epoch: 10	Loss: 0.063198	Acc: 9.2% (920/10000)
[Test]  Epoch: 11	Loss: 0.062654	Acc: 10.0% (1004/10000)
[Test]  Epoch: 12	Loss: 0.062261	Acc: 10.4% (1040/10000)
[Test]  Epoch: 13	Loss: 0.061782	Acc: 10.7% (1065/10000)
[Test]  Epoch: 14	Loss: 0.061598	Acc: 10.3% (1032/10000)
[Test]  Epoch: 15	Loss: 0.061599	Acc: 10.9% (1093/10000)
[Test]  Epoch: 16	Loss: 0.061294	Acc: 11.1% (1114/10000)
[Test]  Epoch: 17	Loss: 0.060973	Acc: 11.8% (1184/10000)
[Test]  Epoch: 18	Loss: 0.060513	Acc: 12.4% (1240/10000)
[Test]  Epoch: 19	Loss: 0.060757	Acc: 12.4% (1237/10000)
[Test]  Epoch: 20	Loss: 0.060575	Acc: 12.2% (1224/10000)
[Test]  Epoch: 21	Loss: 0.060447	Acc: 12.6% (1259/10000)
[Test]  Epoch: 22	Loss: 0.060824	Acc: 12.8% (1281/10000)
[Test]  Epoch: 23	Loss: 0.060564	Acc: 13.4% (1340/10000)
[Test]  Epoch: 24	Loss: 0.060557	Acc: 12.9% (1293/10000)
[Test]  Epoch: 25	Loss: 0.061710	Acc: 12.8% (1279/10000)
[Test]  Epoch: 26	Loss: 0.061024	Acc: 13.3% (1330/10000)
[Test]  Epoch: 27	Loss: 0.061276	Acc: 13.2% (1316/10000)
[Test]  Epoch: 28	Loss: 0.060754	Acc: 13.8% (1378/10000)
[Test]  Epoch: 29	Loss: 0.061329	Acc: 13.6% (1357/10000)
[Test]  Epoch: 30	Loss: 0.061730	Acc: 13.6% (1358/10000)
[Test]  Epoch: 31	Loss: 0.061238	Acc: 14.2% (1422/10000)
[Test]  Epoch: 32	Loss: 0.061553	Acc: 13.8% (1378/10000)
[Test]  Epoch: 33	Loss: 0.062295	Acc: 14.3% (1426/10000)
[Test]  Epoch: 34	Loss: 0.062520	Acc: 13.5% (1347/10000)
[Test]  Epoch: 35	Loss: 0.062369	Acc: 14.5% (1452/10000)
[Test]  Epoch: 36	Loss: 0.062688	Acc: 14.6% (1455/10000)
[Test]  Epoch: 37	Loss: 0.062679	Acc: 15.0% (1498/10000)
[Test]  Epoch: 38	Loss: 0.063740	Acc: 14.0% (1398/10000)
[Test]  Epoch: 39	Loss: 0.063306	Acc: 14.6% (1463/10000)
[Test]  Epoch: 40	Loss: 0.063886	Acc: 14.2% (1417/10000)
[Test]  Epoch: 41	Loss: 0.064012	Acc: 14.6% (1459/10000)
[Test]  Epoch: 42	Loss: 0.064489	Acc: 14.0% (1400/10000)
[Test]  Epoch: 43	Loss: 0.064814	Acc: 14.8% (1483/10000)
[Test]  Epoch: 44	Loss: 0.064404	Acc: 14.7% (1472/10000)
[Test]  Epoch: 45	Loss: 0.064694	Acc: 14.8% (1478/10000)
[Test]  Epoch: 46	Loss: 0.064524	Acc: 15.2% (1520/10000)
[Test]  Epoch: 47	Loss: 0.064825	Acc: 15.0% (1503/10000)
[Test]  Epoch: 48	Loss: 0.065230	Acc: 14.8% (1476/10000)
[Test]  Epoch: 49	Loss: 0.064776	Acc: 15.1% (1513/10000)
[Test]  Epoch: 50	Loss: 0.065949	Acc: 14.9% (1488/10000)
[Test]  Epoch: 51	Loss: 0.065975	Acc: 15.2% (1522/10000)
[Test]  Epoch: 52	Loss: 0.065854	Acc: 15.1% (1507/10000)
[Test]  Epoch: 53	Loss: 0.066363	Acc: 15.1% (1511/10000)
[Test]  Epoch: 54	Loss: 0.066450	Acc: 14.7% (1468/10000)
[Test]  Epoch: 55	Loss: 0.066540	Acc: 14.8% (1477/10000)
[Test]  Epoch: 56	Loss: 0.066778	Acc: 15.5% (1553/10000)
[Test]  Epoch: 57	Loss: 0.067366	Acc: 15.3% (1529/10000)
[Test]  Epoch: 58	Loss: 0.066940	Acc: 15.3% (1530/10000)
[Test]  Epoch: 59	Loss: 0.066800	Acc: 15.5% (1549/10000)
[Test]  Epoch: 60	Loss: 0.067585	Acc: 15.3% (1527/10000)
[Test]  Epoch: 61	Loss: 0.067246	Acc: 15.4% (1536/10000)
[Test]  Epoch: 62	Loss: 0.067080	Acc: 15.3% (1530/10000)
[Test]  Epoch: 63	Loss: 0.067131	Acc: 15.7% (1570/10000)
[Test]  Epoch: 64	Loss: 0.067213	Acc: 15.3% (1530/10000)
[Test]  Epoch: 65	Loss: 0.067074	Acc: 15.7% (1566/10000)
[Test]  Epoch: 66	Loss: 0.066883	Acc: 15.7% (1565/10000)
[Test]  Epoch: 67	Loss: 0.067074	Acc: 15.2% (1524/10000)
[Test]  Epoch: 68	Loss: 0.067133	Acc: 15.6% (1559/10000)
[Test]  Epoch: 69	Loss: 0.067155	Acc: 15.5% (1552/10000)
[Test]  Epoch: 70	Loss: 0.067118	Acc: 16.1% (1615/10000)
[Test]  Epoch: 71	Loss: 0.066924	Acc: 15.5% (1554/10000)
[Test]  Epoch: 72	Loss: 0.067165	Acc: 15.9% (1590/10000)
[Test]  Epoch: 73	Loss: 0.067429	Acc: 15.7% (1569/10000)
[Test]  Epoch: 74	Loss: 0.067065	Acc: 15.5% (1552/10000)
[Test]  Epoch: 75	Loss: 0.067056	Acc: 16.0% (1596/10000)
[Test]  Epoch: 76	Loss: 0.066947	Acc: 15.9% (1595/10000)
[Test]  Epoch: 77	Loss: 0.066910	Acc: 15.8% (1576/10000)
[Test]  Epoch: 78	Loss: 0.067418	Acc: 15.8% (1585/10000)
[Test]  Epoch: 79	Loss: 0.067069	Acc: 15.9% (1591/10000)
[Test]  Epoch: 80	Loss: 0.067347	Acc: 15.6% (1556/10000)
[Test]  Epoch: 81	Loss: 0.067131	Acc: 15.8% (1585/10000)
[Test]  Epoch: 82	Loss: 0.067145	Acc: 16.3% (1627/10000)
[Test]  Epoch: 83	Loss: 0.067270	Acc: 15.8% (1583/10000)
[Test]  Epoch: 84	Loss: 0.067214	Acc: 15.7% (1574/10000)
[Test]  Epoch: 85	Loss: 0.067133	Acc: 16.1% (1611/10000)
[Test]  Epoch: 86	Loss: 0.067103	Acc: 15.6% (1559/10000)
[Test]  Epoch: 87	Loss: 0.067264	Acc: 15.6% (1563/10000)
[Test]  Epoch: 88	Loss: 0.067388	Acc: 15.5% (1547/10000)
[Test]  Epoch: 89	Loss: 0.067407	Acc: 16.1% (1608/10000)
[Test]  Epoch: 90	Loss: 0.067107	Acc: 15.8% (1575/10000)
[Test]  Epoch: 91	Loss: 0.067286	Acc: 15.7% (1566/10000)
[Test]  Epoch: 92	Loss: 0.067332	Acc: 15.7% (1568/10000)
[Test]  Epoch: 93	Loss: 0.067411	Acc: 15.5% (1549/10000)
[Test]  Epoch: 94	Loss: 0.067196	Acc: 15.8% (1579/10000)
[Test]  Epoch: 95	Loss: 0.067413	Acc: 15.9% (1591/10000)
[Test]  Epoch: 96	Loss: 0.067306	Acc: 15.7% (1567/10000)
[Test]  Epoch: 97	Loss: 0.067450	Acc: 16.0% (1597/10000)
[Test]  Epoch: 98	Loss: 0.067177	Acc: 15.9% (1593/10000)
[Test]  Epoch: 99	Loss: 0.067491	Acc: 16.1% (1611/10000)
[Test]  Epoch: 100	Loss: 0.067430	Acc: 15.6% (1555/10000)
===========finish==========
['2024-08-19', '00:40:21.223452', '100', 'test', '0.06742977600097656', '15.55', '16.27']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight', 'features.15.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.11.weight', 'features.31.weight', 'features.21.weight', 'features.38.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight', 'features.7.weight', 'features.40.weight', 'features.10.weight', 'features.37.weight', 'features.34.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.073344	Acc: 1.4% (141/10000)
[Test]  Epoch: 2	Loss: 0.071394	Acc: 3.6% (358/10000)
[Test]  Epoch: 3	Loss: 0.070080	Acc: 4.0% (396/10000)
[Test]  Epoch: 4	Loss: 0.068828	Acc: 4.5% (446/10000)
[Test]  Epoch: 5	Loss: 0.067572	Acc: 6.0% (596/10000)
[Test]  Epoch: 6	Loss: 0.066729	Acc: 7.4% (743/10000)
[Test]  Epoch: 7	Loss: 0.065314	Acc: 7.9% (792/10000)
[Test]  Epoch: 8	Loss: 0.064688	Acc: 8.2% (825/10000)
[Test]  Epoch: 9	Loss: 0.063813	Acc: 8.9% (889/10000)
[Test]  Epoch: 10	Loss: 0.063134	Acc: 9.1% (910/10000)
[Test]  Epoch: 11	Loss: 0.062842	Acc: 10.1% (1007/10000)
[Test]  Epoch: 12	Loss: 0.062380	Acc: 10.2% (1019/10000)
[Test]  Epoch: 13	Loss: 0.062055	Acc: 10.8% (1078/10000)
[Test]  Epoch: 14	Loss: 0.062152	Acc: 10.4% (1044/10000)
[Test]  Epoch: 15	Loss: 0.061351	Acc: 10.8% (1080/10000)
[Test]  Epoch: 16	Loss: 0.061467	Acc: 10.9% (1095/10000)
[Test]  Epoch: 17	Loss: 0.061146	Acc: 11.7% (1169/10000)
[Test]  Epoch: 18	Loss: 0.061351	Acc: 11.7% (1165/10000)
[Test]  Epoch: 19	Loss: 0.060744	Acc: 12.4% (1241/10000)
[Test]  Epoch: 20	Loss: 0.061455	Acc: 11.8% (1180/10000)
[Test]  Epoch: 21	Loss: 0.060585	Acc: 12.7% (1268/10000)
[Test]  Epoch: 22	Loss: 0.061127	Acc: 12.8% (1282/10000)
[Test]  Epoch: 23	Loss: 0.060677	Acc: 13.2% (1322/10000)
[Test]  Epoch: 24	Loss: 0.061169	Acc: 12.3% (1232/10000)
[Test]  Epoch: 25	Loss: 0.061127	Acc: 13.0% (1301/10000)
[Test]  Epoch: 26	Loss: 0.061623	Acc: 12.8% (1275/10000)
[Test]  Epoch: 27	Loss: 0.060741	Acc: 13.9% (1395/10000)
[Test]  Epoch: 28	Loss: 0.061170	Acc: 13.2% (1319/10000)
[Test]  Epoch: 29	Loss: 0.061903	Acc: 13.4% (1345/10000)
[Test]  Epoch: 30	Loss: 0.061004	Acc: 13.9% (1393/10000)
[Test]  Epoch: 31	Loss: 0.061673	Acc: 13.8% (1378/10000)
[Test]  Epoch: 32	Loss: 0.061761	Acc: 14.3% (1435/10000)
[Test]  Epoch: 33	Loss: 0.062040	Acc: 14.1% (1407/10000)
[Test]  Epoch: 34	Loss: 0.062506	Acc: 13.6% (1360/10000)
[Test]  Epoch: 35	Loss: 0.062786	Acc: 14.0% (1397/10000)
[Test]  Epoch: 36	Loss: 0.062561	Acc: 13.8% (1381/10000)
[Test]  Epoch: 37	Loss: 0.063275	Acc: 14.0% (1399/10000)
[Test]  Epoch: 38	Loss: 0.063161	Acc: 13.9% (1395/10000)
[Test]  Epoch: 39	Loss: 0.063839	Acc: 14.0% (1398/10000)
[Test]  Epoch: 40	Loss: 0.063849	Acc: 14.1% (1412/10000)
[Test]  Epoch: 41	Loss: 0.064178	Acc: 13.7% (1371/10000)
[Test]  Epoch: 42	Loss: 0.064379	Acc: 13.8% (1381/10000)
[Test]  Epoch: 43	Loss: 0.064656	Acc: 14.1% (1408/10000)
[Test]  Epoch: 44	Loss: 0.064440	Acc: 14.1% (1407/10000)
[Test]  Epoch: 45	Loss: 0.064432	Acc: 14.3% (1431/10000)
[Test]  Epoch: 46	Loss: 0.064478	Acc: 14.3% (1434/10000)
[Test]  Epoch: 47	Loss: 0.065858	Acc: 14.3% (1435/10000)
[Test]  Epoch: 48	Loss: 0.065200	Acc: 14.9% (1491/10000)
[Test]  Epoch: 49	Loss: 0.065591	Acc: 14.2% (1419/10000)
[Test]  Epoch: 50	Loss: 0.065384	Acc: 14.2% (1423/10000)
[Test]  Epoch: 51	Loss: 0.065945	Acc: 14.4% (1441/10000)
[Test]  Epoch: 52	Loss: 0.066204	Acc: 14.1% (1411/10000)
[Test]  Epoch: 53	Loss: 0.066333	Acc: 14.2% (1418/10000)
[Test]  Epoch: 54	Loss: 0.066402	Acc: 14.4% (1439/10000)
[Test]  Epoch: 55	Loss: 0.066345	Acc: 14.9% (1492/10000)
[Test]  Epoch: 56	Loss: 0.066895	Acc: 14.9% (1486/10000)
[Test]  Epoch: 57	Loss: 0.066750	Acc: 14.9% (1488/10000)
[Test]  Epoch: 58	Loss: 0.066838	Acc: 15.1% (1511/10000)
[Test]  Epoch: 59	Loss: 0.066630	Acc: 14.3% (1428/10000)
[Test]  Epoch: 60	Loss: 0.066790	Acc: 14.7% (1465/10000)
[Test]  Epoch: 61	Loss: 0.066938	Acc: 14.6% (1464/10000)
[Test]  Epoch: 62	Loss: 0.066946	Acc: 14.9% (1490/10000)
[Test]  Epoch: 63	Loss: 0.066794	Acc: 15.4% (1538/10000)
[Test]  Epoch: 64	Loss: 0.066652	Acc: 15.5% (1546/10000)
[Test]  Epoch: 65	Loss: 0.066897	Acc: 15.1% (1510/10000)
[Test]  Epoch: 66	Loss: 0.066741	Acc: 15.2% (1521/10000)
[Test]  Epoch: 67	Loss: 0.066908	Acc: 15.0% (1503/10000)
[Test]  Epoch: 68	Loss: 0.066977	Acc: 15.2% (1524/10000)
[Test]  Epoch: 69	Loss: 0.067077	Acc: 15.0% (1504/10000)
[Test]  Epoch: 70	Loss: 0.067022	Acc: 15.1% (1505/10000)
[Test]  Epoch: 71	Loss: 0.067002	Acc: 15.3% (1530/10000)
[Test]  Epoch: 72	Loss: 0.066884	Acc: 15.1% (1508/10000)
[Test]  Epoch: 73	Loss: 0.067143	Acc: 15.1% (1514/10000)
[Test]  Epoch: 74	Loss: 0.067045	Acc: 15.1% (1510/10000)
[Test]  Epoch: 75	Loss: 0.066815	Acc: 15.1% (1506/10000)
[Test]  Epoch: 76	Loss: 0.067155	Acc: 14.9% (1486/10000)
[Test]  Epoch: 77	Loss: 0.067242	Acc: 15.2% (1517/10000)
[Test]  Epoch: 78	Loss: 0.067087	Acc: 15.3% (1531/10000)
[Test]  Epoch: 79	Loss: 0.066914	Acc: 15.6% (1558/10000)
[Test]  Epoch: 80	Loss: 0.067104	Acc: 15.4% (1543/10000)
[Test]  Epoch: 81	Loss: 0.067201	Acc: 15.2% (1525/10000)
[Test]  Epoch: 82	Loss: 0.067111	Acc: 15.1% (1512/10000)
[Test]  Epoch: 83	Loss: 0.067043	Acc: 15.3% (1530/10000)
[Test]  Epoch: 84	Loss: 0.067101	Acc: 15.3% (1530/10000)
[Test]  Epoch: 85	Loss: 0.067157	Acc: 15.0% (1498/10000)
[Test]  Epoch: 86	Loss: 0.067242	Acc: 14.8% (1481/10000)
[Test]  Epoch: 87	Loss: 0.066962	Acc: 15.2% (1518/10000)
[Test]  Epoch: 88	Loss: 0.067201	Acc: 15.0% (1504/10000)
[Test]  Epoch: 89	Loss: 0.067152	Acc: 15.4% (1543/10000)
[Test]  Epoch: 90	Loss: 0.067137	Acc: 15.2% (1518/10000)
[Test]  Epoch: 91	Loss: 0.067225	Acc: 15.2% (1524/10000)
[Test]  Epoch: 92	Loss: 0.067152	Acc: 15.1% (1509/10000)
[Test]  Epoch: 93	Loss: 0.067286	Acc: 15.2% (1525/10000)
[Test]  Epoch: 94	Loss: 0.067025	Acc: 15.1% (1509/10000)
[Test]  Epoch: 95	Loss: 0.067140	Acc: 15.1% (1512/10000)
[Test]  Epoch: 96	Loss: 0.067247	Acc: 15.5% (1548/10000)
[Test]  Epoch: 97	Loss: 0.067108	Acc: 15.3% (1527/10000)
[Test]  Epoch: 98	Loss: 0.067222	Acc: 15.3% (1533/10000)
[Test]  Epoch: 99	Loss: 0.067231	Acc: 15.2% (1521/10000)
[Test]  Epoch: 100	Loss: 0.067202	Acc: 15.1% (1506/10000)
===========finish==========
['2024-08-19', '00:42:55.133412', '100', 'test', '0.06720152838230133', '15.06', '15.58']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight', 'features.15.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.11.weight', 'features.31.weight', 'features.21.weight', 'features.38.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight', 'features.7.weight', 'features.40.weight', 'features.10.weight', 'features.37.weight', 'features.34.weight', 'features.14.weight', 'features.17.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.074872	Acc: 1.3% (133/10000)
[Test]  Epoch: 2	Loss: 0.071100	Acc: 3.8% (380/10000)
[Test]  Epoch: 3	Loss: 0.070204	Acc: 4.1% (408/10000)
[Test]  Epoch: 4	Loss: 0.069303	Acc: 4.2% (417/10000)
[Test]  Epoch: 5	Loss: 0.068242	Acc: 4.9% (490/10000)
[Test]  Epoch: 6	Loss: 0.067374	Acc: 6.0% (602/10000)
[Test]  Epoch: 7	Loss: 0.066626	Acc: 6.0% (605/10000)
[Test]  Epoch: 8	Loss: 0.065636	Acc: 7.2% (719/10000)
[Test]  Epoch: 9	Loss: 0.065441	Acc: 7.3% (733/10000)
[Test]  Epoch: 10	Loss: 0.064704	Acc: 7.6% (757/10000)
[Test]  Epoch: 11	Loss: 0.064194	Acc: 8.1% (813/10000)
[Test]  Epoch: 12	Loss: 0.063492	Acc: 9.3% (929/10000)
[Test]  Epoch: 13	Loss: 0.063636	Acc: 8.4% (838/10000)
[Test]  Epoch: 14	Loss: 0.063911	Acc: 8.8% (885/10000)
[Test]  Epoch: 15	Loss: 0.063176	Acc: 9.4% (945/10000)
[Test]  Epoch: 16	Loss: 0.063264	Acc: 9.5% (947/10000)
[Test]  Epoch: 17	Loss: 0.062850	Acc: 10.1% (1013/10000)
[Test]  Epoch: 18	Loss: 0.063109	Acc: 9.9% (993/10000)
[Test]  Epoch: 19	Loss: 0.062254	Acc: 10.9% (1092/10000)
[Test]  Epoch: 20	Loss: 0.062617	Acc: 10.1% (1012/10000)
[Test]  Epoch: 21	Loss: 0.062885	Acc: 10.2% (1018/10000)
[Test]  Epoch: 22	Loss: 0.062552	Acc: 11.2% (1121/10000)
[Test]  Epoch: 23	Loss: 0.062806	Acc: 11.7% (1173/10000)
[Test]  Epoch: 24	Loss: 0.063243	Acc: 10.1% (1009/10000)
[Test]  Epoch: 25	Loss: 0.063041	Acc: 11.0% (1102/10000)
[Test]  Epoch: 26	Loss: 0.062784	Acc: 11.3% (1128/10000)
[Test]  Epoch: 27	Loss: 0.062816	Acc: 11.7% (1174/10000)
[Test]  Epoch: 28	Loss: 0.063152	Acc: 11.5% (1154/10000)
[Test]  Epoch: 29	Loss: 0.064870	Acc: 10.9% (1091/10000)
[Test]  Epoch: 30	Loss: 0.063981	Acc: 11.4% (1136/10000)
[Test]  Epoch: 31	Loss: 0.063812	Acc: 11.6% (1160/10000)
[Test]  Epoch: 32	Loss: 0.064461	Acc: 11.6% (1159/10000)
[Test]  Epoch: 33	Loss: 0.064590	Acc: 12.1% (1213/10000)
[Test]  Epoch: 34	Loss: 0.064765	Acc: 11.8% (1175/10000)
[Test]  Epoch: 35	Loss: 0.065854	Acc: 12.3% (1229/10000)
[Test]  Epoch: 36	Loss: 0.065654	Acc: 11.8% (1185/10000)
[Test]  Epoch: 37	Loss: 0.066478	Acc: 11.8% (1183/10000)
[Test]  Epoch: 38	Loss: 0.065612	Acc: 12.1% (1210/10000)
[Test]  Epoch: 39	Loss: 0.066585	Acc: 12.1% (1210/10000)
[Test]  Epoch: 40	Loss: 0.066166	Acc: 11.8% (1184/10000)
[Test]  Epoch: 41	Loss: 0.066447	Acc: 12.1% (1208/10000)
[Test]  Epoch: 42	Loss: 0.067410	Acc: 12.4% (1242/10000)
[Test]  Epoch: 43	Loss: 0.067273	Acc: 12.9% (1288/10000)
[Test]  Epoch: 44	Loss: 0.067751	Acc: 12.8% (1281/10000)
[Test]  Epoch: 45	Loss: 0.067234	Acc: 12.5% (1246/10000)
[Test]  Epoch: 46	Loss: 0.067794	Acc: 12.9% (1288/10000)
[Test]  Epoch: 47	Loss: 0.067537	Acc: 13.1% (1310/10000)
[Test]  Epoch: 48	Loss: 0.068613	Acc: 12.9% (1292/10000)
[Test]  Epoch: 49	Loss: 0.068247	Acc: 12.6% (1256/10000)
[Test]  Epoch: 50	Loss: 0.068612	Acc: 13.0% (1300/10000)
[Test]  Epoch: 51	Loss: 0.068723	Acc: 12.8% (1284/10000)
[Test]  Epoch: 52	Loss: 0.069368	Acc: 13.0% (1300/10000)
[Test]  Epoch: 53	Loss: 0.069741	Acc: 13.1% (1307/10000)
[Test]  Epoch: 54	Loss: 0.069103	Acc: 12.6% (1264/10000)
[Test]  Epoch: 55	Loss: 0.069554	Acc: 13.2% (1317/10000)
[Test]  Epoch: 56	Loss: 0.070505	Acc: 13.0% (1302/10000)
[Test]  Epoch: 57	Loss: 0.070081	Acc: 13.4% (1344/10000)
[Test]  Epoch: 58	Loss: 0.069965	Acc: 13.0% (1298/10000)
[Test]  Epoch: 59	Loss: 0.070630	Acc: 13.5% (1349/10000)
[Test]  Epoch: 60	Loss: 0.071593	Acc: 12.8% (1281/10000)
[Test]  Epoch: 61	Loss: 0.070447	Acc: 13.8% (1378/10000)
[Test]  Epoch: 62	Loss: 0.070537	Acc: 13.6% (1358/10000)
[Test]  Epoch: 63	Loss: 0.070302	Acc: 13.6% (1361/10000)
[Test]  Epoch: 64	Loss: 0.070571	Acc: 13.8% (1381/10000)
[Test]  Epoch: 65	Loss: 0.070473	Acc: 14.1% (1406/10000)
[Test]  Epoch: 66	Loss: 0.070357	Acc: 14.0% (1396/10000)
[Test]  Epoch: 67	Loss: 0.070148	Acc: 13.7% (1369/10000)
[Test]  Epoch: 68	Loss: 0.070600	Acc: 13.9% (1390/10000)
[Test]  Epoch: 69	Loss: 0.070353	Acc: 13.7% (1372/10000)
[Test]  Epoch: 70	Loss: 0.070999	Acc: 13.4% (1341/10000)
[Test]  Epoch: 71	Loss: 0.070384	Acc: 14.0% (1401/10000)
[Test]  Epoch: 72	Loss: 0.070426	Acc: 13.8% (1383/10000)
[Test]  Epoch: 73	Loss: 0.070484	Acc: 14.0% (1399/10000)
[Test]  Epoch: 74	Loss: 0.070593	Acc: 13.8% (1384/10000)
[Test]  Epoch: 75	Loss: 0.070535	Acc: 14.2% (1417/10000)
[Test]  Epoch: 76	Loss: 0.070669	Acc: 13.6% (1358/10000)
[Test]  Epoch: 77	Loss: 0.070696	Acc: 14.1% (1405/10000)
[Test]  Epoch: 78	Loss: 0.070621	Acc: 13.9% (1389/10000)
[Test]  Epoch: 79	Loss: 0.070867	Acc: 13.7% (1368/10000)
[Test]  Epoch: 80	Loss: 0.070680	Acc: 13.4% (1343/10000)
[Test]  Epoch: 81	Loss: 0.070666	Acc: 14.3% (1427/10000)
[Test]  Epoch: 82	Loss: 0.070784	Acc: 13.6% (1364/10000)
[Test]  Epoch: 83	Loss: 0.070872	Acc: 14.0% (1402/10000)
[Test]  Epoch: 84	Loss: 0.070885	Acc: 13.9% (1391/10000)
[Test]  Epoch: 85	Loss: 0.070779	Acc: 13.7% (1374/10000)
[Test]  Epoch: 86	Loss: 0.070749	Acc: 14.0% (1400/10000)
[Test]  Epoch: 87	Loss: 0.070895	Acc: 14.1% (1410/10000)
[Test]  Epoch: 88	Loss: 0.070865	Acc: 13.9% (1394/10000)
[Test]  Epoch: 89	Loss: 0.070743	Acc: 14.2% (1419/10000)
[Test]  Epoch: 90	Loss: 0.070969	Acc: 13.8% (1378/10000)
[Test]  Epoch: 91	Loss: 0.071097	Acc: 13.9% (1388/10000)
[Test]  Epoch: 92	Loss: 0.070978	Acc: 13.9% (1395/10000)
[Test]  Epoch: 93	Loss: 0.071060	Acc: 13.9% (1393/10000)
[Test]  Epoch: 94	Loss: 0.070828	Acc: 14.2% (1415/10000)
[Test]  Epoch: 95	Loss: 0.071175	Acc: 13.5% (1351/10000)
[Test]  Epoch: 96	Loss: 0.071127	Acc: 14.0% (1396/10000)
[Test]  Epoch: 97	Loss: 0.070743	Acc: 14.0% (1397/10000)
[Test]  Epoch: 98	Loss: 0.070598	Acc: 14.5% (1450/10000)
[Test]  Epoch: 99	Loss: 0.071007	Acc: 14.0% (1398/10000)
[Test]  Epoch: 100	Loss: 0.071043	Acc: 13.7% (1370/10000)
===========finish==========
['2024-08-19', '00:45:23.732711', '100', 'test', '0.07104330508708954', '13.7', '14.5']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.41.weight', 'features.1.weight', 'features.8.weight', 'features.4.weight', 'features.18.weight', 'features.15.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.11.weight', 'features.31.weight', 'features.21.weight', 'features.38.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight', 'features.7.weight', 'features.40.weight', 'features.10.weight', 'features.37.weight', 'features.34.weight', 'features.14.weight', 'features.17.weight', 'features.20.weight', 'features.30.weight', 'features.27.weight', 'features.24.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.133015	Acc: 1.0% (100/10000)
[Test]  Epoch: 2	Loss: 0.071846	Acc: 2.4% (243/10000)
[Test]  Epoch: 3	Loss: 0.071091	Acc: 3.4% (339/10000)
[Test]  Epoch: 4	Loss: 0.071170	Acc: 3.4% (338/10000)
[Test]  Epoch: 5	Loss: 0.070321	Acc: 3.7% (366/10000)
[Test]  Epoch: 6	Loss: 0.069685	Acc: 4.0% (405/10000)
[Test]  Epoch: 7	Loss: 0.069080	Acc: 3.7% (370/10000)
[Test]  Epoch: 8	Loss: 0.068597	Acc: 5.2% (525/10000)
[Test]  Epoch: 9	Loss: 0.067995	Acc: 5.6% (556/10000)
[Test]  Epoch: 10	Loss: 0.067777	Acc: 5.4% (537/10000)
[Test]  Epoch: 11	Loss: 0.067001	Acc: 6.3% (629/10000)
[Test]  Epoch: 12	Loss: 0.066307	Acc: 6.8% (678/10000)
[Test]  Epoch: 13	Loss: 0.065730	Acc: 6.5% (654/10000)
[Test]  Epoch: 14	Loss: 0.065807	Acc: 7.2% (719/10000)
[Test]  Epoch: 15	Loss: 0.065296	Acc: 7.6% (758/10000)
[Test]  Epoch: 16	Loss: 0.064313	Acc: 8.0% (797/10000)
[Test]  Epoch: 17	Loss: 0.064162	Acc: 8.7% (870/10000)
[Test]  Epoch: 18	Loss: 0.064312	Acc: 8.3% (828/10000)
[Test]  Epoch: 19	Loss: 0.063642	Acc: 8.7% (871/10000)
[Test]  Epoch: 20	Loss: 0.063589	Acc: 8.5% (848/10000)
[Test]  Epoch: 21	Loss: 0.063253	Acc: 9.1% (911/10000)
[Test]  Epoch: 22	Loss: 0.062912	Acc: 9.5% (953/10000)
[Test]  Epoch: 23	Loss: 0.063791	Acc: 10.3% (1033/10000)
[Test]  Epoch: 24	Loss: 0.063213	Acc: 9.7% (967/10000)
[Test]  Epoch: 25	Loss: 0.063623	Acc: 9.4% (943/10000)
[Test]  Epoch: 26	Loss: 0.063021	Acc: 10.3% (1034/10000)
[Test]  Epoch: 27	Loss: 0.063827	Acc: 10.1% (1014/10000)
[Test]  Epoch: 28	Loss: 0.062959	Acc: 9.7% (974/10000)
[Test]  Epoch: 29	Loss: 0.063599	Acc: 10.2% (1015/10000)
[Test]  Epoch: 30	Loss: 0.062941	Acc: 10.6% (1061/10000)
[Test]  Epoch: 31	Loss: 0.063705	Acc: 10.6% (1059/10000)
[Test]  Epoch: 32	Loss: 0.062836	Acc: 10.8% (1077/10000)
[Test]  Epoch: 33	Loss: 0.063354	Acc: 11.1% (1105/10000)
[Test]  Epoch: 34	Loss: 0.064007	Acc: 11.1% (1106/10000)
[Test]  Epoch: 35	Loss: 0.064057	Acc: 11.6% (1155/10000)
[Test]  Epoch: 36	Loss: 0.063899	Acc: 11.2% (1116/10000)
[Test]  Epoch: 37	Loss: 0.064459	Acc: 11.5% (1148/10000)
[Test]  Epoch: 38	Loss: 0.063586	Acc: 11.4% (1141/10000)
[Test]  Epoch: 39	Loss: 0.067280	Acc: 11.6% (1162/10000)
[Test]  Epoch: 40	Loss: 0.063664	Acc: 11.4% (1144/10000)
[Test]  Epoch: 41	Loss: 0.064947	Acc: 11.4% (1145/10000)
[Test]  Epoch: 42	Loss: 0.066889	Acc: 10.5% (1050/10000)
[Test]  Epoch: 43	Loss: 0.066137	Acc: 10.9% (1091/10000)
[Test]  Epoch: 44	Loss: 0.065951	Acc: 11.5% (1147/10000)
[Test]  Epoch: 45	Loss: 0.067797	Acc: 11.2% (1118/10000)
[Test]  Epoch: 46	Loss: 0.065692	Acc: 12.2% (1224/10000)
[Test]  Epoch: 47	Loss: 0.066920	Acc: 11.6% (1162/10000)
[Test]  Epoch: 48	Loss: 0.067810	Acc: 11.7% (1165/10000)
[Test]  Epoch: 49	Loss: 0.067695	Acc: 11.4% (1144/10000)
[Test]  Epoch: 50	Loss: 0.068292	Acc: 11.7% (1165/10000)
[Test]  Epoch: 51	Loss: 0.066416	Acc: 12.1% (1206/10000)
[Test]  Epoch: 52	Loss: 0.069773	Acc: 12.0% (1200/10000)
[Test]  Epoch: 53	Loss: 0.069860	Acc: 11.8% (1176/10000)
[Test]  Epoch: 54	Loss: 0.070970	Acc: 11.8% (1175/10000)
[Test]  Epoch: 55	Loss: 0.068357	Acc: 12.9% (1286/10000)
[Test]  Epoch: 56	Loss: 0.070961	Acc: 11.8% (1183/10000)
[Test]  Epoch: 57	Loss: 0.068472	Acc: 12.3% (1227/10000)
[Test]  Epoch: 58	Loss: 0.069710	Acc: 12.5% (1246/10000)
[Test]  Epoch: 59	Loss: 0.070706	Acc: 12.5% (1251/10000)
[Test]  Epoch: 60	Loss: 0.070724	Acc: 12.8% (1281/10000)
[Test]  Epoch: 61	Loss: 0.069444	Acc: 12.9% (1292/10000)
[Test]  Epoch: 62	Loss: 0.069358	Acc: 12.6% (1264/10000)
[Test]  Epoch: 63	Loss: 0.069413	Acc: 12.9% (1295/10000)
[Test]  Epoch: 64	Loss: 0.069402	Acc: 12.9% (1289/10000)
[Test]  Epoch: 65	Loss: 0.069470	Acc: 13.4% (1336/10000)
[Test]  Epoch: 66	Loss: 0.069355	Acc: 13.1% (1306/10000)
[Test]  Epoch: 67	Loss: 0.069301	Acc: 13.3% (1328/10000)
[Test]  Epoch: 68	Loss: 0.069575	Acc: 13.0% (1298/10000)
[Test]  Epoch: 69	Loss: 0.069588	Acc: 12.8% (1277/10000)
[Test]  Epoch: 70	Loss: 0.069594	Acc: 13.1% (1309/10000)
[Test]  Epoch: 71	Loss: 0.069398	Acc: 13.1% (1312/10000)
[Test]  Epoch: 72	Loss: 0.069636	Acc: 13.1% (1305/10000)
[Test]  Epoch: 73	Loss: 0.069558	Acc: 13.3% (1330/10000)
[Test]  Epoch: 74	Loss: 0.069713	Acc: 13.1% (1311/10000)
[Test]  Epoch: 75	Loss: 0.070164	Acc: 13.2% (1321/10000)
[Test]  Epoch: 76	Loss: 0.069707	Acc: 13.3% (1331/10000)
[Test]  Epoch: 77	Loss: 0.069669	Acc: 13.4% (1345/10000)
[Test]  Epoch: 78	Loss: 0.069762	Acc: 13.3% (1328/10000)
[Test]  Epoch: 79	Loss: 0.069523	Acc: 13.3% (1334/10000)
[Test]  Epoch: 80	Loss: 0.069524	Acc: 13.0% (1301/10000)
[Test]  Epoch: 81	Loss: 0.069665	Acc: 13.0% (1298/10000)
[Test]  Epoch: 82	Loss: 0.069703	Acc: 13.0% (1296/10000)
[Test]  Epoch: 83	Loss: 0.069680	Acc: 13.4% (1338/10000)
[Test]  Epoch: 84	Loss: 0.069813	Acc: 13.1% (1307/10000)
[Test]  Epoch: 85	Loss: 0.070054	Acc: 13.4% (1336/10000)
[Test]  Epoch: 86	Loss: 0.069817	Acc: 13.3% (1329/10000)
[Test]  Epoch: 87	Loss: 0.069861	Acc: 13.1% (1314/10000)
[Test]  Epoch: 88	Loss: 0.069796	Acc: 13.6% (1356/10000)
[Test]  Epoch: 89	Loss: 0.069896	Acc: 13.5% (1348/10000)
[Test]  Epoch: 90	Loss: 0.070106	Acc: 13.1% (1305/10000)
[Test]  Epoch: 91	Loss: 0.070330	Acc: 13.2% (1321/10000)
[Test]  Epoch: 92	Loss: 0.070059	Acc: 13.3% (1327/10000)
[Test]  Epoch: 93	Loss: 0.069894	Acc: 13.4% (1339/10000)
[Test]  Epoch: 94	Loss: 0.069842	Acc: 13.4% (1342/10000)
[Test]  Epoch: 95	Loss: 0.070155	Acc: 13.5% (1350/10000)
[Test]  Epoch: 96	Loss: 0.070294	Acc: 13.4% (1338/10000)
[Test]  Epoch: 97	Loss: 0.070168	Acc: 13.4% (1336/10000)
[Test]  Epoch: 98	Loss: 0.069885	Acc: 13.4% (1341/10000)
[Test]  Epoch: 99	Loss: 0.070157	Acc: 13.6% (1360/10000)
[Test]  Epoch: 100	Loss: 0.070098	Acc: 13.3% (1327/10000)
===========finish==========
['2024-08-19', '00:48:11.716362', '100', 'test', '0.07009755907058716', '13.27', '13.6']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.029813	Acc: 48.3% (4833/10000)
[Test]  Epoch: 2	Loss: 0.029682	Acc: 48.7% (4872/10000)
[Test]  Epoch: 3	Loss: 0.029686	Acc: 48.8% (4875/10000)
[Test]  Epoch: 4	Loss: 0.029686	Acc: 48.4% (4839/10000)
[Test]  Epoch: 5	Loss: 0.029712	Acc: 48.6% (4863/10000)
[Test]  Epoch: 6	Loss: 0.029707	Acc: 48.6% (4860/10000)
[Test]  Epoch: 7	Loss: 0.029592	Acc: 48.9% (4887/10000)
[Test]  Epoch: 8	Loss: 0.029632	Acc: 48.8% (4880/10000)
[Test]  Epoch: 9	Loss: 0.029525	Acc: 48.6% (4864/10000)
[Test]  Epoch: 10	Loss: 0.029673	Acc: 48.7% (4866/10000)
[Test]  Epoch: 11	Loss: 0.029584	Acc: 48.9% (4885/10000)
[Test]  Epoch: 12	Loss: 0.029523	Acc: 49.1% (4912/10000)
[Test]  Epoch: 13	Loss: 0.029547	Acc: 49.0% (4895/10000)
[Test]  Epoch: 14	Loss: 0.029508	Acc: 49.2% (4916/10000)
[Test]  Epoch: 15	Loss: 0.029630	Acc: 48.7% (4866/10000)
[Test]  Epoch: 16	Loss: 0.029456	Acc: 49.1% (4908/10000)
[Test]  Epoch: 17	Loss: 0.029558	Acc: 48.8% (4880/10000)
[Test]  Epoch: 18	Loss: 0.029626	Acc: 48.9% (4886/10000)
[Test]  Epoch: 19	Loss: 0.029558	Acc: 49.0% (4897/10000)
[Test]  Epoch: 20	Loss: 0.029536	Acc: 48.9% (4894/10000)
[Test]  Epoch: 21	Loss: 0.029501	Acc: 48.8% (4878/10000)
[Test]  Epoch: 22	Loss: 0.029463	Acc: 49.2% (4919/10000)
[Test]  Epoch: 23	Loss: 0.029546	Acc: 48.8% (4881/10000)
[Test]  Epoch: 24	Loss: 0.029560	Acc: 49.1% (4907/10000)
[Test]  Epoch: 25	Loss: 0.029525	Acc: 49.2% (4921/10000)
[Test]  Epoch: 26	Loss: 0.029489	Acc: 49.2% (4917/10000)
[Test]  Epoch: 27	Loss: 0.029580	Acc: 48.9% (4885/10000)
[Test]  Epoch: 28	Loss: 0.029682	Acc: 48.8% (4880/10000)
[Test]  Epoch: 29	Loss: 0.029595	Acc: 49.0% (4904/10000)
[Test]  Epoch: 30	Loss: 0.029521	Acc: 49.1% (4907/10000)
[Test]  Epoch: 31	Loss: 0.029493	Acc: 49.2% (4924/10000)
[Test]  Epoch: 32	Loss: 0.029614	Acc: 48.8% (4876/10000)
[Test]  Epoch: 33	Loss: 0.029529	Acc: 49.1% (4910/10000)
[Test]  Epoch: 34	Loss: 0.029559	Acc: 48.9% (4887/10000)
[Test]  Epoch: 35	Loss: 0.029599	Acc: 48.8% (4881/10000)
[Test]  Epoch: 36	Loss: 0.029687	Acc: 48.8% (4881/10000)
[Test]  Epoch: 37	Loss: 0.029552	Acc: 49.1% (4908/10000)
[Test]  Epoch: 38	Loss: 0.029467	Acc: 49.1% (4913/10000)
[Test]  Epoch: 39	Loss: 0.029570	Acc: 49.1% (4913/10000)
[Test]  Epoch: 40	Loss: 0.029546	Acc: 48.8% (4884/10000)
[Test]  Epoch: 41	Loss: 0.029506	Acc: 49.1% (4910/10000)
[Test]  Epoch: 42	Loss: 0.029688	Acc: 48.7% (4870/10000)
[Test]  Epoch: 43	Loss: 0.029649	Acc: 49.1% (4909/10000)
[Test]  Epoch: 44	Loss: 0.029554	Acc: 49.1% (4910/10000)
[Test]  Epoch: 45	Loss: 0.029700	Acc: 48.9% (4889/10000)
[Test]  Epoch: 46	Loss: 0.029546	Acc: 48.9% (4889/10000)
[Test]  Epoch: 47	Loss: 0.029565	Acc: 49.1% (4907/10000)
[Test]  Epoch: 48	Loss: 0.029588	Acc: 48.9% (4893/10000)
[Test]  Epoch: 49	Loss: 0.029596	Acc: 48.9% (4888/10000)
[Test]  Epoch: 50	Loss: 0.029589	Acc: 49.0% (4897/10000)
[Test]  Epoch: 51	Loss: 0.029622	Acc: 49.0% (4898/10000)
[Test]  Epoch: 52	Loss: 0.029534	Acc: 49.1% (4908/10000)
[Test]  Epoch: 53	Loss: 0.029631	Acc: 48.9% (4892/10000)
[Test]  Epoch: 54	Loss: 0.029669	Acc: 49.0% (4905/10000)
[Test]  Epoch: 55	Loss: 0.029583	Acc: 49.1% (4914/10000)
[Test]  Epoch: 56	Loss: 0.029642	Acc: 48.8% (4881/10000)
[Test]  Epoch: 57	Loss: 0.029654	Acc: 48.7% (4866/10000)
[Test]  Epoch: 58	Loss: 0.029502	Acc: 48.9% (4894/10000)
[Test]  Epoch: 59	Loss: 0.029617	Acc: 48.7% (4873/10000)
[Test]  Epoch: 60	Loss: 0.029803	Acc: 48.5% (4855/10000)
[Test]  Epoch: 61	Loss: 0.029745	Acc: 48.4% (4843/10000)
[Test]  Epoch: 62	Loss: 0.029657	Acc: 48.8% (4876/10000)
[Test]  Epoch: 63	Loss: 0.029650	Acc: 48.8% (4880/10000)
[Test]  Epoch: 64	Loss: 0.029634	Acc: 48.9% (4891/10000)
[Test]  Epoch: 65	Loss: 0.029565	Acc: 49.0% (4896/10000)
[Test]  Epoch: 66	Loss: 0.029589	Acc: 49.0% (4896/10000)
[Test]  Epoch: 67	Loss: 0.029668	Acc: 48.7% (4871/10000)
[Test]  Epoch: 68	Loss: 0.029579	Acc: 49.0% (4896/10000)
[Test]  Epoch: 69	Loss: 0.029593	Acc: 48.8% (4876/10000)
[Test]  Epoch: 70	Loss: 0.029616	Acc: 48.8% (4881/10000)
[Test]  Epoch: 71	Loss: 0.029572	Acc: 49.0% (4904/10000)
[Test]  Epoch: 72	Loss: 0.029605	Acc: 48.8% (4875/10000)
[Test]  Epoch: 73	Loss: 0.029535	Acc: 49.1% (4909/10000)
[Test]  Epoch: 74	Loss: 0.029538	Acc: 48.9% (4892/10000)
[Test]  Epoch: 75	Loss: 0.029558	Acc: 49.0% (4899/10000)
[Test]  Epoch: 76	Loss: 0.029636	Acc: 49.0% (4899/10000)
[Test]  Epoch: 77	Loss: 0.029571	Acc: 48.9% (4894/10000)
[Test]  Epoch: 78	Loss: 0.029629	Acc: 48.8% (4880/10000)
[Test]  Epoch: 79	Loss: 0.029607	Acc: 49.1% (4912/10000)
[Test]  Epoch: 80	Loss: 0.029581	Acc: 48.8% (4883/10000)
[Test]  Epoch: 81	Loss: 0.029525	Acc: 48.9% (4893/10000)
[Test]  Epoch: 82	Loss: 0.029632	Acc: 48.9% (4887/10000)
[Test]  Epoch: 83	Loss: 0.029564	Acc: 49.0% (4899/10000)
[Test]  Epoch: 84	Loss: 0.029570	Acc: 49.0% (4898/10000)
[Test]  Epoch: 85	Loss: 0.029535	Acc: 49.1% (4909/10000)
[Test]  Epoch: 86	Loss: 0.029595	Acc: 49.0% (4904/10000)
[Test]  Epoch: 87	Loss: 0.029556	Acc: 48.9% (4892/10000)
[Test]  Epoch: 88	Loss: 0.029640	Acc: 48.8% (4881/10000)
[Test]  Epoch: 89	Loss: 0.029624	Acc: 49.0% (4895/10000)
[Test]  Epoch: 90	Loss: 0.029547	Acc: 49.0% (4898/10000)
[Test]  Epoch: 91	Loss: 0.029530	Acc: 49.2% (4917/10000)
[Test]  Epoch: 92	Loss: 0.029614	Acc: 48.9% (4890/10000)
[Test]  Epoch: 93	Loss: 0.029573	Acc: 49.1% (4909/10000)
[Test]  Epoch: 94	Loss: 0.029556	Acc: 49.2% (4916/10000)
[Test]  Epoch: 95	Loss: 0.029622	Acc: 49.0% (4896/10000)
[Test]  Epoch: 96	Loss: 0.029537	Acc: 49.1% (4906/10000)
[Test]  Epoch: 97	Loss: 0.029670	Acc: 48.9% (4888/10000)
[Test]  Epoch: 98	Loss: 0.029605	Acc: 49.1% (4908/10000)
[Test]  Epoch: 99	Loss: 0.029631	Acc: 49.0% (4902/10000)
[Test]  Epoch: 100	Loss: 0.029638	Acc: 49.0% (4895/10000)
===========finish==========
['2024-08-19', '00:50:53.526309', '100', 'test', '0.029638237810134888', '48.95', '49.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.034369	Acc: 42.6% (4259/10000)
[Test]  Epoch: 2	Loss: 0.030867	Acc: 46.8% (4676/10000)
[Test]  Epoch: 3	Loss: 0.030315	Acc: 47.9% (4790/10000)
[Test]  Epoch: 4	Loss: 0.030417	Acc: 47.2% (4721/10000)
[Test]  Epoch: 5	Loss: 0.030168	Acc: 48.0% (4799/10000)
[Test]  Epoch: 6	Loss: 0.030064	Acc: 48.3% (4826/10000)
[Test]  Epoch: 7	Loss: 0.030092	Acc: 48.3% (4831/10000)
[Test]  Epoch: 8	Loss: 0.030088	Acc: 48.3% (4831/10000)
[Test]  Epoch: 9	Loss: 0.029981	Acc: 47.9% (4785/10000)
[Test]  Epoch: 10	Loss: 0.029969	Acc: 48.3% (4827/10000)
[Test]  Epoch: 11	Loss: 0.029937	Acc: 48.5% (4853/10000)
[Test]  Epoch: 12	Loss: 0.029862	Acc: 48.7% (4867/10000)
[Test]  Epoch: 13	Loss: 0.029836	Acc: 48.6% (4862/10000)
[Test]  Epoch: 14	Loss: 0.029819	Acc: 48.8% (4879/10000)
[Test]  Epoch: 15	Loss: 0.029977	Acc: 48.1% (4809/10000)
[Test]  Epoch: 16	Loss: 0.029771	Acc: 48.5% (4854/10000)
[Test]  Epoch: 17	Loss: 0.029850	Acc: 48.3% (4834/10000)
[Test]  Epoch: 18	Loss: 0.029943	Acc: 48.5% (4855/10000)
[Test]  Epoch: 19	Loss: 0.029892	Acc: 48.4% (4843/10000)
[Test]  Epoch: 20	Loss: 0.029837	Acc: 48.5% (4855/10000)
[Test]  Epoch: 21	Loss: 0.029745	Acc: 48.5% (4847/10000)
[Test]  Epoch: 22	Loss: 0.029751	Acc: 48.6% (4860/10000)
[Test]  Epoch: 23	Loss: 0.029755	Acc: 48.7% (4866/10000)
[Test]  Epoch: 24	Loss: 0.029838	Acc: 48.8% (4878/10000)
[Test]  Epoch: 25	Loss: 0.029760	Acc: 48.8% (4876/10000)
[Test]  Epoch: 26	Loss: 0.029763	Acc: 48.6% (4857/10000)
[Test]  Epoch: 27	Loss: 0.029859	Acc: 48.6% (4862/10000)
[Test]  Epoch: 28	Loss: 0.029940	Acc: 48.3% (4830/10000)
[Test]  Epoch: 29	Loss: 0.029838	Acc: 48.5% (4855/10000)
[Test]  Epoch: 30	Loss: 0.029770	Acc: 48.8% (4880/10000)
[Test]  Epoch: 31	Loss: 0.029731	Acc: 49.0% (4904/10000)
[Test]  Epoch: 32	Loss: 0.029864	Acc: 48.3% (4831/10000)
[Test]  Epoch: 33	Loss: 0.029802	Acc: 48.7% (4873/10000)
[Test]  Epoch: 34	Loss: 0.029789	Acc: 48.8% (4878/10000)
[Test]  Epoch: 35	Loss: 0.029826	Acc: 48.5% (4849/10000)
[Test]  Epoch: 36	Loss: 0.029933	Acc: 48.5% (4846/10000)
[Test]  Epoch: 37	Loss: 0.029812	Acc: 48.5% (4847/10000)
[Test]  Epoch: 38	Loss: 0.029724	Acc: 48.8% (4877/10000)
[Test]  Epoch: 39	Loss: 0.029797	Acc: 49.1% (4908/10000)
[Test]  Epoch: 40	Loss: 0.029781	Acc: 48.6% (4863/10000)
[Test]  Epoch: 41	Loss: 0.029782	Acc: 48.4% (4844/10000)
[Test]  Epoch: 42	Loss: 0.029916	Acc: 48.5% (4848/10000)
[Test]  Epoch: 43	Loss: 0.029891	Acc: 48.4% (4839/10000)
[Test]  Epoch: 44	Loss: 0.029730	Acc: 48.7% (4867/10000)
[Test]  Epoch: 45	Loss: 0.029978	Acc: 48.5% (4854/10000)
[Test]  Epoch: 46	Loss: 0.029780	Acc: 48.6% (4858/10000)
[Test]  Epoch: 47	Loss: 0.029769	Acc: 48.7% (4873/10000)
[Test]  Epoch: 48	Loss: 0.029790	Acc: 48.8% (4881/10000)
[Test]  Epoch: 49	Loss: 0.029804	Acc: 48.8% (4881/10000)
[Test]  Epoch: 50	Loss: 0.029800	Acc: 49.0% (4897/10000)
[Test]  Epoch: 51	Loss: 0.029816	Acc: 48.7% (4867/10000)
[Test]  Epoch: 52	Loss: 0.029790	Acc: 48.7% (4871/10000)
[Test]  Epoch: 53	Loss: 0.029844	Acc: 48.4% (4838/10000)
[Test]  Epoch: 54	Loss: 0.029942	Acc: 48.5% (4845/10000)
[Test]  Epoch: 55	Loss: 0.029808	Acc: 49.0% (4896/10000)
[Test]  Epoch: 56	Loss: 0.029872	Acc: 48.6% (4865/10000)
[Test]  Epoch: 57	Loss: 0.029887	Acc: 48.4% (4841/10000)
[Test]  Epoch: 58	Loss: 0.029750	Acc: 48.6% (4861/10000)
[Test]  Epoch: 59	Loss: 0.029839	Acc: 48.5% (4854/10000)
[Test]  Epoch: 60	Loss: 0.030049	Acc: 48.2% (4825/10000)
[Test]  Epoch: 61	Loss: 0.029958	Acc: 48.2% (4822/10000)
[Test]  Epoch: 62	Loss: 0.029878	Acc: 48.5% (4848/10000)
[Test]  Epoch: 63	Loss: 0.029868	Acc: 48.4% (4838/10000)
[Test]  Epoch: 64	Loss: 0.029839	Acc: 48.7% (4867/10000)
[Test]  Epoch: 65	Loss: 0.029782	Acc: 48.8% (4876/10000)
[Test]  Epoch: 66	Loss: 0.029800	Acc: 48.8% (4884/10000)
[Test]  Epoch: 67	Loss: 0.029867	Acc: 48.7% (4870/10000)
[Test]  Epoch: 68	Loss: 0.029785	Acc: 48.7% (4869/10000)
[Test]  Epoch: 69	Loss: 0.029795	Acc: 48.8% (4876/10000)
[Test]  Epoch: 70	Loss: 0.029807	Acc: 48.7% (4873/10000)
[Test]  Epoch: 71	Loss: 0.029776	Acc: 48.7% (4873/10000)
[Test]  Epoch: 72	Loss: 0.029795	Acc: 48.6% (4861/10000)
[Test]  Epoch: 73	Loss: 0.029727	Acc: 48.8% (4878/10000)
[Test]  Epoch: 74	Loss: 0.029739	Acc: 48.8% (4879/10000)
[Test]  Epoch: 75	Loss: 0.029770	Acc: 49.0% (4899/10000)
[Test]  Epoch: 76	Loss: 0.029833	Acc: 48.5% (4855/10000)
[Test]  Epoch: 77	Loss: 0.029788	Acc: 48.7% (4873/10000)
[Test]  Epoch: 78	Loss: 0.029829	Acc: 48.6% (4859/10000)
[Test]  Epoch: 79	Loss: 0.029807	Acc: 48.9% (4889/10000)
[Test]  Epoch: 80	Loss: 0.029787	Acc: 48.7% (4872/10000)
[Test]  Epoch: 81	Loss: 0.029727	Acc: 48.7% (4872/10000)
[Test]  Epoch: 82	Loss: 0.029835	Acc: 48.6% (4864/10000)
[Test]  Epoch: 83	Loss: 0.029775	Acc: 48.6% (4859/10000)
[Test]  Epoch: 84	Loss: 0.029769	Acc: 48.7% (4871/10000)
[Test]  Epoch: 85	Loss: 0.029733	Acc: 48.8% (4880/10000)
[Test]  Epoch: 86	Loss: 0.029794	Acc: 48.8% (4877/10000)
[Test]  Epoch: 87	Loss: 0.029752	Acc: 48.7% (4873/10000)
[Test]  Epoch: 88	Loss: 0.029854	Acc: 48.7% (4870/10000)
[Test]  Epoch: 89	Loss: 0.029840	Acc: 48.8% (4875/10000)
[Test]  Epoch: 90	Loss: 0.029767	Acc: 48.8% (4883/10000)
[Test]  Epoch: 91	Loss: 0.029746	Acc: 49.1% (4915/10000)
[Test]  Epoch: 92	Loss: 0.029840	Acc: 48.8% (4883/10000)
[Test]  Epoch: 93	Loss: 0.029780	Acc: 48.8% (4880/10000)
[Test]  Epoch: 94	Loss: 0.029750	Acc: 48.9% (4892/10000)
[Test]  Epoch: 95	Loss: 0.029828	Acc: 48.8% (4884/10000)
[Test]  Epoch: 96	Loss: 0.029743	Acc: 48.9% (4892/10000)
[Test]  Epoch: 97	Loss: 0.029869	Acc: 48.7% (4870/10000)
[Test]  Epoch: 98	Loss: 0.029796	Acc: 49.0% (4900/10000)
[Test]  Epoch: 99	Loss: 0.029839	Acc: 48.7% (4872/10000)
[Test]  Epoch: 100	Loss: 0.029833	Acc: 48.9% (4886/10000)
===========finish==========
['2024-08-19', '00:53:29.152294', '100', 'test', '0.029833117377758026', '48.86', '49.15']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.091415	Acc: 6.9% (690/10000)
[Test]  Epoch: 2	Loss: 0.061767	Acc: 12.7% (1272/10000)
[Test]  Epoch: 3	Loss: 0.056420	Acc: 16.0% (1600/10000)
[Test]  Epoch: 4	Loss: 0.055486	Acc: 16.6% (1663/10000)
[Test]  Epoch: 5	Loss: 0.053984	Acc: 17.9% (1788/10000)
[Test]  Epoch: 6	Loss: 0.054167	Acc: 17.8% (1782/10000)
[Test]  Epoch: 7	Loss: 0.053778	Acc: 18.5% (1849/10000)
[Test]  Epoch: 8	Loss: 0.053763	Acc: 18.9% (1892/10000)
[Test]  Epoch: 9	Loss: 0.053069	Acc: 19.5% (1949/10000)
[Test]  Epoch: 10	Loss: 0.052860	Acc: 19.7% (1967/10000)
[Test]  Epoch: 11	Loss: 0.053001	Acc: 19.8% (1983/10000)
[Test]  Epoch: 12	Loss: 0.053130	Acc: 19.7% (1972/10000)
[Test]  Epoch: 13	Loss: 0.053088	Acc: 19.6% (1965/10000)
[Test]  Epoch: 14	Loss: 0.053222	Acc: 19.8% (1976/10000)
[Test]  Epoch: 15	Loss: 0.053182	Acc: 19.5% (1949/10000)
[Test]  Epoch: 16	Loss: 0.052933	Acc: 20.0% (2003/10000)
[Test]  Epoch: 17	Loss: 0.052707	Acc: 19.7% (1969/10000)
[Test]  Epoch: 18	Loss: 0.053128	Acc: 19.2% (1923/10000)
[Test]  Epoch: 19	Loss: 0.052600	Acc: 19.8% (1980/10000)
[Test]  Epoch: 20	Loss: 0.052687	Acc: 19.9% (1993/10000)
[Test]  Epoch: 21	Loss: 0.052741	Acc: 20.1% (2010/10000)
[Test]  Epoch: 22	Loss: 0.052518	Acc: 20.6% (2059/10000)
[Test]  Epoch: 23	Loss: 0.052537	Acc: 19.9% (1985/10000)
[Test]  Epoch: 24	Loss: 0.052494	Acc: 20.2% (2024/10000)
[Test]  Epoch: 25	Loss: 0.052267	Acc: 20.4% (2044/10000)
[Test]  Epoch: 26	Loss: 0.052052	Acc: 20.6% (2062/10000)
[Test]  Epoch: 27	Loss: 0.052463	Acc: 20.2% (2021/10000)
[Test]  Epoch: 28	Loss: 0.052163	Acc: 20.1% (2008/10000)
[Test]  Epoch: 29	Loss: 0.051737	Acc: 20.6% (2064/10000)
[Test]  Epoch: 30	Loss: 0.052293	Acc: 20.4% (2039/10000)
[Test]  Epoch: 31	Loss: 0.051769	Acc: 20.7% (2073/10000)
[Test]  Epoch: 32	Loss: 0.051360	Acc: 21.3% (2127/10000)
[Test]  Epoch: 33	Loss: 0.051513	Acc: 20.4% (2035/10000)
[Test]  Epoch: 34	Loss: 0.051478	Acc: 21.1% (2106/10000)
[Test]  Epoch: 35	Loss: 0.051341	Acc: 21.0% (2097/10000)
[Test]  Epoch: 36	Loss: 0.051882	Acc: 20.8% (2076/10000)
[Test]  Epoch: 37	Loss: 0.051568	Acc: 20.7% (2072/10000)
[Test]  Epoch: 38	Loss: 0.051313	Acc: 20.9% (2094/10000)
[Test]  Epoch: 39	Loss: 0.051693	Acc: 20.9% (2088/10000)
[Test]  Epoch: 40	Loss: 0.051302	Acc: 20.8% (2080/10000)
[Test]  Epoch: 41	Loss: 0.051247	Acc: 20.6% (2057/10000)
[Test]  Epoch: 42	Loss: 0.051194	Acc: 21.4% (2144/10000)
[Test]  Epoch: 43	Loss: 0.051356	Acc: 21.4% (2141/10000)
[Test]  Epoch: 44	Loss: 0.050942	Acc: 21.2% (2121/10000)
[Test]  Epoch: 45	Loss: 0.051407	Acc: 20.9% (2089/10000)
[Test]  Epoch: 46	Loss: 0.051137	Acc: 21.0% (2104/10000)
[Test]  Epoch: 47	Loss: 0.050955	Acc: 21.2% (2120/10000)
[Test]  Epoch: 48	Loss: 0.050956	Acc: 21.4% (2140/10000)
[Test]  Epoch: 49	Loss: 0.050682	Acc: 21.6% (2157/10000)
[Test]  Epoch: 50	Loss: 0.050670	Acc: 21.7% (2166/10000)
[Test]  Epoch: 51	Loss: 0.051051	Acc: 21.3% (2130/10000)
[Test]  Epoch: 52	Loss: 0.050708	Acc: 21.0% (2102/10000)
[Test]  Epoch: 53	Loss: 0.050966	Acc: 21.4% (2139/10000)
[Test]  Epoch: 54	Loss: 0.050741	Acc: 21.2% (2118/10000)
[Test]  Epoch: 55	Loss: 0.050377	Acc: 22.1% (2209/10000)
[Test]  Epoch: 56	Loss: 0.050609	Acc: 21.7% (2172/10000)
[Test]  Epoch: 57	Loss: 0.050730	Acc: 21.4% (2138/10000)
[Test]  Epoch: 58	Loss: 0.050768	Acc: 21.3% (2134/10000)
[Test]  Epoch: 59	Loss: 0.050523	Acc: 21.5% (2154/10000)
[Test]  Epoch: 60	Loss: 0.050432	Acc: 21.5% (2154/10000)
[Test]  Epoch: 61	Loss: 0.050194	Acc: 21.9% (2195/10000)
[Test]  Epoch: 62	Loss: 0.050064	Acc: 22.1% (2209/10000)
[Test]  Epoch: 63	Loss: 0.050068	Acc: 22.2% (2218/10000)
[Test]  Epoch: 64	Loss: 0.050058	Acc: 22.0% (2202/10000)
[Test]  Epoch: 65	Loss: 0.049949	Acc: 22.2% (2218/10000)
[Test]  Epoch: 66	Loss: 0.050019	Acc: 22.1% (2205/10000)
[Test]  Epoch: 67	Loss: 0.050022	Acc: 22.1% (2206/10000)
[Test]  Epoch: 68	Loss: 0.049998	Acc: 21.9% (2195/10000)
[Test]  Epoch: 69	Loss: 0.049996	Acc: 22.1% (2206/10000)
[Test]  Epoch: 70	Loss: 0.050029	Acc: 22.0% (2204/10000)
[Test]  Epoch: 71	Loss: 0.049939	Acc: 22.1% (2213/10000)
[Test]  Epoch: 72	Loss: 0.049924	Acc: 22.1% (2213/10000)
[Test]  Epoch: 73	Loss: 0.049967	Acc: 22.1% (2210/10000)
[Test]  Epoch: 74	Loss: 0.049902	Acc: 21.9% (2187/10000)
[Test]  Epoch: 75	Loss: 0.050020	Acc: 21.9% (2194/10000)
[Test]  Epoch: 76	Loss: 0.049971	Acc: 21.9% (2195/10000)
[Test]  Epoch: 77	Loss: 0.049905	Acc: 22.1% (2211/10000)
[Test]  Epoch: 78	Loss: 0.050034	Acc: 22.1% (2205/10000)
[Test]  Epoch: 79	Loss: 0.050004	Acc: 22.1% (2212/10000)
[Test]  Epoch: 80	Loss: 0.049915	Acc: 22.2% (2220/10000)
[Test]  Epoch: 81	Loss: 0.049904	Acc: 22.1% (2213/10000)
[Test]  Epoch: 82	Loss: 0.049915	Acc: 22.1% (2205/10000)
[Test]  Epoch: 83	Loss: 0.049905	Acc: 21.9% (2186/10000)
[Test]  Epoch: 84	Loss: 0.050040	Acc: 22.1% (2213/10000)
[Test]  Epoch: 85	Loss: 0.049893	Acc: 21.9% (2193/10000)
[Test]  Epoch: 86	Loss: 0.049936	Acc: 22.1% (2210/10000)
[Test]  Epoch: 87	Loss: 0.049990	Acc: 22.0% (2197/10000)
[Test]  Epoch: 88	Loss: 0.049994	Acc: 21.8% (2181/10000)
[Test]  Epoch: 89	Loss: 0.050008	Acc: 22.1% (2205/10000)
[Test]  Epoch: 90	Loss: 0.049908	Acc: 22.1% (2206/10000)
[Test]  Epoch: 91	Loss: 0.049923	Acc: 22.1% (2207/10000)
[Test]  Epoch: 92	Loss: 0.049917	Acc: 22.1% (2206/10000)
[Test]  Epoch: 93	Loss: 0.049985	Acc: 22.0% (2197/10000)
[Test]  Epoch: 94	Loss: 0.050010	Acc: 21.9% (2194/10000)
[Test]  Epoch: 95	Loss: 0.050082	Acc: 21.8% (2179/10000)
[Test]  Epoch: 96	Loss: 0.049951	Acc: 22.0% (2202/10000)
[Test]  Epoch: 97	Loss: 0.050008	Acc: 22.0% (2197/10000)
[Test]  Epoch: 98	Loss: 0.049882	Acc: 22.0% (2201/10000)
[Test]  Epoch: 99	Loss: 0.049931	Acc: 22.1% (2205/10000)
[Test]  Epoch: 100	Loss: 0.049957	Acc: 22.0% (2201/10000)
===========finish==========
['2024-08-19', '00:55:52.109430', '100', 'test', '0.04995710785388947', '22.01', '22.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight', '_features.4.conv.3.weight', '_features.3.conv.1.1.weight', '_features.0.1.weight', '_features.12.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.5.conv.1.1.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.16.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.110977	Acc: 4.1% (407/10000)
[Test]  Epoch: 2	Loss: 0.069459	Acc: 7.5% (745/10000)
[Test]  Epoch: 3	Loss: 0.064541	Acc: 10.0% (996/10000)
[Test]  Epoch: 4	Loss: 0.062217	Acc: 11.7% (1172/10000)
[Test]  Epoch: 5	Loss: 0.062986	Acc: 10.7% (1068/10000)
[Test]  Epoch: 6	Loss: 0.062031	Acc: 11.4% (1140/10000)
[Test]  Epoch: 7	Loss: 0.062984	Acc: 11.4% (1142/10000)
[Test]  Epoch: 8	Loss: 0.063478	Acc: 11.5% (1150/10000)
[Test]  Epoch: 9	Loss: 0.062648	Acc: 11.6% (1157/10000)
[Test]  Epoch: 10	Loss: 0.063274	Acc: 11.3% (1134/10000)
[Test]  Epoch: 11	Loss: 0.065120	Acc: 10.7% (1071/10000)
[Test]  Epoch: 12	Loss: 0.061507	Acc: 12.9% (1286/10000)
[Test]  Epoch: 13	Loss: 0.061611	Acc: 13.0% (1303/10000)
[Test]  Epoch: 14	Loss: 0.061326	Acc: 12.8% (1279/10000)
[Test]  Epoch: 15	Loss: 0.061125	Acc: 12.9% (1292/10000)
[Test]  Epoch: 16	Loss: 0.061231	Acc: 13.0% (1298/10000)
[Test]  Epoch: 17	Loss: 0.060886	Acc: 13.4% (1339/10000)
[Test]  Epoch: 18	Loss: 0.060463	Acc: 13.6% (1362/10000)
[Test]  Epoch: 19	Loss: 0.060961	Acc: 12.9% (1293/10000)
[Test]  Epoch: 20	Loss: 0.060700	Acc: 13.4% (1339/10000)
[Test]  Epoch: 21	Loss: 0.060584	Acc: 13.3% (1334/10000)
[Test]  Epoch: 22	Loss: 0.059957	Acc: 13.9% (1388/10000)
[Test]  Epoch: 23	Loss: 0.059605	Acc: 13.4% (1336/10000)
[Test]  Epoch: 24	Loss: 0.060508	Acc: 13.7% (1373/10000)
[Test]  Epoch: 25	Loss: 0.059947	Acc: 13.8% (1383/10000)
[Test]  Epoch: 26	Loss: 0.060178	Acc: 14.0% (1398/10000)
[Test]  Epoch: 27	Loss: 0.060009	Acc: 14.0% (1401/10000)
[Test]  Epoch: 28	Loss: 0.060502	Acc: 13.3% (1329/10000)
[Test]  Epoch: 29	Loss: 0.059092	Acc: 14.5% (1453/10000)
[Test]  Epoch: 30	Loss: 0.059660	Acc: 14.4% (1443/10000)
[Test]  Epoch: 31	Loss: 0.059764	Acc: 13.9% (1392/10000)
[Test]  Epoch: 32	Loss: 0.059050	Acc: 15.1% (1507/10000)
[Test]  Epoch: 33	Loss: 0.059138	Acc: 14.5% (1451/10000)
[Test]  Epoch: 34	Loss: 0.058399	Acc: 14.9% (1491/10000)
[Test]  Epoch: 35	Loss: 0.058429	Acc: 14.8% (1485/10000)
[Test]  Epoch: 36	Loss: 0.058623	Acc: 15.0% (1499/10000)
[Test]  Epoch: 37	Loss: 0.058843	Acc: 14.4% (1442/10000)
[Test]  Epoch: 38	Loss: 0.058873	Acc: 14.5% (1447/10000)
[Test]  Epoch: 39	Loss: 0.058336	Acc: 14.9% (1493/10000)
[Test]  Epoch: 40	Loss: 0.058347	Acc: 14.3% (1433/10000)
[Test]  Epoch: 41	Loss: 0.058420	Acc: 14.5% (1449/10000)
[Test]  Epoch: 42	Loss: 0.058419	Acc: 14.7% (1465/10000)
[Test]  Epoch: 43	Loss: 0.058516	Acc: 14.7% (1468/10000)
[Test]  Epoch: 44	Loss: 0.058633	Acc: 14.8% (1475/10000)
[Test]  Epoch: 45	Loss: 0.058022	Acc: 14.8% (1478/10000)
[Test]  Epoch: 46	Loss: 0.057909	Acc: 14.8% (1477/10000)
[Test]  Epoch: 47	Loss: 0.058334	Acc: 14.5% (1452/10000)
[Test]  Epoch: 48	Loss: 0.058147	Acc: 14.8% (1483/10000)
[Test]  Epoch: 49	Loss: 0.058058	Acc: 15.1% (1513/10000)
[Test]  Epoch: 50	Loss: 0.058018	Acc: 14.9% (1493/10000)
[Test]  Epoch: 51	Loss: 0.057958	Acc: 15.0% (1500/10000)
[Test]  Epoch: 52	Loss: 0.058070	Acc: 14.6% (1458/10000)
[Test]  Epoch: 53	Loss: 0.058971	Acc: 14.2% (1418/10000)
[Test]  Epoch: 54	Loss: 0.058005	Acc: 15.7% (1567/10000)
[Test]  Epoch: 55	Loss: 0.058013	Acc: 15.0% (1502/10000)
[Test]  Epoch: 56	Loss: 0.057663	Acc: 14.8% (1481/10000)
[Test]  Epoch: 57	Loss: 0.057741	Acc: 14.9% (1489/10000)
[Test]  Epoch: 58	Loss: 0.057749	Acc: 15.0% (1501/10000)
[Test]  Epoch: 59	Loss: 0.057877	Acc: 15.1% (1509/10000)
[Test]  Epoch: 60	Loss: 0.058117	Acc: 14.9% (1491/10000)
[Test]  Epoch: 61	Loss: 0.057414	Acc: 15.5% (1554/10000)
[Test]  Epoch: 62	Loss: 0.057169	Acc: 15.6% (1563/10000)
[Test]  Epoch: 63	Loss: 0.057157	Acc: 15.7% (1573/10000)
[Test]  Epoch: 64	Loss: 0.057109	Acc: 15.8% (1582/10000)
[Test]  Epoch: 65	Loss: 0.057048	Acc: 15.8% (1576/10000)
[Test]  Epoch: 66	Loss: 0.057012	Acc: 15.7% (1567/10000)
[Test]  Epoch: 67	Loss: 0.057017	Acc: 15.7% (1570/10000)
[Test]  Epoch: 68	Loss: 0.057078	Acc: 15.7% (1566/10000)
[Test]  Epoch: 69	Loss: 0.056990	Acc: 15.9% (1586/10000)
[Test]  Epoch: 70	Loss: 0.057056	Acc: 15.8% (1585/10000)
[Test]  Epoch: 71	Loss: 0.056886	Acc: 15.9% (1590/10000)
[Test]  Epoch: 72	Loss: 0.057010	Acc: 15.6% (1564/10000)
[Test]  Epoch: 73	Loss: 0.057049	Acc: 15.7% (1571/10000)
[Test]  Epoch: 74	Loss: 0.057006	Acc: 15.7% (1569/10000)
[Test]  Epoch: 75	Loss: 0.056983	Acc: 15.7% (1570/10000)
[Test]  Epoch: 76	Loss: 0.057013	Acc: 15.6% (1563/10000)
[Test]  Epoch: 77	Loss: 0.057002	Acc: 15.7% (1566/10000)
[Test]  Epoch: 78	Loss: 0.056978	Acc: 15.8% (1583/10000)
[Test]  Epoch: 79	Loss: 0.056967	Acc: 15.6% (1562/10000)
[Test]  Epoch: 80	Loss: 0.056990	Acc: 15.9% (1592/10000)
[Test]  Epoch: 81	Loss: 0.056891	Acc: 15.8% (1575/10000)
[Test]  Epoch: 82	Loss: 0.056810	Acc: 15.8% (1576/10000)
[Test]  Epoch: 83	Loss: 0.056969	Acc: 15.6% (1559/10000)
[Test]  Epoch: 84	Loss: 0.057039	Acc: 15.6% (1563/10000)
[Test]  Epoch: 85	Loss: 0.056927	Acc: 15.7% (1572/10000)
[Test]  Epoch: 86	Loss: 0.057038	Acc: 15.8% (1579/10000)
[Test]  Epoch: 87	Loss: 0.056998	Acc: 15.9% (1593/10000)
[Test]  Epoch: 88	Loss: 0.056956	Acc: 15.7% (1567/10000)
[Test]  Epoch: 89	Loss: 0.057043	Acc: 15.7% (1567/10000)
[Test]  Epoch: 90	Loss: 0.056949	Acc: 15.9% (1591/10000)
[Test]  Epoch: 91	Loss: 0.056951	Acc: 15.8% (1584/10000)
[Test]  Epoch: 92	Loss: 0.056899	Acc: 15.6% (1556/10000)
[Test]  Epoch: 93	Loss: 0.056902	Acc: 15.8% (1577/10000)
[Test]  Epoch: 94	Loss: 0.056842	Acc: 15.8% (1581/10000)
[Test]  Epoch: 95	Loss: 0.056864	Acc: 15.7% (1574/10000)
[Test]  Epoch: 96	Loss: 0.056845	Acc: 15.8% (1584/10000)
[Test]  Epoch: 97	Loss: 0.056850	Acc: 15.6% (1561/10000)
[Test]  Epoch: 98	Loss: 0.056798	Acc: 15.7% (1571/10000)
[Test]  Epoch: 99	Loss: 0.056933	Acc: 15.7% (1572/10000)
[Test]  Epoch: 100	Loss: 0.056907	Acc: 15.6% (1562/10000)
===========finish==========
['2024-08-19', '00:58:18.960067', '100', 'test', '0.05690691215991974', '15.62', '15.93']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight', '_features.4.conv.3.weight', '_features.3.conv.1.1.weight', '_features.0.1.weight', '_features.12.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.5.conv.1.1.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.16.conv.2.weight', '_features.2.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.4.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.0.0.weight', '_features.8.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.160251	Acc: 2.4% (235/10000)
[Test]  Epoch: 2	Loss: 0.076548	Acc: 5.1% (507/10000)
[Test]  Epoch: 3	Loss: 0.075247	Acc: 6.0% (597/10000)
[Test]  Epoch: 4	Loss: 0.064276	Acc: 9.3% (926/10000)
[Test]  Epoch: 5	Loss: 0.063997	Acc: 9.7% (972/10000)
[Test]  Epoch: 6	Loss: 0.063879	Acc: 9.9% (987/10000)
[Test]  Epoch: 7	Loss: 0.063627	Acc: 10.7% (1066/10000)
[Test]  Epoch: 8	Loss: 0.062934	Acc: 11.5% (1149/10000)
[Test]  Epoch: 9	Loss: 0.063975	Acc: 11.1% (1110/10000)
[Test]  Epoch: 10	Loss: 0.062066	Acc: 12.1% (1214/10000)
[Test]  Epoch: 11	Loss: 0.062558	Acc: 11.8% (1175/10000)
[Test]  Epoch: 12	Loss: 0.062275	Acc: 12.4% (1242/10000)
[Test]  Epoch: 13	Loss: 0.064138	Acc: 11.6% (1160/10000)
[Test]  Epoch: 14	Loss: 0.063123	Acc: 12.0% (1203/10000)
[Test]  Epoch: 15	Loss: 0.062110	Acc: 12.4% (1244/10000)
[Test]  Epoch: 16	Loss: 0.062332	Acc: 12.4% (1243/10000)
[Test]  Epoch: 17	Loss: 0.061088	Acc: 13.0% (1296/10000)
[Test]  Epoch: 18	Loss: 0.062389	Acc: 12.6% (1255/10000)
[Test]  Epoch: 19	Loss: 0.061817	Acc: 13.3% (1326/10000)
[Test]  Epoch: 20	Loss: 0.062752	Acc: 12.9% (1289/10000)
[Test]  Epoch: 21	Loss: 0.061411	Acc: 12.6% (1260/10000)
[Test]  Epoch: 22	Loss: 0.060842	Acc: 13.4% (1341/10000)
[Test]  Epoch: 23	Loss: 0.061797	Acc: 12.9% (1293/10000)
[Test]  Epoch: 24	Loss: 0.061189	Acc: 13.6% (1355/10000)
[Test]  Epoch: 25	Loss: 0.061118	Acc: 13.0% (1301/10000)
[Test]  Epoch: 26	Loss: 0.061267	Acc: 13.8% (1378/10000)
[Test]  Epoch: 27	Loss: 0.062588	Acc: 13.3% (1326/10000)
[Test]  Epoch: 28	Loss: 0.062225	Acc: 12.9% (1288/10000)
[Test]  Epoch: 29	Loss: 0.061244	Acc: 13.1% (1308/10000)
[Test]  Epoch: 30	Loss: 0.060467	Acc: 13.9% (1395/10000)
[Test]  Epoch: 31	Loss: 0.061017	Acc: 13.3% (1329/10000)
[Test]  Epoch: 32	Loss: 0.060727	Acc: 13.8% (1384/10000)
[Test]  Epoch: 33	Loss: 0.060363	Acc: 14.1% (1405/10000)
[Test]  Epoch: 34	Loss: 0.061082	Acc: 13.5% (1349/10000)
[Test]  Epoch: 35	Loss: 0.060844	Acc: 13.2% (1324/10000)
[Test]  Epoch: 36	Loss: 0.060874	Acc: 13.4% (1345/10000)
[Test]  Epoch: 37	Loss: 0.060216	Acc: 13.6% (1361/10000)
[Test]  Epoch: 38	Loss: 0.060111	Acc: 14.1% (1405/10000)
[Test]  Epoch: 39	Loss: 0.060701	Acc: 13.5% (1354/10000)
[Test]  Epoch: 40	Loss: 0.059794	Acc: 13.7% (1373/10000)
[Test]  Epoch: 41	Loss: 0.059923	Acc: 13.9% (1387/10000)
[Test]  Epoch: 42	Loss: 0.059807	Acc: 14.2% (1415/10000)
[Test]  Epoch: 43	Loss: 0.059481	Acc: 14.2% (1422/10000)
[Test]  Epoch: 44	Loss: 0.060347	Acc: 13.6% (1358/10000)
[Test]  Epoch: 45	Loss: 0.059985	Acc: 14.1% (1414/10000)
[Test]  Epoch: 46	Loss: 0.059842	Acc: 14.2% (1425/10000)
[Test]  Epoch: 47	Loss: 0.059848	Acc: 14.4% (1439/10000)
[Test]  Epoch: 48	Loss: 0.059477	Acc: 14.0% (1401/10000)
[Test]  Epoch: 49	Loss: 0.059448	Acc: 14.3% (1429/10000)
[Test]  Epoch: 50	Loss: 0.059672	Acc: 14.2% (1418/10000)
[Test]  Epoch: 51	Loss: 0.059644	Acc: 13.9% (1393/10000)
[Test]  Epoch: 52	Loss: 0.059445	Acc: 14.2% (1415/10000)
[Test]  Epoch: 53	Loss: 0.060403	Acc: 13.6% (1358/10000)
[Test]  Epoch: 54	Loss: 0.059683	Acc: 14.1% (1414/10000)
[Test]  Epoch: 55	Loss: 0.059864	Acc: 13.7% (1370/10000)
[Test]  Epoch: 56	Loss: 0.059202	Acc: 14.6% (1461/10000)
[Test]  Epoch: 57	Loss: 0.059235	Acc: 14.3% (1435/10000)
[Test]  Epoch: 58	Loss: 0.059199	Acc: 14.5% (1452/10000)
[Test]  Epoch: 59	Loss: 0.059281	Acc: 14.7% (1469/10000)
[Test]  Epoch: 60	Loss: 0.059350	Acc: 14.2% (1423/10000)
[Test]  Epoch: 61	Loss: 0.058870	Acc: 14.5% (1454/10000)
[Test]  Epoch: 62	Loss: 0.058708	Acc: 14.6% (1462/10000)
[Test]  Epoch: 63	Loss: 0.058728	Acc: 14.6% (1461/10000)
[Test]  Epoch: 64	Loss: 0.058678	Acc: 14.4% (1444/10000)
[Test]  Epoch: 65	Loss: 0.058588	Acc: 14.8% (1480/10000)
[Test]  Epoch: 66	Loss: 0.058587	Acc: 14.6% (1464/10000)
[Test]  Epoch: 67	Loss: 0.058726	Acc: 14.7% (1467/10000)
[Test]  Epoch: 68	Loss: 0.058706	Acc: 14.5% (1454/10000)
[Test]  Epoch: 69	Loss: 0.058586	Acc: 14.6% (1458/10000)
[Test]  Epoch: 70	Loss: 0.058673	Acc: 14.4% (1443/10000)
[Test]  Epoch: 71	Loss: 0.058665	Acc: 14.5% (1446/10000)
[Test]  Epoch: 72	Loss: 0.058605	Acc: 14.7% (1468/10000)
[Test]  Epoch: 73	Loss: 0.058598	Acc: 14.7% (1467/10000)
[Test]  Epoch: 74	Loss: 0.058482	Acc: 14.6% (1462/10000)
[Test]  Epoch: 75	Loss: 0.058503	Acc: 14.7% (1471/10000)
[Test]  Epoch: 76	Loss: 0.058688	Acc: 14.6% (1460/10000)
[Test]  Epoch: 77	Loss: 0.058594	Acc: 14.4% (1445/10000)
[Test]  Epoch: 78	Loss: 0.058623	Acc: 14.5% (1447/10000)
[Test]  Epoch: 79	Loss: 0.058561	Acc: 14.7% (1465/10000)
[Test]  Epoch: 80	Loss: 0.058651	Acc: 14.3% (1434/10000)
[Test]  Epoch: 81	Loss: 0.058586	Acc: 14.7% (1467/10000)
[Test]  Epoch: 82	Loss: 0.058559	Acc: 14.6% (1458/10000)
[Test]  Epoch: 83	Loss: 0.058495	Acc: 14.3% (1426/10000)
[Test]  Epoch: 84	Loss: 0.058595	Acc: 14.5% (1447/10000)
[Test]  Epoch: 85	Loss: 0.058489	Acc: 14.5% (1454/10000)
[Test]  Epoch: 86	Loss: 0.058576	Acc: 14.5% (1451/10000)
[Test]  Epoch: 87	Loss: 0.058705	Acc: 14.5% (1446/10000)
[Test]  Epoch: 88	Loss: 0.058655	Acc: 14.4% (1441/10000)
[Test]  Epoch: 89	Loss: 0.058620	Acc: 14.4% (1444/10000)
[Test]  Epoch: 90	Loss: 0.058591	Acc: 14.4% (1441/10000)
[Test]  Epoch: 91	Loss: 0.058582	Acc: 14.4% (1439/10000)
[Test]  Epoch: 92	Loss: 0.058529	Acc: 14.7% (1465/10000)
[Test]  Epoch: 93	Loss: 0.058570	Acc: 14.7% (1465/10000)
[Test]  Epoch: 94	Loss: 0.058530	Acc: 14.4% (1443/10000)
[Test]  Epoch: 95	Loss: 0.058616	Acc: 14.3% (1426/10000)
[Test]  Epoch: 96	Loss: 0.058569	Acc: 14.6% (1460/10000)
[Test]  Epoch: 97	Loss: 0.058612	Acc: 14.6% (1455/10000)
[Test]  Epoch: 98	Loss: 0.058520	Acc: 14.4% (1444/10000)
[Test]  Epoch: 99	Loss: 0.058501	Acc: 14.4% (1437/10000)
[Test]  Epoch: 100	Loss: 0.058542	Acc: 14.4% (1436/10000)
===========finish==========
['2024-08-19', '01:00:47.772958', '100', 'test', '0.0585417475938797', '14.36', '14.8']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight', '_features.4.conv.3.weight', '_features.3.conv.1.1.weight', '_features.0.1.weight', '_features.12.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.5.conv.1.1.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.16.conv.2.weight', '_features.2.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.4.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.0.0.weight', '_features.8.conv.3.weight', '_features.5.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.205929	Acc: 1.9% (192/10000)
[Test]  Epoch: 2	Loss: 0.081991	Acc: 5.4% (541/10000)
[Test]  Epoch: 3	Loss: 0.068973	Acc: 7.5% (745/10000)
[Test]  Epoch: 4	Loss: 0.064067	Acc: 8.9% (894/10000)
[Test]  Epoch: 5	Loss: 0.064747	Acc: 8.8% (880/10000)
[Test]  Epoch: 6	Loss: 0.064284	Acc: 9.2% (919/10000)
[Test]  Epoch: 7	Loss: 0.062898	Acc: 10.2% (1023/10000)
[Test]  Epoch: 8	Loss: 0.064295	Acc: 9.2% (918/10000)
[Test]  Epoch: 9	Loss: 0.064017	Acc: 9.7% (965/10000)
[Test]  Epoch: 10	Loss: 0.063086	Acc: 10.9% (1088/10000)
[Test]  Epoch: 11	Loss: 0.063776	Acc: 10.4% (1045/10000)
[Test]  Epoch: 12	Loss: 0.063420	Acc: 10.8% (1075/10000)
[Test]  Epoch: 13	Loss: 0.063423	Acc: 10.8% (1085/10000)
[Test]  Epoch: 14	Loss: 0.063649	Acc: 11.6% (1155/10000)
[Test]  Epoch: 15	Loss: 0.063738	Acc: 11.4% (1140/10000)
[Test]  Epoch: 16	Loss: 0.063900	Acc: 11.0% (1101/10000)
[Test]  Epoch: 17	Loss: 0.063164	Acc: 11.0% (1099/10000)
[Test]  Epoch: 18	Loss: 0.063671	Acc: 11.2% (1117/10000)
[Test]  Epoch: 19	Loss: 0.062844	Acc: 11.7% (1165/10000)
[Test]  Epoch: 20	Loss: 0.063393	Acc: 11.4% (1141/10000)
[Test]  Epoch: 21	Loss: 0.063290	Acc: 11.2% (1123/10000)
[Test]  Epoch: 22	Loss: 0.062800	Acc: 12.1% (1205/10000)
[Test]  Epoch: 23	Loss: 0.062391	Acc: 12.2% (1218/10000)
[Test]  Epoch: 24	Loss: 0.063543	Acc: 12.1% (1207/10000)
[Test]  Epoch: 25	Loss: 0.063255	Acc: 11.9% (1188/10000)
[Test]  Epoch: 26	Loss: 0.062133	Acc: 12.9% (1290/10000)
[Test]  Epoch: 27	Loss: 0.062787	Acc: 12.6% (1262/10000)
[Test]  Epoch: 28	Loss: 0.062816	Acc: 12.1% (1209/10000)
[Test]  Epoch: 29	Loss: 0.062792	Acc: 12.8% (1277/10000)
[Test]  Epoch: 30	Loss: 0.062199	Acc: 13.0% (1303/10000)
[Test]  Epoch: 31	Loss: 0.062546	Acc: 13.0% (1302/10000)
[Test]  Epoch: 32	Loss: 0.061795	Acc: 13.0% (1297/10000)
[Test]  Epoch: 33	Loss: 0.061834	Acc: 12.6% (1260/10000)
[Test]  Epoch: 34	Loss: 0.061106	Acc: 13.8% (1378/10000)
[Test]  Epoch: 35	Loss: 0.062691	Acc: 12.6% (1255/10000)
[Test]  Epoch: 36	Loss: 0.061817	Acc: 13.0% (1302/10000)
[Test]  Epoch: 37	Loss: 0.061791	Acc: 12.8% (1278/10000)
[Test]  Epoch: 38	Loss: 0.061605	Acc: 13.1% (1307/10000)
[Test]  Epoch: 39	Loss: 0.061116	Acc: 12.8% (1284/10000)
[Test]  Epoch: 40	Loss: 0.061593	Acc: 13.2% (1322/10000)
[Test]  Epoch: 41	Loss: 0.062451	Acc: 13.1% (1305/10000)
[Test]  Epoch: 42	Loss: 0.061406	Acc: 12.8% (1283/10000)
[Test]  Epoch: 43	Loss: 0.061505	Acc: 12.9% (1295/10000)
[Test]  Epoch: 44	Loss: 0.061772	Acc: 13.2% (1323/10000)
[Test]  Epoch: 45	Loss: 0.061909	Acc: 12.7% (1273/10000)
[Test]  Epoch: 46	Loss: 0.061445	Acc: 13.1% (1306/10000)
[Test]  Epoch: 47	Loss: 0.060617	Acc: 13.5% (1351/10000)
[Test]  Epoch: 48	Loss: 0.061332	Acc: 13.2% (1324/10000)
[Test]  Epoch: 49	Loss: 0.061175	Acc: 13.0% (1298/10000)
[Test]  Epoch: 50	Loss: 0.061613	Acc: 13.2% (1322/10000)
[Test]  Epoch: 51	Loss: 0.061293	Acc: 13.6% (1362/10000)
[Test]  Epoch: 52	Loss: 0.061518	Acc: 13.1% (1309/10000)
[Test]  Epoch: 53	Loss: 0.061858	Acc: 13.1% (1310/10000)
[Test]  Epoch: 54	Loss: 0.061335	Acc: 13.1% (1306/10000)
[Test]  Epoch: 55	Loss: 0.061681	Acc: 12.9% (1293/10000)
[Test]  Epoch: 56	Loss: 0.060761	Acc: 13.6% (1360/10000)
[Test]  Epoch: 57	Loss: 0.060567	Acc: 13.4% (1344/10000)
[Test]  Epoch: 58	Loss: 0.060860	Acc: 13.0% (1296/10000)
[Test]  Epoch: 59	Loss: 0.061123	Acc: 13.5% (1347/10000)
[Test]  Epoch: 60	Loss: 0.060843	Acc: 13.2% (1325/10000)
[Test]  Epoch: 61	Loss: 0.060454	Acc: 13.3% (1331/10000)
[Test]  Epoch: 62	Loss: 0.060334	Acc: 13.4% (1341/10000)
[Test]  Epoch: 63	Loss: 0.060274	Acc: 13.4% (1339/10000)
[Test]  Epoch: 64	Loss: 0.060189	Acc: 13.5% (1346/10000)
[Test]  Epoch: 65	Loss: 0.059995	Acc: 13.8% (1384/10000)
[Test]  Epoch: 66	Loss: 0.060006	Acc: 13.7% (1369/10000)
[Test]  Epoch: 67	Loss: 0.059968	Acc: 13.8% (1376/10000)
[Test]  Epoch: 68	Loss: 0.059956	Acc: 13.7% (1365/10000)
[Test]  Epoch: 69	Loss: 0.059844	Acc: 13.4% (1341/10000)
[Test]  Epoch: 70	Loss: 0.059882	Acc: 13.5% (1349/10000)
[Test]  Epoch: 71	Loss: 0.059936	Acc: 13.5% (1353/10000)
[Test]  Epoch: 72	Loss: 0.059858	Acc: 13.7% (1367/10000)
[Test]  Epoch: 73	Loss: 0.059812	Acc: 13.4% (1343/10000)
[Test]  Epoch: 74	Loss: 0.059779	Acc: 13.6% (1363/10000)
[Test]  Epoch: 75	Loss: 0.059739	Acc: 13.4% (1343/10000)
[Test]  Epoch: 76	Loss: 0.059751	Acc: 13.6% (1364/10000)
[Test]  Epoch: 77	Loss: 0.059752	Acc: 13.3% (1333/10000)
[Test]  Epoch: 78	Loss: 0.059851	Acc: 13.6% (1357/10000)
[Test]  Epoch: 79	Loss: 0.059872	Acc: 13.6% (1363/10000)
[Test]  Epoch: 80	Loss: 0.059706	Acc: 13.6% (1358/10000)
[Test]  Epoch: 81	Loss: 0.059713	Acc: 13.8% (1377/10000)
[Test]  Epoch: 82	Loss: 0.059735	Acc: 13.5% (1350/10000)
[Test]  Epoch: 83	Loss: 0.059723	Acc: 13.5% (1347/10000)
[Test]  Epoch: 84	Loss: 0.059783	Acc: 13.5% (1348/10000)
[Test]  Epoch: 85	Loss: 0.059680	Acc: 13.6% (1358/10000)
[Test]  Epoch: 86	Loss: 0.059779	Acc: 13.6% (1360/10000)
[Test]  Epoch: 87	Loss: 0.059840	Acc: 13.7% (1366/10000)
[Test]  Epoch: 88	Loss: 0.059815	Acc: 13.5% (1351/10000)
[Test]  Epoch: 89	Loss: 0.059905	Acc: 13.7% (1368/10000)
[Test]  Epoch: 90	Loss: 0.059730	Acc: 13.4% (1342/10000)
[Test]  Epoch: 91	Loss: 0.059763	Acc: 13.5% (1353/10000)
[Test]  Epoch: 92	Loss: 0.059746	Acc: 13.6% (1360/10000)
[Test]  Epoch: 93	Loss: 0.059666	Acc: 13.7% (1368/10000)
[Test]  Epoch: 94	Loss: 0.059624	Acc: 13.9% (1386/10000)
[Test]  Epoch: 95	Loss: 0.059731	Acc: 13.5% (1349/10000)
[Test]  Epoch: 96	Loss: 0.059654	Acc: 13.7% (1371/10000)
[Test]  Epoch: 97	Loss: 0.059735	Acc: 13.6% (1361/10000)
[Test]  Epoch: 98	Loss: 0.059645	Acc: 13.5% (1347/10000)
[Test]  Epoch: 99	Loss: 0.059654	Acc: 13.5% (1350/10000)
[Test]  Epoch: 100	Loss: 0.059768	Acc: 13.5% (1353/10000)
===========finish==========
['2024-08-19', '01:03:02.294420', '100', 'test', '0.05976823451519012', '13.53', '13.86']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight', '_features.4.conv.3.weight', '_features.3.conv.1.1.weight', '_features.0.1.weight', '_features.12.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.5.conv.1.1.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.16.conv.2.weight', '_features.2.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.4.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.0.0.weight', '_features.8.conv.3.weight', '_features.5.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.9.conv.1.1.weight', '_features.9.conv.3.weight', '_features.9.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.3.conv.2.weight', '_features.3.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.138039	Acc: 2.5% (254/10000)
[Test]  Epoch: 2	Loss: 0.079600	Acc: 5.3% (532/10000)
[Test]  Epoch: 3	Loss: 0.070007	Acc: 7.2% (717/10000)
[Test]  Epoch: 4	Loss: 0.065509	Acc: 8.5% (852/10000)
[Test]  Epoch: 5	Loss: 0.063564	Acc: 9.7% (966/10000)
[Test]  Epoch: 6	Loss: 0.063388	Acc: 9.9% (986/10000)
[Test]  Epoch: 7	Loss: 0.062920	Acc: 10.7% (1069/10000)
[Test]  Epoch: 8	Loss: 0.061739	Acc: 11.0% (1102/10000)
[Test]  Epoch: 9	Loss: 0.061416	Acc: 11.7% (1172/10000)
[Test]  Epoch: 10	Loss: 0.062280	Acc: 11.0% (1101/10000)
[Test]  Epoch: 11	Loss: 0.062558	Acc: 11.7% (1173/10000)
[Test]  Epoch: 12	Loss: 0.064187	Acc: 10.9% (1089/10000)
[Test]  Epoch: 13	Loss: 0.064352	Acc: 11.1% (1107/10000)
[Test]  Epoch: 14	Loss: 0.064018	Acc: 10.9% (1094/10000)
[Test]  Epoch: 15	Loss: 0.062688	Acc: 11.5% (1153/10000)
[Test]  Epoch: 16	Loss: 0.062612	Acc: 11.6% (1160/10000)
[Test]  Epoch: 17	Loss: 0.061859	Acc: 11.9% (1193/10000)
[Test]  Epoch: 18	Loss: 0.062012	Acc: 11.6% (1160/10000)
[Test]  Epoch: 19	Loss: 0.062594	Acc: 11.5% (1154/10000)
[Test]  Epoch: 20	Loss: 0.061600	Acc: 12.1% (1206/10000)
[Test]  Epoch: 21	Loss: 0.061585	Acc: 12.0% (1198/10000)
[Test]  Epoch: 22	Loss: 0.061447	Acc: 12.2% (1223/10000)
[Test]  Epoch: 23	Loss: 0.061423	Acc: 12.8% (1284/10000)
[Test]  Epoch: 24	Loss: 0.061843	Acc: 12.2% (1223/10000)
[Test]  Epoch: 25	Loss: 0.061446	Acc: 13.0% (1300/10000)
[Test]  Epoch: 26	Loss: 0.061081	Acc: 12.9% (1295/10000)
[Test]  Epoch: 27	Loss: 0.061524	Acc: 12.9% (1293/10000)
[Test]  Epoch: 28	Loss: 0.061666	Acc: 12.3% (1234/10000)
[Test]  Epoch: 29	Loss: 0.061810	Acc: 12.5% (1246/10000)
[Test]  Epoch: 30	Loss: 0.061626	Acc: 12.3% (1228/10000)
[Test]  Epoch: 31	Loss: 0.061767	Acc: 13.0% (1299/10000)
[Test]  Epoch: 32	Loss: 0.060976	Acc: 12.4% (1241/10000)
[Test]  Epoch: 33	Loss: 0.060836	Acc: 13.2% (1320/10000)
[Test]  Epoch: 34	Loss: 0.060849	Acc: 12.7% (1269/10000)
[Test]  Epoch: 35	Loss: 0.060603	Acc: 13.4% (1336/10000)
[Test]  Epoch: 36	Loss: 0.060921	Acc: 13.1% (1307/10000)
[Test]  Epoch: 37	Loss: 0.060167	Acc: 13.7% (1371/10000)
[Test]  Epoch: 38	Loss: 0.059935	Acc: 13.5% (1350/10000)
[Test]  Epoch: 39	Loss: 0.060546	Acc: 13.3% (1334/10000)
[Test]  Epoch: 40	Loss: 0.060552	Acc: 13.2% (1315/10000)
[Test]  Epoch: 41	Loss: 0.059992	Acc: 13.5% (1346/10000)
[Test]  Epoch: 42	Loss: 0.059988	Acc: 13.6% (1362/10000)
[Test]  Epoch: 43	Loss: 0.059870	Acc: 13.7% (1373/10000)
[Test]  Epoch: 44	Loss: 0.060302	Acc: 13.3% (1334/10000)
[Test]  Epoch: 45	Loss: 0.059937	Acc: 13.5% (1350/10000)
[Test]  Epoch: 46	Loss: 0.059915	Acc: 13.5% (1351/10000)
[Test]  Epoch: 47	Loss: 0.059735	Acc: 13.8% (1378/10000)
[Test]  Epoch: 48	Loss: 0.059581	Acc: 14.2% (1415/10000)
[Test]  Epoch: 49	Loss: 0.059728	Acc: 13.7% (1372/10000)
[Test]  Epoch: 50	Loss: 0.059936	Acc: 13.8% (1378/10000)
[Test]  Epoch: 51	Loss: 0.060500	Acc: 13.6% (1363/10000)
[Test]  Epoch: 52	Loss: 0.059594	Acc: 13.8% (1377/10000)
[Test]  Epoch: 53	Loss: 0.059868	Acc: 13.8% (1385/10000)
[Test]  Epoch: 54	Loss: 0.059867	Acc: 13.6% (1358/10000)
[Test]  Epoch: 55	Loss: 0.059998	Acc: 13.7% (1371/10000)
[Test]  Epoch: 56	Loss: 0.059574	Acc: 13.7% (1374/10000)
[Test]  Epoch: 57	Loss: 0.059418	Acc: 14.2% (1416/10000)
[Test]  Epoch: 58	Loss: 0.060526	Acc: 13.4% (1341/10000)
[Test]  Epoch: 59	Loss: 0.059684	Acc: 13.8% (1379/10000)
[Test]  Epoch: 60	Loss: 0.059623	Acc: 13.6% (1358/10000)
[Test]  Epoch: 61	Loss: 0.059138	Acc: 14.1% (1412/10000)
[Test]  Epoch: 62	Loss: 0.059140	Acc: 14.1% (1414/10000)
[Test]  Epoch: 63	Loss: 0.059042	Acc: 14.2% (1422/10000)
[Test]  Epoch: 64	Loss: 0.059021	Acc: 14.2% (1425/10000)
[Test]  Epoch: 65	Loss: 0.058915	Acc: 14.4% (1438/10000)
[Test]  Epoch: 66	Loss: 0.058845	Acc: 14.4% (1443/10000)
[Test]  Epoch: 67	Loss: 0.059021	Acc: 14.2% (1424/10000)
[Test]  Epoch: 68	Loss: 0.058943	Acc: 14.6% (1455/10000)
[Test]  Epoch: 69	Loss: 0.058912	Acc: 14.6% (1459/10000)
[Test]  Epoch: 70	Loss: 0.058972	Acc: 14.5% (1448/10000)
[Test]  Epoch: 71	Loss: 0.058808	Acc: 14.5% (1451/10000)
[Test]  Epoch: 72	Loss: 0.058815	Acc: 14.5% (1449/10000)
[Test]  Epoch: 73	Loss: 0.058923	Acc: 14.3% (1428/10000)
[Test]  Epoch: 74	Loss: 0.058703	Acc: 14.6% (1457/10000)
[Test]  Epoch: 75	Loss: 0.058771	Acc: 14.5% (1447/10000)
[Test]  Epoch: 76	Loss: 0.058836	Acc: 14.2% (1424/10000)
[Test]  Epoch: 77	Loss: 0.058775	Acc: 14.3% (1433/10000)
[Test]  Epoch: 78	Loss: 0.058924	Acc: 14.4% (1440/10000)
[Test]  Epoch: 79	Loss: 0.058907	Acc: 14.3% (1433/10000)
[Test]  Epoch: 80	Loss: 0.058892	Acc: 14.3% (1434/10000)
[Test]  Epoch: 81	Loss: 0.058738	Acc: 14.6% (1458/10000)
[Test]  Epoch: 82	Loss: 0.058802	Acc: 14.4% (1439/10000)
[Test]  Epoch: 83	Loss: 0.058814	Acc: 14.2% (1423/10000)
[Test]  Epoch: 84	Loss: 0.058808	Acc: 14.3% (1433/10000)
[Test]  Epoch: 85	Loss: 0.058772	Acc: 14.2% (1419/10000)
[Test]  Epoch: 86	Loss: 0.058875	Acc: 14.3% (1431/10000)
[Test]  Epoch: 87	Loss: 0.058834	Acc: 14.3% (1432/10000)
[Test]  Epoch: 88	Loss: 0.058959	Acc: 14.2% (1422/10000)
[Test]  Epoch: 89	Loss: 0.058888	Acc: 14.4% (1438/10000)
[Test]  Epoch: 90	Loss: 0.058851	Acc: 14.4% (1443/10000)
[Test]  Epoch: 91	Loss: 0.058814	Acc: 14.2% (1425/10000)
[Test]  Epoch: 92	Loss: 0.058843	Acc: 14.2% (1419/10000)
[Test]  Epoch: 93	Loss: 0.058807	Acc: 14.3% (1430/10000)
[Test]  Epoch: 94	Loss: 0.058678	Acc: 14.3% (1432/10000)
[Test]  Epoch: 95	Loss: 0.058872	Acc: 14.0% (1404/10000)
[Test]  Epoch: 96	Loss: 0.058718	Acc: 14.4% (1436/10000)
[Test]  Epoch: 97	Loss: 0.058839	Acc: 14.4% (1440/10000)
[Test]  Epoch: 98	Loss: 0.058800	Acc: 14.1% (1413/10000)
[Test]  Epoch: 99	Loss: 0.058868	Acc: 14.2% (1422/10000)
[Test]  Epoch: 100	Loss: 0.058952	Acc: 14.2% (1423/10000)
===========finish==========
['2024-08-19', '01:05:18.318937', '100', 'test', '0.058952098989486695', '14.23', '14.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight', '_features.4.conv.3.weight', '_features.3.conv.1.1.weight', '_features.0.1.weight', '_features.12.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.5.conv.1.1.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.16.conv.2.weight', '_features.2.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.4.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.0.0.weight', '_features.8.conv.3.weight', '_features.5.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.9.conv.1.1.weight', '_features.9.conv.3.weight', '_features.9.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.3.conv.2.weight', '_features.3.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.1.weight', '_features.10.conv.1.1.weight', '_features.5.conv.2.weight', '_features.18.1.weight', '_features.13.conv.2.weight', '_features.7.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.428474	Acc: 1.0% (101/10000)
[Test]  Epoch: 2	Loss: 0.102595	Acc: 2.9% (289/10000)
[Test]  Epoch: 3	Loss: 0.079957	Acc: 3.0% (299/10000)
[Test]  Epoch: 4	Loss: 0.090617	Acc: 2.1% (207/10000)
[Test]  Epoch: 5	Loss: 0.070047	Acc: 4.4% (443/10000)
[Test]  Epoch: 6	Loss: 0.068511	Acc: 5.4% (542/10000)
[Test]  Epoch: 7	Loss: 0.066237	Acc: 6.6% (656/10000)
[Test]  Epoch: 8	Loss: 0.067538	Acc: 5.7% (568/10000)
[Test]  Epoch: 9	Loss: 0.078296	Acc: 3.8% (379/10000)
[Test]  Epoch: 10	Loss: 0.066863	Acc: 5.5% (553/10000)
[Test]  Epoch: 11	Loss: 0.066001	Acc: 7.0% (703/10000)
[Test]  Epoch: 12	Loss: 0.066379	Acc: 6.8% (684/10000)
[Test]  Epoch: 13	Loss: 0.063686	Acc: 8.6% (861/10000)
[Test]  Epoch: 14	Loss: 0.064405	Acc: 8.3% (833/10000)
[Test]  Epoch: 15	Loss: 0.065243	Acc: 8.1% (805/10000)
[Test]  Epoch: 16	Loss: 0.064965	Acc: 8.2% (820/10000)
[Test]  Epoch: 17	Loss: 0.064270	Acc: 9.0% (899/10000)
[Test]  Epoch: 18	Loss: 0.064712	Acc: 9.0% (896/10000)
[Test]  Epoch: 19	Loss: 0.064214	Acc: 9.3% (926/10000)
[Test]  Epoch: 20	Loss: 0.063307	Acc: 9.5% (946/10000)
[Test]  Epoch: 21	Loss: 0.064756	Acc: 9.6% (963/10000)
[Test]  Epoch: 22	Loss: 0.065408	Acc: 8.4% (839/10000)
[Test]  Epoch: 23	Loss: 0.071032	Acc: 6.8% (679/10000)
[Test]  Epoch: 24	Loss: 0.063776	Acc: 9.9% (991/10000)
[Test]  Epoch: 25	Loss: 0.062052	Acc: 10.1% (1013/10000)
[Test]  Epoch: 26	Loss: 0.064130	Acc: 9.8% (978/10000)
[Test]  Epoch: 27	Loss: 0.062626	Acc: 10.4% (1044/10000)
[Test]  Epoch: 28	Loss: 0.064404	Acc: 10.6% (1063/10000)
[Test]  Epoch: 29	Loss: 0.062200	Acc: 10.8% (1081/10000)
[Test]  Epoch: 30	Loss: 0.063257	Acc: 10.6% (1057/10000)
[Test]  Epoch: 31	Loss: 0.069861	Acc: 6.5% (649/10000)
[Test]  Epoch: 32	Loss: 0.064017	Acc: 10.2% (1019/10000)
[Test]  Epoch: 33	Loss: 0.067512	Acc: 7.6% (760/10000)
[Test]  Epoch: 34	Loss: 0.071159	Acc: 7.9% (793/10000)
[Test]  Epoch: 35	Loss: 0.063198	Acc: 10.0% (1001/10000)
[Test]  Epoch: 36	Loss: 0.061238	Acc: 11.3% (1128/10000)
[Test]  Epoch: 37	Loss: 0.061701	Acc: 11.3% (1127/10000)
[Test]  Epoch: 38	Loss: 0.062017	Acc: 11.1% (1106/10000)
[Test]  Epoch: 39	Loss: 0.064538	Acc: 10.7% (1073/10000)
[Test]  Epoch: 40	Loss: 0.062262	Acc: 11.5% (1152/10000)
[Test]  Epoch: 41	Loss: 0.063007	Acc: 11.6% (1161/10000)
[Test]  Epoch: 42	Loss: 0.064954	Acc: 11.2% (1117/10000)
[Test]  Epoch: 43	Loss: 0.063462	Acc: 11.6% (1158/10000)
[Test]  Epoch: 44	Loss: 0.063621	Acc: 11.1% (1108/10000)
[Test]  Epoch: 45	Loss: 0.062280	Acc: 12.5% (1252/10000)
[Test]  Epoch: 46	Loss: 0.065920	Acc: 11.0% (1102/10000)
[Test]  Epoch: 47	Loss: 0.062730	Acc: 11.9% (1192/10000)
[Test]  Epoch: 48	Loss: 0.090880	Acc: 6.3% (633/10000)
[Test]  Epoch: 49	Loss: 0.065434	Acc: 8.7% (873/10000)
[Test]  Epoch: 50	Loss: 0.060920	Acc: 11.4% (1136/10000)
[Test]  Epoch: 51	Loss: 0.061783	Acc: 11.6% (1158/10000)
[Test]  Epoch: 52	Loss: 0.066979	Acc: 8.8% (883/10000)
[Test]  Epoch: 53	Loss: 0.064088	Acc: 10.3% (1030/10000)
[Test]  Epoch: 54	Loss: 0.062472	Acc: 11.8% (1177/10000)
[Test]  Epoch: 55	Loss: 0.061301	Acc: 12.0% (1204/10000)
[Test]  Epoch: 56	Loss: 0.062434	Acc: 11.6% (1157/10000)
[Test]  Epoch: 57	Loss: 0.060548	Acc: 13.2% (1323/10000)
[Test]  Epoch: 58	Loss: 0.065932	Acc: 9.0% (903/10000)
[Test]  Epoch: 59	Loss: 0.062084	Acc: 12.0% (1198/10000)
[Test]  Epoch: 60	Loss: 0.062289	Acc: 12.0% (1199/10000)
[Test]  Epoch: 61	Loss: 0.060324	Acc: 13.3% (1327/10000)
[Test]  Epoch: 62	Loss: 0.060441	Acc: 13.3% (1328/10000)
[Test]  Epoch: 63	Loss: 0.060064	Acc: 13.3% (1328/10000)
[Test]  Epoch: 64	Loss: 0.059879	Acc: 13.4% (1336/10000)
[Test]  Epoch: 65	Loss: 0.059768	Acc: 13.6% (1362/10000)
[Test]  Epoch: 66	Loss: 0.059761	Acc: 13.6% (1358/10000)
[Test]  Epoch: 67	Loss: 0.060021	Acc: 13.6% (1358/10000)
[Test]  Epoch: 68	Loss: 0.059951	Acc: 13.7% (1366/10000)
[Test]  Epoch: 69	Loss: 0.059948	Acc: 13.7% (1371/10000)
[Test]  Epoch: 70	Loss: 0.060025	Acc: 13.7% (1374/10000)
[Test]  Epoch: 71	Loss: 0.060181	Acc: 13.8% (1375/10000)
[Test]  Epoch: 72	Loss: 0.060299	Acc: 13.7% (1373/10000)
[Test]  Epoch: 73	Loss: 0.060343	Acc: 13.8% (1376/10000)
[Test]  Epoch: 74	Loss: 0.060182	Acc: 13.7% (1366/10000)
[Test]  Epoch: 75	Loss: 0.060189	Acc: 13.8% (1380/10000)
[Test]  Epoch: 76	Loss: 0.060088	Acc: 14.0% (1402/10000)
[Test]  Epoch: 77	Loss: 0.060233	Acc: 13.9% (1387/10000)
[Test]  Epoch: 78	Loss: 0.060270	Acc: 13.7% (1366/10000)
[Test]  Epoch: 79	Loss: 0.060366	Acc: 13.5% (1354/10000)
[Test]  Epoch: 80	Loss: 0.060381	Acc: 13.7% (1371/10000)
[Test]  Epoch: 81	Loss: 0.060200	Acc: 13.8% (1381/10000)
[Test]  Epoch: 82	Loss: 0.060131	Acc: 13.6% (1355/10000)
[Test]  Epoch: 83	Loss: 0.060296	Acc: 13.5% (1346/10000)
[Test]  Epoch: 84	Loss: 0.060624	Acc: 13.8% (1380/10000)
[Test]  Epoch: 85	Loss: 0.060442	Acc: 13.8% (1377/10000)
[Test]  Epoch: 86	Loss: 0.060358	Acc: 13.7% (1366/10000)
[Test]  Epoch: 87	Loss: 0.060435	Acc: 13.7% (1373/10000)
[Test]  Epoch: 88	Loss: 0.060587	Acc: 13.6% (1357/10000)
[Test]  Epoch: 89	Loss: 0.060983	Acc: 13.4% (1339/10000)
[Test]  Epoch: 90	Loss: 0.060639	Acc: 13.5% (1347/10000)
[Test]  Epoch: 91	Loss: 0.060485	Acc: 13.2% (1325/10000)
[Test]  Epoch: 92	Loss: 0.060685	Acc: 13.3% (1328/10000)
[Test]  Epoch: 93	Loss: 0.060555	Acc: 13.6% (1358/10000)
[Test]  Epoch: 94	Loss: 0.060489	Acc: 13.9% (1387/10000)
[Test]  Epoch: 95	Loss: 0.060691	Acc: 13.6% (1357/10000)
[Test]  Epoch: 96	Loss: 0.060707	Acc: 13.8% (1378/10000)
[Test]  Epoch: 97	Loss: 0.060867	Acc: 13.6% (1355/10000)
[Test]  Epoch: 98	Loss: 0.060729	Acc: 13.6% (1359/10000)
[Test]  Epoch: 99	Loss: 0.060584	Acc: 13.6% (1360/10000)
[Test]  Epoch: 100	Loss: 0.060657	Acc: 13.7% (1367/10000)
===========finish==========
['2024-08-19', '01:07:36.696157', '100', 'test', '0.06065741040706635', '13.67', '14.02']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight', '_features.4.conv.3.weight', '_features.3.conv.1.1.weight', '_features.0.1.weight', '_features.12.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.5.conv.1.1.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.16.conv.2.weight', '_features.2.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.4.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.0.0.weight', '_features.8.conv.3.weight', '_features.5.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.9.conv.1.1.weight', '_features.9.conv.3.weight', '_features.9.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.3.conv.2.weight', '_features.3.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.1.weight', '_features.10.conv.1.1.weight', '_features.5.conv.2.weight', '_features.18.1.weight', '_features.13.conv.2.weight', '_features.7.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.12.conv.2.weight', '_features.10.conv.0.1.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.8.conv.0.0.weight', '_features.10.conv.1.0.weight', '_features.9.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.0.0.weight', '_features.7.conv.2.weight', '_features.11.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.376912	Acc: 1.1% (108/10000)
[Test]  Epoch: 2	Loss: 0.078895	Acc: 5.0% (496/10000)
[Test]  Epoch: 3	Loss: 0.067568	Acc: 8.0% (797/10000)
[Test]  Epoch: 4	Loss: 0.064161	Acc: 9.2% (920/10000)
[Test]  Epoch: 5	Loss: 0.065282	Acc: 10.3% (1029/10000)
[Test]  Epoch: 6	Loss: 0.064268	Acc: 9.0% (896/10000)
[Test]  Epoch: 7	Loss: 0.060902	Acc: 11.1% (1112/10000)
[Test]  Epoch: 8	Loss: 0.062816	Acc: 11.6% (1156/10000)
[Test]  Epoch: 9	Loss: 0.062117	Acc: 11.6% (1156/10000)
[Test]  Epoch: 10	Loss: 0.059634	Acc: 12.7% (1274/10000)
[Test]  Epoch: 11	Loss: 0.060411	Acc: 13.0% (1297/10000)
[Test]  Epoch: 12	Loss: 0.059274	Acc: 13.3% (1331/10000)
[Test]  Epoch: 13	Loss: 0.060343	Acc: 12.2% (1221/10000)
[Test]  Epoch: 14	Loss: 0.060306	Acc: 13.7% (1368/10000)
[Test]  Epoch: 15	Loss: 0.058585	Acc: 15.3% (1527/10000)
[Test]  Epoch: 16	Loss: 0.063961	Acc: 12.3% (1233/10000)
[Test]  Epoch: 17	Loss: 0.058908	Acc: 14.2% (1418/10000)
[Test]  Epoch: 18	Loss: 0.057781	Acc: 14.9% (1491/10000)
[Test]  Epoch: 19	Loss: 0.058426	Acc: 15.2% (1521/10000)
[Test]  Epoch: 20	Loss: 0.057379	Acc: 15.5% (1546/10000)
[Test]  Epoch: 21	Loss: 0.073746	Acc: 11.2% (1115/10000)
[Test]  Epoch: 22	Loss: 0.059150	Acc: 15.1% (1513/10000)
[Test]  Epoch: 23	Loss: 0.059341	Acc: 15.2% (1525/10000)
[Test]  Epoch: 24	Loss: 0.059557	Acc: 15.4% (1541/10000)
[Test]  Epoch: 25	Loss: 0.058181	Acc: 15.2% (1525/10000)
[Test]  Epoch: 26	Loss: 0.057055	Acc: 15.8% (1577/10000)
[Test]  Epoch: 27	Loss: 0.057608	Acc: 16.0% (1602/10000)
[Test]  Epoch: 28	Loss: 0.056203	Acc: 16.4% (1639/10000)
[Test]  Epoch: 29	Loss: 0.058643	Acc: 15.9% (1595/10000)
[Test]  Epoch: 30	Loss: 0.060052	Acc: 14.3% (1429/10000)
[Test]  Epoch: 31	Loss: 0.057962	Acc: 16.0% (1604/10000)
[Test]  Epoch: 32	Loss: 0.056642	Acc: 16.2% (1619/10000)
[Test]  Epoch: 33	Loss: 0.055499	Acc: 17.0% (1702/10000)
[Test]  Epoch: 34	Loss: 0.058063	Acc: 16.2% (1623/10000)
[Test]  Epoch: 35	Loss: 0.057578	Acc: 16.4% (1636/10000)
[Test]  Epoch: 36	Loss: 0.055927	Acc: 17.7% (1767/10000)
[Test]  Epoch: 37	Loss: 0.073465	Acc: 8.5% (846/10000)
[Test]  Epoch: 38	Loss: 0.058356	Acc: 14.5% (1452/10000)
[Test]  Epoch: 39	Loss: 0.055645	Acc: 16.9% (1690/10000)
[Test]  Epoch: 40	Loss: 0.083769	Acc: 5.6% (561/10000)
[Test]  Epoch: 41	Loss: 0.059088	Acc: 13.9% (1391/10000)
[Test]  Epoch: 42	Loss: 0.057992	Acc: 14.5% (1448/10000)
[Test]  Epoch: 43	Loss: 0.058734	Acc: 14.0% (1401/10000)
[Test]  Epoch: 44	Loss: 0.063026	Acc: 14.1% (1411/10000)
[Test]  Epoch: 45	Loss: 0.056019	Acc: 16.8% (1677/10000)
[Test]  Epoch: 46	Loss: 0.056140	Acc: 16.8% (1681/10000)
[Test]  Epoch: 47	Loss: 0.059218	Acc: 14.8% (1485/10000)
[Test]  Epoch: 48	Loss: 0.058439	Acc: 16.2% (1621/10000)
[Test]  Epoch: 49	Loss: 0.059623	Acc: 16.2% (1616/10000)
[Test]  Epoch: 50	Loss: 0.056673	Acc: 17.3% (1732/10000)
[Test]  Epoch: 51	Loss: 0.058073	Acc: 17.0% (1696/10000)
[Test]  Epoch: 52	Loss: 0.058354	Acc: 15.4% (1541/10000)
[Test]  Epoch: 53	Loss: 0.060535	Acc: 14.0% (1400/10000)
[Test]  Epoch: 54	Loss: 0.056780	Acc: 16.6% (1663/10000)
[Test]  Epoch: 55	Loss: 0.057272	Acc: 17.8% (1782/10000)
[Test]  Epoch: 56	Loss: 0.054325	Acc: 18.9% (1893/10000)
[Test]  Epoch: 57	Loss: 0.054354	Acc: 18.7% (1872/10000)
[Test]  Epoch: 58	Loss: 0.054561	Acc: 18.9% (1892/10000)
[Test]  Epoch: 59	Loss: 0.063711	Acc: 14.9% (1489/10000)
[Test]  Epoch: 60	Loss: 0.054468	Acc: 18.4% (1844/10000)
[Test]  Epoch: 61	Loss: 0.053476	Acc: 19.2% (1925/10000)
[Test]  Epoch: 62	Loss: 0.053395	Acc: 19.3% (1930/10000)
[Test]  Epoch: 63	Loss: 0.053456	Acc: 19.3% (1934/10000)
[Test]  Epoch: 64	Loss: 0.053221	Acc: 19.3% (1931/10000)
[Test]  Epoch: 65	Loss: 0.053203	Acc: 19.2% (1923/10000)
[Test]  Epoch: 66	Loss: 0.053175	Acc: 19.3% (1929/10000)
[Test]  Epoch: 67	Loss: 0.053030	Acc: 19.4% (1942/10000)
[Test]  Epoch: 68	Loss: 0.053107	Acc: 19.6% (1955/10000)
[Test]  Epoch: 69	Loss: 0.053277	Acc: 19.3% (1930/10000)
[Test]  Epoch: 70	Loss: 0.053067	Acc: 19.4% (1936/10000)
[Test]  Epoch: 71	Loss: 0.052984	Acc: 19.7% (1966/10000)
[Test]  Epoch: 72	Loss: 0.052953	Acc: 19.6% (1964/10000)
[Test]  Epoch: 73	Loss: 0.052987	Acc: 19.8% (1983/10000)
[Test]  Epoch: 74	Loss: 0.052934	Acc: 19.7% (1973/10000)
[Test]  Epoch: 75	Loss: 0.052884	Acc: 19.9% (1988/10000)
[Test]  Epoch: 76	Loss: 0.052864	Acc: 19.8% (1977/10000)
[Test]  Epoch: 77	Loss: 0.052831	Acc: 20.0% (1997/10000)
[Test]  Epoch: 78	Loss: 0.052687	Acc: 19.9% (1986/10000)
[Test]  Epoch: 79	Loss: 0.052766	Acc: 19.8% (1975/10000)
[Test]  Epoch: 80	Loss: 0.052665	Acc: 19.9% (1990/10000)
[Test]  Epoch: 81	Loss: 0.052657	Acc: 20.1% (2006/10000)
[Test]  Epoch: 82	Loss: 0.052727	Acc: 19.9% (1991/10000)
[Test]  Epoch: 83	Loss: 0.052659	Acc: 20.3% (2027/10000)
[Test]  Epoch: 84	Loss: 0.052729	Acc: 20.3% (2028/10000)
[Test]  Epoch: 85	Loss: 0.052761	Acc: 20.2% (2016/10000)
[Test]  Epoch: 86	Loss: 0.052722	Acc: 19.9% (1993/10000)
[Test]  Epoch: 87	Loss: 0.052696	Acc: 20.1% (2005/10000)
[Test]  Epoch: 88	Loss: 0.052618	Acc: 20.2% (2017/10000)
[Test]  Epoch: 89	Loss: 0.052640	Acc: 20.1% (2009/10000)
[Test]  Epoch: 90	Loss: 0.052722	Acc: 20.0% (2002/10000)
[Test]  Epoch: 91	Loss: 0.052707	Acc: 20.1% (2015/10000)
[Test]  Epoch: 92	Loss: 0.052688	Acc: 20.2% (2021/10000)
[Test]  Epoch: 93	Loss: 0.052718	Acc: 20.0% (2003/10000)
[Test]  Epoch: 94	Loss: 0.052721	Acc: 20.4% (2038/10000)
[Test]  Epoch: 95	Loss: 0.052693	Acc: 20.1% (2009/10000)
[Test]  Epoch: 96	Loss: 0.052561	Acc: 20.2% (2019/10000)
[Test]  Epoch: 97	Loss: 0.052679	Acc: 20.2% (2016/10000)
[Test]  Epoch: 98	Loss: 0.052557	Acc: 20.4% (2037/10000)
[Test]  Epoch: 99	Loss: 0.052590	Acc: 20.2% (2021/10000)
[Test]  Epoch: 100	Loss: 0.052553	Acc: 20.2% (2023/10000)
===========finish==========
['2024-08-19', '01:09:57.181118', '100', 'test', '0.05255345561504364', '20.23', '20.38']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight', '_features.4.conv.3.weight', '_features.3.conv.1.1.weight', '_features.0.1.weight', '_features.12.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.5.conv.1.1.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.16.conv.2.weight', '_features.2.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.4.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.0.0.weight', '_features.8.conv.3.weight', '_features.5.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.9.conv.1.1.weight', '_features.9.conv.3.weight', '_features.9.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.3.conv.2.weight', '_features.3.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.1.weight', '_features.10.conv.1.1.weight', '_features.5.conv.2.weight', '_features.18.1.weight', '_features.13.conv.2.weight', '_features.7.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.12.conv.2.weight', '_features.10.conv.0.1.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.8.conv.0.0.weight', '_features.10.conv.1.0.weight', '_features.9.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.0.0.weight', '_features.7.conv.2.weight', '_features.11.conv.0.1.weight', '_features.14.conv.3.weight', '_features.8.conv.2.weight', '_features.11.conv.1.0.weight', '_features.10.conv.0.0.weight', '_features.10.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.11.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.474417	Acc: 1.5% (153/10000)
[Test]  Epoch: 2	Loss: 0.077217	Acc: 3.6% (361/10000)
[Test]  Epoch: 3	Loss: 0.068824	Acc: 5.6% (564/10000)
[Test]  Epoch: 4	Loss: 0.066088	Acc: 6.5% (653/10000)
[Test]  Epoch: 5	Loss: 0.065096	Acc: 7.1% (712/10000)
[Test]  Epoch: 6	Loss: 0.082817	Acc: 7.2% (722/10000)
[Test]  Epoch: 7	Loss: 0.064685	Acc: 8.9% (892/10000)
[Test]  Epoch: 8	Loss: 0.063634	Acc: 8.6% (862/10000)
[Test]  Epoch: 9	Loss: 0.255920	Acc: 1.9% (194/10000)
[Test]  Epoch: 10	Loss: 0.065211	Acc: 8.3% (827/10000)
[Test]  Epoch: 11	Loss: 0.065924	Acc: 6.6% (657/10000)
[Test]  Epoch: 12	Loss: 0.064584	Acc: 8.6% (857/10000)
[Test]  Epoch: 13	Loss: 0.064864	Acc: 8.1% (812/10000)
[Test]  Epoch: 14	Loss: 0.062315	Acc: 10.5% (1047/10000)
[Test]  Epoch: 15	Loss: 0.067764	Acc: 7.0% (703/10000)
[Test]  Epoch: 16	Loss: 0.062457	Acc: 9.6% (956/10000)
[Test]  Epoch: 17	Loss: 0.060414	Acc: 11.1% (1114/10000)
[Test]  Epoch: 18	Loss: 0.071790	Acc: 7.3% (734/10000)
[Test]  Epoch: 19	Loss: 0.064163	Acc: 9.7% (973/10000)
[Test]  Epoch: 20	Loss: 0.061960	Acc: 10.9% (1092/10000)
[Test]  Epoch: 21	Loss: 0.063449	Acc: 9.5% (951/10000)
[Test]  Epoch: 22	Loss: 0.063231	Acc: 11.4% (1136/10000)
[Test]  Epoch: 23	Loss: 0.061348	Acc: 11.5% (1150/10000)
[Test]  Epoch: 24	Loss: 0.062561	Acc: 11.5% (1153/10000)
[Test]  Epoch: 25	Loss: 0.064070	Acc: 11.3% (1132/10000)
[Test]  Epoch: 26	Loss: 0.064348	Acc: 11.2% (1123/10000)
[Test]  Epoch: 27	Loss: 0.061926	Acc: 11.6% (1155/10000)
[Test]  Epoch: 28	Loss: 0.062257	Acc: 12.1% (1212/10000)
[Test]  Epoch: 29	Loss: 0.062944	Acc: 11.7% (1172/10000)
[Test]  Epoch: 30	Loss: 0.061698	Acc: 12.0% (1202/10000)
[Test]  Epoch: 31	Loss: 0.061805	Acc: 12.3% (1226/10000)
[Test]  Epoch: 32	Loss: 0.065305	Acc: 11.1% (1109/10000)
[Test]  Epoch: 33	Loss: 0.064008	Acc: 11.6% (1161/10000)
[Test]  Epoch: 34	Loss: 0.062001	Acc: 12.0% (1199/10000)
[Test]  Epoch: 35	Loss: 0.062474	Acc: 12.6% (1264/10000)
[Test]  Epoch: 36	Loss: 0.061853	Acc: 12.5% (1246/10000)
[Test]  Epoch: 37	Loss: 0.063771	Acc: 11.8% (1179/10000)
[Test]  Epoch: 38	Loss: 0.062417	Acc: 11.9% (1194/10000)
[Test]  Epoch: 39	Loss: 0.061757	Acc: 12.2% (1225/10000)
[Test]  Epoch: 40	Loss: 0.062208	Acc: 12.1% (1214/10000)
[Test]  Epoch: 41	Loss: 0.061654	Acc: 12.3% (1228/10000)
[Test]  Epoch: 42	Loss: 0.061157	Acc: 12.6% (1258/10000)
[Test]  Epoch: 43	Loss: 0.061387	Acc: 12.6% (1256/10000)
[Test]  Epoch: 44	Loss: 0.061294	Acc: 12.8% (1276/10000)
[Test]  Epoch: 45	Loss: 0.062837	Acc: 12.7% (1269/10000)
[Test]  Epoch: 46	Loss: 0.061215	Acc: 12.9% (1294/10000)
[Test]  Epoch: 47	Loss: 0.061470	Acc: 12.8% (1283/10000)
[Test]  Epoch: 48	Loss: 0.061903	Acc: 12.6% (1260/10000)
[Test]  Epoch: 49	Loss: 0.061538	Acc: 12.4% (1244/10000)
[Test]  Epoch: 50	Loss: 0.061123	Acc: 13.0% (1300/10000)
[Test]  Epoch: 51	Loss: 0.061966	Acc: 12.9% (1292/10000)
[Test]  Epoch: 52	Loss: 0.061217	Acc: 12.5% (1249/10000)
[Test]  Epoch: 53	Loss: 0.060970	Acc: 12.8% (1276/10000)
[Test]  Epoch: 54	Loss: 0.061332	Acc: 13.2% (1325/10000)
[Test]  Epoch: 55	Loss: 0.072072	Acc: 7.1% (708/10000)
[Test]  Epoch: 56	Loss: 0.062545	Acc: 12.4% (1243/10000)
[Test]  Epoch: 57	Loss: 0.061133	Acc: 13.2% (1324/10000)
[Test]  Epoch: 58	Loss: 0.062785	Acc: 13.0% (1299/10000)
[Test]  Epoch: 59	Loss: 0.061488	Acc: 13.3% (1326/10000)
[Test]  Epoch: 60	Loss: 0.061428	Acc: 12.8% (1283/10000)
[Test]  Epoch: 61	Loss: 0.060599	Acc: 13.3% (1327/10000)
[Test]  Epoch: 62	Loss: 0.060459	Acc: 13.5% (1350/10000)
[Test]  Epoch: 63	Loss: 0.060283	Acc: 13.7% (1374/10000)
[Test]  Epoch: 64	Loss: 0.060273	Acc: 13.6% (1363/10000)
[Test]  Epoch: 65	Loss: 0.060266	Acc: 13.6% (1361/10000)
[Test]  Epoch: 66	Loss: 0.060236	Acc: 13.6% (1360/10000)
[Test]  Epoch: 67	Loss: 0.060526	Acc: 13.3% (1327/10000)
[Test]  Epoch: 68	Loss: 0.060297	Acc: 13.7% (1370/10000)
[Test]  Epoch: 69	Loss: 0.060221	Acc: 13.5% (1354/10000)
[Test]  Epoch: 70	Loss: 0.060278	Acc: 13.5% (1352/10000)
[Test]  Epoch: 71	Loss: 0.060242	Acc: 13.5% (1353/10000)
[Test]  Epoch: 72	Loss: 0.060150	Acc: 13.5% (1352/10000)
[Test]  Epoch: 73	Loss: 0.060253	Acc: 13.5% (1349/10000)
[Test]  Epoch: 74	Loss: 0.060107	Acc: 13.5% (1350/10000)
[Test]  Epoch: 75	Loss: 0.060188	Acc: 13.6% (1364/10000)
[Test]  Epoch: 76	Loss: 0.060261	Acc: 13.5% (1353/10000)
[Test]  Epoch: 77	Loss: 0.060116	Acc: 13.8% (1378/10000)
[Test]  Epoch: 78	Loss: 0.060268	Acc: 13.8% (1385/10000)
[Test]  Epoch: 79	Loss: 0.060385	Acc: 13.7% (1365/10000)
[Test]  Epoch: 80	Loss: 0.060192	Acc: 13.7% (1370/10000)
[Test]  Epoch: 81	Loss: 0.060125	Acc: 13.9% (1390/10000)
[Test]  Epoch: 82	Loss: 0.060050	Acc: 13.8% (1380/10000)
[Test]  Epoch: 83	Loss: 0.060044	Acc: 13.9% (1392/10000)
[Test]  Epoch: 84	Loss: 0.060273	Acc: 13.9% (1390/10000)
[Test]  Epoch: 85	Loss: 0.060358	Acc: 13.6% (1364/10000)
[Test]  Epoch: 86	Loss: 0.060352	Acc: 13.8% (1375/10000)
[Test]  Epoch: 87	Loss: 0.060390	Acc: 13.6% (1363/10000)
[Test]  Epoch: 88	Loss: 0.060199	Acc: 13.7% (1371/10000)
[Test]  Epoch: 89	Loss: 0.060218	Acc: 13.7% (1371/10000)
[Test]  Epoch: 90	Loss: 0.060181	Acc: 13.8% (1381/10000)
[Test]  Epoch: 91	Loss: 0.060138	Acc: 13.8% (1375/10000)
[Test]  Epoch: 92	Loss: 0.060060	Acc: 14.0% (1400/10000)
[Test]  Epoch: 93	Loss: 0.060124	Acc: 13.8% (1382/10000)
[Test]  Epoch: 94	Loss: 0.060088	Acc: 13.9% (1395/10000)
[Test]  Epoch: 95	Loss: 0.060306	Acc: 13.9% (1386/10000)
[Test]  Epoch: 96	Loss: 0.060252	Acc: 13.8% (1378/10000)
[Test]  Epoch: 97	Loss: 0.060124	Acc: 13.9% (1389/10000)
[Test]  Epoch: 98	Loss: 0.060005	Acc: 13.9% (1386/10000)
[Test]  Epoch: 99	Loss: 0.060123	Acc: 13.7% (1372/10000)
[Test]  Epoch: 100	Loss: 0.060204	Acc: 13.9% (1392/10000)
===========finish==========
['2024-08-19', '01:12:15.508888', '100', 'test', '0.06020422613620758', '13.92', '14.0']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.15.conv.3.weight', '_features.3.conv.3.weight', '_features.12.conv.0.0.weight', '_features.16.conv.3.weight', '_features.1.conv.2.weight', '_features.15.conv.2.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.13.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.6.conv.3.weight', '_features.2.conv.1.1.weight', '_features.4.conv.3.weight', '_features.3.conv.1.1.weight', '_features.0.1.weight', '_features.12.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.5.conv.1.1.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.16.conv.2.weight', '_features.2.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.4.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.0.0.weight', '_features.8.conv.3.weight', '_features.5.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.9.conv.1.1.weight', '_features.9.conv.3.weight', '_features.9.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.3.conv.2.weight', '_features.3.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.1.weight', '_features.10.conv.1.1.weight', '_features.5.conv.2.weight', '_features.18.1.weight', '_features.13.conv.2.weight', '_features.7.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.12.conv.2.weight', '_features.10.conv.0.1.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.8.conv.0.0.weight', '_features.10.conv.1.0.weight', '_features.9.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.0.0.weight', '_features.7.conv.2.weight', '_features.11.conv.0.1.weight', '_features.14.conv.3.weight', '_features.8.conv.2.weight', '_features.11.conv.1.0.weight', '_features.10.conv.0.0.weight', '_features.10.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.11.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', 'last_linear.weight', '_features.18.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.088064	Acc: 1.5% (146/10000)
[Test]  Epoch: 2	Loss: 0.068769	Acc: 5.5% (545/10000)
[Test]  Epoch: 3	Loss: 0.065789	Acc: 6.9% (687/10000)
[Test]  Epoch: 4	Loss: 0.063656	Acc: 9.0% (898/10000)
[Test]  Epoch: 5	Loss: 0.062090	Acc: 9.8% (985/10000)
[Test]  Epoch: 6	Loss: 0.061237	Acc: 10.7% (1071/10000)
[Test]  Epoch: 7	Loss: 0.059509	Acc: 11.6% (1163/10000)
[Test]  Epoch: 8	Loss: 0.058829	Acc: 12.1% (1207/10000)
[Test]  Epoch: 9	Loss: 0.058234	Acc: 13.4% (1343/10000)
[Test]  Epoch: 10	Loss: 0.057525	Acc: 13.7% (1371/10000)
[Test]  Epoch: 11	Loss: 0.057172	Acc: 13.7% (1367/10000)
[Test]  Epoch: 12	Loss: 0.056392	Acc: 14.5% (1452/10000)
[Test]  Epoch: 13	Loss: 0.056851	Acc: 14.3% (1428/10000)
[Test]  Epoch: 14	Loss: 0.056296	Acc: 14.4% (1445/10000)
[Test]  Epoch: 15	Loss: 0.055846	Acc: 15.3% (1530/10000)
[Test]  Epoch: 16	Loss: 0.055579	Acc: 15.2% (1522/10000)
[Test]  Epoch: 17	Loss: 0.055526	Acc: 15.9% (1588/10000)
[Test]  Epoch: 18	Loss: 0.055653	Acc: 15.4% (1543/10000)
[Test]  Epoch: 19	Loss: 0.055342	Acc: 15.3% (1534/10000)
[Test]  Epoch: 20	Loss: 0.055251	Acc: 15.5% (1554/10000)
[Test]  Epoch: 21	Loss: 0.055107	Acc: 15.9% (1590/10000)
[Test]  Epoch: 22	Loss: 0.054960	Acc: 16.1% (1606/10000)
[Test]  Epoch: 23	Loss: 0.054710	Acc: 16.1% (1605/10000)
[Test]  Epoch: 24	Loss: 0.054789	Acc: 16.4% (1643/10000)
[Test]  Epoch: 25	Loss: 0.054638	Acc: 16.3% (1631/10000)
[Test]  Epoch: 26	Loss: 0.054469	Acc: 16.3% (1632/10000)
[Test]  Epoch: 27	Loss: 0.054617	Acc: 16.4% (1641/10000)
[Test]  Epoch: 28	Loss: 0.054898	Acc: 16.4% (1635/10000)
[Test]  Epoch: 29	Loss: 0.054402	Acc: 16.7% (1670/10000)
[Test]  Epoch: 30	Loss: 0.055428	Acc: 15.4% (1540/10000)
[Test]  Epoch: 31	Loss: 0.054156	Acc: 17.2% (1723/10000)
[Test]  Epoch: 32	Loss: 0.054135	Acc: 17.2% (1717/10000)
[Test]  Epoch: 33	Loss: 0.054484	Acc: 16.9% (1687/10000)
[Test]  Epoch: 34	Loss: 0.054149	Acc: 17.1% (1711/10000)
[Test]  Epoch: 35	Loss: 0.053983	Acc: 17.2% (1720/10000)
[Test]  Epoch: 36	Loss: 0.054109	Acc: 17.0% (1699/10000)
[Test]  Epoch: 37	Loss: 0.053604	Acc: 17.0% (1702/10000)
[Test]  Epoch: 38	Loss: 0.053675	Acc: 17.3% (1726/10000)
[Test]  Epoch: 39	Loss: 0.053535	Acc: 17.3% (1729/10000)
[Test]  Epoch: 40	Loss: 0.053575	Acc: 17.5% (1749/10000)
[Test]  Epoch: 41	Loss: 0.053599	Acc: 17.3% (1729/10000)
[Test]  Epoch: 42	Loss: 0.053653	Acc: 17.2% (1718/10000)
[Test]  Epoch: 43	Loss: 0.053621	Acc: 17.3% (1729/10000)
[Test]  Epoch: 44	Loss: 0.053398	Acc: 17.4% (1740/10000)
[Test]  Epoch: 45	Loss: 0.053686	Acc: 17.0% (1699/10000)
[Test]  Epoch: 46	Loss: 0.053408	Acc: 17.5% (1751/10000)
[Test]  Epoch: 47	Loss: 0.053333	Acc: 17.6% (1763/10000)
[Test]  Epoch: 48	Loss: 0.053211	Acc: 17.5% (1750/10000)
[Test]  Epoch: 49	Loss: 0.052988	Acc: 17.6% (1759/10000)
[Test]  Epoch: 50	Loss: 0.053086	Acc: 17.7% (1774/10000)
[Test]  Epoch: 51	Loss: 0.053114	Acc: 18.1% (1809/10000)
[Test]  Epoch: 52	Loss: 0.053212	Acc: 17.6% (1765/10000)
[Test]  Epoch: 53	Loss: 0.053200	Acc: 18.0% (1796/10000)
[Test]  Epoch: 54	Loss: 0.054593	Acc: 16.3% (1629/10000)
[Test]  Epoch: 55	Loss: 0.053118	Acc: 17.8% (1782/10000)
[Test]  Epoch: 56	Loss: 0.053537	Acc: 17.6% (1756/10000)
[Test]  Epoch: 57	Loss: 0.053097	Acc: 17.6% (1759/10000)
[Test]  Epoch: 58	Loss: 0.052816	Acc: 17.9% (1787/10000)
[Test]  Epoch: 59	Loss: 0.052845	Acc: 18.0% (1802/10000)
[Test]  Epoch: 60	Loss: 0.052891	Acc: 17.9% (1795/10000)
[Test]  Epoch: 61	Loss: 0.052867	Acc: 17.8% (1783/10000)
[Test]  Epoch: 62	Loss: 0.052751	Acc: 18.0% (1798/10000)
[Test]  Epoch: 63	Loss: 0.052751	Acc: 18.1% (1810/10000)
[Test]  Epoch: 64	Loss: 0.052687	Acc: 18.2% (1820/10000)
[Test]  Epoch: 65	Loss: 0.052539	Acc: 18.2% (1821/10000)
[Test]  Epoch: 66	Loss: 0.052537	Acc: 18.1% (1815/10000)
[Test]  Epoch: 67	Loss: 0.052616	Acc: 18.2% (1816/10000)
[Test]  Epoch: 68	Loss: 0.052628	Acc: 18.2% (1818/10000)
[Test]  Epoch: 69	Loss: 0.052552	Acc: 18.4% (1839/10000)
[Test]  Epoch: 70	Loss: 0.052629	Acc: 18.5% (1846/10000)
[Test]  Epoch: 71	Loss: 0.052534	Acc: 18.5% (1851/10000)
[Test]  Epoch: 72	Loss: 0.052650	Acc: 18.3% (1826/10000)
[Test]  Epoch: 73	Loss: 0.052510	Acc: 18.5% (1848/10000)
[Test]  Epoch: 74	Loss: 0.052463	Acc: 18.5% (1847/10000)
[Test]  Epoch: 75	Loss: 0.052457	Acc: 18.6% (1857/10000)
[Test]  Epoch: 76	Loss: 0.052501	Acc: 18.4% (1839/10000)
[Test]  Epoch: 77	Loss: 0.052494	Acc: 18.4% (1836/10000)
[Test]  Epoch: 78	Loss: 0.052515	Acc: 18.5% (1846/10000)
[Test]  Epoch: 79	Loss: 0.052576	Acc: 18.4% (1836/10000)
[Test]  Epoch: 80	Loss: 0.052545	Acc: 18.3% (1830/10000)
[Test]  Epoch: 81	Loss: 0.052406	Acc: 18.5% (1852/10000)
[Test]  Epoch: 82	Loss: 0.052386	Acc: 18.6% (1855/10000)
[Test]  Epoch: 83	Loss: 0.052424	Acc: 18.6% (1863/10000)
[Test]  Epoch: 84	Loss: 0.052471	Acc: 18.4% (1840/10000)
[Test]  Epoch: 85	Loss: 0.052442	Acc: 18.3% (1834/10000)
[Test]  Epoch: 86	Loss: 0.052542	Acc: 18.3% (1834/10000)
[Test]  Epoch: 87	Loss: 0.052484	Acc: 18.5% (1854/10000)
[Test]  Epoch: 88	Loss: 0.052424	Acc: 18.5% (1849/10000)
[Test]  Epoch: 89	Loss: 0.052494	Acc: 18.2% (1820/10000)
[Test]  Epoch: 90	Loss: 0.052532	Acc: 18.4% (1843/10000)
[Test]  Epoch: 91	Loss: 0.052572	Acc: 18.2% (1824/10000)
[Test]  Epoch: 92	Loss: 0.052498	Acc: 18.3% (1827/10000)
[Test]  Epoch: 93	Loss: 0.052514	Acc: 18.2% (1825/10000)
[Test]  Epoch: 94	Loss: 0.052480	Acc: 18.4% (1842/10000)
[Test]  Epoch: 95	Loss: 0.052536	Acc: 18.5% (1853/10000)
[Test]  Epoch: 96	Loss: 0.052464	Acc: 18.4% (1837/10000)
[Test]  Epoch: 97	Loss: 0.052496	Acc: 18.4% (1843/10000)
[Test]  Epoch: 98	Loss: 0.052445	Acc: 18.5% (1850/10000)
[Test]  Epoch: 99	Loss: 0.052465	Acc: 18.4% (1838/10000)
[Test]  Epoch: 100	Loss: 0.052564	Acc: 18.3% (1831/10000)
===========finish==========
['2024-08-19', '01:14:30.438091', '100', 'test', '0.052564205574989316', '18.31', '18.63']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.005735	Acc: 87.7% (8765/10000)
[Test]  Epoch: 2	Loss: 0.005697	Acc: 87.8% (8781/10000)
[Test]  Epoch: 3	Loss: 0.005521	Acc: 88.3% (8827/10000)
[Test]  Epoch: 4	Loss: 0.005555	Acc: 88.1% (8813/10000)
[Test]  Epoch: 5	Loss: 0.005538	Acc: 88.1% (8810/10000)
[Test]  Epoch: 6	Loss: 0.005409	Acc: 88.5% (8855/10000)
[Test]  Epoch: 7	Loss: 0.005466	Acc: 88.4% (8842/10000)
[Test]  Epoch: 8	Loss: 0.005392	Acc: 88.8% (8878/10000)
[Test]  Epoch: 9	Loss: 0.005350	Acc: 88.6% (8863/10000)
[Test]  Epoch: 10	Loss: 0.005435	Acc: 88.6% (8861/10000)
[Test]  Epoch: 11	Loss: 0.005518	Acc: 88.3% (8827/10000)
[Test]  Epoch: 12	Loss: 0.005338	Acc: 88.9% (8889/10000)
[Test]  Epoch: 13	Loss: 0.005630	Acc: 88.2% (8819/10000)
[Test]  Epoch: 14	Loss: 0.005469	Acc: 88.7% (8868/10000)
[Test]  Epoch: 15	Loss: 0.005476	Acc: 88.5% (8855/10000)
[Test]  Epoch: 16	Loss: 0.005513	Acc: 88.7% (8873/10000)
[Test]  Epoch: 17	Loss: 0.005491	Acc: 88.7% (8865/10000)
[Test]  Epoch: 18	Loss: 0.005401	Acc: 89.0% (8896/10000)
[Test]  Epoch: 19	Loss: 0.005603	Acc: 88.8% (8877/10000)
[Test]  Epoch: 20	Loss: 0.005478	Acc: 89.1% (8908/10000)
[Test]  Epoch: 21	Loss: 0.005581	Acc: 88.8% (8882/10000)
[Test]  Epoch: 22	Loss: 0.005530	Acc: 89.0% (8897/10000)
[Test]  Epoch: 23	Loss: 0.005632	Acc: 88.7% (8866/10000)
[Test]  Epoch: 24	Loss: 0.005692	Acc: 88.6% (8864/10000)
[Test]  Epoch: 25	Loss: 0.005724	Acc: 88.6% (8858/10000)
[Test]  Epoch: 26	Loss: 0.005636	Acc: 89.1% (8913/10000)
[Test]  Epoch: 27	Loss: 0.005718	Acc: 88.7% (8866/10000)
[Test]  Epoch: 28	Loss: 0.005709	Acc: 88.9% (8889/10000)
[Test]  Epoch: 29	Loss: 0.005840	Acc: 88.6% (8857/10000)
[Test]  Epoch: 30	Loss: 0.005679	Acc: 88.9% (8886/10000)
[Test]  Epoch: 31	Loss: 0.005612	Acc: 88.9% (8892/10000)
[Test]  Epoch: 32	Loss: 0.005726	Acc: 88.6% (8864/10000)
[Test]  Epoch: 33	Loss: 0.005757	Acc: 88.6% (8860/10000)
[Test]  Epoch: 34	Loss: 0.005951	Acc: 88.3% (8829/10000)
[Test]  Epoch: 35	Loss: 0.005816	Acc: 88.9% (8887/10000)
[Test]  Epoch: 36	Loss: 0.005815	Acc: 88.4% (8844/10000)
[Test]  Epoch: 37	Loss: 0.005942	Acc: 88.3% (8835/10000)
[Test]  Epoch: 38	Loss: 0.005880	Acc: 88.6% (8863/10000)
[Test]  Epoch: 39	Loss: 0.005895	Acc: 88.5% (8850/10000)
[Test]  Epoch: 40	Loss: 0.005906	Acc: 88.5% (8854/10000)
[Test]  Epoch: 41	Loss: 0.005933	Acc: 88.4% (8844/10000)
[Test]  Epoch: 42	Loss: 0.005919	Acc: 88.6% (8857/10000)
[Test]  Epoch: 43	Loss: 0.005890	Acc: 88.8% (8878/10000)
[Test]  Epoch: 44	Loss: 0.006027	Acc: 88.9% (8888/10000)
[Test]  Epoch: 45	Loss: 0.005816	Acc: 88.6% (8859/10000)
[Test]  Epoch: 46	Loss: 0.006025	Acc: 88.4% (8838/10000)
[Test]  Epoch: 47	Loss: 0.006193	Acc: 88.4% (8843/10000)
[Test]  Epoch: 48	Loss: 0.006049	Acc: 88.6% (8864/10000)
[Test]  Epoch: 49	Loss: 0.006015	Acc: 88.7% (8868/10000)
[Test]  Epoch: 50	Loss: 0.006036	Acc: 88.6% (8856/10000)
[Test]  Epoch: 51	Loss: 0.005931	Acc: 88.9% (8886/10000)
[Test]  Epoch: 52	Loss: 0.005983	Acc: 88.7% (8869/10000)
[Test]  Epoch: 53	Loss: 0.005988	Acc: 88.4% (8843/10000)
[Test]  Epoch: 54	Loss: 0.006150	Acc: 88.6% (8859/10000)
[Test]  Epoch: 55	Loss: 0.006107	Acc: 88.6% (8864/10000)
[Test]  Epoch: 56	Loss: 0.006061	Acc: 88.3% (8828/10000)
[Test]  Epoch: 57	Loss: 0.006062	Acc: 88.4% (8840/10000)
[Test]  Epoch: 58	Loss: 0.006167	Acc: 88.2% (8823/10000)
[Test]  Epoch: 59	Loss: 0.006223	Acc: 88.4% (8837/10000)
[Test]  Epoch: 60	Loss: 0.006221	Acc: 88.6% (8861/10000)
[Test]  Epoch: 61	Loss: 0.005986	Acc: 88.7% (8870/10000)
[Test]  Epoch: 62	Loss: 0.006211	Acc: 88.5% (8852/10000)
[Test]  Epoch: 63	Loss: 0.006067	Acc: 88.9% (8891/10000)
[Test]  Epoch: 64	Loss: 0.006210	Acc: 88.3% (8831/10000)
[Test]  Epoch: 65	Loss: 0.006008	Acc: 88.8% (8883/10000)
[Test]  Epoch: 66	Loss: 0.006096	Acc: 88.5% (8848/10000)
[Test]  Epoch: 67	Loss: 0.006099	Acc: 88.7% (8868/10000)
[Test]  Epoch: 68	Loss: 0.006034	Acc: 88.7% (8869/10000)
[Test]  Epoch: 69	Loss: 0.005973	Acc: 88.9% (8891/10000)
[Test]  Epoch: 70	Loss: 0.006139	Acc: 88.9% (8894/10000)
[Test]  Epoch: 71	Loss: 0.006116	Acc: 88.5% (8848/10000)
[Test]  Epoch: 72	Loss: 0.006140	Acc: 88.5% (8854/10000)
[Test]  Epoch: 73	Loss: 0.006179	Acc: 88.7% (8872/10000)
[Test]  Epoch: 74	Loss: 0.006222	Acc: 88.4% (8842/10000)
[Test]  Epoch: 75	Loss: 0.006083	Acc: 88.7% (8873/10000)
[Test]  Epoch: 76	Loss: 0.006090	Acc: 88.4% (8843/10000)
[Test]  Epoch: 77	Loss: 0.006311	Acc: 88.5% (8852/10000)
[Test]  Epoch: 78	Loss: 0.006109	Acc: 88.8% (8883/10000)
[Test]  Epoch: 79	Loss: 0.006140	Acc: 88.6% (8856/10000)
[Test]  Epoch: 80	Loss: 0.006197	Acc: 88.4% (8844/10000)
[Test]  Epoch: 81	Loss: 0.006266	Acc: 88.6% (8863/10000)
[Test]  Epoch: 82	Loss: 0.006088	Acc: 88.4% (8838/10000)
[Test]  Epoch: 83	Loss: 0.006108	Acc: 88.8% (8877/10000)
[Test]  Epoch: 84	Loss: 0.006189	Acc: 88.8% (8883/10000)
[Test]  Epoch: 85	Loss: 0.006157	Acc: 88.6% (8863/10000)
[Test]  Epoch: 86	Loss: 0.006183	Acc: 88.4% (8842/10000)
[Test]  Epoch: 87	Loss: 0.006214	Acc: 88.5% (8849/10000)
[Test]  Epoch: 88	Loss: 0.006270	Acc: 88.5% (8846/10000)
[Test]  Epoch: 89	Loss: 0.006105	Acc: 88.7% (8874/10000)
[Test]  Epoch: 90	Loss: 0.006116	Acc: 88.8% (8880/10000)
[Test]  Epoch: 91	Loss: 0.006134	Acc: 88.9% (8890/10000)
[Test]  Epoch: 92	Loss: 0.006147	Acc: 88.6% (8860/10000)
[Test]  Epoch: 93	Loss: 0.006050	Acc: 88.6% (8859/10000)
[Test]  Epoch: 94	Loss: 0.006052	Acc: 88.8% (8877/10000)
[Test]  Epoch: 95	Loss: 0.006119	Acc: 88.7% (8874/10000)
[Test]  Epoch: 96	Loss: 0.005985	Acc: 88.7% (8872/10000)
[Test]  Epoch: 97	Loss: 0.006089	Acc: 88.6% (8864/10000)
[Test]  Epoch: 98	Loss: 0.006265	Acc: 88.7% (8873/10000)
[Test]  Epoch: 99	Loss: 0.006200	Acc: 88.7% (8871/10000)
[Test]  Epoch: 100	Loss: 0.006113	Acc: 88.7% (8869/10000)
===========finish==========
['2024-08-19', '01:16:38.265508', '100', 'test', '0.006112907496839762', '88.69', '89.13']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.4.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.006809	Acc: 85.8% (8578/10000)
[Test]  Epoch: 2	Loss: 0.005964	Acc: 87.5% (8750/10000)
[Test]  Epoch: 3	Loss: 0.005721	Acc: 87.8% (8785/10000)
[Test]  Epoch: 4	Loss: 0.005817	Acc: 87.8% (8784/10000)
[Test]  Epoch: 5	Loss: 0.005750	Acc: 87.7% (8774/10000)
[Test]  Epoch: 6	Loss: 0.005585	Acc: 88.2% (8817/10000)
[Test]  Epoch: 7	Loss: 0.005604	Acc: 88.2% (8818/10000)
[Test]  Epoch: 8	Loss: 0.005607	Acc: 88.2% (8824/10000)
[Test]  Epoch: 9	Loss: 0.005535	Acc: 88.4% (8838/10000)
[Test]  Epoch: 10	Loss: 0.005621	Acc: 88.2% (8816/10000)
[Test]  Epoch: 11	Loss: 0.005697	Acc: 88.0% (8798/10000)
[Test]  Epoch: 12	Loss: 0.005554	Acc: 88.2% (8818/10000)
[Test]  Epoch: 13	Loss: 0.005773	Acc: 87.9% (8787/10000)
[Test]  Epoch: 14	Loss: 0.005583	Acc: 88.3% (8828/10000)
[Test]  Epoch: 15	Loss: 0.005631	Acc: 88.2% (8822/10000)
[Test]  Epoch: 16	Loss: 0.005686	Acc: 88.2% (8825/10000)
[Test]  Epoch: 17	Loss: 0.005631	Acc: 88.2% (8823/10000)
[Test]  Epoch: 18	Loss: 0.005557	Acc: 88.5% (8851/10000)
[Test]  Epoch: 19	Loss: 0.005749	Acc: 88.5% (8851/10000)
[Test]  Epoch: 20	Loss: 0.005639	Acc: 88.8% (8879/10000)
[Test]  Epoch: 21	Loss: 0.005769	Acc: 88.3% (8832/10000)
[Test]  Epoch: 22	Loss: 0.005723	Acc: 88.8% (8877/10000)
[Test]  Epoch: 23	Loss: 0.005841	Acc: 88.3% (8827/10000)
[Test]  Epoch: 24	Loss: 0.005820	Acc: 88.5% (8848/10000)
[Test]  Epoch: 25	Loss: 0.005874	Acc: 88.2% (8816/10000)
[Test]  Epoch: 26	Loss: 0.005805	Acc: 88.5% (8854/10000)
[Test]  Epoch: 27	Loss: 0.005851	Acc: 88.5% (8854/10000)
[Test]  Epoch: 28	Loss: 0.005907	Acc: 88.4% (8841/10000)
[Test]  Epoch: 29	Loss: 0.006044	Acc: 88.3% (8826/10000)
[Test]  Epoch: 30	Loss: 0.005907	Acc: 88.3% (8832/10000)
[Test]  Epoch: 31	Loss: 0.005846	Acc: 88.3% (8828/10000)
[Test]  Epoch: 32	Loss: 0.005936	Acc: 88.3% (8829/10000)
[Test]  Epoch: 33	Loss: 0.005962	Acc: 88.3% (8833/10000)
[Test]  Epoch: 34	Loss: 0.006128	Acc: 88.1% (8811/10000)
[Test]  Epoch: 35	Loss: 0.006022	Acc: 88.4% (8838/10000)
[Test]  Epoch: 36	Loss: 0.006095	Acc: 88.2% (8817/10000)
[Test]  Epoch: 37	Loss: 0.006117	Acc: 88.2% (8817/10000)
[Test]  Epoch: 38	Loss: 0.006142	Acc: 88.0% (8798/10000)
[Test]  Epoch: 39	Loss: 0.006142	Acc: 88.5% (8851/10000)
[Test]  Epoch: 40	Loss: 0.006186	Acc: 88.2% (8815/10000)
[Test]  Epoch: 41	Loss: 0.006157	Acc: 88.4% (8844/10000)
[Test]  Epoch: 42	Loss: 0.006141	Acc: 88.2% (8821/10000)
[Test]  Epoch: 43	Loss: 0.006133	Acc: 88.1% (8814/10000)
[Test]  Epoch: 44	Loss: 0.006275	Acc: 88.1% (8809/10000)
[Test]  Epoch: 45	Loss: 0.006096	Acc: 88.3% (8832/10000)
[Test]  Epoch: 46	Loss: 0.006187	Acc: 88.2% (8818/10000)
[Test]  Epoch: 47	Loss: 0.006389	Acc: 88.3% (8832/10000)
[Test]  Epoch: 48	Loss: 0.006260	Acc: 88.3% (8826/10000)
[Test]  Epoch: 49	Loss: 0.006131	Acc: 88.5% (8850/10000)
[Test]  Epoch: 50	Loss: 0.006302	Acc: 88.2% (8821/10000)
[Test]  Epoch: 51	Loss: 0.006170	Acc: 88.4% (8837/10000)
[Test]  Epoch: 52	Loss: 0.006248	Acc: 88.5% (8850/10000)
[Test]  Epoch: 53	Loss: 0.006182	Acc: 88.2% (8820/10000)
[Test]  Epoch: 54	Loss: 0.006377	Acc: 88.2% (8822/10000)
[Test]  Epoch: 55	Loss: 0.006288	Acc: 88.1% (8813/10000)
[Test]  Epoch: 56	Loss: 0.006313	Acc: 88.2% (8816/10000)
[Test]  Epoch: 57	Loss: 0.006213	Acc: 88.3% (8832/10000)
[Test]  Epoch: 58	Loss: 0.006427	Acc: 87.9% (8794/10000)
[Test]  Epoch: 59	Loss: 0.006454	Acc: 88.1% (8810/10000)
[Test]  Epoch: 60	Loss: 0.006377	Acc: 88.4% (8841/10000)
[Test]  Epoch: 61	Loss: 0.006184	Acc: 88.7% (8867/10000)
[Test]  Epoch: 62	Loss: 0.006434	Acc: 88.2% (8820/10000)
[Test]  Epoch: 63	Loss: 0.006362	Acc: 88.3% (8835/10000)
[Test]  Epoch: 64	Loss: 0.006413	Acc: 88.2% (8819/10000)
[Test]  Epoch: 65	Loss: 0.006332	Acc: 88.2% (8818/10000)
[Test]  Epoch: 66	Loss: 0.006248	Acc: 88.3% (8830/10000)
[Test]  Epoch: 67	Loss: 0.006360	Acc: 88.5% (8850/10000)
[Test]  Epoch: 68	Loss: 0.006332	Acc: 88.2% (8822/10000)
[Test]  Epoch: 69	Loss: 0.006185	Acc: 88.7% (8874/10000)
[Test]  Epoch: 70	Loss: 0.006376	Acc: 88.3% (8827/10000)
[Test]  Epoch: 71	Loss: 0.006330	Acc: 88.2% (8819/10000)
[Test]  Epoch: 72	Loss: 0.006409	Acc: 88.1% (8811/10000)
[Test]  Epoch: 73	Loss: 0.006381	Acc: 88.3% (8833/10000)
[Test]  Epoch: 74	Loss: 0.006429	Acc: 88.0% (8803/10000)
[Test]  Epoch: 75	Loss: 0.006281	Acc: 88.8% (8875/10000)
[Test]  Epoch: 76	Loss: 0.006302	Acc: 88.2% (8818/10000)
[Test]  Epoch: 77	Loss: 0.006539	Acc: 88.2% (8816/10000)
[Test]  Epoch: 78	Loss: 0.006363	Acc: 88.4% (8836/10000)
[Test]  Epoch: 79	Loss: 0.006389	Acc: 88.3% (8832/10000)
[Test]  Epoch: 80	Loss: 0.006446	Acc: 88.2% (8822/10000)
[Test]  Epoch: 81	Loss: 0.006472	Acc: 88.1% (8811/10000)
[Test]  Epoch: 82	Loss: 0.006285	Acc: 87.9% (8793/10000)
[Test]  Epoch: 83	Loss: 0.006323	Acc: 88.8% (8878/10000)
[Test]  Epoch: 84	Loss: 0.006396	Acc: 88.5% (8849/10000)
[Test]  Epoch: 85	Loss: 0.006414	Acc: 88.5% (8847/10000)
[Test]  Epoch: 86	Loss: 0.006369	Acc: 88.1% (8811/10000)
[Test]  Epoch: 87	Loss: 0.006445	Acc: 88.0% (8795/10000)
[Test]  Epoch: 88	Loss: 0.006454	Acc: 88.0% (8805/10000)
[Test]  Epoch: 89	Loss: 0.006354	Acc: 88.2% (8820/10000)
[Test]  Epoch: 90	Loss: 0.006312	Acc: 88.6% (8859/10000)
[Test]  Epoch: 91	Loss: 0.006358	Acc: 88.6% (8857/10000)
[Test]  Epoch: 92	Loss: 0.006396	Acc: 88.2% (8816/10000)
[Test]  Epoch: 93	Loss: 0.006318	Acc: 88.3% (8830/10000)
[Test]  Epoch: 94	Loss: 0.006239	Acc: 88.7% (8871/10000)
[Test]  Epoch: 95	Loss: 0.006363	Acc: 88.2% (8825/10000)
[Test]  Epoch: 96	Loss: 0.006200	Acc: 88.6% (8859/10000)
[Test]  Epoch: 97	Loss: 0.006289	Acc: 88.3% (8828/10000)
[Test]  Epoch: 98	Loss: 0.006501	Acc: 88.3% (8829/10000)
[Test]  Epoch: 99	Loss: 0.006374	Acc: 88.3% (8832/10000)
[Test]  Epoch: 100	Loss: 0.006357	Acc: 88.4% (8841/10000)
===========finish==========
['2024-08-19', '01:18:55.017611', '100', 'test', '0.006356608645617962', '88.41', '88.79']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007468	Acc: 84.0% (8398/10000)
[Test]  Epoch: 2	Loss: 0.006234	Acc: 86.9% (8686/10000)
[Test]  Epoch: 3	Loss: 0.006000	Acc: 87.2% (8722/10000)
[Test]  Epoch: 4	Loss: 0.006063	Acc: 87.2% (8719/10000)
[Test]  Epoch: 5	Loss: 0.006009	Acc: 87.3% (8734/10000)
[Test]  Epoch: 6	Loss: 0.005846	Acc: 87.9% (8789/10000)
[Test]  Epoch: 7	Loss: 0.005856	Acc: 87.7% (8766/10000)
[Test]  Epoch: 8	Loss: 0.005890	Acc: 87.5% (8747/10000)
[Test]  Epoch: 9	Loss: 0.005776	Acc: 87.8% (8775/10000)
[Test]  Epoch: 10	Loss: 0.005871	Acc: 87.5% (8753/10000)
[Test]  Epoch: 11	Loss: 0.005869	Acc: 87.6% (8760/10000)
[Test]  Epoch: 12	Loss: 0.005832	Acc: 87.9% (8789/10000)
[Test]  Epoch: 13	Loss: 0.005961	Acc: 87.6% (8759/10000)
[Test]  Epoch: 14	Loss: 0.005852	Acc: 87.7% (8773/10000)
[Test]  Epoch: 15	Loss: 0.005841	Acc: 87.5% (8755/10000)
[Test]  Epoch: 16	Loss: 0.005890	Acc: 87.8% (8781/10000)
[Test]  Epoch: 17	Loss: 0.005859	Acc: 87.7% (8768/10000)
[Test]  Epoch: 18	Loss: 0.005810	Acc: 87.8% (8782/10000)
[Test]  Epoch: 19	Loss: 0.006037	Acc: 87.6% (8763/10000)
[Test]  Epoch: 20	Loss: 0.005944	Acc: 88.2% (8820/10000)
[Test]  Epoch: 21	Loss: 0.006006	Acc: 88.0% (8796/10000)
[Test]  Epoch: 22	Loss: 0.005991	Acc: 88.0% (8803/10000)
[Test]  Epoch: 23	Loss: 0.006114	Acc: 87.7% (8772/10000)
[Test]  Epoch: 24	Loss: 0.006123	Acc: 87.8% (8785/10000)
[Test]  Epoch: 25	Loss: 0.006085	Acc: 88.0% (8796/10000)
[Test]  Epoch: 26	Loss: 0.006051	Acc: 88.0% (8799/10000)
[Test]  Epoch: 27	Loss: 0.006123	Acc: 87.8% (8778/10000)
[Test]  Epoch: 28	Loss: 0.006203	Acc: 87.8% (8783/10000)
[Test]  Epoch: 29	Loss: 0.006293	Acc: 87.7% (8771/10000)
[Test]  Epoch: 30	Loss: 0.006130	Acc: 87.7% (8771/10000)
[Test]  Epoch: 31	Loss: 0.006120	Acc: 87.5% (8754/10000)
[Test]  Epoch: 32	Loss: 0.006232	Acc: 87.8% (8781/10000)
[Test]  Epoch: 33	Loss: 0.006213	Acc: 87.8% (8778/10000)
[Test]  Epoch: 34	Loss: 0.006313	Acc: 87.7% (8772/10000)
[Test]  Epoch: 35	Loss: 0.006263	Acc: 87.7% (8767/10000)
[Test]  Epoch: 36	Loss: 0.006287	Acc: 87.8% (8779/10000)
[Test]  Epoch: 37	Loss: 0.006357	Acc: 87.9% (8789/10000)
[Test]  Epoch: 38	Loss: 0.006443	Acc: 87.6% (8764/10000)
[Test]  Epoch: 39	Loss: 0.006461	Acc: 87.8% (8783/10000)
[Test]  Epoch: 40	Loss: 0.006441	Acc: 87.5% (8748/10000)
[Test]  Epoch: 41	Loss: 0.006353	Acc: 87.8% (8778/10000)
[Test]  Epoch: 42	Loss: 0.006381	Acc: 87.8% (8781/10000)
[Test]  Epoch: 43	Loss: 0.006383	Acc: 87.8% (8785/10000)
[Test]  Epoch: 44	Loss: 0.006467	Acc: 87.8% (8778/10000)
[Test]  Epoch: 45	Loss: 0.006367	Acc: 87.6% (8758/10000)
[Test]  Epoch: 46	Loss: 0.006429	Acc: 87.6% (8756/10000)
[Test]  Epoch: 47	Loss: 0.006604	Acc: 87.6% (8757/10000)
[Test]  Epoch: 48	Loss: 0.006518	Acc: 87.9% (8788/10000)
[Test]  Epoch: 49	Loss: 0.006397	Acc: 88.0% (8800/10000)
[Test]  Epoch: 50	Loss: 0.006507	Acc: 87.5% (8750/10000)
[Test]  Epoch: 51	Loss: 0.006439	Acc: 87.9% (8787/10000)
[Test]  Epoch: 52	Loss: 0.006479	Acc: 87.9% (8789/10000)
[Test]  Epoch: 53	Loss: 0.006473	Acc: 87.9% (8789/10000)
[Test]  Epoch: 54	Loss: 0.006618	Acc: 87.6% (8763/10000)
[Test]  Epoch: 55	Loss: 0.006515	Acc: 87.7% (8772/10000)
[Test]  Epoch: 56	Loss: 0.006547	Acc: 88.0% (8802/10000)
[Test]  Epoch: 57	Loss: 0.006438	Acc: 88.1% (8809/10000)
[Test]  Epoch: 58	Loss: 0.006585	Acc: 87.5% (8754/10000)
[Test]  Epoch: 59	Loss: 0.006685	Acc: 87.7% (8770/10000)
[Test]  Epoch: 60	Loss: 0.006593	Acc: 87.8% (8776/10000)
[Test]  Epoch: 61	Loss: 0.006483	Acc: 88.1% (8814/10000)
[Test]  Epoch: 62	Loss: 0.006721	Acc: 87.7% (8767/10000)
[Test]  Epoch: 63	Loss: 0.006683	Acc: 87.5% (8751/10000)
[Test]  Epoch: 64	Loss: 0.006704	Acc: 87.5% (8748/10000)
[Test]  Epoch: 65	Loss: 0.006604	Acc: 87.6% (8758/10000)
[Test]  Epoch: 66	Loss: 0.006535	Acc: 87.4% (8740/10000)
[Test]  Epoch: 67	Loss: 0.006580	Acc: 88.0% (8797/10000)
[Test]  Epoch: 68	Loss: 0.006611	Acc: 87.8% (8776/10000)
[Test]  Epoch: 69	Loss: 0.006388	Acc: 88.0% (8801/10000)
[Test]  Epoch: 70	Loss: 0.006636	Acc: 87.9% (8792/10000)
[Test]  Epoch: 71	Loss: 0.006572	Acc: 88.0% (8795/10000)
[Test]  Epoch: 72	Loss: 0.006709	Acc: 87.5% (8753/10000)
[Test]  Epoch: 73	Loss: 0.006680	Acc: 87.7% (8767/10000)
[Test]  Epoch: 74	Loss: 0.006713	Acc: 87.5% (8752/10000)
[Test]  Epoch: 75	Loss: 0.006527	Acc: 88.0% (8803/10000)
[Test]  Epoch: 76	Loss: 0.006518	Acc: 88.1% (8807/10000)
[Test]  Epoch: 77	Loss: 0.006826	Acc: 87.7% (8765/10000)
[Test]  Epoch: 78	Loss: 0.006648	Acc: 87.8% (8775/10000)
[Test]  Epoch: 79	Loss: 0.006655	Acc: 87.9% (8789/10000)
[Test]  Epoch: 80	Loss: 0.006654	Acc: 87.9% (8789/10000)
[Test]  Epoch: 81	Loss: 0.006710	Acc: 87.6% (8764/10000)
[Test]  Epoch: 82	Loss: 0.006562	Acc: 87.5% (8751/10000)
[Test]  Epoch: 83	Loss: 0.006577	Acc: 88.2% (8823/10000)
[Test]  Epoch: 84	Loss: 0.006603	Acc: 88.1% (8810/10000)
[Test]  Epoch: 85	Loss: 0.006648	Acc: 87.9% (8793/10000)
[Test]  Epoch: 86	Loss: 0.006629	Acc: 87.7% (8771/10000)
[Test]  Epoch: 87	Loss: 0.006781	Acc: 87.2% (8725/10000)
[Test]  Epoch: 88	Loss: 0.006768	Acc: 87.5% (8749/10000)
[Test]  Epoch: 89	Loss: 0.006573	Acc: 87.8% (8782/10000)
[Test]  Epoch: 90	Loss: 0.006626	Acc: 88.0% (8796/10000)
[Test]  Epoch: 91	Loss: 0.006626	Acc: 87.9% (8788/10000)
[Test]  Epoch: 92	Loss: 0.006681	Acc: 87.6% (8761/10000)
[Test]  Epoch: 93	Loss: 0.006625	Acc: 87.6% (8764/10000)
[Test]  Epoch: 94	Loss: 0.006527	Acc: 88.0% (8797/10000)
[Test]  Epoch: 95	Loss: 0.006603	Acc: 87.7% (8773/10000)
[Test]  Epoch: 96	Loss: 0.006448	Acc: 88.0% (8804/10000)
[Test]  Epoch: 97	Loss: 0.006578	Acc: 87.7% (8772/10000)
[Test]  Epoch: 98	Loss: 0.006764	Acc: 87.4% (8743/10000)
[Test]  Epoch: 99	Loss: 0.006631	Acc: 88.0% (8800/10000)
[Test]  Epoch: 100	Loss: 0.006638	Acc: 87.7% (8769/10000)
===========finish==========
['2024-08-19', '01:21:12.026823', '100', 'test', '0.006637983026355505', '87.69', '88.23']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'features.1.weight', 'features.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.024455	Acc: 55.2% (5519/10000)
[Test]  Epoch: 2	Loss: 0.012266	Acc: 74.1% (7410/10000)
[Test]  Epoch: 3	Loss: 0.010306	Acc: 77.7% (7767/10000)
[Test]  Epoch: 4	Loss: 0.009947	Acc: 79.0% (7902/10000)
[Test]  Epoch: 5	Loss: 0.008949	Acc: 80.7% (8070/10000)
[Test]  Epoch: 6	Loss: 0.008845	Acc: 81.2% (8118/10000)
[Test]  Epoch: 7	Loss: 0.008911	Acc: 81.1% (8110/10000)
[Test]  Epoch: 8	Loss: 0.008295	Acc: 82.3% (8230/10000)
[Test]  Epoch: 9	Loss: 0.008233	Acc: 82.3% (8227/10000)
[Test]  Epoch: 10	Loss: 0.008249	Acc: 82.6% (8258/10000)
[Test]  Epoch: 11	Loss: 0.008272	Acc: 82.2% (8223/10000)
[Test]  Epoch: 12	Loss: 0.007988	Acc: 83.2% (8315/10000)
[Test]  Epoch: 13	Loss: 0.007991	Acc: 83.2% (8323/10000)
[Test]  Epoch: 14	Loss: 0.008063	Acc: 83.2% (8321/10000)
[Test]  Epoch: 15	Loss: 0.007898	Acc: 83.7% (8374/10000)
[Test]  Epoch: 16	Loss: 0.007875	Acc: 83.6% (8362/10000)
[Test]  Epoch: 17	Loss: 0.008018	Acc: 83.0% (8304/10000)
[Test]  Epoch: 18	Loss: 0.007959	Acc: 83.3% (8327/10000)
[Test]  Epoch: 19	Loss: 0.008043	Acc: 83.8% (8383/10000)
[Test]  Epoch: 20	Loss: 0.007867	Acc: 83.9% (8392/10000)
[Test]  Epoch: 21	Loss: 0.007924	Acc: 84.2% (8420/10000)
[Test]  Epoch: 22	Loss: 0.007850	Acc: 84.2% (8417/10000)
[Test]  Epoch: 23	Loss: 0.008051	Acc: 83.9% (8387/10000)
[Test]  Epoch: 24	Loss: 0.008011	Acc: 83.7% (8365/10000)
[Test]  Epoch: 25	Loss: 0.007971	Acc: 83.8% (8379/10000)
[Test]  Epoch: 26	Loss: 0.007829	Acc: 83.9% (8392/10000)
[Test]  Epoch: 27	Loss: 0.007958	Acc: 84.0% (8399/10000)
[Test]  Epoch: 28	Loss: 0.007702	Acc: 84.7% (8471/10000)
[Test]  Epoch: 29	Loss: 0.008179	Acc: 83.8% (8377/10000)
[Test]  Epoch: 30	Loss: 0.008007	Acc: 83.9% (8386/10000)
[Test]  Epoch: 31	Loss: 0.008030	Acc: 84.0% (8399/10000)
[Test]  Epoch: 32	Loss: 0.008006	Acc: 84.1% (8414/10000)
[Test]  Epoch: 33	Loss: 0.008140	Acc: 84.1% (8406/10000)
[Test]  Epoch: 34	Loss: 0.008094	Acc: 84.3% (8429/10000)
[Test]  Epoch: 35	Loss: 0.008286	Acc: 84.0% (8397/10000)
[Test]  Epoch: 36	Loss: 0.008018	Acc: 84.4% (8438/10000)
[Test]  Epoch: 37	Loss: 0.008156	Acc: 84.3% (8427/10000)
[Test]  Epoch: 38	Loss: 0.008307	Acc: 84.3% (8429/10000)
[Test]  Epoch: 39	Loss: 0.008536	Acc: 83.5% (8346/10000)
[Test]  Epoch: 40	Loss: 0.008108	Acc: 84.5% (8453/10000)
[Test]  Epoch: 41	Loss: 0.008321	Acc: 84.4% (8436/10000)
[Test]  Epoch: 42	Loss: 0.008231	Acc: 84.3% (8435/10000)
[Test]  Epoch: 43	Loss: 0.008292	Acc: 84.4% (8441/10000)
[Test]  Epoch: 44	Loss: 0.008235	Acc: 84.3% (8433/10000)
[Test]  Epoch: 45	Loss: 0.008061	Acc: 84.6% (8458/10000)
[Test]  Epoch: 46	Loss: 0.008212	Acc: 84.5% (8451/10000)
[Test]  Epoch: 47	Loss: 0.008494	Acc: 84.3% (8430/10000)
[Test]  Epoch: 48	Loss: 0.008203	Acc: 84.5% (8448/10000)
[Test]  Epoch: 49	Loss: 0.008372	Acc: 84.4% (8439/10000)
[Test]  Epoch: 50	Loss: 0.008490	Acc: 84.5% (8447/10000)
[Test]  Epoch: 51	Loss: 0.008132	Acc: 84.6% (8464/10000)
[Test]  Epoch: 52	Loss: 0.008184	Acc: 84.8% (8477/10000)
[Test]  Epoch: 53	Loss: 0.008328	Acc: 84.4% (8442/10000)
[Test]  Epoch: 54	Loss: 0.008410	Acc: 84.5% (8446/10000)
[Test]  Epoch: 55	Loss: 0.008246	Acc: 84.8% (8479/10000)
[Test]  Epoch: 56	Loss: 0.008530	Acc: 84.5% (8453/10000)
[Test]  Epoch: 57	Loss: 0.008185	Acc: 84.7% (8469/10000)
[Test]  Epoch: 58	Loss: 0.008525	Acc: 84.2% (8420/10000)
[Test]  Epoch: 59	Loss: 0.008694	Acc: 84.0% (8405/10000)
[Test]  Epoch: 60	Loss: 0.008380	Acc: 84.9% (8490/10000)
[Test]  Epoch: 61	Loss: 0.008294	Acc: 85.0% (8499/10000)
[Test]  Epoch: 62	Loss: 0.008323	Acc: 84.8% (8485/10000)
[Test]  Epoch: 63	Loss: 0.008365	Acc: 84.7% (8470/10000)
[Test]  Epoch: 64	Loss: 0.008490	Acc: 84.4% (8441/10000)
[Test]  Epoch: 65	Loss: 0.008190	Acc: 85.0% (8499/10000)
[Test]  Epoch: 66	Loss: 0.008251	Acc: 84.6% (8462/10000)
[Test]  Epoch: 67	Loss: 0.008160	Acc: 85.2% (8515/10000)
[Test]  Epoch: 68	Loss: 0.008194	Acc: 85.0% (8498/10000)
[Test]  Epoch: 69	Loss: 0.008034	Acc: 84.9% (8491/10000)
[Test]  Epoch: 70	Loss: 0.008318	Acc: 84.9% (8486/10000)
[Test]  Epoch: 71	Loss: 0.008343	Acc: 85.0% (8499/10000)
[Test]  Epoch: 72	Loss: 0.008387	Acc: 84.9% (8488/10000)
[Test]  Epoch: 73	Loss: 0.008472	Acc: 84.8% (8476/10000)
[Test]  Epoch: 74	Loss: 0.008366	Acc: 85.0% (8503/10000)
[Test]  Epoch: 75	Loss: 0.008105	Acc: 85.2% (8521/10000)
[Test]  Epoch: 76	Loss: 0.008187	Acc: 85.1% (8514/10000)
[Test]  Epoch: 77	Loss: 0.008490	Acc: 84.7% (8467/10000)
[Test]  Epoch: 78	Loss: 0.008248	Acc: 85.0% (8502/10000)
[Test]  Epoch: 79	Loss: 0.008273	Acc: 84.9% (8487/10000)
[Test]  Epoch: 80	Loss: 0.008305	Acc: 84.6% (8463/10000)
[Test]  Epoch: 81	Loss: 0.008375	Acc: 84.7% (8471/10000)
[Test]  Epoch: 82	Loss: 0.008248	Acc: 84.7% (8472/10000)
[Test]  Epoch: 83	Loss: 0.008384	Acc: 84.7% (8472/10000)
[Test]  Epoch: 84	Loss: 0.008305	Acc: 84.8% (8477/10000)
[Test]  Epoch: 85	Loss: 0.008325	Acc: 85.0% (8499/10000)
[Test]  Epoch: 86	Loss: 0.008325	Acc: 84.7% (8466/10000)
[Test]  Epoch: 87	Loss: 0.008414	Acc: 84.7% (8472/10000)
[Test]  Epoch: 88	Loss: 0.008320	Acc: 84.9% (8489/10000)
[Test]  Epoch: 89	Loss: 0.008310	Acc: 84.7% (8466/10000)
[Test]  Epoch: 90	Loss: 0.008351	Acc: 84.6% (8460/10000)
[Test]  Epoch: 91	Loss: 0.008322	Acc: 84.8% (8479/10000)
[Test]  Epoch: 92	Loss: 0.008338	Acc: 84.9% (8494/10000)
[Test]  Epoch: 93	Loss: 0.008306	Acc: 84.8% (8477/10000)
[Test]  Epoch: 94	Loss: 0.008221	Acc: 84.8% (8485/10000)
[Test]  Epoch: 95	Loss: 0.008174	Acc: 85.2% (8517/10000)
[Test]  Epoch: 96	Loss: 0.008202	Acc: 85.0% (8505/10000)
[Test]  Epoch: 97	Loss: 0.008279	Acc: 84.7% (8466/10000)
[Test]  Epoch: 98	Loss: 0.008376	Acc: 84.9% (8489/10000)
[Test]  Epoch: 99	Loss: 0.008336	Acc: 84.8% (8479/10000)
[Test]  Epoch: 100	Loss: 0.008336	Acc: 84.7% (8465/10000)
===========finish==========
['2024-08-19', '01:23:30.071299', '100', 'test', '0.008336129773408174', '84.65', '85.21']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'features.1.weight', 'features.0.weight', 'features.25.weight', 'features.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.049912	Acc: 24.0% (2402/10000)
[Test]  Epoch: 2	Loss: 0.025091	Acc: 49.0% (4899/10000)
[Test]  Epoch: 3	Loss: 0.019002	Acc: 58.9% (5887/10000)
[Test]  Epoch: 4	Loss: 0.017676	Acc: 60.6% (6065/10000)
[Test]  Epoch: 5	Loss: 0.014065	Acc: 68.6% (6864/10000)
[Test]  Epoch: 6	Loss: 0.013544	Acc: 70.6% (7062/10000)
[Test]  Epoch: 7	Loss: 0.013212	Acc: 71.3% (7130/10000)
[Test]  Epoch: 8	Loss: 0.013262	Acc: 71.6% (7156/10000)
[Test]  Epoch: 9	Loss: 0.011305	Acc: 75.3% (7528/10000)
[Test]  Epoch: 10	Loss: 0.011627	Acc: 74.5% (7448/10000)
[Test]  Epoch: 11	Loss: 0.011209	Acc: 75.3% (7529/10000)
[Test]  Epoch: 12	Loss: 0.010813	Acc: 76.5% (7652/10000)
[Test]  Epoch: 13	Loss: 0.011104	Acc: 75.9% (7586/10000)
[Test]  Epoch: 14	Loss: 0.011353	Acc: 76.0% (7599/10000)
[Test]  Epoch: 15	Loss: 0.010242	Acc: 78.3% (7829/10000)
[Test]  Epoch: 16	Loss: 0.010465	Acc: 77.9% (7790/10000)
[Test]  Epoch: 17	Loss: 0.010338	Acc: 78.2% (7815/10000)
[Test]  Epoch: 18	Loss: 0.010057	Acc: 78.4% (7844/10000)
[Test]  Epoch: 19	Loss: 0.011021	Acc: 77.2% (7720/10000)
[Test]  Epoch: 20	Loss: 0.010224	Acc: 78.7% (7873/10000)
[Test]  Epoch: 21	Loss: 0.010543	Acc: 78.4% (7837/10000)
[Test]  Epoch: 22	Loss: 0.010215	Acc: 79.1% (7906/10000)
[Test]  Epoch: 23	Loss: 0.010129	Acc: 79.1% (7914/10000)
[Test]  Epoch: 24	Loss: 0.011542	Acc: 76.7% (7674/10000)
[Test]  Epoch: 25	Loss: 0.010330	Acc: 79.2% (7921/10000)
[Test]  Epoch: 26	Loss: 0.009950	Acc: 80.0% (7996/10000)
[Test]  Epoch: 27	Loss: 0.010795	Acc: 78.7% (7869/10000)
[Test]  Epoch: 28	Loss: 0.010660	Acc: 78.6% (7858/10000)
[Test]  Epoch: 29	Loss: 0.010955	Acc: 78.3% (7831/10000)
[Test]  Epoch: 30	Loss: 0.010314	Acc: 79.6% (7956/10000)
[Test]  Epoch: 31	Loss: 0.010619	Acc: 78.9% (7891/10000)
[Test]  Epoch: 32	Loss: 0.010630	Acc: 79.6% (7959/10000)
[Test]  Epoch: 33	Loss: 0.010331	Acc: 79.7% (7965/10000)
[Test]  Epoch: 34	Loss: 0.010618	Acc: 79.3% (7932/10000)
[Test]  Epoch: 35	Loss: 0.011149	Acc: 78.7% (7873/10000)
[Test]  Epoch: 36	Loss: 0.011062	Acc: 78.8% (7883/10000)
[Test]  Epoch: 37	Loss: 0.011591	Acc: 77.9% (7791/10000)
[Test]  Epoch: 38	Loss: 0.011046	Acc: 78.8% (7876/10000)
[Test]  Epoch: 39	Loss: 0.011133	Acc: 78.9% (7886/10000)
[Test]  Epoch: 40	Loss: 0.010712	Acc: 79.7% (7966/10000)
[Test]  Epoch: 41	Loss: 0.010904	Acc: 79.5% (7946/10000)
[Test]  Epoch: 42	Loss: 0.010426	Acc: 80.1% (8010/10000)
[Test]  Epoch: 43	Loss: 0.010955	Acc: 79.5% (7950/10000)
[Test]  Epoch: 44	Loss: 0.010648	Acc: 80.1% (8007/10000)
[Test]  Epoch: 45	Loss: 0.010472	Acc: 80.3% (8027/10000)
[Test]  Epoch: 46	Loss: 0.010628	Acc: 80.2% (8019/10000)
[Test]  Epoch: 47	Loss: 0.010875	Acc: 80.1% (8011/10000)
[Test]  Epoch: 48	Loss: 0.010525	Acc: 80.3% (8034/10000)
[Test]  Epoch: 49	Loss: 0.010807	Acc: 79.9% (7994/10000)
[Test]  Epoch: 50	Loss: 0.011064	Acc: 79.6% (7960/10000)
[Test]  Epoch: 51	Loss: 0.010740	Acc: 79.7% (7973/10000)
[Test]  Epoch: 52	Loss: 0.010463	Acc: 80.7% (8071/10000)
[Test]  Epoch: 53	Loss: 0.011387	Acc: 79.2% (7921/10000)
[Test]  Epoch: 54	Loss: 0.011026	Acc: 80.0% (7995/10000)
[Test]  Epoch: 55	Loss: 0.010860	Acc: 79.8% (7982/10000)
[Test]  Epoch: 56	Loss: 0.011197	Acc: 80.0% (7998/10000)
[Test]  Epoch: 57	Loss: 0.011018	Acc: 80.1% (8012/10000)
[Test]  Epoch: 58	Loss: 0.010957	Acc: 80.3% (8027/10000)
[Test]  Epoch: 59	Loss: 0.011298	Acc: 79.6% (7959/10000)
[Test]  Epoch: 60	Loss: 0.010741	Acc: 81.0% (8103/10000)
[Test]  Epoch: 61	Loss: 0.010664	Acc: 81.1% (8114/10000)
[Test]  Epoch: 62	Loss: 0.010805	Acc: 80.7% (8069/10000)
[Test]  Epoch: 63	Loss: 0.010933	Acc: 80.0% (7998/10000)
[Test]  Epoch: 64	Loss: 0.010776	Acc: 80.5% (8049/10000)
[Test]  Epoch: 65	Loss: 0.010667	Acc: 80.8% (8083/10000)
[Test]  Epoch: 66	Loss: 0.010761	Acc: 80.5% (8048/10000)
[Test]  Epoch: 67	Loss: 0.010761	Acc: 80.3% (8035/10000)
[Test]  Epoch: 68	Loss: 0.010683	Acc: 80.7% (8071/10000)
[Test]  Epoch: 69	Loss: 0.010539	Acc: 80.5% (8049/10000)
[Test]  Epoch: 70	Loss: 0.010714	Acc: 80.5% (8050/10000)
[Test]  Epoch: 71	Loss: 0.010766	Acc: 80.7% (8067/10000)
[Test]  Epoch: 72	Loss: 0.010831	Acc: 80.6% (8062/10000)
[Test]  Epoch: 73	Loss: 0.010775	Acc: 80.4% (8043/10000)
[Test]  Epoch: 74	Loss: 0.010751	Acc: 80.8% (8075/10000)
[Test]  Epoch: 75	Loss: 0.010698	Acc: 80.6% (8056/10000)
[Test]  Epoch: 76	Loss: 0.010668	Acc: 81.0% (8095/10000)
[Test]  Epoch: 77	Loss: 0.010822	Acc: 80.6% (8064/10000)
[Test]  Epoch: 78	Loss: 0.010569	Acc: 81.2% (8123/10000)
[Test]  Epoch: 79	Loss: 0.010761	Acc: 80.5% (8053/10000)
[Test]  Epoch: 80	Loss: 0.010620	Acc: 81.1% (8110/10000)
[Test]  Epoch: 81	Loss: 0.010869	Acc: 80.3% (8034/10000)
[Test]  Epoch: 82	Loss: 0.010770	Acc: 80.8% (8080/10000)
[Test]  Epoch: 83	Loss: 0.010738	Acc: 80.7% (8072/10000)
[Test]  Epoch: 84	Loss: 0.010610	Acc: 81.2% (8117/10000)
[Test]  Epoch: 85	Loss: 0.010692	Acc: 80.9% (8090/10000)
[Test]  Epoch: 86	Loss: 0.010565	Acc: 80.9% (8088/10000)
[Test]  Epoch: 87	Loss: 0.010736	Acc: 81.0% (8097/10000)
[Test]  Epoch: 88	Loss: 0.010847	Acc: 80.7% (8066/10000)
[Test]  Epoch: 89	Loss: 0.010843	Acc: 80.7% (8067/10000)
[Test]  Epoch: 90	Loss: 0.010744	Acc: 80.7% (8074/10000)
[Test]  Epoch: 91	Loss: 0.010764	Acc: 80.6% (8062/10000)
[Test]  Epoch: 92	Loss: 0.010935	Acc: 80.3% (8034/10000)
[Test]  Epoch: 93	Loss: 0.010555	Acc: 81.0% (8101/10000)
[Test]  Epoch: 94	Loss: 0.010708	Acc: 80.6% (8061/10000)
[Test]  Epoch: 95	Loss: 0.010591	Acc: 81.0% (8104/10000)
[Test]  Epoch: 96	Loss: 0.010742	Acc: 80.7% (8070/10000)
[Test]  Epoch: 97	Loss: 0.010690	Acc: 80.4% (8044/10000)
[Test]  Epoch: 98	Loss: 0.010825	Acc: 80.6% (8063/10000)
[Test]  Epoch: 99	Loss: 0.010841	Acc: 80.1% (8008/10000)
[Test]  Epoch: 100	Loss: 0.010867	Acc: 80.4% (8041/10000)
===========finish==========
['2024-08-19', '01:25:51.721279', '100', 'test', '0.010866585049033164', '80.41', '81.23']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'features.1.weight', 'features.0.weight', 'features.25.weight', 'features.3.weight', 'classifier.weight', 'features.28.weight', 'features.41.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.033919	Acc: 21.1% (2106/10000)
[Test]  Epoch: 2	Loss: 0.029027	Acc: 31.4% (3142/10000)
[Test]  Epoch: 3	Loss: 0.028452	Acc: 34.7% (3466/10000)
[Test]  Epoch: 4	Loss: 0.028023	Acc: 35.7% (3566/10000)
[Test]  Epoch: 5	Loss: 0.027163	Acc: 38.5% (3850/10000)
[Test]  Epoch: 6	Loss: 0.026104	Acc: 41.6% (4158/10000)
[Test]  Epoch: 7	Loss: 0.027172	Acc: 38.9% (3887/10000)
[Test]  Epoch: 8	Loss: 0.027711	Acc: 38.5% (3845/10000)
[Test]  Epoch: 9	Loss: 0.026080	Acc: 42.1% (4209/10000)
[Test]  Epoch: 10	Loss: 0.025372	Acc: 43.5% (4352/10000)
[Test]  Epoch: 11	Loss: 0.027232	Acc: 42.8% (4281/10000)
[Test]  Epoch: 12	Loss: 0.025910	Acc: 43.9% (4391/10000)
[Test]  Epoch: 13	Loss: 0.026672	Acc: 43.7% (4368/10000)
[Test]  Epoch: 14	Loss: 0.026252	Acc: 44.7% (4466/10000)
[Test]  Epoch: 15	Loss: 0.024263	Acc: 48.6% (4856/10000)
[Test]  Epoch: 16	Loss: 0.024657	Acc: 48.6% (4863/10000)
[Test]  Epoch: 17	Loss: 0.025019	Acc: 48.6% (4864/10000)
[Test]  Epoch: 18	Loss: 0.024273	Acc: 50.6% (5057/10000)
[Test]  Epoch: 19	Loss: 0.025796	Acc: 49.5% (4949/10000)
[Test]  Epoch: 20	Loss: 0.025707	Acc: 50.7% (5072/10000)
[Test]  Epoch: 21	Loss: 0.025064	Acc: 51.5% (5153/10000)
[Test]  Epoch: 22	Loss: 0.026505	Acc: 50.6% (5056/10000)
[Test]  Epoch: 23	Loss: 0.024074	Acc: 53.6% (5364/10000)
[Test]  Epoch: 24	Loss: 0.027081	Acc: 51.2% (5118/10000)
[Test]  Epoch: 25	Loss: 0.026409	Acc: 50.9% (5088/10000)
[Test]  Epoch: 26	Loss: 0.026074	Acc: 52.4% (5243/10000)
[Test]  Epoch: 27	Loss: 0.028040	Acc: 50.3% (5029/10000)
[Test]  Epoch: 28	Loss: 0.027372	Acc: 52.5% (5252/10000)
[Test]  Epoch: 29	Loss: 0.025451	Acc: 55.6% (5563/10000)
[Test]  Epoch: 30	Loss: 0.026071	Acc: 55.8% (5577/10000)
[Test]  Epoch: 31	Loss: 0.027159	Acc: 53.6% (5359/10000)
[Test]  Epoch: 32	Loss: 0.027012	Acc: 54.3% (5429/10000)
[Test]  Epoch: 33	Loss: 0.026180	Acc: 55.2% (5519/10000)
[Test]  Epoch: 34	Loss: 0.028211	Acc: 54.8% (5476/10000)
[Test]  Epoch: 35	Loss: 0.026849	Acc: 55.5% (5555/10000)
[Test]  Epoch: 36	Loss: 0.029962	Acc: 53.8% (5382/10000)
[Test]  Epoch: 37	Loss: 0.029041	Acc: 55.1% (5515/10000)
[Test]  Epoch: 38	Loss: 0.028200	Acc: 55.6% (5563/10000)
[Test]  Epoch: 39	Loss: 0.028151	Acc: 56.6% (5657/10000)
[Test]  Epoch: 40	Loss: 0.030566	Acc: 53.7% (5368/10000)
[Test]  Epoch: 41	Loss: 0.029036	Acc: 56.5% (5654/10000)
[Test]  Epoch: 42	Loss: 0.028783	Acc: 57.0% (5705/10000)
[Test]  Epoch: 43	Loss: 0.030925	Acc: 55.2% (5517/10000)
[Test]  Epoch: 44	Loss: 0.030298	Acc: 56.0% (5597/10000)
[Test]  Epoch: 45	Loss: 0.031121	Acc: 56.0% (5599/10000)
[Test]  Epoch: 46	Loss: 0.030836	Acc: 55.5% (5548/10000)
[Test]  Epoch: 47	Loss: 0.029789	Acc: 57.2% (5719/10000)
[Test]  Epoch: 48	Loss: 0.029147	Acc: 57.9% (5788/10000)
[Test]  Epoch: 49	Loss: 0.031406	Acc: 56.0% (5603/10000)
[Test]  Epoch: 50	Loss: 0.029694	Acc: 57.4% (5743/10000)
[Test]  Epoch: 51	Loss: 0.030956	Acc: 57.3% (5734/10000)
[Test]  Epoch: 52	Loss: 0.030364	Acc: 58.2% (5825/10000)
[Test]  Epoch: 53	Loss: 0.032026	Acc: 57.1% (5708/10000)
[Test]  Epoch: 54	Loss: 0.032453	Acc: 56.9% (5694/10000)
[Test]  Epoch: 55	Loss: 0.030944	Acc: 58.6% (5861/10000)
[Test]  Epoch: 56	Loss: 0.032130	Acc: 56.9% (5685/10000)
[Test]  Epoch: 57	Loss: 0.032105	Acc: 57.6% (5765/10000)
[Test]  Epoch: 58	Loss: 0.032136	Acc: 57.4% (5742/10000)
[Test]  Epoch: 59	Loss: 0.032191	Acc: 57.8% (5778/10000)
[Test]  Epoch: 60	Loss: 0.031764	Acc: 58.5% (5855/10000)
[Test]  Epoch: 61	Loss: 0.031437	Acc: 58.6% (5862/10000)
[Test]  Epoch: 62	Loss: 0.031188	Acc: 58.9% (5894/10000)
[Test]  Epoch: 63	Loss: 0.030950	Acc: 58.8% (5883/10000)
[Test]  Epoch: 64	Loss: 0.030893	Acc: 59.6% (5961/10000)
[Test]  Epoch: 65	Loss: 0.031039	Acc: 58.9% (5885/10000)
[Test]  Epoch: 66	Loss: 0.030668	Acc: 58.9% (5891/10000)
[Test]  Epoch: 67	Loss: 0.031139	Acc: 59.0% (5902/10000)
[Test]  Epoch: 68	Loss: 0.031360	Acc: 59.1% (5912/10000)
[Test]  Epoch: 69	Loss: 0.030552	Acc: 59.8% (5975/10000)
[Test]  Epoch: 70	Loss: 0.030958	Acc: 59.2% (5923/10000)
[Test]  Epoch: 71	Loss: 0.031071	Acc: 59.5% (5952/10000)
[Test]  Epoch: 72	Loss: 0.031173	Acc: 59.5% (5951/10000)
[Test]  Epoch: 73	Loss: 0.031090	Acc: 59.4% (5936/10000)
[Test]  Epoch: 74	Loss: 0.030989	Acc: 59.4% (5937/10000)
[Test]  Epoch: 75	Loss: 0.030933	Acc: 59.6% (5958/10000)
[Test]  Epoch: 76	Loss: 0.030916	Acc: 59.4% (5937/10000)
[Test]  Epoch: 77	Loss: 0.030724	Acc: 59.6% (5963/10000)
[Test]  Epoch: 78	Loss: 0.030797	Acc: 59.4% (5940/10000)
[Test]  Epoch: 79	Loss: 0.030817	Acc: 59.4% (5938/10000)
[Test]  Epoch: 80	Loss: 0.030850	Acc: 59.7% (5967/10000)
[Test]  Epoch: 81	Loss: 0.030575	Acc: 59.9% (5992/10000)
[Test]  Epoch: 82	Loss: 0.030914	Acc: 59.3% (5927/10000)
[Test]  Epoch: 83	Loss: 0.030893	Acc: 59.9% (5989/10000)
[Test]  Epoch: 84	Loss: 0.030816	Acc: 59.8% (5976/10000)
[Test]  Epoch: 85	Loss: 0.030448	Acc: 60.0% (5999/10000)
[Test]  Epoch: 86	Loss: 0.031130	Acc: 59.5% (5951/10000)
[Test]  Epoch: 87	Loss: 0.030727	Acc: 59.2% (5925/10000)
[Test]  Epoch: 88	Loss: 0.030774	Acc: 59.3% (5932/10000)
[Test]  Epoch: 89	Loss: 0.031114	Acc: 58.8% (5883/10000)
[Test]  Epoch: 90	Loss: 0.031134	Acc: 59.1% (5914/10000)
[Test]  Epoch: 91	Loss: 0.030799	Acc: 59.7% (5970/10000)
[Test]  Epoch: 92	Loss: 0.031353	Acc: 59.4% (5939/10000)
[Test]  Epoch: 93	Loss: 0.030640	Acc: 59.8% (5983/10000)
[Test]  Epoch: 94	Loss: 0.030650	Acc: 59.6% (5956/10000)
[Test]  Epoch: 95	Loss: 0.030903	Acc: 59.0% (5903/10000)
[Test]  Epoch: 96	Loss: 0.030360	Acc: 60.3% (6030/10000)
[Test]  Epoch: 97	Loss: 0.031165	Acc: 59.7% (5972/10000)
[Test]  Epoch: 98	Loss: 0.031206	Acc: 59.6% (5956/10000)
[Test]  Epoch: 99	Loss: 0.031200	Acc: 59.7% (5966/10000)
[Test]  Epoch: 100	Loss: 0.031230	Acc: 59.7% (5966/10000)
===========finish==========
['2024-08-19', '01:28:14.287509', '100', 'test', '0.031229701858758927', '59.66', '60.3']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'features.1.weight', 'features.0.weight', 'features.25.weight', 'features.3.weight', 'classifier.weight', 'features.28.weight', 'features.41.weight', 'features.7.weight', 'features.31.weight', 'features.10.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035135	Acc: 21.1% (2106/10000)
[Test]  Epoch: 2	Loss: 0.031987	Acc: 28.4% (2845/10000)
[Test]  Epoch: 3	Loss: 0.031268	Acc: 29.8% (2980/10000)
[Test]  Epoch: 4	Loss: 0.029535	Acc: 31.6% (3163/10000)
[Test]  Epoch: 5	Loss: 0.028111	Acc: 35.5% (3549/10000)
[Test]  Epoch: 6	Loss: 0.029185	Acc: 34.8% (3477/10000)
[Test]  Epoch: 7	Loss: 0.034469	Acc: 29.2% (2924/10000)
[Test]  Epoch: 8	Loss: 0.028100	Acc: 36.5% (3653/10000)
[Test]  Epoch: 9	Loss: 0.027804	Acc: 37.6% (3756/10000)
[Test]  Epoch: 10	Loss: 0.027744	Acc: 40.1% (4006/10000)
[Test]  Epoch: 11	Loss: 0.031939	Acc: 37.0% (3703/10000)
[Test]  Epoch: 12	Loss: 0.033589	Acc: 36.3% (3633/10000)
[Test]  Epoch: 13	Loss: 0.026442	Acc: 42.6% (4265/10000)
[Test]  Epoch: 14	Loss: 0.030287	Acc: 37.9% (3790/10000)
[Test]  Epoch: 15	Loss: 0.033205	Acc: 39.3% (3926/10000)
[Test]  Epoch: 16	Loss: 0.030082	Acc: 40.9% (4094/10000)
[Test]  Epoch: 17	Loss: 0.029438	Acc: 43.0% (4301/10000)
[Test]  Epoch: 18	Loss: 0.030346	Acc: 43.0% (4301/10000)
[Test]  Epoch: 19	Loss: 0.030627	Acc: 42.9% (4288/10000)
[Test]  Epoch: 20	Loss: 0.027762	Acc: 44.5% (4455/10000)
[Test]  Epoch: 21	Loss: 0.032258	Acc: 41.8% (4175/10000)
[Test]  Epoch: 22	Loss: 0.030631	Acc: 44.3% (4426/10000)
[Test]  Epoch: 23	Loss: 0.034523	Acc: 42.0% (4201/10000)
[Test]  Epoch: 24	Loss: 0.029718	Acc: 44.8% (4477/10000)
[Test]  Epoch: 25	Loss: 0.033516	Acc: 41.8% (4182/10000)
[Test]  Epoch: 26	Loss: 0.030910	Acc: 45.7% (4568/10000)
[Test]  Epoch: 27	Loss: 0.032618	Acc: 44.2% (4416/10000)
[Test]  Epoch: 28	Loss: 0.033508	Acc: 44.1% (4408/10000)
[Test]  Epoch: 29	Loss: 0.034148	Acc: 44.5% (4451/10000)
[Test]  Epoch: 30	Loss: 0.033658	Acc: 46.3% (4634/10000)
[Test]  Epoch: 31	Loss: 0.032772	Acc: 47.2% (4720/10000)
[Test]  Epoch: 32	Loss: 0.033066	Acc: 46.4% (4638/10000)
[Test]  Epoch: 33	Loss: 0.031147	Acc: 48.0% (4797/10000)
[Test]  Epoch: 34	Loss: 0.033124	Acc: 46.4% (4635/10000)
[Test]  Epoch: 35	Loss: 0.037693	Acc: 44.0% (4399/10000)
[Test]  Epoch: 36	Loss: 0.038640	Acc: 45.0% (4496/10000)
[Test]  Epoch: 37	Loss: 0.036413	Acc: 45.9% (4589/10000)
[Test]  Epoch: 38	Loss: 0.035271	Acc: 48.1% (4811/10000)
[Test]  Epoch: 39	Loss: 0.035030	Acc: 47.9% (4791/10000)
[Test]  Epoch: 40	Loss: 0.035962	Acc: 46.8% (4677/10000)
[Test]  Epoch: 41	Loss: 0.035070	Acc: 48.2% (4821/10000)
[Test]  Epoch: 42	Loss: 0.041350	Acc: 45.3% (4529/10000)
[Test]  Epoch: 43	Loss: 0.037809	Acc: 47.9% (4793/10000)
[Test]  Epoch: 44	Loss: 0.037572	Acc: 48.2% (4822/10000)
[Test]  Epoch: 45	Loss: 0.036852	Acc: 48.5% (4851/10000)
[Test]  Epoch: 46	Loss: 0.036628	Acc: 48.4% (4838/10000)
[Test]  Epoch: 47	Loss: 0.039048	Acc: 48.6% (4859/10000)
[Test]  Epoch: 48	Loss: 0.036971	Acc: 49.9% (4990/10000)
[Test]  Epoch: 49	Loss: 0.037964	Acc: 47.6% (4756/10000)
[Test]  Epoch: 50	Loss: 0.039919	Acc: 47.7% (4766/10000)
[Test]  Epoch: 51	Loss: 0.038216	Acc: 48.9% (4889/10000)
[Test]  Epoch: 52	Loss: 0.040032	Acc: 48.9% (4893/10000)
[Test]  Epoch: 53	Loss: 0.040826	Acc: 47.3% (4728/10000)
[Test]  Epoch: 54	Loss: 0.042738	Acc: 46.6% (4659/10000)
[Test]  Epoch: 55	Loss: 0.040472	Acc: 48.7% (4869/10000)
[Test]  Epoch: 56	Loss: 0.040518	Acc: 48.5% (4845/10000)
[Test]  Epoch: 57	Loss: 0.043517	Acc: 48.0% (4805/10000)
[Test]  Epoch: 58	Loss: 0.041798	Acc: 47.4% (4742/10000)
[Test]  Epoch: 59	Loss: 0.039374	Acc: 48.6% (4859/10000)
[Test]  Epoch: 60	Loss: 0.041743	Acc: 47.8% (4782/10000)
[Test]  Epoch: 61	Loss: 0.038680	Acc: 51.0% (5105/10000)
[Test]  Epoch: 62	Loss: 0.037902	Acc: 51.4% (5139/10000)
[Test]  Epoch: 63	Loss: 0.037469	Acc: 51.2% (5124/10000)
[Test]  Epoch: 64	Loss: 0.037390	Acc: 51.0% (5097/10000)
[Test]  Epoch: 65	Loss: 0.036807	Acc: 51.7% (5169/10000)
[Test]  Epoch: 66	Loss: 0.037403	Acc: 51.3% (5129/10000)
[Test]  Epoch: 67	Loss: 0.037552	Acc: 51.3% (5130/10000)
[Test]  Epoch: 68	Loss: 0.037264	Acc: 51.5% (5154/10000)
[Test]  Epoch: 69	Loss: 0.037533	Acc: 51.6% (5165/10000)
[Test]  Epoch: 70	Loss: 0.037512	Acc: 51.6% (5158/10000)
[Test]  Epoch: 71	Loss: 0.036829	Acc: 51.8% (5181/10000)
[Test]  Epoch: 72	Loss: 0.037615	Acc: 51.5% (5147/10000)
[Test]  Epoch: 73	Loss: 0.038005	Acc: 50.9% (5090/10000)
[Test]  Epoch: 74	Loss: 0.037641	Acc: 51.1% (5106/10000)
[Test]  Epoch: 75	Loss: 0.037738	Acc: 50.9% (5086/10000)
[Test]  Epoch: 76	Loss: 0.037585	Acc: 51.1% (5108/10000)
[Test]  Epoch: 77	Loss: 0.037741	Acc: 51.1% (5107/10000)
[Test]  Epoch: 78	Loss: 0.037812	Acc: 51.4% (5137/10000)
[Test]  Epoch: 79	Loss: 0.038204	Acc: 51.4% (5144/10000)
[Test]  Epoch: 80	Loss: 0.037624	Acc: 51.5% (5149/10000)
[Test]  Epoch: 81	Loss: 0.037618	Acc: 51.3% (5128/10000)
[Test]  Epoch: 82	Loss: 0.037272	Acc: 51.6% (5165/10000)
[Test]  Epoch: 83	Loss: 0.037665	Acc: 51.5% (5155/10000)
[Test]  Epoch: 84	Loss: 0.037801	Acc: 51.6% (5158/10000)
[Test]  Epoch: 85	Loss: 0.037299	Acc: 52.2% (5220/10000)
[Test]  Epoch: 86	Loss: 0.038068	Acc: 51.1% (5112/10000)
[Test]  Epoch: 87	Loss: 0.037448	Acc: 52.1% (5215/10000)
[Test]  Epoch: 88	Loss: 0.038154	Acc: 51.3% (5126/10000)
[Test]  Epoch: 89	Loss: 0.037744	Acc: 51.4% (5142/10000)
[Test]  Epoch: 90	Loss: 0.038206	Acc: 51.0% (5097/10000)
[Test]  Epoch: 91	Loss: 0.038217	Acc: 51.3% (5131/10000)
[Test]  Epoch: 92	Loss: 0.038268	Acc: 51.4% (5143/10000)
[Test]  Epoch: 93	Loss: 0.038473	Acc: 51.0% (5103/10000)
[Test]  Epoch: 94	Loss: 0.038027	Acc: 51.6% (5160/10000)
[Test]  Epoch: 95	Loss: 0.038084	Acc: 51.4% (5136/10000)
[Test]  Epoch: 96	Loss: 0.038234	Acc: 51.6% (5161/10000)
[Test]  Epoch: 97	Loss: 0.038250	Acc: 51.6% (5157/10000)
[Test]  Epoch: 98	Loss: 0.038071	Acc: 51.7% (5167/10000)
[Test]  Epoch: 99	Loss: 0.038160	Acc: 51.8% (5176/10000)
[Test]  Epoch: 100	Loss: 0.037822	Acc: 51.9% (5194/10000)
===========finish==========
['2024-08-19', '01:30:35.072204', '100', 'test', '0.037821795129776', '51.94', '52.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'features.1.weight', 'features.0.weight', 'features.25.weight', 'features.3.weight', 'classifier.weight', 'features.28.weight', 'features.41.weight', 'features.7.weight', 'features.31.weight', 'features.10.weight', 'features.35.weight', 'features.14.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035796	Acc: 20.1% (2014/10000)
[Test]  Epoch: 2	Loss: 0.030935	Acc: 30.4% (3044/10000)
[Test]  Epoch: 3	Loss: 0.029696	Acc: 32.3% (3233/10000)
[Test]  Epoch: 4	Loss: 0.030059	Acc: 31.9% (3195/10000)
[Test]  Epoch: 5	Loss: 0.026794	Acc: 38.0% (3795/10000)
[Test]  Epoch: 6	Loss: 0.030251	Acc: 35.4% (3540/10000)
[Test]  Epoch: 7	Loss: 0.029010	Acc: 36.3% (3630/10000)
[Test]  Epoch: 8	Loss: 0.027910	Acc: 38.2% (3823/10000)
[Test]  Epoch: 9	Loss: 0.027382	Acc: 39.3% (3933/10000)
[Test]  Epoch: 10	Loss: 0.030547	Acc: 35.9% (3591/10000)
[Test]  Epoch: 11	Loss: 0.033104	Acc: 33.2% (3322/10000)
[Test]  Epoch: 12	Loss: 0.028549	Acc: 39.0% (3902/10000)
[Test]  Epoch: 13	Loss: 0.027094	Acc: 43.4% (4336/10000)
[Test]  Epoch: 14	Loss: 0.028669	Acc: 40.6% (4061/10000)
[Test]  Epoch: 15	Loss: 0.030298	Acc: 41.6% (4163/10000)
[Test]  Epoch: 16	Loss: 0.030141	Acc: 41.5% (4150/10000)
[Test]  Epoch: 17	Loss: 0.028096	Acc: 43.0% (4295/10000)
[Test]  Epoch: 18	Loss: 0.031939	Acc: 40.8% (4079/10000)
[Test]  Epoch: 19	Loss: 0.033036	Acc: 39.7% (3972/10000)
[Test]  Epoch: 20	Loss: 0.030923	Acc: 42.1% (4209/10000)
[Test]  Epoch: 21	Loss: 0.029603	Acc: 44.3% (4429/10000)
[Test]  Epoch: 22	Loss: 0.032089	Acc: 42.1% (4206/10000)
[Test]  Epoch: 23	Loss: 0.030631	Acc: 44.8% (4481/10000)
[Test]  Epoch: 24	Loss: 0.031211	Acc: 44.5% (4448/10000)
[Test]  Epoch: 25	Loss: 0.035170	Acc: 40.5% (4047/10000)
[Test]  Epoch: 26	Loss: 0.033773	Acc: 41.9% (4189/10000)
[Test]  Epoch: 27	Loss: 0.036930	Acc: 40.8% (4082/10000)
[Test]  Epoch: 28	Loss: 0.033471	Acc: 44.5% (4446/10000)
[Test]  Epoch: 29	Loss: 0.037475	Acc: 40.3% (4030/10000)
[Test]  Epoch: 30	Loss: 0.038142	Acc: 41.5% (4149/10000)
[Test]  Epoch: 31	Loss: 0.034059	Acc: 45.7% (4568/10000)
[Test]  Epoch: 32	Loss: 0.035422	Acc: 43.6% (4360/10000)
[Test]  Epoch: 33	Loss: 0.035176	Acc: 44.6% (4460/10000)
[Test]  Epoch: 34	Loss: 0.036387	Acc: 43.9% (4387/10000)
[Test]  Epoch: 35	Loss: 0.034575	Acc: 47.1% (4714/10000)
[Test]  Epoch: 36	Loss: 0.035742	Acc: 45.6% (4556/10000)
[Test]  Epoch: 37	Loss: 0.034820	Acc: 46.7% (4671/10000)
[Test]  Epoch: 38	Loss: 0.038128	Acc: 43.1% (4307/10000)
[Test]  Epoch: 39	Loss: 0.038650	Acc: 43.7% (4369/10000)
[Test]  Epoch: 40	Loss: 0.035737	Acc: 45.8% (4584/10000)
[Test]  Epoch: 41	Loss: 0.039102	Acc: 45.7% (4570/10000)
[Test]  Epoch: 42	Loss: 0.042353	Acc: 43.3% (4331/10000)
[Test]  Epoch: 43	Loss: 0.040007	Acc: 43.2% (4316/10000)
[Test]  Epoch: 44	Loss: 0.037877	Acc: 46.7% (4668/10000)
[Test]  Epoch: 45	Loss: 0.041957	Acc: 45.5% (4548/10000)
[Test]  Epoch: 46	Loss: 0.039655	Acc: 45.0% (4499/10000)
[Test]  Epoch: 47	Loss: 0.040772	Acc: 45.7% (4573/10000)
[Test]  Epoch: 48	Loss: 0.040386	Acc: 45.7% (4567/10000)
[Test]  Epoch: 49	Loss: 0.039142	Acc: 46.8% (4684/10000)
[Test]  Epoch: 50	Loss: 0.040141	Acc: 45.5% (4549/10000)
[Test]  Epoch: 51	Loss: 0.039778	Acc: 47.9% (4792/10000)
[Test]  Epoch: 52	Loss: 0.038402	Acc: 48.4% (4835/10000)
[Test]  Epoch: 53	Loss: 0.040133	Acc: 46.4% (4643/10000)
[Test]  Epoch: 54	Loss: 0.040149	Acc: 47.5% (4747/10000)
[Test]  Epoch: 55	Loss: 0.039889	Acc: 48.3% (4834/10000)
[Test]  Epoch: 56	Loss: 0.041465	Acc: 46.6% (4657/10000)
[Test]  Epoch: 57	Loss: 0.042621	Acc: 45.9% (4590/10000)
[Test]  Epoch: 58	Loss: 0.042987	Acc: 46.7% (4669/10000)
[Test]  Epoch: 59	Loss: 0.041557	Acc: 46.0% (4595/10000)
[Test]  Epoch: 60	Loss: 0.043157	Acc: 46.6% (4657/10000)
[Test]  Epoch: 61	Loss: 0.039104	Acc: 49.1% (4910/10000)
[Test]  Epoch: 62	Loss: 0.038694	Acc: 49.3% (4931/10000)
[Test]  Epoch: 63	Loss: 0.038417	Acc: 49.7% (4969/10000)
[Test]  Epoch: 64	Loss: 0.038351	Acc: 49.9% (4989/10000)
[Test]  Epoch: 65	Loss: 0.038431	Acc: 49.8% (4982/10000)
[Test]  Epoch: 66	Loss: 0.038283	Acc: 50.0% (5002/10000)
[Test]  Epoch: 67	Loss: 0.038383	Acc: 49.6% (4963/10000)
[Test]  Epoch: 68	Loss: 0.038350	Acc: 50.1% (5012/10000)
[Test]  Epoch: 69	Loss: 0.038694	Acc: 49.5% (4945/10000)
[Test]  Epoch: 70	Loss: 0.038330	Acc: 49.5% (4945/10000)
[Test]  Epoch: 71	Loss: 0.037876	Acc: 50.5% (5054/10000)
[Test]  Epoch: 72	Loss: 0.038278	Acc: 49.7% (4967/10000)
[Test]  Epoch: 73	Loss: 0.038418	Acc: 49.7% (4971/10000)
[Test]  Epoch: 74	Loss: 0.038584	Acc: 49.2% (4917/10000)
[Test]  Epoch: 75	Loss: 0.038285	Acc: 49.9% (4992/10000)
[Test]  Epoch: 76	Loss: 0.038667	Acc: 49.7% (4968/10000)
[Test]  Epoch: 77	Loss: 0.038343	Acc: 49.3% (4927/10000)
[Test]  Epoch: 78	Loss: 0.038437	Acc: 49.6% (4960/10000)
[Test]  Epoch: 79	Loss: 0.038467	Acc: 50.0% (4995/10000)
[Test]  Epoch: 80	Loss: 0.038463	Acc: 50.0% (4996/10000)
[Test]  Epoch: 81	Loss: 0.038985	Acc: 50.0% (4996/10000)
[Test]  Epoch: 82	Loss: 0.038330	Acc: 50.0% (4996/10000)
[Test]  Epoch: 83	Loss: 0.038615	Acc: 50.4% (5039/10000)
[Test]  Epoch: 84	Loss: 0.038358	Acc: 49.7% (4967/10000)
[Test]  Epoch: 85	Loss: 0.038267	Acc: 49.6% (4963/10000)
[Test]  Epoch: 86	Loss: 0.038675	Acc: 49.8% (4975/10000)
[Test]  Epoch: 87	Loss: 0.038296	Acc: 50.1% (5010/10000)
[Test]  Epoch: 88	Loss: 0.038297	Acc: 49.7% (4971/10000)
[Test]  Epoch: 89	Loss: 0.038450	Acc: 50.2% (5018/10000)
[Test]  Epoch: 90	Loss: 0.038893	Acc: 50.0% (4995/10000)
[Test]  Epoch: 91	Loss: 0.038340	Acc: 50.1% (5014/10000)
[Test]  Epoch: 92	Loss: 0.038379	Acc: 50.5% (5048/10000)
[Test]  Epoch: 93	Loss: 0.038452	Acc: 50.0% (5000/10000)
[Test]  Epoch: 94	Loss: 0.038492	Acc: 50.1% (5014/10000)
[Test]  Epoch: 95	Loss: 0.038355	Acc: 49.8% (4979/10000)
[Test]  Epoch: 96	Loss: 0.038753	Acc: 49.9% (4987/10000)
[Test]  Epoch: 97	Loss: 0.038512	Acc: 50.0% (4999/10000)
[Test]  Epoch: 98	Loss: 0.038634	Acc: 50.0% (4997/10000)
[Test]  Epoch: 99	Loss: 0.038765	Acc: 49.9% (4987/10000)
[Test]  Epoch: 100	Loss: 0.038287	Acc: 50.4% (5040/10000)
===========finish==========
['2024-08-19', '01:32:55.177502', '100', 'test', '0.03828707568645477', '50.4', '50.54']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'features.1.weight', 'features.0.weight', 'features.25.weight', 'features.3.weight', 'classifier.weight', 'features.28.weight', 'features.41.weight', 'features.7.weight', 'features.31.weight', 'features.10.weight', 'features.35.weight', 'features.14.weight', 'features.38.weight', 'features.17.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.040564	Acc: 19.6% (1959/10000)
[Test]  Epoch: 2	Loss: 0.033211	Acc: 26.6% (2661/10000)
[Test]  Epoch: 3	Loss: 0.028802	Acc: 33.1% (3315/10000)
[Test]  Epoch: 4	Loss: 0.029706	Acc: 32.6% (3259/10000)
[Test]  Epoch: 5	Loss: 0.027636	Acc: 36.6% (3658/10000)
[Test]  Epoch: 6	Loss: 0.029068	Acc: 34.7% (3472/10000)
[Test]  Epoch: 7	Loss: 0.028076	Acc: 36.9% (3694/10000)
[Test]  Epoch: 8	Loss: 0.030872	Acc: 31.8% (3182/10000)
[Test]  Epoch: 9	Loss: 0.030137	Acc: 33.7% (3369/10000)
[Test]  Epoch: 10	Loss: 0.029364	Acc: 37.8% (3778/10000)
[Test]  Epoch: 11	Loss: 0.029078	Acc: 38.6% (3860/10000)
[Test]  Epoch: 12	Loss: 0.030888	Acc: 37.7% (3772/10000)
[Test]  Epoch: 13	Loss: 0.026371	Acc: 42.7% (4274/10000)
[Test]  Epoch: 14	Loss: 0.029780	Acc: 37.6% (3757/10000)
[Test]  Epoch: 15	Loss: 0.027652	Acc: 42.6% (4259/10000)
[Test]  Epoch: 16	Loss: 0.030743	Acc: 39.6% (3960/10000)
[Test]  Epoch: 17	Loss: 0.028233	Acc: 42.2% (4221/10000)
[Test]  Epoch: 18	Loss: 0.030979	Acc: 41.2% (4118/10000)
[Test]  Epoch: 19	Loss: 0.034688	Acc: 39.5% (3946/10000)
[Test]  Epoch: 20	Loss: 0.031833	Acc: 40.3% (4033/10000)
[Test]  Epoch: 21	Loss: 0.031037	Acc: 42.4% (4244/10000)
[Test]  Epoch: 22	Loss: 0.035639	Acc: 39.0% (3896/10000)
[Test]  Epoch: 23	Loss: 0.033877	Acc: 40.7% (4074/10000)
[Test]  Epoch: 24	Loss: 0.033980	Acc: 41.3% (4128/10000)
[Test]  Epoch: 25	Loss: 0.033307	Acc: 41.8% (4184/10000)
[Test]  Epoch: 26	Loss: 0.033849	Acc: 42.4% (4240/10000)
[Test]  Epoch: 27	Loss: 0.034454	Acc: 42.9% (4290/10000)
[Test]  Epoch: 28	Loss: 0.036008	Acc: 41.2% (4116/10000)
[Test]  Epoch: 29	Loss: 0.036327	Acc: 43.3% (4330/10000)
[Test]  Epoch: 30	Loss: 0.036169	Acc: 42.7% (4270/10000)
[Test]  Epoch: 31	Loss: 0.035114	Acc: 44.7% (4471/10000)
[Test]  Epoch: 32	Loss: 0.036822	Acc: 44.9% (4485/10000)
[Test]  Epoch: 33	Loss: 0.037517	Acc: 43.5% (4351/10000)
[Test]  Epoch: 34	Loss: 0.038896	Acc: 43.1% (4312/10000)
[Test]  Epoch: 35	Loss: 0.037020	Acc: 45.4% (4543/10000)
[Test]  Epoch: 36	Loss: 0.037449	Acc: 43.8% (4375/10000)
[Test]  Epoch: 37	Loss: 0.045975	Acc: 41.0% (4099/10000)
[Test]  Epoch: 38	Loss: 0.039322	Acc: 44.4% (4435/10000)
[Test]  Epoch: 39	Loss: 0.040557	Acc: 44.6% (4459/10000)
[Test]  Epoch: 40	Loss: 0.040138	Acc: 46.0% (4605/10000)
[Test]  Epoch: 41	Loss: 0.038797	Acc: 47.5% (4746/10000)
[Test]  Epoch: 42	Loss: 0.042652	Acc: 42.9% (4291/10000)
[Test]  Epoch: 43	Loss: 0.038055	Acc: 47.1% (4710/10000)
[Test]  Epoch: 44	Loss: 0.038188	Acc: 47.1% (4712/10000)
[Test]  Epoch: 45	Loss: 0.041705	Acc: 43.8% (4382/10000)
[Test]  Epoch: 46	Loss: 0.041574	Acc: 45.1% (4506/10000)
[Test]  Epoch: 47	Loss: 0.039840	Acc: 46.8% (4684/10000)
[Test]  Epoch: 48	Loss: 0.040879	Acc: 47.4% (4735/10000)
[Test]  Epoch: 49	Loss: 0.041940	Acc: 45.8% (4576/10000)
[Test]  Epoch: 50	Loss: 0.041499	Acc: 45.7% (4571/10000)
[Test]  Epoch: 51	Loss: 0.042958	Acc: 45.4% (4540/10000)
[Test]  Epoch: 52	Loss: 0.041457	Acc: 48.2% (4819/10000)
[Test]  Epoch: 53	Loss: 0.042469	Acc: 46.0% (4601/10000)
[Test]  Epoch: 54	Loss: 0.040776	Acc: 48.2% (4818/10000)
[Test]  Epoch: 55	Loss: 0.042608	Acc: 46.3% (4632/10000)
[Test]  Epoch: 56	Loss: 0.041960	Acc: 47.0% (4702/10000)
[Test]  Epoch: 57	Loss: 0.042803	Acc: 46.5% (4645/10000)
[Test]  Epoch: 58	Loss: 0.044218	Acc: 47.9% (4788/10000)
[Test]  Epoch: 59	Loss: 0.041115	Acc: 47.5% (4749/10000)
[Test]  Epoch: 60	Loss: 0.040677	Acc: 47.9% (4791/10000)
[Test]  Epoch: 61	Loss: 0.039596	Acc: 49.0% (4898/10000)
[Test]  Epoch: 62	Loss: 0.038993	Acc: 49.5% (4951/10000)
[Test]  Epoch: 63	Loss: 0.039072	Acc: 49.9% (4987/10000)
[Test]  Epoch: 64	Loss: 0.038509	Acc: 50.0% (5000/10000)
[Test]  Epoch: 65	Loss: 0.038540	Acc: 50.0% (5004/10000)
[Test]  Epoch: 66	Loss: 0.038243	Acc: 50.7% (5070/10000)
[Test]  Epoch: 67	Loss: 0.038290	Acc: 49.7% (4968/10000)
[Test]  Epoch: 68	Loss: 0.038515	Acc: 50.3% (5034/10000)
[Test]  Epoch: 69	Loss: 0.037845	Acc: 50.9% (5086/10000)
[Test]  Epoch: 70	Loss: 0.038306	Acc: 50.4% (5035/10000)
[Test]  Epoch: 71	Loss: 0.037743	Acc: 50.3% (5026/10000)
[Test]  Epoch: 72	Loss: 0.038229	Acc: 50.5% (5054/10000)
[Test]  Epoch: 73	Loss: 0.038014	Acc: 50.4% (5036/10000)
[Test]  Epoch: 74	Loss: 0.037898	Acc: 50.6% (5063/10000)
[Test]  Epoch: 75	Loss: 0.037920	Acc: 50.4% (5044/10000)
[Test]  Epoch: 76	Loss: 0.038403	Acc: 50.3% (5030/10000)
[Test]  Epoch: 77	Loss: 0.038767	Acc: 50.5% (5053/10000)
[Test]  Epoch: 78	Loss: 0.038045	Acc: 50.5% (5051/10000)
[Test]  Epoch: 79	Loss: 0.038220	Acc: 50.3% (5032/10000)
[Test]  Epoch: 80	Loss: 0.037990	Acc: 50.6% (5061/10000)
[Test]  Epoch: 81	Loss: 0.038321	Acc: 50.6% (5060/10000)
[Test]  Epoch: 82	Loss: 0.037469	Acc: 51.0% (5099/10000)
[Test]  Epoch: 83	Loss: 0.037858	Acc: 51.0% (5099/10000)
[Test]  Epoch: 84	Loss: 0.038198	Acc: 50.8% (5081/10000)
[Test]  Epoch: 85	Loss: 0.038288	Acc: 50.3% (5027/10000)
[Test]  Epoch: 86	Loss: 0.038095	Acc: 51.0% (5101/10000)
[Test]  Epoch: 87	Loss: 0.038084	Acc: 50.1% (5007/10000)
[Test]  Epoch: 88	Loss: 0.037923	Acc: 50.6% (5057/10000)
[Test]  Epoch: 89	Loss: 0.038159	Acc: 50.9% (5085/10000)
[Test]  Epoch: 90	Loss: 0.037833	Acc: 51.1% (5111/10000)
[Test]  Epoch: 91	Loss: 0.037772	Acc: 51.4% (5136/10000)
[Test]  Epoch: 92	Loss: 0.038249	Acc: 50.2% (5022/10000)
[Test]  Epoch: 93	Loss: 0.038009	Acc: 50.9% (5087/10000)
[Test]  Epoch: 94	Loss: 0.037823	Acc: 50.8% (5083/10000)
[Test]  Epoch: 95	Loss: 0.037624	Acc: 51.0% (5102/10000)
[Test]  Epoch: 96	Loss: 0.037877	Acc: 50.8% (5081/10000)
[Test]  Epoch: 97	Loss: 0.038158	Acc: 50.8% (5079/10000)
[Test]  Epoch: 98	Loss: 0.037618	Acc: 50.9% (5092/10000)
[Test]  Epoch: 99	Loss: 0.037576	Acc: 50.7% (5070/10000)
[Test]  Epoch: 100	Loss: 0.037432	Acc: 51.3% (5132/10000)
===========finish==========
['2024-08-19', '01:35:15.231550', '100', 'test', '0.03743186476230621', '51.32', '51.36']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'features.1.weight', 'features.0.weight', 'features.25.weight', 'features.3.weight', 'classifier.weight', 'features.28.weight', 'features.41.weight', 'features.7.weight', 'features.31.weight', 'features.10.weight', 'features.35.weight', 'features.14.weight', 'features.38.weight', 'features.17.weight', 'features.20.weight', 'features.24.weight', 'features.27.weight', 'features.30.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.055644	Acc: 14.0% (1396/10000)
[Test]  Epoch: 2	Loss: 0.033830	Acc: 21.6% (2156/10000)
[Test]  Epoch: 3	Loss: 0.029203	Acc: 29.7% (2974/10000)
[Test]  Epoch: 4	Loss: 0.030365	Acc: 29.1% (2909/10000)
[Test]  Epoch: 5	Loss: 0.028058	Acc: 34.5% (3452/10000)
[Test]  Epoch: 6	Loss: 0.029315	Acc: 31.2% (3125/10000)
[Test]  Epoch: 7	Loss: 0.032369	Acc: 27.9% (2794/10000)
[Test]  Epoch: 8	Loss: 0.034694	Acc: 26.6% (2665/10000)
[Test]  Epoch: 9	Loss: 0.028866	Acc: 34.5% (3447/10000)
[Test]  Epoch: 10	Loss: 0.030730	Acc: 34.0% (3395/10000)
[Test]  Epoch: 11	Loss: 0.028620	Acc: 37.8% (3782/10000)
[Test]  Epoch: 12	Loss: 0.029251	Acc: 37.1% (3708/10000)
[Test]  Epoch: 13	Loss: 0.030418	Acc: 35.9% (3592/10000)
[Test]  Epoch: 14	Loss: 0.028933	Acc: 36.7% (3672/10000)
[Test]  Epoch: 15	Loss: 0.032523	Acc: 36.6% (3661/10000)
[Test]  Epoch: 16	Loss: 0.032174	Acc: 35.4% (3542/10000)
[Test]  Epoch: 17	Loss: 0.027664	Acc: 39.9% (3993/10000)
[Test]  Epoch: 18	Loss: 0.034572	Acc: 37.2% (3720/10000)
[Test]  Epoch: 19	Loss: 0.033146	Acc: 37.5% (3754/10000)
[Test]  Epoch: 20	Loss: 0.030215	Acc: 40.9% (4094/10000)
[Test]  Epoch: 21	Loss: 0.029386	Acc: 39.8% (3978/10000)
[Test]  Epoch: 22	Loss: 0.032226	Acc: 38.9% (3886/10000)
[Test]  Epoch: 23	Loss: 0.033956	Acc: 39.3% (3930/10000)
[Test]  Epoch: 24	Loss: 0.030323	Acc: 41.1% (4106/10000)
[Test]  Epoch: 25	Loss: 0.033663	Acc: 40.8% (4079/10000)
[Test]  Epoch: 26	Loss: 0.032017	Acc: 41.4% (4135/10000)
[Test]  Epoch: 27	Loss: 0.033984	Acc: 38.6% (3860/10000)
[Test]  Epoch: 28	Loss: 0.036152	Acc: 38.2% (3824/10000)
[Test]  Epoch: 29	Loss: 0.035898	Acc: 39.3% (3932/10000)
[Test]  Epoch: 30	Loss: 0.038619	Acc: 40.1% (4011/10000)
[Test]  Epoch: 31	Loss: 0.035114	Acc: 41.3% (4133/10000)
[Test]  Epoch: 32	Loss: 0.035503	Acc: 42.2% (4223/10000)
[Test]  Epoch: 33	Loss: 0.035717	Acc: 42.5% (4255/10000)
[Test]  Epoch: 34	Loss: 0.035641	Acc: 42.0% (4203/10000)
[Test]  Epoch: 35	Loss: 0.041577	Acc: 39.8% (3983/10000)
[Test]  Epoch: 36	Loss: 0.038702	Acc: 40.2% (4023/10000)
[Test]  Epoch: 37	Loss: 0.037207	Acc: 42.3% (4232/10000)
[Test]  Epoch: 38	Loss: 0.039039	Acc: 41.5% (4148/10000)
[Test]  Epoch: 39	Loss: 0.037296	Acc: 44.8% (4477/10000)
[Test]  Epoch: 40	Loss: 0.040099	Acc: 41.0% (4095/10000)
[Test]  Epoch: 41	Loss: 0.037821	Acc: 44.3% (4432/10000)
[Test]  Epoch: 42	Loss: 0.036460	Acc: 44.8% (4477/10000)
[Test]  Epoch: 43	Loss: 0.038680	Acc: 44.1% (4407/10000)
[Test]  Epoch: 44	Loss: 0.034577	Acc: 46.4% (4640/10000)
[Test]  Epoch: 45	Loss: 0.039706	Acc: 42.7% (4274/10000)
[Test]  Epoch: 46	Loss: 0.041290	Acc: 41.1% (4114/10000)
[Test]  Epoch: 47	Loss: 0.037893	Acc: 45.1% (4508/10000)
[Test]  Epoch: 48	Loss: 0.037018	Acc: 46.8% (4684/10000)
[Test]  Epoch: 49	Loss: 0.038093	Acc: 44.5% (4453/10000)
[Test]  Epoch: 50	Loss: 0.041405	Acc: 43.8% (4379/10000)
[Test]  Epoch: 51	Loss: 0.039147	Acc: 44.9% (4493/10000)
[Test]  Epoch: 52	Loss: 0.043895	Acc: 43.4% (4342/10000)
[Test]  Epoch: 53	Loss: 0.043717	Acc: 43.1% (4308/10000)
[Test]  Epoch: 54	Loss: 0.038576	Acc: 45.0% (4502/10000)
[Test]  Epoch: 55	Loss: 0.039871	Acc: 45.9% (4587/10000)
[Test]  Epoch: 56	Loss: 0.041232	Acc: 44.5% (4451/10000)
[Test]  Epoch: 57	Loss: 0.039401	Acc: 45.3% (4534/10000)
[Test]  Epoch: 58	Loss: 0.040356	Acc: 46.0% (4595/10000)
[Test]  Epoch: 59	Loss: 0.039462	Acc: 46.7% (4667/10000)
[Test]  Epoch: 60	Loss: 0.042284	Acc: 45.5% (4554/10000)
[Test]  Epoch: 61	Loss: 0.039027	Acc: 47.4% (4736/10000)
[Test]  Epoch: 62	Loss: 0.038134	Acc: 47.5% (4755/10000)
[Test]  Epoch: 63	Loss: 0.038187	Acc: 47.5% (4752/10000)
[Test]  Epoch: 64	Loss: 0.037826	Acc: 48.1% (4808/10000)
[Test]  Epoch: 65	Loss: 0.037136	Acc: 47.9% (4789/10000)
[Test]  Epoch: 66	Loss: 0.037629	Acc: 48.3% (4834/10000)
[Test]  Epoch: 67	Loss: 0.037517	Acc: 47.9% (4794/10000)
[Test]  Epoch: 68	Loss: 0.037593	Acc: 47.9% (4794/10000)
[Test]  Epoch: 69	Loss: 0.037496	Acc: 48.2% (4819/10000)
[Test]  Epoch: 70	Loss: 0.036972	Acc: 48.7% (4871/10000)
[Test]  Epoch: 71	Loss: 0.037069	Acc: 48.5% (4853/10000)
[Test]  Epoch: 72	Loss: 0.037195	Acc: 48.7% (4869/10000)
[Test]  Epoch: 73	Loss: 0.037416	Acc: 48.3% (4833/10000)
[Test]  Epoch: 74	Loss: 0.037864	Acc: 48.0% (4804/10000)
[Test]  Epoch: 75	Loss: 0.037448	Acc: 48.5% (4845/10000)
[Test]  Epoch: 76	Loss: 0.037616	Acc: 48.0% (4805/10000)
[Test]  Epoch: 77	Loss: 0.037672	Acc: 48.3% (4830/10000)
[Test]  Epoch: 78	Loss: 0.037857	Acc: 48.1% (4806/10000)
[Test]  Epoch: 79	Loss: 0.037716	Acc: 48.2% (4819/10000)
[Test]  Epoch: 80	Loss: 0.037297	Acc: 48.4% (4835/10000)
[Test]  Epoch: 81	Loss: 0.037936	Acc: 48.1% (4813/10000)
[Test]  Epoch: 82	Loss: 0.037509	Acc: 48.5% (4854/10000)
[Test]  Epoch: 83	Loss: 0.037657	Acc: 48.5% (4849/10000)
[Test]  Epoch: 84	Loss: 0.037812	Acc: 48.4% (4843/10000)
[Test]  Epoch: 85	Loss: 0.037539	Acc: 48.4% (4841/10000)
[Test]  Epoch: 86	Loss: 0.037623	Acc: 48.4% (4842/10000)
[Test]  Epoch: 87	Loss: 0.037588	Acc: 48.7% (4872/10000)
[Test]  Epoch: 88	Loss: 0.037510	Acc: 48.6% (4860/10000)
[Test]  Epoch: 89	Loss: 0.037504	Acc: 48.9% (4892/10000)
[Test]  Epoch: 90	Loss: 0.037684	Acc: 48.1% (4806/10000)
[Test]  Epoch: 91	Loss: 0.037829	Acc: 48.8% (4878/10000)
[Test]  Epoch: 92	Loss: 0.037642	Acc: 48.5% (4850/10000)
[Test]  Epoch: 93	Loss: 0.037822	Acc: 49.2% (4920/10000)
[Test]  Epoch: 94	Loss: 0.037740	Acc: 48.6% (4856/10000)
[Test]  Epoch: 95	Loss: 0.037233	Acc: 48.5% (4855/10000)
[Test]  Epoch: 96	Loss: 0.038155	Acc: 48.5% (4847/10000)
[Test]  Epoch: 97	Loss: 0.037943	Acc: 48.4% (4839/10000)
[Test]  Epoch: 98	Loss: 0.038382	Acc: 48.4% (4841/10000)
[Test]  Epoch: 99	Loss: 0.038130	Acc: 48.8% (4877/10000)
[Test]  Epoch: 100	Loss: 0.037784	Acc: 48.5% (4846/10000)
===========finish==========
['2024-08-19', '01:37:44.601542', '100', 'test', '0.03778394038677216', '48.46', '49.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.4.weight', 'features.11.weight', 'features.8.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'features.1.weight', 'features.0.weight', 'features.25.weight', 'features.3.weight', 'classifier.weight', 'features.28.weight', 'features.41.weight', 'features.7.weight', 'features.31.weight', 'features.10.weight', 'features.35.weight', 'features.14.weight', 'features.38.weight', 'features.17.weight', 'features.20.weight', 'features.24.weight', 'features.27.weight', 'features.30.weight', 'features.34.weight', 'features.37.weight', 'features.40.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.053788	Acc: 10.6% (1056/10000)
[Test]  Epoch: 2	Loss: 0.032764	Acc: 20.3% (2026/10000)
[Test]  Epoch: 3	Loss: 0.030664	Acc: 28.7% (2867/10000)
[Test]  Epoch: 4	Loss: 0.030747	Acc: 26.7% (2668/10000)
[Test]  Epoch: 5	Loss: 0.028948	Acc: 32.2% (3224/10000)
[Test]  Epoch: 6	Loss: 0.029715	Acc: 28.8% (2876/10000)
[Test]  Epoch: 7	Loss: 0.029409	Acc: 31.0% (3099/10000)
[Test]  Epoch: 8	Loss: 0.029690	Acc: 29.6% (2957/10000)
[Test]  Epoch: 9	Loss: 0.029293	Acc: 32.0% (3202/10000)
[Test]  Epoch: 10	Loss: 0.029056	Acc: 33.0% (3297/10000)
[Test]  Epoch: 11	Loss: 0.030350	Acc: 34.0% (3401/10000)
[Test]  Epoch: 12	Loss: 0.027902	Acc: 35.7% (3567/10000)
[Test]  Epoch: 13	Loss: 0.028216	Acc: 36.5% (3645/10000)
[Test]  Epoch: 14	Loss: 0.027420	Acc: 38.1% (3808/10000)
[Test]  Epoch: 15	Loss: 0.031561	Acc: 36.0% (3597/10000)
[Test]  Epoch: 16	Loss: 0.027831	Acc: 37.3% (3728/10000)
[Test]  Epoch: 17	Loss: 0.033154	Acc: 34.8% (3475/10000)
[Test]  Epoch: 18	Loss: 0.031944	Acc: 35.7% (3568/10000)
[Test]  Epoch: 19	Loss: 0.026974	Acc: 41.6% (4164/10000)
[Test]  Epoch: 20	Loss: 0.031187	Acc: 37.5% (3747/10000)
[Test]  Epoch: 21	Loss: 0.027757	Acc: 41.7% (4170/10000)
[Test]  Epoch: 22	Loss: 0.028838	Acc: 40.8% (4081/10000)
[Test]  Epoch: 23	Loss: 0.030570	Acc: 39.3% (3932/10000)
[Test]  Epoch: 24	Loss: 0.028310	Acc: 40.4% (4038/10000)
[Test]  Epoch: 25	Loss: 0.029606	Acc: 41.3% (4130/10000)
[Test]  Epoch: 26	Loss: 0.029329	Acc: 42.1% (4215/10000)
[Test]  Epoch: 27	Loss: 0.030590	Acc: 40.3% (4029/10000)
[Test]  Epoch: 28	Loss: 0.030822	Acc: 40.0% (4004/10000)
[Test]  Epoch: 29	Loss: 0.029243	Acc: 43.8% (4376/10000)
[Test]  Epoch: 30	Loss: 0.030266	Acc: 43.7% (4368/10000)
[Test]  Epoch: 31	Loss: 0.032405	Acc: 38.5% (3853/10000)
[Test]  Epoch: 32	Loss: 0.031775	Acc: 43.1% (4314/10000)
[Test]  Epoch: 33	Loss: 0.030818	Acc: 43.6% (4365/10000)
[Test]  Epoch: 34	Loss: 0.030889	Acc: 44.1% (4414/10000)
[Test]  Epoch: 35	Loss: 0.036504	Acc: 40.6% (4061/10000)
[Test]  Epoch: 36	Loss: 0.032988	Acc: 41.4% (4140/10000)
[Test]  Epoch: 37	Loss: 0.034483	Acc: 42.9% (4293/10000)
[Test]  Epoch: 38	Loss: 0.034190	Acc: 42.6% (4262/10000)
[Test]  Epoch: 39	Loss: 0.035296	Acc: 42.6% (4257/10000)
[Test]  Epoch: 40	Loss: 0.035507	Acc: 42.2% (4219/10000)
[Test]  Epoch: 41	Loss: 0.035132	Acc: 44.3% (4431/10000)
[Test]  Epoch: 42	Loss: 0.037078	Acc: 42.1% (4214/10000)
[Test]  Epoch: 43	Loss: 0.038264	Acc: 43.6% (4364/10000)
[Test]  Epoch: 44	Loss: 0.036161	Acc: 44.0% (4398/10000)
[Test]  Epoch: 45	Loss: 0.034453	Acc: 43.6% (4356/10000)
[Test]  Epoch: 46	Loss: 0.033968	Acc: 45.4% (4538/10000)
[Test]  Epoch: 47	Loss: 0.035993	Acc: 45.3% (4531/10000)
[Test]  Epoch: 48	Loss: 0.037627	Acc: 44.7% (4466/10000)
[Test]  Epoch: 49	Loss: 0.039520	Acc: 44.9% (4487/10000)
[Test]  Epoch: 50	Loss: 0.038492	Acc: 43.5% (4351/10000)
[Test]  Epoch: 51	Loss: 0.037440	Acc: 45.2% (4524/10000)
[Test]  Epoch: 52	Loss: 0.036051	Acc: 46.2% (4620/10000)
[Test]  Epoch: 53	Loss: 0.037839	Acc: 44.9% (4493/10000)
[Test]  Epoch: 54	Loss: 0.037787	Acc: 44.3% (4431/10000)
[Test]  Epoch: 55	Loss: 0.039578	Acc: 45.1% (4512/10000)
[Test]  Epoch: 56	Loss: 0.038199	Acc: 46.0% (4599/10000)
[Test]  Epoch: 57	Loss: 0.039533	Acc: 44.2% (4422/10000)
[Test]  Epoch: 58	Loss: 0.037338	Acc: 46.3% (4630/10000)
[Test]  Epoch: 59	Loss: 0.040993	Acc: 44.3% (4426/10000)
[Test]  Epoch: 60	Loss: 0.041491	Acc: 44.3% (4427/10000)
[Test]  Epoch: 61	Loss: 0.037170	Acc: 46.6% (4658/10000)
[Test]  Epoch: 62	Loss: 0.036097	Acc: 47.2% (4720/10000)
[Test]  Epoch: 63	Loss: 0.036154	Acc: 47.3% (4728/10000)
[Test]  Epoch: 64	Loss: 0.035980	Acc: 47.6% (4756/10000)
[Test]  Epoch: 65	Loss: 0.035436	Acc: 48.2% (4816/10000)
[Test]  Epoch: 66	Loss: 0.035944	Acc: 47.2% (4725/10000)
[Test]  Epoch: 67	Loss: 0.035887	Acc: 47.8% (4784/10000)
[Test]  Epoch: 68	Loss: 0.035775	Acc: 48.1% (4811/10000)
[Test]  Epoch: 69	Loss: 0.035778	Acc: 48.1% (4810/10000)
[Test]  Epoch: 70	Loss: 0.035707	Acc: 48.6% (4859/10000)
[Test]  Epoch: 71	Loss: 0.035883	Acc: 48.2% (4818/10000)
[Test]  Epoch: 72	Loss: 0.035554	Acc: 48.2% (4820/10000)
[Test]  Epoch: 73	Loss: 0.035621	Acc: 48.5% (4850/10000)
[Test]  Epoch: 74	Loss: 0.035649	Acc: 48.4% (4835/10000)
[Test]  Epoch: 75	Loss: 0.035361	Acc: 48.0% (4796/10000)
[Test]  Epoch: 76	Loss: 0.035869	Acc: 47.7% (4770/10000)
[Test]  Epoch: 77	Loss: 0.035782	Acc: 48.0% (4802/10000)
[Test]  Epoch: 78	Loss: 0.035776	Acc: 48.5% (4848/10000)
[Test]  Epoch: 79	Loss: 0.035137	Acc: 49.1% (4912/10000)
[Test]  Epoch: 80	Loss: 0.035551	Acc: 48.2% (4820/10000)
[Test]  Epoch: 81	Loss: 0.035964	Acc: 48.0% (4799/10000)
[Test]  Epoch: 82	Loss: 0.035566	Acc: 47.9% (4792/10000)
[Test]  Epoch: 83	Loss: 0.035622	Acc: 49.0% (4898/10000)
[Test]  Epoch: 84	Loss: 0.035969	Acc: 48.3% (4830/10000)
[Test]  Epoch: 85	Loss: 0.035860	Acc: 48.2% (4823/10000)
[Test]  Epoch: 86	Loss: 0.035599	Acc: 48.0% (4797/10000)
[Test]  Epoch: 87	Loss: 0.035403	Acc: 48.7% (4872/10000)
[Test]  Epoch: 88	Loss: 0.035788	Acc: 48.3% (4829/10000)
[Test]  Epoch: 89	Loss: 0.035592	Acc: 48.5% (4852/10000)
[Test]  Epoch: 90	Loss: 0.035890	Acc: 48.2% (4820/10000)
[Test]  Epoch: 91	Loss: 0.035352	Acc: 48.6% (4859/10000)
[Test]  Epoch: 92	Loss: 0.036003	Acc: 48.8% (4880/10000)
[Test]  Epoch: 93	Loss: 0.035866	Acc: 48.6% (4856/10000)
[Test]  Epoch: 94	Loss: 0.036018	Acc: 47.9% (4792/10000)
[Test]  Epoch: 95	Loss: 0.035520	Acc: 48.5% (4854/10000)
[Test]  Epoch: 96	Loss: 0.036001	Acc: 48.7% (4870/10000)
[Test]  Epoch: 97	Loss: 0.035761	Acc: 48.2% (4816/10000)
[Test]  Epoch: 98	Loss: 0.035984	Acc: 48.7% (4874/10000)
[Test]  Epoch: 99	Loss: 0.036125	Acc: 48.7% (4871/10000)
[Test]  Epoch: 100	Loss: 0.036070	Acc: 48.6% (4865/10000)
===========finish==========
['2024-08-19', '01:40:08.747490', '100', 'test', '0.03607013370990753', '48.65', '49.12']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=0
get_sample_layers not_random
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007625	Acc: 85.2% (8525/10000)
[Test]  Epoch: 2	Loss: 0.007451	Acc: 85.3% (8535/10000)
[Test]  Epoch: 3	Loss: 0.007350	Acc: 85.7% (8570/10000)
[Test]  Epoch: 4	Loss: 0.007241	Acc: 85.8% (8582/10000)
[Test]  Epoch: 5	Loss: 0.007212	Acc: 85.7% (8573/10000)
[Test]  Epoch: 6	Loss: 0.007154	Acc: 85.8% (8577/10000)
[Test]  Epoch: 7	Loss: 0.007091	Acc: 85.8% (8580/10000)
[Test]  Epoch: 8	Loss: 0.007071	Acc: 85.8% (8577/10000)
[Test]  Epoch: 9	Loss: 0.007019	Acc: 85.8% (8575/10000)
[Test]  Epoch: 10	Loss: 0.006988	Acc: 85.8% (8585/10000)
[Test]  Epoch: 11	Loss: 0.006987	Acc: 85.8% (8582/10000)
[Test]  Epoch: 12	Loss: 0.006966	Acc: 85.9% (8588/10000)
[Test]  Epoch: 13	Loss: 0.006902	Acc: 85.9% (8594/10000)
[Test]  Epoch: 14	Loss: 0.006889	Acc: 86.0% (8603/10000)
[Test]  Epoch: 15	Loss: 0.006894	Acc: 86.1% (8606/10000)
[Test]  Epoch: 16	Loss: 0.006865	Acc: 86.0% (8600/10000)
[Test]  Epoch: 17	Loss: 0.006840	Acc: 86.0% (8602/10000)
[Test]  Epoch: 18	Loss: 0.006838	Acc: 86.1% (8610/10000)
[Test]  Epoch: 19	Loss: 0.006827	Acc: 86.0% (8605/10000)
[Test]  Epoch: 20	Loss: 0.006833	Acc: 86.0% (8598/10000)
[Test]  Epoch: 21	Loss: 0.006835	Acc: 86.0% (8597/10000)
[Test]  Epoch: 22	Loss: 0.006836	Acc: 86.0% (8603/10000)
[Test]  Epoch: 23	Loss: 0.006803	Acc: 86.1% (8609/10000)
[Test]  Epoch: 24	Loss: 0.006781	Acc: 86.1% (8611/10000)
[Test]  Epoch: 25	Loss: 0.006787	Acc: 86.1% (8613/10000)
[Test]  Epoch: 26	Loss: 0.006768	Acc: 86.1% (8607/10000)
[Test]  Epoch: 27	Loss: 0.006784	Acc: 86.1% (8610/10000)
[Test]  Epoch: 28	Loss: 0.006745	Acc: 86.1% (8610/10000)
[Test]  Epoch: 29	Loss: 0.006745	Acc: 86.2% (8618/10000)
[Test]  Epoch: 30	Loss: 0.006766	Acc: 86.1% (8607/10000)
[Test]  Epoch: 31	Loss: 0.006743	Acc: 86.2% (8617/10000)
[Test]  Epoch: 32	Loss: 0.006747	Acc: 86.1% (8610/10000)
[Test]  Epoch: 33	Loss: 0.006752	Acc: 86.1% (8612/10000)
[Test]  Epoch: 34	Loss: 0.006723	Acc: 86.1% (8610/10000)
[Test]  Epoch: 35	Loss: 0.006708	Acc: 86.2% (8616/10000)
[Test]  Epoch: 36	Loss: 0.006704	Acc: 86.1% (8612/10000)
[Test]  Epoch: 37	Loss: 0.006738	Acc: 86.1% (8610/10000)
[Test]  Epoch: 38	Loss: 0.006721	Acc: 86.1% (8612/10000)
[Test]  Epoch: 39	Loss: 0.006714	Acc: 86.1% (8613/10000)
[Test]  Epoch: 40	Loss: 0.006708	Acc: 86.1% (8606/10000)
[Test]  Epoch: 41	Loss: 0.006714	Acc: 86.1% (8607/10000)
[Test]  Epoch: 42	Loss: 0.006723	Acc: 86.1% (8606/10000)
[Test]  Epoch: 43	Loss: 0.006703	Acc: 86.1% (8614/10000)
[Test]  Epoch: 44	Loss: 0.006703	Acc: 86.1% (8611/10000)
[Test]  Epoch: 45	Loss: 0.006708	Acc: 86.1% (8609/10000)
[Test]  Epoch: 46	Loss: 0.006688	Acc: 86.2% (8616/10000)
[Test]  Epoch: 47	Loss: 0.006715	Acc: 86.1% (8614/10000)
[Test]  Epoch: 48	Loss: 0.006665	Acc: 86.1% (8613/10000)
[Test]  Epoch: 49	Loss: 0.006707	Acc: 86.2% (8616/10000)
[Test]  Epoch: 50	Loss: 0.006702	Acc: 86.0% (8604/10000)
[Test]  Epoch: 51	Loss: 0.006665	Acc: 86.0% (8600/10000)
[Test]  Epoch: 52	Loss: 0.006654	Acc: 86.2% (8615/10000)
[Test]  Epoch: 53	Loss: 0.006652	Acc: 86.1% (8610/10000)
[Test]  Epoch: 54	Loss: 0.006658	Acc: 86.1% (8608/10000)
[Test]  Epoch: 55	Loss: 0.006657	Acc: 86.1% (8606/10000)
[Test]  Epoch: 56	Loss: 0.006646	Acc: 86.2% (8617/10000)
[Test]  Epoch: 57	Loss: 0.006675	Acc: 86.1% (8610/10000)
[Test]  Epoch: 58	Loss: 0.006676	Acc: 86.2% (8618/10000)
[Test]  Epoch: 59	Loss: 0.006671	Acc: 86.2% (8625/10000)
[Test]  Epoch: 60	Loss: 0.006693	Acc: 86.2% (8616/10000)
[Test]  Epoch: 61	Loss: 0.006676	Acc: 86.1% (8613/10000)
[Test]  Epoch: 62	Loss: 0.006676	Acc: 86.2% (8618/10000)
[Test]  Epoch: 63	Loss: 0.006666	Acc: 86.1% (8612/10000)
[Test]  Epoch: 64	Loss: 0.006650	Acc: 86.1% (8612/10000)
[Test]  Epoch: 65	Loss: 0.006650	Acc: 86.2% (8621/10000)
[Test]  Epoch: 66	Loss: 0.006655	Acc: 86.1% (8609/10000)
[Test]  Epoch: 67	Loss: 0.006647	Acc: 86.2% (8621/10000)
[Test]  Epoch: 68	Loss: 0.006670	Acc: 86.1% (8612/10000)
[Test]  Epoch: 69	Loss: 0.006670	Acc: 86.1% (8613/10000)
[Test]  Epoch: 70	Loss: 0.006660	Acc: 86.1% (8611/10000)
[Test]  Epoch: 71	Loss: 0.006644	Acc: 86.1% (8609/10000)
[Test]  Epoch: 72	Loss: 0.006623	Acc: 86.2% (8617/10000)
[Test]  Epoch: 73	Loss: 0.006641	Acc: 86.1% (8611/10000)
[Test]  Epoch: 74	Loss: 0.006637	Acc: 86.2% (8617/10000)
[Test]  Epoch: 75	Loss: 0.006640	Acc: 86.2% (8618/10000)
[Test]  Epoch: 76	Loss: 0.006650	Acc: 86.1% (8610/10000)
[Test]  Epoch: 77	Loss: 0.006650	Acc: 86.1% (8611/10000)
[Test]  Epoch: 78	Loss: 0.006650	Acc: 86.1% (8612/10000)
[Test]  Epoch: 79	Loss: 0.006661	Acc: 86.2% (8619/10000)
[Test]  Epoch: 80	Loss: 0.006649	Acc: 86.1% (8609/10000)
[Test]  Epoch: 81	Loss: 0.006663	Acc: 86.1% (8608/10000)
[Test]  Epoch: 82	Loss: 0.006660	Acc: 86.0% (8605/10000)
[Test]  Epoch: 83	Loss: 0.006637	Acc: 86.1% (8612/10000)
[Test]  Epoch: 84	Loss: 0.006643	Acc: 86.2% (8617/10000)
[Test]  Epoch: 85	Loss: 0.006650	Acc: 86.1% (8609/10000)
[Test]  Epoch: 86	Loss: 0.006645	Acc: 86.1% (8614/10000)
[Test]  Epoch: 87	Loss: 0.006644	Acc: 86.1% (8607/10000)
[Test]  Epoch: 88	Loss: 0.006635	Acc: 86.1% (8609/10000)
[Test]  Epoch: 89	Loss: 0.006634	Acc: 86.1% (8613/10000)
[Test]  Epoch: 90	Loss: 0.006636	Acc: 86.1% (8613/10000)
[Test]  Epoch: 91	Loss: 0.006642	Acc: 86.0% (8600/10000)
[Test]  Epoch: 92	Loss: 0.006632	Acc: 86.2% (8615/10000)
[Test]  Epoch: 93	Loss: 0.006634	Acc: 86.1% (8611/10000)
[Test]  Epoch: 94	Loss: 0.006636	Acc: 86.1% (8610/10000)
[Test]  Epoch: 95	Loss: 0.006629	Acc: 86.2% (8616/10000)
[Test]  Epoch: 96	Loss: 0.006626	Acc: 86.2% (8621/10000)
[Test]  Epoch: 97	Loss: 0.006643	Acc: 86.1% (8611/10000)
[Test]  Epoch: 98	Loss: 0.006656	Acc: 86.0% (8599/10000)
[Test]  Epoch: 99	Loss: 0.006633	Acc: 86.1% (8612/10000)
[Test]  Epoch: 100	Loss: 0.006653	Acc: 86.1% (8609/10000)
===========finish==========
['2024-08-19', '01:42:36.348664', '100', 'test', '0.006652705573290586', '86.09', '86.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.010550	Acc: 82.6% (8259/10000)
[Test]  Epoch: 2	Loss: 0.008549	Acc: 84.7% (8466/10000)
[Test]  Epoch: 3	Loss: 0.008071	Acc: 84.8% (8478/10000)
[Test]  Epoch: 4	Loss: 0.007688	Acc: 85.2% (8523/10000)
[Test]  Epoch: 5	Loss: 0.007507	Acc: 85.2% (8517/10000)
[Test]  Epoch: 6	Loss: 0.007391	Acc: 85.4% (8541/10000)
[Test]  Epoch: 7	Loss: 0.007325	Acc: 85.2% (8520/10000)
[Test]  Epoch: 8	Loss: 0.007204	Acc: 85.5% (8548/10000)
[Test]  Epoch: 9	Loss: 0.007175	Acc: 85.4% (8544/10000)
[Test]  Epoch: 10	Loss: 0.007143	Acc: 85.5% (8549/10000)
[Test]  Epoch: 11	Loss: 0.007092	Acc: 85.5% (8552/10000)
[Test]  Epoch: 12	Loss: 0.007068	Acc: 85.7% (8565/10000)
[Test]  Epoch: 13	Loss: 0.006980	Acc: 85.6% (8559/10000)
[Test]  Epoch: 14	Loss: 0.006985	Acc: 85.8% (8580/10000)
[Test]  Epoch: 15	Loss: 0.006978	Acc: 85.6% (8560/10000)
[Test]  Epoch: 16	Loss: 0.006973	Acc: 85.8% (8575/10000)
[Test]  Epoch: 17	Loss: 0.006936	Acc: 85.9% (8594/10000)
[Test]  Epoch: 18	Loss: 0.006927	Acc: 85.7% (8574/10000)
[Test]  Epoch: 19	Loss: 0.006904	Acc: 85.8% (8582/10000)
[Test]  Epoch: 20	Loss: 0.006895	Acc: 85.7% (8565/10000)
[Test]  Epoch: 21	Loss: 0.006876	Acc: 85.8% (8579/10000)
[Test]  Epoch: 22	Loss: 0.006889	Acc: 85.8% (8581/10000)
[Test]  Epoch: 23	Loss: 0.006877	Acc: 85.8% (8576/10000)
[Test]  Epoch: 24	Loss: 0.006870	Acc: 85.9% (8588/10000)
[Test]  Epoch: 25	Loss: 0.006880	Acc: 85.8% (8578/10000)
[Test]  Epoch: 26	Loss: 0.006837	Acc: 86.0% (8596/10000)
[Test]  Epoch: 27	Loss: 0.006840	Acc: 85.8% (8584/10000)
[Test]  Epoch: 28	Loss: 0.006793	Acc: 86.0% (8597/10000)
[Test]  Epoch: 29	Loss: 0.006793	Acc: 85.8% (8579/10000)
[Test]  Epoch: 30	Loss: 0.006793	Acc: 85.8% (8578/10000)
[Test]  Epoch: 31	Loss: 0.006768	Acc: 86.0% (8595/10000)
[Test]  Epoch: 32	Loss: 0.006800	Acc: 86.0% (8597/10000)
[Test]  Epoch: 33	Loss: 0.006776	Acc: 85.9% (8590/10000)
[Test]  Epoch: 34	Loss: 0.006757	Acc: 86.0% (8602/10000)
[Test]  Epoch: 35	Loss: 0.006746	Acc: 86.1% (8608/10000)
[Test]  Epoch: 36	Loss: 0.006757	Acc: 86.0% (8596/10000)
[Test]  Epoch: 37	Loss: 0.006779	Acc: 85.9% (8587/10000)
[Test]  Epoch: 38	Loss: 0.006770	Acc: 85.7% (8574/10000)
[Test]  Epoch: 39	Loss: 0.006774	Acc: 85.9% (8590/10000)
[Test]  Epoch: 40	Loss: 0.006773	Acc: 86.0% (8601/10000)
[Test]  Epoch: 41	Loss: 0.006773	Acc: 85.9% (8591/10000)
[Test]  Epoch: 42	Loss: 0.006778	Acc: 85.8% (8576/10000)
[Test]  Epoch: 43	Loss: 0.006756	Acc: 85.9% (8591/10000)
[Test]  Epoch: 44	Loss: 0.006734	Acc: 85.8% (8583/10000)
[Test]  Epoch: 45	Loss: 0.006742	Acc: 85.8% (8582/10000)
[Test]  Epoch: 46	Loss: 0.006721	Acc: 85.9% (8587/10000)
[Test]  Epoch: 47	Loss: 0.006756	Acc: 85.8% (8583/10000)
[Test]  Epoch: 48	Loss: 0.006716	Acc: 85.9% (8593/10000)
[Test]  Epoch: 49	Loss: 0.006751	Acc: 86.0% (8599/10000)
[Test]  Epoch: 50	Loss: 0.006774	Acc: 86.0% (8599/10000)
[Test]  Epoch: 51	Loss: 0.006715	Acc: 85.9% (8593/10000)
[Test]  Epoch: 52	Loss: 0.006707	Acc: 86.1% (8612/10000)
[Test]  Epoch: 53	Loss: 0.006726	Acc: 86.0% (8603/10000)
[Test]  Epoch: 54	Loss: 0.006716	Acc: 86.0% (8602/10000)
[Test]  Epoch: 55	Loss: 0.006723	Acc: 86.0% (8603/10000)
[Test]  Epoch: 56	Loss: 0.006704	Acc: 86.0% (8596/10000)
[Test]  Epoch: 57	Loss: 0.006729	Acc: 86.0% (8599/10000)
[Test]  Epoch: 58	Loss: 0.006720	Acc: 85.9% (8592/10000)
[Test]  Epoch: 59	Loss: 0.006709	Acc: 85.9% (8593/10000)
[Test]  Epoch: 60	Loss: 0.006725	Acc: 86.0% (8604/10000)
[Test]  Epoch: 61	Loss: 0.006722	Acc: 86.0% (8595/10000)
[Test]  Epoch: 62	Loss: 0.006714	Acc: 85.9% (8591/10000)
[Test]  Epoch: 63	Loss: 0.006703	Acc: 85.9% (8589/10000)
[Test]  Epoch: 64	Loss: 0.006698	Acc: 85.9% (8591/10000)
[Test]  Epoch: 65	Loss: 0.006693	Acc: 86.0% (8599/10000)
[Test]  Epoch: 66	Loss: 0.006696	Acc: 85.9% (8593/10000)
[Test]  Epoch: 67	Loss: 0.006674	Acc: 86.0% (8597/10000)
[Test]  Epoch: 68	Loss: 0.006714	Acc: 86.0% (8598/10000)
[Test]  Epoch: 69	Loss: 0.006718	Acc: 85.9% (8589/10000)
[Test]  Epoch: 70	Loss: 0.006689	Acc: 86.0% (8595/10000)
[Test]  Epoch: 71	Loss: 0.006670	Acc: 86.0% (8598/10000)
[Test]  Epoch: 72	Loss: 0.006659	Acc: 86.2% (8615/10000)
[Test]  Epoch: 73	Loss: 0.006679	Acc: 86.0% (8595/10000)
[Test]  Epoch: 74	Loss: 0.006673	Acc: 85.9% (8594/10000)
[Test]  Epoch: 75	Loss: 0.006681	Acc: 86.0% (8599/10000)
[Test]  Epoch: 76	Loss: 0.006671	Acc: 86.0% (8597/10000)
[Test]  Epoch: 77	Loss: 0.006693	Acc: 85.9% (8587/10000)
[Test]  Epoch: 78	Loss: 0.006690	Acc: 86.0% (8602/10000)
[Test]  Epoch: 79	Loss: 0.006703	Acc: 86.0% (8599/10000)
[Test]  Epoch: 80	Loss: 0.006681	Acc: 86.0% (8598/10000)
[Test]  Epoch: 81	Loss: 0.006700	Acc: 86.0% (8596/10000)
[Test]  Epoch: 82	Loss: 0.006704	Acc: 85.9% (8594/10000)
[Test]  Epoch: 83	Loss: 0.006678	Acc: 86.1% (8606/10000)
[Test]  Epoch: 84	Loss: 0.006679	Acc: 86.1% (8607/10000)
[Test]  Epoch: 85	Loss: 0.006691	Acc: 85.9% (8594/10000)
[Test]  Epoch: 86	Loss: 0.006687	Acc: 86.0% (8601/10000)
[Test]  Epoch: 87	Loss: 0.006676	Acc: 86.0% (8602/10000)
[Test]  Epoch: 88	Loss: 0.006675	Acc: 86.0% (8597/10000)
[Test]  Epoch: 89	Loss: 0.006673	Acc: 86.0% (8602/10000)
[Test]  Epoch: 90	Loss: 0.006686	Acc: 86.0% (8595/10000)
[Test]  Epoch: 91	Loss: 0.006673	Acc: 85.9% (8594/10000)
[Test]  Epoch: 92	Loss: 0.006671	Acc: 86.0% (8600/10000)
[Test]  Epoch: 93	Loss: 0.006678	Acc: 86.0% (8598/10000)
[Test]  Epoch: 94	Loss: 0.006676	Acc: 86.0% (8600/10000)
[Test]  Epoch: 95	Loss: 0.006667	Acc: 86.0% (8603/10000)
[Test]  Epoch: 96	Loss: 0.006658	Acc: 86.1% (8613/10000)
[Test]  Epoch: 97	Loss: 0.006678	Acc: 86.1% (8610/10000)
[Test]  Epoch: 98	Loss: 0.006697	Acc: 85.9% (8592/10000)
[Test]  Epoch: 99	Loss: 0.006672	Acc: 86.0% (8595/10000)
[Test]  Epoch: 100	Loss: 0.006689	Acc: 85.9% (8589/10000)
===========finish==========
['2024-08-19', '01:44:29.331565', '100', 'test', '0.006688761995732784', '85.89', '86.15']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.026384	Acc: 70.3% (7033/10000)
[Test]  Epoch: 2	Loss: 0.010904	Acc: 80.1% (8012/10000)
[Test]  Epoch: 3	Loss: 0.009110	Acc: 82.1% (8214/10000)
[Test]  Epoch: 4	Loss: 0.008716	Acc: 82.7% (8265/10000)
[Test]  Epoch: 5	Loss: 0.008363	Acc: 83.2% (8318/10000)
[Test]  Epoch: 6	Loss: 0.008274	Acc: 83.3% (8332/10000)
[Test]  Epoch: 7	Loss: 0.008205	Acc: 83.6% (8356/10000)
[Test]  Epoch: 8	Loss: 0.008164	Acc: 83.5% (8353/10000)
[Test]  Epoch: 9	Loss: 0.008006	Acc: 83.8% (8377/10000)
[Test]  Epoch: 10	Loss: 0.007952	Acc: 83.8% (8384/10000)
[Test]  Epoch: 11	Loss: 0.007974	Acc: 84.0% (8400/10000)
[Test]  Epoch: 12	Loss: 0.008047	Acc: 83.7% (8368/10000)
[Test]  Epoch: 13	Loss: 0.008046	Acc: 83.7% (8371/10000)
[Test]  Epoch: 14	Loss: 0.007917	Acc: 84.0% (8396/10000)
[Test]  Epoch: 15	Loss: 0.007891	Acc: 84.0% (8398/10000)
[Test]  Epoch: 16	Loss: 0.007899	Acc: 83.8% (8376/10000)
[Test]  Epoch: 17	Loss: 0.007774	Acc: 84.0% (8397/10000)
[Test]  Epoch: 18	Loss: 0.007827	Acc: 84.0% (8402/10000)
[Test]  Epoch: 19	Loss: 0.007695	Acc: 84.2% (8423/10000)
[Test]  Epoch: 20	Loss: 0.007652	Acc: 84.2% (8420/10000)
[Test]  Epoch: 21	Loss: 0.007695	Acc: 84.1% (8407/10000)
[Test]  Epoch: 22	Loss: 0.007688	Acc: 84.2% (8417/10000)
[Test]  Epoch: 23	Loss: 0.007655	Acc: 84.3% (8431/10000)
[Test]  Epoch: 24	Loss: 0.007788	Acc: 84.0% (8398/10000)
[Test]  Epoch: 25	Loss: 0.007670	Acc: 84.2% (8417/10000)
[Test]  Epoch: 26	Loss: 0.007675	Acc: 84.3% (8428/10000)
[Test]  Epoch: 27	Loss: 0.007584	Acc: 84.4% (8439/10000)
[Test]  Epoch: 28	Loss: 0.007573	Acc: 84.3% (8429/10000)
[Test]  Epoch: 29	Loss: 0.007622	Acc: 84.2% (8418/10000)
[Test]  Epoch: 30	Loss: 0.007593	Acc: 84.3% (8428/10000)
[Test]  Epoch: 31	Loss: 0.007574	Acc: 84.4% (8441/10000)
[Test]  Epoch: 32	Loss: 0.007580	Acc: 84.4% (8438/10000)
[Test]  Epoch: 33	Loss: 0.007556	Acc: 84.2% (8419/10000)
[Test]  Epoch: 34	Loss: 0.007567	Acc: 84.5% (8445/10000)
[Test]  Epoch: 35	Loss: 0.007752	Acc: 84.1% (8411/10000)
[Test]  Epoch: 36	Loss: 0.007592	Acc: 84.3% (8427/10000)
[Test]  Epoch: 37	Loss: 0.007511	Acc: 84.3% (8434/10000)
[Test]  Epoch: 38	Loss: 0.007745	Acc: 84.1% (8409/10000)
[Test]  Epoch: 39	Loss: 0.007533	Acc: 84.3% (8434/10000)
[Test]  Epoch: 40	Loss: 0.007470	Acc: 84.6% (8459/10000)
[Test]  Epoch: 41	Loss: 0.007471	Acc: 84.4% (8437/10000)
[Test]  Epoch: 42	Loss: 0.007473	Acc: 84.4% (8444/10000)
[Test]  Epoch: 43	Loss: 0.007445	Acc: 84.5% (8449/10000)
[Test]  Epoch: 44	Loss: 0.007463	Acc: 84.5% (8447/10000)
[Test]  Epoch: 45	Loss: 0.007506	Acc: 84.4% (8437/10000)
[Test]  Epoch: 46	Loss: 0.007419	Acc: 84.6% (8457/10000)
[Test]  Epoch: 47	Loss: 0.007442	Acc: 84.6% (8457/10000)
[Test]  Epoch: 48	Loss: 0.007390	Acc: 84.6% (8461/10000)
[Test]  Epoch: 49	Loss: 0.007467	Acc: 84.5% (8451/10000)
[Test]  Epoch: 50	Loss: 0.007450	Acc: 84.5% (8453/10000)
[Test]  Epoch: 51	Loss: 0.007396	Acc: 84.7% (8468/10000)
[Test]  Epoch: 52	Loss: 0.007410	Acc: 84.5% (8451/10000)
[Test]  Epoch: 53	Loss: 0.007450	Acc: 84.7% (8467/10000)
[Test]  Epoch: 54	Loss: 0.007457	Acc: 84.5% (8453/10000)
[Test]  Epoch: 55	Loss: 0.007460	Acc: 84.5% (8449/10000)
[Test]  Epoch: 56	Loss: 0.007411	Acc: 84.6% (8458/10000)
[Test]  Epoch: 57	Loss: 0.007458	Acc: 84.4% (8442/10000)
[Test]  Epoch: 58	Loss: 0.007434	Acc: 84.5% (8446/10000)
[Test]  Epoch: 59	Loss: 0.007497	Acc: 84.5% (8449/10000)
[Test]  Epoch: 60	Loss: 0.007488	Acc: 84.5% (8445/10000)
[Test]  Epoch: 61	Loss: 0.007482	Acc: 84.5% (8453/10000)
[Test]  Epoch: 62	Loss: 0.007457	Acc: 84.6% (8457/10000)
[Test]  Epoch: 63	Loss: 0.007448	Acc: 84.5% (8453/10000)
[Test]  Epoch: 64	Loss: 0.007435	Acc: 84.5% (8454/10000)
[Test]  Epoch: 65	Loss: 0.007423	Acc: 84.6% (8463/10000)
[Test]  Epoch: 66	Loss: 0.007399	Acc: 84.7% (8467/10000)
[Test]  Epoch: 67	Loss: 0.007410	Acc: 84.8% (8476/10000)
[Test]  Epoch: 68	Loss: 0.007417	Acc: 84.6% (8459/10000)
[Test]  Epoch: 69	Loss: 0.007440	Acc: 84.6% (8460/10000)
[Test]  Epoch: 70	Loss: 0.007404	Acc: 84.6% (8462/10000)
[Test]  Epoch: 71	Loss: 0.007400	Acc: 84.6% (8462/10000)
[Test]  Epoch: 72	Loss: 0.007378	Acc: 84.6% (8463/10000)
[Test]  Epoch: 73	Loss: 0.007381	Acc: 84.7% (8469/10000)
[Test]  Epoch: 74	Loss: 0.007387	Acc: 84.7% (8466/10000)
[Test]  Epoch: 75	Loss: 0.007365	Acc: 84.6% (8458/10000)
[Test]  Epoch: 76	Loss: 0.007385	Acc: 84.6% (8461/10000)
[Test]  Epoch: 77	Loss: 0.007378	Acc: 84.8% (8481/10000)
[Test]  Epoch: 78	Loss: 0.007397	Acc: 84.7% (8468/10000)
[Test]  Epoch: 79	Loss: 0.007397	Acc: 84.8% (8478/10000)
[Test]  Epoch: 80	Loss: 0.007389	Acc: 84.7% (8471/10000)
[Test]  Epoch: 81	Loss: 0.007384	Acc: 84.8% (8479/10000)
[Test]  Epoch: 82	Loss: 0.007382	Acc: 84.7% (8470/10000)
[Test]  Epoch: 83	Loss: 0.007381	Acc: 84.7% (8466/10000)
[Test]  Epoch: 84	Loss: 0.007381	Acc: 84.6% (8457/10000)
[Test]  Epoch: 85	Loss: 0.007407	Acc: 84.7% (8472/10000)
[Test]  Epoch: 86	Loss: 0.007406	Acc: 84.7% (8472/10000)
[Test]  Epoch: 87	Loss: 0.007373	Acc: 84.7% (8469/10000)
[Test]  Epoch: 88	Loss: 0.007391	Acc: 84.6% (8464/10000)
[Test]  Epoch: 89	Loss: 0.007383	Acc: 84.7% (8466/10000)
[Test]  Epoch: 90	Loss: 0.007404	Acc: 84.7% (8466/10000)
[Test]  Epoch: 91	Loss: 0.007382	Acc: 84.6% (8463/10000)
[Test]  Epoch: 92	Loss: 0.007365	Acc: 84.7% (8470/10000)
[Test]  Epoch: 93	Loss: 0.007392	Acc: 84.6% (8462/10000)
[Test]  Epoch: 94	Loss: 0.007356	Acc: 84.7% (8474/10000)
[Test]  Epoch: 95	Loss: 0.007360	Acc: 84.7% (8465/10000)
[Test]  Epoch: 96	Loss: 0.007355	Acc: 84.7% (8467/10000)
[Test]  Epoch: 97	Loss: 0.007384	Acc: 84.7% (8474/10000)
[Test]  Epoch: 98	Loss: 0.007396	Acc: 84.8% (8479/10000)
[Test]  Epoch: 99	Loss: 0.007358	Acc: 84.8% (8475/10000)
[Test]  Epoch: 100	Loss: 0.007369	Acc: 84.8% (8477/10000)
===========finish==========
['2024-08-19', '01:46:25.114191', '100', 'test', '0.007368895483016968', '84.77', '84.81']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.0.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.019486	Acc: 75.6% (7563/10000)
[Test]  Epoch: 2	Loss: 0.010040	Acc: 81.8% (8181/10000)
[Test]  Epoch: 3	Loss: 0.009050	Acc: 82.3% (8228/10000)
[Test]  Epoch: 4	Loss: 0.008441	Acc: 83.1% (8310/10000)
[Test]  Epoch: 5	Loss: 0.008206	Acc: 83.5% (8355/10000)
[Test]  Epoch: 6	Loss: 0.008395	Acc: 83.1% (8312/10000)
[Test]  Epoch: 7	Loss: 0.008105	Acc: 83.5% (8346/10000)
[Test]  Epoch: 8	Loss: 0.007875	Acc: 84.0% (8395/10000)
[Test]  Epoch: 9	Loss: 0.007743	Acc: 84.0% (8398/10000)
[Test]  Epoch: 10	Loss: 0.007807	Acc: 83.7% (8374/10000)
[Test]  Epoch: 11	Loss: 0.007784	Acc: 83.9% (8390/10000)
[Test]  Epoch: 12	Loss: 0.008319	Acc: 82.2% (8218/10000)
[Test]  Epoch: 13	Loss: 0.007695	Acc: 84.1% (8407/10000)
[Test]  Epoch: 14	Loss: 0.007732	Acc: 84.0% (8400/10000)
[Test]  Epoch: 15	Loss: 0.007784	Acc: 84.0% (8403/10000)
[Test]  Epoch: 16	Loss: 0.007639	Acc: 84.1% (8409/10000)
[Test]  Epoch: 17	Loss: 0.007570	Acc: 84.3% (8428/10000)
[Test]  Epoch: 18	Loss: 0.007602	Acc: 84.3% (8428/10000)
[Test]  Epoch: 19	Loss: 0.007664	Acc: 84.1% (8412/10000)
[Test]  Epoch: 20	Loss: 0.007667	Acc: 84.0% (8400/10000)
[Test]  Epoch: 21	Loss: 0.007633	Acc: 84.3% (8428/10000)
[Test]  Epoch: 22	Loss: 0.007604	Acc: 84.3% (8434/10000)
[Test]  Epoch: 23	Loss: 0.007545	Acc: 84.4% (8442/10000)
[Test]  Epoch: 24	Loss: 0.007550	Acc: 84.3% (8426/10000)
[Test]  Epoch: 25	Loss: 0.007577	Acc: 84.2% (8420/10000)
[Test]  Epoch: 26	Loss: 0.007491	Acc: 84.4% (8436/10000)
[Test]  Epoch: 27	Loss: 0.007606	Acc: 84.4% (8437/10000)
[Test]  Epoch: 28	Loss: 0.007551	Acc: 84.3% (8428/10000)
[Test]  Epoch: 29	Loss: 0.007570	Acc: 84.5% (8447/10000)
[Test]  Epoch: 30	Loss: 0.007513	Acc: 84.4% (8436/10000)
[Test]  Epoch: 31	Loss: 0.007418	Acc: 84.6% (8457/10000)
[Test]  Epoch: 32	Loss: 0.007488	Acc: 84.5% (8445/10000)
[Test]  Epoch: 33	Loss: 0.007457	Acc: 84.5% (8454/10000)
[Test]  Epoch: 34	Loss: 0.007416	Acc: 84.7% (8470/10000)
[Test]  Epoch: 35	Loss: 0.007460	Acc: 84.5% (8447/10000)
[Test]  Epoch: 36	Loss: 0.007520	Acc: 84.4% (8442/10000)
[Test]  Epoch: 37	Loss: 0.007450	Acc: 84.5% (8450/10000)
[Test]  Epoch: 38	Loss: 0.007492	Acc: 84.4% (8441/10000)
[Test]  Epoch: 39	Loss: 0.007509	Acc: 84.3% (8426/10000)
[Test]  Epoch: 40	Loss: 0.007447	Acc: 84.6% (8461/10000)
[Test]  Epoch: 41	Loss: 0.007518	Acc: 84.3% (8430/10000)
[Test]  Epoch: 42	Loss: 0.007467	Acc: 84.5% (8446/10000)
[Test]  Epoch: 43	Loss: 0.007442	Acc: 84.5% (8451/10000)
[Test]  Epoch: 44	Loss: 0.007449	Acc: 84.4% (8444/10000)
[Test]  Epoch: 45	Loss: 0.007404	Acc: 84.5% (8454/10000)
[Test]  Epoch: 46	Loss: 0.007408	Acc: 84.3% (8431/10000)
[Test]  Epoch: 47	Loss: 0.007410	Acc: 84.6% (8461/10000)
[Test]  Epoch: 48	Loss: 0.007394	Acc: 84.5% (8453/10000)
[Test]  Epoch: 49	Loss: 0.007440	Acc: 84.4% (8444/10000)
[Test]  Epoch: 50	Loss: 0.007457	Acc: 84.5% (8450/10000)
[Test]  Epoch: 51	Loss: 0.007391	Acc: 84.5% (8447/10000)
[Test]  Epoch: 52	Loss: 0.007408	Acc: 84.5% (8455/10000)
[Test]  Epoch: 53	Loss: 0.007415	Acc: 84.6% (8460/10000)
[Test]  Epoch: 54	Loss: 0.007405	Acc: 84.5% (8447/10000)
[Test]  Epoch: 55	Loss: 0.007388	Acc: 84.5% (8448/10000)
[Test]  Epoch: 56	Loss: 0.007360	Acc: 84.5% (8445/10000)
[Test]  Epoch: 57	Loss: 0.007355	Acc: 84.5% (8452/10000)
[Test]  Epoch: 58	Loss: 0.007346	Acc: 84.6% (8464/10000)
[Test]  Epoch: 59	Loss: 0.007406	Acc: 84.5% (8450/10000)
[Test]  Epoch: 60	Loss: 0.007438	Acc: 84.4% (8443/10000)
[Test]  Epoch: 61	Loss: 0.007425	Acc: 84.3% (8435/10000)
[Test]  Epoch: 62	Loss: 0.007438	Acc: 84.4% (8438/10000)
[Test]  Epoch: 63	Loss: 0.007426	Acc: 84.3% (8433/10000)
[Test]  Epoch: 64	Loss: 0.007432	Acc: 84.5% (8451/10000)
[Test]  Epoch: 65	Loss: 0.007417	Acc: 84.5% (8453/10000)
[Test]  Epoch: 66	Loss: 0.007400	Acc: 84.4% (8441/10000)
[Test]  Epoch: 67	Loss: 0.007402	Acc: 84.4% (8444/10000)
[Test]  Epoch: 68	Loss: 0.007434	Acc: 84.3% (8434/10000)
[Test]  Epoch: 69	Loss: 0.007460	Acc: 84.4% (8442/10000)
[Test]  Epoch: 70	Loss: 0.007414	Acc: 84.4% (8441/10000)
[Test]  Epoch: 71	Loss: 0.007382	Acc: 84.5% (8446/10000)
[Test]  Epoch: 72	Loss: 0.007384	Acc: 84.5% (8449/10000)
[Test]  Epoch: 73	Loss: 0.007407	Acc: 84.3% (8426/10000)
[Test]  Epoch: 74	Loss: 0.007384	Acc: 84.3% (8426/10000)
[Test]  Epoch: 75	Loss: 0.007400	Acc: 84.4% (8436/10000)
[Test]  Epoch: 76	Loss: 0.007394	Acc: 84.3% (8427/10000)
[Test]  Epoch: 77	Loss: 0.007392	Acc: 84.4% (8439/10000)
[Test]  Epoch: 78	Loss: 0.007420	Acc: 84.5% (8445/10000)
[Test]  Epoch: 79	Loss: 0.007402	Acc: 84.5% (8446/10000)
[Test]  Epoch: 80	Loss: 0.007391	Acc: 84.6% (8457/10000)
[Test]  Epoch: 81	Loss: 0.007392	Acc: 84.4% (8440/10000)
[Test]  Epoch: 82	Loss: 0.007395	Acc: 84.3% (8429/10000)
[Test]  Epoch: 83	Loss: 0.007399	Acc: 84.5% (8445/10000)
[Test]  Epoch: 84	Loss: 0.007404	Acc: 84.5% (8445/10000)
[Test]  Epoch: 85	Loss: 0.007401	Acc: 84.4% (8442/10000)
[Test]  Epoch: 86	Loss: 0.007406	Acc: 84.5% (8449/10000)
[Test]  Epoch: 87	Loss: 0.007389	Acc: 84.5% (8446/10000)
[Test]  Epoch: 88	Loss: 0.007410	Acc: 84.3% (8432/10000)
[Test]  Epoch: 89	Loss: 0.007383	Acc: 84.5% (8447/10000)
[Test]  Epoch: 90	Loss: 0.007380	Acc: 84.6% (8457/10000)
[Test]  Epoch: 91	Loss: 0.007389	Acc: 84.5% (8446/10000)
[Test]  Epoch: 92	Loss: 0.007359	Acc: 84.6% (8456/10000)
[Test]  Epoch: 93	Loss: 0.007383	Acc: 84.5% (8449/10000)
[Test]  Epoch: 94	Loss: 0.007382	Acc: 84.4% (8440/10000)
[Test]  Epoch: 95	Loss: 0.007377	Acc: 84.5% (8455/10000)
[Test]  Epoch: 96	Loss: 0.007358	Acc: 84.5% (8452/10000)
[Test]  Epoch: 97	Loss: 0.007371	Acc: 84.6% (8456/10000)
[Test]  Epoch: 98	Loss: 0.007409	Acc: 84.5% (8445/10000)
[Test]  Epoch: 99	Loss: 0.007378	Acc: 84.4% (8439/10000)
[Test]  Epoch: 100	Loss: 0.007378	Acc: 84.3% (8435/10000)
===========finish==========
['2024-08-19', '01:48:17.763565', '100', 'test', '0.007378130894899368', '84.35', '84.7']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.0.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.027610	Acc: 61.0% (6096/10000)
[Test]  Epoch: 2	Loss: 0.012524	Acc: 78.0% (7800/10000)
[Test]  Epoch: 3	Loss: 0.010747	Acc: 79.1% (7907/10000)
[Test]  Epoch: 4	Loss: 0.010344	Acc: 79.5% (7955/10000)
[Test]  Epoch: 5	Loss: 0.010324	Acc: 79.6% (7964/10000)
[Test]  Epoch: 6	Loss: 0.009800	Acc: 80.4% (8040/10000)
[Test]  Epoch: 7	Loss: 0.009651	Acc: 80.4% (8044/10000)
[Test]  Epoch: 8	Loss: 0.009722	Acc: 80.4% (8037/10000)
[Test]  Epoch: 9	Loss: 0.009607	Acc: 80.7% (8065/10000)
[Test]  Epoch: 10	Loss: 0.009616	Acc: 80.6% (8061/10000)
[Test]  Epoch: 11	Loss: 0.009497	Acc: 80.9% (8091/10000)
[Test]  Epoch: 12	Loss: 0.009428	Acc: 80.9% (8090/10000)
[Test]  Epoch: 13	Loss: 0.009486	Acc: 81.0% (8103/10000)
[Test]  Epoch: 14	Loss: 0.009420	Acc: 81.0% (8104/10000)
[Test]  Epoch: 15	Loss: 0.009385	Acc: 81.1% (8106/10000)
[Test]  Epoch: 16	Loss: 0.009389	Acc: 81.1% (8113/10000)
[Test]  Epoch: 17	Loss: 0.009362	Acc: 81.2% (8123/10000)
[Test]  Epoch: 18	Loss: 0.009334	Acc: 81.1% (8106/10000)
[Test]  Epoch: 19	Loss: 0.009243	Acc: 81.4% (8137/10000)
[Test]  Epoch: 20	Loss: 0.009234	Acc: 81.4% (8143/10000)
[Test]  Epoch: 21	Loss: 0.009209	Acc: 81.3% (8134/10000)
[Test]  Epoch: 22	Loss: 0.009188	Acc: 81.3% (8130/10000)
[Test]  Epoch: 23	Loss: 0.009272	Acc: 81.2% (8125/10000)
[Test]  Epoch: 24	Loss: 0.009265	Acc: 81.1% (8107/10000)
[Test]  Epoch: 25	Loss: 0.009291	Acc: 81.3% (8134/10000)
[Test]  Epoch: 26	Loss: 0.009218	Acc: 81.4% (8143/10000)
[Test]  Epoch: 27	Loss: 0.009164	Acc: 81.4% (8143/10000)
[Test]  Epoch: 28	Loss: 0.009181	Acc: 81.3% (8132/10000)
[Test]  Epoch: 29	Loss: 0.009224	Acc: 81.5% (8152/10000)
[Test]  Epoch: 30	Loss: 0.009133	Acc: 81.6% (8156/10000)
[Test]  Epoch: 31	Loss: 0.009143	Acc: 81.6% (8156/10000)
[Test]  Epoch: 32	Loss: 0.009143	Acc: 81.5% (8150/10000)
[Test]  Epoch: 33	Loss: 0.009081	Acc: 81.8% (8179/10000)
[Test]  Epoch: 34	Loss: 0.009107	Acc: 81.8% (8176/10000)
[Test]  Epoch: 35	Loss: 0.009048	Acc: 81.6% (8160/10000)
[Test]  Epoch: 36	Loss: 0.009178	Acc: 81.4% (8143/10000)
[Test]  Epoch: 37	Loss: 0.009034	Acc: 81.6% (8161/10000)
[Test]  Epoch: 38	Loss: 0.009150	Acc: 81.5% (8155/10000)
[Test]  Epoch: 39	Loss: 0.009182	Acc: 81.5% (8148/10000)
[Test]  Epoch: 40	Loss: 0.009090	Acc: 81.5% (8154/10000)
[Test]  Epoch: 41	Loss: 0.009106	Acc: 81.6% (8160/10000)
[Test]  Epoch: 42	Loss: 0.009094	Acc: 81.7% (8172/10000)
[Test]  Epoch: 43	Loss: 0.009123	Acc: 81.5% (8154/10000)
[Test]  Epoch: 44	Loss: 0.009061	Acc: 81.4% (8137/10000)
[Test]  Epoch: 45	Loss: 0.009081	Acc: 81.7% (8169/10000)
[Test]  Epoch: 46	Loss: 0.009076	Acc: 81.7% (8168/10000)
[Test]  Epoch: 47	Loss: 0.009119	Acc: 81.7% (8165/10000)
[Test]  Epoch: 48	Loss: 0.009033	Acc: 81.7% (8168/10000)
[Test]  Epoch: 49	Loss: 0.009035	Acc: 81.8% (8177/10000)
[Test]  Epoch: 50	Loss: 0.009017	Acc: 81.9% (8188/10000)
[Test]  Epoch: 51	Loss: 0.008961	Acc: 81.9% (8187/10000)
[Test]  Epoch: 52	Loss: 0.009008	Acc: 81.8% (8184/10000)
[Test]  Epoch: 53	Loss: 0.009009	Acc: 81.6% (8156/10000)
[Test]  Epoch: 54	Loss: 0.008974	Acc: 81.7% (8168/10000)
[Test]  Epoch: 55	Loss: 0.009048	Acc: 81.7% (8167/10000)
[Test]  Epoch: 56	Loss: 0.009070	Acc: 81.6% (8162/10000)
[Test]  Epoch: 57	Loss: 0.008990	Acc: 81.6% (8160/10000)
[Test]  Epoch: 58	Loss: 0.008961	Acc: 81.6% (8161/10000)
[Test]  Epoch: 59	Loss: 0.009016	Acc: 81.7% (8169/10000)
[Test]  Epoch: 60	Loss: 0.009124	Acc: 81.4% (8137/10000)
[Test]  Epoch: 61	Loss: 0.009094	Acc: 81.5% (8148/10000)
[Test]  Epoch: 62	Loss: 0.009061	Acc: 81.5% (8155/10000)
[Test]  Epoch: 63	Loss: 0.009081	Acc: 81.5% (8155/10000)
[Test]  Epoch: 64	Loss: 0.009072	Acc: 81.5% (8146/10000)
[Test]  Epoch: 65	Loss: 0.009030	Acc: 81.5% (8153/10000)
[Test]  Epoch: 66	Loss: 0.008987	Acc: 81.5% (8154/10000)
[Test]  Epoch: 67	Loss: 0.009065	Acc: 81.6% (8161/10000)
[Test]  Epoch: 68	Loss: 0.009017	Acc: 81.5% (8154/10000)
[Test]  Epoch: 69	Loss: 0.009071	Acc: 81.6% (8161/10000)
[Test]  Epoch: 70	Loss: 0.009025	Acc: 81.7% (8172/10000)
[Test]  Epoch: 71	Loss: 0.008997	Acc: 81.7% (8166/10000)
[Test]  Epoch: 72	Loss: 0.009050	Acc: 81.7% (8167/10000)
[Test]  Epoch: 73	Loss: 0.009040	Acc: 81.5% (8154/10000)
[Test]  Epoch: 74	Loss: 0.009025	Acc: 81.6% (8163/10000)
[Test]  Epoch: 75	Loss: 0.009055	Acc: 81.7% (8166/10000)
[Test]  Epoch: 76	Loss: 0.008994	Acc: 81.6% (8163/10000)
[Test]  Epoch: 77	Loss: 0.009025	Acc: 81.7% (8170/10000)
[Test]  Epoch: 78	Loss: 0.009003	Acc: 81.7% (8168/10000)
[Test]  Epoch: 79	Loss: 0.008980	Acc: 81.6% (8160/10000)
[Test]  Epoch: 80	Loss: 0.008976	Acc: 81.7% (8166/10000)
[Test]  Epoch: 81	Loss: 0.009052	Acc: 81.7% (8171/10000)
[Test]  Epoch: 82	Loss: 0.009062	Acc: 81.6% (8161/10000)
[Test]  Epoch: 83	Loss: 0.008988	Acc: 81.7% (8165/10000)
[Test]  Epoch: 84	Loss: 0.009039	Acc: 81.7% (8167/10000)
[Test]  Epoch: 85	Loss: 0.009040	Acc: 81.5% (8152/10000)
[Test]  Epoch: 86	Loss: 0.008956	Acc: 81.6% (8164/10000)
[Test]  Epoch: 87	Loss: 0.009004	Acc: 81.5% (8149/10000)
[Test]  Epoch: 88	Loss: 0.009016	Acc: 81.6% (8160/10000)
[Test]  Epoch: 89	Loss: 0.008987	Acc: 81.7% (8167/10000)
[Test]  Epoch: 90	Loss: 0.008956	Acc: 81.7% (8172/10000)
[Test]  Epoch: 91	Loss: 0.008992	Acc: 81.7% (8167/10000)
[Test]  Epoch: 92	Loss: 0.009017	Acc: 81.7% (8165/10000)
[Test]  Epoch: 93	Loss: 0.008974	Acc: 81.7% (8172/10000)
[Test]  Epoch: 94	Loss: 0.009020	Acc: 81.6% (8162/10000)
[Test]  Epoch: 95	Loss: 0.009010	Acc: 81.7% (8167/10000)
[Test]  Epoch: 96	Loss: 0.008965	Acc: 81.7% (8174/10000)
[Test]  Epoch: 97	Loss: 0.008973	Acc: 81.8% (8183/10000)
[Test]  Epoch: 98	Loss: 0.009059	Acc: 81.7% (8168/10000)
[Test]  Epoch: 99	Loss: 0.009031	Acc: 81.6% (8159/10000)
[Test]  Epoch: 100	Loss: 0.009012	Acc: 81.6% (8156/10000)
===========finish==========
['2024-08-19', '01:50:10.410613', '100', 'test', '0.009012381279468537', '81.56', '81.88']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.0.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.026023	Acc: 61.9% (6193/10000)
[Test]  Epoch: 2	Loss: 0.013496	Acc: 74.4% (7436/10000)
[Test]  Epoch: 3	Loss: 0.012569	Acc: 76.0% (7599/10000)
[Test]  Epoch: 4	Loss: 0.012357	Acc: 75.5% (7554/10000)
[Test]  Epoch: 5	Loss: 0.011527	Acc: 77.4% (7744/10000)
[Test]  Epoch: 6	Loss: 0.011278	Acc: 78.1% (7809/10000)
[Test]  Epoch: 7	Loss: 0.011040	Acc: 78.2% (7823/10000)
[Test]  Epoch: 8	Loss: 0.011570	Acc: 77.6% (7757/10000)
[Test]  Epoch: 9	Loss: 0.010941	Acc: 78.5% (7845/10000)
[Test]  Epoch: 10	Loss: 0.010895	Acc: 78.2% (7817/10000)
[Test]  Epoch: 11	Loss: 0.010874	Acc: 78.5% (7846/10000)
[Test]  Epoch: 12	Loss: 0.011444	Acc: 77.1% (7712/10000)
[Test]  Epoch: 13	Loss: 0.010766	Acc: 78.5% (7849/10000)
[Test]  Epoch: 14	Loss: 0.010667	Acc: 78.9% (7886/10000)
[Test]  Epoch: 15	Loss: 0.010600	Acc: 78.9% (7893/10000)
[Test]  Epoch: 16	Loss: 0.010506	Acc: 78.6% (7861/10000)
[Test]  Epoch: 17	Loss: 0.010483	Acc: 79.0% (7898/10000)
[Test]  Epoch: 18	Loss: 0.010413	Acc: 79.2% (7918/10000)
[Test]  Epoch: 19	Loss: 0.010443	Acc: 79.2% (7924/10000)
[Test]  Epoch: 20	Loss: 0.010364	Acc: 79.3% (7932/10000)
[Test]  Epoch: 21	Loss: 0.010342	Acc: 79.0% (7897/10000)
[Test]  Epoch: 22	Loss: 0.010339	Acc: 79.1% (7914/10000)
[Test]  Epoch: 23	Loss: 0.010307	Acc: 79.2% (7925/10000)
[Test]  Epoch: 24	Loss: 0.010392	Acc: 79.1% (7906/10000)
[Test]  Epoch: 25	Loss: 0.010424	Acc: 79.4% (7941/10000)
[Test]  Epoch: 26	Loss: 0.010453	Acc: 79.2% (7922/10000)
[Test]  Epoch: 27	Loss: 0.010262	Acc: 79.1% (7906/10000)
[Test]  Epoch: 28	Loss: 0.010273	Acc: 79.4% (7938/10000)
[Test]  Epoch: 29	Loss: 0.010213	Acc: 79.3% (7929/10000)
[Test]  Epoch: 30	Loss: 0.010222	Acc: 79.3% (7931/10000)
[Test]  Epoch: 31	Loss: 0.010139	Acc: 79.7% (7966/10000)
[Test]  Epoch: 32	Loss: 0.010188	Acc: 79.3% (7932/10000)
[Test]  Epoch: 33	Loss: 0.010186	Acc: 79.4% (7938/10000)
[Test]  Epoch: 34	Loss: 0.010100	Acc: 79.5% (7951/10000)
[Test]  Epoch: 35	Loss: 0.010192	Acc: 79.4% (7942/10000)
[Test]  Epoch: 36	Loss: 0.010157	Acc: 79.4% (7941/10000)
[Test]  Epoch: 37	Loss: 0.010065	Acc: 79.5% (7951/10000)
[Test]  Epoch: 38	Loss: 0.010104	Acc: 79.5% (7952/10000)
[Test]  Epoch: 39	Loss: 0.010143	Acc: 79.7% (7973/10000)
[Test]  Epoch: 40	Loss: 0.010031	Acc: 79.6% (7962/10000)
[Test]  Epoch: 41	Loss: 0.010092	Acc: 79.7% (7965/10000)
[Test]  Epoch: 42	Loss: 0.010039	Acc: 79.5% (7947/10000)
[Test]  Epoch: 43	Loss: 0.009970	Acc: 79.7% (7973/10000)
[Test]  Epoch: 44	Loss: 0.009983	Acc: 79.8% (7981/10000)
[Test]  Epoch: 45	Loss: 0.009976	Acc: 79.6% (7963/10000)
[Test]  Epoch: 46	Loss: 0.010001	Acc: 79.7% (7969/10000)
[Test]  Epoch: 47	Loss: 0.009929	Acc: 79.7% (7965/10000)
[Test]  Epoch: 48	Loss: 0.009964	Acc: 79.8% (7983/10000)
[Test]  Epoch: 49	Loss: 0.010017	Acc: 79.7% (7972/10000)
[Test]  Epoch: 50	Loss: 0.009971	Acc: 79.8% (7984/10000)
[Test]  Epoch: 51	Loss: 0.009950	Acc: 80.0% (8004/10000)
[Test]  Epoch: 52	Loss: 0.009954	Acc: 79.8% (7978/10000)
[Test]  Epoch: 53	Loss: 0.010036	Acc: 79.7% (7971/10000)
[Test]  Epoch: 54	Loss: 0.009941	Acc: 79.9% (7989/10000)
[Test]  Epoch: 55	Loss: 0.010001	Acc: 79.8% (7981/10000)
[Test]  Epoch: 56	Loss: 0.009980	Acc: 79.9% (7989/10000)
[Test]  Epoch: 57	Loss: 0.009875	Acc: 80.1% (8006/10000)
[Test]  Epoch: 58	Loss: 0.009888	Acc: 79.9% (7988/10000)
[Test]  Epoch: 59	Loss: 0.009889	Acc: 79.7% (7970/10000)
[Test]  Epoch: 60	Loss: 0.009979	Acc: 80.0% (7995/10000)
[Test]  Epoch: 61	Loss: 0.009938	Acc: 80.0% (8004/10000)
[Test]  Epoch: 62	Loss: 0.009916	Acc: 80.0% (8000/10000)
[Test]  Epoch: 63	Loss: 0.009914	Acc: 80.0% (8002/10000)
[Test]  Epoch: 64	Loss: 0.009917	Acc: 80.0% (8000/10000)
[Test]  Epoch: 65	Loss: 0.009885	Acc: 80.0% (7998/10000)
[Test]  Epoch: 66	Loss: 0.009867	Acc: 80.1% (8007/10000)
[Test]  Epoch: 67	Loss: 0.009908	Acc: 80.0% (7999/10000)
[Test]  Epoch: 68	Loss: 0.009906	Acc: 80.0% (8000/10000)
[Test]  Epoch: 69	Loss: 0.009925	Acc: 80.0% (7997/10000)
[Test]  Epoch: 70	Loss: 0.009842	Acc: 80.0% (8003/10000)
[Test]  Epoch: 71	Loss: 0.009881	Acc: 80.2% (8020/10000)
[Test]  Epoch: 72	Loss: 0.009850	Acc: 80.1% (8006/10000)
[Test]  Epoch: 73	Loss: 0.009860	Acc: 80.0% (8001/10000)
[Test]  Epoch: 74	Loss: 0.009881	Acc: 80.2% (8017/10000)
[Test]  Epoch: 75	Loss: 0.009898	Acc: 80.0% (7999/10000)
[Test]  Epoch: 76	Loss: 0.009863	Acc: 80.1% (8011/10000)
[Test]  Epoch: 77	Loss: 0.009900	Acc: 80.1% (8012/10000)
[Test]  Epoch: 78	Loss: 0.009903	Acc: 80.0% (8002/10000)
[Test]  Epoch: 79	Loss: 0.009865	Acc: 80.2% (8016/10000)
[Test]  Epoch: 80	Loss: 0.009881	Acc: 80.1% (8009/10000)
[Test]  Epoch: 81	Loss: 0.009891	Acc: 80.2% (8016/10000)
[Test]  Epoch: 82	Loss: 0.009905	Acc: 79.9% (7992/10000)
[Test]  Epoch: 83	Loss: 0.009875	Acc: 80.1% (8007/10000)
[Test]  Epoch: 84	Loss: 0.009861	Acc: 80.2% (8017/10000)
[Test]  Epoch: 85	Loss: 0.009894	Acc: 80.1% (8007/10000)
[Test]  Epoch: 86	Loss: 0.009904	Acc: 80.1% (8009/10000)
[Test]  Epoch: 87	Loss: 0.009883	Acc: 79.9% (7994/10000)
[Test]  Epoch: 88	Loss: 0.009879	Acc: 80.0% (7998/10000)
[Test]  Epoch: 89	Loss: 0.009844	Acc: 80.1% (8011/10000)
[Test]  Epoch: 90	Loss: 0.009903	Acc: 80.1% (8006/10000)
[Test]  Epoch: 91	Loss: 0.009885	Acc: 80.0% (8001/10000)
[Test]  Epoch: 92	Loss: 0.009856	Acc: 80.1% (8014/10000)
[Test]  Epoch: 93	Loss: 0.009854	Acc: 80.2% (8016/10000)
[Test]  Epoch: 94	Loss: 0.009895	Acc: 80.0% (8002/10000)
[Test]  Epoch: 95	Loss: 0.009856	Acc: 80.0% (8001/10000)
[Test]  Epoch: 96	Loss: 0.009843	Acc: 80.1% (8012/10000)
[Test]  Epoch: 97	Loss: 0.009875	Acc: 80.1% (8006/10000)
[Test]  Epoch: 98	Loss: 0.009908	Acc: 80.0% (8000/10000)
[Test]  Epoch: 99	Loss: 0.009870	Acc: 80.0% (7995/10000)
[Test]  Epoch: 100	Loss: 0.009852	Acc: 80.0% (7999/10000)
===========finish==========
['2024-08-19', '01:52:05.857300', '100', 'test', '0.009852161911129951', '79.99', '80.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.0.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.1.bn1.weight', 'layer2.0.downsample.1.weight', 'last_linear.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.014336	Acc: 72.5% (7246/10000)
[Test]  Epoch: 2	Loss: 0.012689	Acc: 75.6% (7559/10000)
[Test]  Epoch: 3	Loss: 0.012046	Acc: 76.7% (7670/10000)
[Test]  Epoch: 4	Loss: 0.010969	Acc: 77.8% (7785/10000)
[Test]  Epoch: 5	Loss: 0.011008	Acc: 77.9% (7790/10000)
[Test]  Epoch: 6	Loss: 0.010699	Acc: 78.2% (7819/10000)
[Test]  Epoch: 7	Loss: 0.010330	Acc: 78.9% (7891/10000)
[Test]  Epoch: 8	Loss: 0.011206	Acc: 77.6% (7758/10000)
[Test]  Epoch: 9	Loss: 0.010572	Acc: 78.6% (7860/10000)
[Test]  Epoch: 10	Loss: 0.010539	Acc: 79.0% (7899/10000)
[Test]  Epoch: 11	Loss: 0.010498	Acc: 78.7% (7865/10000)
[Test]  Epoch: 12	Loss: 0.010520	Acc: 78.6% (7864/10000)
[Test]  Epoch: 13	Loss: 0.010471	Acc: 78.8% (7878/10000)
[Test]  Epoch: 14	Loss: 0.010226	Acc: 79.0% (7897/10000)
[Test]  Epoch: 15	Loss: 0.010290	Acc: 79.0% (7899/10000)
[Test]  Epoch: 16	Loss: 0.010309	Acc: 79.0% (7895/10000)
[Test]  Epoch: 17	Loss: 0.010198	Acc: 79.1% (7909/10000)
[Test]  Epoch: 18	Loss: 0.010215	Acc: 78.9% (7894/10000)
[Test]  Epoch: 19	Loss: 0.010231	Acc: 78.9% (7894/10000)
[Test]  Epoch: 20	Loss: 0.010208	Acc: 78.9% (7892/10000)
[Test]  Epoch: 21	Loss: 0.010253	Acc: 78.8% (7883/10000)
[Test]  Epoch: 22	Loss: 0.010217	Acc: 78.9% (7892/10000)
[Test]  Epoch: 23	Loss: 0.010173	Acc: 78.7% (7871/10000)
[Test]  Epoch: 24	Loss: 0.010233	Acc: 78.8% (7884/10000)
[Test]  Epoch: 25	Loss: 0.010168	Acc: 79.1% (7909/10000)
[Test]  Epoch: 26	Loss: 0.010115	Acc: 79.4% (7938/10000)
[Test]  Epoch: 27	Loss: 0.010079	Acc: 79.1% (7912/10000)
[Test]  Epoch: 28	Loss: 0.010143	Acc: 79.2% (7923/10000)
[Test]  Epoch: 29	Loss: 0.010133	Acc: 79.4% (7941/10000)
[Test]  Epoch: 30	Loss: 0.010133	Acc: 79.2% (7915/10000)
[Test]  Epoch: 31	Loss: 0.010064	Acc: 79.2% (7916/10000)
[Test]  Epoch: 32	Loss: 0.010161	Acc: 78.9% (7891/10000)
[Test]  Epoch: 33	Loss: 0.010108	Acc: 79.2% (7922/10000)
[Test]  Epoch: 34	Loss: 0.010167	Acc: 79.2% (7916/10000)
[Test]  Epoch: 35	Loss: 0.010088	Acc: 79.2% (7917/10000)
[Test]  Epoch: 36	Loss: 0.010158	Acc: 79.1% (7914/10000)
[Test]  Epoch: 37	Loss: 0.010123	Acc: 79.2% (7920/10000)
[Test]  Epoch: 38	Loss: 0.010286	Acc: 79.2% (7915/10000)
[Test]  Epoch: 39	Loss: 0.010288	Acc: 79.1% (7908/10000)
[Test]  Epoch: 40	Loss: 0.010136	Acc: 79.5% (7948/10000)
[Test]  Epoch: 41	Loss: 0.010115	Acc: 79.2% (7925/10000)
[Test]  Epoch: 42	Loss: 0.010168	Acc: 79.2% (7920/10000)
[Test]  Epoch: 43	Loss: 0.010251	Acc: 79.5% (7945/10000)
[Test]  Epoch: 44	Loss: 0.010140	Acc: 79.2% (7915/10000)
[Test]  Epoch: 45	Loss: 0.010152	Acc: 79.3% (7931/10000)
[Test]  Epoch: 46	Loss: 0.010107	Acc: 79.3% (7926/10000)
[Test]  Epoch: 47	Loss: 0.010147	Acc: 79.3% (7927/10000)
[Test]  Epoch: 48	Loss: 0.010117	Acc: 79.3% (7934/10000)
[Test]  Epoch: 49	Loss: 0.010182	Acc: 79.3% (7929/10000)
[Test]  Epoch: 50	Loss: 0.010176	Acc: 79.3% (7929/10000)
[Test]  Epoch: 51	Loss: 0.010169	Acc: 79.4% (7937/10000)
[Test]  Epoch: 52	Loss: 0.010169	Acc: 79.5% (7945/10000)
[Test]  Epoch: 53	Loss: 0.010239	Acc: 79.4% (7940/10000)
[Test]  Epoch: 54	Loss: 0.010143	Acc: 79.3% (7930/10000)
[Test]  Epoch: 55	Loss: 0.010232	Acc: 79.2% (7915/10000)
[Test]  Epoch: 56	Loss: 0.010166	Acc: 79.3% (7929/10000)
[Test]  Epoch: 57	Loss: 0.010116	Acc: 79.4% (7941/10000)
[Test]  Epoch: 58	Loss: 0.010161	Acc: 79.4% (7940/10000)
[Test]  Epoch: 59	Loss: 0.010115	Acc: 79.5% (7952/10000)
[Test]  Epoch: 60	Loss: 0.010248	Acc: 79.2% (7916/10000)
[Test]  Epoch: 61	Loss: 0.010261	Acc: 79.3% (7927/10000)
[Test]  Epoch: 62	Loss: 0.010265	Acc: 79.3% (7926/10000)
[Test]  Epoch: 63	Loss: 0.010227	Acc: 79.2% (7922/10000)
[Test]  Epoch: 64	Loss: 0.010242	Acc: 79.3% (7934/10000)
[Test]  Epoch: 65	Loss: 0.010237	Acc: 79.4% (7937/10000)
[Test]  Epoch: 66	Loss: 0.010206	Acc: 79.4% (7937/10000)
[Test]  Epoch: 67	Loss: 0.010213	Acc: 79.3% (7927/10000)
[Test]  Epoch: 68	Loss: 0.010230	Acc: 79.4% (7940/10000)
[Test]  Epoch: 69	Loss: 0.010254	Acc: 79.4% (7939/10000)
[Test]  Epoch: 70	Loss: 0.010176	Acc: 79.4% (7938/10000)
[Test]  Epoch: 71	Loss: 0.010168	Acc: 79.4% (7940/10000)
[Test]  Epoch: 72	Loss: 0.010144	Acc: 79.4% (7937/10000)
[Test]  Epoch: 73	Loss: 0.010179	Acc: 79.4% (7939/10000)
[Test]  Epoch: 74	Loss: 0.010162	Acc: 79.3% (7935/10000)
[Test]  Epoch: 75	Loss: 0.010194	Acc: 79.3% (7930/10000)
[Test]  Epoch: 76	Loss: 0.010147	Acc: 79.3% (7934/10000)
[Test]  Epoch: 77	Loss: 0.010178	Acc: 79.3% (7935/10000)
[Test]  Epoch: 78	Loss: 0.010202	Acc: 79.3% (7929/10000)
[Test]  Epoch: 79	Loss: 0.010185	Acc: 79.5% (7951/10000)
[Test]  Epoch: 80	Loss: 0.010198	Acc: 79.5% (7945/10000)
[Test]  Epoch: 81	Loss: 0.010173	Acc: 79.3% (7930/10000)
[Test]  Epoch: 82	Loss: 0.010174	Acc: 79.4% (7944/10000)
[Test]  Epoch: 83	Loss: 0.010171	Acc: 79.6% (7957/10000)
[Test]  Epoch: 84	Loss: 0.010183	Acc: 79.4% (7937/10000)
[Test]  Epoch: 85	Loss: 0.010181	Acc: 79.4% (7940/10000)
[Test]  Epoch: 86	Loss: 0.010162	Acc: 79.5% (7945/10000)
[Test]  Epoch: 87	Loss: 0.010156	Acc: 79.6% (7961/10000)
[Test]  Epoch: 88	Loss: 0.010183	Acc: 79.4% (7941/10000)
[Test]  Epoch: 89	Loss: 0.010185	Acc: 79.5% (7951/10000)
[Test]  Epoch: 90	Loss: 0.010208	Acc: 79.4% (7943/10000)
[Test]  Epoch: 91	Loss: 0.010164	Acc: 79.3% (7933/10000)
[Test]  Epoch: 92	Loss: 0.010146	Acc: 79.6% (7957/10000)
[Test]  Epoch: 93	Loss: 0.010136	Acc: 79.4% (7936/10000)
[Test]  Epoch: 94	Loss: 0.010205	Acc: 79.4% (7944/10000)
[Test]  Epoch: 95	Loss: 0.010184	Acc: 79.5% (7955/10000)
[Test]  Epoch: 96	Loss: 0.010171	Acc: 79.5% (7952/10000)
[Test]  Epoch: 97	Loss: 0.010176	Acc: 79.6% (7956/10000)
[Test]  Epoch: 98	Loss: 0.010177	Acc: 79.4% (7941/10000)
[Test]  Epoch: 99	Loss: 0.010158	Acc: 79.5% (7953/10000)
[Test]  Epoch: 100	Loss: 0.010162	Acc: 79.5% (7946/10000)
===========finish==========
['2024-08-19', '01:53:58.073315', '100', 'test', '0.010162025271356106', '79.46', '79.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.0.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.1.bn1.weight', 'layer2.0.downsample.1.weight', 'last_linear.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.024439	Acc: 51.3% (5132/10000)
[Test]  Epoch: 2	Loss: 0.018525	Acc: 64.0% (6395/10000)
[Test]  Epoch: 3	Loss: 0.016394	Acc: 67.9% (6790/10000)
[Test]  Epoch: 4	Loss: 0.016797	Acc: 67.4% (6739/10000)
[Test]  Epoch: 5	Loss: 0.016320	Acc: 69.0% (6900/10000)
[Test]  Epoch: 6	Loss: 0.015727	Acc: 70.4% (7041/10000)
[Test]  Epoch: 7	Loss: 0.015443	Acc: 70.4% (7040/10000)
[Test]  Epoch: 8	Loss: 0.015157	Acc: 70.9% (7094/10000)
[Test]  Epoch: 9	Loss: 0.015190	Acc: 70.6% (7062/10000)
[Test]  Epoch: 10	Loss: 0.015136	Acc: 70.8% (7084/10000)
[Test]  Epoch: 11	Loss: 0.015287	Acc: 70.5% (7054/10000)
[Test]  Epoch: 12	Loss: 0.014880	Acc: 71.3% (7135/10000)
[Test]  Epoch: 13	Loss: 0.014999	Acc: 71.1% (7113/10000)
[Test]  Epoch: 14	Loss: 0.014684	Acc: 71.4% (7136/10000)
[Test]  Epoch: 15	Loss: 0.014863	Acc: 71.2% (7120/10000)
[Test]  Epoch: 16	Loss: 0.014656	Acc: 71.3% (7127/10000)
[Test]  Epoch: 17	Loss: 0.014590	Acc: 71.8% (7176/10000)
[Test]  Epoch: 18	Loss: 0.014487	Acc: 72.0% (7200/10000)
[Test]  Epoch: 19	Loss: 0.014657	Acc: 71.7% (7170/10000)
[Test]  Epoch: 20	Loss: 0.014592	Acc: 71.9% (7187/10000)
[Test]  Epoch: 21	Loss: 0.014533	Acc: 71.8% (7183/10000)
[Test]  Epoch: 22	Loss: 0.014468	Acc: 71.9% (7192/10000)
[Test]  Epoch: 23	Loss: 0.014442	Acc: 72.0% (7199/10000)
[Test]  Epoch: 24	Loss: 0.014556	Acc: 71.7% (7171/10000)
[Test]  Epoch: 25	Loss: 0.014573	Acc: 72.2% (7216/10000)
[Test]  Epoch: 26	Loss: 0.014454	Acc: 72.0% (7202/10000)
[Test]  Epoch: 27	Loss: 0.014401	Acc: 72.3% (7230/10000)
[Test]  Epoch: 28	Loss: 0.014429	Acc: 72.0% (7202/10000)
[Test]  Epoch: 29	Loss: 0.014454	Acc: 72.0% (7195/10000)
[Test]  Epoch: 30	Loss: 0.014481	Acc: 71.8% (7185/10000)
[Test]  Epoch: 31	Loss: 0.014337	Acc: 72.4% (7237/10000)
[Test]  Epoch: 32	Loss: 0.014397	Acc: 72.4% (7240/10000)
[Test]  Epoch: 33	Loss: 0.014316	Acc: 72.2% (7217/10000)
[Test]  Epoch: 34	Loss: 0.014328	Acc: 72.5% (7251/10000)
[Test]  Epoch: 35	Loss: 0.014334	Acc: 72.4% (7242/10000)
[Test]  Epoch: 36	Loss: 0.014363	Acc: 72.2% (7225/10000)
[Test]  Epoch: 37	Loss: 0.014334	Acc: 72.2% (7225/10000)
[Test]  Epoch: 38	Loss: 0.014354	Acc: 71.8% (7184/10000)
[Test]  Epoch: 39	Loss: 0.014391	Acc: 72.1% (7213/10000)
[Test]  Epoch: 40	Loss: 0.014254	Acc: 72.2% (7223/10000)
[Test]  Epoch: 41	Loss: 0.014261	Acc: 72.1% (7211/10000)
[Test]  Epoch: 42	Loss: 0.014185	Acc: 72.2% (7225/10000)
[Test]  Epoch: 43	Loss: 0.014209	Acc: 72.4% (7242/10000)
[Test]  Epoch: 44	Loss: 0.014222	Acc: 72.3% (7235/10000)
[Test]  Epoch: 45	Loss: 0.014247	Acc: 72.3% (7234/10000)
[Test]  Epoch: 46	Loss: 0.014234	Acc: 72.4% (7239/10000)
[Test]  Epoch: 47	Loss: 0.014267	Acc: 72.5% (7248/10000)
[Test]  Epoch: 48	Loss: 0.014334	Acc: 72.2% (7216/10000)
[Test]  Epoch: 49	Loss: 0.014382	Acc: 72.2% (7219/10000)
[Test]  Epoch: 50	Loss: 0.014292	Acc: 72.5% (7246/10000)
[Test]  Epoch: 51	Loss: 0.014263	Acc: 72.4% (7237/10000)
[Test]  Epoch: 52	Loss: 0.014245	Acc: 72.5% (7247/10000)
[Test]  Epoch: 53	Loss: 0.014286	Acc: 72.4% (7243/10000)
[Test]  Epoch: 54	Loss: 0.014204	Acc: 72.2% (7222/10000)
[Test]  Epoch: 55	Loss: 0.014245	Acc: 72.1% (7209/10000)
[Test]  Epoch: 56	Loss: 0.014234	Acc: 72.4% (7238/10000)
[Test]  Epoch: 57	Loss: 0.014201	Acc: 72.2% (7216/10000)
[Test]  Epoch: 58	Loss: 0.014213	Acc: 72.3% (7227/10000)
[Test]  Epoch: 59	Loss: 0.014306	Acc: 72.2% (7215/10000)
[Test]  Epoch: 60	Loss: 0.014362	Acc: 72.2% (7224/10000)
[Test]  Epoch: 61	Loss: 0.014428	Acc: 72.4% (7237/10000)
[Test]  Epoch: 62	Loss: 0.014377	Acc: 72.2% (7224/10000)
[Test]  Epoch: 63	Loss: 0.014350	Acc: 72.3% (7227/10000)
[Test]  Epoch: 64	Loss: 0.014371	Acc: 72.2% (7220/10000)
[Test]  Epoch: 65	Loss: 0.014327	Acc: 72.3% (7231/10000)
[Test]  Epoch: 66	Loss: 0.014338	Acc: 72.4% (7237/10000)
[Test]  Epoch: 67	Loss: 0.014326	Acc: 72.3% (7229/10000)
[Test]  Epoch: 68	Loss: 0.014336	Acc: 72.2% (7219/10000)
[Test]  Epoch: 69	Loss: 0.014357	Acc: 72.4% (7243/10000)
[Test]  Epoch: 70	Loss: 0.014310	Acc: 72.4% (7241/10000)
[Test]  Epoch: 71	Loss: 0.014358	Acc: 72.4% (7239/10000)
[Test]  Epoch: 72	Loss: 0.014306	Acc: 72.3% (7232/10000)
[Test]  Epoch: 73	Loss: 0.014288	Acc: 72.3% (7232/10000)
[Test]  Epoch: 74	Loss: 0.014252	Acc: 72.4% (7236/10000)
[Test]  Epoch: 75	Loss: 0.014283	Acc: 72.4% (7242/10000)
[Test]  Epoch: 76	Loss: 0.014297	Acc: 72.2% (7224/10000)
[Test]  Epoch: 77	Loss: 0.014274	Acc: 72.4% (7241/10000)
[Test]  Epoch: 78	Loss: 0.014298	Acc: 72.4% (7239/10000)
[Test]  Epoch: 79	Loss: 0.014288	Acc: 72.3% (7235/10000)
[Test]  Epoch: 80	Loss: 0.014291	Acc: 72.4% (7238/10000)
[Test]  Epoch: 81	Loss: 0.014304	Acc: 72.3% (7230/10000)
[Test]  Epoch: 82	Loss: 0.014325	Acc: 72.4% (7238/10000)
[Test]  Epoch: 83	Loss: 0.014304	Acc: 72.4% (7236/10000)
[Test]  Epoch: 84	Loss: 0.014318	Acc: 72.3% (7235/10000)
[Test]  Epoch: 85	Loss: 0.014292	Acc: 72.3% (7231/10000)
[Test]  Epoch: 86	Loss: 0.014281	Acc: 72.3% (7230/10000)
[Test]  Epoch: 87	Loss: 0.014285	Acc: 72.3% (7233/10000)
[Test]  Epoch: 88	Loss: 0.014331	Acc: 72.3% (7233/10000)
[Test]  Epoch: 89	Loss: 0.014333	Acc: 72.4% (7236/10000)
[Test]  Epoch: 90	Loss: 0.014349	Acc: 72.4% (7238/10000)
[Test]  Epoch: 91	Loss: 0.014285	Acc: 72.2% (7217/10000)
[Test]  Epoch: 92	Loss: 0.014306	Acc: 72.3% (7233/10000)
[Test]  Epoch: 93	Loss: 0.014268	Acc: 72.2% (7220/10000)
[Test]  Epoch: 94	Loss: 0.014301	Acc: 72.4% (7242/10000)
[Test]  Epoch: 95	Loss: 0.014297	Acc: 72.4% (7237/10000)
[Test]  Epoch: 96	Loss: 0.014270	Acc: 72.3% (7226/10000)
[Test]  Epoch: 97	Loss: 0.014328	Acc: 72.3% (7234/10000)
[Test]  Epoch: 98	Loss: 0.014309	Acc: 72.4% (7238/10000)
[Test]  Epoch: 99	Loss: 0.014271	Acc: 72.4% (7237/10000)
[Test]  Epoch: 100	Loss: 0.014241	Acc: 72.5% (7247/10000)
===========finish==========
['2024-08-19', '01:55:53.670601', '100', 'test', '0.014241153365373611', '72.47', '72.51']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.0.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.1.bn1.weight', 'layer2.0.downsample.1.weight', 'last_linear.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'bn1.weight', 'layer4.0.downsample.0.weight', 'layer2.0.downsample.0.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.033843	Acc: 29.0% (2903/10000)
[Test]  Epoch: 2	Loss: 0.028616	Acc: 45.6% (4564/10000)
[Test]  Epoch: 3	Loss: 0.023727	Acc: 53.8% (5379/10000)
[Test]  Epoch: 4	Loss: 0.025083	Acc: 52.6% (5263/10000)
[Test]  Epoch: 5	Loss: 0.025047	Acc: 55.1% (5508/10000)
[Test]  Epoch: 6	Loss: 0.023388	Acc: 56.7% (5668/10000)
[Test]  Epoch: 7	Loss: 0.023016	Acc: 56.6% (5664/10000)
[Test]  Epoch: 8	Loss: 0.023659	Acc: 56.8% (5681/10000)
[Test]  Epoch: 9	Loss: 0.022929	Acc: 57.1% (5714/10000)
[Test]  Epoch: 10	Loss: 0.022726	Acc: 58.1% (5806/10000)
[Test]  Epoch: 11	Loss: 0.022771	Acc: 57.5% (5755/10000)
[Test]  Epoch: 12	Loss: 0.022798	Acc: 57.2% (5719/10000)
[Test]  Epoch: 13	Loss: 0.022564	Acc: 57.4% (5740/10000)
[Test]  Epoch: 14	Loss: 0.022557	Acc: 57.7% (5767/10000)
[Test]  Epoch: 15	Loss: 0.022813	Acc: 57.6% (5760/10000)
[Test]  Epoch: 16	Loss: 0.022626	Acc: 57.7% (5772/10000)
[Test]  Epoch: 17	Loss: 0.022469	Acc: 57.8% (5780/10000)
[Test]  Epoch: 18	Loss: 0.022437	Acc: 58.2% (5822/10000)
[Test]  Epoch: 19	Loss: 0.022748	Acc: 57.9% (5794/10000)
[Test]  Epoch: 20	Loss: 0.022547	Acc: 57.9% (5792/10000)
[Test]  Epoch: 21	Loss: 0.022854	Acc: 57.5% (5745/10000)
[Test]  Epoch: 22	Loss: 0.022567	Acc: 57.9% (5785/10000)
[Test]  Epoch: 23	Loss: 0.022432	Acc: 57.8% (5781/10000)
[Test]  Epoch: 24	Loss: 0.022374	Acc: 58.3% (5834/10000)
[Test]  Epoch: 25	Loss: 0.022418	Acc: 58.4% (5835/10000)
[Test]  Epoch: 26	Loss: 0.022452	Acc: 58.0% (5797/10000)
[Test]  Epoch: 27	Loss: 0.022361	Acc: 58.3% (5832/10000)
[Test]  Epoch: 28	Loss: 0.022593	Acc: 58.0% (5798/10000)
[Test]  Epoch: 29	Loss: 0.022576	Acc: 57.9% (5791/10000)
[Test]  Epoch: 30	Loss: 0.022276	Acc: 58.3% (5826/10000)
[Test]  Epoch: 31	Loss: 0.022452	Acc: 58.3% (5832/10000)
[Test]  Epoch: 32	Loss: 0.022378	Acc: 58.5% (5846/10000)
[Test]  Epoch: 33	Loss: 0.022201	Acc: 58.6% (5860/10000)
[Test]  Epoch: 34	Loss: 0.022323	Acc: 58.5% (5853/10000)
[Test]  Epoch: 35	Loss: 0.022391	Acc: 58.4% (5844/10000)
[Test]  Epoch: 36	Loss: 0.022379	Acc: 58.7% (5866/10000)
[Test]  Epoch: 37	Loss: 0.022420	Acc: 58.3% (5828/10000)
[Test]  Epoch: 38	Loss: 0.022945	Acc: 57.8% (5778/10000)
[Test]  Epoch: 39	Loss: 0.023074	Acc: 57.6% (5758/10000)
[Test]  Epoch: 40	Loss: 0.022771	Acc: 58.1% (5815/10000)
[Test]  Epoch: 41	Loss: 0.022658	Acc: 58.5% (5850/10000)
[Test]  Epoch: 42	Loss: 0.022621	Acc: 58.1% (5815/10000)
[Test]  Epoch: 43	Loss: 0.022451	Acc: 58.7% (5874/10000)
[Test]  Epoch: 44	Loss: 0.022384	Acc: 58.5% (5854/10000)
[Test]  Epoch: 45	Loss: 0.022499	Acc: 58.2% (5824/10000)
[Test]  Epoch: 46	Loss: 0.022333	Acc: 58.4% (5837/10000)
[Test]  Epoch: 47	Loss: 0.022299	Acc: 58.5% (5846/10000)
[Test]  Epoch: 48	Loss: 0.022320	Acc: 58.3% (5834/10000)
[Test]  Epoch: 49	Loss: 0.022367	Acc: 58.3% (5826/10000)
[Test]  Epoch: 50	Loss: 0.022498	Acc: 58.5% (5846/10000)
[Test]  Epoch: 51	Loss: 0.022351	Acc: 58.5% (5854/10000)
[Test]  Epoch: 52	Loss: 0.022335	Acc: 58.5% (5852/10000)
[Test]  Epoch: 53	Loss: 0.022542	Acc: 58.4% (5841/10000)
[Test]  Epoch: 54	Loss: 0.022482	Acc: 58.4% (5840/10000)
[Test]  Epoch: 55	Loss: 0.022357	Acc: 58.4% (5840/10000)
[Test]  Epoch: 56	Loss: 0.022544	Acc: 58.4% (5840/10000)
[Test]  Epoch: 57	Loss: 0.022400	Acc: 58.4% (5843/10000)
[Test]  Epoch: 58	Loss: 0.022294	Acc: 58.6% (5857/10000)
[Test]  Epoch: 59	Loss: 0.022390	Acc: 58.6% (5859/10000)
[Test]  Epoch: 60	Loss: 0.022661	Acc: 58.5% (5849/10000)
[Test]  Epoch: 61	Loss: 0.022583	Acc: 58.5% (5849/10000)
[Test]  Epoch: 62	Loss: 0.022541	Acc: 58.8% (5881/10000)
[Test]  Epoch: 63	Loss: 0.022496	Acc: 58.4% (5843/10000)
[Test]  Epoch: 64	Loss: 0.022442	Acc: 58.4% (5842/10000)
[Test]  Epoch: 65	Loss: 0.022357	Acc: 58.7% (5874/10000)
[Test]  Epoch: 66	Loss: 0.022254	Acc: 58.6% (5857/10000)
[Test]  Epoch: 67	Loss: 0.022353	Acc: 58.5% (5852/10000)
[Test]  Epoch: 68	Loss: 0.022303	Acc: 58.8% (5876/10000)
[Test]  Epoch: 69	Loss: 0.022411	Acc: 58.7% (5873/10000)
[Test]  Epoch: 70	Loss: 0.022318	Acc: 58.7% (5871/10000)
[Test]  Epoch: 71	Loss: 0.022330	Acc: 58.7% (5867/10000)
[Test]  Epoch: 72	Loss: 0.022360	Acc: 58.4% (5840/10000)
[Test]  Epoch: 73	Loss: 0.022331	Acc: 58.8% (5881/10000)
[Test]  Epoch: 74	Loss: 0.022281	Acc: 58.6% (5860/10000)
[Test]  Epoch: 75	Loss: 0.022362	Acc: 58.6% (5860/10000)
[Test]  Epoch: 76	Loss: 0.022249	Acc: 58.7% (5866/10000)
[Test]  Epoch: 77	Loss: 0.022278	Acc: 58.5% (5850/10000)
[Test]  Epoch: 78	Loss: 0.022357	Acc: 58.5% (5854/10000)
[Test]  Epoch: 79	Loss: 0.022231	Acc: 58.5% (5847/10000)
[Test]  Epoch: 80	Loss: 0.022280	Acc: 58.7% (5869/10000)
[Test]  Epoch: 81	Loss: 0.022309	Acc: 58.5% (5852/10000)
[Test]  Epoch: 82	Loss: 0.022354	Acc: 58.6% (5863/10000)
[Test]  Epoch: 83	Loss: 0.022194	Acc: 58.6% (5858/10000)
[Test]  Epoch: 84	Loss: 0.022259	Acc: 58.9% (5891/10000)
[Test]  Epoch: 85	Loss: 0.022279	Acc: 58.5% (5846/10000)
[Test]  Epoch: 86	Loss: 0.022244	Acc: 58.6% (5858/10000)
[Test]  Epoch: 87	Loss: 0.022271	Acc: 58.6% (5862/10000)
[Test]  Epoch: 88	Loss: 0.022333	Acc: 58.6% (5865/10000)
[Test]  Epoch: 89	Loss: 0.022319	Acc: 58.7% (5870/10000)
[Test]  Epoch: 90	Loss: 0.022300	Acc: 58.6% (5861/10000)
[Test]  Epoch: 91	Loss: 0.022296	Acc: 58.5% (5855/10000)
[Test]  Epoch: 92	Loss: 0.022240	Acc: 58.7% (5874/10000)
[Test]  Epoch: 93	Loss: 0.022299	Acc: 58.5% (5855/10000)
[Test]  Epoch: 94	Loss: 0.022338	Acc: 58.8% (5877/10000)
[Test]  Epoch: 95	Loss: 0.022237	Acc: 58.7% (5873/10000)
[Test]  Epoch: 96	Loss: 0.022175	Acc: 58.6% (5862/10000)
[Test]  Epoch: 97	Loss: 0.022315	Acc: 58.7% (5869/10000)
[Test]  Epoch: 98	Loss: 0.022462	Acc: 58.6% (5858/10000)
[Test]  Epoch: 99	Loss: 0.022329	Acc: 58.6% (5865/10000)
[Test]  Epoch: 100	Loss: 0.022296	Acc: 58.7% (5867/10000)
===========finish==========
['2024-08-19', '01:57:48.775408', '100', 'test', '0.022295828783512116', '58.67', '58.91']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.0.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.1.bn1.weight', 'layer2.0.downsample.1.weight', 'last_linear.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'bn1.weight', 'layer4.0.downsample.0.weight', 'layer2.0.downsample.0.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer3.0.conv2.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.045203	Acc: 26.0% (2596/10000)
[Test]  Epoch: 2	Loss: 0.028055	Acc: 43.0% (4301/10000)
[Test]  Epoch: 3	Loss: 0.027491	Acc: 47.0% (4702/10000)
[Test]  Epoch: 4	Loss: 0.027077	Acc: 47.9% (4788/10000)
[Test]  Epoch: 5	Loss: 0.027211	Acc: 49.2% (4918/10000)
[Test]  Epoch: 6	Loss: 0.026587	Acc: 50.2% (5019/10000)
[Test]  Epoch: 7	Loss: 0.026102	Acc: 50.8% (5078/10000)
[Test]  Epoch: 8	Loss: 0.026386	Acc: 50.2% (5025/10000)
[Test]  Epoch: 9	Loss: 0.025980	Acc: 51.3% (5127/10000)
[Test]  Epoch: 10	Loss: 0.026009	Acc: 51.2% (5121/10000)
[Test]  Epoch: 11	Loss: 0.025846	Acc: 51.2% (5117/10000)
[Test]  Epoch: 12	Loss: 0.025885	Acc: 51.5% (5146/10000)
[Test]  Epoch: 13	Loss: 0.025735	Acc: 51.8% (5181/10000)
[Test]  Epoch: 14	Loss: 0.025729	Acc: 51.6% (5162/10000)
[Test]  Epoch: 15	Loss: 0.025397	Acc: 51.6% (5163/10000)
[Test]  Epoch: 16	Loss: 0.025040	Acc: 52.1% (5209/10000)
[Test]  Epoch: 17	Loss: 0.025211	Acc: 52.4% (5240/10000)
[Test]  Epoch: 18	Loss: 0.024950	Acc: 52.5% (5249/10000)
[Test]  Epoch: 19	Loss: 0.025165	Acc: 52.8% (5282/10000)
[Test]  Epoch: 20	Loss: 0.025257	Acc: 52.7% (5274/10000)
[Test]  Epoch: 21	Loss: 0.025050	Acc: 52.7% (5269/10000)
[Test]  Epoch: 22	Loss: 0.025275	Acc: 52.5% (5252/10000)
[Test]  Epoch: 23	Loss: 0.024999	Acc: 52.5% (5251/10000)
[Test]  Epoch: 24	Loss: 0.025332	Acc: 52.3% (5232/10000)
[Test]  Epoch: 25	Loss: 0.025363	Acc: 52.9% (5287/10000)
[Test]  Epoch: 26	Loss: 0.025207	Acc: 52.9% (5286/10000)
[Test]  Epoch: 27	Loss: 0.025152	Acc: 52.9% (5291/10000)
[Test]  Epoch: 28	Loss: 0.025171	Acc: 52.7% (5270/10000)
[Test]  Epoch: 29	Loss: 0.025256	Acc: 52.5% (5253/10000)
[Test]  Epoch: 30	Loss: 0.025152	Acc: 52.9% (5286/10000)
[Test]  Epoch: 31	Loss: 0.025232	Acc: 52.6% (5263/10000)
[Test]  Epoch: 32	Loss: 0.025867	Acc: 52.6% (5264/10000)
[Test]  Epoch: 33	Loss: 0.025224	Acc: 52.6% (5264/10000)
[Test]  Epoch: 34	Loss: 0.025135	Acc: 52.4% (5241/10000)
[Test]  Epoch: 35	Loss: 0.025122	Acc: 53.0% (5296/10000)
[Test]  Epoch: 36	Loss: 0.024996	Acc: 52.8% (5284/10000)
[Test]  Epoch: 37	Loss: 0.025230	Acc: 52.7% (5272/10000)
[Test]  Epoch: 38	Loss: 0.025487	Acc: 52.4% (5238/10000)
[Test]  Epoch: 39	Loss: 0.025457	Acc: 52.6% (5262/10000)
[Test]  Epoch: 40	Loss: 0.025327	Acc: 52.5% (5252/10000)
[Test]  Epoch: 41	Loss: 0.025184	Acc: 52.6% (5262/10000)
[Test]  Epoch: 42	Loss: 0.025439	Acc: 52.5% (5255/10000)
[Test]  Epoch: 43	Loss: 0.025481	Acc: 52.4% (5239/10000)
[Test]  Epoch: 44	Loss: 0.025217	Acc: 52.8% (5281/10000)
[Test]  Epoch: 45	Loss: 0.025370	Acc: 52.8% (5283/10000)
[Test]  Epoch: 46	Loss: 0.025145	Acc: 52.8% (5283/10000)
[Test]  Epoch: 47	Loss: 0.025156	Acc: 52.8% (5283/10000)
[Test]  Epoch: 48	Loss: 0.025141	Acc: 52.8% (5278/10000)
[Test]  Epoch: 49	Loss: 0.025280	Acc: 52.9% (5289/10000)
[Test]  Epoch: 50	Loss: 0.025299	Acc: 52.6% (5262/10000)
[Test]  Epoch: 51	Loss: 0.025213	Acc: 52.6% (5256/10000)
[Test]  Epoch: 52	Loss: 0.025193	Acc: 52.5% (5246/10000)
[Test]  Epoch: 53	Loss: 0.025309	Acc: 52.6% (5264/10000)
[Test]  Epoch: 54	Loss: 0.025177	Acc: 52.8% (5275/10000)
[Test]  Epoch: 55	Loss: 0.025082	Acc: 52.9% (5288/10000)
[Test]  Epoch: 56	Loss: 0.025116	Acc: 53.1% (5308/10000)
[Test]  Epoch: 57	Loss: 0.025023	Acc: 52.9% (5288/10000)
[Test]  Epoch: 58	Loss: 0.025109	Acc: 52.9% (5285/10000)
[Test]  Epoch: 59	Loss: 0.025097	Acc: 52.9% (5287/10000)
[Test]  Epoch: 60	Loss: 0.025294	Acc: 52.8% (5279/10000)
[Test]  Epoch: 61	Loss: 0.025206	Acc: 53.2% (5317/10000)
[Test]  Epoch: 62	Loss: 0.025189	Acc: 53.0% (5300/10000)
[Test]  Epoch: 63	Loss: 0.025177	Acc: 53.1% (5307/10000)
[Test]  Epoch: 64	Loss: 0.025199	Acc: 53.1% (5306/10000)
[Test]  Epoch: 65	Loss: 0.025042	Acc: 53.2% (5322/10000)
[Test]  Epoch: 66	Loss: 0.025038	Acc: 53.2% (5321/10000)
[Test]  Epoch: 67	Loss: 0.025143	Acc: 53.2% (5321/10000)
[Test]  Epoch: 68	Loss: 0.025089	Acc: 53.1% (5314/10000)
[Test]  Epoch: 69	Loss: 0.025091	Acc: 53.0% (5305/10000)
[Test]  Epoch: 70	Loss: 0.025014	Acc: 53.0% (5302/10000)
[Test]  Epoch: 71	Loss: 0.025188	Acc: 53.0% (5296/10000)
[Test]  Epoch: 72	Loss: 0.025206	Acc: 53.0% (5303/10000)
[Test]  Epoch: 73	Loss: 0.025123	Acc: 53.2% (5316/10000)
[Test]  Epoch: 74	Loss: 0.025128	Acc: 53.2% (5323/10000)
[Test]  Epoch: 75	Loss: 0.025150	Acc: 53.1% (5306/10000)
[Test]  Epoch: 76	Loss: 0.025042	Acc: 53.1% (5309/10000)
[Test]  Epoch: 77	Loss: 0.025054	Acc: 53.2% (5320/10000)
[Test]  Epoch: 78	Loss: 0.025087	Acc: 53.1% (5308/10000)
[Test]  Epoch: 79	Loss: 0.025042	Acc: 53.1% (5312/10000)
[Test]  Epoch: 80	Loss: 0.025037	Acc: 53.1% (5310/10000)
[Test]  Epoch: 81	Loss: 0.025045	Acc: 53.1% (5315/10000)
[Test]  Epoch: 82	Loss: 0.025084	Acc: 53.2% (5325/10000)
[Test]  Epoch: 83	Loss: 0.025082	Acc: 53.1% (5308/10000)
[Test]  Epoch: 84	Loss: 0.025062	Acc: 53.1% (5313/10000)
[Test]  Epoch: 85	Loss: 0.025007	Acc: 53.2% (5321/10000)
[Test]  Epoch: 86	Loss: 0.025084	Acc: 53.2% (5318/10000)
[Test]  Epoch: 87	Loss: 0.025055	Acc: 53.0% (5305/10000)
[Test]  Epoch: 88	Loss: 0.025128	Acc: 53.0% (5304/10000)
[Test]  Epoch: 89	Loss: 0.025087	Acc: 53.1% (5312/10000)
[Test]  Epoch: 90	Loss: 0.025102	Acc: 53.1% (5307/10000)
[Test]  Epoch: 91	Loss: 0.025079	Acc: 53.1% (5313/10000)
[Test]  Epoch: 92	Loss: 0.025123	Acc: 53.0% (5298/10000)
[Test]  Epoch: 93	Loss: 0.025099	Acc: 53.1% (5310/10000)
[Test]  Epoch: 94	Loss: 0.025170	Acc: 53.1% (5310/10000)
[Test]  Epoch: 95	Loss: 0.025103	Acc: 53.2% (5322/10000)
[Test]  Epoch: 96	Loss: 0.025045	Acc: 53.2% (5319/10000)
[Test]  Epoch: 97	Loss: 0.025095	Acc: 53.2% (5319/10000)
[Test]  Epoch: 98	Loss: 0.025142	Acc: 53.2% (5318/10000)
[Test]  Epoch: 99	Loss: 0.025077	Acc: 53.3% (5327/10000)
[Test]  Epoch: 100	Loss: 0.025058	Acc: 53.2% (5318/10000)
===========finish==========
['2024-08-19', '02:00:00.895451', '100', 'test', '0.0250580020070076', '53.18', '53.27']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['layer3.1.bn1.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv1.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.0.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.1.bn1.weight', 'layer2.0.downsample.1.weight', 'last_linear.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'bn1.weight', 'layer4.0.downsample.0.weight', 'layer2.0.downsample.0.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer3.0.conv2.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight', 'conv1.weight', 'layer2.1.conv1.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer3.0.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.038827	Acc: 30.5% (3054/10000)
[Test]  Epoch: 2	Loss: 0.023947	Acc: 47.3% (4733/10000)
[Test]  Epoch: 3	Loss: 0.023093	Acc: 50.9% (5088/10000)
[Test]  Epoch: 4	Loss: 0.023484	Acc: 51.0% (5096/10000)
[Test]  Epoch: 5	Loss: 0.023997	Acc: 51.5% (5151/10000)
[Test]  Epoch: 6	Loss: 0.024294	Acc: 51.5% (5150/10000)
[Test]  Epoch: 7	Loss: 0.024166	Acc: 51.9% (5188/10000)
[Test]  Epoch: 8	Loss: 0.024298	Acc: 52.1% (5209/10000)
[Test]  Epoch: 9	Loss: 0.024179	Acc: 52.6% (5261/10000)
[Test]  Epoch: 10	Loss: 0.024296	Acc: 52.6% (5259/10000)
[Test]  Epoch: 11	Loss: 0.024125	Acc: 52.5% (5249/10000)
[Test]  Epoch: 12	Loss: 0.024107	Acc: 52.8% (5280/10000)
[Test]  Epoch: 13	Loss: 0.024140	Acc: 53.0% (5301/10000)
[Test]  Epoch: 14	Loss: 0.024164	Acc: 52.7% (5270/10000)
[Test]  Epoch: 15	Loss: 0.024217	Acc: 52.5% (5252/10000)
[Test]  Epoch: 16	Loss: 0.024304	Acc: 52.6% (5258/10000)
[Test]  Epoch: 17	Loss: 0.024112	Acc: 53.2% (5325/10000)
[Test]  Epoch: 18	Loss: 0.024222	Acc: 52.6% (5260/10000)
[Test]  Epoch: 19	Loss: 0.024164	Acc: 52.7% (5268/10000)
[Test]  Epoch: 20	Loss: 0.024013	Acc: 53.2% (5321/10000)
[Test]  Epoch: 21	Loss: 0.023846	Acc: 53.4% (5338/10000)
[Test]  Epoch: 22	Loss: 0.023701	Acc: 53.4% (5344/10000)
[Test]  Epoch: 23	Loss: 0.023695	Acc: 53.4% (5339/10000)
[Test]  Epoch: 24	Loss: 0.023685	Acc: 53.8% (5383/10000)
[Test]  Epoch: 25	Loss: 0.023677	Acc: 53.7% (5366/10000)
[Test]  Epoch: 26	Loss: 0.023722	Acc: 53.5% (5353/10000)
[Test]  Epoch: 27	Loss: 0.023709	Acc: 53.6% (5363/10000)
[Test]  Epoch: 28	Loss: 0.023741	Acc: 53.5% (5353/10000)
[Test]  Epoch: 29	Loss: 0.023915	Acc: 53.4% (5340/10000)
[Test]  Epoch: 30	Loss: 0.023629	Acc: 53.7% (5367/10000)
[Test]  Epoch: 31	Loss: 0.023550	Acc: 54.2% (5425/10000)
[Test]  Epoch: 32	Loss: 0.023612	Acc: 54.0% (5398/10000)
[Test]  Epoch: 33	Loss: 0.023599	Acc: 53.8% (5384/10000)
[Test]  Epoch: 34	Loss: 0.023775	Acc: 53.9% (5388/10000)
[Test]  Epoch: 35	Loss: 0.023602	Acc: 54.0% (5404/10000)
[Test]  Epoch: 36	Loss: 0.023562	Acc: 54.2% (5422/10000)
[Test]  Epoch: 37	Loss: 0.023695	Acc: 53.9% (5385/10000)
[Test]  Epoch: 38	Loss: 0.023707	Acc: 53.9% (5389/10000)
[Test]  Epoch: 39	Loss: 0.023525	Acc: 54.2% (5421/10000)
[Test]  Epoch: 40	Loss: 0.023574	Acc: 54.0% (5401/10000)
[Test]  Epoch: 41	Loss: 0.023529	Acc: 54.1% (5410/10000)
[Test]  Epoch: 42	Loss: 0.023499	Acc: 54.4% (5439/10000)
[Test]  Epoch: 43	Loss: 0.023525	Acc: 54.4% (5442/10000)
[Test]  Epoch: 44	Loss: 0.023505	Acc: 54.2% (5416/10000)
[Test]  Epoch: 45	Loss: 0.023606	Acc: 54.3% (5429/10000)
[Test]  Epoch: 46	Loss: 0.023440	Acc: 54.4% (5437/10000)
[Test]  Epoch: 47	Loss: 0.023519	Acc: 54.0% (5399/10000)
[Test]  Epoch: 48	Loss: 0.023656	Acc: 54.2% (5421/10000)
[Test]  Epoch: 49	Loss: 0.023665	Acc: 54.4% (5444/10000)
[Test]  Epoch: 50	Loss: 0.023747	Acc: 54.4% (5435/10000)
[Test]  Epoch: 51	Loss: 0.023566	Acc: 54.6% (5465/10000)
[Test]  Epoch: 52	Loss: 0.023581	Acc: 54.7% (5471/10000)
[Test]  Epoch: 53	Loss: 0.023503	Acc: 54.6% (5463/10000)
[Test]  Epoch: 54	Loss: 0.023521	Acc: 54.5% (5448/10000)
[Test]  Epoch: 55	Loss: 0.023390	Acc: 54.4% (5440/10000)
[Test]  Epoch: 56	Loss: 0.023499	Acc: 54.2% (5424/10000)
[Test]  Epoch: 57	Loss: 0.023382	Acc: 54.7% (5470/10000)
[Test]  Epoch: 58	Loss: 0.023450	Acc: 54.5% (5447/10000)
[Test]  Epoch: 59	Loss: 0.023531	Acc: 54.3% (5434/10000)
[Test]  Epoch: 60	Loss: 0.023541	Acc: 54.5% (5445/10000)
[Test]  Epoch: 61	Loss: 0.023473	Acc: 54.5% (5446/10000)
[Test]  Epoch: 62	Loss: 0.023402	Acc: 54.4% (5442/10000)
[Test]  Epoch: 63	Loss: 0.023447	Acc: 54.4% (5444/10000)
[Test]  Epoch: 64	Loss: 0.023491	Acc: 54.4% (5435/10000)
[Test]  Epoch: 65	Loss: 0.023416	Acc: 54.5% (5454/10000)
[Test]  Epoch: 66	Loss: 0.023341	Acc: 54.6% (5458/10000)
[Test]  Epoch: 67	Loss: 0.023392	Acc: 54.2% (5416/10000)
[Test]  Epoch: 68	Loss: 0.023360	Acc: 54.7% (5466/10000)
[Test]  Epoch: 69	Loss: 0.023424	Acc: 54.6% (5462/10000)
[Test]  Epoch: 70	Loss: 0.023366	Acc: 54.7% (5468/10000)
[Test]  Epoch: 71	Loss: 0.023406	Acc: 54.4% (5439/10000)
[Test]  Epoch: 72	Loss: 0.023441	Acc: 54.3% (5432/10000)
[Test]  Epoch: 73	Loss: 0.023371	Acc: 54.3% (5427/10000)
[Test]  Epoch: 74	Loss: 0.023261	Acc: 54.7% (5467/10000)
[Test]  Epoch: 75	Loss: 0.023287	Acc: 54.6% (5456/10000)
[Test]  Epoch: 76	Loss: 0.023299	Acc: 54.6% (5460/10000)
[Test]  Epoch: 77	Loss: 0.023310	Acc: 54.6% (5461/10000)
[Test]  Epoch: 78	Loss: 0.023337	Acc: 54.6% (5462/10000)
[Test]  Epoch: 79	Loss: 0.023314	Acc: 54.8% (5479/10000)
[Test]  Epoch: 80	Loss: 0.023334	Acc: 54.5% (5450/10000)
[Test]  Epoch: 81	Loss: 0.023344	Acc: 54.5% (5449/10000)
[Test]  Epoch: 82	Loss: 0.023355	Acc: 54.7% (5468/10000)
[Test]  Epoch: 83	Loss: 0.023318	Acc: 54.6% (5463/10000)
[Test]  Epoch: 84	Loss: 0.023312	Acc: 54.5% (5448/10000)
[Test]  Epoch: 85	Loss: 0.023323	Acc: 54.6% (5457/10000)
[Test]  Epoch: 86	Loss: 0.023325	Acc: 54.7% (5473/10000)
[Test]  Epoch: 87	Loss: 0.023334	Acc: 54.6% (5459/10000)
[Test]  Epoch: 88	Loss: 0.023393	Acc: 54.5% (5454/10000)
[Test]  Epoch: 89	Loss: 0.023371	Acc: 54.5% (5453/10000)
[Test]  Epoch: 90	Loss: 0.023402	Acc: 54.5% (5452/10000)
[Test]  Epoch: 91	Loss: 0.023399	Acc: 54.6% (5460/10000)
[Test]  Epoch: 92	Loss: 0.023333	Acc: 54.5% (5446/10000)
[Test]  Epoch: 93	Loss: 0.023387	Acc: 54.6% (5463/10000)
[Test]  Epoch: 94	Loss: 0.023381	Acc: 54.4% (5443/10000)
[Test]  Epoch: 95	Loss: 0.023328	Acc: 54.5% (5454/10000)
[Test]  Epoch: 96	Loss: 0.023270	Acc: 54.7% (5471/10000)
[Test]  Epoch: 97	Loss: 0.023373	Acc: 54.8% (5478/10000)
[Test]  Epoch: 98	Loss: 0.023461	Acc: 54.7% (5471/10000)
[Test]  Epoch: 99	Loss: 0.023372	Acc: 54.6% (5459/10000)
[Test]  Epoch: 100	Loss: 0.023315	Acc: 54.6% (5457/10000)
===========finish==========
['2024-08-19', '02:02:16.119888', '100', 'test', '0.023314577144384382', '54.57', '54.79']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.008434	Acc: 81.6% (8157/10000)
[Test]  Epoch: 2	Loss: 0.008339	Acc: 81.9% (8189/10000)
[Test]  Epoch: 3	Loss: 0.008344	Acc: 81.9% (8187/10000)
[Test]  Epoch: 4	Loss: 0.008346	Acc: 82.0% (8198/10000)
[Test]  Epoch: 5	Loss: 0.008299	Acc: 82.1% (8211/10000)
[Test]  Epoch: 6	Loss: 0.008318	Acc: 82.1% (8208/10000)
[Test]  Epoch: 7	Loss: 0.008331	Acc: 82.2% (8216/10000)
[Test]  Epoch: 8	Loss: 0.008321	Acc: 82.0% (8203/10000)
[Test]  Epoch: 9	Loss: 0.008357	Acc: 82.0% (8203/10000)
[Test]  Epoch: 10	Loss: 0.008251	Acc: 82.2% (8220/10000)
[Test]  Epoch: 11	Loss: 0.008270	Acc: 82.1% (8213/10000)
[Test]  Epoch: 12	Loss: 0.008223	Acc: 82.3% (8233/10000)
[Test]  Epoch: 13	Loss: 0.008222	Acc: 82.3% (8229/10000)
[Test]  Epoch: 14	Loss: 0.008215	Acc: 82.5% (8248/10000)
[Test]  Epoch: 15	Loss: 0.008206	Acc: 82.4% (8244/10000)
[Test]  Epoch: 16	Loss: 0.008292	Acc: 82.2% (8216/10000)
[Test]  Epoch: 17	Loss: 0.008198	Acc: 82.2% (8225/10000)
[Test]  Epoch: 18	Loss: 0.008257	Acc: 82.1% (8209/10000)
[Test]  Epoch: 19	Loss: 0.008192	Acc: 82.3% (8228/10000)
[Test]  Epoch: 20	Loss: 0.008179	Acc: 82.3% (8227/10000)
[Test]  Epoch: 21	Loss: 0.008194	Acc: 82.4% (8237/10000)
[Test]  Epoch: 22	Loss: 0.008195	Acc: 82.5% (8245/10000)
[Test]  Epoch: 23	Loss: 0.008227	Acc: 82.4% (8238/10000)
[Test]  Epoch: 24	Loss: 0.008233	Acc: 82.4% (8240/10000)
[Test]  Epoch: 25	Loss: 0.008183	Acc: 82.5% (8245/10000)
[Test]  Epoch: 26	Loss: 0.008217	Acc: 82.5% (8247/10000)
[Test]  Epoch: 27	Loss: 0.008271	Acc: 82.3% (8233/10000)
[Test]  Epoch: 28	Loss: 0.008251	Acc: 82.3% (8229/10000)
[Test]  Epoch: 29	Loss: 0.008253	Acc: 82.4% (8237/10000)
[Test]  Epoch: 30	Loss: 0.008219	Acc: 82.4% (8243/10000)
[Test]  Epoch: 31	Loss: 0.008182	Acc: 82.4% (8242/10000)
[Test]  Epoch: 32	Loss: 0.008217	Acc: 82.4% (8236/10000)
[Test]  Epoch: 33	Loss: 0.008250	Acc: 82.4% (8240/10000)
[Test]  Epoch: 34	Loss: 0.008178	Acc: 82.5% (8250/10000)
[Test]  Epoch: 35	Loss: 0.008203	Acc: 82.3% (8229/10000)
[Test]  Epoch: 36	Loss: 0.008219	Acc: 82.4% (8243/10000)
[Test]  Epoch: 37	Loss: 0.008213	Acc: 82.4% (8237/10000)
[Test]  Epoch: 38	Loss: 0.008250	Acc: 82.5% (8248/10000)
[Test]  Epoch: 39	Loss: 0.008249	Acc: 82.1% (8208/10000)
[Test]  Epoch: 40	Loss: 0.008230	Acc: 82.2% (8223/10000)
[Test]  Epoch: 41	Loss: 0.008209	Acc: 82.2% (8223/10000)
[Test]  Epoch: 42	Loss: 0.008258	Acc: 82.2% (8218/10000)
[Test]  Epoch: 43	Loss: 0.008252	Acc: 82.3% (8229/10000)
[Test]  Epoch: 44	Loss: 0.008265	Acc: 82.2% (8215/10000)
[Test]  Epoch: 45	Loss: 0.008246	Acc: 82.3% (8232/10000)
[Test]  Epoch: 46	Loss: 0.008261	Acc: 82.2% (8223/10000)
[Test]  Epoch: 47	Loss: 0.008227	Acc: 82.5% (8248/10000)
[Test]  Epoch: 48	Loss: 0.008230	Acc: 82.5% (8247/10000)
[Test]  Epoch: 49	Loss: 0.008211	Acc: 82.3% (8229/10000)
[Test]  Epoch: 50	Loss: 0.008224	Acc: 82.3% (8229/10000)
[Test]  Epoch: 51	Loss: 0.008237	Acc: 82.4% (8236/10000)
[Test]  Epoch: 52	Loss: 0.008241	Acc: 82.2% (8215/10000)
[Test]  Epoch: 53	Loss: 0.008260	Acc: 82.4% (8239/10000)
[Test]  Epoch: 54	Loss: 0.008225	Acc: 82.4% (8244/10000)
[Test]  Epoch: 55	Loss: 0.008268	Acc: 82.2% (8225/10000)
[Test]  Epoch: 56	Loss: 0.008252	Acc: 82.3% (8227/10000)
[Test]  Epoch: 57	Loss: 0.008280	Acc: 82.6% (8256/10000)
[Test]  Epoch: 58	Loss: 0.008269	Acc: 82.4% (8242/10000)
[Test]  Epoch: 59	Loss: 0.008289	Acc: 82.2% (8220/10000)
[Test]  Epoch: 60	Loss: 0.008273	Acc: 82.2% (8223/10000)
[Test]  Epoch: 61	Loss: 0.008263	Acc: 82.2% (8220/10000)
[Test]  Epoch: 62	Loss: 0.008277	Acc: 82.2% (8220/10000)
[Test]  Epoch: 63	Loss: 0.008286	Acc: 82.2% (8219/10000)
[Test]  Epoch: 64	Loss: 0.008237	Acc: 82.2% (8222/10000)
[Test]  Epoch: 65	Loss: 0.008245	Acc: 82.2% (8219/10000)
[Test]  Epoch: 66	Loss: 0.008253	Acc: 82.3% (8227/10000)
[Test]  Epoch: 67	Loss: 0.008244	Acc: 82.3% (8227/10000)
[Test]  Epoch: 68	Loss: 0.008246	Acc: 82.3% (8230/10000)
[Test]  Epoch: 69	Loss: 0.008269	Acc: 82.2% (8220/10000)
[Test]  Epoch: 70	Loss: 0.008240	Acc: 82.3% (8233/10000)
[Test]  Epoch: 71	Loss: 0.008237	Acc: 82.3% (8233/10000)
[Test]  Epoch: 72	Loss: 0.008226	Acc: 82.3% (8226/10000)
[Test]  Epoch: 73	Loss: 0.008256	Acc: 82.3% (8231/10000)
[Test]  Epoch: 74	Loss: 0.008250	Acc: 82.3% (8233/10000)
[Test]  Epoch: 75	Loss: 0.008251	Acc: 82.2% (8223/10000)
[Test]  Epoch: 76	Loss: 0.008235	Acc: 82.3% (8230/10000)
[Test]  Epoch: 77	Loss: 0.008240	Acc: 82.3% (8230/10000)
[Test]  Epoch: 78	Loss: 0.008222	Acc: 82.3% (8228/10000)
[Test]  Epoch: 79	Loss: 0.008241	Acc: 82.4% (8237/10000)
[Test]  Epoch: 80	Loss: 0.008254	Acc: 82.3% (8231/10000)
[Test]  Epoch: 81	Loss: 0.008231	Acc: 82.4% (8237/10000)
[Test]  Epoch: 82	Loss: 0.008248	Acc: 82.3% (8233/10000)
[Test]  Epoch: 83	Loss: 0.008234	Acc: 82.4% (8238/10000)
[Test]  Epoch: 84	Loss: 0.008236	Acc: 82.4% (8236/10000)
[Test]  Epoch: 85	Loss: 0.008266	Acc: 82.3% (8230/10000)
[Test]  Epoch: 86	Loss: 0.008237	Acc: 82.4% (8242/10000)
[Test]  Epoch: 87	Loss: 0.008255	Acc: 82.2% (8223/10000)
[Test]  Epoch: 88	Loss: 0.008243	Acc: 82.3% (8234/10000)
[Test]  Epoch: 89	Loss: 0.008234	Acc: 82.4% (8239/10000)
[Test]  Epoch: 90	Loss: 0.008242	Acc: 82.3% (8226/10000)
[Test]  Epoch: 91	Loss: 0.008245	Acc: 82.2% (8221/10000)
[Test]  Epoch: 92	Loss: 0.008229	Acc: 82.4% (8239/10000)
[Test]  Epoch: 93	Loss: 0.008244	Acc: 82.3% (8227/10000)
[Test]  Epoch: 94	Loss: 0.008259	Acc: 82.2% (8225/10000)
[Test]  Epoch: 95	Loss: 0.008242	Acc: 82.3% (8227/10000)
[Test]  Epoch: 96	Loss: 0.008233	Acc: 82.3% (8231/10000)
[Test]  Epoch: 97	Loss: 0.008210	Acc: 82.4% (8244/10000)
[Test]  Epoch: 98	Loss: 0.008247	Acc: 82.2% (8216/10000)
[Test]  Epoch: 99	Loss: 0.008254	Acc: 82.4% (8239/10000)
[Test]  Epoch: 100	Loss: 0.008268	Acc: 82.3% (8227/10000)
===========finish==========
['2024-08-19', '02:04:15.819381', '100', 'test', '0.008268197959661484', '82.27', '82.56']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.032103	Acc: 40.7% (4070/10000)
[Test]  Epoch: 2	Loss: 0.022076	Acc: 52.7% (5272/10000)
[Test]  Epoch: 3	Loss: 0.020539	Acc: 55.2% (5519/10000)
[Test]  Epoch: 4	Loss: 0.020344	Acc: 56.4% (5635/10000)
[Test]  Epoch: 5	Loss: 0.019640	Acc: 57.6% (5759/10000)
[Test]  Epoch: 6	Loss: 0.019879	Acc: 57.4% (5742/10000)
[Test]  Epoch: 7	Loss: 0.019735	Acc: 57.5% (5747/10000)
[Test]  Epoch: 8	Loss: 0.019836	Acc: 57.6% (5758/10000)
[Test]  Epoch: 9	Loss: 0.019878	Acc: 57.6% (5763/10000)
[Test]  Epoch: 10	Loss: 0.019841	Acc: 57.7% (5772/10000)
[Test]  Epoch: 11	Loss: 0.019956	Acc: 57.8% (5782/10000)
[Test]  Epoch: 12	Loss: 0.019542	Acc: 58.4% (5836/10000)
[Test]  Epoch: 13	Loss: 0.019828	Acc: 57.9% (5786/10000)
[Test]  Epoch: 14	Loss: 0.019746	Acc: 58.5% (5845/10000)
[Test]  Epoch: 15	Loss: 0.019894	Acc: 58.0% (5804/10000)
[Test]  Epoch: 16	Loss: 0.019969	Acc: 57.6% (5764/10000)
[Test]  Epoch: 17	Loss: 0.019870	Acc: 57.6% (5758/10000)
[Test]  Epoch: 18	Loss: 0.019859	Acc: 57.9% (5787/10000)
[Test]  Epoch: 19	Loss: 0.019626	Acc: 58.5% (5850/10000)
[Test]  Epoch: 20	Loss: 0.019588	Acc: 58.4% (5844/10000)
[Test]  Epoch: 21	Loss: 0.019979	Acc: 57.6% (5758/10000)
[Test]  Epoch: 22	Loss: 0.019629	Acc: 58.3% (5830/10000)
[Test]  Epoch: 23	Loss: 0.019603	Acc: 58.4% (5840/10000)
[Test]  Epoch: 24	Loss: 0.019597	Acc: 58.5% (5847/10000)
[Test]  Epoch: 25	Loss: 0.019783	Acc: 58.1% (5810/10000)
[Test]  Epoch: 26	Loss: 0.019712	Acc: 58.2% (5825/10000)
[Test]  Epoch: 27	Loss: 0.019901	Acc: 57.9% (5789/10000)
[Test]  Epoch: 28	Loss: 0.019441	Acc: 58.8% (5883/10000)
[Test]  Epoch: 29	Loss: 0.019420	Acc: 58.7% (5869/10000)
[Test]  Epoch: 30	Loss: 0.019356	Acc: 58.6% (5863/10000)
[Test]  Epoch: 31	Loss: 0.019299	Acc: 58.4% (5835/10000)
[Test]  Epoch: 32	Loss: 0.019185	Acc: 58.8% (5883/10000)
[Test]  Epoch: 33	Loss: 0.019227	Acc: 58.8% (5880/10000)
[Test]  Epoch: 34	Loss: 0.019387	Acc: 58.9% (5887/10000)
[Test]  Epoch: 35	Loss: 0.019305	Acc: 59.0% (5902/10000)
[Test]  Epoch: 36	Loss: 0.019086	Acc: 59.2% (5922/10000)
[Test]  Epoch: 37	Loss: 0.019261	Acc: 58.8% (5879/10000)
[Test]  Epoch: 38	Loss: 0.019370	Acc: 59.2% (5919/10000)
[Test]  Epoch: 39	Loss: 0.019046	Acc: 59.4% (5936/10000)
[Test]  Epoch: 40	Loss: 0.019056	Acc: 59.1% (5910/10000)
[Test]  Epoch: 41	Loss: 0.019037	Acc: 59.2% (5922/10000)
[Test]  Epoch: 42	Loss: 0.019185	Acc: 58.7% (5874/10000)
[Test]  Epoch: 43	Loss: 0.019151	Acc: 59.4% (5938/10000)
[Test]  Epoch: 44	Loss: 0.019026	Acc: 59.2% (5922/10000)
[Test]  Epoch: 45	Loss: 0.019122	Acc: 59.1% (5909/10000)
[Test]  Epoch: 46	Loss: 0.019182	Acc: 59.0% (5898/10000)
[Test]  Epoch: 47	Loss: 0.018912	Acc: 59.4% (5940/10000)
[Test]  Epoch: 48	Loss: 0.019037	Acc: 59.4% (5938/10000)
[Test]  Epoch: 49	Loss: 0.019114	Acc: 59.4% (5941/10000)
[Test]  Epoch: 50	Loss: 0.019117	Acc: 59.0% (5904/10000)
[Test]  Epoch: 51	Loss: 0.019085	Acc: 58.9% (5889/10000)
[Test]  Epoch: 52	Loss: 0.018951	Acc: 59.3% (5934/10000)
[Test]  Epoch: 53	Loss: 0.019036	Acc: 59.6% (5957/10000)
[Test]  Epoch: 54	Loss: 0.018946	Acc: 59.4% (5937/10000)
[Test]  Epoch: 55	Loss: 0.019016	Acc: 59.2% (5920/10000)
[Test]  Epoch: 56	Loss: 0.018797	Acc: 59.5% (5953/10000)
[Test]  Epoch: 57	Loss: 0.018838	Acc: 59.3% (5928/10000)
[Test]  Epoch: 58	Loss: 0.018925	Acc: 59.1% (5911/10000)
[Test]  Epoch: 59	Loss: 0.018895	Acc: 59.0% (5903/10000)
[Test]  Epoch: 60	Loss: 0.018810	Acc: 59.7% (5973/10000)
[Test]  Epoch: 61	Loss: 0.018786	Acc: 59.6% (5964/10000)
[Test]  Epoch: 62	Loss: 0.018761	Acc: 59.5% (5949/10000)
[Test]  Epoch: 63	Loss: 0.018734	Acc: 59.6% (5961/10000)
[Test]  Epoch: 64	Loss: 0.018757	Acc: 59.6% (5963/10000)
[Test]  Epoch: 65	Loss: 0.018748	Acc: 59.8% (5975/10000)
[Test]  Epoch: 66	Loss: 0.018759	Acc: 59.5% (5945/10000)
[Test]  Epoch: 67	Loss: 0.018819	Acc: 59.4% (5944/10000)
[Test]  Epoch: 68	Loss: 0.018759	Acc: 59.6% (5965/10000)
[Test]  Epoch: 69	Loss: 0.018759	Acc: 59.4% (5944/10000)
[Test]  Epoch: 70	Loss: 0.018758	Acc: 59.6% (5961/10000)
[Test]  Epoch: 71	Loss: 0.018767	Acc: 59.5% (5952/10000)
[Test]  Epoch: 72	Loss: 0.018779	Acc: 59.9% (5990/10000)
[Test]  Epoch: 73	Loss: 0.018769	Acc: 59.6% (5962/10000)
[Test]  Epoch: 74	Loss: 0.018724	Acc: 59.7% (5966/10000)
[Test]  Epoch: 75	Loss: 0.018724	Acc: 59.7% (5967/10000)
[Test]  Epoch: 76	Loss: 0.018725	Acc: 59.6% (5957/10000)
[Test]  Epoch: 77	Loss: 0.018743	Acc: 59.5% (5955/10000)
[Test]  Epoch: 78	Loss: 0.018719	Acc: 59.7% (5973/10000)
[Test]  Epoch: 79	Loss: 0.018715	Acc: 59.9% (5992/10000)
[Test]  Epoch: 80	Loss: 0.018754	Acc: 59.7% (5966/10000)
[Test]  Epoch: 81	Loss: 0.018784	Acc: 59.5% (5948/10000)
[Test]  Epoch: 82	Loss: 0.018801	Acc: 59.5% (5952/10000)
[Test]  Epoch: 83	Loss: 0.018791	Acc: 59.7% (5971/10000)
[Test]  Epoch: 84	Loss: 0.018701	Acc: 59.7% (5972/10000)
[Test]  Epoch: 85	Loss: 0.018751	Acc: 59.6% (5956/10000)
[Test]  Epoch: 86	Loss: 0.018742	Acc: 59.7% (5970/10000)
[Test]  Epoch: 87	Loss: 0.018746	Acc: 59.5% (5951/10000)
[Test]  Epoch: 88	Loss: 0.018673	Acc: 59.6% (5957/10000)
[Test]  Epoch: 89	Loss: 0.018732	Acc: 59.7% (5968/10000)
[Test]  Epoch: 90	Loss: 0.018716	Acc: 59.6% (5962/10000)
[Test]  Epoch: 91	Loss: 0.018718	Acc: 59.5% (5952/10000)
[Test]  Epoch: 92	Loss: 0.018754	Acc: 59.5% (5951/10000)
[Test]  Epoch: 93	Loss: 0.018715	Acc: 59.6% (5956/10000)
[Test]  Epoch: 94	Loss: 0.018717	Acc: 59.3% (5933/10000)
[Test]  Epoch: 95	Loss: 0.018727	Acc: 59.4% (5936/10000)
[Test]  Epoch: 96	Loss: 0.018715	Acc: 59.7% (5966/10000)
[Test]  Epoch: 97	Loss: 0.018731	Acc: 59.5% (5954/10000)
[Test]  Epoch: 98	Loss: 0.018773	Acc: 59.6% (5959/10000)
[Test]  Epoch: 99	Loss: 0.018726	Acc: 59.5% (5952/10000)
[Test]  Epoch: 100	Loss: 0.018736	Acc: 59.5% (5950/10000)
===========finish==========
['2024-08-19', '02:06:40.522556', '100', 'test', '0.01873636656999588', '59.5', '59.92']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.041792	Acc: 31.6% (3164/10000)
[Test]  Epoch: 2	Loss: 0.029672	Acc: 37.2% (3719/10000)
[Test]  Epoch: 3	Loss: 0.025501	Acc: 42.9% (4288/10000)
[Test]  Epoch: 4	Loss: 0.026187	Acc: 43.8% (4375/10000)
[Test]  Epoch: 5	Loss: 0.027252	Acc: 41.8% (4180/10000)
[Test]  Epoch: 6	Loss: 0.026893	Acc: 43.4% (4338/10000)
[Test]  Epoch: 7	Loss: 0.026610	Acc: 45.2% (4519/10000)
[Test]  Epoch: 8	Loss: 0.027543	Acc: 44.0% (4397/10000)
[Test]  Epoch: 9	Loss: 0.027550	Acc: 44.1% (4411/10000)
[Test]  Epoch: 10	Loss: 0.027407	Acc: 43.8% (4378/10000)
[Test]  Epoch: 11	Loss: 0.026978	Acc: 44.9% (4494/10000)
[Test]  Epoch: 12	Loss: 0.028019	Acc: 43.4% (4341/10000)
[Test]  Epoch: 13	Loss: 0.027681	Acc: 45.1% (4514/10000)
[Test]  Epoch: 14	Loss: 0.027417	Acc: 44.9% (4486/10000)
[Test]  Epoch: 15	Loss: 0.026877	Acc: 44.9% (4494/10000)
[Test]  Epoch: 16	Loss: 0.026795	Acc: 45.4% (4544/10000)
[Test]  Epoch: 17	Loss: 0.027657	Acc: 43.3% (4326/10000)
[Test]  Epoch: 18	Loss: 0.027157	Acc: 44.8% (4483/10000)
[Test]  Epoch: 19	Loss: 0.027301	Acc: 45.2% (4516/10000)
[Test]  Epoch: 20	Loss: 0.026387	Acc: 45.9% (4591/10000)
[Test]  Epoch: 21	Loss: 0.027446	Acc: 44.0% (4395/10000)
[Test]  Epoch: 22	Loss: 0.025975	Acc: 47.1% (4708/10000)
[Test]  Epoch: 23	Loss: 0.026154	Acc: 46.3% (4628/10000)
[Test]  Epoch: 24	Loss: 0.026213	Acc: 46.8% (4683/10000)
[Test]  Epoch: 25	Loss: 0.026968	Acc: 45.5% (4555/10000)
[Test]  Epoch: 26	Loss: 0.026171	Acc: 46.1% (4609/10000)
[Test]  Epoch: 27	Loss: 0.025700	Acc: 46.1% (4614/10000)
[Test]  Epoch: 28	Loss: 0.025977	Acc: 46.2% (4622/10000)
[Test]  Epoch: 29	Loss: 0.025669	Acc: 46.9% (4687/10000)
[Test]  Epoch: 30	Loss: 0.025427	Acc: 47.4% (4740/10000)
[Test]  Epoch: 31	Loss: 0.025465	Acc: 46.9% (4692/10000)
[Test]  Epoch: 32	Loss: 0.025557	Acc: 46.5% (4649/10000)
[Test]  Epoch: 33	Loss: 0.025602	Acc: 47.8% (4784/10000)
[Test]  Epoch: 34	Loss: 0.025906	Acc: 46.4% (4640/10000)
[Test]  Epoch: 35	Loss: 0.025876	Acc: 46.6% (4662/10000)
[Test]  Epoch: 36	Loss: 0.025569	Acc: 47.3% (4728/10000)
[Test]  Epoch: 37	Loss: 0.025640	Acc: 46.5% (4652/10000)
[Test]  Epoch: 38	Loss: 0.026406	Acc: 46.2% (4623/10000)
[Test]  Epoch: 39	Loss: 0.025607	Acc: 47.0% (4698/10000)
[Test]  Epoch: 40	Loss: 0.025413	Acc: 46.7% (4670/10000)
[Test]  Epoch: 41	Loss: 0.025494	Acc: 46.5% (4652/10000)
[Test]  Epoch: 42	Loss: 0.025500	Acc: 47.0% (4705/10000)
[Test]  Epoch: 43	Loss: 0.025992	Acc: 46.1% (4610/10000)
[Test]  Epoch: 44	Loss: 0.025263	Acc: 46.8% (4682/10000)
[Test]  Epoch: 45	Loss: 0.025465	Acc: 46.7% (4671/10000)
[Test]  Epoch: 46	Loss: 0.025665	Acc: 46.7% (4673/10000)
[Test]  Epoch: 47	Loss: 0.025310	Acc: 47.1% (4715/10000)
[Test]  Epoch: 48	Loss: 0.025251	Acc: 47.2% (4717/10000)
[Test]  Epoch: 49	Loss: 0.025549	Acc: 46.8% (4681/10000)
[Test]  Epoch: 50	Loss: 0.025096	Acc: 46.8% (4676/10000)
[Test]  Epoch: 51	Loss: 0.025205	Acc: 47.4% (4743/10000)
[Test]  Epoch: 52	Loss: 0.025027	Acc: 47.2% (4724/10000)
[Test]  Epoch: 53	Loss: 0.025106	Acc: 47.5% (4749/10000)
[Test]  Epoch: 54	Loss: 0.025025	Acc: 47.8% (4778/10000)
[Test]  Epoch: 55	Loss: 0.025032	Acc: 47.6% (4764/10000)
[Test]  Epoch: 56	Loss: 0.025045	Acc: 47.2% (4722/10000)
[Test]  Epoch: 57	Loss: 0.025716	Acc: 47.2% (4718/10000)
[Test]  Epoch: 58	Loss: 0.025051	Acc: 48.2% (4822/10000)
[Test]  Epoch: 59	Loss: 0.025233	Acc: 47.2% (4722/10000)
[Test]  Epoch: 60	Loss: 0.025164	Acc: 47.3% (4734/10000)
[Test]  Epoch: 61	Loss: 0.024752	Acc: 47.7% (4773/10000)
[Test]  Epoch: 62	Loss: 0.024679	Acc: 48.0% (4798/10000)
[Test]  Epoch: 63	Loss: 0.024667	Acc: 48.0% (4796/10000)
[Test]  Epoch: 64	Loss: 0.024702	Acc: 48.0% (4797/10000)
[Test]  Epoch: 65	Loss: 0.024683	Acc: 48.0% (4804/10000)
[Test]  Epoch: 66	Loss: 0.024724	Acc: 48.1% (4808/10000)
[Test]  Epoch: 67	Loss: 0.024720	Acc: 47.7% (4773/10000)
[Test]  Epoch: 68	Loss: 0.024777	Acc: 48.0% (4801/10000)
[Test]  Epoch: 69	Loss: 0.024694	Acc: 47.9% (4785/10000)
[Test]  Epoch: 70	Loss: 0.024642	Acc: 47.9% (4792/10000)
[Test]  Epoch: 71	Loss: 0.024691	Acc: 48.0% (4795/10000)
[Test]  Epoch: 72	Loss: 0.024771	Acc: 47.9% (4785/10000)
[Test]  Epoch: 73	Loss: 0.024705	Acc: 47.9% (4788/10000)
[Test]  Epoch: 74	Loss: 0.024763	Acc: 48.0% (4802/10000)
[Test]  Epoch: 75	Loss: 0.024659	Acc: 48.1% (4806/10000)
[Test]  Epoch: 76	Loss: 0.024731	Acc: 47.9% (4792/10000)
[Test]  Epoch: 77	Loss: 0.024650	Acc: 48.1% (4810/10000)
[Test]  Epoch: 78	Loss: 0.024746	Acc: 48.0% (4802/10000)
[Test]  Epoch: 79	Loss: 0.024738	Acc: 48.1% (4812/10000)
[Test]  Epoch: 80	Loss: 0.024694	Acc: 47.9% (4793/10000)
[Test]  Epoch: 81	Loss: 0.024697	Acc: 48.0% (4797/10000)
[Test]  Epoch: 82	Loss: 0.024733	Acc: 47.8% (4775/10000)
[Test]  Epoch: 83	Loss: 0.024741	Acc: 47.7% (4773/10000)
[Test]  Epoch: 84	Loss: 0.024645	Acc: 48.0% (4795/10000)
[Test]  Epoch: 85	Loss: 0.024617	Acc: 47.9% (4790/10000)
[Test]  Epoch: 86	Loss: 0.024658	Acc: 48.0% (4802/10000)
[Test]  Epoch: 87	Loss: 0.024714	Acc: 47.9% (4794/10000)
[Test]  Epoch: 88	Loss: 0.024634	Acc: 48.0% (4800/10000)
[Test]  Epoch: 89	Loss: 0.024721	Acc: 48.0% (4795/10000)
[Test]  Epoch: 90	Loss: 0.024677	Acc: 48.0% (4795/10000)
[Test]  Epoch: 91	Loss: 0.024569	Acc: 48.1% (4812/10000)
[Test]  Epoch: 92	Loss: 0.024705	Acc: 48.0% (4798/10000)
[Test]  Epoch: 93	Loss: 0.024604	Acc: 48.1% (4811/10000)
[Test]  Epoch: 94	Loss: 0.024619	Acc: 48.0% (4797/10000)
[Test]  Epoch: 95	Loss: 0.024662	Acc: 48.1% (4806/10000)
[Test]  Epoch: 96	Loss: 0.024712	Acc: 47.9% (4787/10000)
[Test]  Epoch: 97	Loss: 0.024692	Acc: 48.1% (4807/10000)
[Test]  Epoch: 98	Loss: 0.024744	Acc: 47.8% (4781/10000)
[Test]  Epoch: 99	Loss: 0.024706	Acc: 48.0% (4798/10000)
[Test]  Epoch: 100	Loss: 0.024624	Acc: 48.1% (4807/10000)
===========finish==========
['2024-08-19', '02:08:59.748211', '100', 'test', '0.024624160051345826', '48.07', '48.22']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.4.conv.3.weight', '_features.3.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.113821	Acc: 18.0% (1799/10000)
[Test]  Epoch: 2	Loss: 0.028939	Acc: 36.2% (3624/10000)
[Test]  Epoch: 3	Loss: 0.027243	Acc: 39.7% (3971/10000)
[Test]  Epoch: 4	Loss: 0.028709	Acc: 36.7% (3671/10000)
[Test]  Epoch: 5	Loss: 0.026589	Acc: 41.2% (4124/10000)
[Test]  Epoch: 6	Loss: 0.028072	Acc: 40.2% (4023/10000)
[Test]  Epoch: 7	Loss: 0.028532	Acc: 40.3% (4027/10000)
[Test]  Epoch: 8	Loss: 0.029171	Acc: 39.9% (3985/10000)
[Test]  Epoch: 9	Loss: 0.031356	Acc: 39.0% (3897/10000)
[Test]  Epoch: 10	Loss: 0.028762	Acc: 41.6% (4159/10000)
[Test]  Epoch: 11	Loss: 0.028510	Acc: 41.8% (4177/10000)
[Test]  Epoch: 12	Loss: 0.029522	Acc: 41.0% (4102/10000)
[Test]  Epoch: 13	Loss: 0.030946	Acc: 40.7% (4066/10000)
[Test]  Epoch: 14	Loss: 0.029281	Acc: 42.4% (4242/10000)
[Test]  Epoch: 15	Loss: 0.028444	Acc: 42.6% (4260/10000)
[Test]  Epoch: 16	Loss: 0.029619	Acc: 41.1% (4108/10000)
[Test]  Epoch: 17	Loss: 0.028838	Acc: 43.2% (4324/10000)
[Test]  Epoch: 18	Loss: 0.028715	Acc: 42.8% (4276/10000)
[Test]  Epoch: 19	Loss: 0.029427	Acc: 42.2% (4220/10000)
[Test]  Epoch: 20	Loss: 0.028133	Acc: 43.8% (4377/10000)
[Test]  Epoch: 21	Loss: 0.028286	Acc: 43.0% (4305/10000)
[Test]  Epoch: 22	Loss: 0.028342	Acc: 44.0% (4395/10000)
[Test]  Epoch: 23	Loss: 0.027979	Acc: 44.1% (4409/10000)
[Test]  Epoch: 24	Loss: 0.027787	Acc: 43.6% (4361/10000)
[Test]  Epoch: 25	Loss: 0.028121	Acc: 43.2% (4319/10000)
[Test]  Epoch: 26	Loss: 0.027950	Acc: 43.6% (4359/10000)
[Test]  Epoch: 27	Loss: 0.028048	Acc: 43.4% (4343/10000)
[Test]  Epoch: 28	Loss: 0.028526	Acc: 42.3% (4231/10000)
[Test]  Epoch: 29	Loss: 0.027216	Acc: 44.6% (4465/10000)
[Test]  Epoch: 30	Loss: 0.027980	Acc: 43.9% (4394/10000)
[Test]  Epoch: 31	Loss: 0.027829	Acc: 44.0% (4399/10000)
[Test]  Epoch: 32	Loss: 0.027929	Acc: 43.3% (4326/10000)
[Test]  Epoch: 33	Loss: 0.027841	Acc: 43.7% (4367/10000)
[Test]  Epoch: 34	Loss: 0.027862	Acc: 43.4% (4337/10000)
[Test]  Epoch: 35	Loss: 0.028069	Acc: 42.8% (4284/10000)
[Test]  Epoch: 36	Loss: 0.027421	Acc: 44.8% (4475/10000)
[Test]  Epoch: 37	Loss: 0.027361	Acc: 43.7% (4370/10000)
[Test]  Epoch: 38	Loss: 0.029278	Acc: 42.1% (4206/10000)
[Test]  Epoch: 39	Loss: 0.027933	Acc: 43.3% (4329/10000)
[Test]  Epoch: 40	Loss: 0.027123	Acc: 44.7% (4467/10000)
[Test]  Epoch: 41	Loss: 0.027495	Acc: 44.3% (4428/10000)
[Test]  Epoch: 42	Loss: 0.027065	Acc: 44.6% (4459/10000)
[Test]  Epoch: 43	Loss: 0.027266	Acc: 44.3% (4428/10000)
[Test]  Epoch: 44	Loss: 0.026882	Acc: 45.0% (4502/10000)
[Test]  Epoch: 45	Loss: 0.026626	Acc: 44.6% (4465/10000)
[Test]  Epoch: 46	Loss: 0.027219	Acc: 43.8% (4384/10000)
[Test]  Epoch: 47	Loss: 0.027070	Acc: 44.8% (4480/10000)
[Test]  Epoch: 48	Loss: 0.026698	Acc: 45.0% (4505/10000)
[Test]  Epoch: 49	Loss: 0.026912	Acc: 44.8% (4482/10000)
[Test]  Epoch: 50	Loss: 0.026850	Acc: 44.7% (4469/10000)
[Test]  Epoch: 51	Loss: 0.026616	Acc: 45.2% (4517/10000)
[Test]  Epoch: 52	Loss: 0.026678	Acc: 45.2% (4516/10000)
[Test]  Epoch: 53	Loss: 0.026624	Acc: 44.9% (4491/10000)
[Test]  Epoch: 54	Loss: 0.026666	Acc: 44.8% (4476/10000)
[Test]  Epoch: 55	Loss: 0.026314	Acc: 45.2% (4519/10000)
[Test]  Epoch: 56	Loss: 0.026775	Acc: 45.4% (4540/10000)
[Test]  Epoch: 57	Loss: 0.026769	Acc: 44.5% (4451/10000)
[Test]  Epoch: 58	Loss: 0.026564	Acc: 44.8% (4479/10000)
[Test]  Epoch: 59	Loss: 0.027352	Acc: 44.0% (4404/10000)
[Test]  Epoch: 60	Loss: 0.026670	Acc: 45.2% (4516/10000)
[Test]  Epoch: 61	Loss: 0.026574	Acc: 45.4% (4543/10000)
[Test]  Epoch: 62	Loss: 0.026509	Acc: 45.5% (4552/10000)
[Test]  Epoch: 63	Loss: 0.026357	Acc: 45.6% (4556/10000)
[Test]  Epoch: 64	Loss: 0.026338	Acc: 45.8% (4584/10000)
[Test]  Epoch: 65	Loss: 0.026252	Acc: 45.9% (4591/10000)
[Test]  Epoch: 66	Loss: 0.026275	Acc: 45.9% (4585/10000)
[Test]  Epoch: 67	Loss: 0.026344	Acc: 45.9% (4585/10000)
[Test]  Epoch: 68	Loss: 0.026304	Acc: 46.0% (4596/10000)
[Test]  Epoch: 69	Loss: 0.026313	Acc: 45.9% (4594/10000)
[Test]  Epoch: 70	Loss: 0.026258	Acc: 45.8% (4578/10000)
[Test]  Epoch: 71	Loss: 0.026286	Acc: 46.0% (4596/10000)
[Test]  Epoch: 72	Loss: 0.026296	Acc: 46.2% (4624/10000)
[Test]  Epoch: 73	Loss: 0.026276	Acc: 45.7% (4573/10000)
[Test]  Epoch: 74	Loss: 0.026181	Acc: 46.1% (4609/10000)
[Test]  Epoch: 75	Loss: 0.026108	Acc: 46.1% (4612/10000)
[Test]  Epoch: 76	Loss: 0.026217	Acc: 46.0% (4600/10000)
[Test]  Epoch: 77	Loss: 0.026230	Acc: 46.0% (4603/10000)
[Test]  Epoch: 78	Loss: 0.026216	Acc: 46.2% (4620/10000)
[Test]  Epoch: 79	Loss: 0.026231	Acc: 46.1% (4608/10000)
[Test]  Epoch: 80	Loss: 0.026183	Acc: 46.0% (4595/10000)
[Test]  Epoch: 81	Loss: 0.026270	Acc: 45.6% (4565/10000)
[Test]  Epoch: 82	Loss: 0.026257	Acc: 45.8% (4577/10000)
[Test]  Epoch: 83	Loss: 0.026282	Acc: 45.8% (4580/10000)
[Test]  Epoch: 84	Loss: 0.026285	Acc: 45.9% (4587/10000)
[Test]  Epoch: 85	Loss: 0.026267	Acc: 45.8% (4583/10000)
[Test]  Epoch: 86	Loss: 0.026281	Acc: 46.0% (4597/10000)
[Test]  Epoch: 87	Loss: 0.026276	Acc: 45.9% (4588/10000)
[Test]  Epoch: 88	Loss: 0.026139	Acc: 46.1% (4611/10000)
[Test]  Epoch: 89	Loss: 0.026234	Acc: 46.0% (4602/10000)
[Test]  Epoch: 90	Loss: 0.026198	Acc: 46.0% (4599/10000)
[Test]  Epoch: 91	Loss: 0.026150	Acc: 45.9% (4591/10000)
[Test]  Epoch: 92	Loss: 0.026248	Acc: 45.9% (4594/10000)
[Test]  Epoch: 93	Loss: 0.026200	Acc: 46.1% (4613/10000)
[Test]  Epoch: 94	Loss: 0.026168	Acc: 46.0% (4598/10000)
[Test]  Epoch: 95	Loss: 0.026232	Acc: 45.7% (4574/10000)
[Test]  Epoch: 96	Loss: 0.026254	Acc: 45.9% (4591/10000)
[Test]  Epoch: 97	Loss: 0.026207	Acc: 46.1% (4610/10000)
[Test]  Epoch: 98	Loss: 0.026325	Acc: 45.9% (4587/10000)
[Test]  Epoch: 99	Loss: 0.026258	Acc: 46.0% (4599/10000)
[Test]  Epoch: 100	Loss: 0.026270	Acc: 45.7% (4567/10000)
===========finish==========
['2024-08-19', '02:11:24.330535', '100', 'test', '0.026269593489170073', '45.67', '46.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.4.conv.3.weight', '_features.3.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.0.0.weight', '_features.2.conv.0.0.weight', '_features.6.conv.1.1.weight', '_features.8.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.156127	Acc: 14.2% (1418/10000)
[Test]  Epoch: 2	Loss: 0.037893	Acc: 25.9% (2588/10000)
[Test]  Epoch: 3	Loss: 0.032466	Acc: 30.8% (3079/10000)
[Test]  Epoch: 4	Loss: 0.029566	Acc: 34.7% (3468/10000)
[Test]  Epoch: 5	Loss: 0.030331	Acc: 35.1% (3509/10000)
[Test]  Epoch: 6	Loss: 0.032371	Acc: 34.6% (3463/10000)
[Test]  Epoch: 7	Loss: 0.032237	Acc: 34.4% (3441/10000)
[Test]  Epoch: 8	Loss: 0.032951	Acc: 34.9% (3490/10000)
[Test]  Epoch: 9	Loss: 0.032549	Acc: 36.0% (3596/10000)
[Test]  Epoch: 10	Loss: 0.032427	Acc: 36.0% (3599/10000)
[Test]  Epoch: 11	Loss: 0.037048	Acc: 33.1% (3310/10000)
[Test]  Epoch: 12	Loss: 0.032955	Acc: 35.8% (3579/10000)
[Test]  Epoch: 13	Loss: 0.032618	Acc: 37.2% (3720/10000)
[Test]  Epoch: 14	Loss: 0.031665	Acc: 37.6% (3758/10000)
[Test]  Epoch: 15	Loss: 0.031263	Acc: 38.5% (3848/10000)
[Test]  Epoch: 16	Loss: 0.033104	Acc: 36.4% (3643/10000)
[Test]  Epoch: 17	Loss: 0.032378	Acc: 37.8% (3782/10000)
[Test]  Epoch: 18	Loss: 0.031837	Acc: 39.1% (3912/10000)
[Test]  Epoch: 19	Loss: 0.031928	Acc: 37.4% (3739/10000)
[Test]  Epoch: 20	Loss: 0.030715	Acc: 39.4% (3938/10000)
[Test]  Epoch: 21	Loss: 0.031043	Acc: 38.4% (3842/10000)
[Test]  Epoch: 22	Loss: 0.029600	Acc: 40.3% (4034/10000)
[Test]  Epoch: 23	Loss: 0.030364	Acc: 40.4% (4037/10000)
[Test]  Epoch: 24	Loss: 0.030455	Acc: 39.7% (3973/10000)
[Test]  Epoch: 25	Loss: 0.030242	Acc: 40.8% (4082/10000)
[Test]  Epoch: 26	Loss: 0.030239	Acc: 39.8% (3982/10000)
[Test]  Epoch: 27	Loss: 0.030433	Acc: 39.2% (3923/10000)
[Test]  Epoch: 28	Loss: 0.029928	Acc: 39.9% (3991/10000)
[Test]  Epoch: 29	Loss: 0.029795	Acc: 40.1% (4009/10000)
[Test]  Epoch: 30	Loss: 0.029965	Acc: 39.9% (3985/10000)
[Test]  Epoch: 31	Loss: 0.029069	Acc: 41.2% (4121/10000)
[Test]  Epoch: 32	Loss: 0.029925	Acc: 39.6% (3962/10000)
[Test]  Epoch: 33	Loss: 0.029360	Acc: 40.4% (4041/10000)
[Test]  Epoch: 34	Loss: 0.030119	Acc: 40.1% (4014/10000)
[Test]  Epoch: 35	Loss: 0.029627	Acc: 39.8% (3982/10000)
[Test]  Epoch: 36	Loss: 0.029399	Acc: 41.0% (4105/10000)
[Test]  Epoch: 37	Loss: 0.029559	Acc: 39.9% (3987/10000)
[Test]  Epoch: 38	Loss: 0.030949	Acc: 39.0% (3905/10000)
[Test]  Epoch: 39	Loss: 0.029180	Acc: 40.8% (4084/10000)
[Test]  Epoch: 40	Loss: 0.029083	Acc: 40.4% (4035/10000)
[Test]  Epoch: 41	Loss: 0.029637	Acc: 39.4% (3944/10000)
[Test]  Epoch: 42	Loss: 0.030218	Acc: 39.6% (3963/10000)
[Test]  Epoch: 43	Loss: 0.029084	Acc: 40.9% (4088/10000)
[Test]  Epoch: 44	Loss: 0.028862	Acc: 41.0% (4103/10000)
[Test]  Epoch: 45	Loss: 0.029052	Acc: 40.5% (4047/10000)
[Test]  Epoch: 46	Loss: 0.029016	Acc: 40.3% (4032/10000)
[Test]  Epoch: 47	Loss: 0.028858	Acc: 40.9% (4089/10000)
[Test]  Epoch: 48	Loss: 0.028752	Acc: 40.9% (4094/10000)
[Test]  Epoch: 49	Loss: 0.029085	Acc: 40.5% (4047/10000)
[Test]  Epoch: 50	Loss: 0.028653	Acc: 41.4% (4141/10000)
[Test]  Epoch: 51	Loss: 0.028821	Acc: 41.4% (4141/10000)
[Test]  Epoch: 52	Loss: 0.028999	Acc: 41.1% (4111/10000)
[Test]  Epoch: 53	Loss: 0.028557	Acc: 40.8% (4077/10000)
[Test]  Epoch: 54	Loss: 0.028447	Acc: 41.3% (4128/10000)
[Test]  Epoch: 55	Loss: 0.028813	Acc: 41.3% (4127/10000)
[Test]  Epoch: 56	Loss: 0.028932	Acc: 41.0% (4103/10000)
[Test]  Epoch: 57	Loss: 0.028593	Acc: 40.8% (4081/10000)
[Test]  Epoch: 58	Loss: 0.028473	Acc: 41.9% (4185/10000)
[Test]  Epoch: 59	Loss: 0.029026	Acc: 40.5% (4053/10000)
[Test]  Epoch: 60	Loss: 0.028646	Acc: 41.4% (4137/10000)
[Test]  Epoch: 61	Loss: 0.028514	Acc: 41.9% (4189/10000)
[Test]  Epoch: 62	Loss: 0.028469	Acc: 41.9% (4191/10000)
[Test]  Epoch: 63	Loss: 0.028431	Acc: 41.8% (4177/10000)
[Test]  Epoch: 64	Loss: 0.028459	Acc: 41.9% (4191/10000)
[Test]  Epoch: 65	Loss: 0.028400	Acc: 42.0% (4196/10000)
[Test]  Epoch: 66	Loss: 0.028273	Acc: 42.0% (4202/10000)
[Test]  Epoch: 67	Loss: 0.028494	Acc: 41.7% (4168/10000)
[Test]  Epoch: 68	Loss: 0.028480	Acc: 41.8% (4182/10000)
[Test]  Epoch: 69	Loss: 0.028468	Acc: 41.8% (4180/10000)
[Test]  Epoch: 70	Loss: 0.028406	Acc: 41.9% (4190/10000)
[Test]  Epoch: 71	Loss: 0.028420	Acc: 41.8% (4175/10000)
[Test]  Epoch: 72	Loss: 0.028482	Acc: 41.8% (4181/10000)
[Test]  Epoch: 73	Loss: 0.028404	Acc: 41.9% (4187/10000)
[Test]  Epoch: 74	Loss: 0.028309	Acc: 41.8% (4181/10000)
[Test]  Epoch: 75	Loss: 0.028279	Acc: 41.9% (4193/10000)
[Test]  Epoch: 76	Loss: 0.028347	Acc: 41.9% (4193/10000)
[Test]  Epoch: 77	Loss: 0.028278	Acc: 42.1% (4210/10000)
[Test]  Epoch: 78	Loss: 0.028359	Acc: 42.2% (4220/10000)
[Test]  Epoch: 79	Loss: 0.028376	Acc: 42.1% (4207/10000)
[Test]  Epoch: 80	Loss: 0.028359	Acc: 41.7% (4172/10000)
[Test]  Epoch: 81	Loss: 0.028363	Acc: 41.6% (4163/10000)
[Test]  Epoch: 82	Loss: 0.028391	Acc: 41.6% (4164/10000)
[Test]  Epoch: 83	Loss: 0.028401	Acc: 41.8% (4178/10000)
[Test]  Epoch: 84	Loss: 0.028391	Acc: 41.6% (4159/10000)
[Test]  Epoch: 85	Loss: 0.028365	Acc: 41.6% (4164/10000)
[Test]  Epoch: 86	Loss: 0.028400	Acc: 41.9% (4191/10000)
[Test]  Epoch: 87	Loss: 0.028508	Acc: 42.0% (4195/10000)
[Test]  Epoch: 88	Loss: 0.028282	Acc: 41.9% (4187/10000)
[Test]  Epoch: 89	Loss: 0.028371	Acc: 41.9% (4191/10000)
[Test]  Epoch: 90	Loss: 0.028396	Acc: 41.9% (4191/10000)
[Test]  Epoch: 91	Loss: 0.028338	Acc: 41.9% (4194/10000)
[Test]  Epoch: 92	Loss: 0.028373	Acc: 41.8% (4179/10000)
[Test]  Epoch: 93	Loss: 0.028380	Acc: 42.0% (4198/10000)
[Test]  Epoch: 94	Loss: 0.028380	Acc: 41.9% (4188/10000)
[Test]  Epoch: 95	Loss: 0.028361	Acc: 41.8% (4179/10000)
[Test]  Epoch: 96	Loss: 0.028396	Acc: 41.8% (4177/10000)
[Test]  Epoch: 97	Loss: 0.028400	Acc: 41.7% (4166/10000)
[Test]  Epoch: 98	Loss: 0.028500	Acc: 41.6% (4164/10000)
[Test]  Epoch: 99	Loss: 0.028328	Acc: 41.7% (4173/10000)
[Test]  Epoch: 100	Loss: 0.028267	Acc: 41.7% (4173/10000)
===========finish==========
['2024-08-19', '02:13:45.771606', '100', 'test', '0.028267399895191194', '41.73', '42.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.4.conv.3.weight', '_features.3.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.0.0.weight', '_features.2.conv.0.0.weight', '_features.6.conv.1.1.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.12.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.080074	Acc: 18.1% (1815/10000)
[Test]  Epoch: 2	Loss: 0.031716	Acc: 30.7% (3069/10000)
[Test]  Epoch: 3	Loss: 0.029573	Acc: 34.6% (3464/10000)
[Test]  Epoch: 4	Loss: 0.030375	Acc: 33.6% (3360/10000)
[Test]  Epoch: 5	Loss: 0.031234	Acc: 34.2% (3418/10000)
[Test]  Epoch: 6	Loss: 0.029738	Acc: 35.5% (3548/10000)
[Test]  Epoch: 7	Loss: 0.032020	Acc: 34.1% (3414/10000)
[Test]  Epoch: 8	Loss: 0.032865	Acc: 34.6% (3460/10000)
[Test]  Epoch: 9	Loss: 0.033424	Acc: 33.6% (3359/10000)
[Test]  Epoch: 10	Loss: 0.034226	Acc: 35.3% (3529/10000)
[Test]  Epoch: 11	Loss: 0.033863	Acc: 35.8% (3581/10000)
[Test]  Epoch: 12	Loss: 0.032337	Acc: 35.9% (3592/10000)
[Test]  Epoch: 13	Loss: 0.032422	Acc: 36.3% (3627/10000)
[Test]  Epoch: 14	Loss: 0.033361	Acc: 35.6% (3565/10000)
[Test]  Epoch: 15	Loss: 0.032855	Acc: 37.0% (3702/10000)
[Test]  Epoch: 16	Loss: 0.032027	Acc: 37.2% (3724/10000)
[Test]  Epoch: 17	Loss: 0.033061	Acc: 36.4% (3638/10000)
[Test]  Epoch: 18	Loss: 0.031989	Acc: 37.6% (3763/10000)
[Test]  Epoch: 19	Loss: 0.032894	Acc: 36.9% (3690/10000)
[Test]  Epoch: 20	Loss: 0.031506	Acc: 38.7% (3867/10000)
[Test]  Epoch: 21	Loss: 0.032710	Acc: 36.7% (3670/10000)
[Test]  Epoch: 22	Loss: 0.031566	Acc: 38.4% (3836/10000)
[Test]  Epoch: 23	Loss: 0.030938	Acc: 38.7% (3866/10000)
[Test]  Epoch: 24	Loss: 0.031225	Acc: 38.5% (3849/10000)
[Test]  Epoch: 25	Loss: 0.031206	Acc: 38.8% (3878/10000)
[Test]  Epoch: 26	Loss: 0.030903	Acc: 38.3% (3834/10000)
[Test]  Epoch: 27	Loss: 0.030815	Acc: 38.8% (3881/10000)
[Test]  Epoch: 28	Loss: 0.030934	Acc: 37.9% (3785/10000)
[Test]  Epoch: 29	Loss: 0.030908	Acc: 38.2% (3824/10000)
[Test]  Epoch: 30	Loss: 0.031125	Acc: 37.9% (3794/10000)
[Test]  Epoch: 31	Loss: 0.030489	Acc: 38.4% (3835/10000)
[Test]  Epoch: 32	Loss: 0.030395	Acc: 39.5% (3952/10000)
[Test]  Epoch: 33	Loss: 0.030119	Acc: 39.1% (3909/10000)
[Test]  Epoch: 34	Loss: 0.031085	Acc: 37.5% (3747/10000)
[Test]  Epoch: 35	Loss: 0.030546	Acc: 38.7% (3871/10000)
[Test]  Epoch: 36	Loss: 0.030489	Acc: 39.1% (3915/10000)
[Test]  Epoch: 37	Loss: 0.030155	Acc: 38.6% (3865/10000)
[Test]  Epoch: 38	Loss: 0.032369	Acc: 36.6% (3660/10000)
[Test]  Epoch: 39	Loss: 0.030643	Acc: 38.8% (3878/10000)
[Test]  Epoch: 40	Loss: 0.029986	Acc: 38.7% (3871/10000)
[Test]  Epoch: 41	Loss: 0.030028	Acc: 39.7% (3974/10000)
[Test]  Epoch: 42	Loss: 0.029861	Acc: 39.3% (3933/10000)
[Test]  Epoch: 43	Loss: 0.030252	Acc: 39.3% (3933/10000)
[Test]  Epoch: 44	Loss: 0.029743	Acc: 38.8% (3884/10000)
[Test]  Epoch: 45	Loss: 0.029856	Acc: 39.3% (3928/10000)
[Test]  Epoch: 46	Loss: 0.030154	Acc: 38.7% (3870/10000)
[Test]  Epoch: 47	Loss: 0.030018	Acc: 38.8% (3883/10000)
[Test]  Epoch: 48	Loss: 0.029728	Acc: 39.4% (3943/10000)
[Test]  Epoch: 49	Loss: 0.030862	Acc: 38.1% (3806/10000)
[Test]  Epoch: 50	Loss: 0.030002	Acc: 38.4% (3842/10000)
[Test]  Epoch: 51	Loss: 0.029638	Acc: 38.8% (3876/10000)
[Test]  Epoch: 52	Loss: 0.029857	Acc: 39.1% (3915/10000)
[Test]  Epoch: 53	Loss: 0.029590	Acc: 39.2% (3921/10000)
[Test]  Epoch: 54	Loss: 0.029495	Acc: 39.1% (3914/10000)
[Test]  Epoch: 55	Loss: 0.029561	Acc: 39.6% (3963/10000)
[Test]  Epoch: 56	Loss: 0.029405	Acc: 39.3% (3932/10000)
[Test]  Epoch: 57	Loss: 0.029589	Acc: 39.4% (3942/10000)
[Test]  Epoch: 58	Loss: 0.029659	Acc: 40.2% (4023/10000)
[Test]  Epoch: 59	Loss: 0.029404	Acc: 40.0% (3996/10000)
[Test]  Epoch: 60	Loss: 0.029349	Acc: 39.6% (3957/10000)
[Test]  Epoch: 61	Loss: 0.029354	Acc: 39.6% (3965/10000)
[Test]  Epoch: 62	Loss: 0.029301	Acc: 39.5% (3954/10000)
[Test]  Epoch: 63	Loss: 0.029312	Acc: 39.7% (3972/10000)
[Test]  Epoch: 64	Loss: 0.029293	Acc: 39.8% (3984/10000)
[Test]  Epoch: 65	Loss: 0.029223	Acc: 39.9% (3991/10000)
[Test]  Epoch: 66	Loss: 0.029214	Acc: 39.7% (3972/10000)
[Test]  Epoch: 67	Loss: 0.029286	Acc: 39.8% (3976/10000)
[Test]  Epoch: 68	Loss: 0.029202	Acc: 39.9% (3990/10000)
[Test]  Epoch: 69	Loss: 0.029136	Acc: 39.6% (3958/10000)
[Test]  Epoch: 70	Loss: 0.029171	Acc: 39.7% (3973/10000)
[Test]  Epoch: 71	Loss: 0.029169	Acc: 39.9% (3985/10000)
[Test]  Epoch: 72	Loss: 0.029259	Acc: 40.1% (4009/10000)
[Test]  Epoch: 73	Loss: 0.029178	Acc: 39.7% (3972/10000)
[Test]  Epoch: 74	Loss: 0.029125	Acc: 39.8% (3982/10000)
[Test]  Epoch: 75	Loss: 0.029132	Acc: 39.6% (3964/10000)
[Test]  Epoch: 76	Loss: 0.029181	Acc: 39.7% (3970/10000)
[Test]  Epoch: 77	Loss: 0.029116	Acc: 39.7% (3970/10000)
[Test]  Epoch: 78	Loss: 0.029161	Acc: 39.8% (3977/10000)
[Test]  Epoch: 79	Loss: 0.029297	Acc: 40.1% (4006/10000)
[Test]  Epoch: 80	Loss: 0.029147	Acc: 40.0% (3997/10000)
[Test]  Epoch: 81	Loss: 0.029149	Acc: 39.9% (3986/10000)
[Test]  Epoch: 82	Loss: 0.029191	Acc: 39.7% (3966/10000)
[Test]  Epoch: 83	Loss: 0.029146	Acc: 39.8% (3983/10000)
[Test]  Epoch: 84	Loss: 0.029171	Acc: 39.9% (3990/10000)
[Test]  Epoch: 85	Loss: 0.029201	Acc: 39.8% (3978/10000)
[Test]  Epoch: 86	Loss: 0.029123	Acc: 40.1% (4013/10000)
[Test]  Epoch: 87	Loss: 0.029116	Acc: 40.0% (3996/10000)
[Test]  Epoch: 88	Loss: 0.029054	Acc: 39.8% (3977/10000)
[Test]  Epoch: 89	Loss: 0.029112	Acc: 40.0% (3997/10000)
[Test]  Epoch: 90	Loss: 0.029129	Acc: 39.8% (3982/10000)
[Test]  Epoch: 91	Loss: 0.029149	Acc: 39.9% (3990/10000)
[Test]  Epoch: 92	Loss: 0.029276	Acc: 39.9% (3985/10000)
[Test]  Epoch: 93	Loss: 0.029198	Acc: 39.9% (3987/10000)
[Test]  Epoch: 94	Loss: 0.029121	Acc: 40.1% (4006/10000)
[Test]  Epoch: 95	Loss: 0.029071	Acc: 40.1% (4010/10000)
[Test]  Epoch: 96	Loss: 0.029126	Acc: 40.0% (4003/10000)
[Test]  Epoch: 97	Loss: 0.029169	Acc: 40.2% (4016/10000)
[Test]  Epoch: 98	Loss: 0.029232	Acc: 39.9% (3988/10000)
[Test]  Epoch: 99	Loss: 0.029192	Acc: 39.8% (3978/10000)
[Test]  Epoch: 100	Loss: 0.029204	Acc: 39.7% (3969/10000)
===========finish==========
['2024-08-19', '02:16:10.626128', '100', 'test', '0.029203803610801698', '39.69', '40.23']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.4.conv.3.weight', '_features.3.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.0.0.weight', '_features.2.conv.0.0.weight', '_features.6.conv.1.1.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.13.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.16.conv.0.1.weight', '_features.10.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.11.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.058285	Acc: 27.4% (2744/10000)
[Test]  Epoch: 2	Loss: 0.033190	Acc: 31.6% (3161/10000)
[Test]  Epoch: 3	Loss: 0.029439	Acc: 35.1% (3513/10000)
[Test]  Epoch: 4	Loss: 0.029749	Acc: 36.4% (3640/10000)
[Test]  Epoch: 5	Loss: 0.030317	Acc: 37.1% (3715/10000)
[Test]  Epoch: 6	Loss: 0.032538	Acc: 35.3% (3527/10000)
[Test]  Epoch: 7	Loss: 0.033257	Acc: 34.4% (3438/10000)
[Test]  Epoch: 8	Loss: 0.035425	Acc: 35.4% (3540/10000)
[Test]  Epoch: 9	Loss: 0.031630	Acc: 36.8% (3676/10000)
[Test]  Epoch: 10	Loss: 0.032255	Acc: 37.0% (3699/10000)
[Test]  Epoch: 11	Loss: 0.033521	Acc: 36.0% (3603/10000)
[Test]  Epoch: 12	Loss: 0.033277	Acc: 36.7% (3673/10000)
[Test]  Epoch: 13	Loss: 0.031555	Acc: 38.2% (3819/10000)
[Test]  Epoch: 14	Loss: 0.031387	Acc: 38.3% (3826/10000)
[Test]  Epoch: 15	Loss: 0.031416	Acc: 38.6% (3864/10000)
[Test]  Epoch: 16	Loss: 0.031374	Acc: 38.1% (3812/10000)
[Test]  Epoch: 17	Loss: 0.030792	Acc: 40.2% (4023/10000)
[Test]  Epoch: 18	Loss: 0.031059	Acc: 39.0% (3897/10000)
[Test]  Epoch: 19	Loss: 0.030886	Acc: 39.0% (3904/10000)
[Test]  Epoch: 20	Loss: 0.030766	Acc: 39.6% (3957/10000)
[Test]  Epoch: 21	Loss: 0.030768	Acc: 39.2% (3924/10000)
[Test]  Epoch: 22	Loss: 0.030521	Acc: 40.6% (4058/10000)
[Test]  Epoch: 23	Loss: 0.029642	Acc: 39.9% (3988/10000)
[Test]  Epoch: 24	Loss: 0.030869	Acc: 38.5% (3851/10000)
[Test]  Epoch: 25	Loss: 0.030219	Acc: 40.4% (4042/10000)
[Test]  Epoch: 26	Loss: 0.030113	Acc: 39.9% (3988/10000)
[Test]  Epoch: 27	Loss: 0.029987	Acc: 39.8% (3982/10000)
[Test]  Epoch: 28	Loss: 0.030898	Acc: 40.0% (3998/10000)
[Test]  Epoch: 29	Loss: 0.029622	Acc: 40.5% (4054/10000)
[Test]  Epoch: 30	Loss: 0.029915	Acc: 40.5% (4054/10000)
[Test]  Epoch: 31	Loss: 0.030087	Acc: 40.0% (3996/10000)
[Test]  Epoch: 32	Loss: 0.029832	Acc: 39.9% (3986/10000)
[Test]  Epoch: 33	Loss: 0.029098	Acc: 41.5% (4150/10000)
[Test]  Epoch: 34	Loss: 0.029276	Acc: 40.5% (4052/10000)
[Test]  Epoch: 35	Loss: 0.029337	Acc: 40.4% (4044/10000)
[Test]  Epoch: 36	Loss: 0.029685	Acc: 40.7% (4074/10000)
[Test]  Epoch: 37	Loss: 0.029756	Acc: 39.4% (3938/10000)
[Test]  Epoch: 38	Loss: 0.030210	Acc: 39.1% (3915/10000)
[Test]  Epoch: 39	Loss: 0.029776	Acc: 40.2% (4016/10000)
[Test]  Epoch: 40	Loss: 0.029144	Acc: 40.7% (4066/10000)
[Test]  Epoch: 41	Loss: 0.029383	Acc: 40.7% (4073/10000)
[Test]  Epoch: 42	Loss: 0.029224	Acc: 40.2% (4025/10000)
[Test]  Epoch: 43	Loss: 0.029395	Acc: 40.2% (4020/10000)
[Test]  Epoch: 44	Loss: 0.028894	Acc: 41.0% (4099/10000)
[Test]  Epoch: 45	Loss: 0.029058	Acc: 40.6% (4062/10000)
[Test]  Epoch: 46	Loss: 0.028837	Acc: 41.2% (4124/10000)
[Test]  Epoch: 47	Loss: 0.029037	Acc: 41.4% (4138/10000)
[Test]  Epoch: 48	Loss: 0.029274	Acc: 40.2% (4021/10000)
[Test]  Epoch: 49	Loss: 0.029322	Acc: 40.2% (4023/10000)
[Test]  Epoch: 50	Loss: 0.029108	Acc: 40.9% (4088/10000)
[Test]  Epoch: 51	Loss: 0.028871	Acc: 41.0% (4097/10000)
[Test]  Epoch: 52	Loss: 0.029084	Acc: 40.7% (4073/10000)
[Test]  Epoch: 53	Loss: 0.028773	Acc: 40.9% (4091/10000)
[Test]  Epoch: 54	Loss: 0.028840	Acc: 41.2% (4119/10000)
[Test]  Epoch: 55	Loss: 0.028992	Acc: 41.1% (4106/10000)
[Test]  Epoch: 56	Loss: 0.028663	Acc: 41.4% (4136/10000)
[Test]  Epoch: 57	Loss: 0.028839	Acc: 41.1% (4107/10000)
[Test]  Epoch: 58	Loss: 0.028767	Acc: 41.2% (4123/10000)
[Test]  Epoch: 59	Loss: 0.028697	Acc: 41.0% (4102/10000)
[Test]  Epoch: 60	Loss: 0.028764	Acc: 41.3% (4134/10000)
[Test]  Epoch: 61	Loss: 0.028720	Acc: 41.2% (4122/10000)
[Test]  Epoch: 62	Loss: 0.028665	Acc: 41.4% (4144/10000)
[Test]  Epoch: 63	Loss: 0.028463	Acc: 41.3% (4131/10000)
[Test]  Epoch: 64	Loss: 0.028469	Acc: 41.5% (4153/10000)
[Test]  Epoch: 65	Loss: 0.028473	Acc: 41.6% (4156/10000)
[Test]  Epoch: 66	Loss: 0.028374	Acc: 41.6% (4158/10000)
[Test]  Epoch: 67	Loss: 0.028487	Acc: 41.2% (4120/10000)
[Test]  Epoch: 68	Loss: 0.028468	Acc: 41.4% (4141/10000)
[Test]  Epoch: 69	Loss: 0.028489	Acc: 41.4% (4135/10000)
[Test]  Epoch: 70	Loss: 0.028444	Acc: 41.4% (4143/10000)
[Test]  Epoch: 71	Loss: 0.028474	Acc: 41.3% (4130/10000)
[Test]  Epoch: 72	Loss: 0.028517	Acc: 41.4% (4139/10000)
[Test]  Epoch: 73	Loss: 0.028471	Acc: 41.2% (4122/10000)
[Test]  Epoch: 74	Loss: 0.028370	Acc: 41.6% (4156/10000)
[Test]  Epoch: 75	Loss: 0.028262	Acc: 41.7% (4171/10000)
[Test]  Epoch: 76	Loss: 0.028381	Acc: 41.3% (4134/10000)
[Test]  Epoch: 77	Loss: 0.028336	Acc: 41.7% (4166/10000)
[Test]  Epoch: 78	Loss: 0.028302	Acc: 41.7% (4167/10000)
[Test]  Epoch: 79	Loss: 0.028440	Acc: 41.4% (4144/10000)
[Test]  Epoch: 80	Loss: 0.028299	Acc: 41.5% (4149/10000)
[Test]  Epoch: 81	Loss: 0.028305	Acc: 41.5% (4154/10000)
[Test]  Epoch: 82	Loss: 0.028353	Acc: 41.4% (4142/10000)
[Test]  Epoch: 83	Loss: 0.028415	Acc: 41.4% (4135/10000)
[Test]  Epoch: 84	Loss: 0.028334	Acc: 41.3% (4132/10000)
[Test]  Epoch: 85	Loss: 0.028332	Acc: 41.3% (4134/10000)
[Test]  Epoch: 86	Loss: 0.028395	Acc: 41.3% (4130/10000)
[Test]  Epoch: 87	Loss: 0.028431	Acc: 41.1% (4112/10000)
[Test]  Epoch: 88	Loss: 0.028274	Acc: 41.5% (4154/10000)
[Test]  Epoch: 89	Loss: 0.028452	Acc: 41.4% (4136/10000)
[Test]  Epoch: 90	Loss: 0.028373	Acc: 41.5% (4153/10000)
[Test]  Epoch: 91	Loss: 0.028338	Acc: 41.4% (4143/10000)
[Test]  Epoch: 92	Loss: 0.028428	Acc: 41.3% (4127/10000)
[Test]  Epoch: 93	Loss: 0.028402	Acc: 41.2% (4116/10000)
[Test]  Epoch: 94	Loss: 0.028363	Acc: 41.2% (4118/10000)
[Test]  Epoch: 95	Loss: 0.028309	Acc: 41.2% (4123/10000)
[Test]  Epoch: 96	Loss: 0.028345	Acc: 41.2% (4119/10000)
[Test]  Epoch: 97	Loss: 0.028310	Acc: 41.3% (4132/10000)
[Test]  Epoch: 98	Loss: 0.028463	Acc: 41.4% (4141/10000)
[Test]  Epoch: 99	Loss: 0.028335	Acc: 41.5% (4152/10000)
[Test]  Epoch: 100	Loss: 0.028319	Acc: 41.6% (4161/10000)
===========finish==========
['2024-08-19', '02:18:37.305443', '100', 'test', '0.028318982386589052', '41.61', '41.71']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.4.conv.3.weight', '_features.3.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.0.0.weight', '_features.2.conv.0.0.weight', '_features.6.conv.1.1.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.13.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.16.conv.0.1.weight', '_features.10.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.11.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.0.1.weight', '_features.4.conv.0.0.weight', '_features.8.conv.2.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.3.weight', '_features.9.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.440401	Acc: 11.9% (1195/10000)
[Test]  Epoch: 2	Loss: 0.034507	Acc: 30.6% (3060/10000)
[Test]  Epoch: 3	Loss: 0.032713	Acc: 29.6% (2958/10000)
[Test]  Epoch: 4	Loss: 0.031684	Acc: 32.7% (3271/10000)
[Test]  Epoch: 5	Loss: 0.032350	Acc: 32.0% (3201/10000)
[Test]  Epoch: 6	Loss: 0.031658	Acc: 33.5% (3355/10000)
[Test]  Epoch: 7	Loss: 0.033068	Acc: 33.1% (3312/10000)
[Test]  Epoch: 8	Loss: 0.036055	Acc: 32.0% (3199/10000)
[Test]  Epoch: 9	Loss: 0.035236	Acc: 32.7% (3274/10000)
[Test]  Epoch: 10	Loss: 0.035346	Acc: 33.0% (3302/10000)
[Test]  Epoch: 11	Loss: 0.035430	Acc: 33.8% (3381/10000)
[Test]  Epoch: 12	Loss: 0.033523	Acc: 34.3% (3428/10000)
[Test]  Epoch: 13	Loss: 0.034123	Acc: 34.6% (3465/10000)
[Test]  Epoch: 14	Loss: 0.033168	Acc: 34.8% (3475/10000)
[Test]  Epoch: 15	Loss: 0.033700	Acc: 35.0% (3499/10000)
[Test]  Epoch: 16	Loss: 0.034601	Acc: 32.9% (3291/10000)
[Test]  Epoch: 17	Loss: 0.034505	Acc: 34.6% (3464/10000)
[Test]  Epoch: 18	Loss: 0.033432	Acc: 35.7% (3571/10000)
[Test]  Epoch: 19	Loss: 0.034234	Acc: 34.6% (3465/10000)
[Test]  Epoch: 20	Loss: 0.033545	Acc: 35.3% (3527/10000)
[Test]  Epoch: 21	Loss: 0.032852	Acc: 35.6% (3565/10000)
[Test]  Epoch: 22	Loss: 0.032914	Acc: 36.2% (3622/10000)
[Test]  Epoch: 23	Loss: 0.032199	Acc: 36.2% (3617/10000)
[Test]  Epoch: 24	Loss: 0.032860	Acc: 35.3% (3529/10000)
[Test]  Epoch: 25	Loss: 0.033060	Acc: 36.1% (3607/10000)
[Test]  Epoch: 26	Loss: 0.032774	Acc: 35.2% (3518/10000)
[Test]  Epoch: 27	Loss: 0.033184	Acc: 36.0% (3604/10000)
[Test]  Epoch: 28	Loss: 0.032449	Acc: 36.3% (3628/10000)
[Test]  Epoch: 29	Loss: 0.031954	Acc: 36.1% (3615/10000)
[Test]  Epoch: 30	Loss: 0.031855	Acc: 36.9% (3685/10000)
[Test]  Epoch: 31	Loss: 0.031943	Acc: 36.1% (3609/10000)
[Test]  Epoch: 32	Loss: 0.031864	Acc: 36.1% (3608/10000)
[Test]  Epoch: 33	Loss: 0.031830	Acc: 37.0% (3701/10000)
[Test]  Epoch: 34	Loss: 0.031937	Acc: 36.0% (3602/10000)
[Test]  Epoch: 35	Loss: 0.031450	Acc: 36.4% (3642/10000)
[Test]  Epoch: 36	Loss: 0.031317	Acc: 37.5% (3747/10000)
[Test]  Epoch: 37	Loss: 0.031674	Acc: 36.4% (3644/10000)
[Test]  Epoch: 38	Loss: 0.032100	Acc: 36.8% (3682/10000)
[Test]  Epoch: 39	Loss: 0.031533	Acc: 36.9% (3689/10000)
[Test]  Epoch: 40	Loss: 0.031744	Acc: 36.8% (3679/10000)
[Test]  Epoch: 41	Loss: 0.031224	Acc: 36.3% (3629/10000)
[Test]  Epoch: 42	Loss: 0.031067	Acc: 37.2% (3724/10000)
[Test]  Epoch: 43	Loss: 0.031434	Acc: 35.9% (3594/10000)
[Test]  Epoch: 44	Loss: 0.031187	Acc: 36.7% (3670/10000)
[Test]  Epoch: 45	Loss: 0.030770	Acc: 37.7% (3767/10000)
[Test]  Epoch: 46	Loss: 0.031460	Acc: 36.5% (3651/10000)
[Test]  Epoch: 47	Loss: 0.030839	Acc: 36.7% (3669/10000)
[Test]  Epoch: 48	Loss: 0.031397	Acc: 36.9% (3687/10000)
[Test]  Epoch: 49	Loss: 0.031250	Acc: 37.2% (3721/10000)
[Test]  Epoch: 50	Loss: 0.031141	Acc: 36.8% (3680/10000)
[Test]  Epoch: 51	Loss: 0.030996	Acc: 37.3% (3728/10000)
[Test]  Epoch: 52	Loss: 0.031206	Acc: 36.6% (3656/10000)
[Test]  Epoch: 53	Loss: 0.030723	Acc: 37.4% (3739/10000)
[Test]  Epoch: 54	Loss: 0.030681	Acc: 37.2% (3723/10000)
[Test]  Epoch: 55	Loss: 0.030729	Acc: 36.7% (3674/10000)
[Test]  Epoch: 56	Loss: 0.030860	Acc: 36.3% (3629/10000)
[Test]  Epoch: 57	Loss: 0.030865	Acc: 37.0% (3705/10000)
[Test]  Epoch: 58	Loss: 0.031080	Acc: 37.5% (3751/10000)
[Test]  Epoch: 59	Loss: 0.031001	Acc: 37.1% (3714/10000)
[Test]  Epoch: 60	Loss: 0.030996	Acc: 37.1% (3711/10000)
[Test]  Epoch: 61	Loss: 0.030784	Acc: 37.6% (3765/10000)
[Test]  Epoch: 62	Loss: 0.030774	Acc: 37.8% (3780/10000)
[Test]  Epoch: 63	Loss: 0.030583	Acc: 38.0% (3796/10000)
[Test]  Epoch: 64	Loss: 0.030751	Acc: 38.0% (3795/10000)
[Test]  Epoch: 65	Loss: 0.030639	Acc: 37.8% (3777/10000)
[Test]  Epoch: 66	Loss: 0.030586	Acc: 37.6% (3762/10000)
[Test]  Epoch: 67	Loss: 0.030592	Acc: 37.5% (3751/10000)
[Test]  Epoch: 68	Loss: 0.030607	Acc: 37.8% (3781/10000)
[Test]  Epoch: 69	Loss: 0.030612	Acc: 37.6% (3765/10000)
[Test]  Epoch: 70	Loss: 0.030565	Acc: 37.7% (3773/10000)
[Test]  Epoch: 71	Loss: 0.030471	Acc: 37.6% (3765/10000)
[Test]  Epoch: 72	Loss: 0.030623	Acc: 37.7% (3773/10000)
[Test]  Epoch: 73	Loss: 0.030599	Acc: 37.7% (3772/10000)
[Test]  Epoch: 74	Loss: 0.030574	Acc: 37.6% (3765/10000)
[Test]  Epoch: 75	Loss: 0.030553	Acc: 37.9% (3788/10000)
[Test]  Epoch: 76	Loss: 0.030738	Acc: 37.7% (3770/10000)
[Test]  Epoch: 77	Loss: 0.030562	Acc: 37.6% (3757/10000)
[Test]  Epoch: 78	Loss: 0.030611	Acc: 37.5% (3749/10000)
[Test]  Epoch: 79	Loss: 0.030775	Acc: 37.5% (3748/10000)
[Test]  Epoch: 80	Loss: 0.030600	Acc: 37.6% (3756/10000)
[Test]  Epoch: 81	Loss: 0.030621	Acc: 37.6% (3762/10000)
[Test]  Epoch: 82	Loss: 0.030562	Acc: 37.6% (3761/10000)
[Test]  Epoch: 83	Loss: 0.030624	Acc: 37.3% (3731/10000)
[Test]  Epoch: 84	Loss: 0.030618	Acc: 37.5% (3745/10000)
[Test]  Epoch: 85	Loss: 0.030553	Acc: 37.3% (3728/10000)
[Test]  Epoch: 86	Loss: 0.030594	Acc: 37.6% (3763/10000)
[Test]  Epoch: 87	Loss: 0.030659	Acc: 37.7% (3767/10000)
[Test]  Epoch: 88	Loss: 0.030564	Acc: 37.5% (3753/10000)
[Test]  Epoch: 89	Loss: 0.030620	Acc: 37.6% (3762/10000)
[Test]  Epoch: 90	Loss: 0.030675	Acc: 37.4% (3736/10000)
[Test]  Epoch: 91	Loss: 0.030586	Acc: 37.6% (3759/10000)
[Test]  Epoch: 92	Loss: 0.030588	Acc: 37.7% (3768/10000)
[Test]  Epoch: 93	Loss: 0.030488	Acc: 37.7% (3769/10000)
[Test]  Epoch: 94	Loss: 0.030479	Acc: 37.8% (3776/10000)
[Test]  Epoch: 95	Loss: 0.030499	Acc: 37.8% (3776/10000)
[Test]  Epoch: 96	Loss: 0.030551	Acc: 37.7% (3774/10000)
[Test]  Epoch: 97	Loss: 0.030471	Acc: 37.6% (3764/10000)
[Test]  Epoch: 98	Loss: 0.030538	Acc: 37.7% (3770/10000)
[Test]  Epoch: 99	Loss: 0.030592	Acc: 37.7% (3772/10000)
[Test]  Epoch: 100	Loss: 0.030445	Acc: 37.7% (3766/10000)
===========finish==========
['2024-08-19', '02:20:59.539091', '100', 'test', '0.030444631397724152', '37.66', '37.96']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.4.conv.3.weight', '_features.3.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.0.0.weight', '_features.2.conv.0.0.weight', '_features.6.conv.1.1.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.13.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.16.conv.0.1.weight', '_features.10.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.11.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.0.1.weight', '_features.4.conv.0.0.weight', '_features.8.conv.2.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.3.weight', '_features.9.conv.0.0.weight', '_features.2.conv.2.weight', '_features.3.conv.2.weight', '_features.10.conv.2.weight', '_features.16.conv.3.weight', '_features.1.conv.0.0.weight', '_features.6.conv.2.weight', '_features.11.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.7.conv.2.weight', '_features.1.conv.1.weight', '_features.9.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.204592	Acc: 16.8% (1677/10000)
[Test]  Epoch: 2	Loss: 0.033562	Acc: 34.5% (3449/10000)
[Test]  Epoch: 3	Loss: 0.026550	Acc: 41.3% (4126/10000)
[Test]  Epoch: 4	Loss: 0.027505	Acc: 39.7% (3970/10000)
[Test]  Epoch: 5	Loss: 0.026870	Acc: 42.7% (4273/10000)
[Test]  Epoch: 6	Loss: 0.026531	Acc: 43.6% (4357/10000)
[Test]  Epoch: 7	Loss: 0.026711	Acc: 44.2% (4424/10000)
[Test]  Epoch: 8	Loss: 0.026804	Acc: 43.3% (4327/10000)
[Test]  Epoch: 9	Loss: 0.027199	Acc: 44.1% (4407/10000)
[Test]  Epoch: 10	Loss: 0.028000	Acc: 43.1% (4306/10000)
[Test]  Epoch: 11	Loss: 0.026072	Acc: 45.6% (4556/10000)
[Test]  Epoch: 12	Loss: 0.026295	Acc: 45.0% (4497/10000)
[Test]  Epoch: 13	Loss: 0.025579	Acc: 46.0% (4596/10000)
[Test]  Epoch: 14	Loss: 0.025846	Acc: 45.9% (4593/10000)
[Test]  Epoch: 15	Loss: 0.025899	Acc: 45.9% (4590/10000)
[Test]  Epoch: 16	Loss: 0.025339	Acc: 46.8% (4677/10000)
[Test]  Epoch: 17	Loss: 0.025356	Acc: 46.7% (4670/10000)
[Test]  Epoch: 18	Loss: 0.025126	Acc: 46.4% (4640/10000)
[Test]  Epoch: 19	Loss: 0.024989	Acc: 47.3% (4729/10000)
[Test]  Epoch: 20	Loss: 0.024793	Acc: 47.2% (4718/10000)
[Test]  Epoch: 21	Loss: 0.025105	Acc: 47.0% (4702/10000)
[Test]  Epoch: 22	Loss: 0.024584	Acc: 47.2% (4722/10000)
[Test]  Epoch: 23	Loss: 0.024743	Acc: 47.2% (4717/10000)
[Test]  Epoch: 24	Loss: 0.024861	Acc: 46.9% (4690/10000)
[Test]  Epoch: 25	Loss: 0.024859	Acc: 47.4% (4735/10000)
[Test]  Epoch: 26	Loss: 0.024615	Acc: 47.2% (4716/10000)
[Test]  Epoch: 27	Loss: 0.024752	Acc: 47.2% (4716/10000)
[Test]  Epoch: 28	Loss: 0.024527	Acc: 47.6% (4761/10000)
[Test]  Epoch: 29	Loss: 0.024251	Acc: 48.1% (4809/10000)
[Test]  Epoch: 30	Loss: 0.024172	Acc: 47.7% (4774/10000)
[Test]  Epoch: 31	Loss: 0.023975	Acc: 48.3% (4829/10000)
[Test]  Epoch: 32	Loss: 0.024082	Acc: 47.9% (4785/10000)
[Test]  Epoch: 33	Loss: 0.024593	Acc: 47.5% (4745/10000)
[Test]  Epoch: 34	Loss: 0.024097	Acc: 48.0% (4802/10000)
[Test]  Epoch: 35	Loss: 0.024183	Acc: 48.0% (4805/10000)
[Test]  Epoch: 36	Loss: 0.024058	Acc: 47.9% (4787/10000)
[Test]  Epoch: 37	Loss: 0.024263	Acc: 48.0% (4796/10000)
[Test]  Epoch: 38	Loss: 0.024324	Acc: 47.7% (4767/10000)
[Test]  Epoch: 39	Loss: 0.024210	Acc: 47.5% (4755/10000)
[Test]  Epoch: 40	Loss: 0.024012	Acc: 48.0% (4800/10000)
[Test]  Epoch: 41	Loss: 0.024265	Acc: 47.6% (4760/10000)
[Test]  Epoch: 42	Loss: 0.024372	Acc: 47.3% (4732/10000)
[Test]  Epoch: 43	Loss: 0.024248	Acc: 47.1% (4713/10000)
[Test]  Epoch: 44	Loss: 0.023871	Acc: 48.4% (4839/10000)
[Test]  Epoch: 45	Loss: 0.023863	Acc: 48.0% (4804/10000)
[Test]  Epoch: 46	Loss: 0.023861	Acc: 48.5% (4853/10000)
[Test]  Epoch: 47	Loss: 0.024164	Acc: 47.9% (4786/10000)
[Test]  Epoch: 48	Loss: 0.023905	Acc: 48.7% (4866/10000)
[Test]  Epoch: 49	Loss: 0.023801	Acc: 48.3% (4827/10000)
[Test]  Epoch: 50	Loss: 0.023780	Acc: 48.1% (4809/10000)
[Test]  Epoch: 51	Loss: 0.023651	Acc: 49.1% (4911/10000)
[Test]  Epoch: 52	Loss: 0.023542	Acc: 49.1% (4912/10000)
[Test]  Epoch: 53	Loss: 0.023685	Acc: 48.6% (4861/10000)
[Test]  Epoch: 54	Loss: 0.023596	Acc: 48.2% (4825/10000)
[Test]  Epoch: 55	Loss: 0.023682	Acc: 48.9% (4892/10000)
[Test]  Epoch: 56	Loss: 0.023611	Acc: 48.7% (4874/10000)
[Test]  Epoch: 57	Loss: 0.023696	Acc: 48.2% (4816/10000)
[Test]  Epoch: 58	Loss: 0.023653	Acc: 48.4% (4836/10000)
[Test]  Epoch: 59	Loss: 0.023594	Acc: 48.8% (4877/10000)
[Test]  Epoch: 60	Loss: 0.023795	Acc: 48.9% (4887/10000)
[Test]  Epoch: 61	Loss: 0.023641	Acc: 49.0% (4896/10000)
[Test]  Epoch: 62	Loss: 0.023648	Acc: 49.1% (4910/10000)
[Test]  Epoch: 63	Loss: 0.023502	Acc: 49.1% (4914/10000)
[Test]  Epoch: 64	Loss: 0.023528	Acc: 49.1% (4911/10000)
[Test]  Epoch: 65	Loss: 0.023480	Acc: 49.1% (4912/10000)
[Test]  Epoch: 66	Loss: 0.023472	Acc: 48.9% (4892/10000)
[Test]  Epoch: 67	Loss: 0.023571	Acc: 48.9% (4885/10000)
[Test]  Epoch: 68	Loss: 0.023537	Acc: 48.9% (4889/10000)
[Test]  Epoch: 69	Loss: 0.023535	Acc: 48.8% (4876/10000)
[Test]  Epoch: 70	Loss: 0.023511	Acc: 49.1% (4915/10000)
[Test]  Epoch: 71	Loss: 0.023563	Acc: 48.8% (4881/10000)
[Test]  Epoch: 72	Loss: 0.023597	Acc: 48.6% (4860/10000)
[Test]  Epoch: 73	Loss: 0.023557	Acc: 48.7% (4874/10000)
[Test]  Epoch: 74	Loss: 0.023470	Acc: 48.8% (4880/10000)
[Test]  Epoch: 75	Loss: 0.023468	Acc: 49.0% (4903/10000)
[Test]  Epoch: 76	Loss: 0.023555	Acc: 48.8% (4884/10000)
[Test]  Epoch: 77	Loss: 0.023487	Acc: 49.0% (4898/10000)
[Test]  Epoch: 78	Loss: 0.023523	Acc: 48.8% (4877/10000)
[Test]  Epoch: 79	Loss: 0.023524	Acc: 49.0% (4902/10000)
[Test]  Epoch: 80	Loss: 0.023547	Acc: 48.8% (4883/10000)
[Test]  Epoch: 81	Loss: 0.023513	Acc: 49.1% (4908/10000)
[Test]  Epoch: 82	Loss: 0.023519	Acc: 49.0% (4904/10000)
[Test]  Epoch: 83	Loss: 0.023546	Acc: 48.9% (4889/10000)
[Test]  Epoch: 84	Loss: 0.023525	Acc: 48.9% (4891/10000)
[Test]  Epoch: 85	Loss: 0.023525	Acc: 48.9% (4886/10000)
[Test]  Epoch: 86	Loss: 0.023495	Acc: 49.1% (4912/10000)
[Test]  Epoch: 87	Loss: 0.023469	Acc: 49.1% (4906/10000)
[Test]  Epoch: 88	Loss: 0.023478	Acc: 48.9% (4894/10000)
[Test]  Epoch: 89	Loss: 0.023543	Acc: 49.1% (4912/10000)
[Test]  Epoch: 90	Loss: 0.023492	Acc: 49.1% (4915/10000)
[Test]  Epoch: 91	Loss: 0.023488	Acc: 49.0% (4896/10000)
[Test]  Epoch: 92	Loss: 0.023561	Acc: 48.9% (4894/10000)
[Test]  Epoch: 93	Loss: 0.023466	Acc: 49.0% (4900/10000)
[Test]  Epoch: 94	Loss: 0.023474	Acc: 48.9% (4889/10000)
[Test]  Epoch: 95	Loss: 0.023465	Acc: 48.9% (4890/10000)
[Test]  Epoch: 96	Loss: 0.023502	Acc: 49.2% (4922/10000)
[Test]  Epoch: 97	Loss: 0.023509	Acc: 48.9% (4890/10000)
[Test]  Epoch: 98	Loss: 0.023519	Acc: 48.7% (4870/10000)
[Test]  Epoch: 99	Loss: 0.023488	Acc: 49.0% (4905/10000)
[Test]  Epoch: 100	Loss: 0.023423	Acc: 48.9% (4894/10000)
===========finish==========
['2024-08-19', '02:23:22.437748', '100', 'test', '0.023422706413269042', '48.94', '49.22']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.4.conv.3.weight', '_features.3.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.0.0.weight', '_features.2.conv.0.0.weight', '_features.6.conv.1.1.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.13.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.16.conv.0.1.weight', '_features.10.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.11.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.0.1.weight', '_features.4.conv.0.0.weight', '_features.8.conv.2.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.3.weight', '_features.9.conv.0.0.weight', '_features.2.conv.2.weight', '_features.3.conv.2.weight', '_features.10.conv.2.weight', '_features.16.conv.3.weight', '_features.1.conv.0.0.weight', '_features.6.conv.2.weight', '_features.11.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.7.conv.2.weight', '_features.1.conv.1.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.11.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.15.conv.2.weight', '_features.4.conv.2.weight', '_features.16.conv.2.weight', '_features.14.conv.3.weight', '_features.14.conv.1.1.weight', '_features.14.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.190443	Acc: 10.5% (1051/10000)
[Test]  Epoch: 2	Loss: 0.028403	Acc: 39.6% (3958/10000)
[Test]  Epoch: 3	Loss: 0.025981	Acc: 42.4% (4235/10000)
[Test]  Epoch: 4	Loss: 0.025360	Acc: 43.9% (4392/10000)
[Test]  Epoch: 5	Loss: 0.025359	Acc: 44.6% (4464/10000)
[Test]  Epoch: 6	Loss: 0.025654	Acc: 44.4% (4443/10000)
[Test]  Epoch: 7	Loss: 0.025649	Acc: 45.5% (4552/10000)
[Test]  Epoch: 8	Loss: 0.025526	Acc: 45.2% (4523/10000)
[Test]  Epoch: 9	Loss: 0.025809	Acc: 45.9% (4590/10000)
[Test]  Epoch: 10	Loss: 0.025223	Acc: 46.4% (4640/10000)
[Test]  Epoch: 11	Loss: 0.025158	Acc: 46.5% (4650/10000)
[Test]  Epoch: 12	Loss: 0.025769	Acc: 46.0% (4601/10000)
[Test]  Epoch: 13	Loss: 0.025095	Acc: 47.1% (4706/10000)
[Test]  Epoch: 14	Loss: 0.024565	Acc: 48.0% (4798/10000)
[Test]  Epoch: 15	Loss: 0.024336	Acc: 48.1% (4813/10000)
[Test]  Epoch: 16	Loss: 0.024356	Acc: 48.5% (4845/10000)
[Test]  Epoch: 17	Loss: 0.024533	Acc: 47.7% (4766/10000)
[Test]  Epoch: 18	Loss: 0.024392	Acc: 48.1% (4811/10000)
[Test]  Epoch: 19	Loss: 0.024351	Acc: 48.8% (4876/10000)
[Test]  Epoch: 20	Loss: 0.024146	Acc: 48.8% (4883/10000)
[Test]  Epoch: 21	Loss: 0.024291	Acc: 48.2% (4825/10000)
[Test]  Epoch: 22	Loss: 0.023935	Acc: 49.0% (4895/10000)
[Test]  Epoch: 23	Loss: 0.023906	Acc: 48.7% (4870/10000)
[Test]  Epoch: 24	Loss: 0.023996	Acc: 48.4% (4844/10000)
[Test]  Epoch: 25	Loss: 0.024019	Acc: 48.4% (4838/10000)
[Test]  Epoch: 26	Loss: 0.023785	Acc: 48.8% (4876/10000)
[Test]  Epoch: 27	Loss: 0.023914	Acc: 48.7% (4874/10000)
[Test]  Epoch: 28	Loss: 0.023721	Acc: 48.8% (4878/10000)
[Test]  Epoch: 29	Loss: 0.023749	Acc: 48.2% (4825/10000)
[Test]  Epoch: 30	Loss: 0.023731	Acc: 48.7% (4867/10000)
[Test]  Epoch: 31	Loss: 0.023558	Acc: 48.8% (4876/10000)
[Test]  Epoch: 32	Loss: 0.023465	Acc: 49.1% (4914/10000)
[Test]  Epoch: 33	Loss: 0.023490	Acc: 49.3% (4929/10000)
[Test]  Epoch: 34	Loss: 0.023437	Acc: 49.2% (4917/10000)
[Test]  Epoch: 35	Loss: 0.023320	Acc: 49.6% (4962/10000)
[Test]  Epoch: 36	Loss: 0.023460	Acc: 49.4% (4941/10000)
[Test]  Epoch: 37	Loss: 0.023333	Acc: 49.1% (4911/10000)
[Test]  Epoch: 38	Loss: 0.023497	Acc: 49.1% (4913/10000)
[Test]  Epoch: 39	Loss: 0.023475	Acc: 49.0% (4905/10000)
[Test]  Epoch: 40	Loss: 0.023163	Acc: 49.2% (4921/10000)
[Test]  Epoch: 41	Loss: 0.023103	Acc: 49.9% (4986/10000)
[Test]  Epoch: 42	Loss: 0.023112	Acc: 49.9% (4993/10000)
[Test]  Epoch: 43	Loss: 0.023065	Acc: 50.3% (5029/10000)
[Test]  Epoch: 44	Loss: 0.023079	Acc: 49.9% (4989/10000)
[Test]  Epoch: 45	Loss: 0.023145	Acc: 49.7% (4973/10000)
[Test]  Epoch: 46	Loss: 0.023122	Acc: 49.8% (4980/10000)
[Test]  Epoch: 47	Loss: 0.023146	Acc: 49.7% (4974/10000)
[Test]  Epoch: 48	Loss: 0.023074	Acc: 49.9% (4988/10000)
[Test]  Epoch: 49	Loss: 0.023128	Acc: 50.2% (5022/10000)
[Test]  Epoch: 50	Loss: 0.023221	Acc: 49.8% (4977/10000)
[Test]  Epoch: 51	Loss: 0.023031	Acc: 50.3% (5031/10000)
[Test]  Epoch: 52	Loss: 0.023026	Acc: 50.1% (5010/10000)
[Test]  Epoch: 53	Loss: 0.023052	Acc: 50.0% (4998/10000)
[Test]  Epoch: 54	Loss: 0.023039	Acc: 49.9% (4990/10000)
[Test]  Epoch: 55	Loss: 0.023004	Acc: 50.4% (5038/10000)
[Test]  Epoch: 56	Loss: 0.022947	Acc: 50.3% (5027/10000)
[Test]  Epoch: 57	Loss: 0.022882	Acc: 50.7% (5066/10000)
[Test]  Epoch: 58	Loss: 0.022950	Acc: 50.7% (5068/10000)
[Test]  Epoch: 59	Loss: 0.022887	Acc: 50.0% (5001/10000)
[Test]  Epoch: 60	Loss: 0.023061	Acc: 50.2% (5017/10000)
[Test]  Epoch: 61	Loss: 0.022886	Acc: 50.3% (5034/10000)
[Test]  Epoch: 62	Loss: 0.022812	Acc: 50.5% (5055/10000)
[Test]  Epoch: 63	Loss: 0.022728	Acc: 50.6% (5065/10000)
[Test]  Epoch: 64	Loss: 0.022765	Acc: 50.7% (5067/10000)
[Test]  Epoch: 65	Loss: 0.022740	Acc: 50.6% (5063/10000)
[Test]  Epoch: 66	Loss: 0.022670	Acc: 50.9% (5086/10000)
[Test]  Epoch: 67	Loss: 0.022865	Acc: 50.3% (5028/10000)
[Test]  Epoch: 68	Loss: 0.022718	Acc: 50.7% (5071/10000)
[Test]  Epoch: 69	Loss: 0.022733	Acc: 50.8% (5084/10000)
[Test]  Epoch: 70	Loss: 0.022725	Acc: 50.7% (5072/10000)
[Test]  Epoch: 71	Loss: 0.022810	Acc: 50.3% (5027/10000)
[Test]  Epoch: 72	Loss: 0.022813	Acc: 50.5% (5045/10000)
[Test]  Epoch: 73	Loss: 0.022698	Acc: 50.5% (5054/10000)
[Test]  Epoch: 74	Loss: 0.022745	Acc: 50.5% (5051/10000)
[Test]  Epoch: 75	Loss: 0.022746	Acc: 50.5% (5052/10000)
[Test]  Epoch: 76	Loss: 0.022685	Acc: 50.7% (5072/10000)
[Test]  Epoch: 77	Loss: 0.022710	Acc: 50.6% (5062/10000)
[Test]  Epoch: 78	Loss: 0.022714	Acc: 50.7% (5069/10000)
[Test]  Epoch: 79	Loss: 0.022704	Acc: 50.9% (5093/10000)
[Test]  Epoch: 80	Loss: 0.022745	Acc: 50.3% (5033/10000)
[Test]  Epoch: 81	Loss: 0.022768	Acc: 50.6% (5057/10000)
[Test]  Epoch: 82	Loss: 0.022779	Acc: 50.6% (5056/10000)
[Test]  Epoch: 83	Loss: 0.022679	Acc: 50.4% (5043/10000)
[Test]  Epoch: 84	Loss: 0.022643	Acc: 50.8% (5075/10000)
[Test]  Epoch: 85	Loss: 0.022719	Acc: 50.6% (5065/10000)
[Test]  Epoch: 86	Loss: 0.022644	Acc: 50.7% (5069/10000)
[Test]  Epoch: 87	Loss: 0.022672	Acc: 50.7% (5066/10000)
[Test]  Epoch: 88	Loss: 0.022667	Acc: 50.8% (5079/10000)
[Test]  Epoch: 89	Loss: 0.022719	Acc: 50.7% (5071/10000)
[Test]  Epoch: 90	Loss: 0.022697	Acc: 50.6% (5062/10000)
[Test]  Epoch: 91	Loss: 0.022651	Acc: 50.6% (5064/10000)
[Test]  Epoch: 92	Loss: 0.022677	Acc: 50.6% (5058/10000)
[Test]  Epoch: 93	Loss: 0.022635	Acc: 50.6% (5063/10000)
[Test]  Epoch: 94	Loss: 0.022773	Acc: 50.7% (5070/10000)
[Test]  Epoch: 95	Loss: 0.022659	Acc: 50.5% (5054/10000)
[Test]  Epoch: 96	Loss: 0.022640	Acc: 50.7% (5073/10000)
[Test]  Epoch: 97	Loss: 0.022646	Acc: 50.6% (5063/10000)
[Test]  Epoch: 98	Loss: 0.022769	Acc: 50.5% (5046/10000)
[Test]  Epoch: 99	Loss: 0.022707	Acc: 50.5% (5048/10000)
[Test]  Epoch: 100	Loss: 0.022665	Acc: 50.7% (5072/10000)
===========finish==========
['2024-08-19', '02:25:47.899069', '100', 'test', '0.022665210974216463', '50.72', '50.93']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.13.conv.0.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.3.conv.3.weight', '_features.1.conv.2.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.1.weight', '_features.8.conv.3.weight', '_features.6.conv.3.weight', '_features.2.conv.3.weight', '_features.13.conv.3.weight', '_features.5.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.12.conv.3.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.4.conv.3.weight', '_features.3.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.0.0.weight', '_features.2.conv.0.0.weight', '_features.6.conv.1.1.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.13.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.16.conv.0.1.weight', '_features.10.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.11.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.0.1.weight', '_features.4.conv.0.0.weight', '_features.8.conv.2.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.3.weight', '_features.9.conv.0.0.weight', '_features.2.conv.2.weight', '_features.3.conv.2.weight', '_features.10.conv.2.weight', '_features.16.conv.3.weight', '_features.1.conv.0.0.weight', '_features.6.conv.2.weight', '_features.11.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.7.conv.2.weight', '_features.1.conv.1.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.11.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.15.conv.2.weight', '_features.4.conv.2.weight', '_features.16.conv.2.weight', '_features.14.conv.3.weight', '_features.14.conv.1.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.18.1.weight', 'last_linear.weight', '_features.18.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.071867	Acc: 12.8% (1281/10000)
[Test]  Epoch: 2	Loss: 0.028834	Acc: 34.7% (3470/10000)
[Test]  Epoch: 3	Loss: 0.027553	Acc: 38.5% (3849/10000)
[Test]  Epoch: 4	Loss: 0.026547	Acc: 40.7% (4073/10000)
[Test]  Epoch: 5	Loss: 0.025642	Acc: 45.1% (4510/10000)
[Test]  Epoch: 6	Loss: 0.025267	Acc: 45.8% (4577/10000)
[Test]  Epoch: 7	Loss: 0.025980	Acc: 44.7% (4474/10000)
[Test]  Epoch: 8	Loss: 0.026694	Acc: 44.7% (4470/10000)
[Test]  Epoch: 9	Loss: 0.026003	Acc: 46.8% (4679/10000)
[Test]  Epoch: 10	Loss: 0.026044	Acc: 46.1% (4612/10000)
[Test]  Epoch: 11	Loss: 0.025341	Acc: 47.0% (4702/10000)
[Test]  Epoch: 12	Loss: 0.025097	Acc: 48.0% (4796/10000)
[Test]  Epoch: 13	Loss: 0.025481	Acc: 47.6% (4765/10000)
[Test]  Epoch: 14	Loss: 0.025368	Acc: 47.2% (4723/10000)
[Test]  Epoch: 15	Loss: 0.025063	Acc: 48.3% (4826/10000)
[Test]  Epoch: 16	Loss: 0.024975	Acc: 48.0% (4805/10000)
[Test]  Epoch: 17	Loss: 0.024388	Acc: 49.3% (4926/10000)
[Test]  Epoch: 18	Loss: 0.024709	Acc: 49.7% (4969/10000)
[Test]  Epoch: 19	Loss: 0.024200	Acc: 49.8% (4981/10000)
[Test]  Epoch: 20	Loss: 0.024609	Acc: 49.6% (4959/10000)
[Test]  Epoch: 21	Loss: 0.024342	Acc: 49.1% (4906/10000)
[Test]  Epoch: 22	Loss: 0.024085	Acc: 50.1% (5014/10000)
[Test]  Epoch: 23	Loss: 0.024049	Acc: 49.9% (4988/10000)
[Test]  Epoch: 24	Loss: 0.024068	Acc: 49.6% (4964/10000)
[Test]  Epoch: 25	Loss: 0.023952	Acc: 50.4% (5043/10000)
[Test]  Epoch: 26	Loss: 0.024197	Acc: 49.5% (4949/10000)
[Test]  Epoch: 27	Loss: 0.023944	Acc: 50.3% (5031/10000)
[Test]  Epoch: 28	Loss: 0.023604	Acc: 50.7% (5074/10000)
[Test]  Epoch: 29	Loss: 0.023695	Acc: 50.3% (5026/10000)
[Test]  Epoch: 30	Loss: 0.023805	Acc: 50.0% (4995/10000)
[Test]  Epoch: 31	Loss: 0.023493	Acc: 50.4% (5036/10000)
[Test]  Epoch: 32	Loss: 0.023477	Acc: 50.2% (5021/10000)
[Test]  Epoch: 33	Loss: 0.023460	Acc: 50.5% (5055/10000)
[Test]  Epoch: 34	Loss: 0.023926	Acc: 50.2% (5021/10000)
[Test]  Epoch: 35	Loss: 0.023555	Acc: 50.5% (5046/10000)
[Test]  Epoch: 36	Loss: 0.023689	Acc: 50.4% (5042/10000)
[Test]  Epoch: 37	Loss: 0.023443	Acc: 50.4% (5040/10000)
[Test]  Epoch: 38	Loss: 0.023830	Acc: 49.6% (4964/10000)
[Test]  Epoch: 39	Loss: 0.023483	Acc: 50.3% (5029/10000)
[Test]  Epoch: 40	Loss: 0.023423	Acc: 50.3% (5031/10000)
[Test]  Epoch: 41	Loss: 0.023309	Acc: 50.5% (5049/10000)
[Test]  Epoch: 42	Loss: 0.023319	Acc: 50.8% (5079/10000)
[Test]  Epoch: 43	Loss: 0.023471	Acc: 50.8% (5077/10000)
[Test]  Epoch: 44	Loss: 0.023499	Acc: 50.4% (5040/10000)
[Test]  Epoch: 45	Loss: 0.023339	Acc: 51.0% (5102/10000)
[Test]  Epoch: 46	Loss: 0.023324	Acc: 50.8% (5081/10000)
[Test]  Epoch: 47	Loss: 0.023005	Acc: 51.3% (5127/10000)
[Test]  Epoch: 48	Loss: 0.023018	Acc: 51.7% (5168/10000)
[Test]  Epoch: 49	Loss: 0.022923	Acc: 51.5% (5149/10000)
[Test]  Epoch: 50	Loss: 0.023183	Acc: 51.7% (5167/10000)
[Test]  Epoch: 51	Loss: 0.023008	Acc: 51.6% (5157/10000)
[Test]  Epoch: 52	Loss: 0.023217	Acc: 51.1% (5109/10000)
[Test]  Epoch: 53	Loss: 0.023196	Acc: 51.1% (5112/10000)
[Test]  Epoch: 54	Loss: 0.022911	Acc: 51.4% (5143/10000)
[Test]  Epoch: 55	Loss: 0.022810	Acc: 51.7% (5166/10000)
[Test]  Epoch: 56	Loss: 0.022996	Acc: 51.3% (5127/10000)
[Test]  Epoch: 57	Loss: 0.022980	Acc: 51.1% (5109/10000)
[Test]  Epoch: 58	Loss: 0.023042	Acc: 51.7% (5173/10000)
[Test]  Epoch: 59	Loss: 0.022834	Acc: 51.5% (5149/10000)
[Test]  Epoch: 60	Loss: 0.022890	Acc: 51.9% (5191/10000)
[Test]  Epoch: 61	Loss: 0.022765	Acc: 51.6% (5158/10000)
[Test]  Epoch: 62	Loss: 0.022695	Acc: 51.8% (5177/10000)
[Test]  Epoch: 63	Loss: 0.022615	Acc: 51.8% (5175/10000)
[Test]  Epoch: 64	Loss: 0.022650	Acc: 51.9% (5194/10000)
[Test]  Epoch: 65	Loss: 0.022651	Acc: 51.7% (5167/10000)
[Test]  Epoch: 66	Loss: 0.022591	Acc: 51.8% (5177/10000)
[Test]  Epoch: 67	Loss: 0.022732	Acc: 51.5% (5152/10000)
[Test]  Epoch: 68	Loss: 0.022575	Acc: 51.6% (5161/10000)
[Test]  Epoch: 69	Loss: 0.022738	Acc: 51.7% (5170/10000)
[Test]  Epoch: 70	Loss: 0.022672	Acc: 51.7% (5166/10000)
[Test]  Epoch: 71	Loss: 0.022763	Acc: 51.4% (5144/10000)
[Test]  Epoch: 72	Loss: 0.022706	Acc: 51.5% (5154/10000)
[Test]  Epoch: 73	Loss: 0.022611	Acc: 51.6% (5160/10000)
[Test]  Epoch: 74	Loss: 0.022579	Acc: 51.8% (5175/10000)
[Test]  Epoch: 75	Loss: 0.022617	Acc: 51.7% (5171/10000)
[Test]  Epoch: 76	Loss: 0.022664	Acc: 51.6% (5156/10000)
[Test]  Epoch: 77	Loss: 0.022581	Acc: 51.8% (5177/10000)
[Test]  Epoch: 78	Loss: 0.022620	Acc: 51.6% (5160/10000)
[Test]  Epoch: 79	Loss: 0.022582	Acc: 51.7% (5168/10000)
[Test]  Epoch: 80	Loss: 0.022699	Acc: 51.8% (5177/10000)
[Test]  Epoch: 81	Loss: 0.022667	Acc: 51.9% (5187/10000)
[Test]  Epoch: 82	Loss: 0.022659	Acc: 51.9% (5192/10000)
[Test]  Epoch: 83	Loss: 0.022630	Acc: 51.7% (5166/10000)
[Test]  Epoch: 84	Loss: 0.022583	Acc: 51.8% (5175/10000)
[Test]  Epoch: 85	Loss: 0.022699	Acc: 51.6% (5157/10000)
[Test]  Epoch: 86	Loss: 0.022617	Acc: 51.9% (5187/10000)
[Test]  Epoch: 87	Loss: 0.022631	Acc: 51.6% (5159/10000)
[Test]  Epoch: 88	Loss: 0.022566	Acc: 51.7% (5167/10000)
[Test]  Epoch: 89	Loss: 0.022647	Acc: 51.7% (5166/10000)
[Test]  Epoch: 90	Loss: 0.022567	Acc: 51.9% (5185/10000)
[Test]  Epoch: 91	Loss: 0.022599	Acc: 51.6% (5165/10000)
[Test]  Epoch: 92	Loss: 0.022608	Acc: 51.7% (5166/10000)
[Test]  Epoch: 93	Loss: 0.022605	Acc: 51.7% (5173/10000)
[Test]  Epoch: 94	Loss: 0.022521	Acc: 51.8% (5176/10000)
[Test]  Epoch: 95	Loss: 0.022527	Acc: 51.5% (5152/10000)
[Test]  Epoch: 96	Loss: 0.022581	Acc: 51.7% (5171/10000)
[Test]  Epoch: 97	Loss: 0.022557	Acc: 51.9% (5189/10000)
[Test]  Epoch: 98	Loss: 0.022677	Acc: 51.7% (5169/10000)
[Test]  Epoch: 99	Loss: 0.022566	Acc: 51.7% (5166/10000)
[Test]  Epoch: 100	Loss: 0.022530	Acc: 51.7% (5168/10000)
===========finish==========
['2024-08-19', '02:28:13.991349', '100', 'test', '0.022529909157752992', '51.68', '51.94']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042411	Acc: 38.2% (3822/10000)
[Test]  Epoch: 2	Loss: 0.043066	Acc: 36.7% (3672/10000)
[Test]  Epoch: 3	Loss: 0.043287	Acc: 36.5% (3649/10000)
[Test]  Epoch: 4	Loss: 0.043371	Acc: 36.5% (3648/10000)
[Test]  Epoch: 5	Loss: 0.043843	Acc: 36.3% (3634/10000)
[Test]  Epoch: 6	Loss: 0.044458	Acc: 35.0% (3505/10000)
[Test]  Epoch: 7	Loss: 0.044449	Acc: 35.4% (3540/10000)
[Test]  Epoch: 8	Loss: 0.044542	Acc: 35.4% (3535/10000)
[Test]  Epoch: 9	Loss: 0.044431	Acc: 35.7% (3574/10000)
[Test]  Epoch: 10	Loss: 0.044513	Acc: 35.6% (3557/10000)
[Test]  Epoch: 11	Loss: 0.044716	Acc: 35.1% (3508/10000)
[Test]  Epoch: 12	Loss: 0.044547	Acc: 35.4% (3537/10000)
[Test]  Epoch: 13	Loss: 0.045210	Acc: 34.9% (3492/10000)
[Test]  Epoch: 14	Loss: 0.044889	Acc: 34.8% (3481/10000)
[Test]  Epoch: 15	Loss: 0.044991	Acc: 34.9% (3493/10000)
[Test]  Epoch: 16	Loss: 0.045325	Acc: 34.0% (3395/10000)
[Test]  Epoch: 17	Loss: 0.045199	Acc: 34.8% (3477/10000)
[Test]  Epoch: 18	Loss: 0.045281	Acc: 34.7% (3467/10000)
[Test]  Epoch: 19	Loss: 0.045300	Acc: 33.8% (3381/10000)
[Test]  Epoch: 20	Loss: 0.045411	Acc: 34.6% (3465/10000)
[Test]  Epoch: 21	Loss: 0.045304	Acc: 34.8% (3481/10000)
[Test]  Epoch: 22	Loss: 0.045367	Acc: 34.0% (3396/10000)
[Test]  Epoch: 23	Loss: 0.045634	Acc: 34.2% (3418/10000)
[Test]  Epoch: 24	Loss: 0.045469	Acc: 34.3% (3427/10000)
[Test]  Epoch: 25	Loss: 0.045120	Acc: 34.2% (3419/10000)
[Test]  Epoch: 26	Loss: 0.045646	Acc: 34.1% (3411/10000)
[Test]  Epoch: 27	Loss: 0.045300	Acc: 33.7% (3371/10000)
[Test]  Epoch: 28	Loss: 0.045473	Acc: 34.5% (3455/10000)
[Test]  Epoch: 29	Loss: 0.045654	Acc: 34.0% (3397/10000)
[Test]  Epoch: 30	Loss: 0.046104	Acc: 33.9% (3393/10000)
[Test]  Epoch: 31	Loss: 0.046262	Acc: 32.8% (3284/10000)
[Test]  Epoch: 32	Loss: 0.045741	Acc: 34.3% (3426/10000)
[Test]  Epoch: 33	Loss: 0.046027	Acc: 33.8% (3378/10000)
[Test]  Epoch: 34	Loss: 0.045827	Acc: 33.6% (3361/10000)
[Test]  Epoch: 35	Loss: 0.046155	Acc: 33.9% (3391/10000)
[Test]  Epoch: 36	Loss: 0.046399	Acc: 32.7% (3266/10000)
[Test]  Epoch: 37	Loss: 0.046400	Acc: 33.1% (3307/10000)
[Test]  Epoch: 38	Loss: 0.046306	Acc: 33.4% (3336/10000)
[Test]  Epoch: 39	Loss: 0.046273	Acc: 33.1% (3307/10000)
[Test]  Epoch: 40	Loss: 0.046533	Acc: 33.0% (3299/10000)
[Test]  Epoch: 41	Loss: 0.046513	Acc: 32.8% (3278/10000)
[Test]  Epoch: 42	Loss: 0.046128	Acc: 32.9% (3291/10000)
[Test]  Epoch: 43	Loss: 0.046408	Acc: 32.7% (3270/10000)
[Test]  Epoch: 44	Loss: 0.046174	Acc: 33.3% (3328/10000)
[Test]  Epoch: 45	Loss: 0.046186	Acc: 33.3% (3330/10000)
[Test]  Epoch: 46	Loss: 0.046682	Acc: 33.3% (3328/10000)
[Test]  Epoch: 47	Loss: 0.046727	Acc: 32.3% (3234/10000)
[Test]  Epoch: 48	Loss: 0.046824	Acc: 32.6% (3263/10000)
[Test]  Epoch: 49	Loss: 0.046761	Acc: 32.7% (3268/10000)
[Test]  Epoch: 50	Loss: 0.046303	Acc: 32.9% (3290/10000)
[Test]  Epoch: 51	Loss: 0.046688	Acc: 32.7% (3266/10000)
[Test]  Epoch: 52	Loss: 0.046923	Acc: 32.5% (3247/10000)
[Test]  Epoch: 53	Loss: 0.046837	Acc: 32.3% (3228/10000)
[Test]  Epoch: 54	Loss: 0.046639	Acc: 32.5% (3254/10000)
[Test]  Epoch: 55	Loss: 0.046615	Acc: 33.2% (3322/10000)
[Test]  Epoch: 56	Loss: 0.047354	Acc: 31.8% (3178/10000)
[Test]  Epoch: 57	Loss: 0.046887	Acc: 32.2% (3216/10000)
[Test]  Epoch: 58	Loss: 0.047058	Acc: 31.8% (3181/10000)
[Test]  Epoch: 59	Loss: 0.047083	Acc: 31.4% (3142/10000)
[Test]  Epoch: 60	Loss: 0.047381	Acc: 32.5% (3251/10000)
[Test]  Epoch: 61	Loss: 0.047251	Acc: 32.3% (3227/10000)
[Test]  Epoch: 62	Loss: 0.046993	Acc: 31.9% (3194/10000)
[Test]  Epoch: 63	Loss: 0.046994	Acc: 32.0% (3205/10000)
[Test]  Epoch: 64	Loss: 0.046876	Acc: 32.4% (3240/10000)
[Test]  Epoch: 65	Loss: 0.046756	Acc: 33.2% (3321/10000)
[Test]  Epoch: 66	Loss: 0.047049	Acc: 32.5% (3254/10000)
[Test]  Epoch: 67	Loss: 0.047066	Acc: 31.9% (3190/10000)
[Test]  Epoch: 68	Loss: 0.047019	Acc: 32.0% (3198/10000)
[Test]  Epoch: 69	Loss: 0.047268	Acc: 32.1% (3213/10000)
[Test]  Epoch: 70	Loss: 0.046633	Acc: 32.4% (3235/10000)
[Test]  Epoch: 71	Loss: 0.046936	Acc: 32.1% (3210/10000)
[Test]  Epoch: 72	Loss: 0.046562	Acc: 33.1% (3314/10000)
[Test]  Epoch: 73	Loss: 0.046825	Acc: 32.9% (3287/10000)
[Test]  Epoch: 74	Loss: 0.047060	Acc: 32.7% (3273/10000)
[Test]  Epoch: 75	Loss: 0.046863	Acc: 32.6% (3263/10000)
[Test]  Epoch: 76	Loss: 0.047130	Acc: 32.8% (3279/10000)
[Test]  Epoch: 77	Loss: 0.046838	Acc: 32.8% (3284/10000)
[Test]  Epoch: 78	Loss: 0.046973	Acc: 32.5% (3253/10000)
[Test]  Epoch: 79	Loss: 0.046972	Acc: 32.1% (3214/10000)
[Test]  Epoch: 80	Loss: 0.046415	Acc: 33.2% (3317/10000)
[Test]  Epoch: 81	Loss: 0.047095	Acc: 32.0% (3205/10000)
[Test]  Epoch: 82	Loss: 0.046874	Acc: 32.6% (3261/10000)
[Test]  Epoch: 83	Loss: 0.046849	Acc: 32.8% (3278/10000)
[Test]  Epoch: 84	Loss: 0.046790	Acc: 32.5% (3254/10000)
[Test]  Epoch: 85	Loss: 0.046818	Acc: 32.4% (3236/10000)
[Test]  Epoch: 86	Loss: 0.046949	Acc: 32.0% (3205/10000)
[Test]  Epoch: 87	Loss: 0.046782	Acc: 32.6% (3259/10000)
[Test]  Epoch: 88	Loss: 0.046761	Acc: 32.2% (3222/10000)
[Test]  Epoch: 89	Loss: 0.046794	Acc: 32.7% (3272/10000)
[Test]  Epoch: 90	Loss: 0.046775	Acc: 32.4% (3242/10000)
[Test]  Epoch: 91	Loss: 0.046946	Acc: 32.5% (3247/10000)
[Test]  Epoch: 92	Loss: 0.046717	Acc: 32.6% (3259/10000)
[Test]  Epoch: 93	Loss: 0.046980	Acc: 32.2% (3218/10000)
[Test]  Epoch: 94	Loss: 0.046620	Acc: 33.0% (3299/10000)
[Test]  Epoch: 95	Loss: 0.046765	Acc: 32.5% (3251/10000)
[Test]  Epoch: 96	Loss: 0.046842	Acc: 32.4% (3242/10000)
[Test]  Epoch: 97	Loss: 0.046399	Acc: 33.3% (3326/10000)
[Test]  Epoch: 98	Loss: 0.047138	Acc: 31.9% (3188/10000)
[Test]  Epoch: 99	Loss: 0.046916	Acc: 32.8% (3283/10000)
[Test]  Epoch: 100	Loss: 0.046870	Acc: 31.8% (3176/10000)
===========finish==========
['2024-08-19', '02:30:35.607674', '100', 'test', '0.046869902861118314', '31.76', '38.22']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.0.weight', 'features.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.081279	Acc: 8.3% (831/10000)
[Test]  Epoch: 2	Loss: 0.057861	Acc: 22.3% (2232/10000)
[Test]  Epoch: 3	Loss: 0.054845	Acc: 23.8% (2381/10000)
[Test]  Epoch: 4	Loss: 0.052341	Acc: 26.8% (2675/10000)
[Test]  Epoch: 5	Loss: 0.051143	Acc: 27.7% (2768/10000)
[Test]  Epoch: 6	Loss: 0.050215	Acc: 28.8% (2877/10000)
[Test]  Epoch: 7	Loss: 0.049688	Acc: 29.1% (2914/10000)
[Test]  Epoch: 8	Loss: 0.050340	Acc: 28.6% (2855/10000)
[Test]  Epoch: 9	Loss: 0.049452	Acc: 29.2% (2923/10000)
[Test]  Epoch: 10	Loss: 0.049297	Acc: 29.5% (2948/10000)
[Test]  Epoch: 11	Loss: 0.049025	Acc: 29.8% (2983/10000)
[Test]  Epoch: 12	Loss: 0.048663	Acc: 30.5% (3050/10000)
[Test]  Epoch: 13	Loss: 0.049589	Acc: 29.2% (2925/10000)
[Test]  Epoch: 14	Loss: 0.048987	Acc: 29.9% (2985/10000)
[Test]  Epoch: 15	Loss: 0.048947	Acc: 30.2% (3016/10000)
[Test]  Epoch: 16	Loss: 0.048896	Acc: 29.9% (2993/10000)
[Test]  Epoch: 17	Loss: 0.049041	Acc: 30.4% (3035/10000)
[Test]  Epoch: 18	Loss: 0.048667	Acc: 30.3% (3029/10000)
[Test]  Epoch: 19	Loss: 0.048962	Acc: 29.8% (2984/10000)
[Test]  Epoch: 20	Loss: 0.048832	Acc: 30.5% (3052/10000)
[Test]  Epoch: 21	Loss: 0.048356	Acc: 30.4% (3035/10000)
[Test]  Epoch: 22	Loss: 0.048554	Acc: 30.4% (3039/10000)
[Test]  Epoch: 23	Loss: 0.048768	Acc: 30.8% (3077/10000)
[Test]  Epoch: 24	Loss: 0.048923	Acc: 30.2% (3021/10000)
[Test]  Epoch: 25	Loss: 0.048745	Acc: 30.0% (2998/10000)
[Test]  Epoch: 26	Loss: 0.048644	Acc: 30.3% (3034/10000)
[Test]  Epoch: 27	Loss: 0.048804	Acc: 29.9% (2995/10000)
[Test]  Epoch: 28	Loss: 0.048827	Acc: 30.3% (3028/10000)
[Test]  Epoch: 29	Loss: 0.048633	Acc: 30.6% (3055/10000)
[Test]  Epoch: 30	Loss: 0.048824	Acc: 30.0% (2996/10000)
[Test]  Epoch: 31	Loss: 0.049223	Acc: 29.5% (2949/10000)
[Test]  Epoch: 32	Loss: 0.049046	Acc: 30.0% (3004/10000)
[Test]  Epoch: 33	Loss: 0.049048	Acc: 29.6% (2955/10000)
[Test]  Epoch: 34	Loss: 0.049107	Acc: 29.5% (2952/10000)
[Test]  Epoch: 35	Loss: 0.048850	Acc: 29.9% (2994/10000)
[Test]  Epoch: 36	Loss: 0.050085	Acc: 28.6% (2859/10000)
[Test]  Epoch: 37	Loss: 0.049198	Acc: 29.4% (2943/10000)
[Test]  Epoch: 38	Loss: 0.049145	Acc: 29.8% (2984/10000)
[Test]  Epoch: 39	Loss: 0.049320	Acc: 29.5% (2952/10000)
[Test]  Epoch: 40	Loss: 0.049249	Acc: 29.6% (2959/10000)
[Test]  Epoch: 41	Loss: 0.048943	Acc: 29.6% (2955/10000)
[Test]  Epoch: 42	Loss: 0.048645	Acc: 30.2% (3022/10000)
[Test]  Epoch: 43	Loss: 0.049074	Acc: 29.4% (2938/10000)
[Test]  Epoch: 44	Loss: 0.048785	Acc: 30.0% (3002/10000)
[Test]  Epoch: 45	Loss: 0.048871	Acc: 30.3% (3030/10000)
[Test]  Epoch: 46	Loss: 0.049743	Acc: 28.8% (2883/10000)
[Test]  Epoch: 47	Loss: 0.049490	Acc: 29.3% (2928/10000)
[Test]  Epoch: 48	Loss: 0.049575	Acc: 28.7% (2872/10000)
[Test]  Epoch: 49	Loss: 0.049616	Acc: 29.5% (2947/10000)
[Test]  Epoch: 50	Loss: 0.048796	Acc: 30.3% (3028/10000)
[Test]  Epoch: 51	Loss: 0.049409	Acc: 29.2% (2924/10000)
[Test]  Epoch: 52	Loss: 0.049429	Acc: 29.6% (2958/10000)
[Test]  Epoch: 53	Loss: 0.049188	Acc: 29.4% (2940/10000)
[Test]  Epoch: 54	Loss: 0.049037	Acc: 29.7% (2972/10000)
[Test]  Epoch: 55	Loss: 0.049491	Acc: 29.6% (2965/10000)
[Test]  Epoch: 56	Loss: 0.049959	Acc: 28.9% (2887/10000)
[Test]  Epoch: 57	Loss: 0.049435	Acc: 29.7% (2968/10000)
[Test]  Epoch: 58	Loss: 0.049678	Acc: 29.3% (2929/10000)
[Test]  Epoch: 59	Loss: 0.049573	Acc: 28.9% (2894/10000)
[Test]  Epoch: 60	Loss: 0.050049	Acc: 28.6% (2862/10000)
[Test]  Epoch: 61	Loss: 0.049556	Acc: 29.5% (2950/10000)
[Test]  Epoch: 62	Loss: 0.049380	Acc: 29.6% (2962/10000)
[Test]  Epoch: 63	Loss: 0.049345	Acc: 29.5% (2954/10000)
[Test]  Epoch: 64	Loss: 0.049005	Acc: 29.6% (2959/10000)
[Test]  Epoch: 65	Loss: 0.049088	Acc: 29.8% (2983/10000)
[Test]  Epoch: 66	Loss: 0.049277	Acc: 29.5% (2946/10000)
[Test]  Epoch: 67	Loss: 0.049276	Acc: 29.2% (2925/10000)
[Test]  Epoch: 68	Loss: 0.049261	Acc: 29.2% (2918/10000)
[Test]  Epoch: 69	Loss: 0.049381	Acc: 29.3% (2929/10000)
[Test]  Epoch: 70	Loss: 0.049024	Acc: 29.7% (2969/10000)
[Test]  Epoch: 71	Loss: 0.049108	Acc: 30.2% (3024/10000)
[Test]  Epoch: 72	Loss: 0.048835	Acc: 30.7% (3070/10000)
[Test]  Epoch: 73	Loss: 0.049132	Acc: 29.8% (2983/10000)
[Test]  Epoch: 74	Loss: 0.049028	Acc: 30.2% (3018/10000)
[Test]  Epoch: 75	Loss: 0.049034	Acc: 30.1% (3012/10000)
[Test]  Epoch: 76	Loss: 0.049179	Acc: 30.2% (3020/10000)
[Test]  Epoch: 77	Loss: 0.048890	Acc: 30.1% (3010/10000)
[Test]  Epoch: 78	Loss: 0.049195	Acc: 29.4% (2939/10000)
[Test]  Epoch: 79	Loss: 0.049144	Acc: 29.6% (2957/10000)
[Test]  Epoch: 80	Loss: 0.048633	Acc: 30.3% (3032/10000)
[Test]  Epoch: 81	Loss: 0.049293	Acc: 29.5% (2946/10000)
[Test]  Epoch: 82	Loss: 0.049002	Acc: 30.1% (3011/10000)
[Test]  Epoch: 83	Loss: 0.049000	Acc: 30.2% (3020/10000)
[Test]  Epoch: 84	Loss: 0.049049	Acc: 29.9% (2991/10000)
[Test]  Epoch: 85	Loss: 0.049070	Acc: 29.6% (2956/10000)
[Test]  Epoch: 86	Loss: 0.049077	Acc: 28.9% (2892/10000)
[Test]  Epoch: 87	Loss: 0.048779	Acc: 30.1% (3008/10000)
[Test]  Epoch: 88	Loss: 0.048803	Acc: 30.1% (3014/10000)
[Test]  Epoch: 89	Loss: 0.048884	Acc: 30.1% (3005/10000)
[Test]  Epoch: 90	Loss: 0.048824	Acc: 29.9% (2987/10000)
[Test]  Epoch: 91	Loss: 0.048963	Acc: 30.3% (3030/10000)
[Test]  Epoch: 92	Loss: 0.048924	Acc: 30.0% (3003/10000)
[Test]  Epoch: 93	Loss: 0.049155	Acc: 29.9% (2989/10000)
[Test]  Epoch: 94	Loss: 0.048810	Acc: 29.9% (2994/10000)
[Test]  Epoch: 95	Loss: 0.048868	Acc: 30.4% (3035/10000)
[Test]  Epoch: 96	Loss: 0.049011	Acc: 29.8% (2983/10000)
[Test]  Epoch: 97	Loss: 0.048698	Acc: 30.1% (3012/10000)
[Test]  Epoch: 98	Loss: 0.049313	Acc: 29.2% (2925/10000)
[Test]  Epoch: 99	Loss: 0.049090	Acc: 29.9% (2995/10000)
[Test]  Epoch: 100	Loss: 0.049049	Acc: 29.3% (2928/10000)
===========finish==========
['2024-08-19', '02:33:15.391381', '100', 'test', '0.04904947216510773', '29.28', '30.77']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.100165	Acc: 1.6% (163/10000)
[Test]  Epoch: 2	Loss: 0.083946	Acc: 3.5% (347/10000)
[Test]  Epoch: 3	Loss: 0.077288	Acc: 5.1% (509/10000)
[Test]  Epoch: 4	Loss: 0.076150	Acc: 5.5% (552/10000)
[Test]  Epoch: 5	Loss: 0.075433	Acc: 6.1% (612/10000)
[Test]  Epoch: 6	Loss: 0.074264	Acc: 6.5% (650/10000)
[Test]  Epoch: 7	Loss: 0.072591	Acc: 7.7% (768/10000)
[Test]  Epoch: 8	Loss: 0.072060	Acc: 7.8% (778/10000)
[Test]  Epoch: 9	Loss: 0.070918	Acc: 8.8% (881/10000)
[Test]  Epoch: 10	Loss: 0.072324	Acc: 7.9% (786/10000)
[Test]  Epoch: 11	Loss: 0.069914	Acc: 9.8% (975/10000)
[Test]  Epoch: 12	Loss: 0.068617	Acc: 10.7% (1073/10000)
[Test]  Epoch: 13	Loss: 0.068790	Acc: 10.7% (1070/10000)
[Test]  Epoch: 14	Loss: 0.069100	Acc: 10.7% (1066/10000)
[Test]  Epoch: 15	Loss: 0.068503	Acc: 10.9% (1093/10000)
[Test]  Epoch: 16	Loss: 0.067566	Acc: 11.9% (1194/10000)
[Test]  Epoch: 17	Loss: 0.066054	Acc: 13.0% (1297/10000)
[Test]  Epoch: 18	Loss: 0.066964	Acc: 12.5% (1249/10000)
[Test]  Epoch: 19	Loss: 0.065208	Acc: 14.2% (1417/10000)
[Test]  Epoch: 20	Loss: 0.064527	Acc: 14.7% (1468/10000)
[Test]  Epoch: 21	Loss: 0.064431	Acc: 14.3% (1431/10000)
[Test]  Epoch: 22	Loss: 0.063847	Acc: 14.5% (1447/10000)
[Test]  Epoch: 23	Loss: 0.063328	Acc: 15.0% (1499/10000)
[Test]  Epoch: 24	Loss: 0.063716	Acc: 15.1% (1509/10000)
[Test]  Epoch: 25	Loss: 0.062526	Acc: 15.8% (1582/10000)
[Test]  Epoch: 26	Loss: 0.063393	Acc: 15.1% (1511/10000)
[Test]  Epoch: 27	Loss: 0.064724	Acc: 14.7% (1465/10000)
[Test]  Epoch: 28	Loss: 0.062419	Acc: 15.9% (1595/10000)
[Test]  Epoch: 29	Loss: 0.062843	Acc: 15.7% (1572/10000)
[Test]  Epoch: 30	Loss: 0.062993	Acc: 15.5% (1554/10000)
[Test]  Epoch: 31	Loss: 0.062063	Acc: 16.4% (1638/10000)
[Test]  Epoch: 32	Loss: 0.062033	Acc: 16.6% (1656/10000)
[Test]  Epoch: 33	Loss: 0.062370	Acc: 16.2% (1625/10000)
[Test]  Epoch: 34	Loss: 0.061611	Acc: 16.4% (1645/10000)
[Test]  Epoch: 35	Loss: 0.062018	Acc: 16.8% (1680/10000)
[Test]  Epoch: 36	Loss: 0.061439	Acc: 17.0% (1699/10000)
[Test]  Epoch: 37	Loss: 0.060877	Acc: 18.0% (1796/10000)
[Test]  Epoch: 38	Loss: 0.061430	Acc: 16.9% (1692/10000)
[Test]  Epoch: 39	Loss: 0.061703	Acc: 16.8% (1677/10000)
[Test]  Epoch: 40	Loss: 0.061401	Acc: 16.9% (1695/10000)
[Test]  Epoch: 41	Loss: 0.061333	Acc: 16.7% (1672/10000)
[Test]  Epoch: 42	Loss: 0.060687	Acc: 18.1% (1806/10000)
[Test]  Epoch: 43	Loss: 0.060873	Acc: 17.5% (1753/10000)
[Test]  Epoch: 44	Loss: 0.059715	Acc: 17.9% (1789/10000)
[Test]  Epoch: 45	Loss: 0.060262	Acc: 18.1% (1809/10000)
[Test]  Epoch: 46	Loss: 0.060624	Acc: 18.0% (1796/10000)
[Test]  Epoch: 47	Loss: 0.059760	Acc: 18.2% (1817/10000)
[Test]  Epoch: 48	Loss: 0.059737	Acc: 18.9% (1888/10000)
[Test]  Epoch: 49	Loss: 0.059926	Acc: 19.1% (1905/10000)
[Test]  Epoch: 50	Loss: 0.058986	Acc: 19.6% (1964/10000)
[Test]  Epoch: 51	Loss: 0.059372	Acc: 19.0% (1901/10000)
[Test]  Epoch: 52	Loss: 0.059558	Acc: 19.0% (1904/10000)
[Test]  Epoch: 53	Loss: 0.059932	Acc: 18.4% (1838/10000)
[Test]  Epoch: 54	Loss: 0.059349	Acc: 18.5% (1852/10000)
[Test]  Epoch: 55	Loss: 0.059261	Acc: 18.2% (1825/10000)
[Test]  Epoch: 56	Loss: 0.059903	Acc: 18.8% (1878/10000)
[Test]  Epoch: 57	Loss: 0.059598	Acc: 18.4% (1844/10000)
[Test]  Epoch: 58	Loss: 0.060619	Acc: 18.0% (1798/10000)
[Test]  Epoch: 59	Loss: 0.059889	Acc: 18.2% (1819/10000)
[Test]  Epoch: 60	Loss: 0.059828	Acc: 18.9% (1887/10000)
[Test]  Epoch: 61	Loss: 0.058966	Acc: 18.9% (1887/10000)
[Test]  Epoch: 62	Loss: 0.058569	Acc: 19.4% (1937/10000)
[Test]  Epoch: 63	Loss: 0.058742	Acc: 19.3% (1928/10000)
[Test]  Epoch: 64	Loss: 0.058557	Acc: 19.8% (1977/10000)
[Test]  Epoch: 65	Loss: 0.058330	Acc: 19.8% (1984/10000)
[Test]  Epoch: 66	Loss: 0.058596	Acc: 19.6% (1961/10000)
[Test]  Epoch: 67	Loss: 0.058774	Acc: 19.1% (1906/10000)
[Test]  Epoch: 68	Loss: 0.058381	Acc: 19.1% (1907/10000)
[Test]  Epoch: 69	Loss: 0.058519	Acc: 19.6% (1965/10000)
[Test]  Epoch: 70	Loss: 0.058408	Acc: 19.7% (1967/10000)
[Test]  Epoch: 71	Loss: 0.058218	Acc: 19.6% (1964/10000)
[Test]  Epoch: 72	Loss: 0.058078	Acc: 19.9% (1986/10000)
[Test]  Epoch: 73	Loss: 0.058384	Acc: 19.9% (1986/10000)
[Test]  Epoch: 74	Loss: 0.058466	Acc: 19.6% (1962/10000)
[Test]  Epoch: 75	Loss: 0.058262	Acc: 19.7% (1967/10000)
[Test]  Epoch: 76	Loss: 0.058303	Acc: 19.4% (1944/10000)
[Test]  Epoch: 77	Loss: 0.058151	Acc: 19.9% (1991/10000)
[Test]  Epoch: 78	Loss: 0.058331	Acc: 19.9% (1986/10000)
[Test]  Epoch: 79	Loss: 0.058356	Acc: 19.5% (1946/10000)
[Test]  Epoch: 80	Loss: 0.057816	Acc: 20.2% (2025/10000)
[Test]  Epoch: 81	Loss: 0.058515	Acc: 19.6% (1961/10000)
[Test]  Epoch: 82	Loss: 0.058176	Acc: 19.7% (1972/10000)
[Test]  Epoch: 83	Loss: 0.058304	Acc: 19.8% (1976/10000)
[Test]  Epoch: 84	Loss: 0.058156	Acc: 20.0% (1996/10000)
[Test]  Epoch: 85	Loss: 0.058437	Acc: 19.5% (1946/10000)
[Test]  Epoch: 86	Loss: 0.058290	Acc: 19.3% (1927/10000)
[Test]  Epoch: 87	Loss: 0.058112	Acc: 20.2% (2020/10000)
[Test]  Epoch: 88	Loss: 0.058057	Acc: 19.9% (1994/10000)
[Test]  Epoch: 89	Loss: 0.057773	Acc: 20.6% (2062/10000)
[Test]  Epoch: 90	Loss: 0.057786	Acc: 20.4% (2044/10000)
[Test]  Epoch: 91	Loss: 0.058149	Acc: 19.9% (1986/10000)
[Test]  Epoch: 92	Loss: 0.058005	Acc: 20.1% (2012/10000)
[Test]  Epoch: 93	Loss: 0.058090	Acc: 20.2% (2016/10000)
[Test]  Epoch: 94	Loss: 0.057839	Acc: 19.9% (1989/10000)
[Test]  Epoch: 95	Loss: 0.058340	Acc: 19.7% (1974/10000)
[Test]  Epoch: 96	Loss: 0.058169	Acc: 19.6% (1960/10000)
[Test]  Epoch: 97	Loss: 0.058014	Acc: 19.8% (1976/10000)
[Test]  Epoch: 98	Loss: 0.058144	Acc: 20.2% (2016/10000)
[Test]  Epoch: 99	Loss: 0.058183	Acc: 19.5% (1954/10000)
[Test]  Epoch: 100	Loss: 0.057888	Acc: 20.2% (2021/10000)
===========finish==========
['2024-08-19', '02:36:46.814371', '100', 'test', '0.05788795682191849', '20.21', '20.62']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.128954	Acc: 1.2% (121/10000)
[Test]  Epoch: 2	Loss: 0.084137	Acc: 3.2% (317/10000)
[Test]  Epoch: 3	Loss: 0.078601	Acc: 4.3% (432/10000)
[Test]  Epoch: 4	Loss: 0.076732	Acc: 5.0% (500/10000)
[Test]  Epoch: 5	Loss: 0.075458	Acc: 5.7% (573/10000)
[Test]  Epoch: 6	Loss: 0.074580	Acc: 6.3% (630/10000)
[Test]  Epoch: 7	Loss: 0.074346	Acc: 6.4% (643/10000)
[Test]  Epoch: 8	Loss: 0.073559	Acc: 6.4% (643/10000)
[Test]  Epoch: 9	Loss: 0.072677	Acc: 7.4% (738/10000)
[Test]  Epoch: 10	Loss: 0.071911	Acc: 8.0% (802/10000)
[Test]  Epoch: 11	Loss: 0.072228	Acc: 8.1% (810/10000)
[Test]  Epoch: 12	Loss: 0.070941	Acc: 9.5% (949/10000)
[Test]  Epoch: 13	Loss: 0.071508	Acc: 8.0% (803/10000)
[Test]  Epoch: 14	Loss: 0.070629	Acc: 8.7% (874/10000)
[Test]  Epoch: 15	Loss: 0.071697	Acc: 8.5% (846/10000)
[Test]  Epoch: 16	Loss: 0.070515	Acc: 9.1% (906/10000)
[Test]  Epoch: 17	Loss: 0.069203	Acc: 9.9% (986/10000)
[Test]  Epoch: 18	Loss: 0.070604	Acc: 9.3% (934/10000)
[Test]  Epoch: 19	Loss: 0.069604	Acc: 9.9% (994/10000)
[Test]  Epoch: 20	Loss: 0.069685	Acc: 10.4% (1045/10000)
[Test]  Epoch: 21	Loss: 0.068216	Acc: 11.4% (1136/10000)
[Test]  Epoch: 22	Loss: 0.069124	Acc: 10.3% (1033/10000)
[Test]  Epoch: 23	Loss: 0.070284	Acc: 9.5% (953/10000)
[Test]  Epoch: 24	Loss: 0.068497	Acc: 10.9% (1091/10000)
[Test]  Epoch: 25	Loss: 0.067508	Acc: 11.3% (1135/10000)
[Test]  Epoch: 26	Loss: 0.069549	Acc: 10.4% (1037/10000)
[Test]  Epoch: 27	Loss: 0.069512	Acc: 10.6% (1059/10000)
[Test]  Epoch: 28	Loss: 0.067299	Acc: 11.6% (1162/10000)
[Test]  Epoch: 29	Loss: 0.066779	Acc: 12.1% (1207/10000)
[Test]  Epoch: 30	Loss: 0.069434	Acc: 10.6% (1059/10000)
[Test]  Epoch: 31	Loss: 0.067214	Acc: 11.8% (1182/10000)
[Test]  Epoch: 32	Loss: 0.067345	Acc: 11.8% (1176/10000)
[Test]  Epoch: 33	Loss: 0.067264	Acc: 11.4% (1140/10000)
[Test]  Epoch: 34	Loss: 0.066891	Acc: 12.4% (1239/10000)
[Test]  Epoch: 35	Loss: 0.068017	Acc: 11.3% (1131/10000)
[Test]  Epoch: 36	Loss: 0.066272	Acc: 12.3% (1232/10000)
[Test]  Epoch: 37	Loss: 0.066774	Acc: 12.3% (1227/10000)
[Test]  Epoch: 38	Loss: 0.066674	Acc: 12.2% (1215/10000)
[Test]  Epoch: 39	Loss: 0.066496	Acc: 12.5% (1253/10000)
[Test]  Epoch: 40	Loss: 0.066428	Acc: 12.2% (1221/10000)
[Test]  Epoch: 41	Loss: 0.067050	Acc: 12.1% (1207/10000)
[Test]  Epoch: 42	Loss: 0.066198	Acc: 13.0% (1299/10000)
[Test]  Epoch: 43	Loss: 0.065751	Acc: 13.0% (1304/10000)
[Test]  Epoch: 44	Loss: 0.065642	Acc: 13.3% (1332/10000)
[Test]  Epoch: 45	Loss: 0.065566	Acc: 13.4% (1338/10000)
[Test]  Epoch: 46	Loss: 0.065923	Acc: 12.3% (1230/10000)
[Test]  Epoch: 47	Loss: 0.065437	Acc: 13.1% (1309/10000)
[Test]  Epoch: 48	Loss: 0.065073	Acc: 13.9% (1395/10000)
[Test]  Epoch: 49	Loss: 0.065710	Acc: 13.4% (1338/10000)
[Test]  Epoch: 50	Loss: 0.065920	Acc: 13.4% (1343/10000)
[Test]  Epoch: 51	Loss: 0.066003	Acc: 13.0% (1302/10000)
[Test]  Epoch: 52	Loss: 0.065119	Acc: 13.8% (1377/10000)
[Test]  Epoch: 53	Loss: 0.066200	Acc: 13.2% (1315/10000)
[Test]  Epoch: 54	Loss: 0.065172	Acc: 13.2% (1315/10000)
[Test]  Epoch: 55	Loss: 0.064785	Acc: 13.7% (1366/10000)
[Test]  Epoch: 56	Loss: 0.066399	Acc: 12.7% (1268/10000)
[Test]  Epoch: 57	Loss: 0.064849	Acc: 13.8% (1383/10000)
[Test]  Epoch: 58	Loss: 0.064954	Acc: 13.5% (1353/10000)
[Test]  Epoch: 59	Loss: 0.065456	Acc: 13.0% (1298/10000)
[Test]  Epoch: 60	Loss: 0.065443	Acc: 13.7% (1367/10000)
[Test]  Epoch: 61	Loss: 0.064215	Acc: 14.0% (1404/10000)
[Test]  Epoch: 62	Loss: 0.063991	Acc: 14.4% (1441/10000)
[Test]  Epoch: 63	Loss: 0.064245	Acc: 14.0% (1401/10000)
[Test]  Epoch: 64	Loss: 0.063714	Acc: 15.0% (1497/10000)
[Test]  Epoch: 65	Loss: 0.063747	Acc: 14.9% (1488/10000)
[Test]  Epoch: 66	Loss: 0.063775	Acc: 14.4% (1445/10000)
[Test]  Epoch: 67	Loss: 0.064104	Acc: 14.3% (1433/10000)
[Test]  Epoch: 68	Loss: 0.063708	Acc: 14.2% (1425/10000)
[Test]  Epoch: 69	Loss: 0.063834	Acc: 14.3% (1432/10000)
[Test]  Epoch: 70	Loss: 0.063808	Acc: 14.3% (1431/10000)
[Test]  Epoch: 71	Loss: 0.063983	Acc: 14.6% (1460/10000)
[Test]  Epoch: 72	Loss: 0.063411	Acc: 15.3% (1533/10000)
[Test]  Epoch: 73	Loss: 0.063633	Acc: 14.7% (1467/10000)
[Test]  Epoch: 74	Loss: 0.064036	Acc: 14.5% (1449/10000)
[Test]  Epoch: 75	Loss: 0.063445	Acc: 15.1% (1508/10000)
[Test]  Epoch: 76	Loss: 0.063566	Acc: 14.3% (1435/10000)
[Test]  Epoch: 77	Loss: 0.063481	Acc: 14.7% (1465/10000)
[Test]  Epoch: 78	Loss: 0.063594	Acc: 14.6% (1462/10000)
[Test]  Epoch: 79	Loss: 0.063604	Acc: 15.1% (1507/10000)
[Test]  Epoch: 80	Loss: 0.063136	Acc: 14.7% (1471/10000)
[Test]  Epoch: 81	Loss: 0.063578	Acc: 14.7% (1466/10000)
[Test]  Epoch: 82	Loss: 0.063391	Acc: 15.1% (1508/10000)
[Test]  Epoch: 83	Loss: 0.063641	Acc: 14.5% (1446/10000)
[Test]  Epoch: 84	Loss: 0.063655	Acc: 14.8% (1476/10000)
[Test]  Epoch: 85	Loss: 0.063906	Acc: 14.3% (1430/10000)
[Test]  Epoch: 86	Loss: 0.063522	Acc: 14.8% (1476/10000)
[Test]  Epoch: 87	Loss: 0.063318	Acc: 15.1% (1514/10000)
[Test]  Epoch: 88	Loss: 0.063334	Acc: 14.8% (1481/10000)
[Test]  Epoch: 89	Loss: 0.063372	Acc: 14.7% (1469/10000)
[Test]  Epoch: 90	Loss: 0.063368	Acc: 14.7% (1468/10000)
[Test]  Epoch: 91	Loss: 0.063507	Acc: 14.7% (1467/10000)
[Test]  Epoch: 92	Loss: 0.063099	Acc: 15.2% (1525/10000)
[Test]  Epoch: 93	Loss: 0.063456	Acc: 15.1% (1511/10000)
[Test]  Epoch: 94	Loss: 0.063200	Acc: 15.0% (1501/10000)
[Test]  Epoch: 95	Loss: 0.063634	Acc: 14.4% (1441/10000)
[Test]  Epoch: 96	Loss: 0.063357	Acc: 14.8% (1481/10000)
[Test]  Epoch: 97	Loss: 0.063251	Acc: 14.8% (1483/10000)
[Test]  Epoch: 98	Loss: 0.063329	Acc: 14.6% (1463/10000)
[Test]  Epoch: 99	Loss: 0.063443	Acc: 14.8% (1475/10000)
[Test]  Epoch: 100	Loss: 0.063194	Acc: 15.2% (1521/10000)
===========finish==========
['2024-08-19', '02:40:46.151174', '100', 'test', '0.06319354937076568', '15.21', '15.33']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.142996	Acc: 1.0% (97/10000)
[Test]  Epoch: 2	Loss: 0.083530	Acc: 2.4% (237/10000)
[Test]  Epoch: 3	Loss: 0.078913	Acc: 3.0% (304/10000)
[Test]  Epoch: 4	Loss: 0.078878	Acc: 3.1% (314/10000)
[Test]  Epoch: 5	Loss: 0.078108	Acc: 4.0% (395/10000)
[Test]  Epoch: 6	Loss: 0.077829	Acc: 4.0% (396/10000)
[Test]  Epoch: 7	Loss: 0.076921	Acc: 4.1% (413/10000)
[Test]  Epoch: 8	Loss: 0.076405	Acc: 4.7% (467/10000)
[Test]  Epoch: 9	Loss: 0.075510	Acc: 4.6% (458/10000)
[Test]  Epoch: 10	Loss: 0.075787	Acc: 5.0% (505/10000)
[Test]  Epoch: 11	Loss: 0.076613	Acc: 4.5% (455/10000)
[Test]  Epoch: 12	Loss: 0.074751	Acc: 5.8% (583/10000)
[Test]  Epoch: 13	Loss: 0.075239	Acc: 5.3% (532/10000)
[Test]  Epoch: 14	Loss: 0.075561	Acc: 5.2% (521/10000)
[Test]  Epoch: 15	Loss: 0.074297	Acc: 6.5% (648/10000)
[Test]  Epoch: 16	Loss: 0.076039	Acc: 5.4% (540/10000)
[Test]  Epoch: 17	Loss: 0.074031	Acc: 6.1% (609/10000)
[Test]  Epoch: 18	Loss: 0.074889	Acc: 6.0% (600/10000)
[Test]  Epoch: 19	Loss: 0.074965	Acc: 6.7% (666/10000)
[Test]  Epoch: 20	Loss: 0.072574	Acc: 7.6% (759/10000)
[Test]  Epoch: 21	Loss: 0.073861	Acc: 6.8% (681/10000)
[Test]  Epoch: 22	Loss: 0.073798	Acc: 7.1% (708/10000)
[Test]  Epoch: 23	Loss: 0.073924	Acc: 6.5% (649/10000)
[Test]  Epoch: 24	Loss: 0.072578	Acc: 7.8% (779/10000)
[Test]  Epoch: 25	Loss: 0.072031	Acc: 8.3% (835/10000)
[Test]  Epoch: 26	Loss: 0.072261	Acc: 8.2% (819/10000)
[Test]  Epoch: 27	Loss: 0.072681	Acc: 7.5% (751/10000)
[Test]  Epoch: 28	Loss: 0.072862	Acc: 7.4% (741/10000)
[Test]  Epoch: 29	Loss: 0.074120	Acc: 6.8% (678/10000)
[Test]  Epoch: 30	Loss: 0.072408	Acc: 8.1% (813/10000)
[Test]  Epoch: 31	Loss: 0.071321	Acc: 8.5% (852/10000)
[Test]  Epoch: 32	Loss: 0.071269	Acc: 8.4% (845/10000)
[Test]  Epoch: 33	Loss: 0.074939	Acc: 7.0% (700/10000)
[Test]  Epoch: 34	Loss: 0.071559	Acc: 8.8% (877/10000)
[Test]  Epoch: 35	Loss: 0.073073	Acc: 7.7% (774/10000)
[Test]  Epoch: 36	Loss: 0.072412	Acc: 8.4% (841/10000)
[Test]  Epoch: 37	Loss: 0.071369	Acc: 8.3% (833/10000)
[Test]  Epoch: 38	Loss: 0.070378	Acc: 8.8% (877/10000)
[Test]  Epoch: 39	Loss: 0.071913	Acc: 8.1% (808/10000)
[Test]  Epoch: 40	Loss: 0.071289	Acc: 8.7% (869/10000)
[Test]  Epoch: 41	Loss: 0.070428	Acc: 9.2% (919/10000)
[Test]  Epoch: 42	Loss: 0.070932	Acc: 9.2% (916/10000)
[Test]  Epoch: 43	Loss: 0.070492	Acc: 9.3% (932/10000)
[Test]  Epoch: 44	Loss: 0.070932	Acc: 9.7% (972/10000)
[Test]  Epoch: 45	Loss: 0.072897	Acc: 8.1% (813/10000)
[Test]  Epoch: 46	Loss: 0.071651	Acc: 9.0% (896/10000)
[Test]  Epoch: 47	Loss: 0.070815	Acc: 9.4% (938/10000)
[Test]  Epoch: 48	Loss: 0.070684	Acc: 9.4% (936/10000)
[Test]  Epoch: 49	Loss: 0.071290	Acc: 8.8% (883/10000)
[Test]  Epoch: 50	Loss: 0.070302	Acc: 10.4% (1039/10000)
[Test]  Epoch: 51	Loss: 0.070641	Acc: 9.6% (960/10000)
[Test]  Epoch: 52	Loss: 0.070652	Acc: 9.3% (930/10000)
[Test]  Epoch: 53	Loss: 0.071704	Acc: 8.8% (880/10000)
[Test]  Epoch: 54	Loss: 0.069859	Acc: 9.8% (979/10000)
[Test]  Epoch: 55	Loss: 0.071376	Acc: 9.1% (913/10000)
[Test]  Epoch: 56	Loss: 0.070451	Acc: 9.7% (965/10000)
[Test]  Epoch: 57	Loss: 0.070583	Acc: 9.7% (969/10000)
[Test]  Epoch: 58	Loss: 0.071365	Acc: 9.1% (905/10000)
[Test]  Epoch: 59	Loss: 0.070136	Acc: 9.8% (982/10000)
[Test]  Epoch: 60	Loss: 0.070060	Acc: 9.6% (956/10000)
[Test]  Epoch: 61	Loss: 0.069055	Acc: 10.4% (1041/10000)
[Test]  Epoch: 62	Loss: 0.068848	Acc: 10.0% (998/10000)
[Test]  Epoch: 63	Loss: 0.068702	Acc: 10.7% (1070/10000)
[Test]  Epoch: 64	Loss: 0.068426	Acc: 10.8% (1077/10000)
[Test]  Epoch: 65	Loss: 0.068616	Acc: 10.4% (1045/10000)
[Test]  Epoch: 66	Loss: 0.068336	Acc: 10.8% (1076/10000)
[Test]  Epoch: 67	Loss: 0.068337	Acc: 10.6% (1056/10000)
[Test]  Epoch: 68	Loss: 0.068411	Acc: 10.7% (1067/10000)
[Test]  Epoch: 69	Loss: 0.068322	Acc: 10.9% (1086/10000)
[Test]  Epoch: 70	Loss: 0.067995	Acc: 11.1% (1107/10000)
[Test]  Epoch: 71	Loss: 0.068141	Acc: 10.6% (1064/10000)
[Test]  Epoch: 72	Loss: 0.068005	Acc: 11.0% (1102/10000)
[Test]  Epoch: 73	Loss: 0.068306	Acc: 11.2% (1118/10000)
[Test]  Epoch: 74	Loss: 0.068456	Acc: 10.4% (1041/10000)
[Test]  Epoch: 75	Loss: 0.068050	Acc: 11.0% (1098/10000)
[Test]  Epoch: 76	Loss: 0.068046	Acc: 11.1% (1107/10000)
[Test]  Epoch: 77	Loss: 0.068295	Acc: 11.2% (1116/10000)
[Test]  Epoch: 78	Loss: 0.068438	Acc: 10.8% (1084/10000)
[Test]  Epoch: 79	Loss: 0.068158	Acc: 10.8% (1085/10000)
[Test]  Epoch: 80	Loss: 0.067620	Acc: 11.3% (1134/10000)
[Test]  Epoch: 81	Loss: 0.068045	Acc: 10.9% (1093/10000)
[Test]  Epoch: 82	Loss: 0.067903	Acc: 11.3% (1126/10000)
[Test]  Epoch: 83	Loss: 0.068321	Acc: 10.6% (1063/10000)
[Test]  Epoch: 84	Loss: 0.068289	Acc: 10.6% (1060/10000)
[Test]  Epoch: 85	Loss: 0.068195	Acc: 11.1% (1110/10000)
[Test]  Epoch: 86	Loss: 0.068213	Acc: 10.8% (1076/10000)
[Test]  Epoch: 87	Loss: 0.068145	Acc: 10.9% (1086/10000)
[Test]  Epoch: 88	Loss: 0.067944	Acc: 11.3% (1126/10000)
[Test]  Epoch: 89	Loss: 0.067640	Acc: 11.2% (1123/10000)
[Test]  Epoch: 90	Loss: 0.068029	Acc: 11.2% (1121/10000)
[Test]  Epoch: 91	Loss: 0.068009	Acc: 11.2% (1119/10000)
[Test]  Epoch: 92	Loss: 0.067852	Acc: 11.4% (1136/10000)
[Test]  Epoch: 93	Loss: 0.068127	Acc: 11.2% (1121/10000)
[Test]  Epoch: 94	Loss: 0.068065	Acc: 11.1% (1112/10000)
[Test]  Epoch: 95	Loss: 0.068054	Acc: 11.2% (1123/10000)
[Test]  Epoch: 96	Loss: 0.068285	Acc: 10.7% (1065/10000)
[Test]  Epoch: 97	Loss: 0.067766	Acc: 10.7% (1069/10000)
[Test]  Epoch: 98	Loss: 0.067859	Acc: 11.0% (1103/10000)
[Test]  Epoch: 99	Loss: 0.068063	Acc: 10.8% (1077/10000)
[Test]  Epoch: 100	Loss: 0.067769	Acc: 11.4% (1136/10000)
===========finish==========
['2024-08-19', '02:44:42.472457', '100', 'test', '0.06776854093074798', '11.36', '11.36']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.202826	Acc: 0.6% (58/10000)
[Test]  Epoch: 2	Loss: 0.083079	Acc: 1.9% (195/10000)
[Test]  Epoch: 3	Loss: 0.080358	Acc: 2.8% (278/10000)
[Test]  Epoch: 4	Loss: 0.081576	Acc: 2.8% (284/10000)
[Test]  Epoch: 5	Loss: 0.078743	Acc: 3.3% (331/10000)
[Test]  Epoch: 6	Loss: 0.078524	Acc: 3.8% (381/10000)
[Test]  Epoch: 7	Loss: 0.077704	Acc: 3.8% (375/10000)
[Test]  Epoch: 8	Loss: 0.077685	Acc: 3.8% (380/10000)
[Test]  Epoch: 9	Loss: 0.077537	Acc: 3.7% (370/10000)
[Test]  Epoch: 10	Loss: 0.076129	Acc: 4.4% (444/10000)
[Test]  Epoch: 11	Loss: 0.077339	Acc: 4.0% (396/10000)
[Test]  Epoch: 12	Loss: 0.075581	Acc: 4.9% (487/10000)
[Test]  Epoch: 13	Loss: 0.077191	Acc: 4.0% (403/10000)
[Test]  Epoch: 14	Loss: 0.076596	Acc: 4.8% (478/10000)
[Test]  Epoch: 15	Loss: 0.076251	Acc: 5.2% (524/10000)
[Test]  Epoch: 16	Loss: 0.076581	Acc: 4.9% (490/10000)
[Test]  Epoch: 17	Loss: 0.075787	Acc: 5.5% (551/10000)
[Test]  Epoch: 18	Loss: 0.076522	Acc: 4.8% (480/10000)
[Test]  Epoch: 19	Loss: 0.075345	Acc: 5.8% (583/10000)
[Test]  Epoch: 20	Loss: 0.076728	Acc: 5.7% (565/10000)
[Test]  Epoch: 21	Loss: 0.074636	Acc: 6.2% (625/10000)
[Test]  Epoch: 22	Loss: 0.074814	Acc: 6.2% (618/10000)
[Test]  Epoch: 23	Loss: 0.075091	Acc: 5.6% (563/10000)
[Test]  Epoch: 24	Loss: 0.075101	Acc: 5.6% (563/10000)
[Test]  Epoch: 25	Loss: 0.074794	Acc: 6.0% (595/10000)
[Test]  Epoch: 26	Loss: 0.074300	Acc: 6.0% (598/10000)
[Test]  Epoch: 27	Loss: 0.075050	Acc: 5.8% (584/10000)
[Test]  Epoch: 28	Loss: 0.073335	Acc: 6.5% (647/10000)
[Test]  Epoch: 29	Loss: 0.074777	Acc: 6.0% (599/10000)
[Test]  Epoch: 30	Loss: 0.073671	Acc: 7.0% (695/10000)
[Test]  Epoch: 31	Loss: 0.074187	Acc: 6.6% (663/10000)
[Test]  Epoch: 32	Loss: 0.074420	Acc: 6.5% (645/10000)
[Test]  Epoch: 33	Loss: 0.073264	Acc: 6.9% (688/10000)
[Test]  Epoch: 34	Loss: 0.073579	Acc: 7.2% (719/10000)
[Test]  Epoch: 35	Loss: 0.073585	Acc: 6.9% (692/10000)
[Test]  Epoch: 36	Loss: 0.075004	Acc: 6.5% (647/10000)
[Test]  Epoch: 37	Loss: 0.074160	Acc: 6.9% (694/10000)
[Test]  Epoch: 38	Loss: 0.074404	Acc: 6.8% (679/10000)
[Test]  Epoch: 39	Loss: 0.075214	Acc: 6.8% (676/10000)
[Test]  Epoch: 40	Loss: 0.073977	Acc: 6.5% (651/10000)
[Test]  Epoch: 41	Loss: 0.073973	Acc: 7.1% (712/10000)
[Test]  Epoch: 42	Loss: 0.073811	Acc: 7.2% (718/10000)
[Test]  Epoch: 43	Loss: 0.073252	Acc: 7.7% (766/10000)
[Test]  Epoch: 44	Loss: 0.072437	Acc: 7.6% (757/10000)
[Test]  Epoch: 45	Loss: 0.074258	Acc: 6.3% (633/10000)
[Test]  Epoch: 46	Loss: 0.073923	Acc: 7.2% (715/10000)
[Test]  Epoch: 47	Loss: 0.073903	Acc: 7.1% (711/10000)
[Test]  Epoch: 48	Loss: 0.072981	Acc: 7.8% (783/10000)
[Test]  Epoch: 49	Loss: 0.073012	Acc: 7.6% (759/10000)
[Test]  Epoch: 50	Loss: 0.073958	Acc: 7.4% (739/10000)
[Test]  Epoch: 51	Loss: 0.073762	Acc: 7.3% (732/10000)
[Test]  Epoch: 52	Loss: 0.073463	Acc: 7.0% (702/10000)
[Test]  Epoch: 53	Loss: 0.072707	Acc: 7.9% (789/10000)
[Test]  Epoch: 54	Loss: 0.073037	Acc: 7.4% (741/10000)
[Test]  Epoch: 55	Loss: 0.072657	Acc: 8.0% (800/10000)
[Test]  Epoch: 56	Loss: 0.074545	Acc: 7.1% (706/10000)
[Test]  Epoch: 57	Loss: 0.072220	Acc: 8.0% (795/10000)
[Test]  Epoch: 58	Loss: 0.073123	Acc: 8.0% (796/10000)
[Test]  Epoch: 59	Loss: 0.073697	Acc: 7.2% (724/10000)
[Test]  Epoch: 60	Loss: 0.072568	Acc: 8.0% (802/10000)
[Test]  Epoch: 61	Loss: 0.071394	Acc: 8.6% (859/10000)
[Test]  Epoch: 62	Loss: 0.071133	Acc: 8.7% (869/10000)
[Test]  Epoch: 63	Loss: 0.070830	Acc: 8.8% (879/10000)
[Test]  Epoch: 64	Loss: 0.070712	Acc: 8.9% (893/10000)
[Test]  Epoch: 65	Loss: 0.070642	Acc: 8.8% (885/10000)
[Test]  Epoch: 66	Loss: 0.070498	Acc: 9.2% (924/10000)
[Test]  Epoch: 67	Loss: 0.070812	Acc: 8.9% (893/10000)
[Test]  Epoch: 68	Loss: 0.070868	Acc: 9.2% (918/10000)
[Test]  Epoch: 69	Loss: 0.070690	Acc: 8.9% (894/10000)
[Test]  Epoch: 70	Loss: 0.070292	Acc: 9.0% (899/10000)
[Test]  Epoch: 71	Loss: 0.070798	Acc: 9.4% (937/10000)
[Test]  Epoch: 72	Loss: 0.070464	Acc: 9.0% (898/10000)
[Test]  Epoch: 73	Loss: 0.070485	Acc: 9.0% (900/10000)
[Test]  Epoch: 74	Loss: 0.070778	Acc: 8.7% (874/10000)
[Test]  Epoch: 75	Loss: 0.070703	Acc: 8.5% (854/10000)
[Test]  Epoch: 76	Loss: 0.070620	Acc: 8.9% (894/10000)
[Test]  Epoch: 77	Loss: 0.070443	Acc: 9.4% (937/10000)
[Test]  Epoch: 78	Loss: 0.070732	Acc: 9.1% (909/10000)
[Test]  Epoch: 79	Loss: 0.070729	Acc: 9.1% (907/10000)
[Test]  Epoch: 80	Loss: 0.070259	Acc: 9.6% (956/10000)
[Test]  Epoch: 81	Loss: 0.070567	Acc: 9.3% (930/10000)
[Test]  Epoch: 82	Loss: 0.070744	Acc: 8.8% (880/10000)
[Test]  Epoch: 83	Loss: 0.070760	Acc: 9.2% (920/10000)
[Test]  Epoch: 84	Loss: 0.070631	Acc: 9.3% (930/10000)
[Test]  Epoch: 85	Loss: 0.070429	Acc: 9.1% (906/10000)
[Test]  Epoch: 86	Loss: 0.070548	Acc: 9.2% (925/10000)
[Test]  Epoch: 87	Loss: 0.070610	Acc: 9.2% (915/10000)
[Test]  Epoch: 88	Loss: 0.070244	Acc: 9.6% (962/10000)
[Test]  Epoch: 89	Loss: 0.070473	Acc: 9.0% (896/10000)
[Test]  Epoch: 90	Loss: 0.070374	Acc: 9.1% (911/10000)
[Test]  Epoch: 91	Loss: 0.070784	Acc: 9.3% (926/10000)
[Test]  Epoch: 92	Loss: 0.070279	Acc: 9.3% (933/10000)
[Test]  Epoch: 93	Loss: 0.070565	Acc: 9.4% (945/10000)
[Test]  Epoch: 94	Loss: 0.070462	Acc: 8.9% (890/10000)
[Test]  Epoch: 95	Loss: 0.070541	Acc: 9.3% (933/10000)
[Test]  Epoch: 96	Loss: 0.070938	Acc: 9.1% (911/10000)
[Test]  Epoch: 97	Loss: 0.070375	Acc: 9.2% (915/10000)
[Test]  Epoch: 98	Loss: 0.070626	Acc: 9.5% (946/10000)
[Test]  Epoch: 99	Loss: 0.070346	Acc: 9.1% (912/10000)
[Test]  Epoch: 100	Loss: 0.070456	Acc: 9.2% (918/10000)
===========finish==========
['2024-08-19', '02:48:34.275285', '100', 'test', '0.07045646049976349', '9.18', '9.62']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.222285	Acc: 0.5% (54/10000)
[Test]  Epoch: 2	Loss: 0.084416	Acc: 2.4% (241/10000)
[Test]  Epoch: 3	Loss: 0.082264	Acc: 2.9% (287/10000)
[Test]  Epoch: 4	Loss: 0.079645	Acc: 3.0% (295/10000)
[Test]  Epoch: 5	Loss: 0.078364	Acc: 3.5% (345/10000)
[Test]  Epoch: 6	Loss: 0.078023	Acc: 3.7% (372/10000)
[Test]  Epoch: 7	Loss: 0.078145	Acc: 3.7% (367/10000)
[Test]  Epoch: 8	Loss: 0.078741	Acc: 3.5% (345/10000)
[Test]  Epoch: 9	Loss: 0.077420	Acc: 3.8% (380/10000)
[Test]  Epoch: 10	Loss: 0.078496	Acc: 3.6% (365/10000)
[Test]  Epoch: 11	Loss: 0.077344	Acc: 3.9% (386/10000)
[Test]  Epoch: 12	Loss: 0.077511	Acc: 3.9% (390/10000)
[Test]  Epoch: 13	Loss: 0.077647	Acc: 4.4% (443/10000)
[Test]  Epoch: 14	Loss: 0.076960	Acc: 4.7% (466/10000)
[Test]  Epoch: 15	Loss: 0.077103	Acc: 4.7% (467/10000)
[Test]  Epoch: 16	Loss: 0.077106	Acc: 4.5% (453/10000)
[Test]  Epoch: 17	Loss: 0.076027	Acc: 5.0% (502/10000)
[Test]  Epoch: 18	Loss: 0.077895	Acc: 4.4% (442/10000)
[Test]  Epoch: 19	Loss: 0.076222	Acc: 5.1% (512/10000)
[Test]  Epoch: 20	Loss: 0.077350	Acc: 5.1% (507/10000)
[Test]  Epoch: 21	Loss: 0.075698	Acc: 5.5% (548/10000)
[Test]  Epoch: 22	Loss: 0.076162	Acc: 5.0% (499/10000)
[Test]  Epoch: 23	Loss: 0.077104	Acc: 4.5% (451/10000)
[Test]  Epoch: 24	Loss: 0.076118	Acc: 5.1% (512/10000)
[Test]  Epoch: 25	Loss: 0.076582	Acc: 5.0% (500/10000)
[Test]  Epoch: 26	Loss: 0.075454	Acc: 5.6% (561/10000)
[Test]  Epoch: 27	Loss: 0.075534	Acc: 5.5% (547/10000)
[Test]  Epoch: 28	Loss: 0.076102	Acc: 5.5% (555/10000)
[Test]  Epoch: 29	Loss: 0.075456	Acc: 5.6% (561/10000)
[Test]  Epoch: 30	Loss: 0.074507	Acc: 5.9% (593/10000)
[Test]  Epoch: 31	Loss: 0.076720	Acc: 5.1% (509/10000)
[Test]  Epoch: 32	Loss: 0.074766	Acc: 6.0% (595/10000)
[Test]  Epoch: 33	Loss: 0.076507	Acc: 5.3% (529/10000)
[Test]  Epoch: 34	Loss: 0.075654	Acc: 5.9% (589/10000)
[Test]  Epoch: 35	Loss: 0.076537	Acc: 5.1% (506/10000)
[Test]  Epoch: 36	Loss: 0.076086	Acc: 5.7% (574/10000)
[Test]  Epoch: 37	Loss: 0.075990	Acc: 5.5% (549/10000)
[Test]  Epoch: 38	Loss: 0.074723	Acc: 6.0% (604/10000)
[Test]  Epoch: 39	Loss: 0.074571	Acc: 6.7% (674/10000)
[Test]  Epoch: 40	Loss: 0.074758	Acc: 6.1% (610/10000)
[Test]  Epoch: 41	Loss: 0.075137	Acc: 6.1% (607/10000)
[Test]  Epoch: 42	Loss: 0.074783	Acc: 6.2% (615/10000)
[Test]  Epoch: 43	Loss: 0.075383	Acc: 6.0% (595/10000)
[Test]  Epoch: 44	Loss: 0.074710	Acc: 6.2% (623/10000)
[Test]  Epoch: 45	Loss: 0.075639	Acc: 6.0% (598/10000)
[Test]  Epoch: 46	Loss: 0.075687	Acc: 5.6% (563/10000)
[Test]  Epoch: 47	Loss: 0.074521	Acc: 6.3% (633/10000)
[Test]  Epoch: 48	Loss: 0.075127	Acc: 6.4% (636/10000)
[Test]  Epoch: 49	Loss: 0.075485	Acc: 6.2% (618/10000)
[Test]  Epoch: 50	Loss: 0.074874	Acc: 6.6% (664/10000)
[Test]  Epoch: 51	Loss: 0.075515	Acc: 5.9% (591/10000)
[Test]  Epoch: 52	Loss: 0.074853	Acc: 6.3% (633/10000)
[Test]  Epoch: 53	Loss: 0.074730	Acc: 6.7% (673/10000)
[Test]  Epoch: 54	Loss: 0.075135	Acc: 6.1% (613/10000)
[Test]  Epoch: 55	Loss: 0.075228	Acc: 6.7% (667/10000)
[Test]  Epoch: 56	Loss: 0.075026	Acc: 6.7% (674/10000)
[Test]  Epoch: 57	Loss: 0.074831	Acc: 6.8% (685/10000)
[Test]  Epoch: 58	Loss: 0.074623	Acc: 6.6% (659/10000)
[Test]  Epoch: 59	Loss: 0.074983	Acc: 6.2% (623/10000)
[Test]  Epoch: 60	Loss: 0.075149	Acc: 6.2% (625/10000)
[Test]  Epoch: 61	Loss: 0.072932	Acc: 7.2% (719/10000)
[Test]  Epoch: 62	Loss: 0.072702	Acc: 7.4% (738/10000)
[Test]  Epoch: 63	Loss: 0.072837	Acc: 7.8% (775/10000)
[Test]  Epoch: 64	Loss: 0.072771	Acc: 7.4% (739/10000)
[Test]  Epoch: 65	Loss: 0.072631	Acc: 7.6% (757/10000)
[Test]  Epoch: 66	Loss: 0.072383	Acc: 7.8% (780/10000)
[Test]  Epoch: 67	Loss: 0.072839	Acc: 7.4% (740/10000)
[Test]  Epoch: 68	Loss: 0.072316	Acc: 7.8% (779/10000)
[Test]  Epoch: 69	Loss: 0.072473	Acc: 7.6% (761/10000)
[Test]  Epoch: 70	Loss: 0.072472	Acc: 7.8% (782/10000)
[Test]  Epoch: 71	Loss: 0.072594	Acc: 7.5% (747/10000)
[Test]  Epoch: 72	Loss: 0.072460	Acc: 7.7% (773/10000)
[Test]  Epoch: 73	Loss: 0.072306	Acc: 8.0% (799/10000)
[Test]  Epoch: 74	Loss: 0.072758	Acc: 7.5% (745/10000)
[Test]  Epoch: 75	Loss: 0.072562	Acc: 7.5% (755/10000)
[Test]  Epoch: 76	Loss: 0.072332	Acc: 7.6% (759/10000)
[Test]  Epoch: 77	Loss: 0.072618	Acc: 7.5% (748/10000)
[Test]  Epoch: 78	Loss: 0.072429	Acc: 7.7% (766/10000)
[Test]  Epoch: 79	Loss: 0.072561	Acc: 8.1% (805/10000)
[Test]  Epoch: 80	Loss: 0.071975	Acc: 8.3% (829/10000)
[Test]  Epoch: 81	Loss: 0.072478	Acc: 7.5% (746/10000)
[Test]  Epoch: 82	Loss: 0.072330	Acc: 7.8% (784/10000)
[Test]  Epoch: 83	Loss: 0.072511	Acc: 8.1% (813/10000)
[Test]  Epoch: 84	Loss: 0.072383	Acc: 7.7% (766/10000)
[Test]  Epoch: 85	Loss: 0.072306	Acc: 8.1% (809/10000)
[Test]  Epoch: 86	Loss: 0.072516	Acc: 7.4% (742/10000)
[Test]  Epoch: 87	Loss: 0.072232	Acc: 8.2% (819/10000)
[Test]  Epoch: 88	Loss: 0.072197	Acc: 8.1% (807/10000)
[Test]  Epoch: 89	Loss: 0.072298	Acc: 8.2% (816/10000)
[Test]  Epoch: 90	Loss: 0.072293	Acc: 8.0% (796/10000)
[Test]  Epoch: 91	Loss: 0.072335	Acc: 8.2% (821/10000)
[Test]  Epoch: 92	Loss: 0.072268	Acc: 8.1% (805/10000)
[Test]  Epoch: 93	Loss: 0.072370	Acc: 8.1% (808/10000)
[Test]  Epoch: 94	Loss: 0.072210	Acc: 7.8% (783/10000)
[Test]  Epoch: 95	Loss: 0.072698	Acc: 7.6% (763/10000)
[Test]  Epoch: 96	Loss: 0.072731	Acc: 8.0% (803/10000)
[Test]  Epoch: 97	Loss: 0.072074	Acc: 8.0% (804/10000)
[Test]  Epoch: 98	Loss: 0.072388	Acc: 8.0% (803/10000)
[Test]  Epoch: 99	Loss: 0.072542	Acc: 7.8% (779/10000)
[Test]  Epoch: 100	Loss: 0.072461	Acc: 7.9% (789/10000)
===========finish==========
['2024-08-19', '02:52:31.960524', '100', 'test', '0.07246058030128479', '7.89', '8.29']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.288291	Acc: 0.6% (58/10000)
[Test]  Epoch: 2	Loss: 0.080783	Acc: 2.7% (268/10000)
[Test]  Epoch: 3	Loss: 0.079809	Acc: 2.5% (254/10000)
[Test]  Epoch: 4	Loss: 0.078838	Acc: 3.1% (314/10000)
[Test]  Epoch: 5	Loss: 0.077885	Acc: 3.4% (337/10000)
[Test]  Epoch: 6	Loss: 0.078704	Acc: 3.1% (315/10000)
[Test]  Epoch: 7	Loss: 0.077263	Acc: 3.7% (366/10000)
[Test]  Epoch: 8	Loss: 0.078286	Acc: 3.4% (338/10000)
[Test]  Epoch: 9	Loss: 0.077687	Acc: 3.7% (371/10000)
[Test]  Epoch: 10	Loss: 0.077157	Acc: 4.0% (401/10000)
[Test]  Epoch: 11	Loss: 0.077499	Acc: 3.9% (386/10000)
[Test]  Epoch: 12	Loss: 0.076574	Acc: 4.4% (444/10000)
[Test]  Epoch: 13	Loss: 0.076299	Acc: 4.3% (428/10000)
[Test]  Epoch: 14	Loss: 0.077199	Acc: 4.3% (435/10000)
[Test]  Epoch: 15	Loss: 0.078066	Acc: 4.1% (411/10000)
[Test]  Epoch: 16	Loss: 0.076820	Acc: 4.7% (466/10000)
[Test]  Epoch: 17	Loss: 0.075955	Acc: 5.1% (510/10000)
[Test]  Epoch: 18	Loss: 0.075920	Acc: 4.7% (472/10000)
[Test]  Epoch: 19	Loss: 0.076219	Acc: 4.9% (490/10000)
[Test]  Epoch: 20	Loss: 0.075805	Acc: 5.4% (543/10000)
[Test]  Epoch: 21	Loss: 0.076154	Acc: 4.9% (492/10000)
[Test]  Epoch: 22	Loss: 0.076185	Acc: 4.9% (494/10000)
[Test]  Epoch: 23	Loss: 0.077368	Acc: 5.2% (516/10000)
[Test]  Epoch: 24	Loss: 0.075688	Acc: 5.4% (543/10000)
[Test]  Epoch: 25	Loss: 0.076785	Acc: 5.4% (537/10000)
[Test]  Epoch: 26	Loss: 0.075110	Acc: 5.5% (545/10000)
[Test]  Epoch: 27	Loss: 0.075305	Acc: 5.3% (535/10000)
[Test]  Epoch: 28	Loss: 0.075161	Acc: 5.6% (563/10000)
[Test]  Epoch: 29	Loss: 0.075879	Acc: 5.1% (513/10000)
[Test]  Epoch: 30	Loss: 0.075912	Acc: 5.4% (537/10000)
[Test]  Epoch: 31	Loss: 0.076279	Acc: 5.0% (500/10000)
[Test]  Epoch: 32	Loss: 0.075140	Acc: 5.7% (568/10000)
[Test]  Epoch: 33	Loss: 0.076189	Acc: 5.4% (538/10000)
[Test]  Epoch: 34	Loss: 0.074990	Acc: 5.9% (594/10000)
[Test]  Epoch: 35	Loss: 0.076239	Acc: 5.3% (530/10000)
[Test]  Epoch: 36	Loss: 0.076124	Acc: 5.8% (583/10000)
[Test]  Epoch: 37	Loss: 0.076075	Acc: 5.8% (576/10000)
[Test]  Epoch: 38	Loss: 0.075484	Acc: 5.8% (575/10000)
[Test]  Epoch: 39	Loss: 0.076197	Acc: 6.0% (598/10000)
[Test]  Epoch: 40	Loss: 0.075269	Acc: 5.5% (551/10000)
[Test]  Epoch: 41	Loss: 0.074284	Acc: 6.2% (621/10000)
[Test]  Epoch: 42	Loss: 0.074671	Acc: 6.2% (620/10000)
[Test]  Epoch: 43	Loss: 0.075528	Acc: 6.2% (616/10000)
[Test]  Epoch: 44	Loss: 0.075038	Acc: 6.1% (613/10000)
[Test]  Epoch: 45	Loss: 0.075795	Acc: 5.5% (554/10000)
[Test]  Epoch: 46	Loss: 0.075186	Acc: 6.1% (610/10000)
[Test]  Epoch: 47	Loss: 0.074944	Acc: 6.3% (631/10000)
[Test]  Epoch: 48	Loss: 0.074718	Acc: 6.8% (675/10000)
[Test]  Epoch: 49	Loss: 0.076923	Acc: 5.6% (562/10000)
[Test]  Epoch: 50	Loss: 0.076670	Acc: 5.9% (592/10000)
[Test]  Epoch: 51	Loss: 0.074928	Acc: 6.2% (616/10000)
[Test]  Epoch: 52	Loss: 0.076743	Acc: 5.7% (574/10000)
[Test]  Epoch: 53	Loss: 0.074259	Acc: 6.5% (648/10000)
[Test]  Epoch: 54	Loss: 0.074813	Acc: 6.4% (640/10000)
[Test]  Epoch: 55	Loss: 0.075093	Acc: 6.5% (652/10000)
[Test]  Epoch: 56	Loss: 0.075605	Acc: 6.0% (595/10000)
[Test]  Epoch: 57	Loss: 0.075732	Acc: 6.4% (642/10000)
[Test]  Epoch: 58	Loss: 0.074698	Acc: 6.3% (631/10000)
[Test]  Epoch: 59	Loss: 0.074541	Acc: 6.4% (638/10000)
[Test]  Epoch: 60	Loss: 0.075123	Acc: 6.4% (639/10000)
[Test]  Epoch: 61	Loss: 0.073244	Acc: 7.2% (720/10000)
[Test]  Epoch: 62	Loss: 0.072852	Acc: 7.5% (748/10000)
[Test]  Epoch: 63	Loss: 0.073155	Acc: 7.4% (742/10000)
[Test]  Epoch: 64	Loss: 0.072758	Acc: 7.6% (763/10000)
[Test]  Epoch: 65	Loss: 0.072619	Acc: 7.2% (724/10000)
[Test]  Epoch: 66	Loss: 0.072608	Acc: 7.6% (762/10000)
[Test]  Epoch: 67	Loss: 0.072927	Acc: 7.3% (734/10000)
[Test]  Epoch: 68	Loss: 0.072732	Acc: 7.4% (742/10000)
[Test]  Epoch: 69	Loss: 0.072832	Acc: 7.6% (759/10000)
[Test]  Epoch: 70	Loss: 0.072480	Acc: 7.7% (767/10000)
[Test]  Epoch: 71	Loss: 0.072590	Acc: 7.8% (777/10000)
[Test]  Epoch: 72	Loss: 0.072592	Acc: 7.6% (764/10000)
[Test]  Epoch: 73	Loss: 0.072512	Acc: 7.6% (763/10000)
[Test]  Epoch: 74	Loss: 0.072806	Acc: 7.2% (721/10000)
[Test]  Epoch: 75	Loss: 0.072638	Acc: 7.4% (737/10000)
[Test]  Epoch: 76	Loss: 0.072612	Acc: 7.5% (748/10000)
[Test]  Epoch: 77	Loss: 0.072671	Acc: 7.8% (776/10000)
[Test]  Epoch: 78	Loss: 0.072801	Acc: 7.6% (760/10000)
[Test]  Epoch: 79	Loss: 0.072914	Acc: 7.9% (789/10000)
[Test]  Epoch: 80	Loss: 0.072195	Acc: 8.0% (795/10000)
[Test]  Epoch: 81	Loss: 0.072555	Acc: 7.9% (792/10000)
[Test]  Epoch: 82	Loss: 0.072442	Acc: 7.4% (739/10000)
[Test]  Epoch: 83	Loss: 0.072857	Acc: 7.3% (731/10000)
[Test]  Epoch: 84	Loss: 0.072690	Acc: 7.6% (760/10000)
[Test]  Epoch: 85	Loss: 0.072527	Acc: 7.9% (789/10000)
[Test]  Epoch: 86	Loss: 0.072771	Acc: 7.5% (753/10000)
[Test]  Epoch: 87	Loss: 0.072690	Acc: 7.8% (783/10000)
[Test]  Epoch: 88	Loss: 0.072344	Acc: 7.8% (775/10000)
[Test]  Epoch: 89	Loss: 0.072480	Acc: 7.5% (746/10000)
[Test]  Epoch: 90	Loss: 0.072447	Acc: 7.6% (761/10000)
[Test]  Epoch: 91	Loss: 0.072650	Acc: 8.0% (795/10000)
[Test]  Epoch: 92	Loss: 0.072514	Acc: 7.6% (757/10000)
[Test]  Epoch: 93	Loss: 0.072743	Acc: 8.0% (798/10000)
[Test]  Epoch: 94	Loss: 0.072766	Acc: 7.6% (757/10000)
[Test]  Epoch: 95	Loss: 0.072911	Acc: 7.7% (770/10000)
[Test]  Epoch: 96	Loss: 0.073007	Acc: 7.7% (767/10000)
[Test]  Epoch: 97	Loss: 0.072503	Acc: 7.7% (772/10000)
[Test]  Epoch: 98	Loss: 0.072656	Acc: 7.7% (773/10000)
[Test]  Epoch: 99	Loss: 0.072501	Acc: 7.5% (750/10000)
[Test]  Epoch: 100	Loss: 0.072916	Acc: 7.7% (772/10000)
===========finish==========
['2024-08-19', '02:56:23.504303', '100', 'test', '0.07291643536090851', '7.72', '7.98']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.192546	Acc: 0.7% (70/10000)
[Test]  Epoch: 2	Loss: 0.083330	Acc: 2.0% (199/10000)
[Test]  Epoch: 3	Loss: 0.080151	Acc: 2.5% (247/10000)
[Test]  Epoch: 4	Loss: 0.080421	Acc: 2.6% (262/10000)
[Test]  Epoch: 5	Loss: 0.078509	Acc: 3.1% (307/10000)
[Test]  Epoch: 6	Loss: 0.078744	Acc: 3.0% (301/10000)
[Test]  Epoch: 7	Loss: 0.078228	Acc: 3.4% (341/10000)
[Test]  Epoch: 8	Loss: 0.078495	Acc: 3.3% (330/10000)
[Test]  Epoch: 9	Loss: 0.077882	Acc: 3.2% (321/10000)
[Test]  Epoch: 10	Loss: 0.077533	Acc: 3.5% (353/10000)
[Test]  Epoch: 11	Loss: 0.078267	Acc: 3.1% (309/10000)
[Test]  Epoch: 12	Loss: 0.077104	Acc: 3.8% (384/10000)
[Test]  Epoch: 13	Loss: 0.077785	Acc: 3.4% (343/10000)
[Test]  Epoch: 14	Loss: 0.078036	Acc: 3.8% (379/10000)
[Test]  Epoch: 15	Loss: 0.077320	Acc: 3.7% (372/10000)
[Test]  Epoch: 16	Loss: 0.076894	Acc: 4.0% (399/10000)
[Test]  Epoch: 17	Loss: 0.077633	Acc: 4.0% (403/10000)
[Test]  Epoch: 18	Loss: 0.077215	Acc: 4.1% (409/10000)
[Test]  Epoch: 19	Loss: 0.076349	Acc: 4.4% (437/10000)
[Test]  Epoch: 20	Loss: 0.076816	Acc: 4.3% (433/10000)
[Test]  Epoch: 21	Loss: 0.076905	Acc: 4.3% (431/10000)
[Test]  Epoch: 22	Loss: 0.075913	Acc: 4.8% (475/10000)
[Test]  Epoch: 23	Loss: 0.077614	Acc: 4.2% (422/10000)
[Test]  Epoch: 24	Loss: 0.075694	Acc: 4.8% (477/10000)
[Test]  Epoch: 25	Loss: 0.076096	Acc: 4.7% (465/10000)
[Test]  Epoch: 26	Loss: 0.077015	Acc: 4.8% (479/10000)
[Test]  Epoch: 27	Loss: 0.076872	Acc: 4.6% (458/10000)
[Test]  Epoch: 28	Loss: 0.076027	Acc: 4.8% (482/10000)
[Test]  Epoch: 29	Loss: 0.075772	Acc: 4.6% (459/10000)
[Test]  Epoch: 30	Loss: 0.075249	Acc: 5.2% (523/10000)
[Test]  Epoch: 31	Loss: 0.076655	Acc: 4.6% (460/10000)
[Test]  Epoch: 32	Loss: 0.074946	Acc: 5.2% (516/10000)
[Test]  Epoch: 33	Loss: 0.076099	Acc: 5.1% (506/10000)
[Test]  Epoch: 34	Loss: 0.075782	Acc: 5.2% (520/10000)
[Test]  Epoch: 35	Loss: 0.076656	Acc: 4.8% (483/10000)
[Test]  Epoch: 36	Loss: 0.075101	Acc: 5.5% (545/10000)
[Test]  Epoch: 37	Loss: 0.076801	Acc: 4.6% (462/10000)
[Test]  Epoch: 38	Loss: 0.075038	Acc: 5.5% (554/10000)
[Test]  Epoch: 39	Loss: 0.075568	Acc: 5.4% (542/10000)
[Test]  Epoch: 40	Loss: 0.074698	Acc: 5.8% (579/10000)
[Test]  Epoch: 41	Loss: 0.074981	Acc: 5.5% (551/10000)
[Test]  Epoch: 42	Loss: 0.075211	Acc: 5.5% (545/10000)
[Test]  Epoch: 43	Loss: 0.076199	Acc: 5.2% (524/10000)
[Test]  Epoch: 44	Loss: 0.075733	Acc: 5.4% (542/10000)
[Test]  Epoch: 45	Loss: 0.075662	Acc: 5.2% (524/10000)
[Test]  Epoch: 46	Loss: 0.075214	Acc: 5.7% (568/10000)
[Test]  Epoch: 47	Loss: 0.075630	Acc: 5.6% (559/10000)
[Test]  Epoch: 48	Loss: 0.075541	Acc: 5.9% (590/10000)
[Test]  Epoch: 49	Loss: 0.075717	Acc: 5.4% (540/10000)
[Test]  Epoch: 50	Loss: 0.075987	Acc: 5.2% (519/10000)
[Test]  Epoch: 51	Loss: 0.076098	Acc: 5.3% (528/10000)
[Test]  Epoch: 52	Loss: 0.076282	Acc: 6.1% (606/10000)
[Test]  Epoch: 53	Loss: 0.075964	Acc: 5.4% (544/10000)
[Test]  Epoch: 54	Loss: 0.075291	Acc: 5.3% (531/10000)
[Test]  Epoch: 55	Loss: 0.076635	Acc: 5.4% (542/10000)
[Test]  Epoch: 56	Loss: 0.075099	Acc: 5.7% (573/10000)
[Test]  Epoch: 57	Loss: 0.076767	Acc: 5.5% (554/10000)
[Test]  Epoch: 58	Loss: 0.075404	Acc: 5.8% (579/10000)
[Test]  Epoch: 59	Loss: 0.076256	Acc: 5.4% (537/10000)
[Test]  Epoch: 60	Loss: 0.075564	Acc: 5.8% (579/10000)
[Test]  Epoch: 61	Loss: 0.073982	Acc: 6.3% (629/10000)
[Test]  Epoch: 62	Loss: 0.073842	Acc: 6.6% (660/10000)
[Test]  Epoch: 63	Loss: 0.073880	Acc: 6.4% (641/10000)
[Test]  Epoch: 64	Loss: 0.073541	Acc: 6.7% (674/10000)
[Test]  Epoch: 65	Loss: 0.073757	Acc: 6.6% (657/10000)
[Test]  Epoch: 66	Loss: 0.073657	Acc: 6.8% (679/10000)
[Test]  Epoch: 67	Loss: 0.073947	Acc: 6.4% (639/10000)
[Test]  Epoch: 68	Loss: 0.073643	Acc: 6.5% (647/10000)
[Test]  Epoch: 69	Loss: 0.073634	Acc: 6.8% (685/10000)
[Test]  Epoch: 70	Loss: 0.073349	Acc: 6.9% (690/10000)
[Test]  Epoch: 71	Loss: 0.073453	Acc: 6.8% (682/10000)
[Test]  Epoch: 72	Loss: 0.073628	Acc: 6.5% (654/10000)
[Test]  Epoch: 73	Loss: 0.073472	Acc: 6.7% (673/10000)
[Test]  Epoch: 74	Loss: 0.073647	Acc: 6.8% (675/10000)
[Test]  Epoch: 75	Loss: 0.073559	Acc: 6.7% (666/10000)
[Test]  Epoch: 76	Loss: 0.073450	Acc: 6.8% (685/10000)
[Test]  Epoch: 77	Loss: 0.073577	Acc: 7.0% (696/10000)
[Test]  Epoch: 78	Loss: 0.073669	Acc: 6.9% (690/10000)
[Test]  Epoch: 79	Loss: 0.073627	Acc: 6.7% (672/10000)
[Test]  Epoch: 80	Loss: 0.073182	Acc: 7.0% (698/10000)
[Test]  Epoch: 81	Loss: 0.073504	Acc: 6.9% (691/10000)
[Test]  Epoch: 82	Loss: 0.073537	Acc: 6.8% (682/10000)
[Test]  Epoch: 83	Loss: 0.073665	Acc: 6.9% (689/10000)
[Test]  Epoch: 84	Loss: 0.073682	Acc: 6.6% (662/10000)
[Test]  Epoch: 85	Loss: 0.073488	Acc: 6.8% (679/10000)
[Test]  Epoch: 86	Loss: 0.073597	Acc: 6.8% (677/10000)
[Test]  Epoch: 87	Loss: 0.073573	Acc: 6.4% (642/10000)
[Test]  Epoch: 88	Loss: 0.073344	Acc: 7.0% (702/10000)
[Test]  Epoch: 89	Loss: 0.073589	Acc: 6.8% (681/10000)
[Test]  Epoch: 90	Loss: 0.073403	Acc: 6.8% (682/10000)
[Test]  Epoch: 91	Loss: 0.073905	Acc: 7.0% (698/10000)
[Test]  Epoch: 92	Loss: 0.073363	Acc: 7.0% (702/10000)
[Test]  Epoch: 93	Loss: 0.073699	Acc: 7.0% (698/10000)
[Test]  Epoch: 94	Loss: 0.073702	Acc: 6.7% (668/10000)
[Test]  Epoch: 95	Loss: 0.073705	Acc: 7.2% (723/10000)
[Test]  Epoch: 96	Loss: 0.073816	Acc: 6.6% (660/10000)
[Test]  Epoch: 97	Loss: 0.073377	Acc: 7.0% (698/10000)
[Test]  Epoch: 98	Loss: 0.073756	Acc: 6.5% (653/10000)
[Test]  Epoch: 99	Loss: 0.073675	Acc: 7.0% (698/10000)
[Test]  Epoch: 100	Loss: 0.073918	Acc: 7.0% (702/10000)
===========finish==========
['2024-08-19', '03:01:05.305102', '100', 'test', '0.07391837737560272', '7.02', '7.23']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.290920	Acc: 0.5% (49/10000)
[Test]  Epoch: 2	Loss: 0.083920	Acc: 1.6% (165/10000)
[Test]  Epoch: 3	Loss: 0.082229	Acc: 2.1% (206/10000)
[Test]  Epoch: 4	Loss: 0.081281	Acc: 2.3% (232/10000)
[Test]  Epoch: 5	Loss: 0.079209	Acc: 2.7% (267/10000)
[Test]  Epoch: 6	Loss: 0.080060	Acc: 2.7% (266/10000)
[Test]  Epoch: 7	Loss: 0.078560	Acc: 3.1% (311/10000)
[Test]  Epoch: 8	Loss: 0.079094	Acc: 2.8% (281/10000)
[Test]  Epoch: 9	Loss: 0.079878	Acc: 2.7% (266/10000)
[Test]  Epoch: 10	Loss: 0.078483	Acc: 2.9% (293/10000)
[Test]  Epoch: 11	Loss: 0.078392	Acc: 3.3% (329/10000)
[Test]  Epoch: 12	Loss: 0.077603	Acc: 3.5% (346/10000)
[Test]  Epoch: 13	Loss: 0.077780	Acc: 3.5% (348/10000)
[Test]  Epoch: 14	Loss: 0.077231	Acc: 3.8% (378/10000)
[Test]  Epoch: 15	Loss: 0.077746	Acc: 3.8% (380/10000)
[Test]  Epoch: 16	Loss: 0.077350	Acc: 4.0% (397/10000)
[Test]  Epoch: 17	Loss: 0.078698	Acc: 3.6% (359/10000)
[Test]  Epoch: 18	Loss: 0.077362	Acc: 4.0% (403/10000)
[Test]  Epoch: 19	Loss: 0.076778	Acc: 4.2% (415/10000)
[Test]  Epoch: 20	Loss: 0.076579	Acc: 4.3% (426/10000)
[Test]  Epoch: 21	Loss: 0.076492	Acc: 4.5% (448/10000)
[Test]  Epoch: 22	Loss: 0.077178	Acc: 4.0% (396/10000)
[Test]  Epoch: 23	Loss: 0.077383	Acc: 4.1% (411/10000)
[Test]  Epoch: 24	Loss: 0.076484	Acc: 3.8% (382/10000)
[Test]  Epoch: 25	Loss: 0.076184	Acc: 4.7% (467/10000)
[Test]  Epoch: 26	Loss: 0.076054	Acc: 4.7% (465/10000)
[Test]  Epoch: 27	Loss: 0.076917	Acc: 4.1% (411/10000)
[Test]  Epoch: 28	Loss: 0.076316	Acc: 4.3% (428/10000)
[Test]  Epoch: 29	Loss: 0.076479	Acc: 4.4% (442/10000)
[Test]  Epoch: 30	Loss: 0.075509	Acc: 4.4% (444/10000)
[Test]  Epoch: 31	Loss: 0.077814	Acc: 4.3% (428/10000)
[Test]  Epoch: 32	Loss: 0.075401	Acc: 5.1% (508/10000)
[Test]  Epoch: 33	Loss: 0.076548	Acc: 4.6% (458/10000)
[Test]  Epoch: 34	Loss: 0.075673	Acc: 5.0% (498/10000)
[Test]  Epoch: 35	Loss: 0.077208	Acc: 4.1% (410/10000)
[Test]  Epoch: 36	Loss: 0.076362	Acc: 4.8% (485/10000)
[Test]  Epoch: 37	Loss: 0.076783	Acc: 4.5% (448/10000)
[Test]  Epoch: 38	Loss: 0.075961	Acc: 5.0% (496/10000)
[Test]  Epoch: 39	Loss: 0.075608	Acc: 5.2% (515/10000)
[Test]  Epoch: 40	Loss: 0.075798	Acc: 4.8% (482/10000)
[Test]  Epoch: 41	Loss: 0.076555	Acc: 5.1% (514/10000)
[Test]  Epoch: 42	Loss: 0.075267	Acc: 5.1% (512/10000)
[Test]  Epoch: 43	Loss: 0.075894	Acc: 5.1% (512/10000)
[Test]  Epoch: 44	Loss: 0.075460	Acc: 5.2% (520/10000)
[Test]  Epoch: 45	Loss: 0.076206	Acc: 4.8% (475/10000)
[Test]  Epoch: 46	Loss: 0.075889	Acc: 5.0% (502/10000)
[Test]  Epoch: 47	Loss: 0.076789	Acc: 5.0% (501/10000)
[Test]  Epoch: 48	Loss: 0.076063	Acc: 5.3% (530/10000)
[Test]  Epoch: 49	Loss: 0.077120	Acc: 5.0% (505/10000)
[Test]  Epoch: 50	Loss: 0.075889	Acc: 5.4% (544/10000)
[Test]  Epoch: 51	Loss: 0.076595	Acc: 5.3% (530/10000)
[Test]  Epoch: 52	Loss: 0.076082	Acc: 5.1% (511/10000)
[Test]  Epoch: 53	Loss: 0.075281	Acc: 5.6% (556/10000)
[Test]  Epoch: 54	Loss: 0.075266	Acc: 5.4% (537/10000)
[Test]  Epoch: 55	Loss: 0.077126	Acc: 5.0% (505/10000)
[Test]  Epoch: 56	Loss: 0.076226	Acc: 5.2% (523/10000)
[Test]  Epoch: 57	Loss: 0.076914	Acc: 5.1% (514/10000)
[Test]  Epoch: 58	Loss: 0.075558	Acc: 5.4% (544/10000)
[Test]  Epoch: 59	Loss: 0.075902	Acc: 5.2% (517/10000)
[Test]  Epoch: 60	Loss: 0.076209	Acc: 5.7% (570/10000)
[Test]  Epoch: 61	Loss: 0.074764	Acc: 6.0% (600/10000)
[Test]  Epoch: 62	Loss: 0.074261	Acc: 6.3% (629/10000)
[Test]  Epoch: 63	Loss: 0.074631	Acc: 6.1% (609/10000)
[Test]  Epoch: 64	Loss: 0.074349	Acc: 6.1% (609/10000)
[Test]  Epoch: 65	Loss: 0.074211	Acc: 6.2% (619/10000)
[Test]  Epoch: 66	Loss: 0.073984	Acc: 6.6% (660/10000)
[Test]  Epoch: 67	Loss: 0.074302	Acc: 6.2% (615/10000)
[Test]  Epoch: 68	Loss: 0.074013	Acc: 6.0% (599/10000)
[Test]  Epoch: 69	Loss: 0.074328	Acc: 6.4% (643/10000)
[Test]  Epoch: 70	Loss: 0.073856	Acc: 6.7% (672/10000)
[Test]  Epoch: 71	Loss: 0.074021	Acc: 6.4% (644/10000)
[Test]  Epoch: 72	Loss: 0.074232	Acc: 6.1% (610/10000)
[Test]  Epoch: 73	Loss: 0.074089	Acc: 6.3% (628/10000)
[Test]  Epoch: 74	Loss: 0.074388	Acc: 6.2% (622/10000)
[Test]  Epoch: 75	Loss: 0.074132	Acc: 6.1% (609/10000)
[Test]  Epoch: 76	Loss: 0.074360	Acc: 6.1% (614/10000)
[Test]  Epoch: 77	Loss: 0.074261	Acc: 6.3% (634/10000)
[Test]  Epoch: 78	Loss: 0.074224	Acc: 6.5% (655/10000)
[Test]  Epoch: 79	Loss: 0.074270	Acc: 6.6% (660/10000)
[Test]  Epoch: 80	Loss: 0.073679	Acc: 6.7% (666/10000)
[Test]  Epoch: 81	Loss: 0.074335	Acc: 6.1% (611/10000)
[Test]  Epoch: 82	Loss: 0.074143	Acc: 6.2% (625/10000)
[Test]  Epoch: 83	Loss: 0.074348	Acc: 6.3% (629/10000)
[Test]  Epoch: 84	Loss: 0.074026	Acc: 6.6% (661/10000)
[Test]  Epoch: 85	Loss: 0.074078	Acc: 6.7% (667/10000)
[Test]  Epoch: 86	Loss: 0.074236	Acc: 6.3% (630/10000)
[Test]  Epoch: 87	Loss: 0.074317	Acc: 6.3% (627/10000)
[Test]  Epoch: 88	Loss: 0.074135	Acc: 6.3% (634/10000)
[Test]  Epoch: 89	Loss: 0.074160	Acc: 6.3% (628/10000)
[Test]  Epoch: 90	Loss: 0.074184	Acc: 6.3% (632/10000)
[Test]  Epoch: 91	Loss: 0.074398	Acc: 6.4% (638/10000)
[Test]  Epoch: 92	Loss: 0.074057	Acc: 6.5% (647/10000)
[Test]  Epoch: 93	Loss: 0.074028	Acc: 6.6% (658/10000)
[Test]  Epoch: 94	Loss: 0.074190	Acc: 6.4% (638/10000)
[Test]  Epoch: 95	Loss: 0.074532	Acc: 6.7% (666/10000)
[Test]  Epoch: 96	Loss: 0.074366	Acc: 6.5% (649/10000)
[Test]  Epoch: 97	Loss: 0.073876	Acc: 6.6% (658/10000)
[Test]  Epoch: 98	Loss: 0.074258	Acc: 6.5% (651/10000)
[Test]  Epoch: 99	Loss: 0.074264	Acc: 6.3% (633/10000)
[Test]  Epoch: 100	Loss: 0.074508	Acc: 6.2% (623/10000)
===========finish==========
['2024-08-19', '03:05:28.833128', '100', 'test', '0.07450829174518585', '6.23', '6.72']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight', 'features.40.weight', 'features.41.weight', 'classifier.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.103804	Acc: 0.5% (54/10000)
[Test]  Epoch: 2	Loss: 0.083523	Acc: 0.8% (76/10000)
[Test]  Epoch: 3	Loss: 0.083427	Acc: 0.9% (92/10000)
[Test]  Epoch: 4	Loss: 0.083359	Acc: 0.9% (86/10000)
[Test]  Epoch: 5	Loss: 0.082887	Acc: 1.1% (112/10000)
[Test]  Epoch: 6	Loss: 0.082721	Acc: 1.2% (121/10000)
[Test]  Epoch: 7	Loss: 0.082486	Acc: 1.6% (159/10000)
[Test]  Epoch: 8	Loss: 0.082325	Acc: 1.7% (166/10000)
[Test]  Epoch: 9	Loss: 0.081892	Acc: 1.8% (181/10000)
[Test]  Epoch: 10	Loss: 0.081465	Acc: 2.0% (197/10000)
[Test]  Epoch: 11	Loss: 0.081234	Acc: 2.0% (197/10000)
[Test]  Epoch: 12	Loss: 0.081129	Acc: 2.0% (202/10000)
[Test]  Epoch: 13	Loss: 0.080698	Acc: 2.3% (226/10000)
[Test]  Epoch: 14	Loss: 0.080445	Acc: 2.7% (273/10000)
[Test]  Epoch: 15	Loss: 0.080184	Acc: 2.5% (253/10000)
[Test]  Epoch: 16	Loss: 0.079961	Acc: 2.6% (261/10000)
[Test]  Epoch: 17	Loss: 0.079908	Acc: 2.8% (279/10000)
[Test]  Epoch: 18	Loss: 0.079519	Acc: 2.8% (284/10000)
[Test]  Epoch: 19	Loss: 0.079221	Acc: 3.1% (314/10000)
[Test]  Epoch: 20	Loss: 0.078922	Acc: 3.1% (309/10000)
[Test]  Epoch: 21	Loss: 0.078906	Acc: 3.3% (329/10000)
[Test]  Epoch: 22	Loss: 0.078969	Acc: 3.0% (300/10000)
[Test]  Epoch: 23	Loss: 0.078757	Acc: 3.1% (306/10000)
[Test]  Epoch: 24	Loss: 0.078471	Acc: 3.2% (316/10000)
[Test]  Epoch: 25	Loss: 0.078143	Acc: 3.4% (336/10000)
[Test]  Epoch: 26	Loss: 0.077954	Acc: 3.6% (362/10000)
[Test]  Epoch: 27	Loss: 0.077826	Acc: 3.3% (331/10000)
[Test]  Epoch: 28	Loss: 0.077690	Acc: 3.6% (363/10000)
[Test]  Epoch: 29	Loss: 0.077708	Acc: 3.4% (340/10000)
[Test]  Epoch: 30	Loss: 0.077854	Acc: 3.6% (360/10000)
[Test]  Epoch: 31	Loss: 0.077382	Acc: 3.9% (386/10000)
[Test]  Epoch: 32	Loss: 0.077376	Acc: 3.6% (364/10000)
[Test]  Epoch: 33	Loss: 0.077305	Acc: 3.9% (387/10000)
[Test]  Epoch: 34	Loss: 0.077360	Acc: 3.7% (372/10000)
[Test]  Epoch: 35	Loss: 0.077185	Acc: 3.5% (354/10000)
[Test]  Epoch: 36	Loss: 0.077121	Acc: 3.7% (374/10000)
[Test]  Epoch: 37	Loss: 0.076689	Acc: 3.9% (385/10000)
[Test]  Epoch: 38	Loss: 0.077113	Acc: 4.0% (405/10000)
[Test]  Epoch: 39	Loss: 0.076665	Acc: 3.9% (393/10000)
[Test]  Epoch: 40	Loss: 0.076498	Acc: 4.2% (416/10000)
[Test]  Epoch: 41	Loss: 0.076649	Acc: 3.7% (368/10000)
[Test]  Epoch: 42	Loss: 0.076973	Acc: 3.5% (354/10000)
[Test]  Epoch: 43	Loss: 0.076422	Acc: 3.8% (384/10000)
[Test]  Epoch: 44	Loss: 0.075989	Acc: 4.1% (406/10000)
[Test]  Epoch: 45	Loss: 0.076362	Acc: 3.8% (376/10000)
[Test]  Epoch: 46	Loss: 0.076139	Acc: 4.3% (435/10000)
[Test]  Epoch: 47	Loss: 0.076167	Acc: 4.5% (449/10000)
[Test]  Epoch: 48	Loss: 0.076384	Acc: 4.3% (431/10000)
[Test]  Epoch: 49	Loss: 0.076484	Acc: 4.2% (425/10000)
[Test]  Epoch: 50	Loss: 0.075723	Acc: 4.3% (435/10000)
[Test]  Epoch: 51	Loss: 0.076917	Acc: 4.0% (404/10000)
[Test]  Epoch: 52	Loss: 0.076539	Acc: 4.5% (450/10000)
[Test]  Epoch: 53	Loss: 0.076212	Acc: 4.2% (419/10000)
[Test]  Epoch: 54	Loss: 0.075894	Acc: 4.1% (409/10000)
[Test]  Epoch: 55	Loss: 0.075705	Acc: 4.5% (448/10000)
[Test]  Epoch: 56	Loss: 0.076661	Acc: 4.2% (425/10000)
[Test]  Epoch: 57	Loss: 0.075992	Acc: 4.5% (450/10000)
[Test]  Epoch: 58	Loss: 0.075959	Acc: 4.2% (415/10000)
[Test]  Epoch: 59	Loss: 0.075665	Acc: 4.5% (452/10000)
[Test]  Epoch: 60	Loss: 0.076485	Acc: 4.1% (409/10000)
[Test]  Epoch: 61	Loss: 0.075310	Acc: 4.5% (455/10000)
[Test]  Epoch: 62	Loss: 0.074998	Acc: 4.6% (457/10000)
[Test]  Epoch: 63	Loss: 0.075166	Acc: 4.8% (477/10000)
[Test]  Epoch: 64	Loss: 0.075069	Acc: 4.8% (477/10000)
[Test]  Epoch: 65	Loss: 0.075033	Acc: 4.7% (473/10000)
[Test]  Epoch: 66	Loss: 0.075112	Acc: 4.6% (463/10000)
[Test]  Epoch: 67	Loss: 0.075140	Acc: 4.6% (461/10000)
[Test]  Epoch: 68	Loss: 0.075006	Acc: 4.6% (460/10000)
[Test]  Epoch: 69	Loss: 0.075147	Acc: 4.6% (464/10000)
[Test]  Epoch: 70	Loss: 0.074808	Acc: 4.9% (492/10000)
[Test]  Epoch: 71	Loss: 0.074974	Acc: 4.7% (473/10000)
[Test]  Epoch: 72	Loss: 0.074879	Acc: 4.8% (478/10000)
[Test]  Epoch: 73	Loss: 0.075145	Acc: 4.8% (482/10000)
[Test]  Epoch: 74	Loss: 0.074861	Acc: 4.6% (461/10000)
[Test]  Epoch: 75	Loss: 0.075043	Acc: 4.5% (446/10000)
[Test]  Epoch: 76	Loss: 0.074884	Acc: 4.8% (482/10000)
[Test]  Epoch: 77	Loss: 0.074924	Acc: 4.7% (474/10000)
[Test]  Epoch: 78	Loss: 0.074951	Acc: 4.6% (464/10000)
[Test]  Epoch: 79	Loss: 0.074953	Acc: 5.0% (499/10000)
[Test]  Epoch: 80	Loss: 0.074616	Acc: 4.9% (491/10000)
[Test]  Epoch: 81	Loss: 0.074975	Acc: 4.9% (490/10000)
[Test]  Epoch: 82	Loss: 0.074835	Acc: 4.6% (457/10000)
[Test]  Epoch: 83	Loss: 0.074964	Acc: 5.0% (499/10000)
[Test]  Epoch: 84	Loss: 0.074979	Acc: 4.9% (487/10000)
[Test]  Epoch: 85	Loss: 0.074786	Acc: 5.1% (510/10000)
[Test]  Epoch: 86	Loss: 0.074872	Acc: 5.1% (506/10000)
[Test]  Epoch: 87	Loss: 0.075015	Acc: 4.9% (488/10000)
[Test]  Epoch: 88	Loss: 0.074615	Acc: 4.8% (482/10000)
[Test]  Epoch: 89	Loss: 0.074698	Acc: 4.7% (471/10000)
[Test]  Epoch: 90	Loss: 0.074720	Acc: 5.1% (508/10000)
[Test]  Epoch: 91	Loss: 0.074773	Acc: 5.2% (517/10000)
[Test]  Epoch: 92	Loss: 0.074753	Acc: 4.8% (481/10000)
[Test]  Epoch: 93	Loss: 0.074874	Acc: 5.0% (500/10000)
[Test]  Epoch: 94	Loss: 0.074826	Acc: 5.0% (504/10000)
[Test]  Epoch: 95	Loss: 0.074774	Acc: 4.9% (489/10000)
[Test]  Epoch: 96	Loss: 0.074927	Acc: 4.9% (488/10000)
[Test]  Epoch: 97	Loss: 0.074547	Acc: 5.2% (522/10000)
[Test]  Epoch: 98	Loss: 0.074939	Acc: 4.9% (489/10000)
[Test]  Epoch: 99	Loss: 0.074690	Acc: 5.1% (506/10000)
[Test]  Epoch: 100	Loss: 0.074849	Acc: 5.3% (529/10000)
===========finish==========
['2024-08-19', '03:10:10.318063', '100', 'test', '0.07484873492717743', '5.29', '5.29']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=0
get_sample_layers not_random
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039888	Acc: 43.8% (4384/10000)
[Test]  Epoch: 2	Loss: 0.039520	Acc: 44.2% (4424/10000)
[Test]  Epoch: 3	Loss: 0.039681	Acc: 43.9% (4391/10000)
[Test]  Epoch: 4	Loss: 0.039572	Acc: 44.1% (4409/10000)
[Test]  Epoch: 5	Loss: 0.039677	Acc: 43.8% (4384/10000)
[Test]  Epoch: 6	Loss: 0.039750	Acc: 44.0% (4399/10000)
[Test]  Epoch: 7	Loss: 0.039860	Acc: 43.8% (4380/10000)
[Test]  Epoch: 8	Loss: 0.039848	Acc: 44.0% (4396/10000)
[Test]  Epoch: 9	Loss: 0.039921	Acc: 43.8% (4382/10000)
[Test]  Epoch: 10	Loss: 0.039994	Acc: 43.7% (4372/10000)
[Test]  Epoch: 11	Loss: 0.040026	Acc: 43.7% (4366/10000)
[Test]  Epoch: 12	Loss: 0.039994	Acc: 43.7% (4374/10000)
[Test]  Epoch: 13	Loss: 0.040034	Acc: 43.5% (4353/10000)
[Test]  Epoch: 14	Loss: 0.039997	Acc: 43.6% (4363/10000)
[Test]  Epoch: 15	Loss: 0.040073	Acc: 43.7% (4367/10000)
[Test]  Epoch: 16	Loss: 0.040097	Acc: 43.8% (4382/10000)
[Test]  Epoch: 17	Loss: 0.040071	Acc: 43.6% (4365/10000)
[Test]  Epoch: 18	Loss: 0.040097	Acc: 43.8% (4375/10000)
[Test]  Epoch: 19	Loss: 0.040129	Acc: 43.8% (4376/10000)
[Test]  Epoch: 20	Loss: 0.040222	Acc: 43.5% (4351/10000)
[Test]  Epoch: 21	Loss: 0.040256	Acc: 43.3% (4332/10000)
[Test]  Epoch: 22	Loss: 0.040214	Acc: 43.6% (4365/10000)
[Test]  Epoch: 23	Loss: 0.040216	Acc: 43.5% (4347/10000)
[Test]  Epoch: 24	Loss: 0.040235	Acc: 43.6% (4361/10000)
[Test]  Epoch: 25	Loss: 0.040237	Acc: 43.5% (4353/10000)
[Test]  Epoch: 26	Loss: 0.040390	Acc: 43.2% (4321/10000)
[Test]  Epoch: 27	Loss: 0.040249	Acc: 43.6% (4365/10000)
[Test]  Epoch: 28	Loss: 0.040381	Acc: 43.5% (4354/10000)
[Test]  Epoch: 29	Loss: 0.040377	Acc: 43.3% (4331/10000)
[Test]  Epoch: 30	Loss: 0.040332	Acc: 43.6% (4364/10000)
[Test]  Epoch: 31	Loss: 0.040464	Acc: 43.5% (4347/10000)
[Test]  Epoch: 32	Loss: 0.040446	Acc: 43.4% (4342/10000)
[Test]  Epoch: 33	Loss: 0.040337	Acc: 43.6% (4365/10000)
[Test]  Epoch: 34	Loss: 0.040418	Acc: 43.4% (4341/10000)
[Test]  Epoch: 35	Loss: 0.040421	Acc: 43.5% (4355/10000)
[Test]  Epoch: 36	Loss: 0.040363	Acc: 43.5% (4355/10000)
[Test]  Epoch: 37	Loss: 0.040400	Acc: 43.5% (4355/10000)
[Test]  Epoch: 38	Loss: 0.040445	Acc: 43.5% (4355/10000)
[Test]  Epoch: 39	Loss: 0.040500	Acc: 43.2% (4324/10000)
[Test]  Epoch: 40	Loss: 0.040444	Acc: 43.4% (4343/10000)
[Test]  Epoch: 41	Loss: 0.040546	Acc: 43.4% (4341/10000)
[Test]  Epoch: 42	Loss: 0.040508	Acc: 43.5% (4351/10000)
[Test]  Epoch: 43	Loss: 0.040488	Acc: 43.5% (4350/10000)
[Test]  Epoch: 44	Loss: 0.040482	Acc: 43.5% (4348/10000)
[Test]  Epoch: 45	Loss: 0.040492	Acc: 43.7% (4366/10000)
[Test]  Epoch: 46	Loss: 0.040595	Acc: 43.3% (4331/10000)
[Test]  Epoch: 47	Loss: 0.040512	Acc: 43.5% (4351/10000)
[Test]  Epoch: 48	Loss: 0.040480	Acc: 43.5% (4346/10000)
[Test]  Epoch: 49	Loss: 0.040525	Acc: 43.5% (4350/10000)
[Test]  Epoch: 50	Loss: 0.040518	Acc: 43.5% (4352/10000)
[Test]  Epoch: 51	Loss: 0.040535	Acc: 43.6% (4363/10000)
[Test]  Epoch: 52	Loss: 0.040557	Acc: 43.5% (4348/10000)
[Test]  Epoch: 53	Loss: 0.040574	Acc: 43.5% (4348/10000)
[Test]  Epoch: 54	Loss: 0.040576	Acc: 43.6% (4363/10000)
[Test]  Epoch: 55	Loss: 0.040627	Acc: 43.7% (4366/10000)
[Test]  Epoch: 56	Loss: 0.040625	Acc: 43.5% (4349/10000)
[Test]  Epoch: 57	Loss: 0.040648	Acc: 43.6% (4359/10000)
[Test]  Epoch: 58	Loss: 0.040573	Acc: 43.5% (4345/10000)
[Test]  Epoch: 59	Loss: 0.040635	Acc: 43.5% (4348/10000)
[Test]  Epoch: 60	Loss: 0.040723	Acc: 43.4% (4344/10000)
[Test]  Epoch: 61	Loss: 0.040748	Acc: 43.3% (4328/10000)
[Test]  Epoch: 62	Loss: 0.040685	Acc: 43.4% (4343/10000)
[Test]  Epoch: 63	Loss: 0.040641	Acc: 43.4% (4343/10000)
[Test]  Epoch: 64	Loss: 0.040605	Acc: 43.5% (4353/10000)
[Test]  Epoch: 65	Loss: 0.040657	Acc: 43.4% (4341/10000)
[Test]  Epoch: 66	Loss: 0.040667	Acc: 43.5% (4350/10000)
[Test]  Epoch: 67	Loss: 0.040688	Acc: 43.7% (4369/10000)
[Test]  Epoch: 68	Loss: 0.040717	Acc: 43.4% (4337/10000)
[Test]  Epoch: 69	Loss: 0.040689	Acc: 43.5% (4345/10000)
[Test]  Epoch: 70	Loss: 0.040647	Acc: 43.4% (4342/10000)
[Test]  Epoch: 71	Loss: 0.040662	Acc: 43.5% (4346/10000)
[Test]  Epoch: 72	Loss: 0.040676	Acc: 43.3% (4332/10000)
[Test]  Epoch: 73	Loss: 0.040650	Acc: 43.5% (4350/10000)
[Test]  Epoch: 74	Loss: 0.040644	Acc: 43.6% (4359/10000)
[Test]  Epoch: 75	Loss: 0.040706	Acc: 43.5% (4355/10000)
[Test]  Epoch: 76	Loss: 0.040637	Acc: 43.6% (4361/10000)
[Test]  Epoch: 77	Loss: 0.040635	Acc: 43.6% (4357/10000)
[Test]  Epoch: 78	Loss: 0.040666	Acc: 43.6% (4357/10000)
[Test]  Epoch: 79	Loss: 0.040682	Acc: 43.6% (4361/10000)
[Test]  Epoch: 80	Loss: 0.040623	Acc: 43.7% (4367/10000)
[Test]  Epoch: 81	Loss: 0.040632	Acc: 43.7% (4369/10000)
[Test]  Epoch: 82	Loss: 0.040682	Acc: 43.6% (4365/10000)
[Test]  Epoch: 83	Loss: 0.040722	Acc: 43.5% (4354/10000)
[Test]  Epoch: 84	Loss: 0.040732	Acc: 43.5% (4345/10000)
[Test]  Epoch: 85	Loss: 0.040703	Acc: 43.5% (4355/10000)
[Test]  Epoch: 86	Loss: 0.040663	Acc: 43.6% (4360/10000)
[Test]  Epoch: 87	Loss: 0.040634	Acc: 43.5% (4346/10000)
[Test]  Epoch: 88	Loss: 0.040732	Acc: 43.5% (4345/10000)
[Test]  Epoch: 89	Loss: 0.040670	Acc: 43.5% (4355/10000)
[Test]  Epoch: 90	Loss: 0.040672	Acc: 43.6% (4357/10000)
[Test]  Epoch: 91	Loss: 0.040690	Acc: 43.7% (4366/10000)
[Test]  Epoch: 92	Loss: 0.040635	Acc: 43.6% (4363/10000)
[Test]  Epoch: 93	Loss: 0.040690	Acc: 43.5% (4352/10000)
[Test]  Epoch: 94	Loss: 0.040665	Acc: 43.7% (4367/10000)
[Test]  Epoch: 95	Loss: 0.040651	Acc: 43.5% (4354/10000)
[Test]  Epoch: 96	Loss: 0.040667	Acc: 43.5% (4355/10000)
[Test]  Epoch: 97	Loss: 0.040675	Acc: 43.5% (4353/10000)
[Test]  Epoch: 98	Loss: 0.040652	Acc: 43.5% (4350/10000)
[Test]  Epoch: 99	Loss: 0.040672	Acc: 43.5% (4355/10000)
[Test]  Epoch: 100	Loss: 0.040713	Acc: 43.4% (4336/10000)
===========finish==========
['2024-08-19', '03:14:53.528235', '100', 'test', '0.0407128211915493', '43.36', '44.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.058556	Acc: 19.2% (1917/10000)
[Test]  Epoch: 2	Loss: 0.046019	Acc: 34.0% (3398/10000)
[Test]  Epoch: 3	Loss: 0.044591	Acc: 36.7% (3671/10000)
[Test]  Epoch: 4	Loss: 0.043832	Acc: 37.9% (3794/10000)
[Test]  Epoch: 5	Loss: 0.044042	Acc: 37.9% (3789/10000)
[Test]  Epoch: 6	Loss: 0.043570	Acc: 38.9% (3889/10000)
[Test]  Epoch: 7	Loss: 0.043379	Acc: 39.0% (3902/10000)
[Test]  Epoch: 8	Loss: 0.043643	Acc: 38.8% (3875/10000)
[Test]  Epoch: 9	Loss: 0.043403	Acc: 39.4% (3937/10000)
[Test]  Epoch: 10	Loss: 0.043557	Acc: 39.2% (3921/10000)
[Test]  Epoch: 11	Loss: 0.043548	Acc: 39.2% (3918/10000)
[Test]  Epoch: 12	Loss: 0.043551	Acc: 39.5% (3948/10000)
[Test]  Epoch: 13	Loss: 0.043364	Acc: 39.7% (3967/10000)
[Test]  Epoch: 14	Loss: 0.043293	Acc: 39.9% (3991/10000)
[Test]  Epoch: 15	Loss: 0.043355	Acc: 40.2% (4022/10000)
[Test]  Epoch: 16	Loss: 0.043301	Acc: 39.8% (3980/10000)
[Test]  Epoch: 17	Loss: 0.043271	Acc: 40.0% (4005/10000)
[Test]  Epoch: 18	Loss: 0.043189	Acc: 40.3% (4026/10000)
[Test]  Epoch: 19	Loss: 0.043331	Acc: 40.2% (4016/10000)
[Test]  Epoch: 20	Loss: 0.043383	Acc: 40.1% (4015/10000)
[Test]  Epoch: 21	Loss: 0.043143	Acc: 40.1% (4015/10000)
[Test]  Epoch: 22	Loss: 0.043632	Acc: 39.8% (3975/10000)
[Test]  Epoch: 23	Loss: 0.043305	Acc: 40.2% (4021/10000)
[Test]  Epoch: 24	Loss: 0.043263	Acc: 40.4% (4040/10000)
[Test]  Epoch: 25	Loss: 0.043147	Acc: 40.4% (4042/10000)
[Test]  Epoch: 26	Loss: 0.043374	Acc: 40.1% (4011/10000)
[Test]  Epoch: 27	Loss: 0.043185	Acc: 40.2% (4024/10000)
[Test]  Epoch: 28	Loss: 0.043243	Acc: 40.4% (4035/10000)
[Test]  Epoch: 29	Loss: 0.043277	Acc: 40.4% (4037/10000)
[Test]  Epoch: 30	Loss: 0.043344	Acc: 40.5% (4052/10000)
[Test]  Epoch: 31	Loss: 0.043269	Acc: 40.2% (4022/10000)
[Test]  Epoch: 32	Loss: 0.043130	Acc: 40.5% (4053/10000)
[Test]  Epoch: 33	Loss: 0.043311	Acc: 40.2% (4022/10000)
[Test]  Epoch: 34	Loss: 0.043358	Acc: 40.6% (4060/10000)
[Test]  Epoch: 35	Loss: 0.043235	Acc: 40.1% (4015/10000)
[Test]  Epoch: 36	Loss: 0.043139	Acc: 40.9% (4085/10000)
[Test]  Epoch: 37	Loss: 0.043299	Acc: 40.6% (4057/10000)
[Test]  Epoch: 38	Loss: 0.043198	Acc: 40.7% (4066/10000)
[Test]  Epoch: 39	Loss: 0.043477	Acc: 40.1% (4009/10000)
[Test]  Epoch: 40	Loss: 0.043460	Acc: 40.5% (4046/10000)
[Test]  Epoch: 41	Loss: 0.043354	Acc: 40.5% (4050/10000)
[Test]  Epoch: 42	Loss: 0.043261	Acc: 40.5% (4046/10000)
[Test]  Epoch: 43	Loss: 0.043278	Acc: 40.5% (4051/10000)
[Test]  Epoch: 44	Loss: 0.043322	Acc: 40.4% (4038/10000)
[Test]  Epoch: 45	Loss: 0.043312	Acc: 40.2% (4017/10000)
[Test]  Epoch: 46	Loss: 0.043242	Acc: 40.5% (4051/10000)
[Test]  Epoch: 47	Loss: 0.043456	Acc: 40.4% (4043/10000)
[Test]  Epoch: 48	Loss: 0.043429	Acc: 40.8% (4075/10000)
[Test]  Epoch: 49	Loss: 0.043447	Acc: 40.6% (4057/10000)
[Test]  Epoch: 50	Loss: 0.043331	Acc: 40.5% (4052/10000)
[Test]  Epoch: 51	Loss: 0.043359	Acc: 40.6% (4061/10000)
[Test]  Epoch: 52	Loss: 0.043289	Acc: 40.7% (4073/10000)
[Test]  Epoch: 53	Loss: 0.043472	Acc: 40.5% (4049/10000)
[Test]  Epoch: 54	Loss: 0.043461	Acc: 40.8% (4084/10000)
[Test]  Epoch: 55	Loss: 0.043328	Acc: 40.7% (4074/10000)
[Test]  Epoch: 56	Loss: 0.043496	Acc: 40.6% (4060/10000)
[Test]  Epoch: 57	Loss: 0.043302	Acc: 40.5% (4049/10000)
[Test]  Epoch: 58	Loss: 0.043376	Acc: 40.7% (4074/10000)
[Test]  Epoch: 59	Loss: 0.043428	Acc: 40.8% (4081/10000)
[Test]  Epoch: 60	Loss: 0.043502	Acc: 40.5% (4052/10000)
[Test]  Epoch: 61	Loss: 0.043540	Acc: 40.5% (4047/10000)
[Test]  Epoch: 62	Loss: 0.043506	Acc: 40.4% (4044/10000)
[Test]  Epoch: 63	Loss: 0.043457	Acc: 40.6% (4061/10000)
[Test]  Epoch: 64	Loss: 0.043434	Acc: 40.7% (4068/10000)
[Test]  Epoch: 65	Loss: 0.043477	Acc: 40.7% (4067/10000)
[Test]  Epoch: 66	Loss: 0.043485	Acc: 40.7% (4067/10000)
[Test]  Epoch: 67	Loss: 0.043508	Acc: 40.7% (4073/10000)
[Test]  Epoch: 68	Loss: 0.043552	Acc: 40.5% (4048/10000)
[Test]  Epoch: 69	Loss: 0.043504	Acc: 40.7% (4068/10000)
[Test]  Epoch: 70	Loss: 0.043478	Acc: 40.8% (4075/10000)
[Test]  Epoch: 71	Loss: 0.043490	Acc: 40.8% (4081/10000)
[Test]  Epoch: 72	Loss: 0.043522	Acc: 40.6% (4065/10000)
[Test]  Epoch: 73	Loss: 0.043455	Acc: 41.0% (4099/10000)
[Test]  Epoch: 74	Loss: 0.043448	Acc: 40.9% (4087/10000)
[Test]  Epoch: 75	Loss: 0.043513	Acc: 40.7% (4068/10000)
[Test]  Epoch: 76	Loss: 0.043446	Acc: 40.9% (4091/10000)
[Test]  Epoch: 77	Loss: 0.043444	Acc: 40.9% (4093/10000)
[Test]  Epoch: 78	Loss: 0.043488	Acc: 40.6% (4056/10000)
[Test]  Epoch: 79	Loss: 0.043504	Acc: 40.8% (4078/10000)
[Test]  Epoch: 80	Loss: 0.043441	Acc: 40.6% (4063/10000)
[Test]  Epoch: 81	Loss: 0.043447	Acc: 40.6% (4058/10000)
[Test]  Epoch: 82	Loss: 0.043448	Acc: 40.6% (4059/10000)
[Test]  Epoch: 83	Loss: 0.043519	Acc: 40.5% (4046/10000)
[Test]  Epoch: 84	Loss: 0.043541	Acc: 40.7% (4071/10000)
[Test]  Epoch: 85	Loss: 0.043505	Acc: 40.6% (4065/10000)
[Test]  Epoch: 86	Loss: 0.043498	Acc: 40.6% (4062/10000)
[Test]  Epoch: 87	Loss: 0.043467	Acc: 40.7% (4069/10000)
[Test]  Epoch: 88	Loss: 0.043523	Acc: 40.7% (4066/10000)
[Test]  Epoch: 89	Loss: 0.043453	Acc: 40.6% (4064/10000)
[Test]  Epoch: 90	Loss: 0.043476	Acc: 40.8% (4083/10000)
[Test]  Epoch: 91	Loss: 0.043470	Acc: 40.8% (4078/10000)
[Test]  Epoch: 92	Loss: 0.043473	Acc: 40.9% (4091/10000)
[Test]  Epoch: 93	Loss: 0.043519	Acc: 40.7% (4071/10000)
[Test]  Epoch: 94	Loss: 0.043479	Acc: 40.6% (4065/10000)
[Test]  Epoch: 95	Loss: 0.043406	Acc: 40.8% (4077/10000)
[Test]  Epoch: 96	Loss: 0.043433	Acc: 40.7% (4072/10000)
[Test]  Epoch: 97	Loss: 0.043472	Acc: 40.7% (4068/10000)
[Test]  Epoch: 98	Loss: 0.043440	Acc: 40.8% (4083/10000)
[Test]  Epoch: 99	Loss: 0.043456	Acc: 40.8% (4075/10000)
[Test]  Epoch: 100	Loss: 0.043502	Acc: 40.7% (4073/10000)
===========finish==========
['2024-08-19', '03:18:44.274892', '100', 'test', '0.04350238270163536', '40.73', '40.99']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.052557	Acc: 24.7% (2471/10000)
[Test]  Epoch: 2	Loss: 0.047130	Acc: 31.4% (3139/10000)
[Test]  Epoch: 3	Loss: 0.045566	Acc: 34.4% (3439/10000)
[Test]  Epoch: 4	Loss: 0.044927	Acc: 36.1% (3606/10000)
[Test]  Epoch: 5	Loss: 0.045312	Acc: 35.8% (3577/10000)
[Test]  Epoch: 6	Loss: 0.044845	Acc: 36.4% (3644/10000)
[Test]  Epoch: 7	Loss: 0.044767	Acc: 36.8% (3682/10000)
[Test]  Epoch: 8	Loss: 0.044630	Acc: 37.1% (3713/10000)
[Test]  Epoch: 9	Loss: 0.044788	Acc: 37.1% (3706/10000)
[Test]  Epoch: 10	Loss: 0.044657	Acc: 37.5% (3745/10000)
[Test]  Epoch: 11	Loss: 0.044859	Acc: 37.4% (3740/10000)
[Test]  Epoch: 12	Loss: 0.044677	Acc: 37.6% (3764/10000)
[Test]  Epoch: 13	Loss: 0.044481	Acc: 38.0% (3801/10000)
[Test]  Epoch: 14	Loss: 0.044419	Acc: 37.8% (3777/10000)
[Test]  Epoch: 15	Loss: 0.044546	Acc: 38.0% (3805/10000)
[Test]  Epoch: 16	Loss: 0.044535	Acc: 37.9% (3786/10000)
[Test]  Epoch: 17	Loss: 0.044311	Acc: 38.3% (3833/10000)
[Test]  Epoch: 18	Loss: 0.044352	Acc: 38.4% (3838/10000)
[Test]  Epoch: 19	Loss: 0.044578	Acc: 38.2% (3819/10000)
[Test]  Epoch: 20	Loss: 0.044480	Acc: 38.1% (3813/10000)
[Test]  Epoch: 21	Loss: 0.044304	Acc: 38.4% (3839/10000)
[Test]  Epoch: 22	Loss: 0.044724	Acc: 37.9% (3792/10000)
[Test]  Epoch: 23	Loss: 0.044356	Acc: 38.4% (3841/10000)
[Test]  Epoch: 24	Loss: 0.044315	Acc: 38.6% (3860/10000)
[Test]  Epoch: 25	Loss: 0.044382	Acc: 38.7% (3868/10000)
[Test]  Epoch: 26	Loss: 0.044488	Acc: 38.4% (3839/10000)
[Test]  Epoch: 27	Loss: 0.044202	Acc: 38.5% (3854/10000)
[Test]  Epoch: 28	Loss: 0.044485	Acc: 38.2% (3816/10000)
[Test]  Epoch: 29	Loss: 0.044412	Acc: 38.5% (3852/10000)
[Test]  Epoch: 30	Loss: 0.044445	Acc: 38.7% (3868/10000)
[Test]  Epoch: 31	Loss: 0.044535	Acc: 38.4% (3840/10000)
[Test]  Epoch: 32	Loss: 0.044315	Acc: 38.8% (3880/10000)
[Test]  Epoch: 33	Loss: 0.044393	Acc: 38.5% (3848/10000)
[Test]  Epoch: 34	Loss: 0.044533	Acc: 38.6% (3862/10000)
[Test]  Epoch: 35	Loss: 0.044359	Acc: 38.9% (3888/10000)
[Test]  Epoch: 36	Loss: 0.044285	Acc: 38.7% (3873/10000)
[Test]  Epoch: 37	Loss: 0.044427	Acc: 38.8% (3875/10000)
[Test]  Epoch: 38	Loss: 0.044371	Acc: 38.9% (3894/10000)
[Test]  Epoch: 39	Loss: 0.044486	Acc: 38.6% (3857/10000)
[Test]  Epoch: 40	Loss: 0.044520	Acc: 38.7% (3874/10000)
[Test]  Epoch: 41	Loss: 0.044466	Acc: 38.8% (3884/10000)
[Test]  Epoch: 42	Loss: 0.044426	Acc: 39.0% (3904/10000)
[Test]  Epoch: 43	Loss: 0.044306	Acc: 38.9% (3887/10000)
[Test]  Epoch: 44	Loss: 0.044471	Acc: 38.9% (3890/10000)
[Test]  Epoch: 45	Loss: 0.044396	Acc: 38.7% (3868/10000)
[Test]  Epoch: 46	Loss: 0.044452	Acc: 38.7% (3867/10000)
[Test]  Epoch: 47	Loss: 0.044485	Acc: 38.9% (3885/10000)
[Test]  Epoch: 48	Loss: 0.044617	Acc: 38.9% (3890/10000)
[Test]  Epoch: 49	Loss: 0.044582	Acc: 39.1% (3912/10000)
[Test]  Epoch: 50	Loss: 0.044357	Acc: 39.1% (3907/10000)
[Test]  Epoch: 51	Loss: 0.044410	Acc: 39.2% (3919/10000)
[Test]  Epoch: 52	Loss: 0.044515	Acc: 38.5% (3852/10000)
[Test]  Epoch: 53	Loss: 0.044592	Acc: 38.7% (3868/10000)
[Test]  Epoch: 54	Loss: 0.044489	Acc: 39.1% (3912/10000)
[Test]  Epoch: 55	Loss: 0.044443	Acc: 39.0% (3903/10000)
[Test]  Epoch: 56	Loss: 0.044521	Acc: 39.0% (3899/10000)
[Test]  Epoch: 57	Loss: 0.044400	Acc: 39.0% (3902/10000)
[Test]  Epoch: 58	Loss: 0.044498	Acc: 39.0% (3904/10000)
[Test]  Epoch: 59	Loss: 0.044578	Acc: 39.0% (3898/10000)
[Test]  Epoch: 60	Loss: 0.044571	Acc: 38.9% (3892/10000)
[Test]  Epoch: 61	Loss: 0.044640	Acc: 38.9% (3891/10000)
[Test]  Epoch: 62	Loss: 0.044592	Acc: 38.9% (3885/10000)
[Test]  Epoch: 63	Loss: 0.044514	Acc: 39.0% (3895/10000)
[Test]  Epoch: 64	Loss: 0.044493	Acc: 39.2% (3916/10000)
[Test]  Epoch: 65	Loss: 0.044551	Acc: 38.9% (3887/10000)
[Test]  Epoch: 66	Loss: 0.044553	Acc: 38.9% (3885/10000)
[Test]  Epoch: 67	Loss: 0.044643	Acc: 38.9% (3888/10000)
[Test]  Epoch: 68	Loss: 0.044666	Acc: 38.9% (3889/10000)
[Test]  Epoch: 69	Loss: 0.044613	Acc: 38.8% (3877/10000)
[Test]  Epoch: 70	Loss: 0.044568	Acc: 39.0% (3902/10000)
[Test]  Epoch: 71	Loss: 0.044578	Acc: 39.0% (3901/10000)
[Test]  Epoch: 72	Loss: 0.044597	Acc: 38.9% (3888/10000)
[Test]  Epoch: 73	Loss: 0.044554	Acc: 39.0% (3895/10000)
[Test]  Epoch: 74	Loss: 0.044542	Acc: 39.1% (3909/10000)
[Test]  Epoch: 75	Loss: 0.044597	Acc: 39.0% (3899/10000)
[Test]  Epoch: 76	Loss: 0.044529	Acc: 39.1% (3915/10000)
[Test]  Epoch: 77	Loss: 0.044537	Acc: 39.1% (3906/10000)
[Test]  Epoch: 78	Loss: 0.044576	Acc: 38.9% (3894/10000)
[Test]  Epoch: 79	Loss: 0.044591	Acc: 39.0% (3897/10000)
[Test]  Epoch: 80	Loss: 0.044530	Acc: 39.1% (3909/10000)
[Test]  Epoch: 81	Loss: 0.044551	Acc: 38.9% (3893/10000)
[Test]  Epoch: 82	Loss: 0.044531	Acc: 38.9% (3891/10000)
[Test]  Epoch: 83	Loss: 0.044580	Acc: 38.8% (3879/10000)
[Test]  Epoch: 84	Loss: 0.044611	Acc: 38.9% (3889/10000)
[Test]  Epoch: 85	Loss: 0.044600	Acc: 38.8% (3882/10000)
[Test]  Epoch: 86	Loss: 0.044553	Acc: 38.9% (3888/10000)
[Test]  Epoch: 87	Loss: 0.044548	Acc: 39.0% (3905/10000)
[Test]  Epoch: 88	Loss: 0.044600	Acc: 38.9% (3893/10000)
[Test]  Epoch: 89	Loss: 0.044540	Acc: 38.9% (3889/10000)
[Test]  Epoch: 90	Loss: 0.044564	Acc: 38.9% (3888/10000)
[Test]  Epoch: 91	Loss: 0.044573	Acc: 39.1% (3910/10000)
[Test]  Epoch: 92	Loss: 0.044525	Acc: 39.0% (3900/10000)
[Test]  Epoch: 93	Loss: 0.044579	Acc: 38.9% (3892/10000)
[Test]  Epoch: 94	Loss: 0.044583	Acc: 38.9% (3893/10000)
[Test]  Epoch: 95	Loss: 0.044541	Acc: 39.1% (3914/10000)
[Test]  Epoch: 96	Loss: 0.044522	Acc: 39.1% (3911/10000)
[Test]  Epoch: 97	Loss: 0.044564	Acc: 38.8% (3881/10000)
[Test]  Epoch: 98	Loss: 0.044543	Acc: 39.0% (3904/10000)
[Test]  Epoch: 99	Loss: 0.044561	Acc: 38.9% (3891/10000)
[Test]  Epoch: 100	Loss: 0.044580	Acc: 38.9% (3890/10000)
===========finish==========
['2024-08-19', '03:22:45.106136', '100', 'test', '0.04457979283332825', '38.9', '39.19']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.057194	Acc: 20.0% (2000/10000)
[Test]  Epoch: 2	Loss: 0.049172	Acc: 29.0% (2903/10000)
[Test]  Epoch: 3	Loss: 0.047713	Acc: 31.8% (3177/10000)
[Test]  Epoch: 4	Loss: 0.047011	Acc: 33.0% (3302/10000)
[Test]  Epoch: 5	Loss: 0.047068	Acc: 33.6% (3358/10000)
[Test]  Epoch: 6	Loss: 0.046610	Acc: 34.0% (3396/10000)
[Test]  Epoch: 7	Loss: 0.046350	Acc: 34.5% (3447/10000)
[Test]  Epoch: 8	Loss: 0.046193	Acc: 34.6% (3465/10000)
[Test]  Epoch: 9	Loss: 0.046353	Acc: 35.0% (3502/10000)
[Test]  Epoch: 10	Loss: 0.046129	Acc: 35.0% (3495/10000)
[Test]  Epoch: 11	Loss: 0.046280	Acc: 34.5% (3454/10000)
[Test]  Epoch: 12	Loss: 0.046100	Acc: 35.4% (3539/10000)
[Test]  Epoch: 13	Loss: 0.045988	Acc: 35.5% (3545/10000)
[Test]  Epoch: 14	Loss: 0.045760	Acc: 35.8% (3580/10000)
[Test]  Epoch: 15	Loss: 0.045933	Acc: 35.9% (3591/10000)
[Test]  Epoch: 16	Loss: 0.045884	Acc: 35.7% (3571/10000)
[Test]  Epoch: 17	Loss: 0.045676	Acc: 35.7% (3572/10000)
[Test]  Epoch: 18	Loss: 0.045734	Acc: 35.9% (3588/10000)
[Test]  Epoch: 19	Loss: 0.045901	Acc: 35.8% (3581/10000)
[Test]  Epoch: 20	Loss: 0.045732	Acc: 36.2% (3618/10000)
[Test]  Epoch: 21	Loss: 0.045656	Acc: 36.4% (3639/10000)
[Test]  Epoch: 22	Loss: 0.045916	Acc: 36.3% (3626/10000)
[Test]  Epoch: 23	Loss: 0.045612	Acc: 36.2% (3621/10000)
[Test]  Epoch: 24	Loss: 0.045622	Acc: 36.5% (3655/10000)
[Test]  Epoch: 25	Loss: 0.045533	Acc: 36.5% (3652/10000)
[Test]  Epoch: 26	Loss: 0.045721	Acc: 36.6% (3661/10000)
[Test]  Epoch: 27	Loss: 0.045375	Acc: 36.9% (3685/10000)
[Test]  Epoch: 28	Loss: 0.045652	Acc: 36.5% (3648/10000)
[Test]  Epoch: 29	Loss: 0.045576	Acc: 36.5% (3647/10000)
[Test]  Epoch: 30	Loss: 0.045676	Acc: 36.8% (3680/10000)
[Test]  Epoch: 31	Loss: 0.045697	Acc: 36.7% (3671/10000)
[Test]  Epoch: 32	Loss: 0.045529	Acc: 36.6% (3661/10000)
[Test]  Epoch: 33	Loss: 0.045499	Acc: 36.8% (3683/10000)
[Test]  Epoch: 34	Loss: 0.045574	Acc: 36.8% (3681/10000)
[Test]  Epoch: 35	Loss: 0.045473	Acc: 36.9% (3685/10000)
[Test]  Epoch: 36	Loss: 0.045387	Acc: 37.2% (3720/10000)
[Test]  Epoch: 37	Loss: 0.045453	Acc: 37.2% (3725/10000)
[Test]  Epoch: 38	Loss: 0.045517	Acc: 37.3% (3726/10000)
[Test]  Epoch: 39	Loss: 0.045599	Acc: 36.9% (3685/10000)
[Test]  Epoch: 40	Loss: 0.045565	Acc: 37.3% (3727/10000)
[Test]  Epoch: 41	Loss: 0.045516	Acc: 37.0% (3699/10000)
[Test]  Epoch: 42	Loss: 0.045518	Acc: 37.5% (3753/10000)
[Test]  Epoch: 43	Loss: 0.045321	Acc: 37.5% (3753/10000)
[Test]  Epoch: 44	Loss: 0.045483	Acc: 37.4% (3741/10000)
[Test]  Epoch: 45	Loss: 0.045452	Acc: 37.2% (3719/10000)
[Test]  Epoch: 46	Loss: 0.045435	Acc: 37.2% (3720/10000)
[Test]  Epoch: 47	Loss: 0.045584	Acc: 37.4% (3736/10000)
[Test]  Epoch: 48	Loss: 0.045635	Acc: 37.6% (3764/10000)
[Test]  Epoch: 49	Loss: 0.045573	Acc: 37.4% (3738/10000)
[Test]  Epoch: 50	Loss: 0.045316	Acc: 37.7% (3772/10000)
[Test]  Epoch: 51	Loss: 0.045430	Acc: 37.6% (3759/10000)
[Test]  Epoch: 52	Loss: 0.045501	Acc: 37.5% (3748/10000)
[Test]  Epoch: 53	Loss: 0.045581	Acc: 37.4% (3744/10000)
[Test]  Epoch: 54	Loss: 0.045494	Acc: 37.9% (3786/10000)
[Test]  Epoch: 55	Loss: 0.045429	Acc: 37.6% (3759/10000)
[Test]  Epoch: 56	Loss: 0.045578	Acc: 37.6% (3759/10000)
[Test]  Epoch: 57	Loss: 0.045447	Acc: 37.3% (3727/10000)
[Test]  Epoch: 58	Loss: 0.045467	Acc: 37.9% (3786/10000)
[Test]  Epoch: 59	Loss: 0.045520	Acc: 37.6% (3761/10000)
[Test]  Epoch: 60	Loss: 0.045549	Acc: 37.6% (3758/10000)
[Test]  Epoch: 61	Loss: 0.045590	Acc: 37.5% (3749/10000)
[Test]  Epoch: 62	Loss: 0.045555	Acc: 37.6% (3764/10000)
[Test]  Epoch: 63	Loss: 0.045461	Acc: 37.6% (3765/10000)
[Test]  Epoch: 64	Loss: 0.045461	Acc: 37.6% (3759/10000)
[Test]  Epoch: 65	Loss: 0.045527	Acc: 37.5% (3755/10000)
[Test]  Epoch: 66	Loss: 0.045498	Acc: 37.7% (3770/10000)
[Test]  Epoch: 67	Loss: 0.045570	Acc: 37.7% (3770/10000)
[Test]  Epoch: 68	Loss: 0.045604	Acc: 37.5% (3751/10000)
[Test]  Epoch: 69	Loss: 0.045581	Acc: 37.5% (3751/10000)
[Test]  Epoch: 70	Loss: 0.045536	Acc: 37.8% (3782/10000)
[Test]  Epoch: 71	Loss: 0.045531	Acc: 37.6% (3765/10000)
[Test]  Epoch: 72	Loss: 0.045545	Acc: 37.6% (3761/10000)
[Test]  Epoch: 73	Loss: 0.045519	Acc: 37.7% (3772/10000)
[Test]  Epoch: 74	Loss: 0.045490	Acc: 37.7% (3773/10000)
[Test]  Epoch: 75	Loss: 0.045543	Acc: 37.7% (3767/10000)
[Test]  Epoch: 76	Loss: 0.045489	Acc: 37.9% (3785/10000)
[Test]  Epoch: 77	Loss: 0.045485	Acc: 37.8% (3781/10000)
[Test]  Epoch: 78	Loss: 0.045554	Acc: 37.5% (3755/10000)
[Test]  Epoch: 79	Loss: 0.045564	Acc: 37.7% (3773/10000)
[Test]  Epoch: 80	Loss: 0.045484	Acc: 37.8% (3784/10000)
[Test]  Epoch: 81	Loss: 0.045474	Acc: 37.6% (3765/10000)
[Test]  Epoch: 82	Loss: 0.045461	Acc: 37.5% (3753/10000)
[Test]  Epoch: 83	Loss: 0.045533	Acc: 37.5% (3749/10000)
[Test]  Epoch: 84	Loss: 0.045560	Acc: 37.8% (3775/10000)
[Test]  Epoch: 85	Loss: 0.045543	Acc: 37.7% (3768/10000)
[Test]  Epoch: 86	Loss: 0.045506	Acc: 37.6% (3759/10000)
[Test]  Epoch: 87	Loss: 0.045510	Acc: 37.8% (3781/10000)
[Test]  Epoch: 88	Loss: 0.045570	Acc: 37.7% (3771/10000)
[Test]  Epoch: 89	Loss: 0.045523	Acc: 37.6% (3761/10000)
[Test]  Epoch: 90	Loss: 0.045522	Acc: 37.7% (3773/10000)
[Test]  Epoch: 91	Loss: 0.045506	Acc: 37.9% (3790/10000)
[Test]  Epoch: 92	Loss: 0.045453	Acc: 37.9% (3791/10000)
[Test]  Epoch: 93	Loss: 0.045509	Acc: 37.7% (3772/10000)
[Test]  Epoch: 94	Loss: 0.045522	Acc: 37.6% (3759/10000)
[Test]  Epoch: 95	Loss: 0.045518	Acc: 37.7% (3771/10000)
[Test]  Epoch: 96	Loss: 0.045480	Acc: 37.9% (3789/10000)
[Test]  Epoch: 97	Loss: 0.045497	Acc: 37.7% (3766/10000)
[Test]  Epoch: 98	Loss: 0.045486	Acc: 37.7% (3774/10000)
[Test]  Epoch: 99	Loss: 0.045507	Acc: 37.9% (3788/10000)
[Test]  Epoch: 100	Loss: 0.045553	Acc: 37.7% (3770/10000)
===========finish==========
['2024-08-19', '03:26:36.291653', '100', 'test', '0.045552931654453274', '37.7', '37.91']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer2.0.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.063849	Acc: 14.1% (1409/10000)
[Test]  Epoch: 2	Loss: 0.051799	Acc: 25.6% (2559/10000)
[Test]  Epoch: 3	Loss: 0.050036	Acc: 28.8% (2877/10000)
[Test]  Epoch: 4	Loss: 0.049336	Acc: 30.0% (2997/10000)
[Test]  Epoch: 5	Loss: 0.049276	Acc: 30.5% (3048/10000)
[Test]  Epoch: 6	Loss: 0.048978	Acc: 30.7% (3067/10000)
[Test]  Epoch: 7	Loss: 0.048483	Acc: 31.7% (3171/10000)
[Test]  Epoch: 8	Loss: 0.048260	Acc: 31.9% (3188/10000)
[Test]  Epoch: 9	Loss: 0.048261	Acc: 32.3% (3228/10000)
[Test]  Epoch: 10	Loss: 0.048233	Acc: 32.0% (3202/10000)
[Test]  Epoch: 11	Loss: 0.048230	Acc: 32.8% (3279/10000)
[Test]  Epoch: 12	Loss: 0.048074	Acc: 32.9% (3286/10000)
[Test]  Epoch: 13	Loss: 0.047901	Acc: 32.9% (3290/10000)
[Test]  Epoch: 14	Loss: 0.047810	Acc: 33.4% (3343/10000)
[Test]  Epoch: 15	Loss: 0.047981	Acc: 33.3% (3327/10000)
[Test]  Epoch: 16	Loss: 0.047878	Acc: 33.2% (3318/10000)
[Test]  Epoch: 17	Loss: 0.047674	Acc: 33.9% (3389/10000)
[Test]  Epoch: 18	Loss: 0.047696	Acc: 33.5% (3354/10000)
[Test]  Epoch: 19	Loss: 0.047749	Acc: 33.7% (3370/10000)
[Test]  Epoch: 20	Loss: 0.047799	Acc: 33.8% (3378/10000)
[Test]  Epoch: 21	Loss: 0.047506	Acc: 33.7% (3373/10000)
[Test]  Epoch: 22	Loss: 0.047916	Acc: 33.6% (3358/10000)
[Test]  Epoch: 23	Loss: 0.047540	Acc: 34.2% (3420/10000)
[Test]  Epoch: 24	Loss: 0.047496	Acc: 34.0% (3403/10000)
[Test]  Epoch: 25	Loss: 0.047450	Acc: 34.3% (3427/10000)
[Test]  Epoch: 26	Loss: 0.047634	Acc: 34.1% (3407/10000)
[Test]  Epoch: 27	Loss: 0.047275	Acc: 34.4% (3436/10000)
[Test]  Epoch: 28	Loss: 0.047484	Acc: 34.1% (3407/10000)
[Test]  Epoch: 29	Loss: 0.047482	Acc: 34.3% (3427/10000)
[Test]  Epoch: 30	Loss: 0.047601	Acc: 34.2% (3418/10000)
[Test]  Epoch: 31	Loss: 0.047509	Acc: 34.1% (3409/10000)
[Test]  Epoch: 32	Loss: 0.047401	Acc: 34.5% (3449/10000)
[Test]  Epoch: 33	Loss: 0.047474	Acc: 34.3% (3426/10000)
[Test]  Epoch: 34	Loss: 0.047567	Acc: 34.0% (3396/10000)
[Test]  Epoch: 35	Loss: 0.047327	Acc: 34.1% (3410/10000)
[Test]  Epoch: 36	Loss: 0.047261	Acc: 34.8% (3484/10000)
[Test]  Epoch: 37	Loss: 0.047308	Acc: 34.8% (3477/10000)
[Test]  Epoch: 38	Loss: 0.047369	Acc: 34.7% (3473/10000)
[Test]  Epoch: 39	Loss: 0.047389	Acc: 34.8% (3475/10000)
[Test]  Epoch: 40	Loss: 0.047512	Acc: 34.6% (3464/10000)
[Test]  Epoch: 41	Loss: 0.047359	Acc: 34.5% (3449/10000)
[Test]  Epoch: 42	Loss: 0.047345	Acc: 34.5% (3451/10000)
[Test]  Epoch: 43	Loss: 0.047269	Acc: 34.5% (3453/10000)
[Test]  Epoch: 44	Loss: 0.047223	Acc: 34.8% (3476/10000)
[Test]  Epoch: 45	Loss: 0.047325	Acc: 34.9% (3486/10000)
[Test]  Epoch: 46	Loss: 0.047312	Acc: 34.6% (3462/10000)
[Test]  Epoch: 47	Loss: 0.047395	Acc: 35.2% (3517/10000)
[Test]  Epoch: 48	Loss: 0.047454	Acc: 34.8% (3481/10000)
[Test]  Epoch: 49	Loss: 0.047473	Acc: 34.9% (3493/10000)
[Test]  Epoch: 50	Loss: 0.047230	Acc: 35.0% (3502/10000)
[Test]  Epoch: 51	Loss: 0.047232	Acc: 35.2% (3520/10000)
[Test]  Epoch: 52	Loss: 0.047351	Acc: 35.0% (3498/10000)
[Test]  Epoch: 53	Loss: 0.047425	Acc: 35.0% (3496/10000)
[Test]  Epoch: 54	Loss: 0.047399	Acc: 35.1% (3508/10000)
[Test]  Epoch: 55	Loss: 0.047262	Acc: 35.3% (3533/10000)
[Test]  Epoch: 56	Loss: 0.047449	Acc: 34.8% (3479/10000)
[Test]  Epoch: 57	Loss: 0.047298	Acc: 35.0% (3501/10000)
[Test]  Epoch: 58	Loss: 0.047245	Acc: 35.4% (3542/10000)
[Test]  Epoch: 59	Loss: 0.047281	Acc: 35.5% (3547/10000)
[Test]  Epoch: 60	Loss: 0.047439	Acc: 35.1% (3510/10000)
[Test]  Epoch: 61	Loss: 0.047482	Acc: 35.0% (3500/10000)
[Test]  Epoch: 62	Loss: 0.047433	Acc: 35.0% (3504/10000)
[Test]  Epoch: 63	Loss: 0.047338	Acc: 35.2% (3516/10000)
[Test]  Epoch: 64	Loss: 0.047320	Acc: 35.3% (3526/10000)
[Test]  Epoch: 65	Loss: 0.047372	Acc: 35.2% (3521/10000)
[Test]  Epoch: 66	Loss: 0.047373	Acc: 35.1% (3515/10000)
[Test]  Epoch: 67	Loss: 0.047440	Acc: 35.1% (3514/10000)
[Test]  Epoch: 68	Loss: 0.047478	Acc: 35.1% (3508/10000)
[Test]  Epoch: 69	Loss: 0.047447	Acc: 35.0% (3497/10000)
[Test]  Epoch: 70	Loss: 0.047385	Acc: 35.2% (3523/10000)
[Test]  Epoch: 71	Loss: 0.047416	Acc: 35.3% (3526/10000)
[Test]  Epoch: 72	Loss: 0.047413	Acc: 35.1% (3515/10000)
[Test]  Epoch: 73	Loss: 0.047347	Acc: 35.2% (3519/10000)
[Test]  Epoch: 74	Loss: 0.047328	Acc: 35.3% (3534/10000)
[Test]  Epoch: 75	Loss: 0.047368	Acc: 35.3% (3531/10000)
[Test]  Epoch: 76	Loss: 0.047346	Acc: 35.3% (3526/10000)
[Test]  Epoch: 77	Loss: 0.047334	Acc: 35.2% (3518/10000)
[Test]  Epoch: 78	Loss: 0.047367	Acc: 35.0% (3504/10000)
[Test]  Epoch: 79	Loss: 0.047430	Acc: 35.4% (3535/10000)
[Test]  Epoch: 80	Loss: 0.047345	Acc: 35.4% (3543/10000)
[Test]  Epoch: 81	Loss: 0.047327	Acc: 35.4% (3535/10000)
[Test]  Epoch: 82	Loss: 0.047351	Acc: 35.2% (3521/10000)
[Test]  Epoch: 83	Loss: 0.047429	Acc: 35.0% (3500/10000)
[Test]  Epoch: 84	Loss: 0.047423	Acc: 35.1% (3514/10000)
[Test]  Epoch: 85	Loss: 0.047420	Acc: 35.1% (3508/10000)
[Test]  Epoch: 86	Loss: 0.047404	Acc: 35.2% (3520/10000)
[Test]  Epoch: 87	Loss: 0.047359	Acc: 35.1% (3512/10000)
[Test]  Epoch: 88	Loss: 0.047427	Acc: 35.0% (3497/10000)
[Test]  Epoch: 89	Loss: 0.047405	Acc: 35.1% (3515/10000)
[Test]  Epoch: 90	Loss: 0.047416	Acc: 35.2% (3519/10000)
[Test]  Epoch: 91	Loss: 0.047378	Acc: 35.2% (3524/10000)
[Test]  Epoch: 92	Loss: 0.047311	Acc: 35.4% (3538/10000)
[Test]  Epoch: 93	Loss: 0.047376	Acc: 35.3% (3529/10000)
[Test]  Epoch: 94	Loss: 0.047357	Acc: 35.4% (3537/10000)
[Test]  Epoch: 95	Loss: 0.047369	Acc: 35.2% (3525/10000)
[Test]  Epoch: 96	Loss: 0.047302	Acc: 35.3% (3526/10000)
[Test]  Epoch: 97	Loss: 0.047344	Acc: 35.3% (3528/10000)
[Test]  Epoch: 98	Loss: 0.047324	Acc: 35.3% (3526/10000)
[Test]  Epoch: 99	Loss: 0.047385	Acc: 35.2% (3518/10000)
[Test]  Epoch: 100	Loss: 0.047401	Acc: 35.1% (3515/10000)
===========finish==========
['2024-08-19', '03:30:33.420576', '100', 'test', '0.047401312386989594', '35.15', '35.47']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer3.0.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.062895	Acc: 17.0% (1701/10000)
[Test]  Epoch: 2	Loss: 0.050871	Acc: 28.0% (2797/10000)
[Test]  Epoch: 3	Loss: 0.048860	Acc: 30.8% (3083/10000)
[Test]  Epoch: 4	Loss: 0.048333	Acc: 31.9% (3195/10000)
[Test]  Epoch: 5	Loss: 0.048237	Acc: 32.4% (3236/10000)
[Test]  Epoch: 6	Loss: 0.047976	Acc: 32.7% (3273/10000)
[Test]  Epoch: 7	Loss: 0.047531	Acc: 33.0% (3304/10000)
[Test]  Epoch: 8	Loss: 0.047460	Acc: 33.5% (3352/10000)
[Test]  Epoch: 9	Loss: 0.047455	Acc: 33.6% (3363/10000)
[Test]  Epoch: 10	Loss: 0.047428	Acc: 33.1% (3308/10000)
[Test]  Epoch: 11	Loss: 0.047480	Acc: 33.7% (3369/10000)
[Test]  Epoch: 12	Loss: 0.047409	Acc: 33.8% (3381/10000)
[Test]  Epoch: 13	Loss: 0.047155	Acc: 33.8% (3376/10000)
[Test]  Epoch: 14	Loss: 0.047064	Acc: 34.1% (3410/10000)
[Test]  Epoch: 15	Loss: 0.047240	Acc: 34.4% (3435/10000)
[Test]  Epoch: 16	Loss: 0.047038	Acc: 34.6% (3457/10000)
[Test]  Epoch: 17	Loss: 0.047015	Acc: 34.5% (3446/10000)
[Test]  Epoch: 18	Loss: 0.047012	Acc: 34.4% (3439/10000)
[Test]  Epoch: 19	Loss: 0.047056	Acc: 34.2% (3421/10000)
[Test]  Epoch: 20	Loss: 0.047082	Acc: 34.6% (3459/10000)
[Test]  Epoch: 21	Loss: 0.046849	Acc: 34.6% (3464/10000)
[Test]  Epoch: 22	Loss: 0.047135	Acc: 34.4% (3444/10000)
[Test]  Epoch: 23	Loss: 0.046847	Acc: 34.9% (3489/10000)
[Test]  Epoch: 24	Loss: 0.046717	Acc: 34.9% (3491/10000)
[Test]  Epoch: 25	Loss: 0.046846	Acc: 35.0% (3499/10000)
[Test]  Epoch: 26	Loss: 0.046879	Acc: 35.1% (3512/10000)
[Test]  Epoch: 27	Loss: 0.046712	Acc: 35.1% (3510/10000)
[Test]  Epoch: 28	Loss: 0.046798	Acc: 35.0% (3497/10000)
[Test]  Epoch: 29	Loss: 0.046821	Acc: 35.1% (3506/10000)
[Test]  Epoch: 30	Loss: 0.046865	Acc: 35.0% (3503/10000)
[Test]  Epoch: 31	Loss: 0.047025	Acc: 34.8% (3475/10000)
[Test]  Epoch: 32	Loss: 0.046689	Acc: 35.4% (3538/10000)
[Test]  Epoch: 33	Loss: 0.046775	Acc: 35.3% (3530/10000)
[Test]  Epoch: 34	Loss: 0.046898	Acc: 35.2% (3518/10000)
[Test]  Epoch: 35	Loss: 0.046785	Acc: 35.1% (3508/10000)
[Test]  Epoch: 36	Loss: 0.046591	Acc: 35.7% (3567/10000)
[Test]  Epoch: 37	Loss: 0.046653	Acc: 35.5% (3553/10000)
[Test]  Epoch: 38	Loss: 0.046672	Acc: 35.3% (3531/10000)
[Test]  Epoch: 39	Loss: 0.046815	Acc: 35.2% (3524/10000)
[Test]  Epoch: 40	Loss: 0.046933	Acc: 35.2% (3517/10000)
[Test]  Epoch: 41	Loss: 0.046774	Acc: 35.2% (3522/10000)
[Test]  Epoch: 42	Loss: 0.046572	Acc: 35.5% (3546/10000)
[Test]  Epoch: 43	Loss: 0.046567	Acc: 35.7% (3567/10000)
[Test]  Epoch: 44	Loss: 0.046639	Acc: 35.7% (3573/10000)
[Test]  Epoch: 45	Loss: 0.046779	Acc: 35.6% (3557/10000)
[Test]  Epoch: 46	Loss: 0.046619	Acc: 35.5% (3547/10000)
[Test]  Epoch: 47	Loss: 0.046732	Acc: 35.5% (3550/10000)
[Test]  Epoch: 48	Loss: 0.046794	Acc: 35.8% (3580/10000)
[Test]  Epoch: 49	Loss: 0.046849	Acc: 35.7% (3568/10000)
[Test]  Epoch: 50	Loss: 0.046575	Acc: 35.9% (3593/10000)
[Test]  Epoch: 51	Loss: 0.046655	Acc: 35.8% (3580/10000)
[Test]  Epoch: 52	Loss: 0.046650	Acc: 35.8% (3580/10000)
[Test]  Epoch: 53	Loss: 0.046836	Acc: 35.8% (3577/10000)
[Test]  Epoch: 54	Loss: 0.046661	Acc: 36.0% (3605/10000)
[Test]  Epoch: 55	Loss: 0.046655	Acc: 35.8% (3582/10000)
[Test]  Epoch: 56	Loss: 0.046798	Acc: 35.8% (3584/10000)
[Test]  Epoch: 57	Loss: 0.046722	Acc: 35.9% (3594/10000)
[Test]  Epoch: 58	Loss: 0.046633	Acc: 35.8% (3576/10000)
[Test]  Epoch: 59	Loss: 0.046738	Acc: 35.9% (3586/10000)
[Test]  Epoch: 60	Loss: 0.046774	Acc: 36.0% (3602/10000)
[Test]  Epoch: 61	Loss: 0.046813	Acc: 35.7% (3572/10000)
[Test]  Epoch: 62	Loss: 0.046814	Acc: 35.7% (3568/10000)
[Test]  Epoch: 63	Loss: 0.046726	Acc: 35.7% (3572/10000)
[Test]  Epoch: 64	Loss: 0.046696	Acc: 36.0% (3596/10000)
[Test]  Epoch: 65	Loss: 0.046779	Acc: 35.8% (3584/10000)
[Test]  Epoch: 66	Loss: 0.046794	Acc: 35.9% (3590/10000)
[Test]  Epoch: 67	Loss: 0.046846	Acc: 36.0% (3597/10000)
[Test]  Epoch: 68	Loss: 0.046898	Acc: 35.8% (3583/10000)
[Test]  Epoch: 69	Loss: 0.046831	Acc: 35.8% (3581/10000)
[Test]  Epoch: 70	Loss: 0.046783	Acc: 36.0% (3596/10000)
[Test]  Epoch: 71	Loss: 0.046782	Acc: 36.0% (3597/10000)
[Test]  Epoch: 72	Loss: 0.046792	Acc: 36.0% (3596/10000)
[Test]  Epoch: 73	Loss: 0.046754	Acc: 35.9% (3587/10000)
[Test]  Epoch: 74	Loss: 0.046746	Acc: 36.0% (3597/10000)
[Test]  Epoch: 75	Loss: 0.046769	Acc: 36.0% (3597/10000)
[Test]  Epoch: 76	Loss: 0.046755	Acc: 35.8% (3584/10000)
[Test]  Epoch: 77	Loss: 0.046746	Acc: 35.9% (3591/10000)
[Test]  Epoch: 78	Loss: 0.046785	Acc: 35.9% (3591/10000)
[Test]  Epoch: 79	Loss: 0.046825	Acc: 36.0% (3597/10000)
[Test]  Epoch: 80	Loss: 0.046760	Acc: 36.0% (3600/10000)
[Test]  Epoch: 81	Loss: 0.046722	Acc: 35.9% (3592/10000)
[Test]  Epoch: 82	Loss: 0.046748	Acc: 36.0% (3596/10000)
[Test]  Epoch: 83	Loss: 0.046822	Acc: 35.8% (3581/10000)
[Test]  Epoch: 84	Loss: 0.046833	Acc: 36.0% (3600/10000)
[Test]  Epoch: 85	Loss: 0.046839	Acc: 35.9% (3591/10000)
[Test]  Epoch: 86	Loss: 0.046794	Acc: 35.9% (3594/10000)
[Test]  Epoch: 87	Loss: 0.046765	Acc: 36.0% (3596/10000)
[Test]  Epoch: 88	Loss: 0.046861	Acc: 35.8% (3582/10000)
[Test]  Epoch: 89	Loss: 0.046793	Acc: 36.1% (3609/10000)
[Test]  Epoch: 90	Loss: 0.046834	Acc: 36.2% (3616/10000)
[Test]  Epoch: 91	Loss: 0.046771	Acc: 36.0% (3600/10000)
[Test]  Epoch: 92	Loss: 0.046736	Acc: 36.0% (3599/10000)
[Test]  Epoch: 93	Loss: 0.046786	Acc: 36.0% (3603/10000)
[Test]  Epoch: 94	Loss: 0.046774	Acc: 36.2% (3616/10000)
[Test]  Epoch: 95	Loss: 0.046770	Acc: 35.9% (3594/10000)
[Test]  Epoch: 96	Loss: 0.046710	Acc: 36.0% (3601/10000)
[Test]  Epoch: 97	Loss: 0.046763	Acc: 36.0% (3600/10000)
[Test]  Epoch: 98	Loss: 0.046697	Acc: 36.1% (3607/10000)
[Test]  Epoch: 99	Loss: 0.046746	Acc: 36.0% (3602/10000)
[Test]  Epoch: 100	Loss: 0.046770	Acc: 35.8% (3582/10000)
===========finish==========
['2024-08-19', '03:34:33.448275', '100', 'test', '0.04677023513317108', '35.82', '36.16']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'bn1.weight', 'layer4.1.conv2.weight', 'layer3.0.downsample.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.085251	Acc: 6.1% (611/10000)
[Test]  Epoch: 2	Loss: 0.062096	Acc: 14.4% (1439/10000)
[Test]  Epoch: 3	Loss: 0.058048	Acc: 17.6% (1759/10000)
[Test]  Epoch: 4	Loss: 0.056575	Acc: 19.2% (1919/10000)
[Test]  Epoch: 5	Loss: 0.056717	Acc: 19.3% (1934/10000)
[Test]  Epoch: 6	Loss: 0.055912	Acc: 20.1% (2006/10000)
[Test]  Epoch: 7	Loss: 0.055386	Acc: 20.6% (2057/10000)
[Test]  Epoch: 8	Loss: 0.055081	Acc: 21.2% (2121/10000)
[Test]  Epoch: 9	Loss: 0.055038	Acc: 21.8% (2181/10000)
[Test]  Epoch: 10	Loss: 0.055302	Acc: 21.1% (2110/10000)
[Test]  Epoch: 11	Loss: 0.054766	Acc: 22.1% (2207/10000)
[Test]  Epoch: 12	Loss: 0.054631	Acc: 22.2% (2224/10000)
[Test]  Epoch: 13	Loss: 0.054662	Acc: 22.2% (2218/10000)
[Test]  Epoch: 14	Loss: 0.054275	Acc: 23.0% (2300/10000)
[Test]  Epoch: 15	Loss: 0.054471	Acc: 22.7% (2274/10000)
[Test]  Epoch: 16	Loss: 0.054332	Acc: 23.0% (2299/10000)
[Test]  Epoch: 17	Loss: 0.054224	Acc: 23.4% (2337/10000)
[Test]  Epoch: 18	Loss: 0.054132	Acc: 23.4% (2337/10000)
[Test]  Epoch: 19	Loss: 0.054311	Acc: 23.1% (2315/10000)
[Test]  Epoch: 20	Loss: 0.053861	Acc: 24.2% (2419/10000)
[Test]  Epoch: 21	Loss: 0.053854	Acc: 23.9% (2386/10000)
[Test]  Epoch: 22	Loss: 0.054203	Acc: 23.9% (2385/10000)
[Test]  Epoch: 23	Loss: 0.053649	Acc: 24.4% (2437/10000)
[Test]  Epoch: 24	Loss: 0.053864	Acc: 24.1% (2408/10000)
[Test]  Epoch: 25	Loss: 0.053994	Acc: 23.8% (2383/10000)
[Test]  Epoch: 26	Loss: 0.053711	Acc: 24.2% (2420/10000)
[Test]  Epoch: 27	Loss: 0.053525	Acc: 24.4% (2443/10000)
[Test]  Epoch: 28	Loss: 0.053640	Acc: 24.6% (2456/10000)
[Test]  Epoch: 29	Loss: 0.053950	Acc: 24.0% (2401/10000)
[Test]  Epoch: 30	Loss: 0.053729	Acc: 24.5% (2449/10000)
[Test]  Epoch: 31	Loss: 0.053815	Acc: 24.4% (2444/10000)
[Test]  Epoch: 32	Loss: 0.053594	Acc: 24.4% (2435/10000)
[Test]  Epoch: 33	Loss: 0.053577	Acc: 24.4% (2443/10000)
[Test]  Epoch: 34	Loss: 0.053577	Acc: 25.0% (2497/10000)
[Test]  Epoch: 35	Loss: 0.053520	Acc: 24.9% (2486/10000)
[Test]  Epoch: 36	Loss: 0.053396	Acc: 25.0% (2498/10000)
[Test]  Epoch: 37	Loss: 0.053564	Acc: 25.2% (2518/10000)
[Test]  Epoch: 38	Loss: 0.053469	Acc: 24.8% (2477/10000)
[Test]  Epoch: 39	Loss: 0.053628	Acc: 24.8% (2484/10000)
[Test]  Epoch: 40	Loss: 0.053619	Acc: 24.8% (2475/10000)
[Test]  Epoch: 41	Loss: 0.053431	Acc: 24.8% (2484/10000)
[Test]  Epoch: 42	Loss: 0.053296	Acc: 25.6% (2561/10000)
[Test]  Epoch: 43	Loss: 0.053267	Acc: 25.2% (2519/10000)
[Test]  Epoch: 44	Loss: 0.053291	Acc: 25.4% (2543/10000)
[Test]  Epoch: 45	Loss: 0.053415	Acc: 25.0% (2503/10000)
[Test]  Epoch: 46	Loss: 0.053367	Acc: 25.2% (2518/10000)
[Test]  Epoch: 47	Loss: 0.053553	Acc: 25.2% (2524/10000)
[Test]  Epoch: 48	Loss: 0.053304	Acc: 25.6% (2559/10000)
[Test]  Epoch: 49	Loss: 0.053482	Acc: 25.5% (2548/10000)
[Test]  Epoch: 50	Loss: 0.053340	Acc: 25.6% (2560/10000)
[Test]  Epoch: 51	Loss: 0.053234	Acc: 25.6% (2561/10000)
[Test]  Epoch: 52	Loss: 0.053200	Acc: 25.5% (2550/10000)
[Test]  Epoch: 53	Loss: 0.053376	Acc: 25.6% (2557/10000)
[Test]  Epoch: 54	Loss: 0.053293	Acc: 26.0% (2603/10000)
[Test]  Epoch: 55	Loss: 0.053359	Acc: 25.5% (2551/10000)
[Test]  Epoch: 56	Loss: 0.053350	Acc: 26.0% (2600/10000)
[Test]  Epoch: 57	Loss: 0.053202	Acc: 25.9% (2593/10000)
[Test]  Epoch: 58	Loss: 0.053331	Acc: 25.9% (2586/10000)
[Test]  Epoch: 59	Loss: 0.053219	Acc: 25.7% (2567/10000)
[Test]  Epoch: 60	Loss: 0.053549	Acc: 25.5% (2551/10000)
[Test]  Epoch: 61	Loss: 0.053422	Acc: 25.8% (2575/10000)
[Test]  Epoch: 62	Loss: 0.053386	Acc: 25.9% (2586/10000)
[Test]  Epoch: 63	Loss: 0.053266	Acc: 25.7% (2571/10000)
[Test]  Epoch: 64	Loss: 0.053285	Acc: 25.9% (2594/10000)
[Test]  Epoch: 65	Loss: 0.053306	Acc: 25.8% (2581/10000)
[Test]  Epoch: 66	Loss: 0.053302	Acc: 25.8% (2583/10000)
[Test]  Epoch: 67	Loss: 0.053346	Acc: 25.9% (2587/10000)
[Test]  Epoch: 68	Loss: 0.053408	Acc: 25.6% (2559/10000)
[Test]  Epoch: 69	Loss: 0.053315	Acc: 25.8% (2578/10000)
[Test]  Epoch: 70	Loss: 0.053284	Acc: 25.8% (2575/10000)
[Test]  Epoch: 71	Loss: 0.053350	Acc: 25.8% (2583/10000)
[Test]  Epoch: 72	Loss: 0.053340	Acc: 25.6% (2565/10000)
[Test]  Epoch: 73	Loss: 0.053286	Acc: 26.1% (2609/10000)
[Test]  Epoch: 74	Loss: 0.053281	Acc: 25.9% (2589/10000)
[Test]  Epoch: 75	Loss: 0.053314	Acc: 25.9% (2595/10000)
[Test]  Epoch: 76	Loss: 0.053314	Acc: 26.0% (2599/10000)
[Test]  Epoch: 77	Loss: 0.053300	Acc: 26.0% (2601/10000)
[Test]  Epoch: 78	Loss: 0.053335	Acc: 25.8% (2579/10000)
[Test]  Epoch: 79	Loss: 0.053343	Acc: 25.9% (2590/10000)
[Test]  Epoch: 80	Loss: 0.053319	Acc: 26.1% (2612/10000)
[Test]  Epoch: 81	Loss: 0.053275	Acc: 25.9% (2585/10000)
[Test]  Epoch: 82	Loss: 0.053290	Acc: 26.0% (2596/10000)
[Test]  Epoch: 83	Loss: 0.053307	Acc: 26.0% (2604/10000)
[Test]  Epoch: 84	Loss: 0.053314	Acc: 25.9% (2591/10000)
[Test]  Epoch: 85	Loss: 0.053321	Acc: 25.9% (2594/10000)
[Test]  Epoch: 86	Loss: 0.053294	Acc: 26.0% (2596/10000)
[Test]  Epoch: 87	Loss: 0.053310	Acc: 25.9% (2587/10000)
[Test]  Epoch: 88	Loss: 0.053372	Acc: 25.9% (2586/10000)
[Test]  Epoch: 89	Loss: 0.053314	Acc: 25.9% (2588/10000)
[Test]  Epoch: 90	Loss: 0.053292	Acc: 26.0% (2601/10000)
[Test]  Epoch: 91	Loss: 0.053258	Acc: 26.2% (2617/10000)
[Test]  Epoch: 92	Loss: 0.053243	Acc: 26.0% (2601/10000)
[Test]  Epoch: 93	Loss: 0.053298	Acc: 25.9% (2592/10000)
[Test]  Epoch: 94	Loss: 0.053256	Acc: 26.1% (2607/10000)
[Test]  Epoch: 95	Loss: 0.053340	Acc: 26.0% (2598/10000)
[Test]  Epoch: 96	Loss: 0.053211	Acc: 26.1% (2611/10000)
[Test]  Epoch: 97	Loss: 0.053283	Acc: 26.0% (2601/10000)
[Test]  Epoch: 98	Loss: 0.053248	Acc: 26.1% (2610/10000)
[Test]  Epoch: 99	Loss: 0.053287	Acc: 26.2% (2616/10000)
[Test]  Epoch: 100	Loss: 0.053263	Acc: 26.1% (2615/10000)
===========finish==========
['2024-08-19', '03:38:35.558941', '100', 'test', '0.05326345627307892', '26.15', '26.17']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'bn1.weight', 'layer4.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer1.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.086208	Acc: 7.8% (782/10000)
[Test]  Epoch: 2	Loss: 0.064236	Acc: 14.2% (1422/10000)
[Test]  Epoch: 3	Loss: 0.060272	Acc: 16.6% (1661/10000)
[Test]  Epoch: 4	Loss: 0.058742	Acc: 18.2% (1818/10000)
[Test]  Epoch: 5	Loss: 0.058241	Acc: 18.8% (1880/10000)
[Test]  Epoch: 6	Loss: 0.057480	Acc: 19.7% (1974/10000)
[Test]  Epoch: 7	Loss: 0.057077	Acc: 19.9% (1994/10000)
[Test]  Epoch: 8	Loss: 0.056593	Acc: 20.6% (2065/10000)
[Test]  Epoch: 9	Loss: 0.056750	Acc: 21.0% (2096/10000)
[Test]  Epoch: 10	Loss: 0.056531	Acc: 20.8% (2076/10000)
[Test]  Epoch: 11	Loss: 0.056220	Acc: 21.3% (2130/10000)
[Test]  Epoch: 12	Loss: 0.056040	Acc: 21.7% (2167/10000)
[Test]  Epoch: 13	Loss: 0.056305	Acc: 21.6% (2156/10000)
[Test]  Epoch: 14	Loss: 0.055592	Acc: 22.1% (2206/10000)
[Test]  Epoch: 15	Loss: 0.055739	Acc: 22.0% (2200/10000)
[Test]  Epoch: 16	Loss: 0.055580	Acc: 21.9% (2192/10000)
[Test]  Epoch: 17	Loss: 0.055520	Acc: 22.4% (2240/10000)
[Test]  Epoch: 18	Loss: 0.055454	Acc: 22.6% (2257/10000)
[Test]  Epoch: 19	Loss: 0.055523	Acc: 22.4% (2238/10000)
[Test]  Epoch: 20	Loss: 0.055581	Acc: 22.7% (2266/10000)
[Test]  Epoch: 21	Loss: 0.055136	Acc: 22.9% (2286/10000)
[Test]  Epoch: 22	Loss: 0.055648	Acc: 22.4% (2242/10000)
[Test]  Epoch: 23	Loss: 0.055048	Acc: 22.8% (2280/10000)
[Test]  Epoch: 24	Loss: 0.055048	Acc: 22.8% (2282/10000)
[Test]  Epoch: 25	Loss: 0.055113	Acc: 23.2% (2320/10000)
[Test]  Epoch: 26	Loss: 0.055097	Acc: 23.2% (2324/10000)
[Test]  Epoch: 27	Loss: 0.054883	Acc: 23.2% (2316/10000)
[Test]  Epoch: 28	Loss: 0.055173	Acc: 22.8% (2279/10000)
[Test]  Epoch: 29	Loss: 0.055300	Acc: 22.9% (2286/10000)
[Test]  Epoch: 30	Loss: 0.054992	Acc: 23.0% (2303/10000)
[Test]  Epoch: 31	Loss: 0.055127	Acc: 23.0% (2299/10000)
[Test]  Epoch: 32	Loss: 0.054771	Acc: 23.3% (2332/10000)
[Test]  Epoch: 33	Loss: 0.054793	Acc: 23.3% (2334/10000)
[Test]  Epoch: 34	Loss: 0.054881	Acc: 23.4% (2343/10000)
[Test]  Epoch: 35	Loss: 0.054651	Acc: 23.9% (2386/10000)
[Test]  Epoch: 36	Loss: 0.054630	Acc: 23.7% (2371/10000)
[Test]  Epoch: 37	Loss: 0.054708	Acc: 23.7% (2367/10000)
[Test]  Epoch: 38	Loss: 0.054605	Acc: 24.0% (2396/10000)
[Test]  Epoch: 39	Loss: 0.055034	Acc: 23.4% (2341/10000)
[Test]  Epoch: 40	Loss: 0.054811	Acc: 23.6% (2364/10000)
[Test]  Epoch: 41	Loss: 0.054559	Acc: 24.1% (2408/10000)
[Test]  Epoch: 42	Loss: 0.054758	Acc: 23.9% (2391/10000)
[Test]  Epoch: 43	Loss: 0.054613	Acc: 23.7% (2370/10000)
[Test]  Epoch: 44	Loss: 0.054679	Acc: 23.6% (2359/10000)
[Test]  Epoch: 45	Loss: 0.054707	Acc: 24.0% (2399/10000)
[Test]  Epoch: 46	Loss: 0.054671	Acc: 24.0% (2400/10000)
[Test]  Epoch: 47	Loss: 0.054759	Acc: 23.9% (2385/10000)
[Test]  Epoch: 48	Loss: 0.054631	Acc: 24.4% (2435/10000)
[Test]  Epoch: 49	Loss: 0.054585	Acc: 24.2% (2418/10000)
[Test]  Epoch: 50	Loss: 0.054535	Acc: 24.3% (2429/10000)
[Test]  Epoch: 51	Loss: 0.054646	Acc: 24.1% (2413/10000)
[Test]  Epoch: 52	Loss: 0.054497	Acc: 24.2% (2421/10000)
[Test]  Epoch: 53	Loss: 0.054642	Acc: 23.8% (2384/10000)
[Test]  Epoch: 54	Loss: 0.054478	Acc: 24.4% (2435/10000)
[Test]  Epoch: 55	Loss: 0.054424	Acc: 24.1% (2409/10000)
[Test]  Epoch: 56	Loss: 0.054762	Acc: 24.1% (2409/10000)
[Test]  Epoch: 57	Loss: 0.054457	Acc: 24.2% (2418/10000)
[Test]  Epoch: 58	Loss: 0.054492	Acc: 24.2% (2420/10000)
[Test]  Epoch: 59	Loss: 0.054515	Acc: 24.2% (2416/10000)
[Test]  Epoch: 60	Loss: 0.054626	Acc: 24.3% (2434/10000)
[Test]  Epoch: 61	Loss: 0.054657	Acc: 24.0% (2396/10000)
[Test]  Epoch: 62	Loss: 0.054582	Acc: 24.3% (2431/10000)
[Test]  Epoch: 63	Loss: 0.054460	Acc: 24.4% (2439/10000)
[Test]  Epoch: 64	Loss: 0.054483	Acc: 24.2% (2423/10000)
[Test]  Epoch: 65	Loss: 0.054554	Acc: 24.3% (2427/10000)
[Test]  Epoch: 66	Loss: 0.054510	Acc: 24.3% (2431/10000)
[Test]  Epoch: 67	Loss: 0.054547	Acc: 24.5% (2448/10000)
[Test]  Epoch: 68	Loss: 0.054613	Acc: 24.3% (2427/10000)
[Test]  Epoch: 69	Loss: 0.054548	Acc: 24.3% (2427/10000)
[Test]  Epoch: 70	Loss: 0.054538	Acc: 24.5% (2448/10000)
[Test]  Epoch: 71	Loss: 0.054514	Acc: 24.4% (2435/10000)
[Test]  Epoch: 72	Loss: 0.054537	Acc: 24.5% (2454/10000)
[Test]  Epoch: 73	Loss: 0.054460	Acc: 24.5% (2447/10000)
[Test]  Epoch: 74	Loss: 0.054454	Acc: 24.4% (2442/10000)
[Test]  Epoch: 75	Loss: 0.054519	Acc: 24.5% (2448/10000)
[Test]  Epoch: 76	Loss: 0.054510	Acc: 24.5% (2447/10000)
[Test]  Epoch: 77	Loss: 0.054493	Acc: 24.5% (2454/10000)
[Test]  Epoch: 78	Loss: 0.054525	Acc: 24.4% (2445/10000)
[Test]  Epoch: 79	Loss: 0.054488	Acc: 24.4% (2437/10000)
[Test]  Epoch: 80	Loss: 0.054525	Acc: 24.3% (2431/10000)
[Test]  Epoch: 81	Loss: 0.054402	Acc: 24.4% (2438/10000)
[Test]  Epoch: 82	Loss: 0.054469	Acc: 24.3% (2426/10000)
[Test]  Epoch: 83	Loss: 0.054552	Acc: 24.4% (2440/10000)
[Test]  Epoch: 84	Loss: 0.054503	Acc: 24.4% (2441/10000)
[Test]  Epoch: 85	Loss: 0.054504	Acc: 24.3% (2434/10000)
[Test]  Epoch: 86	Loss: 0.054488	Acc: 24.6% (2456/10000)
[Test]  Epoch: 87	Loss: 0.054459	Acc: 24.4% (2441/10000)
[Test]  Epoch: 88	Loss: 0.054525	Acc: 24.4% (2435/10000)
[Test]  Epoch: 89	Loss: 0.054446	Acc: 24.4% (2442/10000)
[Test]  Epoch: 90	Loss: 0.054447	Acc: 24.6% (2456/10000)
[Test]  Epoch: 91	Loss: 0.054448	Acc: 24.6% (2463/10000)
[Test]  Epoch: 92	Loss: 0.054371	Acc: 24.4% (2435/10000)
[Test]  Epoch: 93	Loss: 0.054466	Acc: 24.4% (2442/10000)
[Test]  Epoch: 94	Loss: 0.054478	Acc: 24.6% (2456/10000)
[Test]  Epoch: 95	Loss: 0.054483	Acc: 24.5% (2450/10000)
[Test]  Epoch: 96	Loss: 0.054384	Acc: 24.6% (2456/10000)
[Test]  Epoch: 97	Loss: 0.054495	Acc: 24.4% (2437/10000)
[Test]  Epoch: 98	Loss: 0.054494	Acc: 24.3% (2433/10000)
[Test]  Epoch: 99	Loss: 0.054473	Acc: 24.2% (2419/10000)
[Test]  Epoch: 100	Loss: 0.054474	Acc: 24.4% (2445/10000)
===========finish==========
['2024-08-19', '03:42:32.047421', '100', 'test', '0.05447359478473663', '24.45', '24.63']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'bn1.weight', 'layer4.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer1.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.downsample.0.weight', 'layer1.0.conv1.weight', 'layer2.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.079119	Acc: 5.6% (559/10000)
[Test]  Epoch: 2	Loss: 0.066217	Acc: 12.2% (1220/10000)
[Test]  Epoch: 3	Loss: 0.062956	Acc: 14.1% (1411/10000)
[Test]  Epoch: 4	Loss: 0.061634	Acc: 14.9% (1491/10000)
[Test]  Epoch: 5	Loss: 0.060936	Acc: 15.7% (1571/10000)
[Test]  Epoch: 6	Loss: 0.060097	Acc: 16.3% (1631/10000)
[Test]  Epoch: 7	Loss: 0.059811	Acc: 16.5% (1651/10000)
[Test]  Epoch: 8	Loss: 0.059450	Acc: 16.9% (1695/10000)
[Test]  Epoch: 9	Loss: 0.059792	Acc: 16.7% (1673/10000)
[Test]  Epoch: 10	Loss: 0.059535	Acc: 16.9% (1694/10000)
[Test]  Epoch: 11	Loss: 0.059359	Acc: 17.0% (1698/10000)
[Test]  Epoch: 12	Loss: 0.059213	Acc: 17.0% (1701/10000)
[Test]  Epoch: 13	Loss: 0.059122	Acc: 17.7% (1768/10000)
[Test]  Epoch: 14	Loss: 0.058755	Acc: 17.9% (1786/10000)
[Test]  Epoch: 15	Loss: 0.058983	Acc: 17.7% (1773/10000)
[Test]  Epoch: 16	Loss: 0.058843	Acc: 17.8% (1776/10000)
[Test]  Epoch: 17	Loss: 0.058604	Acc: 18.1% (1814/10000)
[Test]  Epoch: 18	Loss: 0.058748	Acc: 18.0% (1800/10000)
[Test]  Epoch: 19	Loss: 0.058621	Acc: 17.6% (1764/10000)
[Test]  Epoch: 20	Loss: 0.058517	Acc: 18.1% (1806/10000)
[Test]  Epoch: 21	Loss: 0.058492	Acc: 18.1% (1814/10000)
[Test]  Epoch: 22	Loss: 0.058607	Acc: 18.1% (1806/10000)
[Test]  Epoch: 23	Loss: 0.058531	Acc: 18.1% (1806/10000)
[Test]  Epoch: 24	Loss: 0.058423	Acc: 18.3% (1834/10000)
[Test]  Epoch: 25	Loss: 0.058338	Acc: 18.4% (1843/10000)
[Test]  Epoch: 26	Loss: 0.058377	Acc: 18.5% (1847/10000)
[Test]  Epoch: 27	Loss: 0.058246	Acc: 18.6% (1861/10000)
[Test]  Epoch: 28	Loss: 0.058363	Acc: 18.6% (1855/10000)
[Test]  Epoch: 29	Loss: 0.058463	Acc: 18.6% (1860/10000)
[Test]  Epoch: 30	Loss: 0.058280	Acc: 18.9% (1889/10000)
[Test]  Epoch: 31	Loss: 0.058457	Acc: 18.7% (1873/10000)
[Test]  Epoch: 32	Loss: 0.058285	Acc: 18.7% (1873/10000)
[Test]  Epoch: 33	Loss: 0.058129	Acc: 19.1% (1915/10000)
[Test]  Epoch: 34	Loss: 0.058194	Acc: 18.8% (1879/10000)
[Test]  Epoch: 35	Loss: 0.058261	Acc: 18.8% (1875/10000)
[Test]  Epoch: 36	Loss: 0.058049	Acc: 18.9% (1890/10000)
[Test]  Epoch: 37	Loss: 0.058229	Acc: 18.9% (1894/10000)
[Test]  Epoch: 38	Loss: 0.057892	Acc: 19.3% (1928/10000)
[Test]  Epoch: 39	Loss: 0.058318	Acc: 18.8% (1880/10000)
[Test]  Epoch: 40	Loss: 0.058251	Acc: 18.9% (1888/10000)
[Test]  Epoch: 41	Loss: 0.058053	Acc: 18.9% (1894/10000)
[Test]  Epoch: 42	Loss: 0.058105	Acc: 19.2% (1917/10000)
[Test]  Epoch: 43	Loss: 0.058063	Acc: 19.1% (1911/10000)
[Test]  Epoch: 44	Loss: 0.058159	Acc: 19.0% (1900/10000)
[Test]  Epoch: 45	Loss: 0.058113	Acc: 19.1% (1915/10000)
[Test]  Epoch: 46	Loss: 0.058146	Acc: 19.0% (1904/10000)
[Test]  Epoch: 47	Loss: 0.058175	Acc: 19.2% (1922/10000)
[Test]  Epoch: 48	Loss: 0.058082	Acc: 19.3% (1930/10000)
[Test]  Epoch: 49	Loss: 0.058010	Acc: 19.4% (1945/10000)
[Test]  Epoch: 50	Loss: 0.057985	Acc: 19.4% (1942/10000)
[Test]  Epoch: 51	Loss: 0.057950	Acc: 19.2% (1916/10000)
[Test]  Epoch: 52	Loss: 0.057988	Acc: 19.4% (1939/10000)
[Test]  Epoch: 53	Loss: 0.057975	Acc: 19.5% (1946/10000)
[Test]  Epoch: 54	Loss: 0.058093	Acc: 19.6% (1955/10000)
[Test]  Epoch: 55	Loss: 0.057981	Acc: 19.3% (1933/10000)
[Test]  Epoch: 56	Loss: 0.058113	Acc: 19.5% (1948/10000)
[Test]  Epoch: 57	Loss: 0.057929	Acc: 19.6% (1958/10000)
[Test]  Epoch: 58	Loss: 0.057869	Acc: 19.4% (1938/10000)
[Test]  Epoch: 59	Loss: 0.057930	Acc: 19.5% (1946/10000)
[Test]  Epoch: 60	Loss: 0.057997	Acc: 19.5% (1949/10000)
[Test]  Epoch: 61	Loss: 0.058003	Acc: 19.4% (1940/10000)
[Test]  Epoch: 62	Loss: 0.057972	Acc: 19.4% (1941/10000)
[Test]  Epoch: 63	Loss: 0.057819	Acc: 19.5% (1954/10000)
[Test]  Epoch: 64	Loss: 0.057899	Acc: 19.5% (1950/10000)
[Test]  Epoch: 65	Loss: 0.057986	Acc: 19.3% (1934/10000)
[Test]  Epoch: 66	Loss: 0.057917	Acc: 19.6% (1960/10000)
[Test]  Epoch: 67	Loss: 0.057949	Acc: 19.6% (1956/10000)
[Test]  Epoch: 68	Loss: 0.058040	Acc: 19.4% (1945/10000)
[Test]  Epoch: 69	Loss: 0.057976	Acc: 19.5% (1950/10000)
[Test]  Epoch: 70	Loss: 0.057927	Acc: 19.6% (1959/10000)
[Test]  Epoch: 71	Loss: 0.057920	Acc: 19.4% (1941/10000)
[Test]  Epoch: 72	Loss: 0.058001	Acc: 19.4% (1938/10000)
[Test]  Epoch: 73	Loss: 0.057866	Acc: 19.5% (1952/10000)
[Test]  Epoch: 74	Loss: 0.057860	Acc: 19.5% (1946/10000)
[Test]  Epoch: 75	Loss: 0.057933	Acc: 19.6% (1957/10000)
[Test]  Epoch: 76	Loss: 0.057892	Acc: 19.7% (1969/10000)
[Test]  Epoch: 77	Loss: 0.057911	Acc: 19.6% (1962/10000)
[Test]  Epoch: 78	Loss: 0.057971	Acc: 19.6% (1955/10000)
[Test]  Epoch: 79	Loss: 0.057943	Acc: 19.5% (1954/10000)
[Test]  Epoch: 80	Loss: 0.057912	Acc: 19.6% (1960/10000)
[Test]  Epoch: 81	Loss: 0.057861	Acc: 19.6% (1955/10000)
[Test]  Epoch: 82	Loss: 0.057914	Acc: 19.7% (1967/10000)
[Test]  Epoch: 83	Loss: 0.057965	Acc: 19.6% (1957/10000)
[Test]  Epoch: 84	Loss: 0.057931	Acc: 19.7% (1966/10000)
[Test]  Epoch: 85	Loss: 0.057944	Acc: 19.6% (1959/10000)
[Test]  Epoch: 86	Loss: 0.057937	Acc: 19.7% (1971/10000)
[Test]  Epoch: 87	Loss: 0.057913	Acc: 19.6% (1958/10000)
[Test]  Epoch: 88	Loss: 0.057991	Acc: 19.5% (1950/10000)
[Test]  Epoch: 89	Loss: 0.057956	Acc: 19.6% (1961/10000)
[Test]  Epoch: 90	Loss: 0.057958	Acc: 19.5% (1953/10000)
[Test]  Epoch: 91	Loss: 0.057896	Acc: 19.7% (1969/10000)
[Test]  Epoch: 92	Loss: 0.057847	Acc: 19.7% (1966/10000)
[Test]  Epoch: 93	Loss: 0.057959	Acc: 19.6% (1962/10000)
[Test]  Epoch: 94	Loss: 0.057960	Acc: 19.7% (1973/10000)
[Test]  Epoch: 95	Loss: 0.057963	Acc: 19.5% (1952/10000)
[Test]  Epoch: 96	Loss: 0.057870	Acc: 19.6% (1963/10000)
[Test]  Epoch: 97	Loss: 0.057934	Acc: 19.5% (1950/10000)
[Test]  Epoch: 98	Loss: 0.057914	Acc: 19.7% (1969/10000)
[Test]  Epoch: 99	Loss: 0.057916	Acc: 19.6% (1963/10000)
[Test]  Epoch: 100	Loss: 0.057891	Acc: 19.7% (1972/10000)
===========finish==========
['2024-08-19', '03:47:04.396856', '100', 'test', '0.05789061689376831', '19.72', '19.73']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'bn1.weight', 'layer4.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer1.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.downsample.0.weight', 'layer1.0.conv1.weight', 'layer2.1.conv1.weight', 'last_linear.weight', 'layer4.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer2.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.085790	Acc: 1.1% (110/10000)
[Test]  Epoch: 2	Loss: 0.079795	Acc: 3.1% (311/10000)
[Test]  Epoch: 3	Loss: 0.077970	Acc: 4.1% (411/10000)
[Test]  Epoch: 4	Loss: 0.076805	Acc: 4.6% (461/10000)
[Test]  Epoch: 5	Loss: 0.076075	Acc: 5.0% (496/10000)
[Test]  Epoch: 6	Loss: 0.074969	Acc: 5.2% (521/10000)
[Test]  Epoch: 7	Loss: 0.074557	Acc: 5.3% (534/10000)
[Test]  Epoch: 8	Loss: 0.073760	Acc: 5.7% (573/10000)
[Test]  Epoch: 9	Loss: 0.073512	Acc: 5.7% (567/10000)
[Test]  Epoch: 10	Loss: 0.072560	Acc: 6.1% (606/10000)
[Test]  Epoch: 11	Loss: 0.072523	Acc: 6.0% (595/10000)
[Test]  Epoch: 12	Loss: 0.072220	Acc: 6.2% (618/10000)
[Test]  Epoch: 13	Loss: 0.072204	Acc: 6.0% (603/10000)
[Test]  Epoch: 14	Loss: 0.071530	Acc: 6.4% (644/10000)
[Test]  Epoch: 15	Loss: 0.071310	Acc: 6.4% (639/10000)
[Test]  Epoch: 16	Loss: 0.071284	Acc: 6.6% (662/10000)
[Test]  Epoch: 17	Loss: 0.070759	Acc: 6.9% (690/10000)
[Test]  Epoch: 18	Loss: 0.070988	Acc: 6.7% (672/10000)
[Test]  Epoch: 19	Loss: 0.070755	Acc: 6.8% (677/10000)
[Test]  Epoch: 20	Loss: 0.070469	Acc: 6.9% (691/10000)
[Test]  Epoch: 21	Loss: 0.070472	Acc: 6.7% (674/10000)
[Test]  Epoch: 22	Loss: 0.070238	Acc: 7.3% (726/10000)
[Test]  Epoch: 23	Loss: 0.070199	Acc: 7.1% (712/10000)
[Test]  Epoch: 24	Loss: 0.070172	Acc: 7.0% (702/10000)
[Test]  Epoch: 25	Loss: 0.069933	Acc: 7.4% (736/10000)
[Test]  Epoch: 26	Loss: 0.070173	Acc: 7.0% (699/10000)
[Test]  Epoch: 27	Loss: 0.069569	Acc: 7.7% (766/10000)
[Test]  Epoch: 28	Loss: 0.069941	Acc: 7.4% (742/10000)
[Test]  Epoch: 29	Loss: 0.069966	Acc: 7.2% (720/10000)
[Test]  Epoch: 30	Loss: 0.069776	Acc: 7.3% (729/10000)
[Test]  Epoch: 31	Loss: 0.069700	Acc: 7.4% (742/10000)
[Test]  Epoch: 32	Loss: 0.069706	Acc: 7.2% (723/10000)
[Test]  Epoch: 33	Loss: 0.069388	Acc: 7.4% (738/10000)
[Test]  Epoch: 34	Loss: 0.068825	Acc: 7.9% (788/10000)
[Test]  Epoch: 35	Loss: 0.069675	Acc: 7.2% (725/10000)
[Test]  Epoch: 36	Loss: 0.069125	Acc: 7.8% (775/10000)
[Test]  Epoch: 37	Loss: 0.068877	Acc: 7.9% (790/10000)
[Test]  Epoch: 38	Loss: 0.069229	Acc: 7.7% (765/10000)
[Test]  Epoch: 39	Loss: 0.068863	Acc: 7.9% (794/10000)
[Test]  Epoch: 40	Loss: 0.068973	Acc: 7.7% (768/10000)
[Test]  Epoch: 41	Loss: 0.068814	Acc: 7.8% (776/10000)
[Test]  Epoch: 42	Loss: 0.068864	Acc: 7.7% (768/10000)
[Test]  Epoch: 43	Loss: 0.069036	Acc: 7.8% (778/10000)
[Test]  Epoch: 44	Loss: 0.068935	Acc: 7.7% (772/10000)
[Test]  Epoch: 45	Loss: 0.068527	Acc: 8.3% (828/10000)
[Test]  Epoch: 46	Loss: 0.068811	Acc: 7.7% (772/10000)
[Test]  Epoch: 47	Loss: 0.068483	Acc: 8.3% (831/10000)
[Test]  Epoch: 48	Loss: 0.068605	Acc: 7.8% (781/10000)
[Test]  Epoch: 49	Loss: 0.068603	Acc: 8.0% (799/10000)
[Test]  Epoch: 50	Loss: 0.068700	Acc: 8.0% (797/10000)
[Test]  Epoch: 51	Loss: 0.068630	Acc: 8.1% (814/10000)
[Test]  Epoch: 52	Loss: 0.068566	Acc: 7.8% (782/10000)
[Test]  Epoch: 53	Loss: 0.068492	Acc: 8.1% (806/10000)
[Test]  Epoch: 54	Loss: 0.068802	Acc: 7.8% (785/10000)
[Test]  Epoch: 55	Loss: 0.068522	Acc: 8.2% (818/10000)
[Test]  Epoch: 56	Loss: 0.068653	Acc: 8.1% (811/10000)
[Test]  Epoch: 57	Loss: 0.068450	Acc: 8.2% (820/10000)
[Test]  Epoch: 58	Loss: 0.068399	Acc: 8.2% (815/10000)
[Test]  Epoch: 59	Loss: 0.068072	Acc: 8.1% (809/10000)
[Test]  Epoch: 60	Loss: 0.068514	Acc: 8.0% (803/10000)
[Test]  Epoch: 61	Loss: 0.068227	Acc: 8.2% (819/10000)
[Test]  Epoch: 62	Loss: 0.068246	Acc: 8.2% (821/10000)
[Test]  Epoch: 63	Loss: 0.068114	Acc: 8.1% (814/10000)
[Test]  Epoch: 64	Loss: 0.068183	Acc: 8.2% (822/10000)
[Test]  Epoch: 65	Loss: 0.068206	Acc: 8.2% (817/10000)
[Test]  Epoch: 66	Loss: 0.068183	Acc: 8.2% (822/10000)
[Test]  Epoch: 67	Loss: 0.068231	Acc: 8.2% (815/10000)
[Test]  Epoch: 68	Loss: 0.068293	Acc: 8.1% (814/10000)
[Test]  Epoch: 69	Loss: 0.068202	Acc: 8.2% (820/10000)
[Test]  Epoch: 70	Loss: 0.068258	Acc: 8.1% (808/10000)
[Test]  Epoch: 71	Loss: 0.068216	Acc: 8.1% (806/10000)
[Test]  Epoch: 72	Loss: 0.068292	Acc: 8.1% (809/10000)
[Test]  Epoch: 73	Loss: 0.068181	Acc: 8.1% (810/10000)
[Test]  Epoch: 74	Loss: 0.068130	Acc: 8.2% (818/10000)
[Test]  Epoch: 75	Loss: 0.068209	Acc: 8.1% (813/10000)
[Test]  Epoch: 76	Loss: 0.068155	Acc: 8.3% (828/10000)
[Test]  Epoch: 77	Loss: 0.068173	Acc: 8.2% (823/10000)
[Test]  Epoch: 78	Loss: 0.068217	Acc: 8.2% (823/10000)
[Test]  Epoch: 79	Loss: 0.068245	Acc: 8.2% (818/10000)
[Test]  Epoch: 80	Loss: 0.068149	Acc: 8.1% (814/10000)
[Test]  Epoch: 81	Loss: 0.068104	Acc: 8.1% (813/10000)
[Test]  Epoch: 82	Loss: 0.068239	Acc: 8.0% (800/10000)
[Test]  Epoch: 83	Loss: 0.068240	Acc: 8.1% (814/10000)
[Test]  Epoch: 84	Loss: 0.068206	Acc: 8.2% (815/10000)
[Test]  Epoch: 85	Loss: 0.068186	Acc: 8.2% (817/10000)
[Test]  Epoch: 86	Loss: 0.068213	Acc: 8.1% (813/10000)
[Test]  Epoch: 87	Loss: 0.068137	Acc: 8.1% (812/10000)
[Test]  Epoch: 88	Loss: 0.068221	Acc: 8.2% (816/10000)
[Test]  Epoch: 89	Loss: 0.068203	Acc: 8.1% (810/10000)
[Test]  Epoch: 90	Loss: 0.068227	Acc: 8.1% (805/10000)
[Test]  Epoch: 91	Loss: 0.068205	Acc: 8.2% (824/10000)
[Test]  Epoch: 92	Loss: 0.068116	Acc: 8.2% (818/10000)
[Test]  Epoch: 93	Loss: 0.068214	Acc: 8.1% (814/10000)
[Test]  Epoch: 94	Loss: 0.068177	Acc: 8.2% (820/10000)
[Test]  Epoch: 95	Loss: 0.068232	Acc: 8.1% (812/10000)
[Test]  Epoch: 96	Loss: 0.068123	Acc: 8.1% (806/10000)
[Test]  Epoch: 97	Loss: 0.068155	Acc: 8.1% (811/10000)
[Test]  Epoch: 98	Loss: 0.068151	Acc: 8.2% (818/10000)
[Test]  Epoch: 99	Loss: 0.068176	Acc: 8.1% (810/10000)
[Test]  Epoch: 100	Loss: 0.068173	Acc: 8.1% (813/10000)
===========finish==========
['2024-08-19', '03:51:14.106600', '100', 'test', '0.0681734973192215', '8.13', '8.31']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'bn1.weight', 'layer4.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer1.1.conv2.weight', 'layer4.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.downsample.0.weight', 'layer1.0.conv1.weight', 'layer2.1.conv1.weight', 'last_linear.weight', 'layer4.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer2.0.conv2.weight', 'layer4.0.conv1.weight', 'layer2.0.conv1.weight', 'conv1.weight', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.087206	Acc: 0.8% (76/10000)
[Test]  Epoch: 2	Loss: 0.082021	Acc: 2.5% (246/10000)
[Test]  Epoch: 3	Loss: 0.079224	Acc: 4.0% (402/10000)
[Test]  Epoch: 4	Loss: 0.077089	Acc: 5.2% (519/10000)
[Test]  Epoch: 5	Loss: 0.075463	Acc: 6.3% (632/10000)
[Test]  Epoch: 6	Loss: 0.074164	Acc: 6.8% (680/10000)
[Test]  Epoch: 7	Loss: 0.073075	Acc: 7.3% (729/10000)
[Test]  Epoch: 8	Loss: 0.071942	Acc: 7.9% (789/10000)
[Test]  Epoch: 9	Loss: 0.071087	Acc: 8.1% (810/10000)
[Test]  Epoch: 10	Loss: 0.070200	Acc: 8.7% (866/10000)
[Test]  Epoch: 11	Loss: 0.069448	Acc: 9.0% (900/10000)
[Test]  Epoch: 12	Loss: 0.068804	Acc: 9.3% (930/10000)
[Test]  Epoch: 13	Loss: 0.068218	Acc: 9.6% (961/10000)
[Test]  Epoch: 14	Loss: 0.067754	Acc: 9.8% (979/10000)
[Test]  Epoch: 15	Loss: 0.067345	Acc: 10.2% (1019/10000)
[Test]  Epoch: 16	Loss: 0.066982	Acc: 10.4% (1039/10000)
[Test]  Epoch: 17	Loss: 0.066597	Acc: 10.5% (1051/10000)
[Test]  Epoch: 18	Loss: 0.066399	Acc: 10.7% (1072/10000)
[Test]  Epoch: 19	Loss: 0.066084	Acc: 10.9% (1091/10000)
[Test]  Epoch: 20	Loss: 0.065651	Acc: 11.2% (1123/10000)
[Test]  Epoch: 21	Loss: 0.065436	Acc: 11.3% (1131/10000)
[Test]  Epoch: 22	Loss: 0.065267	Acc: 11.3% (1127/10000)
[Test]  Epoch: 23	Loss: 0.064959	Acc: 11.6% (1162/10000)
[Test]  Epoch: 24	Loss: 0.064757	Acc: 11.5% (1153/10000)
[Test]  Epoch: 25	Loss: 0.064678	Acc: 11.7% (1172/10000)
[Test]  Epoch: 26	Loss: 0.064428	Acc: 11.9% (1191/10000)
[Test]  Epoch: 27	Loss: 0.064239	Acc: 12.3% (1229/10000)
[Test]  Epoch: 28	Loss: 0.064091	Acc: 12.2% (1216/10000)
[Test]  Epoch: 29	Loss: 0.063832	Acc: 12.4% (1239/10000)
[Test]  Epoch: 30	Loss: 0.063823	Acc: 12.3% (1228/10000)
[Test]  Epoch: 31	Loss: 0.063765	Acc: 12.4% (1238/10000)
[Test]  Epoch: 32	Loss: 0.063608	Acc: 12.5% (1250/10000)
[Test]  Epoch: 33	Loss: 0.063450	Acc: 12.4% (1239/10000)
[Test]  Epoch: 34	Loss: 0.063333	Acc: 12.6% (1256/10000)
[Test]  Epoch: 35	Loss: 0.063161	Acc: 12.7% (1273/10000)
[Test]  Epoch: 36	Loss: 0.063103	Acc: 12.7% (1271/10000)
[Test]  Epoch: 37	Loss: 0.062990	Acc: 12.8% (1281/10000)
[Test]  Epoch: 38	Loss: 0.062846	Acc: 13.0% (1299/10000)
[Test]  Epoch: 39	Loss: 0.062817	Acc: 12.8% (1285/10000)
[Test]  Epoch: 40	Loss: 0.062699	Acc: 13.2% (1324/10000)
[Test]  Epoch: 41	Loss: 0.062786	Acc: 13.2% (1322/10000)
[Test]  Epoch: 42	Loss: 0.062564	Acc: 13.0% (1303/10000)
[Test]  Epoch: 43	Loss: 0.062514	Acc: 13.2% (1321/10000)
[Test]  Epoch: 44	Loss: 0.062430	Acc: 13.3% (1326/10000)
[Test]  Epoch: 45	Loss: 0.062404	Acc: 13.2% (1316/10000)
[Test]  Epoch: 46	Loss: 0.062345	Acc: 13.3% (1326/10000)
[Test]  Epoch: 47	Loss: 0.062258	Acc: 13.4% (1341/10000)
[Test]  Epoch: 48	Loss: 0.062086	Acc: 13.7% (1374/10000)
[Test]  Epoch: 49	Loss: 0.062124	Acc: 13.7% (1366/10000)
[Test]  Epoch: 50	Loss: 0.062105	Acc: 13.5% (1353/10000)
[Test]  Epoch: 51	Loss: 0.061976	Acc: 13.7% (1367/10000)
[Test]  Epoch: 52	Loss: 0.061886	Acc: 13.8% (1381/10000)
[Test]  Epoch: 53	Loss: 0.061920	Acc: 13.7% (1371/10000)
[Test]  Epoch: 54	Loss: 0.061914	Acc: 13.8% (1380/10000)
[Test]  Epoch: 55	Loss: 0.061898	Acc: 13.7% (1368/10000)
[Test]  Epoch: 56	Loss: 0.061803	Acc: 13.8% (1383/10000)
[Test]  Epoch: 57	Loss: 0.061886	Acc: 13.7% (1370/10000)
[Test]  Epoch: 58	Loss: 0.061601	Acc: 14.2% (1415/10000)
[Test]  Epoch: 59	Loss: 0.061558	Acc: 14.1% (1410/10000)
[Test]  Epoch: 60	Loss: 0.061600	Acc: 14.1% (1405/10000)
[Test]  Epoch: 61	Loss: 0.061644	Acc: 14.0% (1397/10000)
[Test]  Epoch: 62	Loss: 0.061611	Acc: 14.1% (1410/10000)
[Test]  Epoch: 63	Loss: 0.061531	Acc: 14.1% (1410/10000)
[Test]  Epoch: 64	Loss: 0.061530	Acc: 14.3% (1428/10000)
[Test]  Epoch: 65	Loss: 0.061583	Acc: 14.0% (1403/10000)
[Test]  Epoch: 66	Loss: 0.061566	Acc: 13.9% (1391/10000)
[Test]  Epoch: 67	Loss: 0.061610	Acc: 14.1% (1405/10000)
[Test]  Epoch: 68	Loss: 0.061613	Acc: 14.0% (1402/10000)
[Test]  Epoch: 69	Loss: 0.061530	Acc: 14.2% (1422/10000)
[Test]  Epoch: 70	Loss: 0.061529	Acc: 14.2% (1422/10000)
[Test]  Epoch: 71	Loss: 0.061534	Acc: 14.2% (1417/10000)
[Test]  Epoch: 72	Loss: 0.061590	Acc: 14.0% (1401/10000)
[Test]  Epoch: 73	Loss: 0.061535	Acc: 14.1% (1413/10000)
[Test]  Epoch: 74	Loss: 0.061463	Acc: 14.1% (1408/10000)
[Test]  Epoch: 75	Loss: 0.061489	Acc: 14.0% (1400/10000)
[Test]  Epoch: 76	Loss: 0.061512	Acc: 14.2% (1416/10000)
[Test]  Epoch: 77	Loss: 0.061515	Acc: 14.2% (1417/10000)
[Test]  Epoch: 78	Loss: 0.061515	Acc: 14.0% (1396/10000)
[Test]  Epoch: 79	Loss: 0.061499	Acc: 14.0% (1403/10000)
[Test]  Epoch: 80	Loss: 0.061446	Acc: 14.2% (1416/10000)
[Test]  Epoch: 81	Loss: 0.061422	Acc: 14.1% (1409/10000)
[Test]  Epoch: 82	Loss: 0.061485	Acc: 14.3% (1431/10000)
[Test]  Epoch: 83	Loss: 0.061455	Acc: 14.1% (1408/10000)
[Test]  Epoch: 84	Loss: 0.061463	Acc: 14.2% (1415/10000)
[Test]  Epoch: 85	Loss: 0.061490	Acc: 14.2% (1418/10000)
[Test]  Epoch: 86	Loss: 0.061440	Acc: 14.2% (1417/10000)
[Test]  Epoch: 87	Loss: 0.061421	Acc: 14.2% (1418/10000)
[Test]  Epoch: 88	Loss: 0.061491	Acc: 14.3% (1431/10000)
[Test]  Epoch: 89	Loss: 0.061449	Acc: 14.2% (1420/10000)
[Test]  Epoch: 90	Loss: 0.061450	Acc: 14.1% (1412/10000)
[Test]  Epoch: 91	Loss: 0.061431	Acc: 14.3% (1427/10000)
[Test]  Epoch: 92	Loss: 0.061373	Acc: 14.3% (1427/10000)
[Test]  Epoch: 93	Loss: 0.061398	Acc: 14.3% (1427/10000)
[Test]  Epoch: 94	Loss: 0.061381	Acc: 14.2% (1425/10000)
[Test]  Epoch: 95	Loss: 0.061478	Acc: 14.1% (1406/10000)
[Test]  Epoch: 96	Loss: 0.061420	Acc: 14.2% (1420/10000)
[Test]  Epoch: 97	Loss: 0.061430	Acc: 14.1% (1412/10000)
[Test]  Epoch: 98	Loss: 0.061478	Acc: 14.1% (1412/10000)
[Test]  Epoch: 99	Loss: 0.061389	Acc: 14.1% (1407/10000)
[Test]  Epoch: 100	Loss: 0.061377	Acc: 14.2% (1420/10000)
===========finish==========
['2024-08-19', '03:55:28.134357', '100', 'test', '0.061376829504966736', '14.2', '14.31']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039146	Acc: 45.2% (4519/10000)
[Test]  Epoch: 2	Loss: 0.038963	Acc: 45.9% (4585/10000)
[Test]  Epoch: 3	Loss: 0.039221	Acc: 45.6% (4564/10000)
[Test]  Epoch: 4	Loss: 0.039423	Acc: 45.4% (4535/10000)
[Test]  Epoch: 5	Loss: 0.039472	Acc: 45.4% (4544/10000)
[Test]  Epoch: 6	Loss: 0.039669	Acc: 45.5% (4548/10000)
[Test]  Epoch: 7	Loss: 0.039654	Acc: 45.4% (4538/10000)
[Test]  Epoch: 8	Loss: 0.039850	Acc: 45.0% (4496/10000)
[Test]  Epoch: 9	Loss: 0.039947	Acc: 45.0% (4497/10000)
[Test]  Epoch: 10	Loss: 0.040075	Acc: 45.0% (4502/10000)
[Test]  Epoch: 11	Loss: 0.040164	Acc: 45.0% (4499/10000)
[Test]  Epoch: 12	Loss: 0.040052	Acc: 45.1% (4514/10000)
[Test]  Epoch: 13	Loss: 0.040199	Acc: 45.2% (4517/10000)
[Test]  Epoch: 14	Loss: 0.040232	Acc: 45.0% (4505/10000)
[Test]  Epoch: 15	Loss: 0.040411	Acc: 45.0% (4498/10000)
[Test]  Epoch: 16	Loss: 0.040169	Acc: 45.1% (4514/10000)
[Test]  Epoch: 17	Loss: 0.040336	Acc: 45.0% (4501/10000)
[Test]  Epoch: 18	Loss: 0.040379	Acc: 45.0% (4499/10000)
[Test]  Epoch: 19	Loss: 0.040375	Acc: 44.9% (4488/10000)
[Test]  Epoch: 20	Loss: 0.040494	Acc: 45.0% (4501/10000)
[Test]  Epoch: 21	Loss: 0.040526	Acc: 45.2% (4524/10000)
[Test]  Epoch: 22	Loss: 0.040607	Acc: 44.8% (4478/10000)
[Test]  Epoch: 23	Loss: 0.040616	Acc: 44.9% (4490/10000)
[Test]  Epoch: 24	Loss: 0.040673	Acc: 44.8% (4482/10000)
[Test]  Epoch: 25	Loss: 0.040606	Acc: 45.1% (4508/10000)
[Test]  Epoch: 26	Loss: 0.040759	Acc: 44.8% (4478/10000)
[Test]  Epoch: 27	Loss: 0.040591	Acc: 45.1% (4509/10000)
[Test]  Epoch: 28	Loss: 0.040849	Acc: 44.6% (4462/10000)
[Test]  Epoch: 29	Loss: 0.040749	Acc: 44.8% (4477/10000)
[Test]  Epoch: 30	Loss: 0.040879	Acc: 44.7% (4468/10000)
[Test]  Epoch: 31	Loss: 0.041032	Acc: 44.9% (4485/10000)
[Test]  Epoch: 32	Loss: 0.040931	Acc: 44.8% (4478/10000)
[Test]  Epoch: 33	Loss: 0.040785	Acc: 44.9% (4492/10000)
[Test]  Epoch: 34	Loss: 0.040868	Acc: 44.8% (4480/10000)
[Test]  Epoch: 35	Loss: 0.040860	Acc: 44.9% (4491/10000)
[Test]  Epoch: 36	Loss: 0.040803	Acc: 44.9% (4492/10000)
[Test]  Epoch: 37	Loss: 0.040922	Acc: 45.0% (4500/10000)
[Test]  Epoch: 38	Loss: 0.041045	Acc: 44.7% (4466/10000)
[Test]  Epoch: 39	Loss: 0.041061	Acc: 44.7% (4467/10000)
[Test]  Epoch: 40	Loss: 0.040993	Acc: 44.6% (4465/10000)
[Test]  Epoch: 41	Loss: 0.041058	Acc: 44.8% (4478/10000)
[Test]  Epoch: 42	Loss: 0.040999	Acc: 45.0% (4495/10000)
[Test]  Epoch: 43	Loss: 0.040998	Acc: 44.9% (4485/10000)
[Test]  Epoch: 44	Loss: 0.041049	Acc: 44.9% (4487/10000)
[Test]  Epoch: 45	Loss: 0.041100	Acc: 44.8% (4484/10000)
[Test]  Epoch: 46	Loss: 0.041145	Acc: 44.7% (4469/10000)
[Test]  Epoch: 47	Loss: 0.041215	Acc: 44.8% (4483/10000)
[Test]  Epoch: 48	Loss: 0.041164	Acc: 44.8% (4483/10000)
[Test]  Epoch: 49	Loss: 0.041196	Acc: 44.8% (4476/10000)
[Test]  Epoch: 50	Loss: 0.041252	Acc: 44.6% (4456/10000)
[Test]  Epoch: 51	Loss: 0.041139	Acc: 44.7% (4474/10000)
[Test]  Epoch: 52	Loss: 0.041059	Acc: 44.9% (4486/10000)
[Test]  Epoch: 53	Loss: 0.041183	Acc: 44.8% (4477/10000)
[Test]  Epoch: 54	Loss: 0.041329	Acc: 44.7% (4466/10000)
[Test]  Epoch: 55	Loss: 0.041293	Acc: 44.6% (4463/10000)
[Test]  Epoch: 56	Loss: 0.041276	Acc: 44.6% (4459/10000)
[Test]  Epoch: 57	Loss: 0.041373	Acc: 44.4% (4436/10000)
[Test]  Epoch: 58	Loss: 0.041149	Acc: 44.7% (4469/10000)
[Test]  Epoch: 59	Loss: 0.041278	Acc: 44.6% (4462/10000)
[Test]  Epoch: 60	Loss: 0.041240	Acc: 44.7% (4471/10000)
[Test]  Epoch: 61	Loss: 0.041354	Acc: 44.7% (4474/10000)
[Test]  Epoch: 62	Loss: 0.041357	Acc: 44.5% (4452/10000)
[Test]  Epoch: 63	Loss: 0.041148	Acc: 45.0% (4495/10000)
[Test]  Epoch: 64	Loss: 0.041179	Acc: 44.8% (4481/10000)
[Test]  Epoch: 65	Loss: 0.041288	Acc: 44.5% (4450/10000)
[Test]  Epoch: 66	Loss: 0.041248	Acc: 44.8% (4484/10000)
[Test]  Epoch: 67	Loss: 0.041313	Acc: 44.6% (4462/10000)
[Test]  Epoch: 68	Loss: 0.041423	Acc: 44.4% (4440/10000)
[Test]  Epoch: 69	Loss: 0.041355	Acc: 44.5% (4448/10000)
[Test]  Epoch: 70	Loss: 0.041337	Acc: 44.6% (4457/10000)
[Test]  Epoch: 71	Loss: 0.041293	Acc: 44.7% (4466/10000)
[Test]  Epoch: 72	Loss: 0.041367	Acc: 44.4% (4444/10000)
[Test]  Epoch: 73	Loss: 0.041216	Acc: 44.6% (4463/10000)
[Test]  Epoch: 74	Loss: 0.041309	Acc: 44.5% (4453/10000)
[Test]  Epoch: 75	Loss: 0.041385	Acc: 44.6% (4460/10000)
[Test]  Epoch: 76	Loss: 0.041229	Acc: 44.7% (4468/10000)
[Test]  Epoch: 77	Loss: 0.041345	Acc: 44.6% (4463/10000)
[Test]  Epoch: 78	Loss: 0.041401	Acc: 44.4% (4439/10000)
[Test]  Epoch: 79	Loss: 0.041464	Acc: 44.5% (4447/10000)
[Test]  Epoch: 80	Loss: 0.041284	Acc: 44.9% (4487/10000)
[Test]  Epoch: 81	Loss: 0.041382	Acc: 44.7% (4467/10000)
[Test]  Epoch: 82	Loss: 0.041393	Acc: 44.6% (4462/10000)
[Test]  Epoch: 83	Loss: 0.041414	Acc: 44.5% (4445/10000)
[Test]  Epoch: 84	Loss: 0.041517	Acc: 44.5% (4452/10000)
[Test]  Epoch: 85	Loss: 0.041408	Acc: 44.4% (4443/10000)
[Test]  Epoch: 86	Loss: 0.041467	Acc: 44.4% (4443/10000)
[Test]  Epoch: 87	Loss: 0.041371	Acc: 44.4% (4441/10000)
[Test]  Epoch: 88	Loss: 0.041373	Acc: 44.5% (4455/10000)
[Test]  Epoch: 89	Loss: 0.041344	Acc: 44.5% (4445/10000)
[Test]  Epoch: 90	Loss: 0.041428	Acc: 44.4% (4438/10000)
[Test]  Epoch: 91	Loss: 0.041444	Acc: 44.6% (4459/10000)
[Test]  Epoch: 92	Loss: 0.041310	Acc: 44.6% (4459/10000)
[Test]  Epoch: 93	Loss: 0.041391	Acc: 44.4% (4444/10000)
[Test]  Epoch: 94	Loss: 0.041426	Acc: 44.5% (4454/10000)
[Test]  Epoch: 95	Loss: 0.041377	Acc: 44.6% (4456/10000)
[Test]  Epoch: 96	Loss: 0.041342	Acc: 44.8% (4478/10000)
[Test]  Epoch: 97	Loss: 0.041427	Acc: 44.6% (4463/10000)
[Test]  Epoch: 98	Loss: 0.041390	Acc: 44.5% (4449/10000)
[Test]  Epoch: 99	Loss: 0.041424	Acc: 44.4% (4440/10000)
[Test]  Epoch: 100	Loss: 0.041363	Acc: 44.5% (4445/10000)
===========finish==========
['2024-08-19', '04:00:00.465975', '100', 'test', '0.04136283494234085', '44.45', '45.85']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.057285	Acc: 23.0% (2303/10000)
[Test]  Epoch: 2	Loss: 0.045654	Acc: 33.8% (3379/10000)
[Test]  Epoch: 3	Loss: 0.044387	Acc: 35.2% (3522/10000)
[Test]  Epoch: 4	Loss: 0.043572	Acc: 36.9% (3686/10000)
[Test]  Epoch: 5	Loss: 0.043378	Acc: 37.1% (3709/10000)
[Test]  Epoch: 6	Loss: 0.042989	Acc: 38.2% (3823/10000)
[Test]  Epoch: 7	Loss: 0.043067	Acc: 38.2% (3817/10000)
[Test]  Epoch: 8	Loss: 0.043019	Acc: 38.1% (3815/10000)
[Test]  Epoch: 9	Loss: 0.043080	Acc: 38.8% (3881/10000)
[Test]  Epoch: 10	Loss: 0.043238	Acc: 38.5% (3847/10000)
[Test]  Epoch: 11	Loss: 0.043400	Acc: 38.7% (3869/10000)
[Test]  Epoch: 12	Loss: 0.043203	Acc: 39.2% (3924/10000)
[Test]  Epoch: 13	Loss: 0.043224	Acc: 39.1% (3906/10000)
[Test]  Epoch: 14	Loss: 0.043121	Acc: 39.3% (3934/10000)
[Test]  Epoch: 15	Loss: 0.043264	Acc: 39.4% (3940/10000)
[Test]  Epoch: 16	Loss: 0.043069	Acc: 39.3% (3927/10000)
[Test]  Epoch: 17	Loss: 0.043290	Acc: 39.4% (3938/10000)
[Test]  Epoch: 18	Loss: 0.043115	Acc: 39.5% (3954/10000)
[Test]  Epoch: 19	Loss: 0.043004	Acc: 39.8% (3978/10000)
[Test]  Epoch: 20	Loss: 0.043318	Acc: 40.1% (4008/10000)
[Test]  Epoch: 21	Loss: 0.043072	Acc: 39.9% (3987/10000)
[Test]  Epoch: 22	Loss: 0.043500	Acc: 39.6% (3958/10000)
[Test]  Epoch: 23	Loss: 0.043449	Acc: 39.9% (3991/10000)
[Test]  Epoch: 24	Loss: 0.043237	Acc: 40.2% (4023/10000)
[Test]  Epoch: 25	Loss: 0.043091	Acc: 40.0% (4001/10000)
[Test]  Epoch: 26	Loss: 0.043454	Acc: 39.9% (3992/10000)
[Test]  Epoch: 27	Loss: 0.043015	Acc: 39.8% (3982/10000)
[Test]  Epoch: 28	Loss: 0.043255	Acc: 40.0% (3997/10000)
[Test]  Epoch: 29	Loss: 0.043173	Acc: 40.1% (4013/10000)
[Test]  Epoch: 30	Loss: 0.043449	Acc: 39.9% (3988/10000)
[Test]  Epoch: 31	Loss: 0.043542	Acc: 40.1% (4007/10000)
[Test]  Epoch: 32	Loss: 0.043310	Acc: 40.2% (4021/10000)
[Test]  Epoch: 33	Loss: 0.043471	Acc: 40.2% (4018/10000)
[Test]  Epoch: 34	Loss: 0.043371	Acc: 40.4% (4040/10000)
[Test]  Epoch: 35	Loss: 0.043261	Acc: 40.2% (4024/10000)
[Test]  Epoch: 36	Loss: 0.043418	Acc: 40.2% (4018/10000)
[Test]  Epoch: 37	Loss: 0.043553	Acc: 40.6% (4064/10000)
[Test]  Epoch: 38	Loss: 0.043571	Acc: 40.7% (4066/10000)
[Test]  Epoch: 39	Loss: 0.043729	Acc: 40.2% (4017/10000)
[Test]  Epoch: 40	Loss: 0.043494	Acc: 40.6% (4060/10000)
[Test]  Epoch: 41	Loss: 0.043566	Acc: 40.6% (4061/10000)
[Test]  Epoch: 42	Loss: 0.043268	Acc: 40.8% (4079/10000)
[Test]  Epoch: 43	Loss: 0.043143	Acc: 41.0% (4100/10000)
[Test]  Epoch: 44	Loss: 0.043728	Acc: 40.5% (4048/10000)
[Test]  Epoch: 45	Loss: 0.043488	Acc: 40.7% (4071/10000)
[Test]  Epoch: 46	Loss: 0.043585	Acc: 40.5% (4055/10000)
[Test]  Epoch: 47	Loss: 0.043656	Acc: 40.6% (4065/10000)
[Test]  Epoch: 48	Loss: 0.043955	Acc: 40.5% (4048/10000)
[Test]  Epoch: 49	Loss: 0.043770	Acc: 40.5% (4054/10000)
[Test]  Epoch: 50	Loss: 0.043574	Acc: 40.7% (4074/10000)
[Test]  Epoch: 51	Loss: 0.043610	Acc: 40.7% (4067/10000)
[Test]  Epoch: 52	Loss: 0.043592	Acc: 40.8% (4083/10000)
[Test]  Epoch: 53	Loss: 0.043775	Acc: 40.7% (4073/10000)
[Test]  Epoch: 54	Loss: 0.043905	Acc: 40.7% (4068/10000)
[Test]  Epoch: 55	Loss: 0.043623	Acc: 40.9% (4092/10000)
[Test]  Epoch: 56	Loss: 0.043707	Acc: 40.7% (4068/10000)
[Test]  Epoch: 57	Loss: 0.043599	Acc: 40.8% (4075/10000)
[Test]  Epoch: 58	Loss: 0.043581	Acc: 40.9% (4093/10000)
[Test]  Epoch: 59	Loss: 0.043731	Acc: 41.1% (4114/10000)
[Test]  Epoch: 60	Loss: 0.043791	Acc: 40.5% (4053/10000)
[Test]  Epoch: 61	Loss: 0.043866	Acc: 40.5% (4054/10000)
[Test]  Epoch: 62	Loss: 0.043846	Acc: 40.5% (4047/10000)
[Test]  Epoch: 63	Loss: 0.043689	Acc: 40.8% (4075/10000)
[Test]  Epoch: 64	Loss: 0.043674	Acc: 40.7% (4073/10000)
[Test]  Epoch: 65	Loss: 0.043771	Acc: 40.8% (4081/10000)
[Test]  Epoch: 66	Loss: 0.043717	Acc: 40.9% (4087/10000)
[Test]  Epoch: 67	Loss: 0.043754	Acc: 41.0% (4096/10000)
[Test]  Epoch: 68	Loss: 0.043893	Acc: 40.8% (4081/10000)
[Test]  Epoch: 69	Loss: 0.043835	Acc: 40.7% (4069/10000)
[Test]  Epoch: 70	Loss: 0.043853	Acc: 40.7% (4071/10000)
[Test]  Epoch: 71	Loss: 0.043729	Acc: 40.8% (4078/10000)
[Test]  Epoch: 72	Loss: 0.043805	Acc: 40.8% (4077/10000)
[Test]  Epoch: 73	Loss: 0.043723	Acc: 40.8% (4079/10000)
[Test]  Epoch: 74	Loss: 0.043777	Acc: 40.7% (4073/10000)
[Test]  Epoch: 75	Loss: 0.043827	Acc: 40.8% (4076/10000)
[Test]  Epoch: 76	Loss: 0.043708	Acc: 41.0% (4096/10000)
[Test]  Epoch: 77	Loss: 0.043863	Acc: 40.8% (4083/10000)
[Test]  Epoch: 78	Loss: 0.043809	Acc: 40.8% (4079/10000)
[Test]  Epoch: 79	Loss: 0.043859	Acc: 40.6% (4061/10000)
[Test]  Epoch: 80	Loss: 0.043704	Acc: 40.9% (4088/10000)
[Test]  Epoch: 81	Loss: 0.043803	Acc: 40.8% (4075/10000)
[Test]  Epoch: 82	Loss: 0.043821	Acc: 40.7% (4074/10000)
[Test]  Epoch: 83	Loss: 0.043905	Acc: 40.6% (4059/10000)
[Test]  Epoch: 84	Loss: 0.044008	Acc: 40.6% (4057/10000)
[Test]  Epoch: 85	Loss: 0.043876	Acc: 40.8% (4084/10000)
[Test]  Epoch: 86	Loss: 0.043919	Acc: 40.8% (4080/10000)
[Test]  Epoch: 87	Loss: 0.043795	Acc: 40.8% (4076/10000)
[Test]  Epoch: 88	Loss: 0.043841	Acc: 40.7% (4074/10000)
[Test]  Epoch: 89	Loss: 0.043792	Acc: 40.8% (4079/10000)
[Test]  Epoch: 90	Loss: 0.043873	Acc: 40.8% (4077/10000)
[Test]  Epoch: 91	Loss: 0.043859	Acc: 40.8% (4078/10000)
[Test]  Epoch: 92	Loss: 0.043753	Acc: 40.8% (4084/10000)
[Test]  Epoch: 93	Loss: 0.043865	Acc: 40.9% (4087/10000)
[Test]  Epoch: 94	Loss: 0.043980	Acc: 40.8% (4079/10000)
[Test]  Epoch: 95	Loss: 0.043843	Acc: 40.7% (4073/10000)
[Test]  Epoch: 96	Loss: 0.043762	Acc: 40.8% (4084/10000)
[Test]  Epoch: 97	Loss: 0.043833	Acc: 40.8% (4082/10000)
[Test]  Epoch: 98	Loss: 0.043811	Acc: 40.9% (4091/10000)
[Test]  Epoch: 99	Loss: 0.043867	Acc: 40.7% (4071/10000)
[Test]  Epoch: 100	Loss: 0.043781	Acc: 40.9% (4088/10000)
===========finish==========
['2024-08-19', '04:05:52.281120', '100', 'test', '0.043780616652965544', '40.88', '41.14']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.071958	Acc: 12.8% (1285/10000)
[Test]  Epoch: 2	Loss: 0.050477	Acc: 27.6% (2757/10000)
[Test]  Epoch: 3	Loss: 0.047152	Acc: 31.8% (3179/10000)
[Test]  Epoch: 4	Loss: 0.045711	Acc: 33.8% (3380/10000)
[Test]  Epoch: 5	Loss: 0.046161	Acc: 34.0% (3401/10000)
[Test]  Epoch: 6	Loss: 0.045764	Acc: 35.0% (3500/10000)
[Test]  Epoch: 7	Loss: 0.045659	Acc: 35.2% (3519/10000)
[Test]  Epoch: 8	Loss: 0.045619	Acc: 35.4% (3543/10000)
[Test]  Epoch: 9	Loss: 0.045665	Acc: 35.9% (3585/10000)
[Test]  Epoch: 10	Loss: 0.045645	Acc: 36.2% (3622/10000)
[Test]  Epoch: 11	Loss: 0.045539	Acc: 36.4% (3643/10000)
[Test]  Epoch: 12	Loss: 0.045397	Acc: 36.8% (3682/10000)
[Test]  Epoch: 13	Loss: 0.045593	Acc: 36.8% (3681/10000)
[Test]  Epoch: 14	Loss: 0.045283	Acc: 36.7% (3666/10000)
[Test]  Epoch: 15	Loss: 0.045490	Acc: 37.0% (3704/10000)
[Test]  Epoch: 16	Loss: 0.044997	Acc: 37.6% (3756/10000)
[Test]  Epoch: 17	Loss: 0.045358	Acc: 37.5% (3747/10000)
[Test]  Epoch: 18	Loss: 0.045224	Acc: 37.5% (3749/10000)
[Test]  Epoch: 19	Loss: 0.045323	Acc: 37.6% (3759/10000)
[Test]  Epoch: 20	Loss: 0.045486	Acc: 37.5% (3753/10000)
[Test]  Epoch: 21	Loss: 0.045324	Acc: 37.9% (3787/10000)
[Test]  Epoch: 22	Loss: 0.045512	Acc: 37.9% (3786/10000)
[Test]  Epoch: 23	Loss: 0.045477	Acc: 38.2% (3825/10000)
[Test]  Epoch: 24	Loss: 0.045382	Acc: 38.1% (3806/10000)
[Test]  Epoch: 25	Loss: 0.045178	Acc: 38.2% (3817/10000)
[Test]  Epoch: 26	Loss: 0.045376	Acc: 38.1% (3807/10000)
[Test]  Epoch: 27	Loss: 0.045155	Acc: 38.3% (3830/10000)
[Test]  Epoch: 28	Loss: 0.045334	Acc: 38.8% (3877/10000)
[Test]  Epoch: 29	Loss: 0.045093	Acc: 38.5% (3853/10000)
[Test]  Epoch: 30	Loss: 0.045286	Acc: 38.6% (3858/10000)
[Test]  Epoch: 31	Loss: 0.045423	Acc: 38.6% (3857/10000)
[Test]  Epoch: 32	Loss: 0.045164	Acc: 38.8% (3884/10000)
[Test]  Epoch: 33	Loss: 0.045248	Acc: 38.8% (3876/10000)
[Test]  Epoch: 34	Loss: 0.045251	Acc: 39.0% (3895/10000)
[Test]  Epoch: 35	Loss: 0.045232	Acc: 38.7% (3873/10000)
[Test]  Epoch: 36	Loss: 0.045297	Acc: 38.7% (3872/10000)
[Test]  Epoch: 37	Loss: 0.045430	Acc: 38.9% (3894/10000)
[Test]  Epoch: 38	Loss: 0.045348	Acc: 39.1% (3907/10000)
[Test]  Epoch: 39	Loss: 0.045505	Acc: 39.0% (3899/10000)
[Test]  Epoch: 40	Loss: 0.045230	Acc: 39.1% (3908/10000)
[Test]  Epoch: 41	Loss: 0.045321	Acc: 39.0% (3900/10000)
[Test]  Epoch: 42	Loss: 0.045098	Acc: 39.5% (3953/10000)
[Test]  Epoch: 43	Loss: 0.044894	Acc: 39.8% (3983/10000)
[Test]  Epoch: 44	Loss: 0.045388	Acc: 39.3% (3927/10000)
[Test]  Epoch: 45	Loss: 0.045249	Acc: 39.4% (3944/10000)
[Test]  Epoch: 46	Loss: 0.045300	Acc: 39.2% (3923/10000)
[Test]  Epoch: 47	Loss: 0.045319	Acc: 39.3% (3932/10000)
[Test]  Epoch: 48	Loss: 0.045529	Acc: 39.2% (3918/10000)
[Test]  Epoch: 49	Loss: 0.045351	Acc: 39.5% (3945/10000)
[Test]  Epoch: 50	Loss: 0.045294	Acc: 39.5% (3947/10000)
[Test]  Epoch: 51	Loss: 0.045178	Acc: 40.0% (3996/10000)
[Test]  Epoch: 52	Loss: 0.045197	Acc: 39.7% (3970/10000)
[Test]  Epoch: 53	Loss: 0.045463	Acc: 39.6% (3956/10000)
[Test]  Epoch: 54	Loss: 0.045427	Acc: 39.6% (3964/10000)
[Test]  Epoch: 55	Loss: 0.045185	Acc: 39.7% (3974/10000)
[Test]  Epoch: 56	Loss: 0.045296	Acc: 39.7% (3966/10000)
[Test]  Epoch: 57	Loss: 0.045184	Acc: 39.8% (3976/10000)
[Test]  Epoch: 58	Loss: 0.045118	Acc: 39.9% (3986/10000)
[Test]  Epoch: 59	Loss: 0.045341	Acc: 39.8% (3983/10000)
[Test]  Epoch: 60	Loss: 0.045270	Acc: 39.6% (3957/10000)
[Test]  Epoch: 61	Loss: 0.045333	Acc: 39.5% (3954/10000)
[Test]  Epoch: 62	Loss: 0.045454	Acc: 39.5% (3950/10000)
[Test]  Epoch: 63	Loss: 0.045235	Acc: 39.8% (3982/10000)
[Test]  Epoch: 64	Loss: 0.045233	Acc: 39.7% (3971/10000)
[Test]  Epoch: 65	Loss: 0.045319	Acc: 39.7% (3969/10000)
[Test]  Epoch: 66	Loss: 0.045283	Acc: 39.8% (3983/10000)
[Test]  Epoch: 67	Loss: 0.045312	Acc: 39.7% (3969/10000)
[Test]  Epoch: 68	Loss: 0.045438	Acc: 39.4% (3937/10000)
[Test]  Epoch: 69	Loss: 0.045379	Acc: 39.6% (3958/10000)
[Test]  Epoch: 70	Loss: 0.045385	Acc: 39.7% (3966/10000)
[Test]  Epoch: 71	Loss: 0.045327	Acc: 39.9% (3989/10000)
[Test]  Epoch: 72	Loss: 0.045394	Acc: 39.6% (3961/10000)
[Test]  Epoch: 73	Loss: 0.045299	Acc: 39.6% (3961/10000)
[Test]  Epoch: 74	Loss: 0.045389	Acc: 39.6% (3963/10000)
[Test]  Epoch: 75	Loss: 0.045401	Acc: 39.6% (3964/10000)
[Test]  Epoch: 76	Loss: 0.045262	Acc: 40.1% (4009/10000)
[Test]  Epoch: 77	Loss: 0.045435	Acc: 39.5% (3955/10000)
[Test]  Epoch: 78	Loss: 0.045446	Acc: 39.4% (3942/10000)
[Test]  Epoch: 79	Loss: 0.045457	Acc: 39.5% (3951/10000)
[Test]  Epoch: 80	Loss: 0.045389	Acc: 39.8% (3982/10000)
[Test]  Epoch: 81	Loss: 0.045364	Acc: 39.8% (3978/10000)
[Test]  Epoch: 82	Loss: 0.045422	Acc: 39.5% (3946/10000)
[Test]  Epoch: 83	Loss: 0.045428	Acc: 39.5% (3950/10000)
[Test]  Epoch: 84	Loss: 0.045517	Acc: 39.6% (3959/10000)
[Test]  Epoch: 85	Loss: 0.045393	Acc: 39.7% (3969/10000)
[Test]  Epoch: 86	Loss: 0.045441	Acc: 39.6% (3963/10000)
[Test]  Epoch: 87	Loss: 0.045412	Acc: 39.8% (3975/10000)
[Test]  Epoch: 88	Loss: 0.045430	Acc: 39.7% (3966/10000)
[Test]  Epoch: 89	Loss: 0.045340	Acc: 39.7% (3971/10000)
[Test]  Epoch: 90	Loss: 0.045454	Acc: 39.5% (3954/10000)
[Test]  Epoch: 91	Loss: 0.045435	Acc: 39.6% (3958/10000)
[Test]  Epoch: 92	Loss: 0.045309	Acc: 39.8% (3975/10000)
[Test]  Epoch: 93	Loss: 0.045450	Acc: 39.8% (3982/10000)
[Test]  Epoch: 94	Loss: 0.045546	Acc: 39.7% (3966/10000)
[Test]  Epoch: 95	Loss: 0.045395	Acc: 39.6% (3965/10000)
[Test]  Epoch: 96	Loss: 0.045242	Acc: 40.0% (4000/10000)
[Test]  Epoch: 97	Loss: 0.045318	Acc: 40.0% (3997/10000)
[Test]  Epoch: 98	Loss: 0.045324	Acc: 39.9% (3987/10000)
[Test]  Epoch: 99	Loss: 0.045382	Acc: 39.7% (3974/10000)
[Test]  Epoch: 100	Loss: 0.045319	Acc: 39.8% (3975/10000)
===========finish==========
['2024-08-19', '04:10:52.184124', '100', 'test', '0.04531890711784363', '39.75', '40.09']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.13.conv.0.1.weight', '_features.15.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.5.conv.1.1.weight', '_features.15.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.5.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.081103	Acc: 8.3% (831/10000)
[Test]  Epoch: 2	Loss: 0.053799	Acc: 23.9% (2390/10000)
[Test]  Epoch: 3	Loss: 0.048863	Acc: 29.3% (2934/10000)
[Test]  Epoch: 4	Loss: 0.047726	Acc: 31.0% (3099/10000)
[Test]  Epoch: 5	Loss: 0.048383	Acc: 30.6% (3065/10000)
[Test]  Epoch: 6	Loss: 0.047635	Acc: 32.1% (3211/10000)
[Test]  Epoch: 7	Loss: 0.047695	Acc: 32.0% (3202/10000)
[Test]  Epoch: 8	Loss: 0.047624	Acc: 32.6% (3265/10000)
[Test]  Epoch: 9	Loss: 0.047557	Acc: 33.1% (3315/10000)
[Test]  Epoch: 10	Loss: 0.047402	Acc: 33.2% (3321/10000)
[Test]  Epoch: 11	Loss: 0.047315	Acc: 33.5% (3347/10000)
[Test]  Epoch: 12	Loss: 0.047271	Acc: 33.7% (3368/10000)
[Test]  Epoch: 13	Loss: 0.047409	Acc: 33.8% (3375/10000)
[Test]  Epoch: 14	Loss: 0.047165	Acc: 34.1% (3409/10000)
[Test]  Epoch: 15	Loss: 0.047244	Acc: 34.1% (3414/10000)
[Test]  Epoch: 16	Loss: 0.047034	Acc: 34.6% (3456/10000)
[Test]  Epoch: 17	Loss: 0.047161	Acc: 34.5% (3447/10000)
[Test]  Epoch: 18	Loss: 0.047078	Acc: 34.5% (3454/10000)
[Test]  Epoch: 19	Loss: 0.047116	Acc: 34.8% (3483/10000)
[Test]  Epoch: 20	Loss: 0.047229	Acc: 34.5% (3448/10000)
[Test]  Epoch: 21	Loss: 0.046910	Acc: 35.0% (3498/10000)
[Test]  Epoch: 22	Loss: 0.047211	Acc: 35.0% (3500/10000)
[Test]  Epoch: 23	Loss: 0.047160	Acc: 35.0% (3505/10000)
[Test]  Epoch: 24	Loss: 0.047059	Acc: 35.3% (3528/10000)
[Test]  Epoch: 25	Loss: 0.046923	Acc: 35.2% (3524/10000)
[Test]  Epoch: 26	Loss: 0.046978	Acc: 35.0% (3495/10000)
[Test]  Epoch: 27	Loss: 0.046842	Acc: 35.3% (3531/10000)
[Test]  Epoch: 28	Loss: 0.047033	Acc: 35.4% (3536/10000)
[Test]  Epoch: 29	Loss: 0.046902	Acc: 35.4% (3539/10000)
[Test]  Epoch: 30	Loss: 0.047090	Acc: 35.5% (3549/10000)
[Test]  Epoch: 31	Loss: 0.047341	Acc: 35.2% (3521/10000)
[Test]  Epoch: 32	Loss: 0.046913	Acc: 35.7% (3567/10000)
[Test]  Epoch: 33	Loss: 0.046941	Acc: 35.4% (3543/10000)
[Test]  Epoch: 34	Loss: 0.046885	Acc: 35.6% (3563/10000)
[Test]  Epoch: 35	Loss: 0.046894	Acc: 35.6% (3558/10000)
[Test]  Epoch: 36	Loss: 0.047006	Acc: 35.8% (3576/10000)
[Test]  Epoch: 37	Loss: 0.046939	Acc: 35.9% (3591/10000)
[Test]  Epoch: 38	Loss: 0.047120	Acc: 35.8% (3581/10000)
[Test]  Epoch: 39	Loss: 0.047227	Acc: 35.8% (3577/10000)
[Test]  Epoch: 40	Loss: 0.047022	Acc: 35.7% (3571/10000)
[Test]  Epoch: 41	Loss: 0.046906	Acc: 35.8% (3579/10000)
[Test]  Epoch: 42	Loss: 0.046607	Acc: 36.4% (3640/10000)
[Test]  Epoch: 43	Loss: 0.046636	Acc: 36.9% (3692/10000)
[Test]  Epoch: 44	Loss: 0.047106	Acc: 36.1% (3610/10000)
[Test]  Epoch: 45	Loss: 0.046830	Acc: 36.3% (3627/10000)
[Test]  Epoch: 46	Loss: 0.046872	Acc: 35.6% (3562/10000)
[Test]  Epoch: 47	Loss: 0.046921	Acc: 36.2% (3622/10000)
[Test]  Epoch: 48	Loss: 0.047133	Acc: 36.0% (3596/10000)
[Test]  Epoch: 49	Loss: 0.046907	Acc: 36.3% (3634/10000)
[Test]  Epoch: 50	Loss: 0.046955	Acc: 35.9% (3588/10000)
[Test]  Epoch: 51	Loss: 0.046912	Acc: 36.5% (3652/10000)
[Test]  Epoch: 52	Loss: 0.046777	Acc: 36.9% (3685/10000)
[Test]  Epoch: 53	Loss: 0.047711	Acc: 35.6% (3556/10000)
[Test]  Epoch: 54	Loss: 0.047099	Acc: 36.6% (3662/10000)
[Test]  Epoch: 55	Loss: 0.046827	Acc: 36.5% (3654/10000)
[Test]  Epoch: 56	Loss: 0.046981	Acc: 36.4% (3640/10000)
[Test]  Epoch: 57	Loss: 0.046812	Acc: 36.7% (3674/10000)
[Test]  Epoch: 58	Loss: 0.046630	Acc: 37.2% (3717/10000)
[Test]  Epoch: 59	Loss: 0.046807	Acc: 36.7% (3669/10000)
[Test]  Epoch: 60	Loss: 0.046858	Acc: 36.7% (3674/10000)
[Test]  Epoch: 61	Loss: 0.046924	Acc: 36.8% (3679/10000)
[Test]  Epoch: 62	Loss: 0.046979	Acc: 36.9% (3691/10000)
[Test]  Epoch: 63	Loss: 0.046758	Acc: 37.0% (3702/10000)
[Test]  Epoch: 64	Loss: 0.046800	Acc: 36.9% (3691/10000)
[Test]  Epoch: 65	Loss: 0.046869	Acc: 36.7% (3673/10000)
[Test]  Epoch: 66	Loss: 0.046876	Acc: 37.0% (3695/10000)
[Test]  Epoch: 67	Loss: 0.046836	Acc: 36.9% (3690/10000)
[Test]  Epoch: 68	Loss: 0.046987	Acc: 36.8% (3684/10000)
[Test]  Epoch: 69	Loss: 0.046927	Acc: 36.8% (3681/10000)
[Test]  Epoch: 70	Loss: 0.046925	Acc: 36.7% (3674/10000)
[Test]  Epoch: 71	Loss: 0.046880	Acc: 36.8% (3678/10000)
[Test]  Epoch: 72	Loss: 0.046961	Acc: 36.6% (3661/10000)
[Test]  Epoch: 73	Loss: 0.046901	Acc: 36.8% (3684/10000)
[Test]  Epoch: 74	Loss: 0.046921	Acc: 36.9% (3690/10000)
[Test]  Epoch: 75	Loss: 0.046945	Acc: 36.7% (3667/10000)
[Test]  Epoch: 76	Loss: 0.046802	Acc: 37.0% (3698/10000)
[Test]  Epoch: 77	Loss: 0.046984	Acc: 36.7% (3667/10000)
[Test]  Epoch: 78	Loss: 0.046924	Acc: 36.7% (3668/10000)
[Test]  Epoch: 79	Loss: 0.046982	Acc: 36.9% (3685/10000)
[Test]  Epoch: 80	Loss: 0.046858	Acc: 36.9% (3691/10000)
[Test]  Epoch: 81	Loss: 0.046922	Acc: 36.9% (3690/10000)
[Test]  Epoch: 82	Loss: 0.046962	Acc: 36.7% (3671/10000)
[Test]  Epoch: 83	Loss: 0.046999	Acc: 36.6% (3665/10000)
[Test]  Epoch: 84	Loss: 0.047135	Acc: 36.7% (3666/10000)
[Test]  Epoch: 85	Loss: 0.047000	Acc: 36.8% (3683/10000)
[Test]  Epoch: 86	Loss: 0.046969	Acc: 36.6% (3656/10000)
[Test]  Epoch: 87	Loss: 0.046954	Acc: 36.9% (3685/10000)
[Test]  Epoch: 88	Loss: 0.046949	Acc: 36.8% (3681/10000)
[Test]  Epoch: 89	Loss: 0.046860	Acc: 36.8% (3680/10000)
[Test]  Epoch: 90	Loss: 0.047014	Acc: 36.9% (3685/10000)
[Test]  Epoch: 91	Loss: 0.046941	Acc: 36.9% (3691/10000)
[Test]  Epoch: 92	Loss: 0.046806	Acc: 37.0% (3699/10000)
[Test]  Epoch: 93	Loss: 0.046975	Acc: 36.9% (3685/10000)
[Test]  Epoch: 94	Loss: 0.046993	Acc: 36.7% (3670/10000)
[Test]  Epoch: 95	Loss: 0.046940	Acc: 36.8% (3678/10000)
[Test]  Epoch: 96	Loss: 0.046850	Acc: 37.2% (3723/10000)
[Test]  Epoch: 97	Loss: 0.046874	Acc: 36.8% (3682/10000)
[Test]  Epoch: 98	Loss: 0.046849	Acc: 36.8% (3679/10000)
[Test]  Epoch: 99	Loss: 0.046937	Acc: 36.8% (3683/10000)
[Test]  Epoch: 100	Loss: 0.046857	Acc: 37.0% (3701/10000)
===========finish==========
['2024-08-19', '04:15:49.925970', '100', 'test', '0.046856600201129917', '37.01', '37.23']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.13.conv.0.1.weight', '_features.15.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.5.conv.1.1.weight', '_features.15.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.13.conv.3.weight', '_features.13.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.3.weight', '_features.12.conv.1.0.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.10.conv.2.weight', '_features.9.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.106633	Acc: 4.2% (424/10000)
[Test]  Epoch: 2	Loss: 0.058124	Acc: 18.9% (1890/10000)
[Test]  Epoch: 3	Loss: 0.052063	Acc: 24.7% (2472/10000)
[Test]  Epoch: 4	Loss: 0.050690	Acc: 27.0% (2697/10000)
[Test]  Epoch: 5	Loss: 0.050958	Acc: 27.6% (2759/10000)
[Test]  Epoch: 6	Loss: 0.049766	Acc: 28.7% (2866/10000)
[Test]  Epoch: 7	Loss: 0.049850	Acc: 29.3% (2933/10000)
[Test]  Epoch: 8	Loss: 0.049909	Acc: 29.3% (2928/10000)
[Test]  Epoch: 9	Loss: 0.050228	Acc: 29.3% (2931/10000)
[Test]  Epoch: 10	Loss: 0.049943	Acc: 29.5% (2951/10000)
[Test]  Epoch: 11	Loss: 0.049651	Acc: 30.0% (2996/10000)
[Test]  Epoch: 12	Loss: 0.050016	Acc: 30.0% (3004/10000)
[Test]  Epoch: 13	Loss: 0.049521	Acc: 31.1% (3108/10000)
[Test]  Epoch: 14	Loss: 0.049510	Acc: 30.8% (3081/10000)
[Test]  Epoch: 15	Loss: 0.049480	Acc: 31.2% (3119/10000)
[Test]  Epoch: 16	Loss: 0.049313	Acc: 31.6% (3165/10000)
[Test]  Epoch: 17	Loss: 0.049582	Acc: 31.1% (3107/10000)
[Test]  Epoch: 18	Loss: 0.049307	Acc: 31.4% (3141/10000)
[Test]  Epoch: 19	Loss: 0.049358	Acc: 31.8% (3183/10000)
[Test]  Epoch: 20	Loss: 0.049769	Acc: 31.2% (3122/10000)
[Test]  Epoch: 21	Loss: 0.049445	Acc: 31.4% (3139/10000)
[Test]  Epoch: 22	Loss: 0.049706	Acc: 31.8% (3182/10000)
[Test]  Epoch: 23	Loss: 0.049416	Acc: 31.8% (3175/10000)
[Test]  Epoch: 24	Loss: 0.049288	Acc: 32.5% (3246/10000)
[Test]  Epoch: 25	Loss: 0.049188	Acc: 32.3% (3233/10000)
[Test]  Epoch: 26	Loss: 0.049200	Acc: 32.5% (3250/10000)
[Test]  Epoch: 27	Loss: 0.049128	Acc: 32.1% (3212/10000)
[Test]  Epoch: 28	Loss: 0.049228	Acc: 32.5% (3254/10000)
[Test]  Epoch: 29	Loss: 0.049152	Acc: 32.4% (3237/10000)
[Test]  Epoch: 30	Loss: 0.049432	Acc: 32.3% (3227/10000)
[Test]  Epoch: 31	Loss: 0.049315	Acc: 32.7% (3271/10000)
[Test]  Epoch: 32	Loss: 0.049009	Acc: 33.0% (3303/10000)
[Test]  Epoch: 33	Loss: 0.049244	Acc: 32.6% (3265/10000)
[Test]  Epoch: 34	Loss: 0.049367	Acc: 32.7% (3274/10000)
[Test]  Epoch: 35	Loss: 0.049017	Acc: 33.1% (3312/10000)
[Test]  Epoch: 36	Loss: 0.049244	Acc: 32.8% (3282/10000)
[Test]  Epoch: 37	Loss: 0.049281	Acc: 33.2% (3321/10000)
[Test]  Epoch: 38	Loss: 0.049327	Acc: 32.8% (3278/10000)
[Test]  Epoch: 39	Loss: 0.049493	Acc: 32.9% (3287/10000)
[Test]  Epoch: 40	Loss: 0.049207	Acc: 33.0% (3295/10000)
[Test]  Epoch: 41	Loss: 0.049136	Acc: 33.1% (3315/10000)
[Test]  Epoch: 42	Loss: 0.048887	Acc: 33.7% (3366/10000)
[Test]  Epoch: 43	Loss: 0.048811	Acc: 33.7% (3373/10000)
[Test]  Epoch: 44	Loss: 0.049406	Acc: 33.0% (3299/10000)
[Test]  Epoch: 45	Loss: 0.049131	Acc: 33.4% (3336/10000)
[Test]  Epoch: 46	Loss: 0.049029	Acc: 33.7% (3374/10000)
[Test]  Epoch: 47	Loss: 0.049200	Acc: 33.5% (3353/10000)
[Test]  Epoch: 48	Loss: 0.049299	Acc: 33.6% (3362/10000)
[Test]  Epoch: 49	Loss: 0.049265	Acc: 33.6% (3359/10000)
[Test]  Epoch: 50	Loss: 0.049203	Acc: 33.4% (3335/10000)
[Test]  Epoch: 51	Loss: 0.049054	Acc: 33.9% (3389/10000)
[Test]  Epoch: 52	Loss: 0.048989	Acc: 34.1% (3414/10000)
[Test]  Epoch: 53	Loss: 0.049424	Acc: 33.5% (3354/10000)
[Test]  Epoch: 54	Loss: 0.049275	Acc: 33.8% (3375/10000)
[Test]  Epoch: 55	Loss: 0.049005	Acc: 33.7% (3369/10000)
[Test]  Epoch: 56	Loss: 0.049131	Acc: 33.8% (3379/10000)
[Test]  Epoch: 57	Loss: 0.049143	Acc: 33.6% (3363/10000)
[Test]  Epoch: 58	Loss: 0.048869	Acc: 34.1% (3411/10000)
[Test]  Epoch: 59	Loss: 0.049240	Acc: 34.0% (3403/10000)
[Test]  Epoch: 60	Loss: 0.049349	Acc: 33.5% (3353/10000)
[Test]  Epoch: 61	Loss: 0.049285	Acc: 33.9% (3391/10000)
[Test]  Epoch: 62	Loss: 0.049283	Acc: 33.8% (3381/10000)
[Test]  Epoch: 63	Loss: 0.049033	Acc: 34.0% (3404/10000)
[Test]  Epoch: 64	Loss: 0.049016	Acc: 34.0% (3404/10000)
[Test]  Epoch: 65	Loss: 0.049132	Acc: 34.0% (3403/10000)
[Test]  Epoch: 66	Loss: 0.049111	Acc: 34.0% (3399/10000)
[Test]  Epoch: 67	Loss: 0.049119	Acc: 34.0% (3400/10000)
[Test]  Epoch: 68	Loss: 0.049202	Acc: 34.1% (3406/10000)
[Test]  Epoch: 69	Loss: 0.049125	Acc: 33.9% (3392/10000)
[Test]  Epoch: 70	Loss: 0.049172	Acc: 33.9% (3385/10000)
[Test]  Epoch: 71	Loss: 0.049077	Acc: 34.2% (3418/10000)
[Test]  Epoch: 72	Loss: 0.049207	Acc: 33.9% (3391/10000)
[Test]  Epoch: 73	Loss: 0.049084	Acc: 34.1% (3407/10000)
[Test]  Epoch: 74	Loss: 0.049185	Acc: 34.0% (3397/10000)
[Test]  Epoch: 75	Loss: 0.049209	Acc: 34.0% (3404/10000)
[Test]  Epoch: 76	Loss: 0.049116	Acc: 34.2% (3425/10000)
[Test]  Epoch: 77	Loss: 0.049195	Acc: 34.1% (3407/10000)
[Test]  Epoch: 78	Loss: 0.049222	Acc: 33.8% (3384/10000)
[Test]  Epoch: 79	Loss: 0.049265	Acc: 34.0% (3397/10000)
[Test]  Epoch: 80	Loss: 0.049062	Acc: 34.3% (3433/10000)
[Test]  Epoch: 81	Loss: 0.049142	Acc: 34.1% (3414/10000)
[Test]  Epoch: 82	Loss: 0.049202	Acc: 34.1% (3410/10000)
[Test]  Epoch: 83	Loss: 0.049195	Acc: 33.9% (3388/10000)
[Test]  Epoch: 84	Loss: 0.049319	Acc: 34.1% (3406/10000)
[Test]  Epoch: 85	Loss: 0.049276	Acc: 33.9% (3392/10000)
[Test]  Epoch: 86	Loss: 0.049229	Acc: 33.8% (3376/10000)
[Test]  Epoch: 87	Loss: 0.049219	Acc: 34.0% (3403/10000)
[Test]  Epoch: 88	Loss: 0.049243	Acc: 33.9% (3390/10000)
[Test]  Epoch: 89	Loss: 0.049184	Acc: 34.0% (3405/10000)
[Test]  Epoch: 90	Loss: 0.049265	Acc: 33.9% (3388/10000)
[Test]  Epoch: 91	Loss: 0.049221	Acc: 33.9% (3386/10000)
[Test]  Epoch: 92	Loss: 0.049065	Acc: 34.0% (3404/10000)
[Test]  Epoch: 93	Loss: 0.049161	Acc: 34.0% (3402/10000)
[Test]  Epoch: 94	Loss: 0.049283	Acc: 33.9% (3391/10000)
[Test]  Epoch: 95	Loss: 0.049185	Acc: 34.0% (3397/10000)
[Test]  Epoch: 96	Loss: 0.049090	Acc: 34.1% (3409/10000)
[Test]  Epoch: 97	Loss: 0.049136	Acc: 34.0% (3401/10000)
[Test]  Epoch: 98	Loss: 0.049025	Acc: 34.1% (3412/10000)
[Test]  Epoch: 99	Loss: 0.049123	Acc: 34.1% (3412/10000)
[Test]  Epoch: 100	Loss: 0.049015	Acc: 34.2% (3417/10000)
===========finish==========
['2024-08-19', '04:20:50.544177', '100', 'test', '0.04901455812454224', '34.17', '34.33']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.13.conv.0.1.weight', '_features.15.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.5.conv.1.1.weight', '_features.15.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.13.conv.3.weight', '_features.13.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.3.weight', '_features.12.conv.1.0.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.10.conv.2.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.2.weight', '_features.3.conv.3.weight', '_features.3.conv.1.1.weight', '_features.6.conv.1.0.weight', '_features.4.conv.3.weight', '_features.3.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.104197	Acc: 4.6% (456/10000)
[Test]  Epoch: 2	Loss: 0.055414	Acc: 21.9% (2194/10000)
[Test]  Epoch: 3	Loss: 0.050439	Acc: 26.9% (2685/10000)
[Test]  Epoch: 4	Loss: 0.049558	Acc: 28.4% (2841/10000)
[Test]  Epoch: 5	Loss: 0.049765	Acc: 28.9% (2891/10000)
[Test]  Epoch: 6	Loss: 0.049159	Acc: 29.6% (2965/10000)
[Test]  Epoch: 7	Loss: 0.048666	Acc: 30.7% (3067/10000)
[Test]  Epoch: 8	Loss: 0.048773	Acc: 30.6% (3058/10000)
[Test]  Epoch: 9	Loss: 0.048863	Acc: 30.5% (3051/10000)
[Test]  Epoch: 10	Loss: 0.048623	Acc: 31.2% (3120/10000)
[Test]  Epoch: 11	Loss: 0.048866	Acc: 31.0% (3101/10000)
[Test]  Epoch: 12	Loss: 0.048658	Acc: 31.4% (3139/10000)
[Test]  Epoch: 13	Loss: 0.048416	Acc: 32.1% (3214/10000)
[Test]  Epoch: 14	Loss: 0.048437	Acc: 32.1% (3213/10000)
[Test]  Epoch: 15	Loss: 0.048706	Acc: 31.5% (3152/10000)
[Test]  Epoch: 16	Loss: 0.048339	Acc: 32.0% (3199/10000)
[Test]  Epoch: 17	Loss: 0.048594	Acc: 32.1% (3212/10000)
[Test]  Epoch: 18	Loss: 0.048121	Acc: 32.9% (3294/10000)
[Test]  Epoch: 19	Loss: 0.048326	Acc: 32.7% (3266/10000)
[Test]  Epoch: 20	Loss: 0.048698	Acc: 32.4% (3244/10000)
[Test]  Epoch: 21	Loss: 0.048195	Acc: 32.7% (3269/10000)
[Test]  Epoch: 22	Loss: 0.048495	Acc: 33.0% (3298/10000)
[Test]  Epoch: 23	Loss: 0.048570	Acc: 32.9% (3288/10000)
[Test]  Epoch: 24	Loss: 0.048461	Acc: 33.1% (3306/10000)
[Test]  Epoch: 25	Loss: 0.048272	Acc: 33.1% (3314/10000)
[Test]  Epoch: 26	Loss: 0.048382	Acc: 33.1% (3313/10000)
[Test]  Epoch: 27	Loss: 0.048149	Acc: 33.2% (3316/10000)
[Test]  Epoch: 28	Loss: 0.048530	Acc: 33.1% (3310/10000)
[Test]  Epoch: 29	Loss: 0.047958	Acc: 33.8% (3380/10000)
[Test]  Epoch: 30	Loss: 0.048503	Acc: 33.3% (3329/10000)
[Test]  Epoch: 31	Loss: 0.048347	Acc: 33.5% (3345/10000)
[Test]  Epoch: 32	Loss: 0.048300	Acc: 33.9% (3388/10000)
[Test]  Epoch: 33	Loss: 0.048324	Acc: 33.4% (3339/10000)
[Test]  Epoch: 34	Loss: 0.048145	Acc: 33.7% (3366/10000)
[Test]  Epoch: 35	Loss: 0.048288	Acc: 33.7% (3367/10000)
[Test]  Epoch: 36	Loss: 0.048328	Acc: 33.7% (3372/10000)
[Test]  Epoch: 37	Loss: 0.048289	Acc: 34.1% (3413/10000)
[Test]  Epoch: 38	Loss: 0.048430	Acc: 33.5% (3349/10000)
[Test]  Epoch: 39	Loss: 0.048756	Acc: 33.5% (3350/10000)
[Test]  Epoch: 40	Loss: 0.048401	Acc: 34.0% (3405/10000)
[Test]  Epoch: 41	Loss: 0.048204	Acc: 34.1% (3407/10000)
[Test]  Epoch: 42	Loss: 0.048148	Acc: 34.4% (3443/10000)
[Test]  Epoch: 43	Loss: 0.048064	Acc: 34.4% (3442/10000)
[Test]  Epoch: 44	Loss: 0.048630	Acc: 33.9% (3387/10000)
[Test]  Epoch: 45	Loss: 0.048178	Acc: 34.4% (3438/10000)
[Test]  Epoch: 46	Loss: 0.048426	Acc: 34.0% (3395/10000)
[Test]  Epoch: 47	Loss: 0.048337	Acc: 34.2% (3424/10000)
[Test]  Epoch: 48	Loss: 0.048651	Acc: 33.7% (3369/10000)
[Test]  Epoch: 49	Loss: 0.048313	Acc: 34.5% (3452/10000)
[Test]  Epoch: 50	Loss: 0.048436	Acc: 34.4% (3443/10000)
[Test]  Epoch: 51	Loss: 0.048324	Acc: 34.4% (3439/10000)
[Test]  Epoch: 52	Loss: 0.048276	Acc: 34.5% (3445/10000)
[Test]  Epoch: 53	Loss: 0.048472	Acc: 34.5% (3449/10000)
[Test]  Epoch: 54	Loss: 0.048410	Acc: 34.5% (3450/10000)
[Test]  Epoch: 55	Loss: 0.048194	Acc: 34.6% (3465/10000)
[Test]  Epoch: 56	Loss: 0.048340	Acc: 34.5% (3445/10000)
[Test]  Epoch: 57	Loss: 0.048345	Acc: 34.4% (3442/10000)
[Test]  Epoch: 58	Loss: 0.048132	Acc: 34.7% (3471/10000)
[Test]  Epoch: 59	Loss: 0.048592	Acc: 34.3% (3428/10000)
[Test]  Epoch: 60	Loss: 0.048394	Acc: 34.8% (3480/10000)
[Test]  Epoch: 61	Loss: 0.048433	Acc: 34.8% (3477/10000)
[Test]  Epoch: 62	Loss: 0.048446	Acc: 34.7% (3466/10000)
[Test]  Epoch: 63	Loss: 0.048249	Acc: 34.9% (3493/10000)
[Test]  Epoch: 64	Loss: 0.048265	Acc: 35.0% (3497/10000)
[Test]  Epoch: 65	Loss: 0.048444	Acc: 34.6% (3465/10000)
[Test]  Epoch: 66	Loss: 0.048355	Acc: 34.9% (3485/10000)
[Test]  Epoch: 67	Loss: 0.048319	Acc: 34.9% (3487/10000)
[Test]  Epoch: 68	Loss: 0.048436	Acc: 34.9% (3485/10000)
[Test]  Epoch: 69	Loss: 0.048402	Acc: 34.9% (3488/10000)
[Test]  Epoch: 70	Loss: 0.048441	Acc: 34.8% (3482/10000)
[Test]  Epoch: 71	Loss: 0.048366	Acc: 34.8% (3481/10000)
[Test]  Epoch: 72	Loss: 0.048521	Acc: 34.6% (3462/10000)
[Test]  Epoch: 73	Loss: 0.048381	Acc: 34.6% (3460/10000)
[Test]  Epoch: 74	Loss: 0.048410	Acc: 34.6% (3460/10000)
[Test]  Epoch: 75	Loss: 0.048416	Acc: 34.8% (3475/10000)
[Test]  Epoch: 76	Loss: 0.048322	Acc: 34.8% (3484/10000)
[Test]  Epoch: 77	Loss: 0.048451	Acc: 34.7% (3474/10000)
[Test]  Epoch: 78	Loss: 0.048414	Acc: 34.7% (3467/10000)
[Test]  Epoch: 79	Loss: 0.048453	Acc: 34.8% (3475/10000)
[Test]  Epoch: 80	Loss: 0.048364	Acc: 34.6% (3459/10000)
[Test]  Epoch: 81	Loss: 0.048388	Acc: 34.7% (3467/10000)
[Test]  Epoch: 82	Loss: 0.048450	Acc: 34.7% (3473/10000)
[Test]  Epoch: 83	Loss: 0.048518	Acc: 34.7% (3466/10000)
[Test]  Epoch: 84	Loss: 0.048593	Acc: 34.6% (3459/10000)
[Test]  Epoch: 85	Loss: 0.048563	Acc: 34.5% (3454/10000)
[Test]  Epoch: 86	Loss: 0.048581	Acc: 34.6% (3460/10000)
[Test]  Epoch: 87	Loss: 0.048455	Acc: 34.7% (3466/10000)
[Test]  Epoch: 88	Loss: 0.048497	Acc: 34.8% (3480/10000)
[Test]  Epoch: 89	Loss: 0.048460	Acc: 34.7% (3470/10000)
[Test]  Epoch: 90	Loss: 0.048507	Acc: 34.8% (3478/10000)
[Test]  Epoch: 91	Loss: 0.048434	Acc: 34.6% (3462/10000)
[Test]  Epoch: 92	Loss: 0.048374	Acc: 34.8% (3478/10000)
[Test]  Epoch: 93	Loss: 0.048461	Acc: 34.7% (3469/10000)
[Test]  Epoch: 94	Loss: 0.048498	Acc: 34.6% (3463/10000)
[Test]  Epoch: 95	Loss: 0.048461	Acc: 34.6% (3459/10000)
[Test]  Epoch: 96	Loss: 0.048386	Acc: 34.8% (3481/10000)
[Test]  Epoch: 97	Loss: 0.048435	Acc: 34.6% (3458/10000)
[Test]  Epoch: 98	Loss: 0.048345	Acc: 34.7% (3466/10000)
[Test]  Epoch: 99	Loss: 0.048426	Acc: 34.6% (3457/10000)
[Test]  Epoch: 100	Loss: 0.048368	Acc: 34.8% (3481/10000)
===========finish==========
['2024-08-19', '04:25:45.392531', '100', 'test', '0.04836768342256546', '34.81', '34.97']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.13.conv.0.1.weight', '_features.15.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.5.conv.1.1.weight', '_features.15.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.13.conv.3.weight', '_features.13.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.3.weight', '_features.12.conv.1.0.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.10.conv.2.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.2.weight', '_features.3.conv.3.weight', '_features.3.conv.1.1.weight', '_features.6.conv.1.0.weight', '_features.4.conv.3.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.5.conv.0.0.weight', '_features.5.conv.1.0.weight', '_features.13.conv.2.weight', '_features.12.conv.2.weight', '_features.13.conv.0.0.weight', '_features.6.conv.2.weight', '_features.12.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.2.weight', '_features.14.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.150382	Acc: 2.6% (261/10000)
[Test]  Epoch: 2	Loss: 0.059511	Acc: 18.7% (1866/10000)
[Test]  Epoch: 3	Loss: 0.053215	Acc: 23.6% (2358/10000)
[Test]  Epoch: 4	Loss: 0.052141	Acc: 25.0% (2499/10000)
[Test]  Epoch: 5	Loss: 0.051784	Acc: 25.9% (2594/10000)
[Test]  Epoch: 6	Loss: 0.050826	Acc: 26.8% (2680/10000)
[Test]  Epoch: 7	Loss: 0.050657	Acc: 27.7% (2768/10000)
[Test]  Epoch: 8	Loss: 0.050607	Acc: 27.6% (2757/10000)
[Test]  Epoch: 9	Loss: 0.050695	Acc: 27.9% (2787/10000)
[Test]  Epoch: 10	Loss: 0.050426	Acc: 28.2% (2817/10000)
[Test]  Epoch: 11	Loss: 0.050372	Acc: 28.5% (2847/10000)
[Test]  Epoch: 12	Loss: 0.050509	Acc: 28.7% (2868/10000)
[Test]  Epoch: 13	Loss: 0.050293	Acc: 29.0% (2899/10000)
[Test]  Epoch: 14	Loss: 0.050255	Acc: 28.9% (2893/10000)
[Test]  Epoch: 15	Loss: 0.050218	Acc: 29.4% (2944/10000)
[Test]  Epoch: 16	Loss: 0.050290	Acc: 29.3% (2930/10000)
[Test]  Epoch: 17	Loss: 0.050231	Acc: 29.6% (2959/10000)
[Test]  Epoch: 18	Loss: 0.049942	Acc: 29.8% (2975/10000)
[Test]  Epoch: 19	Loss: 0.050087	Acc: 29.4% (2944/10000)
[Test]  Epoch: 20	Loss: 0.050495	Acc: 29.7% (2968/10000)
[Test]  Epoch: 21	Loss: 0.050053	Acc: 30.2% (3019/10000)
[Test]  Epoch: 22	Loss: 0.050162	Acc: 30.0% (3000/10000)
[Test]  Epoch: 23	Loss: 0.050280	Acc: 30.0% (3004/10000)
[Test]  Epoch: 24	Loss: 0.049993	Acc: 30.8% (3075/10000)
[Test]  Epoch: 25	Loss: 0.050031	Acc: 30.2% (3024/10000)
[Test]  Epoch: 26	Loss: 0.050011	Acc: 30.6% (3055/10000)
[Test]  Epoch: 27	Loss: 0.049447	Acc: 30.8% (3084/10000)
[Test]  Epoch: 28	Loss: 0.050438	Acc: 30.3% (3027/10000)
[Test]  Epoch: 29	Loss: 0.049621	Acc: 31.1% (3111/10000)
[Test]  Epoch: 30	Loss: 0.050273	Acc: 30.7% (3068/10000)
[Test]  Epoch: 31	Loss: 0.050207	Acc: 30.8% (3077/10000)
[Test]  Epoch: 32	Loss: 0.049791	Acc: 31.4% (3135/10000)
[Test]  Epoch: 33	Loss: 0.049978	Acc: 30.9% (3091/10000)
[Test]  Epoch: 34	Loss: 0.049875	Acc: 30.8% (3079/10000)
[Test]  Epoch: 35	Loss: 0.049777	Acc: 31.1% (3114/10000)
[Test]  Epoch: 36	Loss: 0.049922	Acc: 31.2% (3119/10000)
[Test]  Epoch: 37	Loss: 0.049693	Acc: 31.7% (3172/10000)
[Test]  Epoch: 38	Loss: 0.050077	Acc: 31.6% (3158/10000)
[Test]  Epoch: 39	Loss: 0.049965	Acc: 31.3% (3126/10000)
[Test]  Epoch: 40	Loss: 0.049766	Acc: 31.6% (3156/10000)
[Test]  Epoch: 41	Loss: 0.049677	Acc: 31.9% (3185/10000)
[Test]  Epoch: 42	Loss: 0.049633	Acc: 31.7% (3174/10000)
[Test]  Epoch: 43	Loss: 0.049754	Acc: 31.9% (3195/10000)
[Test]  Epoch: 44	Loss: 0.050202	Acc: 31.6% (3155/10000)
[Test]  Epoch: 45	Loss: 0.049709	Acc: 32.0% (3205/10000)
[Test]  Epoch: 46	Loss: 0.049907	Acc: 31.5% (3146/10000)
[Test]  Epoch: 47	Loss: 0.049765	Acc: 32.0% (3198/10000)
[Test]  Epoch: 48	Loss: 0.050162	Acc: 31.6% (3158/10000)
[Test]  Epoch: 49	Loss: 0.049810	Acc: 32.2% (3218/10000)
[Test]  Epoch: 50	Loss: 0.049635	Acc: 32.4% (3243/10000)
[Test]  Epoch: 51	Loss: 0.049664	Acc: 32.5% (3252/10000)
[Test]  Epoch: 52	Loss: 0.049724	Acc: 32.3% (3233/10000)
[Test]  Epoch: 53	Loss: 0.049805	Acc: 32.4% (3235/10000)
[Test]  Epoch: 54	Loss: 0.049828	Acc: 32.3% (3229/10000)
[Test]  Epoch: 55	Loss: 0.049649	Acc: 32.4% (3242/10000)
[Test]  Epoch: 56	Loss: 0.049882	Acc: 31.7% (3173/10000)
[Test]  Epoch: 57	Loss: 0.049713	Acc: 32.6% (3256/10000)
[Test]  Epoch: 58	Loss: 0.049712	Acc: 32.8% (3279/10000)
[Test]  Epoch: 59	Loss: 0.049812	Acc: 32.7% (3271/10000)
[Test]  Epoch: 60	Loss: 0.049833	Acc: 32.4% (3239/10000)
[Test]  Epoch: 61	Loss: 0.049830	Acc: 32.6% (3259/10000)
[Test]  Epoch: 62	Loss: 0.049818	Acc: 32.4% (3235/10000)
[Test]  Epoch: 63	Loss: 0.049623	Acc: 32.7% (3267/10000)
[Test]  Epoch: 64	Loss: 0.049722	Acc: 32.7% (3266/10000)
[Test]  Epoch: 65	Loss: 0.049786	Acc: 32.6% (3257/10000)
[Test]  Epoch: 66	Loss: 0.049729	Acc: 32.5% (3252/10000)
[Test]  Epoch: 67	Loss: 0.049716	Acc: 32.5% (3248/10000)
[Test]  Epoch: 68	Loss: 0.049838	Acc: 32.3% (3231/10000)
[Test]  Epoch: 69	Loss: 0.049822	Acc: 32.2% (3223/10000)
[Test]  Epoch: 70	Loss: 0.049870	Acc: 32.6% (3257/10000)
[Test]  Epoch: 71	Loss: 0.049784	Acc: 32.7% (3267/10000)
[Test]  Epoch: 72	Loss: 0.049862	Acc: 32.5% (3249/10000)
[Test]  Epoch: 73	Loss: 0.049726	Acc: 32.6% (3263/10000)
[Test]  Epoch: 74	Loss: 0.049788	Acc: 32.7% (3268/10000)
[Test]  Epoch: 75	Loss: 0.049690	Acc: 32.6% (3260/10000)
[Test]  Epoch: 76	Loss: 0.049675	Acc: 32.8% (3275/10000)
[Test]  Epoch: 77	Loss: 0.049721	Acc: 32.6% (3260/10000)
[Test]  Epoch: 78	Loss: 0.049722	Acc: 32.6% (3261/10000)
[Test]  Epoch: 79	Loss: 0.049784	Acc: 32.4% (3241/10000)
[Test]  Epoch: 80	Loss: 0.049748	Acc: 32.8% (3276/10000)
[Test]  Epoch: 81	Loss: 0.049758	Acc: 32.6% (3263/10000)
[Test]  Epoch: 82	Loss: 0.049715	Acc: 32.7% (3271/10000)
[Test]  Epoch: 83	Loss: 0.049795	Acc: 32.5% (3251/10000)
[Test]  Epoch: 84	Loss: 0.049878	Acc: 32.6% (3260/10000)
[Test]  Epoch: 85	Loss: 0.049778	Acc: 32.5% (3245/10000)
[Test]  Epoch: 86	Loss: 0.049820	Acc: 32.6% (3259/10000)
[Test]  Epoch: 87	Loss: 0.049746	Acc: 32.6% (3256/10000)
[Test]  Epoch: 88	Loss: 0.049789	Acc: 32.6% (3264/10000)
[Test]  Epoch: 89	Loss: 0.049679	Acc: 32.9% (3291/10000)
[Test]  Epoch: 90	Loss: 0.049710	Acc: 32.6% (3259/10000)
[Test]  Epoch: 91	Loss: 0.049726	Acc: 32.7% (3270/10000)
[Test]  Epoch: 92	Loss: 0.049660	Acc: 32.6% (3262/10000)
[Test]  Epoch: 93	Loss: 0.049772	Acc: 32.8% (3280/10000)
[Test]  Epoch: 94	Loss: 0.049839	Acc: 32.7% (3272/10000)
[Test]  Epoch: 95	Loss: 0.049713	Acc: 32.7% (3267/10000)
[Test]  Epoch: 96	Loss: 0.049585	Acc: 32.8% (3275/10000)
[Test]  Epoch: 97	Loss: 0.049767	Acc: 32.7% (3268/10000)
[Test]  Epoch: 98	Loss: 0.049718	Acc: 32.7% (3269/10000)
[Test]  Epoch: 99	Loss: 0.049721	Acc: 32.7% (3270/10000)
[Test]  Epoch: 100	Loss: 0.049670	Acc: 32.8% (3281/10000)
===========finish==========
['2024-08-19', '04:30:52.958640', '100', 'test', '0.0496695674777031', '32.81', '32.91']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.13.conv.0.1.weight', '_features.15.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.5.conv.1.1.weight', '_features.15.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.13.conv.3.weight', '_features.13.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.3.weight', '_features.12.conv.1.0.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.10.conv.2.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.2.weight', '_features.3.conv.3.weight', '_features.3.conv.1.1.weight', '_features.6.conv.1.0.weight', '_features.4.conv.3.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.5.conv.0.0.weight', '_features.5.conv.1.0.weight', '_features.13.conv.2.weight', '_features.12.conv.2.weight', '_features.13.conv.0.0.weight', '_features.6.conv.2.weight', '_features.12.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.2.weight', '_features.14.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.17.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.0.1.weight', '_features.17.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.164099	Acc: 2.9% (288/10000)
[Test]  Epoch: 2	Loss: 0.062333	Acc: 16.5% (1648/10000)
[Test]  Epoch: 3	Loss: 0.055873	Acc: 20.8% (2080/10000)
[Test]  Epoch: 4	Loss: 0.054404	Acc: 22.7% (2268/10000)
[Test]  Epoch: 5	Loss: 0.053888	Acc: 23.8% (2376/10000)
[Test]  Epoch: 6	Loss: 0.052871	Acc: 25.3% (2530/10000)
[Test]  Epoch: 7	Loss: 0.052729	Acc: 25.5% (2552/10000)
[Test]  Epoch: 8	Loss: 0.052393	Acc: 26.2% (2620/10000)
[Test]  Epoch: 9	Loss: 0.052110	Acc: 26.7% (2674/10000)
[Test]  Epoch: 10	Loss: 0.052176	Acc: 27.1% (2712/10000)
[Test]  Epoch: 11	Loss: 0.052243	Acc: 26.9% (2691/10000)
[Test]  Epoch: 12	Loss: 0.051882	Acc: 27.6% (2755/10000)
[Test]  Epoch: 13	Loss: 0.051801	Acc: 27.6% (2760/10000)
[Test]  Epoch: 14	Loss: 0.051586	Acc: 27.7% (2772/10000)
[Test]  Epoch: 15	Loss: 0.051644	Acc: 28.0% (2797/10000)
[Test]  Epoch: 16	Loss: 0.051307	Acc: 28.4% (2842/10000)
[Test]  Epoch: 17	Loss: 0.051675	Acc: 28.3% (2833/10000)
[Test]  Epoch: 18	Loss: 0.051179	Acc: 28.5% (2851/10000)
[Test]  Epoch: 19	Loss: 0.051259	Acc: 29.1% (2906/10000)
[Test]  Epoch: 20	Loss: 0.051404	Acc: 28.3% (2834/10000)
[Test]  Epoch: 21	Loss: 0.051248	Acc: 28.9% (2888/10000)
[Test]  Epoch: 22	Loss: 0.051355	Acc: 28.7% (2872/10000)
[Test]  Epoch: 23	Loss: 0.051395	Acc: 28.9% (2885/10000)
[Test]  Epoch: 24	Loss: 0.051236	Acc: 28.9% (2887/10000)
[Test]  Epoch: 25	Loss: 0.050937	Acc: 29.0% (2896/10000)
[Test]  Epoch: 26	Loss: 0.051224	Acc: 29.0% (2902/10000)
[Test]  Epoch: 27	Loss: 0.050995	Acc: 29.1% (2913/10000)
[Test]  Epoch: 28	Loss: 0.051111	Acc: 29.2% (2917/10000)
[Test]  Epoch: 29	Loss: 0.050959	Acc: 29.2% (2918/10000)
[Test]  Epoch: 30	Loss: 0.050968	Acc: 29.7% (2969/10000)
[Test]  Epoch: 31	Loss: 0.051063	Acc: 29.3% (2933/10000)
[Test]  Epoch: 32	Loss: 0.050895	Acc: 29.4% (2941/10000)
[Test]  Epoch: 33	Loss: 0.050918	Acc: 29.3% (2930/10000)
[Test]  Epoch: 34	Loss: 0.050886	Acc: 29.6% (2965/10000)
[Test]  Epoch: 35	Loss: 0.050910	Acc: 29.7% (2973/10000)
[Test]  Epoch: 36	Loss: 0.050784	Acc: 29.9% (2985/10000)
[Test]  Epoch: 37	Loss: 0.050853	Acc: 30.1% (3005/10000)
[Test]  Epoch: 38	Loss: 0.050970	Acc: 30.0% (2996/10000)
[Test]  Epoch: 39	Loss: 0.051112	Acc: 29.6% (2955/10000)
[Test]  Epoch: 40	Loss: 0.050891	Acc: 29.8% (2977/10000)
[Test]  Epoch: 41	Loss: 0.050690	Acc: 30.1% (3008/10000)
[Test]  Epoch: 42	Loss: 0.050616	Acc: 30.2% (3016/10000)
[Test]  Epoch: 43	Loss: 0.050632	Acc: 30.0% (2998/10000)
[Test]  Epoch: 44	Loss: 0.050994	Acc: 30.0% (3004/10000)
[Test]  Epoch: 45	Loss: 0.050778	Acc: 30.0% (3001/10000)
[Test]  Epoch: 46	Loss: 0.050576	Acc: 30.3% (3030/10000)
[Test]  Epoch: 47	Loss: 0.050804	Acc: 30.4% (3036/10000)
[Test]  Epoch: 48	Loss: 0.050971	Acc: 30.1% (3014/10000)
[Test]  Epoch: 49	Loss: 0.050724	Acc: 30.3% (3032/10000)
[Test]  Epoch: 50	Loss: 0.050716	Acc: 30.6% (3055/10000)
[Test]  Epoch: 51	Loss: 0.050637	Acc: 30.3% (3032/10000)
[Test]  Epoch: 52	Loss: 0.050728	Acc: 30.1% (3012/10000)
[Test]  Epoch: 53	Loss: 0.050832	Acc: 30.5% (3048/10000)
[Test]  Epoch: 54	Loss: 0.050732	Acc: 30.4% (3038/10000)
[Test]  Epoch: 55	Loss: 0.050606	Acc: 30.4% (3035/10000)
[Test]  Epoch: 56	Loss: 0.050820	Acc: 30.1% (3012/10000)
[Test]  Epoch: 57	Loss: 0.050579	Acc: 30.2% (3025/10000)
[Test]  Epoch: 58	Loss: 0.050612	Acc: 30.3% (3034/10000)
[Test]  Epoch: 59	Loss: 0.050763	Acc: 30.3% (3026/10000)
[Test]  Epoch: 60	Loss: 0.050767	Acc: 30.1% (3007/10000)
[Test]  Epoch: 61	Loss: 0.050725	Acc: 30.3% (3030/10000)
[Test]  Epoch: 62	Loss: 0.050802	Acc: 30.2% (3018/10000)
[Test]  Epoch: 63	Loss: 0.050574	Acc: 30.3% (3034/10000)
[Test]  Epoch: 64	Loss: 0.050556	Acc: 30.4% (3043/10000)
[Test]  Epoch: 65	Loss: 0.050611	Acc: 30.5% (3046/10000)
[Test]  Epoch: 66	Loss: 0.050698	Acc: 30.5% (3048/10000)
[Test]  Epoch: 67	Loss: 0.050669	Acc: 30.4% (3037/10000)
[Test]  Epoch: 68	Loss: 0.050719	Acc: 30.6% (3060/10000)
[Test]  Epoch: 69	Loss: 0.050744	Acc: 30.4% (3045/10000)
[Test]  Epoch: 70	Loss: 0.050711	Acc: 30.4% (3041/10000)
[Test]  Epoch: 71	Loss: 0.050624	Acc: 30.4% (3039/10000)
[Test]  Epoch: 72	Loss: 0.050710	Acc: 30.4% (3039/10000)
[Test]  Epoch: 73	Loss: 0.050664	Acc: 30.3% (3033/10000)
[Test]  Epoch: 74	Loss: 0.050597	Acc: 30.5% (3050/10000)
[Test]  Epoch: 75	Loss: 0.050695	Acc: 30.4% (3039/10000)
[Test]  Epoch: 76	Loss: 0.050641	Acc: 30.5% (3046/10000)
[Test]  Epoch: 77	Loss: 0.050754	Acc: 30.4% (3038/10000)
[Test]  Epoch: 78	Loss: 0.050731	Acc: 30.2% (3019/10000)
[Test]  Epoch: 79	Loss: 0.050741	Acc: 30.4% (3043/10000)
[Test]  Epoch: 80	Loss: 0.050595	Acc: 30.4% (3038/10000)
[Test]  Epoch: 81	Loss: 0.050664	Acc: 30.2% (3021/10000)
[Test]  Epoch: 82	Loss: 0.050655	Acc: 30.5% (3048/10000)
[Test]  Epoch: 83	Loss: 0.050737	Acc: 30.3% (3033/10000)
[Test]  Epoch: 84	Loss: 0.050791	Acc: 30.5% (3051/10000)
[Test]  Epoch: 85	Loss: 0.050741	Acc: 30.5% (3052/10000)
[Test]  Epoch: 86	Loss: 0.050758	Acc: 30.3% (3027/10000)
[Test]  Epoch: 87	Loss: 0.050709	Acc: 30.3% (3032/10000)
[Test]  Epoch: 88	Loss: 0.050756	Acc: 30.3% (3034/10000)
[Test]  Epoch: 89	Loss: 0.050658	Acc: 30.2% (3024/10000)
[Test]  Epoch: 90	Loss: 0.050768	Acc: 30.2% (3024/10000)
[Test]  Epoch: 91	Loss: 0.050733	Acc: 30.3% (3032/10000)
[Test]  Epoch: 92	Loss: 0.050587	Acc: 30.7% (3067/10000)
[Test]  Epoch: 93	Loss: 0.050717	Acc: 30.5% (3053/10000)
[Test]  Epoch: 94	Loss: 0.050745	Acc: 30.4% (3038/10000)
[Test]  Epoch: 95	Loss: 0.050623	Acc: 30.6% (3062/10000)
[Test]  Epoch: 96	Loss: 0.050511	Acc: 30.5% (3053/10000)
[Test]  Epoch: 97	Loss: 0.050653	Acc: 30.3% (3031/10000)
[Test]  Epoch: 98	Loss: 0.050554	Acc: 30.7% (3066/10000)
[Test]  Epoch: 99	Loss: 0.050614	Acc: 30.6% (3057/10000)
[Test]  Epoch: 100	Loss: 0.050564	Acc: 30.5% (3046/10000)
===========finish==========
['2024-08-19', '04:35:44.074878', '100', 'test', '0.05056409788131714', '30.46', '30.67']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.13.conv.0.1.weight', '_features.15.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.5.conv.1.1.weight', '_features.15.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.13.conv.3.weight', '_features.13.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.3.weight', '_features.12.conv.1.0.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.10.conv.2.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.2.weight', '_features.3.conv.3.weight', '_features.3.conv.1.1.weight', '_features.6.conv.1.0.weight', '_features.4.conv.3.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.5.conv.0.0.weight', '_features.5.conv.1.0.weight', '_features.13.conv.2.weight', '_features.12.conv.2.weight', '_features.13.conv.0.0.weight', '_features.6.conv.2.weight', '_features.12.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.2.weight', '_features.14.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.17.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.0.1.weight', '_features.17.conv.3.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.4.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.286303	Acc: 0.7% (68/10000)
[Test]  Epoch: 2	Loss: 0.066366	Acc: 14.1% (1412/10000)
[Test]  Epoch: 3	Loss: 0.056676	Acc: 19.1% (1909/10000)
[Test]  Epoch: 4	Loss: 0.054865	Acc: 21.8% (2179/10000)
[Test]  Epoch: 5	Loss: 0.054153	Acc: 22.9% (2286/10000)
[Test]  Epoch: 6	Loss: 0.053583	Acc: 23.6% (2362/10000)
[Test]  Epoch: 7	Loss: 0.053236	Acc: 24.5% (2447/10000)
[Test]  Epoch: 8	Loss: 0.053134	Acc: 24.7% (2467/10000)
[Test]  Epoch: 9	Loss: 0.052892	Acc: 25.4% (2545/10000)
[Test]  Epoch: 10	Loss: 0.053442	Acc: 25.1% (2509/10000)
[Test]  Epoch: 11	Loss: 0.052774	Acc: 25.6% (2564/10000)
[Test]  Epoch: 12	Loss: 0.052261	Acc: 26.5% (2653/10000)
[Test]  Epoch: 13	Loss: 0.052364	Acc: 26.3% (2630/10000)
[Test]  Epoch: 14	Loss: 0.052289	Acc: 26.4% (2640/10000)
[Test]  Epoch: 15	Loss: 0.052050	Acc: 26.7% (2672/10000)
[Test]  Epoch: 16	Loss: 0.052229	Acc: 26.7% (2667/10000)
[Test]  Epoch: 17	Loss: 0.052269	Acc: 26.5% (2654/10000)
[Test]  Epoch: 18	Loss: 0.051787	Acc: 26.8% (2681/10000)
[Test]  Epoch: 19	Loss: 0.051797	Acc: 27.4% (2736/10000)
[Test]  Epoch: 20	Loss: 0.051860	Acc: 27.5% (2746/10000)
[Test]  Epoch: 21	Loss: 0.051765	Acc: 27.4% (2735/10000)
[Test]  Epoch: 22	Loss: 0.051835	Acc: 27.5% (2747/10000)
[Test]  Epoch: 23	Loss: 0.052023	Acc: 27.1% (2714/10000)
[Test]  Epoch: 24	Loss: 0.052306	Acc: 27.4% (2736/10000)
[Test]  Epoch: 25	Loss: 0.051652	Acc: 27.5% (2754/10000)
[Test]  Epoch: 26	Loss: 0.051794	Acc: 27.6% (2763/10000)
[Test]  Epoch: 27	Loss: 0.051464	Acc: 27.8% (2779/10000)
[Test]  Epoch: 28	Loss: 0.051526	Acc: 27.7% (2770/10000)
[Test]  Epoch: 29	Loss: 0.051593	Acc: 27.0% (2704/10000)
[Test]  Epoch: 30	Loss: 0.051678	Acc: 27.6% (2759/10000)
[Test]  Epoch: 31	Loss: 0.051763	Acc: 28.1% (2815/10000)
[Test]  Epoch: 32	Loss: 0.051385	Acc: 27.7% (2768/10000)
[Test]  Epoch: 33	Loss: 0.051660	Acc: 27.6% (2762/10000)
[Test]  Epoch: 34	Loss: 0.051344	Acc: 28.6% (2856/10000)
[Test]  Epoch: 35	Loss: 0.051841	Acc: 27.9% (2794/10000)
[Test]  Epoch: 36	Loss: 0.051443	Acc: 28.2% (2818/10000)
[Test]  Epoch: 37	Loss: 0.051737	Acc: 28.8% (2876/10000)
[Test]  Epoch: 38	Loss: 0.051159	Acc: 28.8% (2880/10000)
[Test]  Epoch: 39	Loss: 0.051420	Acc: 28.9% (2886/10000)
[Test]  Epoch: 40	Loss: 0.051446	Acc: 28.6% (2856/10000)
[Test]  Epoch: 41	Loss: 0.051584	Acc: 29.0% (2903/10000)
[Test]  Epoch: 42	Loss: 0.051141	Acc: 28.9% (2889/10000)
[Test]  Epoch: 43	Loss: 0.050914	Acc: 29.2% (2916/10000)
[Test]  Epoch: 44	Loss: 0.050972	Acc: 29.2% (2923/10000)
[Test]  Epoch: 45	Loss: 0.051284	Acc: 28.8% (2883/10000)
[Test]  Epoch: 46	Loss: 0.051183	Acc: 28.8% (2884/10000)
[Test]  Epoch: 47	Loss: 0.051094	Acc: 29.2% (2920/10000)
[Test]  Epoch: 48	Loss: 0.051437	Acc: 28.9% (2888/10000)
[Test]  Epoch: 49	Loss: 0.051186	Acc: 29.1% (2914/10000)
[Test]  Epoch: 50	Loss: 0.050998	Acc: 29.8% (2976/10000)
[Test]  Epoch: 51	Loss: 0.051345	Acc: 28.6% (2860/10000)
[Test]  Epoch: 52	Loss: 0.050978	Acc: 29.2% (2923/10000)
[Test]  Epoch: 53	Loss: 0.051166	Acc: 29.1% (2910/10000)
[Test]  Epoch: 54	Loss: 0.051140	Acc: 29.2% (2920/10000)
[Test]  Epoch: 55	Loss: 0.050767	Acc: 29.5% (2947/10000)
[Test]  Epoch: 56	Loss: 0.051140	Acc: 29.0% (2904/10000)
[Test]  Epoch: 57	Loss: 0.051172	Acc: 29.1% (2906/10000)
[Test]  Epoch: 58	Loss: 0.051168	Acc: 29.3% (2931/10000)
[Test]  Epoch: 59	Loss: 0.051040	Acc: 28.9% (2892/10000)
[Test]  Epoch: 60	Loss: 0.050983	Acc: 29.4% (2936/10000)
[Test]  Epoch: 61	Loss: 0.051036	Acc: 29.6% (2957/10000)
[Test]  Epoch: 62	Loss: 0.051130	Acc: 29.4% (2944/10000)
[Test]  Epoch: 63	Loss: 0.050827	Acc: 29.6% (2955/10000)
[Test]  Epoch: 64	Loss: 0.050900	Acc: 29.8% (2975/10000)
[Test]  Epoch: 65	Loss: 0.051099	Acc: 29.5% (2947/10000)
[Test]  Epoch: 66	Loss: 0.051037	Acc: 29.9% (2989/10000)
[Test]  Epoch: 67	Loss: 0.051046	Acc: 29.7% (2972/10000)
[Test]  Epoch: 68	Loss: 0.051174	Acc: 29.6% (2957/10000)
[Test]  Epoch: 69	Loss: 0.051015	Acc: 29.4% (2938/10000)
[Test]  Epoch: 70	Loss: 0.051054	Acc: 29.3% (2932/10000)
[Test]  Epoch: 71	Loss: 0.051042	Acc: 29.5% (2953/10000)
[Test]  Epoch: 72	Loss: 0.051094	Acc: 29.4% (2936/10000)
[Test]  Epoch: 73	Loss: 0.051015	Acc: 29.6% (2960/10000)
[Test]  Epoch: 74	Loss: 0.051013	Acc: 29.7% (2973/10000)
[Test]  Epoch: 75	Loss: 0.050980	Acc: 29.5% (2953/10000)
[Test]  Epoch: 76	Loss: 0.051021	Acc: 29.9% (2991/10000)
[Test]  Epoch: 77	Loss: 0.051027	Acc: 29.4% (2935/10000)
[Test]  Epoch: 78	Loss: 0.051050	Acc: 29.3% (2930/10000)
[Test]  Epoch: 79	Loss: 0.051208	Acc: 29.4% (2938/10000)
[Test]  Epoch: 80	Loss: 0.050983	Acc: 29.7% (2974/10000)
[Test]  Epoch: 81	Loss: 0.050957	Acc: 29.5% (2947/10000)
[Test]  Epoch: 82	Loss: 0.051004	Acc: 29.7% (2968/10000)
[Test]  Epoch: 83	Loss: 0.051043	Acc: 29.6% (2959/10000)
[Test]  Epoch: 84	Loss: 0.051032	Acc: 29.3% (2933/10000)
[Test]  Epoch: 85	Loss: 0.051130	Acc: 29.2% (2923/10000)
[Test]  Epoch: 86	Loss: 0.051092	Acc: 29.3% (2926/10000)
[Test]  Epoch: 87	Loss: 0.051015	Acc: 29.4% (2935/10000)
[Test]  Epoch: 88	Loss: 0.051071	Acc: 29.5% (2953/10000)
[Test]  Epoch: 89	Loss: 0.051014	Acc: 29.2% (2925/10000)
[Test]  Epoch: 90	Loss: 0.051139	Acc: 29.5% (2953/10000)
[Test]  Epoch: 91	Loss: 0.051062	Acc: 29.7% (2968/10000)
[Test]  Epoch: 92	Loss: 0.050890	Acc: 29.6% (2963/10000)
[Test]  Epoch: 93	Loss: 0.051117	Acc: 29.5% (2953/10000)
[Test]  Epoch: 94	Loss: 0.051163	Acc: 29.4% (2943/10000)
[Test]  Epoch: 95	Loss: 0.051078	Acc: 29.3% (2928/10000)
[Test]  Epoch: 96	Loss: 0.050928	Acc: 29.6% (2956/10000)
[Test]  Epoch: 97	Loss: 0.051077	Acc: 29.6% (2957/10000)
[Test]  Epoch: 98	Loss: 0.051109	Acc: 29.7% (2969/10000)
[Test]  Epoch: 99	Loss: 0.050984	Acc: 29.6% (2957/10000)
[Test]  Epoch: 100	Loss: 0.050899	Acc: 29.4% (2943/10000)
===========finish==========
['2024-08-19', '04:40:44.315345', '100', 'test', '0.050898563122749325', '29.43', '29.91']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.13.conv.0.1.weight', '_features.15.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.5.conv.1.1.weight', '_features.15.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.13.conv.3.weight', '_features.13.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.3.weight', '_features.12.conv.1.0.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.10.conv.2.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.2.weight', '_features.3.conv.3.weight', '_features.3.conv.1.1.weight', '_features.6.conv.1.0.weight', '_features.4.conv.3.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.5.conv.0.0.weight', '_features.5.conv.1.0.weight', '_features.13.conv.2.weight', '_features.12.conv.2.weight', '_features.13.conv.0.0.weight', '_features.6.conv.2.weight', '_features.12.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.2.weight', '_features.14.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.17.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.0.1.weight', '_features.17.conv.3.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.2.weight', '_features.2.conv.0.1.weight', 'last_linear.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.1.conv.1.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.093291	Acc: 0.7% (69/10000)
[Test]  Epoch: 2	Loss: 0.078901	Acc: 4.5% (450/10000)
[Test]  Epoch: 3	Loss: 0.075521	Acc: 6.8% (683/10000)
[Test]  Epoch: 4	Loss: 0.072887	Acc: 8.3% (827/10000)
[Test]  Epoch: 5	Loss: 0.070763	Acc: 9.6% (959/10000)
[Test]  Epoch: 6	Loss: 0.069178	Acc: 10.4% (1041/10000)
[Test]  Epoch: 7	Loss: 0.067819	Acc: 11.2% (1122/10000)
[Test]  Epoch: 8	Loss: 0.066624	Acc: 12.1% (1206/10000)
[Test]  Epoch: 9	Loss: 0.065565	Acc: 13.0% (1296/10000)
[Test]  Epoch: 10	Loss: 0.064707	Acc: 13.6% (1355/10000)
[Test]  Epoch: 11	Loss: 0.063946	Acc: 14.2% (1415/10000)
[Test]  Epoch: 12	Loss: 0.063369	Acc: 14.8% (1484/10000)
[Test]  Epoch: 13	Loss: 0.063043	Acc: 15.0% (1499/10000)
[Test]  Epoch: 14	Loss: 0.062559	Acc: 15.3% (1532/10000)
[Test]  Epoch: 15	Loss: 0.062075	Acc: 15.9% (1594/10000)
[Test]  Epoch: 16	Loss: 0.061682	Acc: 15.8% (1580/10000)
[Test]  Epoch: 17	Loss: 0.061394	Acc: 16.6% (1659/10000)
[Test]  Epoch: 18	Loss: 0.061199	Acc: 16.5% (1646/10000)
[Test]  Epoch: 19	Loss: 0.061016	Acc: 16.5% (1651/10000)
[Test]  Epoch: 20	Loss: 0.060799	Acc: 16.8% (1682/10000)
[Test]  Epoch: 21	Loss: 0.060393	Acc: 17.2% (1725/10000)
[Test]  Epoch: 22	Loss: 0.060233	Acc: 17.5% (1753/10000)
[Test]  Epoch: 23	Loss: 0.059930	Acc: 17.6% (1760/10000)
[Test]  Epoch: 24	Loss: 0.059952	Acc: 17.7% (1772/10000)
[Test]  Epoch: 25	Loss: 0.059801	Acc: 18.1% (1809/10000)
[Test]  Epoch: 26	Loss: 0.059498	Acc: 18.3% (1832/10000)
[Test]  Epoch: 27	Loss: 0.059426	Acc: 18.5% (1850/10000)
[Test]  Epoch: 28	Loss: 0.059309	Acc: 18.5% (1851/10000)
[Test]  Epoch: 29	Loss: 0.059187	Acc: 18.4% (1841/10000)
[Test]  Epoch: 30	Loss: 0.059125	Acc: 18.9% (1885/10000)
[Test]  Epoch: 31	Loss: 0.059124	Acc: 18.6% (1858/10000)
[Test]  Epoch: 32	Loss: 0.059012	Acc: 18.6% (1863/10000)
[Test]  Epoch: 33	Loss: 0.058712	Acc: 19.1% (1905/10000)
[Test]  Epoch: 34	Loss: 0.058704	Acc: 19.2% (1921/10000)
[Test]  Epoch: 35	Loss: 0.058525	Acc: 19.0% (1901/10000)
[Test]  Epoch: 36	Loss: 0.058572	Acc: 18.9% (1893/10000)
[Test]  Epoch: 37	Loss: 0.058424	Acc: 19.6% (1963/10000)
[Test]  Epoch: 38	Loss: 0.058366	Acc: 19.4% (1935/10000)
[Test]  Epoch: 39	Loss: 0.058390	Acc: 19.6% (1962/10000)
[Test]  Epoch: 40	Loss: 0.058154	Acc: 19.6% (1961/10000)
[Test]  Epoch: 41	Loss: 0.058299	Acc: 19.7% (1974/10000)
[Test]  Epoch: 42	Loss: 0.058177	Acc: 19.6% (1959/10000)
[Test]  Epoch: 43	Loss: 0.058074	Acc: 20.1% (2007/10000)
[Test]  Epoch: 44	Loss: 0.058002	Acc: 19.9% (1989/10000)
[Test]  Epoch: 45	Loss: 0.057995	Acc: 20.0% (1996/10000)
[Test]  Epoch: 46	Loss: 0.057948	Acc: 20.2% (2023/10000)
[Test]  Epoch: 47	Loss: 0.057798	Acc: 20.4% (2045/10000)
[Test]  Epoch: 48	Loss: 0.057775	Acc: 20.3% (2032/10000)
[Test]  Epoch: 49	Loss: 0.057781	Acc: 20.4% (2040/10000)
[Test]  Epoch: 50	Loss: 0.057817	Acc: 20.2% (2024/10000)
[Test]  Epoch: 51	Loss: 0.057714	Acc: 20.5% (2051/10000)
[Test]  Epoch: 52	Loss: 0.057542	Acc: 20.8% (2075/10000)
[Test]  Epoch: 53	Loss: 0.057623	Acc: 20.3% (2033/10000)
[Test]  Epoch: 54	Loss: 0.057601	Acc: 20.6% (2056/10000)
[Test]  Epoch: 55	Loss: 0.057504	Acc: 20.8% (2080/10000)
[Test]  Epoch: 56	Loss: 0.057578	Acc: 20.6% (2059/10000)
[Test]  Epoch: 57	Loss: 0.057562	Acc: 21.0% (2097/10000)
[Test]  Epoch: 58	Loss: 0.057153	Acc: 21.0% (2101/10000)
[Test]  Epoch: 59	Loss: 0.057425	Acc: 21.0% (2102/10000)
[Test]  Epoch: 60	Loss: 0.057371	Acc: 20.8% (2082/10000)
[Test]  Epoch: 61	Loss: 0.057388	Acc: 20.9% (2092/10000)
[Test]  Epoch: 62	Loss: 0.057411	Acc: 21.1% (2114/10000)
[Test]  Epoch: 63	Loss: 0.057157	Acc: 21.3% (2131/10000)
[Test]  Epoch: 64	Loss: 0.057156	Acc: 21.4% (2141/10000)
[Test]  Epoch: 65	Loss: 0.057253	Acc: 21.4% (2143/10000)
[Test]  Epoch: 66	Loss: 0.057264	Acc: 21.3% (2126/10000)
[Test]  Epoch: 67	Loss: 0.057266	Acc: 21.2% (2116/10000)
[Test]  Epoch: 68	Loss: 0.057332	Acc: 21.1% (2105/10000)
[Test]  Epoch: 69	Loss: 0.057244	Acc: 21.2% (2116/10000)
[Test]  Epoch: 70	Loss: 0.057257	Acc: 21.1% (2108/10000)
[Test]  Epoch: 71	Loss: 0.057268	Acc: 21.2% (2117/10000)
[Test]  Epoch: 72	Loss: 0.057350	Acc: 21.1% (2109/10000)
[Test]  Epoch: 73	Loss: 0.057239	Acc: 21.4% (2135/10000)
[Test]  Epoch: 74	Loss: 0.057199	Acc: 21.2% (2122/10000)
[Test]  Epoch: 75	Loss: 0.057242	Acc: 21.1% (2111/10000)
[Test]  Epoch: 76	Loss: 0.057205	Acc: 21.1% (2114/10000)
[Test]  Epoch: 77	Loss: 0.057201	Acc: 21.3% (2129/10000)
[Test]  Epoch: 78	Loss: 0.057179	Acc: 21.1% (2114/10000)
[Test]  Epoch: 79	Loss: 0.057254	Acc: 21.2% (2118/10000)
[Test]  Epoch: 80	Loss: 0.057191	Acc: 21.3% (2132/10000)
[Test]  Epoch: 81	Loss: 0.057139	Acc: 21.4% (2136/10000)
[Test]  Epoch: 82	Loss: 0.057192	Acc: 21.2% (2119/10000)
[Test]  Epoch: 83	Loss: 0.057197	Acc: 21.1% (2114/10000)
[Test]  Epoch: 84	Loss: 0.057263	Acc: 21.3% (2126/10000)
[Test]  Epoch: 85	Loss: 0.057285	Acc: 21.2% (2124/10000)
[Test]  Epoch: 86	Loss: 0.057305	Acc: 21.0% (2099/10000)
[Test]  Epoch: 87	Loss: 0.057220	Acc: 21.2% (2125/10000)
[Test]  Epoch: 88	Loss: 0.057246	Acc: 21.2% (2121/10000)
[Test]  Epoch: 89	Loss: 0.057145	Acc: 21.2% (2124/10000)
[Test]  Epoch: 90	Loss: 0.057217	Acc: 21.3% (2131/10000)
[Test]  Epoch: 91	Loss: 0.057209	Acc: 21.3% (2130/10000)
[Test]  Epoch: 92	Loss: 0.057133	Acc: 21.4% (2144/10000)
[Test]  Epoch: 93	Loss: 0.057172	Acc: 21.3% (2129/10000)
[Test]  Epoch: 94	Loss: 0.057247	Acc: 21.2% (2118/10000)
[Test]  Epoch: 95	Loss: 0.057201	Acc: 21.4% (2135/10000)
[Test]  Epoch: 96	Loss: 0.057091	Acc: 21.3% (2131/10000)
[Test]  Epoch: 97	Loss: 0.057184	Acc: 21.2% (2125/10000)
[Test]  Epoch: 98	Loss: 0.057218	Acc: 21.2% (2117/10000)
[Test]  Epoch: 99	Loss: 0.057156	Acc: 21.1% (2114/10000)
[Test]  Epoch: 100	Loss: 0.057144	Acc: 21.3% (2134/10000)
===========finish==========
['2024-08-19', '04:45:49.344270', '100', 'test', '0.05714409697055817', '21.34', '21.44']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.18.1.weight', '_features.15.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.9.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.0.1.weight', '_features.9.conv.3.weight', '_features.16.conv.3.weight', '_features.10.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.10.conv.1.0.weight', '_features.13.conv.0.1.weight', '_features.15.conv.2.weight', '_features.9.conv.1.0.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.5.conv.1.1.weight', '_features.15.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.13.conv.3.weight', '_features.13.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.7.conv.3.weight', '_features.12.conv.1.0.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.10.conv.2.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.2.weight', '_features.3.conv.3.weight', '_features.3.conv.1.1.weight', '_features.6.conv.1.0.weight', '_features.4.conv.3.weight', '_features.3.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.5.conv.0.0.weight', '_features.5.conv.1.0.weight', '_features.13.conv.2.weight', '_features.12.conv.2.weight', '_features.13.conv.0.0.weight', '_features.6.conv.2.weight', '_features.12.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.2.weight', '_features.14.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.17.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.0.1.weight', '_features.17.conv.3.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.2.weight', '_features.2.conv.0.1.weight', 'last_linear.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.1.conv.1.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight', '_features.11.conv.0.0.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', '_features.11.conv.2.weight', '_features.4.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.2.conv.2.weight', '_features.17.conv.2.weight', '_features.14.conv.2.weight', '_features.17.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.105320	Acc: 0.6% (61/10000)
[Test]  Epoch: 2	Loss: 0.082540	Acc: 2.2% (223/10000)
[Test]  Epoch: 3	Loss: 0.079758	Acc: 3.8% (381/10000)
[Test]  Epoch: 4	Loss: 0.077898	Acc: 5.0% (497/10000)
[Test]  Epoch: 5	Loss: 0.076113	Acc: 6.1% (612/10000)
[Test]  Epoch: 6	Loss: 0.074431	Acc: 7.0% (698/10000)
[Test]  Epoch: 7	Loss: 0.073249	Acc: 7.5% (755/10000)
[Test]  Epoch: 8	Loss: 0.071804	Acc: 8.3% (827/10000)
[Test]  Epoch: 9	Loss: 0.070683	Acc: 9.1% (908/10000)
[Test]  Epoch: 10	Loss: 0.069674	Acc: 9.6% (960/10000)
[Test]  Epoch: 11	Loss: 0.068960	Acc: 9.7% (966/10000)
[Test]  Epoch: 12	Loss: 0.068125	Acc: 10.3% (1029/10000)
[Test]  Epoch: 13	Loss: 0.067567	Acc: 10.7% (1067/10000)
[Test]  Epoch: 14	Loss: 0.066970	Acc: 11.0% (1102/10000)
[Test]  Epoch: 15	Loss: 0.066317	Acc: 11.5% (1152/10000)
[Test]  Epoch: 16	Loss: 0.065865	Acc: 11.7% (1173/10000)
[Test]  Epoch: 17	Loss: 0.065436	Acc: 12.3% (1226/10000)
[Test]  Epoch: 18	Loss: 0.064975	Acc: 12.3% (1228/10000)
[Test]  Epoch: 19	Loss: 0.064637	Acc: 12.7% (1267/10000)
[Test]  Epoch: 20	Loss: 0.064242	Acc: 13.1% (1308/10000)
[Test]  Epoch: 21	Loss: 0.063961	Acc: 13.2% (1322/10000)
[Test]  Epoch: 22	Loss: 0.063665	Acc: 13.4% (1337/10000)
[Test]  Epoch: 23	Loss: 0.063462	Acc: 13.9% (1394/10000)
[Test]  Epoch: 24	Loss: 0.063251	Acc: 13.6% (1363/10000)
[Test]  Epoch: 25	Loss: 0.063084	Acc: 14.0% (1397/10000)
[Test]  Epoch: 26	Loss: 0.062893	Acc: 14.4% (1438/10000)
[Test]  Epoch: 27	Loss: 0.062705	Acc: 14.3% (1433/10000)
[Test]  Epoch: 28	Loss: 0.062480	Acc: 14.6% (1458/10000)
[Test]  Epoch: 29	Loss: 0.062247	Acc: 14.6% (1462/10000)
[Test]  Epoch: 30	Loss: 0.062146	Acc: 15.0% (1501/10000)
[Test]  Epoch: 31	Loss: 0.062022	Acc: 15.1% (1507/10000)
[Test]  Epoch: 32	Loss: 0.061848	Acc: 15.0% (1498/10000)
[Test]  Epoch: 33	Loss: 0.061801	Acc: 15.3% (1535/10000)
[Test]  Epoch: 34	Loss: 0.061531	Acc: 15.4% (1543/10000)
[Test]  Epoch: 35	Loss: 0.061418	Acc: 15.6% (1558/10000)
[Test]  Epoch: 36	Loss: 0.061372	Acc: 15.6% (1556/10000)
[Test]  Epoch: 37	Loss: 0.061282	Acc: 15.8% (1575/10000)
[Test]  Epoch: 38	Loss: 0.061205	Acc: 15.8% (1583/10000)
[Test]  Epoch: 39	Loss: 0.061112	Acc: 15.7% (1572/10000)
[Test]  Epoch: 40	Loss: 0.061051	Acc: 16.0% (1604/10000)
[Test]  Epoch: 41	Loss: 0.061023	Acc: 16.2% (1622/10000)
[Test]  Epoch: 42	Loss: 0.060898	Acc: 16.3% (1626/10000)
[Test]  Epoch: 43	Loss: 0.060718	Acc: 16.0% (1600/10000)
[Test]  Epoch: 44	Loss: 0.060584	Acc: 16.5% (1654/10000)
[Test]  Epoch: 45	Loss: 0.060629	Acc: 16.4% (1640/10000)
[Test]  Epoch: 46	Loss: 0.060587	Acc: 16.8% (1678/10000)
[Test]  Epoch: 47	Loss: 0.060575	Acc: 16.5% (1654/10000)
[Test]  Epoch: 48	Loss: 0.060362	Acc: 16.7% (1674/10000)
[Test]  Epoch: 49	Loss: 0.060442	Acc: 16.6% (1665/10000)
[Test]  Epoch: 50	Loss: 0.060480	Acc: 16.6% (1662/10000)
[Test]  Epoch: 51	Loss: 0.060282	Acc: 17.1% (1714/10000)
[Test]  Epoch: 52	Loss: 0.060048	Acc: 17.1% (1715/10000)
[Test]  Epoch: 53	Loss: 0.060086	Acc: 17.0% (1699/10000)
[Test]  Epoch: 54	Loss: 0.060083	Acc: 17.1% (1705/10000)
[Test]  Epoch: 55	Loss: 0.060102	Acc: 16.9% (1691/10000)
[Test]  Epoch: 56	Loss: 0.060098	Acc: 17.2% (1724/10000)
[Test]  Epoch: 57	Loss: 0.060030	Acc: 17.4% (1738/10000)
[Test]  Epoch: 58	Loss: 0.059806	Acc: 17.2% (1721/10000)
[Test]  Epoch: 59	Loss: 0.059829	Acc: 17.2% (1718/10000)
[Test]  Epoch: 60	Loss: 0.059969	Acc: 17.1% (1709/10000)
[Test]  Epoch: 61	Loss: 0.059966	Acc: 17.2% (1722/10000)
[Test]  Epoch: 62	Loss: 0.059956	Acc: 17.2% (1722/10000)
[Test]  Epoch: 63	Loss: 0.059833	Acc: 17.6% (1758/10000)
[Test]  Epoch: 64	Loss: 0.059788	Acc: 17.3% (1734/10000)
[Test]  Epoch: 65	Loss: 0.059810	Acc: 17.5% (1747/10000)
[Test]  Epoch: 66	Loss: 0.059832	Acc: 17.6% (1758/10000)
[Test]  Epoch: 67	Loss: 0.059929	Acc: 17.2% (1725/10000)
[Test]  Epoch: 68	Loss: 0.059944	Acc: 17.2% (1723/10000)
[Test]  Epoch: 69	Loss: 0.059781	Acc: 17.4% (1739/10000)
[Test]  Epoch: 70	Loss: 0.059818	Acc: 17.4% (1745/10000)
[Test]  Epoch: 71	Loss: 0.059782	Acc: 17.4% (1745/10000)
[Test]  Epoch: 72	Loss: 0.059849	Acc: 17.3% (1729/10000)
[Test]  Epoch: 73	Loss: 0.059750	Acc: 17.4% (1744/10000)
[Test]  Epoch: 74	Loss: 0.059731	Acc: 17.5% (1747/10000)
[Test]  Epoch: 75	Loss: 0.059768	Acc: 17.6% (1755/10000)
[Test]  Epoch: 76	Loss: 0.059738	Acc: 17.5% (1748/10000)
[Test]  Epoch: 77	Loss: 0.059802	Acc: 17.4% (1743/10000)
[Test]  Epoch: 78	Loss: 0.059753	Acc: 17.3% (1731/10000)
[Test]  Epoch: 79	Loss: 0.059798	Acc: 17.4% (1739/10000)
[Test]  Epoch: 80	Loss: 0.059715	Acc: 17.8% (1776/10000)
[Test]  Epoch: 81	Loss: 0.059697	Acc: 17.6% (1765/10000)
[Test]  Epoch: 82	Loss: 0.059726	Acc: 17.4% (1744/10000)
[Test]  Epoch: 83	Loss: 0.059753	Acc: 17.4% (1737/10000)
[Test]  Epoch: 84	Loss: 0.059832	Acc: 17.5% (1751/10000)
[Test]  Epoch: 85	Loss: 0.059750	Acc: 17.6% (1756/10000)
[Test]  Epoch: 86	Loss: 0.059803	Acc: 17.3% (1734/10000)
[Test]  Epoch: 87	Loss: 0.059755	Acc: 17.5% (1753/10000)
[Test]  Epoch: 88	Loss: 0.059727	Acc: 17.6% (1760/10000)
[Test]  Epoch: 89	Loss: 0.059612	Acc: 17.5% (1749/10000)
[Test]  Epoch: 90	Loss: 0.059681	Acc: 17.6% (1764/10000)
[Test]  Epoch: 91	Loss: 0.059730	Acc: 17.5% (1754/10000)
[Test]  Epoch: 92	Loss: 0.059663	Acc: 17.8% (1776/10000)
[Test]  Epoch: 93	Loss: 0.059733	Acc: 17.6% (1756/10000)
[Test]  Epoch: 94	Loss: 0.059776	Acc: 17.6% (1755/10000)
[Test]  Epoch: 95	Loss: 0.059750	Acc: 17.5% (1753/10000)
[Test]  Epoch: 96	Loss: 0.059636	Acc: 17.4% (1745/10000)
[Test]  Epoch: 97	Loss: 0.059729	Acc: 17.6% (1757/10000)
[Test]  Epoch: 98	Loss: 0.059780	Acc: 17.6% (1763/10000)
[Test]  Epoch: 99	Loss: 0.059691	Acc: 17.4% (1737/10000)
[Test]  Epoch: 100	Loss: 0.059708	Acc: 17.6% (1755/10000)
===========finish==========
['2024-08-19', '04:50:31.192192', '100', 'test', '0.05970827784538269', '17.55', '17.76']
result path:  /home/gpu2/jbw/other_XAI/knockoffnets/ms_elastictrainer_result_resnet50.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/cifar100-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.bias', 0.0), ('layer4.2.bn3.weight', -2.796565756746361e-11), ('layer4.2.bn1.weight', -1.1173761471638954e-08), ('layer4.2.conv3.weight', -3.621464372827177e-08), ('layer4.2.bn2.weight', -5.0407209073455306e-08), ('layer4.2.conv1.weight', -6.543898223299038e-08), ('layer4.2.conv2.weight', -9.742643669596873e-08), ('layer4.1.bn2.weight', -0.0002659109595697373), ('layer4.1.conv3.weight', -0.005553205497562885), ('layer4.1.conv2.weight', -0.009551426395773888), ('layer4.1.bn1.weight', -0.009885878302156925), ('layer4.1.bn3.weight', -0.03348839282989502), ('layer4.1.conv1.weight', -0.12830905616283417), ('layer4.0.downsample.1.weight', -0.5931509733200073), ('layer4.0.bn3.weight', -0.7245938777923584), ('layer1.2.bn1.weight', -1.599315881729126), ('layer1.1.bn1.weight', -1.763886570930481), ('layer2.2.bn1.weight', -2.16501784324646), ('layer1.1.bn2.weight', -2.1882238388061523), ('layer2.1.bn1.weight', -2.1973235607147217), ('layer3.5.bn2.weight', -2.2182352542877197), ('layer3.3.bn1.weight', -2.2421233654022217), ('layer1.2.bn3.weight', -2.2539501190185547), ('layer3.5.bn1.weight', -2.274055004119873), ('layer2.3.bn1.weight', -2.277749538421631), ('layer1.1.bn3.weight', -2.389230251312256), ('layer1.0.bn1.weight', -2.400341749191284), ('layer1.0.bn2.weight', -2.452147960662842), ('layer1.2.bn2.weight', -2.4845314025878906), ('layer3.3.bn2.weight', -2.5216164588928223), ('layer3.3.bn3.weight', -2.684535503387451), ('layer3.4.bn1.weight', -2.7073583602905273), ('layer3.5.bn3.weight', -2.722883701324463), ('layer3.4.bn2.weight', -3.0565810203552246), ('layer2.2.bn2.weight', -3.0759668350219727), ('layer2.1.bn2.weight', -3.114966869354248), ('layer3.2.bn1.weight', -3.1395998001098633), ('layer3.4.bn3.weight', -3.1437554359436035), ('layer2.1.bn3.weight', -3.481882095336914), ('layer1.0.bn3.weight', -3.62553334236145), ('layer2.3.bn3.weight', -3.8113412857055664), ('layer2.3.bn2.weight', -3.8321220874786377), ('layer3.2.bn2.weight', -3.8913896083831787), ('layer3.2.bn3.weight', -4.009030818939209), ('layer2.2.bn3.weight', -4.171347618103027), ('layer3.1.bn1.weight', -4.2874956130981445), ('layer3.1.bn3.weight', -5.078227996826172), ('layer3.1.bn2.weight', -5.112521171569824), ('layer2.0.bn2.weight', -6.078924179077148), ('layer2.0.bn1.weight', -6.44139289855957), ('layer1.0.downsample.1.weight', -7.001186370849609), ('layer2.0.bn3.weight', -7.263997554779053), ('layer1.2.conv3.weight', -8.020730972290039), ('layer3.0.bn2.weight', -8.033346176147461), ('layer1.1.conv1.weight', -8.220519065856934), ('layer2.0.downsample.1.weight', -8.935426712036133), ('layer1.1.conv3.weight', -9.125679016113281), ('layer1.2.conv1.weight', -9.765183448791504), ('layer3.0.bn1.weight', -9.991058349609375), ('layer3.0.bn3.weight', -10.967714309692383), ('layer3.5.conv3.weight', -11.250844955444336), ('layer3.0.downsample.1.weight', -11.68297290802002), ('layer2.1.conv1.weight', -11.967966079711914), ('layer3.3.conv3.weight', -14.504709243774414), ('layer1.0.conv3.weight', -15.408893585205078), ('layer1.1.conv2.weight', -15.500421524047852), ('layer3.4.conv3.weight', -16.83079719543457), ('layer3.5.conv2.weight', -17.170225143432617), ('layer3.3.conv1.weight', -17.294475555419922), ('layer3.5.conv1.weight', -17.858196258544922), ('layer2.2.conv1.weight', -18.080230712890625), ('layer2.1.conv3.weight', -19.511505126953125), ('layer1.2.conv2.weight', -19.635677337646484), ('bn1.weight', -20.395837783813477), ('layer1.0.conv2.weight', -20.63766860961914), ('layer3.4.conv1.weight', -21.522377014160156), ('layer3.3.conv2.weight', -21.618375778198242), ('layer2.3.conv1.weight', -23.05990982055664), ('layer3.2.conv3.weight', -23.487503051757812), ('layer3.2.conv1.weight', -23.750139236450195), ('layer2.2.conv3.weight', -24.262832641601562), ('layer1.0.conv1.weight', -25.0862979888916), ('layer2.3.conv3.weight', -25.646915435791016), ('layer4.0.bn2.weight', -26.098766326904297), ('layer4.0.bn1.weight', -26.179033279418945), ('layer2.1.conv2.weight', -26.203369140625), ('layer3.4.conv2.weight', -26.267196655273438), ('layer3.1.conv1.weight', -30.545597076416016), ('layer3.1.conv3.weight', -33.778228759765625), ('layer2.2.conv2.weight', -35.06265640258789), ('layer3.2.conv2.weight', -35.551780700683594), ('last_linear.weight', -41.13892364501953), ('layer2.0.conv3.weight', -42.38539123535156), ('layer2.0.conv1.weight', -43.32361602783203), ('layer2.3.conv2.weight', -44.87567138671875), ('layer3.1.conv2.weight', -51.59434509277344), ('layer2.0.conv2.weight', -55.529258728027344), ('layer2.0.downsample.0.weight', -72.40243530273438), ('layer3.0.conv1.weight', -87.65536499023438), ('layer4.0.downsample.0.weight', -97.71741485595703), ('layer1.0.downsample.0.weight', -98.25236511230469), ('layer3.0.conv3.weight', -99.65571594238281), ('layer3.0.downsample.0.weight', -103.67259979248047), ('layer3.0.conv2.weight', -127.21721649169922), ('conv1.weight', -141.6210479736328), ('layer4.0.conv3.weight', -180.46319580078125), ('layer4.0.conv1.weight', -186.3433837890625), ('layer4.0.conv2.weight', -284.7369079589844)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('last_linear.bias', 0.0), ('layer4.2.conv3.weight', -3.621464372827177e-08), ('layer4.2.conv1.weight', -6.543898223299038e-08), ('layer4.2.conv2.weight', -9.742643669596873e-08), ('layer4.1.conv3.weight', -0.005553205497562885), ('layer4.1.conv2.weight', -0.009551426395773888), ('layer4.1.conv1.weight', -0.12830905616283417), ('layer1.2.conv3.weight', -8.020730972290039), ('layer1.1.conv1.weight', -8.220519065856934), ('layer1.1.conv3.weight', -9.125679016113281), ('layer1.2.conv1.weight', -9.765183448791504), ('layer3.5.conv3.weight', -11.250844955444336), ('layer2.1.conv1.weight', -11.967966079711914), ('layer3.3.conv3.weight', -14.504709243774414), ('layer1.0.conv3.weight', -15.408893585205078), ('layer1.1.conv2.weight', -15.500421524047852), ('layer3.4.conv3.weight', -16.83079719543457), ('layer3.5.conv2.weight', -17.170225143432617), ('layer3.3.conv1.weight', -17.294475555419922), ('layer3.5.conv1.weight', -17.858196258544922), ('layer2.2.conv1.weight', -18.080230712890625), ('layer2.1.conv3.weight', -19.511505126953125), ('layer1.2.conv2.weight', -19.635677337646484), ('layer1.0.conv2.weight', -20.63766860961914), ('layer3.4.conv1.weight', -21.522377014160156), ('layer3.3.conv2.weight', -21.618375778198242), ('layer2.3.conv1.weight', -23.05990982055664), ('layer3.2.conv3.weight', -23.487503051757812), ('layer3.2.conv1.weight', -23.750139236450195), ('layer2.2.conv3.weight', -24.262832641601562), ('layer1.0.conv1.weight', -25.0862979888916), ('layer2.3.conv3.weight', -25.646915435791016), ('layer2.1.conv2.weight', -26.203369140625), ('layer3.4.conv2.weight', -26.267196655273438), ('layer3.1.conv1.weight', -30.545597076416016), ('layer3.1.conv3.weight', -33.778228759765625), ('layer2.2.conv2.weight', -35.06265640258789), ('layer3.2.conv2.weight', -35.551780700683594), ('last_linear.weight', -41.13892364501953), ('layer2.0.conv3.weight', -42.38539123535156), ('layer2.0.conv1.weight', -43.32361602783203), ('layer2.3.conv2.weight', -44.87567138671875), ('layer3.1.conv2.weight', -51.59434509277344), ('layer2.0.conv2.weight', -55.529258728027344), ('layer3.0.conv1.weight', -87.65536499023438), ('layer3.0.conv3.weight', -99.65571594238281), ('layer3.0.conv2.weight', -127.21721649169922), ('conv1.weight', -141.6210479736328), ('layer4.0.conv3.weight', -180.46319580078125), ('layer4.0.conv1.weight', -186.3433837890625), ('layer4.0.conv2.weight', -284.7369079589844)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.026454	Acc: 63.2% (6320/10000)
[Test]  Epoch: 2	Loss: 0.026501	Acc: 63.0% (6303/10000)
[Test]  Epoch: 3	Loss: 0.026496	Acc: 62.8% (6283/10000)
[Test]  Epoch: 4	Loss: 0.026590	Acc: 62.8% (6279/10000)
[Test]  Epoch: 5	Loss: 0.026554	Acc: 62.8% (6281/10000)
[Test]  Epoch: 6	Loss: 0.026502	Acc: 62.7% (6268/10000)
[Test]  Epoch: 7	Loss: 0.026489	Acc: 62.6% (6260/10000)
[Test]  Epoch: 8	Loss: 0.026469	Acc: 63.0% (6302/10000)
[Test]  Epoch: 9	Loss: 0.026423	Acc: 62.9% (6288/10000)
[Test]  Epoch: 10	Loss: 0.026352	Acc: 62.9% (6290/10000)
[Test]  Epoch: 11	Loss: 0.026257	Acc: 62.9% (6291/10000)
[Test]  Epoch: 12	Loss: 0.026277	Acc: 63.0% (6297/10000)
[Test]  Epoch: 13	Loss: 0.026254	Acc: 63.1% (6315/10000)
[Test]  Epoch: 14	Loss: 0.026270	Acc: 63.1% (6307/10000)
[Test]  Epoch: 15	Loss: 0.026209	Acc: 63.3% (6331/10000)
[Test]  Epoch: 16	Loss: 0.026271	Acc: 63.2% (6321/10000)
[Test]  Epoch: 17	Loss: 0.026114	Acc: 63.2% (6323/10000)
[Test]  Epoch: 18	Loss: 0.026169	Acc: 63.2% (6317/10000)
[Test]  Epoch: 19	Loss: 0.026084	Acc: 63.2% (6324/10000)
[Test]  Epoch: 20	Loss: 0.026001	Acc: 63.5% (6350/10000)
[Test]  Epoch: 21	Loss: 0.026046	Acc: 63.3% (6327/10000)
[Test]  Epoch: 22	Loss: 0.025972	Acc: 63.4% (6338/10000)
[Test]  Epoch: 23	Loss: 0.026046	Acc: 63.2% (6321/10000)
[Test]  Epoch: 24	Loss: 0.026036	Acc: 63.1% (6308/10000)
[Test]  Epoch: 25	Loss: 0.026031	Acc: 63.5% (6350/10000)
[Test]  Epoch: 26	Loss: 0.026051	Acc: 63.4% (6335/10000)
[Test]  Epoch: 27	Loss: 0.026122	Acc: 63.2% (6324/10000)
[Test]  Epoch: 28	Loss: 0.026000	Acc: 63.3% (6326/10000)
[Test]  Epoch: 29	Loss: 0.026002	Acc: 63.3% (6333/10000)
[Test]  Epoch: 30	Loss: 0.025985	Acc: 63.3% (6331/10000)
[Test]  Epoch: 31	Loss: 0.025934	Acc: 63.4% (6340/10000)
[Test]  Epoch: 32	Loss: 0.025958	Acc: 63.5% (6346/10000)
[Test]  Epoch: 33	Loss: 0.025984	Acc: 63.2% (6316/10000)
[Test]  Epoch: 34	Loss: 0.026015	Acc: 63.2% (6318/10000)
[Test]  Epoch: 35	Loss: 0.025902	Acc: 63.2% (6324/10000)
[Test]  Epoch: 36	Loss: 0.025888	Acc: 63.4% (6340/10000)
[Test]  Epoch: 37	Loss: 0.025880	Acc: 63.3% (6328/10000)
[Test]  Epoch: 38	Loss: 0.025959	Acc: 63.3% (6334/10000)
[Test]  Epoch: 39	Loss: 0.025977	Acc: 63.3% (6334/10000)
[Test]  Epoch: 40	Loss: 0.025895	Acc: 63.4% (6340/10000)
[Test]  Epoch: 41	Loss: 0.025864	Acc: 63.4% (6341/10000)
[Test]  Epoch: 42	Loss: 0.025829	Acc: 63.4% (6342/10000)
[Test]  Epoch: 43	Loss: 0.025931	Acc: 63.7% (6371/10000)
[Test]  Epoch: 44	Loss: 0.025903	Acc: 63.5% (6347/10000)
[Test]  Epoch: 45	Loss: 0.025812	Acc: 63.6% (6357/10000)
[Test]  Epoch: 46	Loss: 0.025771	Acc: 63.5% (6355/10000)
[Test]  Epoch: 47	Loss: 0.025876	Acc: 63.4% (6344/10000)
[Test]  Epoch: 48	Loss: 0.025835	Acc: 63.5% (6354/10000)
[Test]  Epoch: 49	Loss: 0.025814	Acc: 63.3% (6331/10000)
[Test]  Epoch: 50	Loss: 0.025805	Acc: 63.5% (6354/10000)
[Test]  Epoch: 51	Loss: 0.025907	Acc: 63.2% (6319/10000)
[Test]  Epoch: 52	Loss: 0.025784	Acc: 63.5% (6347/10000)
[Test]  Epoch: 53	Loss: 0.025756	Acc: 63.5% (6349/10000)
[Test]  Epoch: 54	Loss: 0.025812	Acc: 63.5% (6350/10000)
[Test]  Epoch: 55	Loss: 0.025729	Acc: 63.6% (6356/10000)
[Test]  Epoch: 56	Loss: 0.025829	Acc: 63.3% (6331/10000)
[Test]  Epoch: 57	Loss: 0.025813	Acc: 63.5% (6347/10000)
[Test]  Epoch: 58	Loss: 0.025733	Acc: 63.5% (6351/10000)
[Test]  Epoch: 59	Loss: 0.025798	Acc: 63.4% (6344/10000)
[Test]  Epoch: 60	Loss: 0.025799	Acc: 63.4% (6339/10000)
[Test]  Epoch: 61	Loss: 0.025812	Acc: 63.4% (6339/10000)
[Test]  Epoch: 62	Loss: 0.025821	Acc: 63.3% (6334/10000)
[Test]  Epoch: 63	Loss: 0.025768	Acc: 63.5% (6355/10000)
[Test]  Epoch: 64	Loss: 0.025768	Acc: 63.5% (6348/10000)
[Test]  Epoch: 65	Loss: 0.025690	Acc: 63.3% (6333/10000)
[Test]  Epoch: 66	Loss: 0.025760	Acc: 63.5% (6350/10000)
[Test]  Epoch: 67	Loss: 0.025806	Acc: 63.3% (6327/10000)
[Test]  Epoch: 68	Loss: 0.025742	Acc: 63.4% (6343/10000)
[Test]  Epoch: 69	Loss: 0.025765	Acc: 63.5% (6346/10000)
[Test]  Epoch: 70	Loss: 0.025726	Acc: 63.5% (6352/10000)
[Test]  Epoch: 71	Loss: 0.025741	Acc: 63.5% (6348/10000)
[Test]  Epoch: 72	Loss: 0.025700	Acc: 63.4% (6340/10000)
[Test]  Epoch: 73	Loss: 0.025741	Acc: 63.6% (6363/10000)
[Test]  Epoch: 74	Loss: 0.025735	Acc: 63.4% (6340/10000)
[Test]  Epoch: 75	Loss: 0.025677	Acc: 63.6% (6363/10000)
[Test]  Epoch: 76	Loss: 0.025787	Acc: 63.3% (6329/10000)
[Test]  Epoch: 77	Loss: 0.025823	Acc: 63.2% (6323/10000)
[Test]  Epoch: 78	Loss: 0.025744	Acc: 63.4% (6342/10000)
[Test]  Epoch: 79	Loss: 0.025824	Acc: 63.3% (6330/10000)
[Test]  Epoch: 80	Loss: 0.025766	Acc: 63.5% (6354/10000)
[Test]  Epoch: 81	Loss: 0.025768	Acc: 63.4% (6341/10000)
[Test]  Epoch: 82	Loss: 0.025769	Acc: 63.5% (6347/10000)
[Test]  Epoch: 83	Loss: 0.025738	Acc: 63.2% (6325/10000)
[Test]  Epoch: 84	Loss: 0.025692	Acc: 63.4% (6339/10000)
[Test]  Epoch: 85	Loss: 0.025772	Acc: 63.5% (6354/10000)
[Test]  Epoch: 86	Loss: 0.025780	Acc: 63.7% (6367/10000)
[Test]  Epoch: 87	Loss: 0.025731	Acc: 63.7% (6366/10000)
[Test]  Epoch: 88	Loss: 0.025796	Acc: 63.5% (6351/10000)
[Test]  Epoch: 89	Loss: 0.025754	Acc: 63.4% (6341/10000)
[Test]  Epoch: 90	Loss: 0.025678	Acc: 63.5% (6351/10000)
[Test]  Epoch: 91	Loss: 0.025692	Acc: 63.5% (6349/10000)
[Test]  Epoch: 92	Loss: 0.025800	Acc: 63.4% (6339/10000)
[Test]  Epoch: 93	Loss: 0.025702	Acc: 63.7% (6370/10000)
[Test]  Epoch: 94	Loss: 0.025679	Acc: 63.6% (6359/10000)
[Test]  Epoch: 95	Loss: 0.025779	Acc: 63.3% (6334/10000)
[Test]  Epoch: 96	Loss: 0.025742	Acc: 63.5% (6353/10000)
[Test]  Epoch: 97	Loss: 0.025723	Acc: 63.5% (6353/10000)
[Test]  Epoch: 98	Loss: 0.025707	Acc: 63.7% (6373/10000)
[Test]  Epoch: 99	Loss: 0.025785	Acc: 63.5% (6353/10000)
[Test]  Epoch: 100	Loss: 0.025770	Acc: 63.4% (6341/10000)
===========finish==========
['2024-08-19', '16:16:32.530735', '100', 'test', '0.02577047591805458', '63.41', '63.73']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.034794	Acc: 59.5% (5949/10000)
[Test]  Epoch: 2	Loss: 0.034243	Acc: 59.5% (5953/10000)
[Test]  Epoch: 3	Loss: 0.033433	Acc: 60.1% (6015/10000)
[Test]  Epoch: 4	Loss: 0.033033	Acc: 60.1% (6013/10000)
[Test]  Epoch: 5	Loss: 0.032781	Acc: 60.3% (6031/10000)
[Test]  Epoch: 6	Loss: 0.032356	Acc: 60.5% (6045/10000)
[Test]  Epoch: 7	Loss: 0.032367	Acc: 60.5% (6046/10000)
[Test]  Epoch: 8	Loss: 0.032129	Acc: 60.4% (6036/10000)
[Test]  Epoch: 9	Loss: 0.032115	Acc: 60.6% (6063/10000)
[Test]  Epoch: 10	Loss: 0.032023	Acc: 60.3% (6031/10000)
[Test]  Epoch: 11	Loss: 0.031979	Acc: 60.4% (6043/10000)
[Test]  Epoch: 12	Loss: 0.031718	Acc: 60.8% (6081/10000)
[Test]  Epoch: 13	Loss: 0.031576	Acc: 60.7% (6070/10000)
[Test]  Epoch: 14	Loss: 0.031325	Acc: 60.8% (6077/10000)
[Test]  Epoch: 15	Loss: 0.031267	Acc: 61.0% (6097/10000)
[Test]  Epoch: 16	Loss: 0.031160	Acc: 60.8% (6084/10000)
[Test]  Epoch: 17	Loss: 0.031158	Acc: 60.6% (6064/10000)
[Test]  Epoch: 18	Loss: 0.031130	Acc: 60.8% (6079/10000)
[Test]  Epoch: 19	Loss: 0.031174	Acc: 60.6% (6065/10000)
[Test]  Epoch: 20	Loss: 0.030869	Acc: 60.6% (6063/10000)
[Test]  Epoch: 21	Loss: 0.030736	Acc: 60.8% (6075/10000)
[Test]  Epoch: 22	Loss: 0.030542	Acc: 60.8% (6080/10000)
[Test]  Epoch: 23	Loss: 0.030506	Acc: 60.6% (6061/10000)
[Test]  Epoch: 24	Loss: 0.030524	Acc: 60.6% (6065/10000)
[Test]  Epoch: 25	Loss: 0.030692	Acc: 60.6% (6059/10000)
[Test]  Epoch: 26	Loss: 0.030524	Acc: 60.7% (6067/10000)
[Test]  Epoch: 27	Loss: 0.030326	Acc: 60.7% (6070/10000)
[Test]  Epoch: 28	Loss: 0.030305	Acc: 60.8% (6079/10000)
[Test]  Epoch: 29	Loss: 0.030291	Acc: 60.8% (6082/10000)
[Test]  Epoch: 30	Loss: 0.030167	Acc: 60.8% (6078/10000)
[Test]  Epoch: 31	Loss: 0.029957	Acc: 60.8% (6081/10000)
[Test]  Epoch: 32	Loss: 0.029941	Acc: 60.9% (6093/10000)
[Test]  Epoch: 33	Loss: 0.030068	Acc: 60.9% (6087/10000)
[Test]  Epoch: 34	Loss: 0.029952	Acc: 60.8% (6077/10000)
[Test]  Epoch: 35	Loss: 0.029745	Acc: 60.8% (6083/10000)
[Test]  Epoch: 36	Loss: 0.029857	Acc: 60.8% (6080/10000)
[Test]  Epoch: 37	Loss: 0.029716	Acc: 60.9% (6087/10000)
[Test]  Epoch: 38	Loss: 0.029736	Acc: 60.8% (6083/10000)
[Test]  Epoch: 39	Loss: 0.029776	Acc: 60.9% (6088/10000)
[Test]  Epoch: 40	Loss: 0.029691	Acc: 60.8% (6084/10000)
[Test]  Epoch: 41	Loss: 0.029689	Acc: 60.8% (6075/10000)
[Test]  Epoch: 42	Loss: 0.029565	Acc: 60.9% (6086/10000)
[Test]  Epoch: 43	Loss: 0.029587	Acc: 60.9% (6092/10000)
[Test]  Epoch: 44	Loss: 0.029385	Acc: 61.0% (6097/10000)
[Test]  Epoch: 45	Loss: 0.029442	Acc: 61.0% (6097/10000)
[Test]  Epoch: 46	Loss: 0.029412	Acc: 61.0% (6096/10000)
[Test]  Epoch: 47	Loss: 0.029414	Acc: 60.9% (6092/10000)
[Test]  Epoch: 48	Loss: 0.029360	Acc: 60.7% (6071/10000)
[Test]  Epoch: 49	Loss: 0.029189	Acc: 60.9% (6089/10000)
[Test]  Epoch: 50	Loss: 0.029143	Acc: 60.9% (6090/10000)
[Test]  Epoch: 51	Loss: 0.029235	Acc: 60.8% (6077/10000)
[Test]  Epoch: 52	Loss: 0.029315	Acc: 60.8% (6084/10000)
[Test]  Epoch: 53	Loss: 0.028997	Acc: 60.9% (6092/10000)
[Test]  Epoch: 54	Loss: 0.029130	Acc: 60.8% (6084/10000)
[Test]  Epoch: 55	Loss: 0.028995	Acc: 60.9% (6091/10000)
[Test]  Epoch: 56	Loss: 0.029248	Acc: 60.8% (6084/10000)
[Test]  Epoch: 57	Loss: 0.029029	Acc: 61.0% (6095/10000)
[Test]  Epoch: 58	Loss: 0.028865	Acc: 60.9% (6094/10000)
[Test]  Epoch: 59	Loss: 0.028933	Acc: 60.6% (6064/10000)
[Test]  Epoch: 60	Loss: 0.028935	Acc: 60.8% (6075/10000)
[Test]  Epoch: 61	Loss: 0.028863	Acc: 61.0% (6098/10000)
[Test]  Epoch: 62	Loss: 0.028999	Acc: 60.8% (6077/10000)
[Test]  Epoch: 63	Loss: 0.028995	Acc: 61.0% (6100/10000)
[Test]  Epoch: 64	Loss: 0.028823	Acc: 61.0% (6096/10000)
[Test]  Epoch: 65	Loss: 0.028786	Acc: 61.0% (6097/10000)
[Test]  Epoch: 66	Loss: 0.028774	Acc: 61.1% (6106/10000)
[Test]  Epoch: 67	Loss: 0.028927	Acc: 60.9% (6094/10000)
[Test]  Epoch: 68	Loss: 0.028871	Acc: 61.0% (6105/10000)
[Test]  Epoch: 69	Loss: 0.028871	Acc: 60.8% (6079/10000)
[Test]  Epoch: 70	Loss: 0.028831	Acc: 60.9% (6089/10000)
[Test]  Epoch: 71	Loss: 0.028900	Acc: 60.9% (6094/10000)
[Test]  Epoch: 72	Loss: 0.028887	Acc: 61.0% (6100/10000)
[Test]  Epoch: 73	Loss: 0.028876	Acc: 61.0% (6096/10000)
[Test]  Epoch: 74	Loss: 0.028855	Acc: 61.0% (6095/10000)
[Test]  Epoch: 75	Loss: 0.028746	Acc: 61.0% (6100/10000)
[Test]  Epoch: 76	Loss: 0.028917	Acc: 61.0% (6105/10000)
[Test]  Epoch: 77	Loss: 0.028909	Acc: 60.9% (6094/10000)
[Test]  Epoch: 78	Loss: 0.028747	Acc: 61.0% (6095/10000)
[Test]  Epoch: 79	Loss: 0.028818	Acc: 61.0% (6098/10000)
[Test]  Epoch: 80	Loss: 0.028874	Acc: 61.0% (6102/10000)
[Test]  Epoch: 81	Loss: 0.028763	Acc: 61.0% (6098/10000)
[Test]  Epoch: 82	Loss: 0.028722	Acc: 61.0% (6100/10000)
[Test]  Epoch: 83	Loss: 0.028853	Acc: 60.8% (6076/10000)
[Test]  Epoch: 84	Loss: 0.028824	Acc: 60.8% (6080/10000)
[Test]  Epoch: 85	Loss: 0.028821	Acc: 60.9% (6087/10000)
[Test]  Epoch: 86	Loss: 0.028787	Acc: 61.0% (6097/10000)
[Test]  Epoch: 87	Loss: 0.028801	Acc: 61.0% (6105/10000)
[Test]  Epoch: 88	Loss: 0.028819	Acc: 61.0% (6104/10000)
[Test]  Epoch: 89	Loss: 0.028723	Acc: 61.1% (6109/10000)
[Test]  Epoch: 90	Loss: 0.028702	Acc: 61.0% (6101/10000)
[Test]  Epoch: 91	Loss: 0.028727	Acc: 61.0% (6099/10000)
[Test]  Epoch: 92	Loss: 0.028695	Acc: 61.0% (6105/10000)
[Test]  Epoch: 93	Loss: 0.028610	Acc: 61.1% (6114/10000)
[Test]  Epoch: 94	Loss: 0.028685	Acc: 61.0% (6098/10000)
[Test]  Epoch: 95	Loss: 0.028809	Acc: 61.0% (6102/10000)
[Test]  Epoch: 96	Loss: 0.028827	Acc: 61.0% (6099/10000)
[Test]  Epoch: 97	Loss: 0.028768	Acc: 61.0% (6096/10000)
[Test]  Epoch: 98	Loss: 0.028721	Acc: 61.0% (6099/10000)
[Test]  Epoch: 99	Loss: 0.028720	Acc: 61.1% (6114/10000)
[Test]  Epoch: 100	Loss: 0.028653	Acc: 61.1% (6113/10000)
===========finish==========
['2024-08-19', '16:20:51.692022', '100', 'test', '0.02865269544124603', '61.13', '61.14']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.045349	Acc: 55.5% (5554/10000)
[Test]  Epoch: 2	Loss: 0.043665	Acc: 56.2% (5621/10000)
[Test]  Epoch: 3	Loss: 0.041356	Acc: 57.0% (5699/10000)
[Test]  Epoch: 4	Loss: 0.039295	Acc: 58.6% (5859/10000)
[Test]  Epoch: 5	Loss: 0.038682	Acc: 58.0% (5796/10000)
[Test]  Epoch: 6	Loss: 0.038145	Acc: 58.3% (5831/10000)
[Test]  Epoch: 7	Loss: 0.037364	Acc: 58.8% (5879/10000)
[Test]  Epoch: 8	Loss: 0.037141	Acc: 59.0% (5895/10000)
[Test]  Epoch: 9	Loss: 0.036683	Acc: 59.1% (5915/10000)
[Test]  Epoch: 10	Loss: 0.036834	Acc: 58.9% (5885/10000)
[Test]  Epoch: 11	Loss: 0.036098	Acc: 59.4% (5944/10000)
[Test]  Epoch: 12	Loss: 0.036017	Acc: 59.3% (5930/10000)
[Test]  Epoch: 13	Loss: 0.036597	Acc: 58.9% (5887/10000)
[Test]  Epoch: 14	Loss: 0.036161	Acc: 58.8% (5881/10000)
[Test]  Epoch: 15	Loss: 0.035601	Acc: 59.0% (5897/10000)
[Test]  Epoch: 16	Loss: 0.035468	Acc: 59.0% (5897/10000)
[Test]  Epoch: 17	Loss: 0.035794	Acc: 58.4% (5835/10000)
[Test]  Epoch: 18	Loss: 0.035076	Acc: 58.8% (5879/10000)
[Test]  Epoch: 19	Loss: 0.035219	Acc: 59.0% (5902/10000)
[Test]  Epoch: 20	Loss: 0.034814	Acc: 59.0% (5904/10000)
[Test]  Epoch: 21	Loss: 0.034661	Acc: 59.0% (5902/10000)
[Test]  Epoch: 22	Loss: 0.034675	Acc: 59.0% (5895/10000)
[Test]  Epoch: 23	Loss: 0.034611	Acc: 59.0% (5895/10000)
[Test]  Epoch: 24	Loss: 0.034573	Acc: 58.8% (5881/10000)
[Test]  Epoch: 25	Loss: 0.034610	Acc: 59.0% (5905/10000)
[Test]  Epoch: 26	Loss: 0.034522	Acc: 59.0% (5904/10000)
[Test]  Epoch: 27	Loss: 0.034297	Acc: 59.1% (5911/10000)
[Test]  Epoch: 28	Loss: 0.034429	Acc: 59.1% (5910/10000)
[Test]  Epoch: 29	Loss: 0.034309	Acc: 59.3% (5930/10000)
[Test]  Epoch: 30	Loss: 0.033953	Acc: 59.1% (5906/10000)
[Test]  Epoch: 31	Loss: 0.033800	Acc: 59.0% (5902/10000)
[Test]  Epoch: 32	Loss: 0.033757	Acc: 59.3% (5930/10000)
[Test]  Epoch: 33	Loss: 0.033650	Acc: 59.0% (5895/10000)
[Test]  Epoch: 34	Loss: 0.033785	Acc: 59.1% (5912/10000)
[Test]  Epoch: 35	Loss: 0.033517	Acc: 59.3% (5927/10000)
[Test]  Epoch: 36	Loss: 0.033392	Acc: 59.4% (5939/10000)
[Test]  Epoch: 37	Loss: 0.033222	Acc: 59.4% (5940/10000)
[Test]  Epoch: 38	Loss: 0.032997	Acc: 59.3% (5927/10000)
[Test]  Epoch: 39	Loss: 0.033048	Acc: 59.5% (5951/10000)
[Test]  Epoch: 40	Loss: 0.033041	Acc: 59.5% (5947/10000)
[Test]  Epoch: 41	Loss: 0.033209	Acc: 59.2% (5923/10000)
[Test]  Epoch: 42	Loss: 0.032889	Acc: 59.4% (5943/10000)
[Test]  Epoch: 43	Loss: 0.033045	Acc: 59.3% (5929/10000)
[Test]  Epoch: 44	Loss: 0.032787	Acc: 59.5% (5946/10000)
[Test]  Epoch: 45	Loss: 0.032592	Acc: 59.5% (5953/10000)
[Test]  Epoch: 46	Loss: 0.032486	Acc: 59.4% (5936/10000)
[Test]  Epoch: 47	Loss: 0.032518	Acc: 59.4% (5941/10000)
[Test]  Epoch: 48	Loss: 0.032554	Acc: 59.5% (5947/10000)
[Test]  Epoch: 49	Loss: 0.032345	Acc: 59.5% (5952/10000)
[Test]  Epoch: 50	Loss: 0.032371	Acc: 59.5% (5950/10000)
[Test]  Epoch: 51	Loss: 0.032378	Acc: 59.5% (5946/10000)
[Test]  Epoch: 52	Loss: 0.032145	Acc: 59.5% (5950/10000)
[Test]  Epoch: 53	Loss: 0.031912	Acc: 59.6% (5957/10000)
[Test]  Epoch: 54	Loss: 0.032066	Acc: 59.4% (5938/10000)
[Test]  Epoch: 55	Loss: 0.031951	Acc: 59.2% (5925/10000)
[Test]  Epoch: 56	Loss: 0.032034	Acc: 59.2% (5920/10000)
[Test]  Epoch: 57	Loss: 0.031822	Acc: 59.4% (5936/10000)
[Test]  Epoch: 58	Loss: 0.031900	Acc: 59.3% (5934/10000)
[Test]  Epoch: 59	Loss: 0.031849	Acc: 59.4% (5939/10000)
[Test]  Epoch: 60	Loss: 0.031713	Acc: 59.5% (5948/10000)
[Test]  Epoch: 61	Loss: 0.031601	Acc: 59.6% (5960/10000)
[Test]  Epoch: 62	Loss: 0.031651	Acc: 59.7% (5968/10000)
[Test]  Epoch: 63	Loss: 0.031611	Acc: 59.6% (5957/10000)
[Test]  Epoch: 64	Loss: 0.031731	Acc: 59.6% (5961/10000)
[Test]  Epoch: 65	Loss: 0.031466	Acc: 59.6% (5956/10000)
[Test]  Epoch: 66	Loss: 0.031563	Acc: 59.6% (5962/10000)
[Test]  Epoch: 67	Loss: 0.031693	Acc: 59.6% (5957/10000)
[Test]  Epoch: 68	Loss: 0.031739	Acc: 59.7% (5967/10000)
[Test]  Epoch: 69	Loss: 0.031475	Acc: 59.5% (5955/10000)
[Test]  Epoch: 70	Loss: 0.031640	Acc: 59.6% (5957/10000)
[Test]  Epoch: 71	Loss: 0.031519	Acc: 59.5% (5950/10000)
[Test]  Epoch: 72	Loss: 0.031377	Acc: 59.4% (5944/10000)
[Test]  Epoch: 73	Loss: 0.031492	Acc: 59.4% (5939/10000)
[Test]  Epoch: 74	Loss: 0.031539	Acc: 59.5% (5951/10000)
[Test]  Epoch: 75	Loss: 0.031386	Acc: 59.6% (5963/10000)
[Test]  Epoch: 76	Loss: 0.031768	Acc: 59.5% (5948/10000)
[Test]  Epoch: 77	Loss: 0.031608	Acc: 59.4% (5935/10000)
[Test]  Epoch: 78	Loss: 0.031438	Acc: 59.5% (5950/10000)
[Test]  Epoch: 79	Loss: 0.031650	Acc: 59.6% (5961/10000)
[Test]  Epoch: 80	Loss: 0.031585	Acc: 59.4% (5943/10000)
[Test]  Epoch: 81	Loss: 0.031646	Acc: 59.5% (5947/10000)
[Test]  Epoch: 82	Loss: 0.031400	Acc: 59.6% (5957/10000)
[Test]  Epoch: 83	Loss: 0.031572	Acc: 59.5% (5950/10000)
[Test]  Epoch: 84	Loss: 0.031559	Acc: 59.6% (5962/10000)
[Test]  Epoch: 85	Loss: 0.031605	Acc: 59.5% (5948/10000)
[Test]  Epoch: 86	Loss: 0.031626	Acc: 59.6% (5964/10000)
[Test]  Epoch: 87	Loss: 0.031528	Acc: 59.6% (5962/10000)
[Test]  Epoch: 88	Loss: 0.031427	Acc: 59.6% (5959/10000)
[Test]  Epoch: 89	Loss: 0.031471	Acc: 59.6% (5961/10000)
[Test]  Epoch: 90	Loss: 0.031348	Acc: 59.5% (5946/10000)
[Test]  Epoch: 91	Loss: 0.031486	Acc: 59.7% (5971/10000)
[Test]  Epoch: 92	Loss: 0.031579	Acc: 59.5% (5946/10000)
[Test]  Epoch: 93	Loss: 0.031309	Acc: 59.7% (5968/10000)
[Test]  Epoch: 94	Loss: 0.031243	Acc: 59.6% (5962/10000)
[Test]  Epoch: 95	Loss: 0.031489	Acc: 59.5% (5948/10000)
[Test]  Epoch: 96	Loss: 0.031522	Acc: 59.6% (5959/10000)
[Test]  Epoch: 97	Loss: 0.031439	Acc: 59.6% (5962/10000)
[Test]  Epoch: 98	Loss: 0.031384	Acc: 59.8% (5980/10000)
[Test]  Epoch: 99	Loss: 0.031494	Acc: 59.6% (5957/10000)
[Test]  Epoch: 100	Loss: 0.031424	Acc: 59.5% (5952/10000)
===========finish==========
['2024-08-19', '16:25:26.324733', '100', 'test', '0.03142430766820908', '59.52', '59.8']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn1.weight', 'layer2.3.bn1.weight', 'layer1.1.bn3.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn3.weight', 'layer3.4.bn1.weight', 'layer3.5.bn3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.086505	Acc: 29.6% (2965/10000)
[Test]  Epoch: 2	Loss: 0.050283	Acc: 49.8% (4977/10000)
[Test]  Epoch: 3	Loss: 0.043246	Acc: 53.8% (5383/10000)
[Test]  Epoch: 4	Loss: 0.038515	Acc: 56.2% (5621/10000)
[Test]  Epoch: 5	Loss: 0.037614	Acc: 56.6% (5665/10000)
[Test]  Epoch: 6	Loss: 0.036716	Acc: 56.9% (5694/10000)
[Test]  Epoch: 7	Loss: 0.036399	Acc: 57.5% (5754/10000)
[Test]  Epoch: 8	Loss: 0.036324	Acc: 57.0% (5704/10000)
[Test]  Epoch: 9	Loss: 0.035989	Acc: 57.2% (5716/10000)
[Test]  Epoch: 10	Loss: 0.035392	Acc: 57.4% (5735/10000)
[Test]  Epoch: 11	Loss: 0.035327	Acc: 57.3% (5732/10000)
[Test]  Epoch: 12	Loss: 0.035148	Acc: 57.3% (5729/10000)
[Test]  Epoch: 13	Loss: 0.035118	Acc: 57.4% (5738/10000)
[Test]  Epoch: 14	Loss: 0.035051	Acc: 57.5% (5747/10000)
[Test]  Epoch: 15	Loss: 0.034640	Acc: 57.5% (5745/10000)
[Test]  Epoch: 16	Loss: 0.034807	Acc: 57.4% (5735/10000)
[Test]  Epoch: 17	Loss: 0.034509	Acc: 57.5% (5750/10000)
[Test]  Epoch: 18	Loss: 0.034242	Acc: 57.7% (5769/10000)
[Test]  Epoch: 19	Loss: 0.034243	Acc: 58.0% (5805/10000)
[Test]  Epoch: 20	Loss: 0.033849	Acc: 57.9% (5789/10000)
[Test]  Epoch: 21	Loss: 0.033958	Acc: 57.6% (5761/10000)
[Test]  Epoch: 22	Loss: 0.033626	Acc: 57.6% (5765/10000)
[Test]  Epoch: 23	Loss: 0.033705	Acc: 57.6% (5763/10000)
[Test]  Epoch: 24	Loss: 0.033417	Acc: 57.8% (5777/10000)
[Test]  Epoch: 25	Loss: 0.033435	Acc: 57.8% (5781/10000)
[Test]  Epoch: 26	Loss: 0.033566	Acc: 57.6% (5765/10000)
[Test]  Epoch: 27	Loss: 0.033350	Acc: 58.0% (5795/10000)
[Test]  Epoch: 28	Loss: 0.033245	Acc: 57.7% (5771/10000)
[Test]  Epoch: 29	Loss: 0.033340	Acc: 57.8% (5783/10000)
[Test]  Epoch: 30	Loss: 0.033038	Acc: 57.8% (5783/10000)
[Test]  Epoch: 31	Loss: 0.033021	Acc: 58.1% (5807/10000)
[Test]  Epoch: 32	Loss: 0.032942	Acc: 58.3% (5826/10000)
[Test]  Epoch: 33	Loss: 0.032767	Acc: 58.2% (5821/10000)
[Test]  Epoch: 34	Loss: 0.032680	Acc: 58.2% (5816/10000)
[Test]  Epoch: 35	Loss: 0.032650	Acc: 58.2% (5823/10000)
[Test]  Epoch: 36	Loss: 0.032533	Acc: 58.4% (5843/10000)
[Test]  Epoch: 37	Loss: 0.032381	Acc: 58.2% (5821/10000)
[Test]  Epoch: 38	Loss: 0.032807	Acc: 57.9% (5792/10000)
[Test]  Epoch: 39	Loss: 0.032652	Acc: 58.2% (5825/10000)
[Test]  Epoch: 40	Loss: 0.032444	Acc: 58.2% (5822/10000)
[Test]  Epoch: 41	Loss: 0.032116	Acc: 58.4% (5839/10000)
[Test]  Epoch: 42	Loss: 0.032083	Acc: 58.3% (5834/10000)
[Test]  Epoch: 43	Loss: 0.032104	Acc: 58.4% (5839/10000)
[Test]  Epoch: 44	Loss: 0.031906	Acc: 58.5% (5848/10000)
[Test]  Epoch: 45	Loss: 0.031814	Acc: 58.5% (5851/10000)
[Test]  Epoch: 46	Loss: 0.032096	Acc: 58.3% (5827/10000)
[Test]  Epoch: 47	Loss: 0.031988	Acc: 58.4% (5836/10000)
[Test]  Epoch: 48	Loss: 0.031715	Acc: 58.4% (5835/10000)
[Test]  Epoch: 49	Loss: 0.031589	Acc: 58.4% (5835/10000)
[Test]  Epoch: 50	Loss: 0.031583	Acc: 58.4% (5842/10000)
[Test]  Epoch: 51	Loss: 0.031518	Acc: 58.6% (5856/10000)
[Test]  Epoch: 52	Loss: 0.031425	Acc: 58.4% (5840/10000)
[Test]  Epoch: 53	Loss: 0.031382	Acc: 58.6% (5857/10000)
[Test]  Epoch: 54	Loss: 0.031361	Acc: 58.6% (5859/10000)
[Test]  Epoch: 55	Loss: 0.031234	Acc: 58.4% (5837/10000)
[Test]  Epoch: 56	Loss: 0.031285	Acc: 58.3% (5832/10000)
[Test]  Epoch: 57	Loss: 0.031239	Acc: 58.5% (5845/10000)
[Test]  Epoch: 58	Loss: 0.031098	Acc: 58.7% (5869/10000)
[Test]  Epoch: 59	Loss: 0.031307	Acc: 58.3% (5832/10000)
[Test]  Epoch: 60	Loss: 0.031126	Acc: 58.6% (5863/10000)
[Test]  Epoch: 61	Loss: 0.030995	Acc: 58.5% (5854/10000)
[Test]  Epoch: 62	Loss: 0.031006	Acc: 58.4% (5838/10000)
[Test]  Epoch: 63	Loss: 0.031137	Acc: 58.6% (5860/10000)
[Test]  Epoch: 64	Loss: 0.031183	Acc: 58.5% (5855/10000)
[Test]  Epoch: 65	Loss: 0.031192	Acc: 58.5% (5854/10000)
[Test]  Epoch: 66	Loss: 0.031055	Acc: 58.5% (5854/10000)
[Test]  Epoch: 67	Loss: 0.031239	Acc: 58.4% (5838/10000)
[Test]  Epoch: 68	Loss: 0.031091	Acc: 58.5% (5848/10000)
[Test]  Epoch: 69	Loss: 0.030945	Acc: 58.5% (5851/10000)
[Test]  Epoch: 70	Loss: 0.031055	Acc: 58.6% (5858/10000)
[Test]  Epoch: 71	Loss: 0.030900	Acc: 58.4% (5841/10000)
[Test]  Epoch: 72	Loss: 0.030888	Acc: 58.6% (5861/10000)
[Test]  Epoch: 73	Loss: 0.030882	Acc: 58.6% (5861/10000)
[Test]  Epoch: 74	Loss: 0.030961	Acc: 58.5% (5854/10000)
[Test]  Epoch: 75	Loss: 0.030851	Acc: 58.6% (5858/10000)
[Test]  Epoch: 76	Loss: 0.031030	Acc: 58.7% (5871/10000)
[Test]  Epoch: 77	Loss: 0.030940	Acc: 58.6% (5862/10000)
[Test]  Epoch: 78	Loss: 0.030953	Acc: 58.6% (5864/10000)
[Test]  Epoch: 79	Loss: 0.031061	Acc: 58.4% (5843/10000)
[Test]  Epoch: 80	Loss: 0.030856	Acc: 58.6% (5865/10000)
[Test]  Epoch: 81	Loss: 0.030954	Acc: 58.6% (5862/10000)
[Test]  Epoch: 82	Loss: 0.030866	Acc: 58.6% (5860/10000)
[Test]  Epoch: 83	Loss: 0.030996	Acc: 58.5% (5855/10000)
[Test]  Epoch: 84	Loss: 0.031004	Acc: 58.4% (5841/10000)
[Test]  Epoch: 85	Loss: 0.030982	Acc: 58.6% (5859/10000)
[Test]  Epoch: 86	Loss: 0.031028	Acc: 58.6% (5865/10000)
[Test]  Epoch: 87	Loss: 0.030876	Acc: 58.7% (5871/10000)
[Test]  Epoch: 88	Loss: 0.030896	Acc: 58.7% (5874/10000)
[Test]  Epoch: 89	Loss: 0.030745	Acc: 58.6% (5865/10000)
[Test]  Epoch: 90	Loss: 0.030844	Acc: 58.6% (5857/10000)
[Test]  Epoch: 91	Loss: 0.030956	Acc: 58.6% (5860/10000)
[Test]  Epoch: 92	Loss: 0.031032	Acc: 58.5% (5851/10000)
[Test]  Epoch: 93	Loss: 0.030855	Acc: 58.6% (5864/10000)
[Test]  Epoch: 94	Loss: 0.030714	Acc: 58.6% (5860/10000)
[Test]  Epoch: 95	Loss: 0.030821	Acc: 58.6% (5863/10000)
[Test]  Epoch: 96	Loss: 0.030891	Acc: 58.6% (5861/10000)
[Test]  Epoch: 97	Loss: 0.030845	Acc: 58.5% (5852/10000)
[Test]  Epoch: 98	Loss: 0.030951	Acc: 58.5% (5852/10000)
[Test]  Epoch: 99	Loss: 0.030821	Acc: 58.7% (5870/10000)
[Test]  Epoch: 100	Loss: 0.030725	Acc: 58.8% (5876/10000)
===========finish==========
['2024-08-19', '16:29:42.942505', '100', 'test', '0.03072518547773361', '58.76', '58.76']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn1.weight', 'layer2.3.bn1.weight', 'layer1.1.bn3.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn3.weight', 'layer3.4.bn1.weight', 'layer3.5.bn3.weight', 'layer3.4.bn2.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer3.2.bn1.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn2.weight', 'layer3.2.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.069418	Acc: 31.1% (3107/10000)
[Test]  Epoch: 2	Loss: 0.047354	Acc: 47.7% (4769/10000)
[Test]  Epoch: 3	Loss: 0.041515	Acc: 52.6% (5263/10000)
[Test]  Epoch: 4	Loss: 0.037920	Acc: 55.4% (5537/10000)
[Test]  Epoch: 5	Loss: 0.037776	Acc: 55.5% (5552/10000)
[Test]  Epoch: 6	Loss: 0.037017	Acc: 56.0% (5596/10000)
[Test]  Epoch: 7	Loss: 0.036579	Acc: 56.1% (5615/10000)
[Test]  Epoch: 8	Loss: 0.036539	Acc: 56.3% (5629/10000)
[Test]  Epoch: 9	Loss: 0.036533	Acc: 56.1% (5611/10000)
[Test]  Epoch: 10	Loss: 0.036017	Acc: 56.2% (5622/10000)
[Test]  Epoch: 11	Loss: 0.035981	Acc: 56.2% (5623/10000)
[Test]  Epoch: 12	Loss: 0.035712	Acc: 56.3% (5628/10000)
[Test]  Epoch: 13	Loss: 0.035760	Acc: 56.2% (5619/10000)
[Test]  Epoch: 14	Loss: 0.035451	Acc: 56.1% (5607/10000)
[Test]  Epoch: 15	Loss: 0.035431	Acc: 56.1% (5614/10000)
[Test]  Epoch: 16	Loss: 0.035244	Acc: 56.4% (5636/10000)
[Test]  Epoch: 17	Loss: 0.035048	Acc: 56.2% (5620/10000)
[Test]  Epoch: 18	Loss: 0.034975	Acc: 56.3% (5634/10000)
[Test]  Epoch: 19	Loss: 0.034950	Acc: 56.4% (5635/10000)
[Test]  Epoch: 20	Loss: 0.034589	Acc: 56.4% (5641/10000)
[Test]  Epoch: 21	Loss: 0.034284	Acc: 56.7% (5669/10000)
[Test]  Epoch: 22	Loss: 0.034284	Acc: 56.5% (5645/10000)
[Test]  Epoch: 23	Loss: 0.034365	Acc: 56.6% (5662/10000)
[Test]  Epoch: 24	Loss: 0.034192	Acc: 56.6% (5658/10000)
[Test]  Epoch: 25	Loss: 0.034170	Acc: 56.8% (5676/10000)
[Test]  Epoch: 26	Loss: 0.033849	Acc: 56.9% (5688/10000)
[Test]  Epoch: 27	Loss: 0.033772	Acc: 57.0% (5698/10000)
[Test]  Epoch: 28	Loss: 0.033660	Acc: 56.9% (5693/10000)
[Test]  Epoch: 29	Loss: 0.033670	Acc: 57.0% (5697/10000)
[Test]  Epoch: 30	Loss: 0.033632	Acc: 56.8% (5677/10000)
[Test]  Epoch: 31	Loss: 0.033259	Acc: 56.9% (5686/10000)
[Test]  Epoch: 32	Loss: 0.033256	Acc: 56.9% (5689/10000)
[Test]  Epoch: 33	Loss: 0.033206	Acc: 56.9% (5690/10000)
[Test]  Epoch: 34	Loss: 0.033211	Acc: 56.9% (5694/10000)
[Test]  Epoch: 35	Loss: 0.032900	Acc: 57.1% (5708/10000)
[Test]  Epoch: 36	Loss: 0.033002	Acc: 57.0% (5700/10000)
[Test]  Epoch: 37	Loss: 0.032937	Acc: 57.0% (5704/10000)
[Test]  Epoch: 38	Loss: 0.032805	Acc: 57.0% (5705/10000)
[Test]  Epoch: 39	Loss: 0.032776	Acc: 57.1% (5709/10000)
[Test]  Epoch: 40	Loss: 0.032810	Acc: 57.0% (5698/10000)
[Test]  Epoch: 41	Loss: 0.032632	Acc: 57.2% (5719/10000)
[Test]  Epoch: 42	Loss: 0.032562	Acc: 57.1% (5713/10000)
[Test]  Epoch: 43	Loss: 0.032661	Acc: 57.2% (5718/10000)
[Test]  Epoch: 44	Loss: 0.032414	Acc: 57.2% (5720/10000)
[Test]  Epoch: 45	Loss: 0.032414	Acc: 57.1% (5714/10000)
[Test]  Epoch: 46	Loss: 0.032518	Acc: 56.9% (5691/10000)
[Test]  Epoch: 47	Loss: 0.032695	Acc: 57.0% (5702/10000)
[Test]  Epoch: 48	Loss: 0.032342	Acc: 57.2% (5719/10000)
[Test]  Epoch: 49	Loss: 0.032148	Acc: 57.1% (5706/10000)
[Test]  Epoch: 50	Loss: 0.032235	Acc: 57.1% (5706/10000)
[Test]  Epoch: 51	Loss: 0.032196	Acc: 57.2% (5724/10000)
[Test]  Epoch: 52	Loss: 0.032019	Acc: 57.2% (5716/10000)
[Test]  Epoch: 53	Loss: 0.031868	Acc: 57.2% (5725/10000)
[Test]  Epoch: 54	Loss: 0.031960	Acc: 57.1% (5714/10000)
[Test]  Epoch: 55	Loss: 0.031886	Acc: 57.2% (5722/10000)
[Test]  Epoch: 56	Loss: 0.032011	Acc: 57.1% (5710/10000)
[Test]  Epoch: 57	Loss: 0.031898	Acc: 57.3% (5728/10000)
[Test]  Epoch: 58	Loss: 0.031772	Acc: 57.3% (5731/10000)
[Test]  Epoch: 59	Loss: 0.031821	Acc: 57.2% (5721/10000)
[Test]  Epoch: 60	Loss: 0.031793	Acc: 57.2% (5718/10000)
[Test]  Epoch: 61	Loss: 0.031626	Acc: 57.2% (5720/10000)
[Test]  Epoch: 62	Loss: 0.031725	Acc: 57.3% (5734/10000)
[Test]  Epoch: 63	Loss: 0.031723	Acc: 57.2% (5717/10000)
[Test]  Epoch: 64	Loss: 0.031692	Acc: 57.4% (5737/10000)
[Test]  Epoch: 65	Loss: 0.031663	Acc: 57.3% (5732/10000)
[Test]  Epoch: 66	Loss: 0.031575	Acc: 57.5% (5746/10000)
[Test]  Epoch: 67	Loss: 0.031788	Acc: 57.3% (5729/10000)
[Test]  Epoch: 68	Loss: 0.031689	Acc: 57.4% (5736/10000)
[Test]  Epoch: 69	Loss: 0.031566	Acc: 57.4% (5740/10000)
[Test]  Epoch: 70	Loss: 0.031671	Acc: 57.4% (5737/10000)
[Test]  Epoch: 71	Loss: 0.031559	Acc: 57.3% (5730/10000)
[Test]  Epoch: 72	Loss: 0.031643	Acc: 57.4% (5736/10000)
[Test]  Epoch: 73	Loss: 0.031573	Acc: 57.6% (5760/10000)
[Test]  Epoch: 74	Loss: 0.031600	Acc: 57.6% (5757/10000)
[Test]  Epoch: 75	Loss: 0.031594	Acc: 57.3% (5733/10000)
[Test]  Epoch: 76	Loss: 0.031773	Acc: 57.3% (5729/10000)
[Test]  Epoch: 77	Loss: 0.031741	Acc: 57.4% (5736/10000)
[Test]  Epoch: 78	Loss: 0.031563	Acc: 57.6% (5758/10000)
[Test]  Epoch: 79	Loss: 0.031636	Acc: 57.4% (5741/10000)
[Test]  Epoch: 80	Loss: 0.031573	Acc: 57.5% (5747/10000)
[Test]  Epoch: 81	Loss: 0.031489	Acc: 57.3% (5734/10000)
[Test]  Epoch: 82	Loss: 0.031611	Acc: 57.3% (5734/10000)
[Test]  Epoch: 83	Loss: 0.031690	Acc: 57.2% (5725/10000)
[Test]  Epoch: 84	Loss: 0.031664	Acc: 57.2% (5721/10000)
[Test]  Epoch: 85	Loss: 0.031587	Acc: 57.2% (5722/10000)
[Test]  Epoch: 86	Loss: 0.031635	Acc: 57.5% (5746/10000)
[Test]  Epoch: 87	Loss: 0.031480	Acc: 57.5% (5747/10000)
[Test]  Epoch: 88	Loss: 0.031524	Acc: 57.4% (5735/10000)
[Test]  Epoch: 89	Loss: 0.031597	Acc: 57.4% (5735/10000)
[Test]  Epoch: 90	Loss: 0.031396	Acc: 57.4% (5743/10000)
[Test]  Epoch: 91	Loss: 0.031491	Acc: 57.4% (5742/10000)
[Test]  Epoch: 92	Loss: 0.031665	Acc: 57.3% (5726/10000)
[Test]  Epoch: 93	Loss: 0.031520	Acc: 57.4% (5742/10000)
[Test]  Epoch: 94	Loss: 0.031357	Acc: 57.5% (5754/10000)
[Test]  Epoch: 95	Loss: 0.031548	Acc: 57.5% (5749/10000)
[Test]  Epoch: 96	Loss: 0.031499	Acc: 57.4% (5743/10000)
[Test]  Epoch: 97	Loss: 0.031561	Acc: 57.5% (5746/10000)
[Test]  Epoch: 98	Loss: 0.031474	Acc: 57.5% (5752/10000)
[Test]  Epoch: 99	Loss: 0.031595	Acc: 57.5% (5751/10000)
[Test]  Epoch: 100	Loss: 0.031529	Acc: 57.4% (5737/10000)
===========finish==========
['2024-08-19', '16:34:12.245065', '100', 'test', '0.03152940847873688', '57.37', '57.6']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn1.weight', 'layer2.3.bn1.weight', 'layer1.1.bn3.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn3.weight', 'layer3.4.bn1.weight', 'layer3.5.bn3.weight', 'layer3.4.bn2.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer3.2.bn1.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn1.weight', 'layer3.1.bn3.weight', 'layer3.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer1.2.conv3.weight', 'layer3.0.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.093764	Acc: 22.3% (2230/10000)
[Test]  Epoch: 2	Loss: 0.049568	Acc: 46.4% (4635/10000)
[Test]  Epoch: 3	Loss: 0.042052	Acc: 50.8% (5075/10000)
[Test]  Epoch: 4	Loss: 0.039314	Acc: 52.9% (5288/10000)
[Test]  Epoch: 5	Loss: 0.038148	Acc: 53.4% (5335/10000)
[Test]  Epoch: 6	Loss: 0.037759	Acc: 53.4% (5343/10000)
[Test]  Epoch: 7	Loss: 0.037548	Acc: 53.3% (5333/10000)
[Test]  Epoch: 8	Loss: 0.037234	Acc: 53.6% (5358/10000)
[Test]  Epoch: 9	Loss: 0.036731	Acc: 53.6% (5361/10000)
[Test]  Epoch: 10	Loss: 0.036233	Acc: 53.8% (5382/10000)
[Test]  Epoch: 11	Loss: 0.036229	Acc: 53.9% (5392/10000)
[Test]  Epoch: 12	Loss: 0.036068	Acc: 53.9% (5394/10000)
[Test]  Epoch: 13	Loss: 0.036076	Acc: 53.9% (5385/10000)
[Test]  Epoch: 14	Loss: 0.035784	Acc: 53.9% (5393/10000)
[Test]  Epoch: 15	Loss: 0.035670	Acc: 53.8% (5382/10000)
[Test]  Epoch: 16	Loss: 0.035506	Acc: 53.7% (5369/10000)
[Test]  Epoch: 17	Loss: 0.035213	Acc: 54.2% (5419/10000)
[Test]  Epoch: 18	Loss: 0.035054	Acc: 54.3% (5430/10000)
[Test]  Epoch: 19	Loss: 0.035116	Acc: 54.2% (5421/10000)
[Test]  Epoch: 20	Loss: 0.034710	Acc: 54.4% (5444/10000)
[Test]  Epoch: 21	Loss: 0.034532	Acc: 54.2% (5421/10000)
[Test]  Epoch: 22	Loss: 0.034491	Acc: 54.2% (5422/10000)
[Test]  Epoch: 23	Loss: 0.034382	Acc: 54.2% (5418/10000)
[Test]  Epoch: 24	Loss: 0.034424	Acc: 54.1% (5407/10000)
[Test]  Epoch: 25	Loss: 0.034339	Acc: 54.3% (5430/10000)
[Test]  Epoch: 26	Loss: 0.034397	Acc: 54.3% (5434/10000)
[Test]  Epoch: 27	Loss: 0.034486	Acc: 54.1% (5409/10000)
[Test]  Epoch: 28	Loss: 0.034081	Acc: 54.3% (5434/10000)
[Test]  Epoch: 29	Loss: 0.033926	Acc: 54.7% (5473/10000)
[Test]  Epoch: 30	Loss: 0.033820	Acc: 54.7% (5467/10000)
[Test]  Epoch: 31	Loss: 0.033827	Acc: 54.5% (5451/10000)
[Test]  Epoch: 32	Loss: 0.033781	Acc: 54.5% (5447/10000)
[Test]  Epoch: 33	Loss: 0.033600	Acc: 54.8% (5476/10000)
[Test]  Epoch: 34	Loss: 0.033451	Acc: 54.9% (5488/10000)
[Test]  Epoch: 35	Loss: 0.033409	Acc: 54.8% (5479/10000)
[Test]  Epoch: 36	Loss: 0.033492	Acc: 54.8% (5477/10000)
[Test]  Epoch: 37	Loss: 0.033351	Acc: 54.8% (5476/10000)
[Test]  Epoch: 38	Loss: 0.033222	Acc: 55.0% (5495/10000)
[Test]  Epoch: 39	Loss: 0.033338	Acc: 54.8% (5484/10000)
[Test]  Epoch: 40	Loss: 0.033214	Acc: 54.8% (5484/10000)
[Test]  Epoch: 41	Loss: 0.033074	Acc: 55.0% (5502/10000)
[Test]  Epoch: 42	Loss: 0.032860	Acc: 55.0% (5504/10000)
[Test]  Epoch: 43	Loss: 0.032955	Acc: 55.1% (5513/10000)
[Test]  Epoch: 44	Loss: 0.032870	Acc: 55.0% (5503/10000)
[Test]  Epoch: 45	Loss: 0.032795	Acc: 55.1% (5509/10000)
[Test]  Epoch: 46	Loss: 0.032702	Acc: 55.2% (5516/10000)
[Test]  Epoch: 47	Loss: 0.032782	Acc: 55.2% (5516/10000)
[Test]  Epoch: 48	Loss: 0.032571	Acc: 55.2% (5525/10000)
[Test]  Epoch: 49	Loss: 0.032478	Acc: 55.3% (5528/10000)
[Test]  Epoch: 50	Loss: 0.032505	Acc: 55.4% (5543/10000)
[Test]  Epoch: 51	Loss: 0.032625	Acc: 55.3% (5526/10000)
[Test]  Epoch: 52	Loss: 0.032372	Acc: 55.3% (5531/10000)
[Test]  Epoch: 53	Loss: 0.032325	Acc: 55.4% (5537/10000)
[Test]  Epoch: 54	Loss: 0.032415	Acc: 55.2% (5524/10000)
[Test]  Epoch: 55	Loss: 0.032367	Acc: 55.3% (5530/10000)
[Test]  Epoch: 56	Loss: 0.032315	Acc: 55.4% (5542/10000)
[Test]  Epoch: 57	Loss: 0.032307	Acc: 55.5% (5546/10000)
[Test]  Epoch: 58	Loss: 0.032206	Acc: 55.4% (5538/10000)
[Test]  Epoch: 59	Loss: 0.032186	Acc: 55.6% (5565/10000)
[Test]  Epoch: 60	Loss: 0.032301	Acc: 55.3% (5533/10000)
[Test]  Epoch: 61	Loss: 0.032196	Acc: 55.4% (5540/10000)
[Test]  Epoch: 62	Loss: 0.032231	Acc: 55.5% (5550/10000)
[Test]  Epoch: 63	Loss: 0.032190	Acc: 55.5% (5549/10000)
[Test]  Epoch: 64	Loss: 0.032167	Acc: 55.4% (5540/10000)
[Test]  Epoch: 65	Loss: 0.032170	Acc: 55.5% (5547/10000)
[Test]  Epoch: 66	Loss: 0.032089	Acc: 55.4% (5537/10000)
[Test]  Epoch: 67	Loss: 0.032189	Acc: 55.3% (5531/10000)
[Test]  Epoch: 68	Loss: 0.032184	Acc: 55.5% (5548/10000)
[Test]  Epoch: 69	Loss: 0.031979	Acc: 55.5% (5553/10000)
[Test]  Epoch: 70	Loss: 0.032061	Acc: 55.4% (5537/10000)
[Test]  Epoch: 71	Loss: 0.032013	Acc: 55.5% (5546/10000)
[Test]  Epoch: 72	Loss: 0.032133	Acc: 55.3% (5534/10000)
[Test]  Epoch: 73	Loss: 0.032030	Acc: 55.5% (5550/10000)
[Test]  Epoch: 74	Loss: 0.032088	Acc: 55.4% (5542/10000)
[Test]  Epoch: 75	Loss: 0.032009	Acc: 55.5% (5555/10000)
[Test]  Epoch: 76	Loss: 0.032126	Acc: 55.4% (5535/10000)
[Test]  Epoch: 77	Loss: 0.032063	Acc: 55.4% (5539/10000)
[Test]  Epoch: 78	Loss: 0.032059	Acc: 55.5% (5554/10000)
[Test]  Epoch: 79	Loss: 0.032056	Acc: 55.5% (5546/10000)
[Test]  Epoch: 80	Loss: 0.031958	Acc: 55.5% (5555/10000)
[Test]  Epoch: 81	Loss: 0.032069	Acc: 55.4% (5543/10000)
[Test]  Epoch: 82	Loss: 0.032044	Acc: 55.5% (5549/10000)
[Test]  Epoch: 83	Loss: 0.032103	Acc: 55.4% (5537/10000)
[Test]  Epoch: 84	Loss: 0.032129	Acc: 55.4% (5539/10000)
[Test]  Epoch: 85	Loss: 0.032001	Acc: 55.5% (5552/10000)
[Test]  Epoch: 86	Loss: 0.031951	Acc: 55.6% (5561/10000)
[Test]  Epoch: 87	Loss: 0.031807	Acc: 55.6% (5563/10000)
[Test]  Epoch: 88	Loss: 0.031968	Acc: 55.5% (5552/10000)
[Test]  Epoch: 89	Loss: 0.031917	Acc: 55.6% (5558/10000)
[Test]  Epoch: 90	Loss: 0.031928	Acc: 55.4% (5542/10000)
[Test]  Epoch: 91	Loss: 0.031942	Acc: 55.6% (5559/10000)
[Test]  Epoch: 92	Loss: 0.032050	Acc: 55.5% (5545/10000)
[Test]  Epoch: 93	Loss: 0.031880	Acc: 55.5% (5548/10000)
[Test]  Epoch: 94	Loss: 0.031891	Acc: 55.5% (5553/10000)
[Test]  Epoch: 95	Loss: 0.031983	Acc: 55.5% (5552/10000)
[Test]  Epoch: 96	Loss: 0.031973	Acc: 55.5% (5549/10000)
[Test]  Epoch: 97	Loss: 0.031992	Acc: 55.5% (5550/10000)
[Test]  Epoch: 98	Loss: 0.031965	Acc: 55.4% (5539/10000)
[Test]  Epoch: 99	Loss: 0.031922	Acc: 55.5% (5554/10000)
[Test]  Epoch: 100	Loss: 0.031923	Acc: 55.5% (5554/10000)
===========finish==========
['2024-08-19', '16:38:46.894902', '100', 'test', '0.03192334358692169', '55.54', '55.65']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn1.weight', 'layer2.3.bn1.weight', 'layer1.1.bn3.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn3.weight', 'layer3.4.bn1.weight', 'layer3.5.bn3.weight', 'layer3.4.bn2.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer3.2.bn1.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn1.weight', 'layer3.1.bn3.weight', 'layer3.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer1.2.conv3.weight', 'layer3.0.bn2.weight', 'layer1.1.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.5.conv3.weight', 'layer3.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer3.3.conv3.weight', 'layer1.0.conv3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.137766	Acc: 20.7% (2073/10000)
[Test]  Epoch: 2	Loss: 0.048553	Acc: 46.1% (4606/10000)
[Test]  Epoch: 3	Loss: 0.042460	Acc: 49.3% (4926/10000)
[Test]  Epoch: 4	Loss: 0.040530	Acc: 50.4% (5035/10000)
[Test]  Epoch: 5	Loss: 0.038946	Acc: 51.4% (5141/10000)
[Test]  Epoch: 6	Loss: 0.038502	Acc: 52.1% (5206/10000)
[Test]  Epoch: 7	Loss: 0.037867	Acc: 52.5% (5245/10000)
[Test]  Epoch: 8	Loss: 0.038015	Acc: 52.0% (5201/10000)
[Test]  Epoch: 9	Loss: 0.037804	Acc: 52.1% (5211/10000)
[Test]  Epoch: 10	Loss: 0.037634	Acc: 52.5% (5255/10000)
[Test]  Epoch: 11	Loss: 0.037442	Acc: 52.5% (5251/10000)
[Test]  Epoch: 12	Loss: 0.037448	Acc: 52.4% (5236/10000)
[Test]  Epoch: 13	Loss: 0.037066	Acc: 52.6% (5261/10000)
[Test]  Epoch: 14	Loss: 0.036882	Acc: 52.6% (5261/10000)
[Test]  Epoch: 15	Loss: 0.036536	Acc: 52.8% (5277/10000)
[Test]  Epoch: 16	Loss: 0.036687	Acc: 52.5% (5254/10000)
[Test]  Epoch: 17	Loss: 0.036435	Acc: 52.6% (5259/10000)
[Test]  Epoch: 18	Loss: 0.036325	Acc: 52.9% (5285/10000)
[Test]  Epoch: 19	Loss: 0.036157	Acc: 53.0% (5304/10000)
[Test]  Epoch: 20	Loss: 0.035676	Acc: 53.4% (5341/10000)
[Test]  Epoch: 21	Loss: 0.035720	Acc: 53.3% (5334/10000)
[Test]  Epoch: 22	Loss: 0.035470	Acc: 53.1% (5315/10000)
[Test]  Epoch: 23	Loss: 0.035348	Acc: 53.2% (5325/10000)
[Test]  Epoch: 24	Loss: 0.035286	Acc: 53.0% (5305/10000)
[Test]  Epoch: 25	Loss: 0.035333	Acc: 53.0% (5298/10000)
[Test]  Epoch: 26	Loss: 0.035285	Acc: 53.1% (5312/10000)
[Test]  Epoch: 27	Loss: 0.035142	Acc: 53.0% (5298/10000)
[Test]  Epoch: 28	Loss: 0.034859	Acc: 53.3% (5332/10000)
[Test]  Epoch: 29	Loss: 0.034832	Acc: 53.5% (5349/10000)
[Test]  Epoch: 30	Loss: 0.034846	Acc: 53.4% (5339/10000)
[Test]  Epoch: 31	Loss: 0.034835	Acc: 53.2% (5318/10000)
[Test]  Epoch: 32	Loss: 0.034789	Acc: 53.3% (5334/10000)
[Test]  Epoch: 33	Loss: 0.034667	Acc: 53.4% (5343/10000)
[Test]  Epoch: 34	Loss: 0.034608	Acc: 53.5% (5348/10000)
[Test]  Epoch: 35	Loss: 0.034328	Acc: 53.8% (5379/10000)
[Test]  Epoch: 36	Loss: 0.034179	Acc: 53.7% (5370/10000)
[Test]  Epoch: 37	Loss: 0.034124	Acc: 53.7% (5374/10000)
[Test]  Epoch: 38	Loss: 0.034043	Acc: 53.8% (5379/10000)
[Test]  Epoch: 39	Loss: 0.034091	Acc: 53.8% (5382/10000)
[Test]  Epoch: 40	Loss: 0.034023	Acc: 53.8% (5383/10000)
[Test]  Epoch: 41	Loss: 0.033846	Acc: 54.1% (5407/10000)
[Test]  Epoch: 42	Loss: 0.033852	Acc: 53.9% (5393/10000)
[Test]  Epoch: 43	Loss: 0.033829	Acc: 54.2% (5418/10000)
[Test]  Epoch: 44	Loss: 0.033535	Acc: 54.0% (5405/10000)
[Test]  Epoch: 45	Loss: 0.033689	Acc: 54.1% (5407/10000)
[Test]  Epoch: 46	Loss: 0.033582	Acc: 53.8% (5382/10000)
[Test]  Epoch: 47	Loss: 0.033520	Acc: 54.0% (5400/10000)
[Test]  Epoch: 48	Loss: 0.033484	Acc: 54.2% (5421/10000)
[Test]  Epoch: 49	Loss: 0.033426	Acc: 54.2% (5418/10000)
[Test]  Epoch: 50	Loss: 0.033528	Acc: 54.0% (5397/10000)
[Test]  Epoch: 51	Loss: 0.033497	Acc: 53.7% (5374/10000)
[Test]  Epoch: 52	Loss: 0.033266	Acc: 54.0% (5404/10000)
[Test]  Epoch: 53	Loss: 0.033184	Acc: 53.9% (5386/10000)
[Test]  Epoch: 54	Loss: 0.033180	Acc: 54.0% (5396/10000)
[Test]  Epoch: 55	Loss: 0.033086	Acc: 54.1% (5413/10000)
[Test]  Epoch: 56	Loss: 0.033073	Acc: 54.3% (5433/10000)
[Test]  Epoch: 57	Loss: 0.032956	Acc: 54.5% (5449/10000)
[Test]  Epoch: 58	Loss: 0.033080	Acc: 54.2% (5424/10000)
[Test]  Epoch: 59	Loss: 0.033085	Acc: 54.0% (5405/10000)
[Test]  Epoch: 60	Loss: 0.032988	Acc: 54.4% (5436/10000)
[Test]  Epoch: 61	Loss: 0.032906	Acc: 54.4% (5435/10000)
[Test]  Epoch: 62	Loss: 0.032979	Acc: 54.2% (5424/10000)
[Test]  Epoch: 63	Loss: 0.032961	Acc: 54.4% (5439/10000)
[Test]  Epoch: 64	Loss: 0.032943	Acc: 54.3% (5431/10000)
[Test]  Epoch: 65	Loss: 0.032904	Acc: 54.2% (5425/10000)
[Test]  Epoch: 66	Loss: 0.032849	Acc: 54.3% (5432/10000)
[Test]  Epoch: 67	Loss: 0.033026	Acc: 54.2% (5417/10000)
[Test]  Epoch: 68	Loss: 0.032812	Acc: 54.3% (5434/10000)
[Test]  Epoch: 69	Loss: 0.032798	Acc: 54.4% (5441/10000)
[Test]  Epoch: 70	Loss: 0.032941	Acc: 54.4% (5436/10000)
[Test]  Epoch: 71	Loss: 0.032787	Acc: 54.4% (5441/10000)
[Test]  Epoch: 72	Loss: 0.032790	Acc: 54.5% (5445/10000)
[Test]  Epoch: 73	Loss: 0.032880	Acc: 54.5% (5454/10000)
[Test]  Epoch: 74	Loss: 0.032851	Acc: 54.5% (5446/10000)
[Test]  Epoch: 75	Loss: 0.032837	Acc: 54.4% (5443/10000)
[Test]  Epoch: 76	Loss: 0.032905	Acc: 54.2% (5417/10000)
[Test]  Epoch: 77	Loss: 0.032823	Acc: 54.2% (5421/10000)
[Test]  Epoch: 78	Loss: 0.032811	Acc: 54.4% (5438/10000)
[Test]  Epoch: 79	Loss: 0.032922	Acc: 54.2% (5421/10000)
[Test]  Epoch: 80	Loss: 0.032834	Acc: 54.4% (5439/10000)
[Test]  Epoch: 81	Loss: 0.032830	Acc: 54.2% (5425/10000)
[Test]  Epoch: 82	Loss: 0.032749	Acc: 54.4% (5435/10000)
[Test]  Epoch: 83	Loss: 0.032783	Acc: 54.4% (5440/10000)
[Test]  Epoch: 84	Loss: 0.032761	Acc: 54.2% (5425/10000)
[Test]  Epoch: 85	Loss: 0.032801	Acc: 54.5% (5450/10000)
[Test]  Epoch: 86	Loss: 0.032781	Acc: 54.5% (5448/10000)
[Test]  Epoch: 87	Loss: 0.032667	Acc: 54.5% (5445/10000)
[Test]  Epoch: 88	Loss: 0.032710	Acc: 54.4% (5440/10000)
[Test]  Epoch: 89	Loss: 0.032741	Acc: 54.4% (5436/10000)
[Test]  Epoch: 90	Loss: 0.032662	Acc: 54.4% (5442/10000)
[Test]  Epoch: 91	Loss: 0.032731	Acc: 54.6% (5459/10000)
[Test]  Epoch: 92	Loss: 0.032824	Acc: 54.3% (5433/10000)
[Test]  Epoch: 93	Loss: 0.032687	Acc: 54.5% (5455/10000)
[Test]  Epoch: 94	Loss: 0.032651	Acc: 54.4% (5441/10000)
[Test]  Epoch: 95	Loss: 0.032783	Acc: 54.3% (5431/10000)
[Test]  Epoch: 96	Loss: 0.032782	Acc: 54.3% (5427/10000)
[Test]  Epoch: 97	Loss: 0.032850	Acc: 54.4% (5437/10000)
[Test]  Epoch: 98	Loss: 0.032643	Acc: 54.6% (5458/10000)
[Test]  Epoch: 99	Loss: 0.032699	Acc: 54.6% (5456/10000)
[Test]  Epoch: 100	Loss: 0.032767	Acc: 54.5% (5452/10000)
===========finish==========
['2024-08-19', '16:43:14.123629', '100', 'test', '0.0327666233420372', '54.52', '54.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn1.weight', 'layer2.3.bn1.weight', 'layer1.1.bn3.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn3.weight', 'layer3.4.bn1.weight', 'layer3.5.bn3.weight', 'layer3.4.bn2.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer3.2.bn1.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn1.weight', 'layer3.1.bn3.weight', 'layer3.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer1.2.conv3.weight', 'layer3.0.bn2.weight', 'layer1.1.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.5.conv3.weight', 'layer3.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer3.3.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer3.4.conv3.weight', 'layer3.5.conv2.weight', 'layer3.3.conv1.weight', 'layer3.5.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'layer1.2.conv2.weight', 'bn1.weight', 'layer1.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.126229	Acc: 18.7% (1866/10000)
[Test]  Epoch: 2	Loss: 0.049391	Acc: 43.7% (4373/10000)
[Test]  Epoch: 3	Loss: 0.042874	Acc: 47.9% (4786/10000)
[Test]  Epoch: 4	Loss: 0.040977	Acc: 49.0% (4903/10000)
[Test]  Epoch: 5	Loss: 0.041305	Acc: 48.9% (4891/10000)
[Test]  Epoch: 6	Loss: 0.040375	Acc: 49.7% (4971/10000)
[Test]  Epoch: 7	Loss: 0.039382	Acc: 50.0% (4995/10000)
[Test]  Epoch: 8	Loss: 0.039093	Acc: 50.1% (5015/10000)
[Test]  Epoch: 9	Loss: 0.038765	Acc: 50.3% (5029/10000)
[Test]  Epoch: 10	Loss: 0.038178	Acc: 50.5% (5050/10000)
[Test]  Epoch: 11	Loss: 0.037981	Acc: 50.8% (5076/10000)
[Test]  Epoch: 12	Loss: 0.037929	Acc: 50.8% (5080/10000)
[Test]  Epoch: 13	Loss: 0.038046	Acc: 50.5% (5052/10000)
[Test]  Epoch: 14	Loss: 0.037567	Acc: 50.9% (5094/10000)
[Test]  Epoch: 15	Loss: 0.037334	Acc: 51.0% (5104/10000)
[Test]  Epoch: 16	Loss: 0.037199	Acc: 50.7% (5066/10000)
[Test]  Epoch: 17	Loss: 0.036984	Acc: 51.0% (5097/10000)
[Test]  Epoch: 18	Loss: 0.036866	Acc: 51.0% (5101/10000)
[Test]  Epoch: 19	Loss: 0.037065	Acc: 50.9% (5088/10000)
[Test]  Epoch: 20	Loss: 0.036670	Acc: 51.0% (5101/10000)
[Test]  Epoch: 21	Loss: 0.036622	Acc: 51.2% (5123/10000)
[Test]  Epoch: 22	Loss: 0.036390	Acc: 51.2% (5122/10000)
[Test]  Epoch: 23	Loss: 0.036154	Acc: 51.4% (5138/10000)
[Test]  Epoch: 24	Loss: 0.036026	Acc: 51.5% (5155/10000)
[Test]  Epoch: 25	Loss: 0.035973	Acc: 51.5% (5150/10000)
[Test]  Epoch: 26	Loss: 0.036269	Acc: 51.3% (5127/10000)
[Test]  Epoch: 27	Loss: 0.036182	Acc: 51.5% (5154/10000)
[Test]  Epoch: 28	Loss: 0.035918	Acc: 51.6% (5165/10000)
[Test]  Epoch: 29	Loss: 0.035944	Acc: 51.5% (5152/10000)
[Test]  Epoch: 30	Loss: 0.035681	Acc: 51.7% (5169/10000)
[Test]  Epoch: 31	Loss: 0.035536	Acc: 51.6% (5161/10000)
[Test]  Epoch: 32	Loss: 0.035620	Acc: 51.4% (5138/10000)
[Test]  Epoch: 33	Loss: 0.035641	Acc: 51.8% (5183/10000)
[Test]  Epoch: 34	Loss: 0.035565	Acc: 51.9% (5187/10000)
[Test]  Epoch: 35	Loss: 0.035249	Acc: 51.8% (5176/10000)
[Test]  Epoch: 36	Loss: 0.035297	Acc: 51.6% (5164/10000)
[Test]  Epoch: 37	Loss: 0.035264	Acc: 51.5% (5149/10000)
[Test]  Epoch: 38	Loss: 0.035152	Acc: 51.6% (5156/10000)
[Test]  Epoch: 39	Loss: 0.035183	Acc: 51.5% (5155/10000)
[Test]  Epoch: 40	Loss: 0.035039	Acc: 52.0% (5200/10000)
[Test]  Epoch: 41	Loss: 0.034834	Acc: 52.0% (5200/10000)
[Test]  Epoch: 42	Loss: 0.034747	Acc: 52.0% (5202/10000)
[Test]  Epoch: 43	Loss: 0.034850	Acc: 52.0% (5204/10000)
[Test]  Epoch: 44	Loss: 0.034650	Acc: 51.8% (5184/10000)
[Test]  Epoch: 45	Loss: 0.034632	Acc: 52.2% (5220/10000)
[Test]  Epoch: 46	Loss: 0.034539	Acc: 52.1% (5209/10000)
[Test]  Epoch: 47	Loss: 0.034502	Acc: 52.0% (5205/10000)
[Test]  Epoch: 48	Loss: 0.034417	Acc: 52.2% (5219/10000)
[Test]  Epoch: 49	Loss: 0.034230	Acc: 52.2% (5221/10000)
[Test]  Epoch: 50	Loss: 0.034249	Acc: 52.2% (5219/10000)
[Test]  Epoch: 51	Loss: 0.034241	Acc: 52.1% (5210/10000)
[Test]  Epoch: 52	Loss: 0.034084	Acc: 52.2% (5222/10000)
[Test]  Epoch: 53	Loss: 0.034124	Acc: 52.2% (5217/10000)
[Test]  Epoch: 54	Loss: 0.034142	Acc: 52.2% (5224/10000)
[Test]  Epoch: 55	Loss: 0.034116	Acc: 52.1% (5208/10000)
[Test]  Epoch: 56	Loss: 0.034141	Acc: 52.3% (5226/10000)
[Test]  Epoch: 57	Loss: 0.034039	Acc: 52.3% (5232/10000)
[Test]  Epoch: 58	Loss: 0.034038	Acc: 52.1% (5212/10000)
[Test]  Epoch: 59	Loss: 0.034059	Acc: 52.1% (5209/10000)
[Test]  Epoch: 60	Loss: 0.034066	Acc: 52.3% (5231/10000)
[Test]  Epoch: 61	Loss: 0.033922	Acc: 52.2% (5224/10000)
[Test]  Epoch: 62	Loss: 0.033924	Acc: 52.3% (5227/10000)
[Test]  Epoch: 63	Loss: 0.033898	Acc: 52.5% (5253/10000)
[Test]  Epoch: 64	Loss: 0.033991	Acc: 52.3% (5231/10000)
[Test]  Epoch: 65	Loss: 0.033954	Acc: 52.1% (5213/10000)
[Test]  Epoch: 66	Loss: 0.033927	Acc: 52.2% (5216/10000)
[Test]  Epoch: 67	Loss: 0.033854	Acc: 52.3% (5233/10000)
[Test]  Epoch: 68	Loss: 0.033939	Acc: 52.1% (5209/10000)
[Test]  Epoch: 69	Loss: 0.033787	Acc: 52.2% (5218/10000)
[Test]  Epoch: 70	Loss: 0.033770	Acc: 52.3% (5232/10000)
[Test]  Epoch: 71	Loss: 0.033822	Acc: 52.4% (5241/10000)
[Test]  Epoch: 72	Loss: 0.033772	Acc: 52.4% (5240/10000)
[Test]  Epoch: 73	Loss: 0.033821	Acc: 52.4% (5235/10000)
[Test]  Epoch: 74	Loss: 0.033861	Acc: 52.3% (5233/10000)
[Test]  Epoch: 75	Loss: 0.033697	Acc: 52.4% (5241/10000)
[Test]  Epoch: 76	Loss: 0.033943	Acc: 52.0% (5204/10000)
[Test]  Epoch: 77	Loss: 0.033900	Acc: 52.3% (5226/10000)
[Test]  Epoch: 78	Loss: 0.033735	Acc: 52.2% (5223/10000)
[Test]  Epoch: 79	Loss: 0.033923	Acc: 52.2% (5219/10000)
[Test]  Epoch: 80	Loss: 0.033808	Acc: 52.3% (5232/10000)
[Test]  Epoch: 81	Loss: 0.033792	Acc: 52.5% (5248/10000)
[Test]  Epoch: 82	Loss: 0.033759	Acc: 52.2% (5216/10000)
[Test]  Epoch: 83	Loss: 0.033800	Acc: 52.1% (5209/10000)
[Test]  Epoch: 84	Loss: 0.033878	Acc: 52.2% (5220/10000)
[Test]  Epoch: 85	Loss: 0.033817	Acc: 52.4% (5236/10000)
[Test]  Epoch: 86	Loss: 0.033718	Acc: 52.6% (5261/10000)
[Test]  Epoch: 87	Loss: 0.033713	Acc: 52.7% (5267/10000)
[Test]  Epoch: 88	Loss: 0.033747	Acc: 52.5% (5250/10000)
[Test]  Epoch: 89	Loss: 0.033615	Acc: 52.4% (5240/10000)
[Test]  Epoch: 90	Loss: 0.033683	Acc: 52.5% (5245/10000)
[Test]  Epoch: 91	Loss: 0.033825	Acc: 52.3% (5232/10000)
[Test]  Epoch: 92	Loss: 0.033715	Acc: 52.3% (5226/10000)
[Test]  Epoch: 93	Loss: 0.033536	Acc: 52.2% (5221/10000)
[Test]  Epoch: 94	Loss: 0.033535	Acc: 52.5% (5250/10000)
[Test]  Epoch: 95	Loss: 0.033765	Acc: 52.4% (5239/10000)
[Test]  Epoch: 96	Loss: 0.033846	Acc: 52.2% (5221/10000)
[Test]  Epoch: 97	Loss: 0.033733	Acc: 52.4% (5243/10000)
[Test]  Epoch: 98	Loss: 0.033706	Acc: 52.2% (5222/10000)
[Test]  Epoch: 99	Loss: 0.033746	Acc: 52.2% (5222/10000)
[Test]  Epoch: 100	Loss: 0.033585	Acc: 52.5% (5249/10000)
===========finish==========
['2024-08-19', '16:47:47.036928', '100', 'test', '0.03358463851213455', '52.49', '52.67']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn1.weight', 'layer2.3.bn1.weight', 'layer1.1.bn3.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn3.weight', 'layer3.4.bn1.weight', 'layer3.5.bn3.weight', 'layer3.4.bn2.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer3.2.bn1.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn1.weight', 'layer3.1.bn3.weight', 'layer3.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer1.2.conv3.weight', 'layer3.0.bn2.weight', 'layer1.1.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.5.conv3.weight', 'layer3.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer3.3.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer3.4.conv3.weight', 'layer3.5.conv2.weight', 'layer3.3.conv1.weight', 'layer3.5.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'layer1.2.conv2.weight', 'bn1.weight', 'layer1.0.conv2.weight', 'layer3.4.conv1.weight', 'layer3.3.conv2.weight', 'layer2.3.conv1.weight', 'layer3.2.conv3.weight', 'layer3.2.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.conv1.weight', 'layer2.3.conv3.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer2.1.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.139551	Acc: 20.9% (2090/10000)
[Test]  Epoch: 2	Loss: 0.052178	Acc: 37.6% (3756/10000)
[Test]  Epoch: 3	Loss: 0.044707	Acc: 42.6% (4257/10000)
[Test]  Epoch: 4	Loss: 0.042730	Acc: 43.7% (4368/10000)
[Test]  Epoch: 5	Loss: 0.042200	Acc: 44.0% (4403/10000)
[Test]  Epoch: 6	Loss: 0.040885	Acc: 45.2% (4525/10000)
[Test]  Epoch: 7	Loss: 0.040609	Acc: 45.4% (4541/10000)
[Test]  Epoch: 8	Loss: 0.040493	Acc: 45.6% (4556/10000)
[Test]  Epoch: 9	Loss: 0.040891	Acc: 45.5% (4550/10000)
[Test]  Epoch: 10	Loss: 0.040151	Acc: 46.1% (4607/10000)
[Test]  Epoch: 11	Loss: 0.040381	Acc: 45.7% (4573/10000)
[Test]  Epoch: 12	Loss: 0.040162	Acc: 46.0% (4601/10000)
[Test]  Epoch: 13	Loss: 0.040119	Acc: 46.2% (4616/10000)
[Test]  Epoch: 14	Loss: 0.039856	Acc: 46.0% (4597/10000)
[Test]  Epoch: 15	Loss: 0.039472	Acc: 46.0% (4602/10000)
[Test]  Epoch: 16	Loss: 0.039290	Acc: 46.4% (4638/10000)
[Test]  Epoch: 17	Loss: 0.039233	Acc: 46.3% (4629/10000)
[Test]  Epoch: 18	Loss: 0.039143	Acc: 46.1% (4613/10000)
[Test]  Epoch: 19	Loss: 0.039103	Acc: 46.0% (4602/10000)
[Test]  Epoch: 20	Loss: 0.038765	Acc: 46.4% (4639/10000)
[Test]  Epoch: 21	Loss: 0.038655	Acc: 46.5% (4650/10000)
[Test]  Epoch: 22	Loss: 0.038565	Acc: 46.2% (4618/10000)
[Test]  Epoch: 23	Loss: 0.038621	Acc: 46.4% (4640/10000)
[Test]  Epoch: 24	Loss: 0.038310	Acc: 46.4% (4638/10000)
[Test]  Epoch: 25	Loss: 0.038245	Acc: 46.5% (4655/10000)
[Test]  Epoch: 26	Loss: 0.038179	Acc: 46.6% (4664/10000)
[Test]  Epoch: 27	Loss: 0.038253	Acc: 46.5% (4653/10000)
[Test]  Epoch: 28	Loss: 0.038220	Acc: 46.6% (4658/10000)
[Test]  Epoch: 29	Loss: 0.037949	Acc: 46.6% (4657/10000)
[Test]  Epoch: 30	Loss: 0.038079	Acc: 46.7% (4672/10000)
[Test]  Epoch: 31	Loss: 0.038104	Acc: 46.5% (4649/10000)
[Test]  Epoch: 32	Loss: 0.037941	Acc: 46.9% (4689/10000)
[Test]  Epoch: 33	Loss: 0.037794	Acc: 46.9% (4685/10000)
[Test]  Epoch: 34	Loss: 0.037825	Acc: 46.7% (4673/10000)
[Test]  Epoch: 35	Loss: 0.037711	Acc: 46.8% (4681/10000)
[Test]  Epoch: 36	Loss: 0.037595	Acc: 47.0% (4702/10000)
[Test]  Epoch: 37	Loss: 0.037436	Acc: 46.9% (4692/10000)
[Test]  Epoch: 38	Loss: 0.037242	Acc: 47.2% (4717/10000)
[Test]  Epoch: 39	Loss: 0.037299	Acc: 47.0% (4703/10000)
[Test]  Epoch: 40	Loss: 0.037212	Acc: 46.9% (4694/10000)
[Test]  Epoch: 41	Loss: 0.037093	Acc: 47.3% (4732/10000)
[Test]  Epoch: 42	Loss: 0.037085	Acc: 47.4% (4741/10000)
[Test]  Epoch: 43	Loss: 0.037259	Acc: 47.2% (4716/10000)
[Test]  Epoch: 44	Loss: 0.037144	Acc: 47.1% (4709/10000)
[Test]  Epoch: 45	Loss: 0.037073	Acc: 47.3% (4728/10000)
[Test]  Epoch: 46	Loss: 0.037235	Acc: 47.1% (4715/10000)
[Test]  Epoch: 47	Loss: 0.037182	Acc: 47.1% (4709/10000)
[Test]  Epoch: 48	Loss: 0.037005	Acc: 47.0% (4703/10000)
[Test]  Epoch: 49	Loss: 0.036915	Acc: 47.2% (4722/10000)
[Test]  Epoch: 50	Loss: 0.036733	Acc: 47.1% (4709/10000)
[Test]  Epoch: 51	Loss: 0.036850	Acc: 47.3% (4727/10000)
[Test]  Epoch: 52	Loss: 0.036779	Acc: 47.3% (4733/10000)
[Test]  Epoch: 53	Loss: 0.036734	Acc: 47.2% (4725/10000)
[Test]  Epoch: 54	Loss: 0.036883	Acc: 47.2% (4725/10000)
[Test]  Epoch: 55	Loss: 0.036820	Acc: 47.5% (4747/10000)
[Test]  Epoch: 56	Loss: 0.036665	Acc: 47.4% (4742/10000)
[Test]  Epoch: 57	Loss: 0.036515	Acc: 47.5% (4754/10000)
[Test]  Epoch: 58	Loss: 0.036510	Acc: 47.6% (4764/10000)
[Test]  Epoch: 59	Loss: 0.036437	Acc: 47.4% (4744/10000)
[Test]  Epoch: 60	Loss: 0.036496	Acc: 47.4% (4744/10000)
[Test]  Epoch: 61	Loss: 0.036464	Acc: 47.5% (4747/10000)
[Test]  Epoch: 62	Loss: 0.036538	Acc: 47.4% (4742/10000)
[Test]  Epoch: 63	Loss: 0.036423	Acc: 47.6% (4763/10000)
[Test]  Epoch: 64	Loss: 0.036541	Acc: 47.5% (4755/10000)
[Test]  Epoch: 65	Loss: 0.036471	Acc: 47.5% (4751/10000)
[Test]  Epoch: 66	Loss: 0.036353	Acc: 47.6% (4757/10000)
[Test]  Epoch: 67	Loss: 0.036464	Acc: 47.6% (4758/10000)
[Test]  Epoch: 68	Loss: 0.036438	Acc: 47.6% (4764/10000)
[Test]  Epoch: 69	Loss: 0.036403	Acc: 47.6% (4759/10000)
[Test]  Epoch: 70	Loss: 0.036408	Acc: 47.7% (4773/10000)
[Test]  Epoch: 71	Loss: 0.036450	Acc: 47.6% (4758/10000)
[Test]  Epoch: 72	Loss: 0.036410	Acc: 47.7% (4769/10000)
[Test]  Epoch: 73	Loss: 0.036416	Acc: 47.6% (4761/10000)
[Test]  Epoch: 74	Loss: 0.036369	Acc: 47.6% (4762/10000)
[Test]  Epoch: 75	Loss: 0.036464	Acc: 47.5% (4755/10000)
[Test]  Epoch: 76	Loss: 0.036493	Acc: 47.6% (4760/10000)
[Test]  Epoch: 77	Loss: 0.036460	Acc: 47.5% (4752/10000)
[Test]  Epoch: 78	Loss: 0.036299	Acc: 47.6% (4759/10000)
[Test]  Epoch: 79	Loss: 0.036328	Acc: 47.7% (4768/10000)
[Test]  Epoch: 80	Loss: 0.036425	Acc: 47.5% (4754/10000)
[Test]  Epoch: 81	Loss: 0.036361	Acc: 47.6% (4756/10000)
[Test]  Epoch: 82	Loss: 0.036336	Acc: 47.6% (4761/10000)
[Test]  Epoch: 83	Loss: 0.036392	Acc: 47.6% (4761/10000)
[Test]  Epoch: 84	Loss: 0.036354	Acc: 47.8% (4775/10000)
[Test]  Epoch: 85	Loss: 0.036361	Acc: 47.6% (4763/10000)
[Test]  Epoch: 86	Loss: 0.036342	Acc: 47.5% (4746/10000)
[Test]  Epoch: 87	Loss: 0.036369	Acc: 47.7% (4766/10000)
[Test]  Epoch: 88	Loss: 0.036318	Acc: 47.5% (4748/10000)
[Test]  Epoch: 89	Loss: 0.036469	Acc: 47.5% (4754/10000)
[Test]  Epoch: 90	Loss: 0.036387	Acc: 47.6% (4759/10000)
[Test]  Epoch: 91	Loss: 0.036411	Acc: 47.8% (4775/10000)
[Test]  Epoch: 92	Loss: 0.036376	Acc: 47.4% (4741/10000)
[Test]  Epoch: 93	Loss: 0.036269	Acc: 47.5% (4754/10000)
[Test]  Epoch: 94	Loss: 0.036357	Acc: 47.6% (4759/10000)
[Test]  Epoch: 95	Loss: 0.036384	Acc: 47.3% (4732/10000)
[Test]  Epoch: 96	Loss: 0.036402	Acc: 47.6% (4765/10000)
[Test]  Epoch: 97	Loss: 0.036450	Acc: 47.6% (4758/10000)
[Test]  Epoch: 98	Loss: 0.036220	Acc: 47.8% (4777/10000)
[Test]  Epoch: 99	Loss: 0.036227	Acc: 47.5% (4755/10000)
[Test]  Epoch: 100	Loss: 0.036342	Acc: 47.7% (4768/10000)
===========finish==========
['2024-08-19', '16:52:30.712449', '100', 'test', '0.036341688680648805', '47.68', '47.77']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn1.weight', 'layer2.3.bn1.weight', 'layer1.1.bn3.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn3.weight', 'layer3.4.bn1.weight', 'layer3.5.bn3.weight', 'layer3.4.bn2.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer3.2.bn1.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn1.weight', 'layer3.1.bn3.weight', 'layer3.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer1.2.conv3.weight', 'layer3.0.bn2.weight', 'layer1.1.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.5.conv3.weight', 'layer3.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer3.3.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer3.4.conv3.weight', 'layer3.5.conv2.weight', 'layer3.3.conv1.weight', 'layer3.5.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'layer1.2.conv2.weight', 'bn1.weight', 'layer1.0.conv2.weight', 'layer3.4.conv1.weight', 'layer3.3.conv2.weight', 'layer2.3.conv1.weight', 'layer3.2.conv3.weight', 'layer3.2.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.conv1.weight', 'layer2.3.conv3.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer2.1.conv2.weight', 'layer3.4.conv2.weight', 'layer3.1.conv1.weight', 'layer3.1.conv3.weight', 'layer2.2.conv2.weight', 'layer3.2.conv2.weight', 'last_linear.weight', 'layer2.0.conv3.weight', 'layer2.0.conv1.weight', 'layer2.3.conv2.weight', 'layer3.1.conv2.weight', 'layer2.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.065751	Acc: 9.6% (957/10000)
[Test]  Epoch: 2	Loss: 0.056543	Acc: 19.7% (1973/10000)
[Test]  Epoch: 3	Loss: 0.051293	Acc: 24.4% (2445/10000)
[Test]  Epoch: 4	Loss: 0.048724	Acc: 27.2% (2719/10000)
[Test]  Epoch: 5	Loss: 0.046880	Acc: 29.1% (2907/10000)
[Test]  Epoch: 6	Loss: 0.045963	Acc: 29.8% (2977/10000)
[Test]  Epoch: 7	Loss: 0.045564	Acc: 29.7% (2972/10000)
[Test]  Epoch: 8	Loss: 0.045138	Acc: 30.3% (3030/10000)
[Test]  Epoch: 9	Loss: 0.045029	Acc: 30.5% (3050/10000)
[Test]  Epoch: 10	Loss: 0.044802	Acc: 30.9% (3086/10000)
[Test]  Epoch: 11	Loss: 0.044799	Acc: 30.8% (3078/10000)
[Test]  Epoch: 12	Loss: 0.044823	Acc: 30.8% (3076/10000)
[Test]  Epoch: 13	Loss: 0.044652	Acc: 31.1% (3107/10000)
[Test]  Epoch: 14	Loss: 0.044647	Acc: 30.9% (3086/10000)
[Test]  Epoch: 15	Loss: 0.044613	Acc: 30.9% (3091/10000)
[Test]  Epoch: 16	Loss: 0.044614	Acc: 30.9% (3093/10000)
[Test]  Epoch: 17	Loss: 0.044534	Acc: 31.0% (3098/10000)
[Test]  Epoch: 18	Loss: 0.044734	Acc: 30.9% (3088/10000)
[Test]  Epoch: 19	Loss: 0.044636	Acc: 31.1% (3113/10000)
[Test]  Epoch: 20	Loss: 0.044663	Acc: 30.8% (3081/10000)
[Test]  Epoch: 21	Loss: 0.044608	Acc: 31.1% (3108/10000)
[Test]  Epoch: 22	Loss: 0.044611	Acc: 31.1% (3115/10000)
[Test]  Epoch: 23	Loss: 0.044505	Acc: 31.1% (3115/10000)
[Test]  Epoch: 24	Loss: 0.044531	Acc: 31.3% (3129/10000)
[Test]  Epoch: 25	Loss: 0.044496	Acc: 31.3% (3127/10000)
[Test]  Epoch: 26	Loss: 0.044425	Acc: 31.2% (3117/10000)
[Test]  Epoch: 27	Loss: 0.044506	Acc: 31.2% (3118/10000)
[Test]  Epoch: 28	Loss: 0.044494	Acc: 31.1% (3112/10000)
[Test]  Epoch: 29	Loss: 0.044382	Acc: 31.4% (3144/10000)
[Test]  Epoch: 30	Loss: 0.044375	Acc: 31.3% (3130/10000)
[Test]  Epoch: 31	Loss: 0.044425	Acc: 31.2% (3122/10000)
[Test]  Epoch: 32	Loss: 0.044335	Acc: 31.4% (3143/10000)
[Test]  Epoch: 33	Loss: 0.044333	Acc: 31.2% (3120/10000)
[Test]  Epoch: 34	Loss: 0.044447	Acc: 31.3% (3130/10000)
[Test]  Epoch: 35	Loss: 0.044366	Acc: 31.4% (3141/10000)
[Test]  Epoch: 36	Loss: 0.044221	Acc: 31.9% (3190/10000)
[Test]  Epoch: 37	Loss: 0.044229	Acc: 31.7% (3169/10000)
[Test]  Epoch: 38	Loss: 0.044291	Acc: 31.4% (3140/10000)
[Test]  Epoch: 39	Loss: 0.044255	Acc: 31.9% (3188/10000)
[Test]  Epoch: 40	Loss: 0.044251	Acc: 31.5% (3152/10000)
[Test]  Epoch: 41	Loss: 0.044204	Acc: 31.7% (3167/10000)
[Test]  Epoch: 42	Loss: 0.044070	Acc: 31.7% (3173/10000)
[Test]  Epoch: 43	Loss: 0.044166	Acc: 31.7% (3167/10000)
[Test]  Epoch: 44	Loss: 0.044216	Acc: 31.7% (3171/10000)
[Test]  Epoch: 45	Loss: 0.044109	Acc: 31.8% (3184/10000)
[Test]  Epoch: 46	Loss: 0.044241	Acc: 31.5% (3146/10000)
[Test]  Epoch: 47	Loss: 0.044276	Acc: 31.5% (3153/10000)
[Test]  Epoch: 48	Loss: 0.044289	Acc: 31.5% (3151/10000)
[Test]  Epoch: 49	Loss: 0.044339	Acc: 31.4% (3137/10000)
[Test]  Epoch: 50	Loss: 0.044216	Acc: 31.5% (3153/10000)
[Test]  Epoch: 51	Loss: 0.044240	Acc: 31.6% (3165/10000)
[Test]  Epoch: 52	Loss: 0.044178	Acc: 31.8% (3176/10000)
[Test]  Epoch: 53	Loss: 0.044342	Acc: 31.5% (3148/10000)
[Test]  Epoch: 54	Loss: 0.044360	Acc: 31.3% (3130/10000)
[Test]  Epoch: 55	Loss: 0.044241	Acc: 31.6% (3159/10000)
[Test]  Epoch: 56	Loss: 0.044211	Acc: 31.6% (3156/10000)
[Test]  Epoch: 57	Loss: 0.044213	Acc: 31.6% (3159/10000)
[Test]  Epoch: 58	Loss: 0.044248	Acc: 31.7% (3171/10000)
[Test]  Epoch: 59	Loss: 0.044196	Acc: 31.8% (3179/10000)
[Test]  Epoch: 60	Loss: 0.044245	Acc: 31.8% (3177/10000)
[Test]  Epoch: 61	Loss: 0.044234	Acc: 31.9% (3187/10000)
[Test]  Epoch: 62	Loss: 0.044162	Acc: 31.9% (3193/10000)
[Test]  Epoch: 63	Loss: 0.044104	Acc: 32.0% (3200/10000)
[Test]  Epoch: 64	Loss: 0.044065	Acc: 32.0% (3198/10000)
[Test]  Epoch: 65	Loss: 0.044187	Acc: 31.9% (3188/10000)
[Test]  Epoch: 66	Loss: 0.044145	Acc: 31.8% (3178/10000)
[Test]  Epoch: 67	Loss: 0.044213	Acc: 31.7% (3167/10000)
[Test]  Epoch: 68	Loss: 0.044202	Acc: 31.6% (3165/10000)
[Test]  Epoch: 69	Loss: 0.044216	Acc: 31.7% (3166/10000)
[Test]  Epoch: 70	Loss: 0.044105	Acc: 31.8% (3183/10000)
[Test]  Epoch: 71	Loss: 0.044087	Acc: 31.7% (3172/10000)
[Test]  Epoch: 72	Loss: 0.044129	Acc: 31.6% (3161/10000)
[Test]  Epoch: 73	Loss: 0.044073	Acc: 31.8% (3179/10000)
[Test]  Epoch: 74	Loss: 0.044111	Acc: 31.6% (3162/10000)
[Test]  Epoch: 75	Loss: 0.044121	Acc: 31.6% (3163/10000)
[Test]  Epoch: 76	Loss: 0.044129	Acc: 31.9% (3187/10000)
[Test]  Epoch: 77	Loss: 0.044130	Acc: 31.9% (3191/10000)
[Test]  Epoch: 78	Loss: 0.044098	Acc: 31.6% (3165/10000)
[Test]  Epoch: 79	Loss: 0.044105	Acc: 31.6% (3163/10000)
[Test]  Epoch: 80	Loss: 0.044103	Acc: 31.7% (3169/10000)
[Test]  Epoch: 81	Loss: 0.044237	Acc: 31.5% (3153/10000)
[Test]  Epoch: 82	Loss: 0.044285	Acc: 31.4% (3145/10000)
[Test]  Epoch: 83	Loss: 0.044167	Acc: 31.5% (3147/10000)
[Test]  Epoch: 84	Loss: 0.044135	Acc: 31.8% (3175/10000)
[Test]  Epoch: 85	Loss: 0.044039	Acc: 31.9% (3188/10000)
[Test]  Epoch: 86	Loss: 0.044085	Acc: 31.8% (3180/10000)
[Test]  Epoch: 87	Loss: 0.043940	Acc: 32.0% (3196/10000)
[Test]  Epoch: 88	Loss: 0.044011	Acc: 31.9% (3194/10000)
[Test]  Epoch: 89	Loss: 0.044141	Acc: 31.7% (3171/10000)
[Test]  Epoch: 90	Loss: 0.044221	Acc: 31.7% (3170/10000)
[Test]  Epoch: 91	Loss: 0.044071	Acc: 31.8% (3175/10000)
[Test]  Epoch: 92	Loss: 0.044090	Acc: 31.6% (3158/10000)
[Test]  Epoch: 93	Loss: 0.044019	Acc: 31.9% (3189/10000)
[Test]  Epoch: 94	Loss: 0.044025	Acc: 31.7% (3167/10000)
[Test]  Epoch: 95	Loss: 0.044046	Acc: 32.0% (3196/10000)
[Test]  Epoch: 96	Loss: 0.044100	Acc: 31.8% (3175/10000)
[Test]  Epoch: 97	Loss: 0.044183	Acc: 31.7% (3166/10000)
[Test]  Epoch: 98	Loss: 0.044011	Acc: 31.6% (3164/10000)
[Test]  Epoch: 99	Loss: 0.044079	Acc: 31.8% (3178/10000)
[Test]  Epoch: 100	Loss: 0.044136	Acc: 31.5% (3147/10000)
===========finish==========
['2024-08-19', '16:56:56.368902', '100', 'test', '0.04413598644733429', '31.47', '32.0']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.conv3.weight', 'layer4.2.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer2.1.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn1.weight', 'layer2.3.bn1.weight', 'layer1.1.bn3.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn3.weight', 'layer3.4.bn1.weight', 'layer3.5.bn3.weight', 'layer3.4.bn2.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer3.2.bn1.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn1.weight', 'layer3.1.bn3.weight', 'layer3.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer1.2.conv3.weight', 'layer3.0.bn2.weight', 'layer1.1.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.5.conv3.weight', 'layer3.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer3.3.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer3.4.conv3.weight', 'layer3.5.conv2.weight', 'layer3.3.conv1.weight', 'layer3.5.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'layer1.2.conv2.weight', 'bn1.weight', 'layer1.0.conv2.weight', 'layer3.4.conv1.weight', 'layer3.3.conv2.weight', 'layer2.3.conv1.weight', 'layer3.2.conv3.weight', 'layer3.2.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.conv1.weight', 'layer2.3.conv3.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer2.1.conv2.weight', 'layer3.4.conv2.weight', 'layer3.1.conv1.weight', 'layer3.1.conv3.weight', 'layer2.2.conv2.weight', 'layer3.2.conv2.weight', 'last_linear.weight', 'layer2.0.conv3.weight', 'layer2.0.conv1.weight', 'layer2.3.conv2.weight', 'layer3.1.conv2.weight', 'layer2.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer3.0.conv1.weight', 'layer4.0.downsample.0.weight', 'layer1.0.downsample.0.weight', 'layer3.0.conv3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.conv2.weight', 'conv1.weight', 'layer4.0.conv3.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.072521	Acc: 4.8% (480/10000)
[Test]  Epoch: 2	Loss: 0.064816	Acc: 11.6% (1160/10000)
[Test]  Epoch: 3	Loss: 0.061466	Acc: 13.8% (1381/10000)
[Test]  Epoch: 4	Loss: 0.059330	Acc: 16.1% (1608/10000)
[Test]  Epoch: 5	Loss: 0.057627	Acc: 17.4% (1735/10000)
[Test]  Epoch: 6	Loss: 0.056558	Acc: 18.4% (1836/10000)
[Test]  Epoch: 7	Loss: 0.056307	Acc: 18.6% (1855/10000)
[Test]  Epoch: 8	Loss: 0.055880	Acc: 18.9% (1893/10000)
[Test]  Epoch: 9	Loss: 0.056012	Acc: 18.7% (1874/10000)
[Test]  Epoch: 10	Loss: 0.055750	Acc: 19.2% (1920/10000)
[Test]  Epoch: 11	Loss: 0.055809	Acc: 19.2% (1922/10000)
[Test]  Epoch: 12	Loss: 0.055698	Acc: 19.4% (1940/10000)
[Test]  Epoch: 13	Loss: 0.055582	Acc: 19.6% (1956/10000)
[Test]  Epoch: 14	Loss: 0.055662	Acc: 19.7% (1971/10000)
[Test]  Epoch: 15	Loss: 0.055529	Acc: 19.6% (1965/10000)
[Test]  Epoch: 16	Loss: 0.055688	Acc: 19.6% (1965/10000)
[Test]  Epoch: 17	Loss: 0.055569	Acc: 19.6% (1958/10000)
[Test]  Epoch: 18	Loss: 0.055648	Acc: 19.8% (1979/10000)
[Test]  Epoch: 19	Loss: 0.055616	Acc: 20.1% (2006/10000)
[Test]  Epoch: 20	Loss: 0.055617	Acc: 20.0% (2000/10000)
[Test]  Epoch: 21	Loss: 0.055375	Acc: 20.2% (2018/10000)
[Test]  Epoch: 22	Loss: 0.055521	Acc: 20.2% (2025/10000)
[Test]  Epoch: 23	Loss: 0.055479	Acc: 20.1% (2009/10000)
[Test]  Epoch: 24	Loss: 0.055483	Acc: 20.0% (1997/10000)
[Test]  Epoch: 25	Loss: 0.055424	Acc: 20.4% (2038/10000)
[Test]  Epoch: 26	Loss: 0.055364	Acc: 20.2% (2018/10000)
[Test]  Epoch: 27	Loss: 0.055350	Acc: 20.7% (2066/10000)
[Test]  Epoch: 28	Loss: 0.055343	Acc: 20.5% (2048/10000)
[Test]  Epoch: 29	Loss: 0.055411	Acc: 20.4% (2036/10000)
[Test]  Epoch: 30	Loss: 0.055266	Acc: 20.4% (2039/10000)
[Test]  Epoch: 31	Loss: 0.055388	Acc: 20.6% (2063/10000)
[Test]  Epoch: 32	Loss: 0.055239	Acc: 20.6% (2055/10000)
[Test]  Epoch: 33	Loss: 0.055368	Acc: 20.7% (2073/10000)
[Test]  Epoch: 34	Loss: 0.055446	Acc: 20.3% (2034/10000)
[Test]  Epoch: 35	Loss: 0.055279	Acc: 20.6% (2060/10000)
[Test]  Epoch: 36	Loss: 0.055217	Acc: 20.6% (2056/10000)
[Test]  Epoch: 37	Loss: 0.055256	Acc: 20.4% (2041/10000)
[Test]  Epoch: 38	Loss: 0.055238	Acc: 20.8% (2078/10000)
[Test]  Epoch: 39	Loss: 0.055240	Acc: 20.7% (2069/10000)
[Test]  Epoch: 40	Loss: 0.055166	Acc: 20.7% (2071/10000)
[Test]  Epoch: 41	Loss: 0.055218	Acc: 20.8% (2080/10000)
[Test]  Epoch: 42	Loss: 0.055112	Acc: 20.9% (2095/10000)
[Test]  Epoch: 43	Loss: 0.055123	Acc: 20.7% (2072/10000)
[Test]  Epoch: 44	Loss: 0.055090	Acc: 20.9% (2091/10000)
[Test]  Epoch: 45	Loss: 0.055129	Acc: 20.9% (2094/10000)
[Test]  Epoch: 46	Loss: 0.055239	Acc: 20.8% (2077/10000)
[Test]  Epoch: 47	Loss: 0.055242	Acc: 20.7% (2073/10000)
[Test]  Epoch: 48	Loss: 0.055204	Acc: 20.7% (2074/10000)
[Test]  Epoch: 49	Loss: 0.055220	Acc: 20.7% (2073/10000)
[Test]  Epoch: 50	Loss: 0.055129	Acc: 20.8% (2083/10000)
[Test]  Epoch: 51	Loss: 0.055186	Acc: 20.7% (2071/10000)
[Test]  Epoch: 52	Loss: 0.055118	Acc: 20.8% (2078/10000)
[Test]  Epoch: 53	Loss: 0.055140	Acc: 21.0% (2098/10000)
[Test]  Epoch: 54	Loss: 0.055179	Acc: 21.0% (2097/10000)
[Test]  Epoch: 55	Loss: 0.055164	Acc: 20.8% (2080/10000)
[Test]  Epoch: 56	Loss: 0.055112	Acc: 21.1% (2105/10000)
[Test]  Epoch: 57	Loss: 0.055103	Acc: 21.0% (2099/10000)
[Test]  Epoch: 58	Loss: 0.055125	Acc: 20.9% (2094/10000)
[Test]  Epoch: 59	Loss: 0.055048	Acc: 21.1% (2105/10000)
[Test]  Epoch: 60	Loss: 0.055077	Acc: 20.9% (2094/10000)
[Test]  Epoch: 61	Loss: 0.055093	Acc: 21.0% (2096/10000)
[Test]  Epoch: 62	Loss: 0.055058	Acc: 21.1% (2105/10000)
[Test]  Epoch: 63	Loss: 0.055092	Acc: 20.9% (2088/10000)
[Test]  Epoch: 64	Loss: 0.055028	Acc: 21.0% (2099/10000)
[Test]  Epoch: 65	Loss: 0.055083	Acc: 21.0% (2098/10000)
[Test]  Epoch: 66	Loss: 0.055051	Acc: 20.9% (2091/10000)
[Test]  Epoch: 67	Loss: 0.055067	Acc: 20.8% (2079/10000)
[Test]  Epoch: 68	Loss: 0.055051	Acc: 20.8% (2079/10000)
[Test]  Epoch: 69	Loss: 0.055088	Acc: 20.8% (2084/10000)
[Test]  Epoch: 70	Loss: 0.055073	Acc: 20.8% (2082/10000)
[Test]  Epoch: 71	Loss: 0.055054	Acc: 20.8% (2076/10000)
[Test]  Epoch: 72	Loss: 0.055102	Acc: 20.7% (2072/10000)
[Test]  Epoch: 73	Loss: 0.055034	Acc: 20.9% (2095/10000)
[Test]  Epoch: 74	Loss: 0.055026	Acc: 20.8% (2083/10000)
[Test]  Epoch: 75	Loss: 0.055072	Acc: 21.0% (2104/10000)
[Test]  Epoch: 76	Loss: 0.055080	Acc: 21.1% (2109/10000)
[Test]  Epoch: 77	Loss: 0.055049	Acc: 20.7% (2073/10000)
[Test]  Epoch: 78	Loss: 0.054967	Acc: 20.8% (2081/10000)
[Test]  Epoch: 79	Loss: 0.055013	Acc: 20.9% (2089/10000)
[Test]  Epoch: 80	Loss: 0.055122	Acc: 20.8% (2076/10000)
[Test]  Epoch: 81	Loss: 0.055065	Acc: 20.5% (2053/10000)
[Test]  Epoch: 82	Loss: 0.055098	Acc: 20.7% (2066/10000)
[Test]  Epoch: 83	Loss: 0.055050	Acc: 20.9% (2086/10000)
[Test]  Epoch: 84	Loss: 0.054926	Acc: 20.9% (2093/10000)
[Test]  Epoch: 85	Loss: 0.054974	Acc: 20.8% (2077/10000)
[Test]  Epoch: 86	Loss: 0.055090	Acc: 20.9% (2086/10000)
[Test]  Epoch: 87	Loss: 0.054983	Acc: 20.7% (2069/10000)
[Test]  Epoch: 88	Loss: 0.054996	Acc: 21.0% (2097/10000)
[Test]  Epoch: 89	Loss: 0.055083	Acc: 21.0% (2097/10000)
[Test]  Epoch: 90	Loss: 0.055091	Acc: 20.7% (2074/10000)
[Test]  Epoch: 91	Loss: 0.055029	Acc: 20.8% (2079/10000)
[Test]  Epoch: 92	Loss: 0.055017	Acc: 20.7% (2073/10000)
[Test]  Epoch: 93	Loss: 0.054923	Acc: 21.0% (2104/10000)
[Test]  Epoch: 94	Loss: 0.055007	Acc: 20.9% (2094/10000)
[Test]  Epoch: 95	Loss: 0.054917	Acc: 20.8% (2084/10000)
[Test]  Epoch: 96	Loss: 0.055003	Acc: 20.9% (2095/10000)
[Test]  Epoch: 97	Loss: 0.055019	Acc: 20.8% (2078/10000)
[Test]  Epoch: 98	Loss: 0.054986	Acc: 21.0% (2097/10000)
[Test]  Epoch: 99	Loss: 0.055023	Acc: 20.9% (2085/10000)
[Test]  Epoch: 100	Loss: 0.055113	Acc: 20.8% (2075/10000)
===========finish==========
['2024-08-19', '17:01:18.773381', '100', 'test', '0.05511326236724853', '20.75', '21.09']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('conv1.weight', nan), ('bn1.weight', nan), ('bn1.bias', 0.0), ('layer1.0.conv1.weight', nan), ('layer1.0.bn1.weight', nan), ('layer1.0.bn1.bias', 0.0), ('layer1.0.conv2.weight', nan), ('layer1.0.bn2.weight', nan), ('layer1.0.bn2.bias', 0.0), ('layer1.0.conv3.weight', nan), ('layer1.0.bn3.weight', nan), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.0.weight', nan), ('layer1.0.downsample.1.weight', nan), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.conv1.weight', nan), ('layer1.1.bn1.weight', nan), ('layer1.1.bn1.bias', 0.0), ('layer1.1.conv2.weight', nan), ('layer1.1.bn2.weight', nan), ('layer1.1.bn2.bias', 0.0), ('layer1.1.conv3.weight', nan), ('layer1.1.bn3.weight', nan), ('layer1.1.bn3.bias', 0.0), ('layer1.2.conv1.weight', nan), ('layer1.2.bn1.weight', nan), ('layer1.2.bn1.bias', 0.0), ('layer1.2.conv2.weight', nan), ('layer1.2.bn2.weight', nan), ('layer1.2.bn2.bias', 0.0), ('layer1.2.conv3.weight', nan), ('layer1.2.bn3.weight', nan), ('layer1.2.bn3.bias', 0.0), ('layer2.0.conv1.weight', nan), ('layer2.0.bn1.weight', nan), ('layer2.0.bn1.bias', 0.0), ('layer2.0.conv2.weight', nan), ('layer2.0.bn2.weight', nan), ('layer2.0.bn2.bias', 0.0), ('layer2.0.conv3.weight', nan), ('layer2.0.bn3.weight', nan), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.0.weight', nan), ('layer2.0.downsample.1.weight', nan), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.conv1.weight', nan), ('layer2.1.bn1.weight', nan), ('layer2.1.bn1.bias', 0.0), ('layer2.1.conv2.weight', nan), ('layer2.1.bn2.weight', nan), ('layer2.1.bn2.bias', 0.0), ('layer2.1.conv3.weight', nan), ('layer2.1.bn3.weight', nan), ('layer2.1.bn3.bias', 0.0), ('layer2.2.conv1.weight', nan), ('layer2.2.bn1.weight', nan), ('layer2.2.bn1.bias', 0.0), ('layer2.2.conv2.weight', nan), ('layer2.2.bn2.weight', nan), ('layer2.2.bn2.bias', 0.0), ('layer2.2.conv3.weight', nan), ('layer2.2.bn3.weight', nan), ('layer2.2.bn3.bias', 0.0), ('layer2.3.conv1.weight', nan), ('layer2.3.bn1.weight', nan), ('layer2.3.bn1.bias', 0.0), ('layer2.3.conv2.weight', nan), ('layer2.3.bn2.weight', nan), ('layer2.3.bn2.bias', 0.0), ('layer2.3.conv3.weight', nan), ('layer2.3.bn3.weight', nan), ('layer2.3.bn3.bias', 0.0), ('layer3.0.conv1.weight', nan), ('layer3.0.bn1.weight', nan), ('layer3.0.bn1.bias', 0.0), ('layer3.0.conv2.weight', nan), ('layer3.0.bn2.weight', nan), ('layer3.0.bn2.bias', 0.0), ('layer3.0.conv3.weight', nan), ('layer3.0.bn3.weight', nan), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.0.weight', nan), ('layer3.0.downsample.1.weight', nan), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.conv1.weight', nan), ('layer3.1.bn1.weight', nan), ('layer3.1.bn1.bias', 0.0), ('layer3.1.conv2.weight', nan), ('layer3.1.bn2.weight', nan), ('layer3.1.bn2.bias', 0.0), ('layer3.1.conv3.weight', nan), ('layer3.1.bn3.weight', nan), ('layer3.1.bn3.bias', 0.0), ('layer3.2.conv1.weight', nan), ('layer3.2.bn1.weight', nan), ('layer3.2.bn1.bias', 0.0), ('layer3.2.conv2.weight', nan), ('layer3.2.bn2.weight', nan), ('layer3.2.bn2.bias', 0.0), ('layer3.2.conv3.weight', nan), ('layer3.2.bn3.weight', nan), ('layer3.2.bn3.bias', 0.0), ('layer3.3.conv1.weight', nan), ('layer3.3.bn1.weight', nan), ('layer3.3.bn1.bias', 0.0), ('layer3.3.conv2.weight', nan), ('layer3.3.bn2.weight', nan), ('layer3.3.bn2.bias', 0.0), ('layer3.3.conv3.weight', nan), ('layer3.3.bn3.weight', nan), ('layer3.3.bn3.bias', 0.0), ('layer3.4.conv1.weight', nan), ('layer3.4.bn1.weight', nan), ('layer3.4.bn1.bias', 0.0), ('layer3.4.conv2.weight', nan), ('layer3.4.bn2.weight', nan), ('layer3.4.bn2.bias', 0.0), ('layer3.4.conv3.weight', nan), ('layer3.4.bn3.weight', nan), ('layer3.4.bn3.bias', 0.0), ('layer3.5.conv1.weight', nan), ('layer3.5.bn1.weight', nan), ('layer3.5.bn1.bias', 0.0), ('layer3.5.conv2.weight', nan), ('layer3.5.bn2.weight', nan), ('layer3.5.bn2.bias', 0.0), ('layer3.5.conv3.weight', nan), ('layer3.5.bn3.weight', nan), ('layer3.5.bn3.bias', 0.0), ('layer4.0.conv1.weight', nan), ('layer4.0.bn1.weight', nan), ('layer4.0.bn1.bias', 0.0), ('layer4.0.conv2.weight', nan), ('layer4.0.bn2.weight', nan), ('layer4.0.bn2.bias', 0.0), ('layer4.0.conv3.weight', nan), ('layer4.0.bn3.weight', nan), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.0.weight', nan), ('layer4.0.downsample.1.weight', nan), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.conv1.weight', nan), ('layer4.1.bn1.weight', nan), ('layer4.1.bn1.bias', 0.0), ('layer4.1.conv2.weight', nan), ('layer4.1.bn2.weight', nan), ('layer4.1.bn2.bias', 0.0), ('layer4.1.conv3.weight', nan), ('layer4.1.bn3.weight', nan), ('layer4.1.bn3.bias', 0.0), ('layer4.2.conv1.weight', nan), ('layer4.2.bn1.weight', nan), ('layer4.2.bn1.bias', 0.0), ('layer4.2.conv2.weight', nan), ('layer4.2.bn2.weight', nan), ('layer4.2.bn2.bias', 0.0), ('layer4.2.conv3.weight', nan), ('layer4.2.bn3.weight', nan), ('layer4.2.bn3.bias', 0.0), ('last_linear.weight', nan), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('conv1.weight', nan), ('layer1.0.conv1.weight', nan), ('layer1.0.conv2.weight', nan), ('layer1.0.conv3.weight', nan), ('layer1.1.conv1.weight', nan), ('layer1.1.conv2.weight', nan), ('layer1.1.conv3.weight', nan), ('layer1.2.conv1.weight', nan), ('layer1.2.conv2.weight', nan), ('layer1.2.conv3.weight', nan), ('layer2.0.conv1.weight', nan), ('layer2.0.conv2.weight', nan), ('layer2.0.conv3.weight', nan), ('layer2.1.conv1.weight', nan), ('layer2.1.conv2.weight', nan), ('layer2.1.conv3.weight', nan), ('layer2.2.conv1.weight', nan), ('layer2.2.conv2.weight', nan), ('layer2.2.conv3.weight', nan), ('layer2.3.conv1.weight', nan), ('layer2.3.conv2.weight', nan), ('layer2.3.conv3.weight', nan), ('layer3.0.conv1.weight', nan), ('layer3.0.conv2.weight', nan), ('layer3.0.conv3.weight', nan), ('layer3.1.conv1.weight', nan), ('layer3.1.conv2.weight', nan), ('layer3.1.conv3.weight', nan), ('layer3.2.conv1.weight', nan), ('layer3.2.conv2.weight', nan), ('layer3.2.conv3.weight', nan), ('layer3.3.conv1.weight', nan), ('layer3.3.conv2.weight', nan), ('layer3.3.conv3.weight', nan), ('layer3.4.conv1.weight', nan), ('layer3.4.conv2.weight', nan), ('layer3.4.conv3.weight', nan), ('layer3.5.conv1.weight', nan), ('layer3.5.conv2.weight', nan), ('layer3.5.conv3.weight', nan), ('layer4.0.conv1.weight', nan), ('layer4.0.conv2.weight', nan), ('layer4.0.conv3.weight', nan), ('layer4.1.conv1.weight', nan), ('layer4.1.conv2.weight', nan), ('layer4.1.conv3.weight', nan), ('layer4.2.conv1.weight', nan), ('layer4.2.conv2.weight', nan), ('layer4.2.conv3.weight', nan), ('last_linear.weight', nan), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007165	Acc: 85.8% (8583/10000)
[Test]  Epoch: 2	Loss: 0.006689	Acc: 86.6% (8657/10000)
[Test]  Epoch: 3	Loss: 0.006653	Acc: 86.5% (8652/10000)
[Test]  Epoch: 4	Loss: 0.006608	Acc: 86.6% (8664/10000)
[Test]  Epoch: 5	Loss: 0.006630	Acc: 86.7% (8667/10000)
[Test]  Epoch: 6	Loss: 0.006570	Acc: 86.7% (8672/10000)
[Test]  Epoch: 7	Loss: 0.006477	Acc: 86.7% (8673/10000)
[Test]  Epoch: 8	Loss: 0.006559	Acc: 86.5% (8649/10000)
[Test]  Epoch: 9	Loss: 0.006459	Acc: 86.7% (8671/10000)
[Test]  Epoch: 10	Loss: 0.006422	Acc: 86.8% (8682/10000)
[Test]  Epoch: 11	Loss: 0.006426	Acc: 86.6% (8664/10000)
[Test]  Epoch: 12	Loss: 0.006388	Acc: 86.8% (8681/10000)
[Test]  Epoch: 13	Loss: 0.006380	Acc: 86.7% (8674/10000)
[Test]  Epoch: 14	Loss: 0.006323	Acc: 87.0% (8705/10000)
[Test]  Epoch: 15	Loss: 0.006385	Acc: 86.9% (8687/10000)
[Test]  Epoch: 16	Loss: 0.006340	Acc: 86.9% (8686/10000)
[Test]  Epoch: 17	Loss: 0.006372	Acc: 86.9% (8692/10000)
[Test]  Epoch: 18	Loss: 0.006304	Acc: 86.8% (8678/10000)
[Test]  Epoch: 19	Loss: 0.006275	Acc: 86.9% (8687/10000)
[Test]  Epoch: 20	Loss: 0.006276	Acc: 87.0% (8697/10000)
[Test]  Epoch: 21	Loss: 0.006308	Acc: 86.9% (8694/10000)
[Test]  Epoch: 22	Loss: 0.006273	Acc: 87.1% (8707/10000)
[Test]  Epoch: 23	Loss: 0.006280	Acc: 87.0% (8697/10000)
[Test]  Epoch: 24	Loss: 0.006247	Acc: 86.9% (8694/10000)
[Test]  Epoch: 25	Loss: 0.006287	Acc: 87.0% (8698/10000)
[Test]  Epoch: 26	Loss: 0.006270	Acc: 86.8% (8682/10000)
[Test]  Epoch: 27	Loss: 0.006244	Acc: 87.0% (8699/10000)
[Test]  Epoch: 28	Loss: 0.006232	Acc: 87.1% (8708/10000)
[Test]  Epoch: 29	Loss: 0.006249	Acc: 87.0% (8703/10000)
[Test]  Epoch: 30	Loss: 0.006229	Acc: 87.0% (8697/10000)
[Test]  Epoch: 31	Loss: 0.006218	Acc: 87.0% (8702/10000)
[Test]  Epoch: 32	Loss: 0.006261	Acc: 87.0% (8705/10000)
[Test]  Epoch: 33	Loss: 0.006241	Acc: 87.0% (8701/10000)
[Test]  Epoch: 34	Loss: 0.006225	Acc: 87.0% (8702/10000)
[Test]  Epoch: 35	Loss: 0.006237	Acc: 87.1% (8707/10000)
[Test]  Epoch: 36	Loss: 0.006253	Acc: 86.9% (8690/10000)
[Test]  Epoch: 37	Loss: 0.006235	Acc: 87.1% (8706/10000)
[Test]  Epoch: 38	Loss: 0.006233	Acc: 87.0% (8698/10000)
[Test]  Epoch: 39	Loss: 0.006213	Acc: 87.1% (8708/10000)
[Test]  Epoch: 40	Loss: 0.006227	Acc: 87.0% (8703/10000)
[Test]  Epoch: 41	Loss: 0.006218	Acc: 87.0% (8703/10000)
[Test]  Epoch: 42	Loss: 0.006188	Acc: 87.1% (8708/10000)
[Test]  Epoch: 43	Loss: 0.006220	Acc: 87.0% (8703/10000)
[Test]  Epoch: 44	Loss: 0.006200	Acc: 87.0% (8703/10000)
[Test]  Epoch: 45	Loss: 0.006190	Acc: 87.1% (8709/10000)
[Test]  Epoch: 46	Loss: 0.006189	Acc: 87.0% (8695/10000)
[Test]  Epoch: 47	Loss: 0.006189	Acc: 87.0% (8701/10000)
[Test]  Epoch: 48	Loss: 0.006181	Acc: 87.1% (8713/10000)
[Test]  Epoch: 49	Loss: 0.006184	Acc: 87.0% (8697/10000)
[Test]  Epoch: 50	Loss: 0.006188	Acc: 87.0% (8697/10000)
[Test]  Epoch: 51	Loss: 0.006152	Acc: 87.0% (8702/10000)
[Test]  Epoch: 52	Loss: 0.006163	Acc: 87.1% (8712/10000)
[Test]  Epoch: 53	Loss: 0.006171	Acc: 87.0% (8704/10000)
[Test]  Epoch: 54	Loss: 0.006170	Acc: 87.1% (8713/10000)
[Test]  Epoch: 55	Loss: 0.006187	Acc: 87.0% (8699/10000)
[Test]  Epoch: 56	Loss: 0.006153	Acc: 87.1% (8708/10000)
[Test]  Epoch: 57	Loss: 0.006164	Acc: 87.0% (8701/10000)
[Test]  Epoch: 58	Loss: 0.006176	Acc: 87.0% (8698/10000)
[Test]  Epoch: 59	Loss: 0.006186	Acc: 87.0% (8701/10000)
[Test]  Epoch: 60	Loss: 0.006184	Acc: 87.1% (8714/10000)
[Test]  Epoch: 61	Loss: 0.006170	Acc: 87.1% (8713/10000)
[Test]  Epoch: 62	Loss: 0.006188	Acc: 87.1% (8713/10000)
[Test]  Epoch: 63	Loss: 0.006197	Acc: 87.1% (8706/10000)
[Test]  Epoch: 64	Loss: 0.006160	Acc: 87.2% (8717/10000)
[Test]  Epoch: 65	Loss: 0.006161	Acc: 87.1% (8714/10000)
[Test]  Epoch: 66	Loss: 0.006163	Acc: 87.0% (8702/10000)
[Test]  Epoch: 67	Loss: 0.006160	Acc: 87.0% (8703/10000)
[Test]  Epoch: 68	Loss: 0.006169	Acc: 87.1% (8710/10000)
[Test]  Epoch: 69	Loss: 0.006192	Acc: 87.0% (8698/10000)
[Test]  Epoch: 70	Loss: 0.006169	Acc: 87.1% (8708/10000)
[Test]  Epoch: 71	Loss: 0.006144	Acc: 87.1% (8713/10000)
[Test]  Epoch: 72	Loss: 0.006140	Acc: 87.1% (8712/10000)
[Test]  Epoch: 73	Loss: 0.006162	Acc: 87.1% (8707/10000)
[Test]  Epoch: 74	Loss: 0.006150	Acc: 87.0% (8700/10000)
[Test]  Epoch: 75	Loss: 0.006161	Acc: 87.0% (8703/10000)
[Test]  Epoch: 76	Loss: 0.006142	Acc: 87.2% (8716/10000)
[Test]  Epoch: 77	Loss: 0.006170	Acc: 87.0% (8703/10000)
[Test]  Epoch: 78	Loss: 0.006151	Acc: 87.1% (8713/10000)
[Test]  Epoch: 79	Loss: 0.006156	Acc: 87.0% (8705/10000)
[Test]  Epoch: 80	Loss: 0.006158	Acc: 87.0% (8705/10000)
[Test]  Epoch: 81	Loss: 0.006172	Acc: 87.0% (8699/10000)
[Test]  Epoch: 82	Loss: 0.006172	Acc: 87.0% (8700/10000)
[Test]  Epoch: 83	Loss: 0.006138	Acc: 87.0% (8703/10000)
[Test]  Epoch: 84	Loss: 0.006148	Acc: 87.1% (8712/10000)
[Test]  Epoch: 85	Loss: 0.006156	Acc: 87.0% (8705/10000)
[Test]  Epoch: 86	Loss: 0.006138	Acc: 87.1% (8708/10000)
[Test]  Epoch: 87	Loss: 0.006151	Acc: 87.2% (8716/10000)
[Test]  Epoch: 88	Loss: 0.006149	Acc: 87.1% (8710/10000)
[Test]  Epoch: 89	Loss: 0.006156	Acc: 87.1% (8710/10000)
[Test]  Epoch: 90	Loss: 0.006159	Acc: 87.0% (8705/10000)
[Test]  Epoch: 91	Loss: 0.006174	Acc: 87.0% (8698/10000)
[Test]  Epoch: 92	Loss: 0.006149	Acc: 87.0% (8705/10000)
[Test]  Epoch: 93	Loss: 0.006141	Acc: 87.0% (8703/10000)
[Test]  Epoch: 94	Loss: 0.006162	Acc: 87.0% (8700/10000)
[Test]  Epoch: 95	Loss: 0.006141	Acc: 87.1% (8713/10000)
[Test]  Epoch: 96	Loss: 0.006140	Acc: 87.1% (8712/10000)
[Test]  Epoch: 97	Loss: 0.006138	Acc: 87.1% (8708/10000)
[Test]  Epoch: 98	Loss: 0.006165	Acc: 87.1% (8710/10000)
[Test]  Epoch: 99	Loss: 0.006155	Acc: 87.1% (8707/10000)
[Test]  Epoch: 100	Loss: 0.006181	Acc: 87.0% (8699/10000)
===========finish==========
['2024-08-19', '17:07:53.809208', '100', 'test', '0.006181160934269428', '86.99', '87.17']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.024818	Acc: 54.1% (5411/10000)
[Test]  Epoch: 2	Loss: 0.016998	Acc: 66.0% (6597/10000)
[Test]  Epoch: 3	Loss: 0.016092	Acc: 68.0% (6802/10000)
[Test]  Epoch: 4	Loss: 0.015779	Acc: 68.9% (6887/10000)
[Test]  Epoch: 5	Loss: 0.015593	Acc: 69.2% (6917/10000)
[Test]  Epoch: 6	Loss: 0.015480	Acc: 69.1% (6913/10000)
[Test]  Epoch: 7	Loss: 0.015382	Acc: 69.4% (6943/10000)
[Test]  Epoch: 8	Loss: 0.015457	Acc: 69.5% (6955/10000)
[Test]  Epoch: 9	Loss: 0.015459	Acc: 69.5% (6949/10000)
[Test]  Epoch: 10	Loss: 0.015326	Acc: 69.8% (6985/10000)
[Test]  Epoch: 11	Loss: 0.015418	Acc: 69.8% (6978/10000)
[Test]  Epoch: 12	Loss: 0.015327	Acc: 69.9% (6988/10000)
[Test]  Epoch: 13	Loss: 0.015216	Acc: 70.0% (6999/10000)
[Test]  Epoch: 14	Loss: 0.015136	Acc: 70.3% (7028/10000)
[Test]  Epoch: 15	Loss: 0.015057	Acc: 70.2% (7023/10000)
[Test]  Epoch: 16	Loss: 0.015080	Acc: 70.3% (7027/10000)
[Test]  Epoch: 17	Loss: 0.015022	Acc: 70.3% (7034/10000)
[Test]  Epoch: 18	Loss: 0.014928	Acc: 70.4% (7038/10000)
[Test]  Epoch: 19	Loss: 0.014946	Acc: 70.7% (7072/10000)
[Test]  Epoch: 20	Loss: 0.014910	Acc: 70.6% (7062/10000)
[Test]  Epoch: 21	Loss: 0.014959	Acc: 70.6% (7056/10000)
[Test]  Epoch: 22	Loss: 0.014838	Acc: 70.7% (7068/10000)
[Test]  Epoch: 23	Loss: 0.014869	Acc: 70.5% (7050/10000)
[Test]  Epoch: 24	Loss: 0.014844	Acc: 70.7% (7071/10000)
[Test]  Epoch: 25	Loss: 0.014873	Acc: 70.6% (7062/10000)
[Test]  Epoch: 26	Loss: 0.014851	Acc: 70.8% (7081/10000)
[Test]  Epoch: 27	Loss: 0.014742	Acc: 70.9% (7089/10000)
[Test]  Epoch: 28	Loss: 0.014742	Acc: 70.8% (7081/10000)
[Test]  Epoch: 29	Loss: 0.014729	Acc: 70.7% (7066/10000)
[Test]  Epoch: 30	Loss: 0.014712	Acc: 70.7% (7072/10000)
[Test]  Epoch: 31	Loss: 0.014597	Acc: 71.1% (7112/10000)
[Test]  Epoch: 32	Loss: 0.014814	Acc: 70.6% (7060/10000)
[Test]  Epoch: 33	Loss: 0.014659	Acc: 71.0% (7101/10000)
[Test]  Epoch: 34	Loss: 0.014571	Acc: 71.0% (7095/10000)
[Test]  Epoch: 35	Loss: 0.014535	Acc: 71.0% (7099/10000)
[Test]  Epoch: 36	Loss: 0.014621	Acc: 70.9% (7089/10000)
[Test]  Epoch: 37	Loss: 0.014579	Acc: 71.2% (7117/10000)
[Test]  Epoch: 38	Loss: 0.014653	Acc: 70.9% (7091/10000)
[Test]  Epoch: 39	Loss: 0.014592	Acc: 71.0% (7105/10000)
[Test]  Epoch: 40	Loss: 0.014561	Acc: 70.9% (7094/10000)
[Test]  Epoch: 41	Loss: 0.014536	Acc: 71.1% (7108/10000)
[Test]  Epoch: 42	Loss: 0.014469	Acc: 71.0% (7104/10000)
[Test]  Epoch: 43	Loss: 0.014592	Acc: 71.0% (7102/10000)
[Test]  Epoch: 44	Loss: 0.014467	Acc: 70.8% (7077/10000)
[Test]  Epoch: 45	Loss: 0.014472	Acc: 71.2% (7117/10000)
[Test]  Epoch: 46	Loss: 0.014449	Acc: 71.1% (7111/10000)
[Test]  Epoch: 47	Loss: 0.014387	Acc: 71.4% (7141/10000)
[Test]  Epoch: 48	Loss: 0.014468	Acc: 71.0% (7104/10000)
[Test]  Epoch: 49	Loss: 0.014486	Acc: 71.2% (7122/10000)
[Test]  Epoch: 50	Loss: 0.014453	Acc: 71.2% (7124/10000)
[Test]  Epoch: 51	Loss: 0.014433	Acc: 71.3% (7126/10000)
[Test]  Epoch: 52	Loss: 0.014436	Acc: 71.2% (7125/10000)
[Test]  Epoch: 53	Loss: 0.014463	Acc: 71.1% (7110/10000)
[Test]  Epoch: 54	Loss: 0.014306	Acc: 71.6% (7159/10000)
[Test]  Epoch: 55	Loss: 0.014444	Acc: 71.0% (7104/10000)
[Test]  Epoch: 56	Loss: 0.014358	Acc: 71.3% (7135/10000)
[Test]  Epoch: 57	Loss: 0.014389	Acc: 71.2% (7125/10000)
[Test]  Epoch: 58	Loss: 0.014237	Acc: 71.5% (7155/10000)
[Test]  Epoch: 59	Loss: 0.014338	Acc: 71.5% (7154/10000)
[Test]  Epoch: 60	Loss: 0.014425	Acc: 71.3% (7133/10000)
[Test]  Epoch: 61	Loss: 0.014353	Acc: 71.4% (7142/10000)
[Test]  Epoch: 62	Loss: 0.014349	Acc: 71.5% (7148/10000)
[Test]  Epoch: 63	Loss: 0.014308	Acc: 71.5% (7146/10000)
[Test]  Epoch: 64	Loss: 0.014277	Acc: 71.6% (7158/10000)
[Test]  Epoch: 65	Loss: 0.014306	Acc: 71.5% (7152/10000)
[Test]  Epoch: 66	Loss: 0.014305	Acc: 71.4% (7144/10000)
[Test]  Epoch: 67	Loss: 0.014261	Acc: 71.5% (7148/10000)
[Test]  Epoch: 68	Loss: 0.014316	Acc: 71.4% (7138/10000)
[Test]  Epoch: 69	Loss: 0.014346	Acc: 71.3% (7134/10000)
[Test]  Epoch: 70	Loss: 0.014273	Acc: 71.5% (7147/10000)
[Test]  Epoch: 71	Loss: 0.014290	Acc: 71.4% (7139/10000)
[Test]  Epoch: 72	Loss: 0.014252	Acc: 71.6% (7161/10000)
[Test]  Epoch: 73	Loss: 0.014254	Acc: 71.6% (7163/10000)
[Test]  Epoch: 74	Loss: 0.014284	Acc: 71.4% (7143/10000)
[Test]  Epoch: 75	Loss: 0.014255	Acc: 71.4% (7143/10000)
[Test]  Epoch: 76	Loss: 0.014267	Acc: 71.6% (7159/10000)
[Test]  Epoch: 77	Loss: 0.014311	Acc: 71.5% (7151/10000)
[Test]  Epoch: 78	Loss: 0.014282	Acc: 71.5% (7153/10000)
[Test]  Epoch: 79	Loss: 0.014308	Acc: 71.5% (7150/10000)
[Test]  Epoch: 80	Loss: 0.014288	Acc: 71.5% (7149/10000)
[Test]  Epoch: 81	Loss: 0.014275	Acc: 71.5% (7149/10000)
[Test]  Epoch: 82	Loss: 0.014282	Acc: 71.3% (7134/10000)
[Test]  Epoch: 83	Loss: 0.014256	Acc: 71.6% (7162/10000)
[Test]  Epoch: 84	Loss: 0.014258	Acc: 71.5% (7154/10000)
[Test]  Epoch: 85	Loss: 0.014250	Acc: 71.5% (7153/10000)
[Test]  Epoch: 86	Loss: 0.014240	Acc: 71.7% (7170/10000)
[Test]  Epoch: 87	Loss: 0.014228	Acc: 71.6% (7156/10000)
[Test]  Epoch: 88	Loss: 0.014272	Acc: 71.4% (7142/10000)
[Test]  Epoch: 89	Loss: 0.014274	Acc: 71.3% (7130/10000)
[Test]  Epoch: 90	Loss: 0.014285	Acc: 71.4% (7140/10000)
[Test]  Epoch: 91	Loss: 0.014258	Acc: 71.4% (7144/10000)
[Test]  Epoch: 92	Loss: 0.014305	Acc: 71.4% (7142/10000)
[Test]  Epoch: 93	Loss: 0.014236	Acc: 71.5% (7149/10000)
[Test]  Epoch: 94	Loss: 0.014269	Acc: 71.5% (7153/10000)
[Test]  Epoch: 95	Loss: 0.014247	Acc: 71.5% (7155/10000)
[Test]  Epoch: 96	Loss: 0.014224	Acc: 71.7% (7165/10000)
[Test]  Epoch: 97	Loss: 0.014256	Acc: 71.5% (7155/10000)
[Test]  Epoch: 98	Loss: 0.014231	Acc: 71.5% (7148/10000)
[Test]  Epoch: 99	Loss: 0.014233	Acc: 71.4% (7137/10000)
[Test]  Epoch: 100	Loss: 0.014222	Acc: 71.5% (7148/10000)
===========finish==========
['2024-08-19', '17:13:23.235766', '100', 'test', '0.014221907171607017', '71.48', '71.7']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.038496	Acc: 38.0% (3801/10000)
[Test]  Epoch: 2	Loss: 0.019541	Acc: 60.2% (6024/10000)
[Test]  Epoch: 3	Loss: 0.018210	Acc: 62.8% (6280/10000)
[Test]  Epoch: 4	Loss: 0.017955	Acc: 63.1% (6306/10000)
[Test]  Epoch: 5	Loss: 0.017790	Acc: 63.7% (6367/10000)
[Test]  Epoch: 6	Loss: 0.017819	Acc: 63.9% (6390/10000)
[Test]  Epoch: 7	Loss: 0.017725	Acc: 64.2% (6416/10000)
[Test]  Epoch: 8	Loss: 0.017783	Acc: 64.1% (6409/10000)
[Test]  Epoch: 9	Loss: 0.017834	Acc: 64.1% (6414/10000)
[Test]  Epoch: 10	Loss: 0.017721	Acc: 64.5% (6451/10000)
[Test]  Epoch: 11	Loss: 0.017597	Acc: 64.7% (6470/10000)
[Test]  Epoch: 12	Loss: 0.017770	Acc: 64.2% (6424/10000)
[Test]  Epoch: 13	Loss: 0.017671	Acc: 64.5% (6455/10000)
[Test]  Epoch: 14	Loss: 0.017444	Acc: 64.8% (6480/10000)
[Test]  Epoch: 15	Loss: 0.017499	Acc: 64.7% (6470/10000)
[Test]  Epoch: 16	Loss: 0.017427	Acc: 64.9% (6494/10000)
[Test]  Epoch: 17	Loss: 0.017450	Acc: 64.9% (6491/10000)
[Test]  Epoch: 18	Loss: 0.017495	Acc: 64.8% (6483/10000)
[Test]  Epoch: 19	Loss: 0.017308	Acc: 65.1% (6512/10000)
[Test]  Epoch: 20	Loss: 0.017238	Acc: 65.2% (6516/10000)
[Test]  Epoch: 21	Loss: 0.017366	Acc: 65.0% (6504/10000)
[Test]  Epoch: 22	Loss: 0.017192	Acc: 65.2% (6515/10000)
[Test]  Epoch: 23	Loss: 0.017274	Acc: 65.1% (6507/10000)
[Test]  Epoch: 24	Loss: 0.017216	Acc: 65.5% (6548/10000)
[Test]  Epoch: 25	Loss: 0.017351	Acc: 65.1% (6508/10000)
[Test]  Epoch: 26	Loss: 0.017257	Acc: 65.4% (6537/10000)
[Test]  Epoch: 27	Loss: 0.017265	Acc: 65.4% (6540/10000)
[Test]  Epoch: 28	Loss: 0.017292	Acc: 65.0% (6505/10000)
[Test]  Epoch: 29	Loss: 0.017176	Acc: 65.3% (6532/10000)
[Test]  Epoch: 30	Loss: 0.017136	Acc: 65.6% (6559/10000)
[Test]  Epoch: 31	Loss: 0.017085	Acc: 65.8% (6580/10000)
[Test]  Epoch: 32	Loss: 0.017180	Acc: 65.7% (6565/10000)
[Test]  Epoch: 33	Loss: 0.017050	Acc: 65.7% (6571/10000)
[Test]  Epoch: 34	Loss: 0.017029	Acc: 65.6% (6562/10000)
[Test]  Epoch: 35	Loss: 0.017034	Acc: 65.6% (6559/10000)
[Test]  Epoch: 36	Loss: 0.016990	Acc: 65.6% (6559/10000)
[Test]  Epoch: 37	Loss: 0.017040	Acc: 65.9% (6587/10000)
[Test]  Epoch: 38	Loss: 0.017093	Acc: 65.9% (6594/10000)
[Test]  Epoch: 39	Loss: 0.016981	Acc: 65.8% (6585/10000)
[Test]  Epoch: 40	Loss: 0.016964	Acc: 65.8% (6582/10000)
[Test]  Epoch: 41	Loss: 0.016932	Acc: 65.9% (6593/10000)
[Test]  Epoch: 42	Loss: 0.016969	Acc: 65.9% (6591/10000)
[Test]  Epoch: 43	Loss: 0.016898	Acc: 65.7% (6568/10000)
[Test]  Epoch: 44	Loss: 0.016751	Acc: 65.9% (6593/10000)
[Test]  Epoch: 45	Loss: 0.016834	Acc: 65.9% (6589/10000)
[Test]  Epoch: 46	Loss: 0.016873	Acc: 65.8% (6584/10000)
[Test]  Epoch: 47	Loss: 0.016856	Acc: 65.8% (6576/10000)
[Test]  Epoch: 48	Loss: 0.016880	Acc: 65.8% (6581/10000)
[Test]  Epoch: 49	Loss: 0.016868	Acc: 66.0% (6596/10000)
[Test]  Epoch: 50	Loss: 0.016832	Acc: 65.8% (6584/10000)
[Test]  Epoch: 51	Loss: 0.016730	Acc: 66.1% (6607/10000)
[Test]  Epoch: 52	Loss: 0.016762	Acc: 66.1% (6606/10000)
[Test]  Epoch: 53	Loss: 0.016763	Acc: 66.1% (6608/10000)
[Test]  Epoch: 54	Loss: 0.016720	Acc: 66.1% (6611/10000)
[Test]  Epoch: 55	Loss: 0.016662	Acc: 66.4% (6639/10000)
[Test]  Epoch: 56	Loss: 0.016695	Acc: 66.3% (6629/10000)
[Test]  Epoch: 57	Loss: 0.016736	Acc: 66.0% (6600/10000)
[Test]  Epoch: 58	Loss: 0.016587	Acc: 66.0% (6599/10000)
[Test]  Epoch: 59	Loss: 0.016708	Acc: 66.1% (6609/10000)
[Test]  Epoch: 60	Loss: 0.016826	Acc: 66.1% (6610/10000)
[Test]  Epoch: 61	Loss: 0.016753	Acc: 66.1% (6609/10000)
[Test]  Epoch: 62	Loss: 0.016687	Acc: 66.3% (6627/10000)
[Test]  Epoch: 63	Loss: 0.016659	Acc: 66.2% (6617/10000)
[Test]  Epoch: 64	Loss: 0.016646	Acc: 66.1% (6612/10000)
[Test]  Epoch: 65	Loss: 0.016635	Acc: 66.1% (6607/10000)
[Test]  Epoch: 66	Loss: 0.016625	Acc: 66.0% (6605/10000)
[Test]  Epoch: 67	Loss: 0.016606	Acc: 66.2% (6625/10000)
[Test]  Epoch: 68	Loss: 0.016663	Acc: 66.1% (6608/10000)
[Test]  Epoch: 69	Loss: 0.016692	Acc: 66.1% (6610/10000)
[Test]  Epoch: 70	Loss: 0.016576	Acc: 66.3% (6627/10000)
[Test]  Epoch: 71	Loss: 0.016617	Acc: 66.2% (6619/10000)
[Test]  Epoch: 72	Loss: 0.016580	Acc: 66.3% (6630/10000)
[Test]  Epoch: 73	Loss: 0.016605	Acc: 66.3% (6627/10000)
[Test]  Epoch: 74	Loss: 0.016592	Acc: 66.3% (6630/10000)
[Test]  Epoch: 75	Loss: 0.016621	Acc: 66.2% (6619/10000)
[Test]  Epoch: 76	Loss: 0.016609	Acc: 66.2% (6620/10000)
[Test]  Epoch: 77	Loss: 0.016671	Acc: 66.1% (6608/10000)
[Test]  Epoch: 78	Loss: 0.016620	Acc: 66.2% (6622/10000)
[Test]  Epoch: 79	Loss: 0.016621	Acc: 66.3% (6632/10000)
[Test]  Epoch: 80	Loss: 0.016617	Acc: 66.3% (6626/10000)
[Test]  Epoch: 81	Loss: 0.016631	Acc: 66.2% (6617/10000)
[Test]  Epoch: 82	Loss: 0.016614	Acc: 66.2% (6625/10000)
[Test]  Epoch: 83	Loss: 0.016615	Acc: 66.2% (6624/10000)
[Test]  Epoch: 84	Loss: 0.016616	Acc: 66.2% (6618/10000)
[Test]  Epoch: 85	Loss: 0.016615	Acc: 66.2% (6622/10000)
[Test]  Epoch: 86	Loss: 0.016608	Acc: 66.2% (6624/10000)
[Test]  Epoch: 87	Loss: 0.016600	Acc: 66.2% (6624/10000)
[Test]  Epoch: 88	Loss: 0.016613	Acc: 66.3% (6629/10000)
[Test]  Epoch: 89	Loss: 0.016623	Acc: 66.3% (6630/10000)
[Test]  Epoch: 90	Loss: 0.016644	Acc: 66.3% (6627/10000)
[Test]  Epoch: 91	Loss: 0.016597	Acc: 66.3% (6628/10000)
[Test]  Epoch: 92	Loss: 0.016643	Acc: 66.3% (6630/10000)
[Test]  Epoch: 93	Loss: 0.016603	Acc: 66.3% (6628/10000)
[Test]  Epoch: 94	Loss: 0.016638	Acc: 66.3% (6629/10000)
[Test]  Epoch: 95	Loss: 0.016577	Acc: 66.3% (6632/10000)
[Test]  Epoch: 96	Loss: 0.016580	Acc: 66.2% (6624/10000)
[Test]  Epoch: 97	Loss: 0.016630	Acc: 66.2% (6625/10000)
[Test]  Epoch: 98	Loss: 0.016596	Acc: 66.3% (6629/10000)
[Test]  Epoch: 99	Loss: 0.016607	Acc: 66.2% (6625/10000)
[Test]  Epoch: 100	Loss: 0.016620	Acc: 66.2% (6618/10000)
===========finish==========
['2024-08-19', '17:17:47.163204', '100', 'test', '0.0166202770113945', '66.18', '66.39']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035003	Acc: 39.0% (3900/10000)
[Test]  Epoch: 2	Loss: 0.023593	Acc: 52.0% (5197/10000)
[Test]  Epoch: 3	Loss: 0.022292	Acc: 54.4% (5439/10000)
[Test]  Epoch: 4	Loss: 0.022378	Acc: 54.8% (5484/10000)
[Test]  Epoch: 5	Loss: 0.022476	Acc: 54.8% (5481/10000)
[Test]  Epoch: 6	Loss: 0.022163	Acc: 55.2% (5520/10000)
[Test]  Epoch: 7	Loss: 0.022183	Acc: 54.9% (5491/10000)
[Test]  Epoch: 8	Loss: 0.022011	Acc: 55.8% (5584/10000)
[Test]  Epoch: 9	Loss: 0.021935	Acc: 56.0% (5596/10000)
[Test]  Epoch: 10	Loss: 0.022080	Acc: 55.7% (5567/10000)
[Test]  Epoch: 11	Loss: 0.021926	Acc: 56.0% (5599/10000)
[Test]  Epoch: 12	Loss: 0.022083	Acc: 55.9% (5586/10000)
[Test]  Epoch: 13	Loss: 0.021788	Acc: 56.0% (5602/10000)
[Test]  Epoch: 14	Loss: 0.021973	Acc: 56.2% (5619/10000)
[Test]  Epoch: 15	Loss: 0.021924	Acc: 56.0% (5604/10000)
[Test]  Epoch: 16	Loss: 0.021783	Acc: 56.3% (5629/10000)
[Test]  Epoch: 17	Loss: 0.021757	Acc: 56.5% (5645/10000)
[Test]  Epoch: 18	Loss: 0.021809	Acc: 56.2% (5622/10000)
[Test]  Epoch: 19	Loss: 0.021733	Acc: 56.3% (5627/10000)
[Test]  Epoch: 20	Loss: 0.021707	Acc: 56.4% (5644/10000)
[Test]  Epoch: 21	Loss: 0.021830	Acc: 56.5% (5646/10000)
[Test]  Epoch: 22	Loss: 0.021750	Acc: 56.3% (5626/10000)
[Test]  Epoch: 23	Loss: 0.021730	Acc: 56.4% (5641/10000)
[Test]  Epoch: 24	Loss: 0.021694	Acc: 56.5% (5654/10000)
[Test]  Epoch: 25	Loss: 0.021540	Acc: 56.7% (5674/10000)
[Test]  Epoch: 26	Loss: 0.021469	Acc: 56.8% (5679/10000)
[Test]  Epoch: 27	Loss: 0.021550	Acc: 57.0% (5698/10000)
[Test]  Epoch: 28	Loss: 0.021640	Acc: 56.5% (5655/10000)
[Test]  Epoch: 29	Loss: 0.021605	Acc: 57.0% (5699/10000)
[Test]  Epoch: 30	Loss: 0.021590	Acc: 56.6% (5665/10000)
[Test]  Epoch: 31	Loss: 0.021545	Acc: 56.9% (5694/10000)
[Test]  Epoch: 32	Loss: 0.021444	Acc: 57.0% (5702/10000)
[Test]  Epoch: 33	Loss: 0.021444	Acc: 57.1% (5711/10000)
[Test]  Epoch: 34	Loss: 0.021326	Acc: 57.0% (5703/10000)
[Test]  Epoch: 35	Loss: 0.021370	Acc: 57.1% (5707/10000)
[Test]  Epoch: 36	Loss: 0.021404	Acc: 57.1% (5713/10000)
[Test]  Epoch: 37	Loss: 0.021387	Acc: 57.1% (5710/10000)
[Test]  Epoch: 38	Loss: 0.021429	Acc: 57.0% (5703/10000)
[Test]  Epoch: 39	Loss: 0.021363	Acc: 57.3% (5730/10000)
[Test]  Epoch: 40	Loss: 0.021307	Acc: 57.4% (5736/10000)
[Test]  Epoch: 41	Loss: 0.021337	Acc: 57.0% (5703/10000)
[Test]  Epoch: 42	Loss: 0.021312	Acc: 57.1% (5715/10000)
[Test]  Epoch: 43	Loss: 0.021390	Acc: 57.4% (5736/10000)
[Test]  Epoch: 44	Loss: 0.021214	Acc: 57.7% (5768/10000)
[Test]  Epoch: 45	Loss: 0.021284	Acc: 57.4% (5743/10000)
[Test]  Epoch: 46	Loss: 0.021256	Acc: 57.3% (5732/10000)
[Test]  Epoch: 47	Loss: 0.021166	Acc: 57.4% (5741/10000)
[Test]  Epoch: 48	Loss: 0.021297	Acc: 57.4% (5741/10000)
[Test]  Epoch: 49	Loss: 0.021326	Acc: 57.4% (5736/10000)
[Test]  Epoch: 50	Loss: 0.021178	Acc: 57.6% (5765/10000)
[Test]  Epoch: 51	Loss: 0.021269	Acc: 57.3% (5729/10000)
[Test]  Epoch: 52	Loss: 0.021271	Acc: 57.4% (5741/10000)
[Test]  Epoch: 53	Loss: 0.021142	Acc: 57.5% (5752/10000)
[Test]  Epoch: 54	Loss: 0.021176	Acc: 57.7% (5768/10000)
[Test]  Epoch: 55	Loss: 0.021133	Acc: 57.6% (5764/10000)
[Test]  Epoch: 56	Loss: 0.021207	Acc: 57.8% (5776/10000)
[Test]  Epoch: 57	Loss: 0.021149	Acc: 57.6% (5759/10000)
[Test]  Epoch: 58	Loss: 0.021083	Acc: 57.7% (5771/10000)
[Test]  Epoch: 59	Loss: 0.021159	Acc: 57.9% (5788/10000)
[Test]  Epoch: 60	Loss: 0.021211	Acc: 57.5% (5751/10000)
[Test]  Epoch: 61	Loss: 0.021198	Acc: 57.8% (5776/10000)
[Test]  Epoch: 62	Loss: 0.021187	Acc: 57.6% (5763/10000)
[Test]  Epoch: 63	Loss: 0.021131	Acc: 57.7% (5774/10000)
[Test]  Epoch: 64	Loss: 0.021089	Acc: 57.9% (5794/10000)
[Test]  Epoch: 65	Loss: 0.021066	Acc: 57.9% (5794/10000)
[Test]  Epoch: 66	Loss: 0.021131	Acc: 57.6% (5758/10000)
[Test]  Epoch: 67	Loss: 0.021070	Acc: 57.8% (5783/10000)
[Test]  Epoch: 68	Loss: 0.021180	Acc: 57.4% (5741/10000)
[Test]  Epoch: 69	Loss: 0.021167	Acc: 57.5% (5750/10000)
[Test]  Epoch: 70	Loss: 0.021116	Acc: 57.4% (5744/10000)
[Test]  Epoch: 71	Loss: 0.021059	Acc: 57.9% (5788/10000)
[Test]  Epoch: 72	Loss: 0.021064	Acc: 57.7% (5770/10000)
[Test]  Epoch: 73	Loss: 0.021137	Acc: 57.6% (5759/10000)
[Test]  Epoch: 74	Loss: 0.021075	Acc: 57.8% (5777/10000)
[Test]  Epoch: 75	Loss: 0.021077	Acc: 57.7% (5768/10000)
[Test]  Epoch: 76	Loss: 0.021098	Acc: 57.7% (5771/10000)
[Test]  Epoch: 77	Loss: 0.021152	Acc: 57.6% (5761/10000)
[Test]  Epoch: 78	Loss: 0.021180	Acc: 57.8% (5777/10000)
[Test]  Epoch: 79	Loss: 0.021116	Acc: 57.8% (5778/10000)
[Test]  Epoch: 80	Loss: 0.021069	Acc: 57.6% (5757/10000)
[Test]  Epoch: 81	Loss: 0.021096	Acc: 57.5% (5753/10000)
[Test]  Epoch: 82	Loss: 0.021118	Acc: 57.6% (5756/10000)
[Test]  Epoch: 83	Loss: 0.021072	Acc: 57.6% (5764/10000)
[Test]  Epoch: 84	Loss: 0.021111	Acc: 57.7% (5774/10000)
[Test]  Epoch: 85	Loss: 0.021075	Acc: 57.8% (5775/10000)
[Test]  Epoch: 86	Loss: 0.021080	Acc: 57.8% (5782/10000)
[Test]  Epoch: 87	Loss: 0.021118	Acc: 57.7% (5766/10000)
[Test]  Epoch: 88	Loss: 0.021092	Acc: 57.7% (5772/10000)
[Test]  Epoch: 89	Loss: 0.021166	Acc: 57.9% (5790/10000)
[Test]  Epoch: 90	Loss: 0.021124	Acc: 57.7% (5769/10000)
[Test]  Epoch: 91	Loss: 0.021135	Acc: 57.7% (5766/10000)
[Test]  Epoch: 92	Loss: 0.021121	Acc: 57.7% (5769/10000)
[Test]  Epoch: 93	Loss: 0.021068	Acc: 57.7% (5772/10000)
[Test]  Epoch: 94	Loss: 0.021160	Acc: 57.5% (5755/10000)
[Test]  Epoch: 95	Loss: 0.021079	Acc: 57.6% (5762/10000)
[Test]  Epoch: 96	Loss: 0.021071	Acc: 57.8% (5776/10000)
[Test]  Epoch: 97	Loss: 0.021073	Acc: 57.8% (5779/10000)
[Test]  Epoch: 98	Loss: 0.021148	Acc: 57.8% (5784/10000)
[Test]  Epoch: 99	Loss: 0.021072	Acc: 57.8% (5778/10000)
[Test]  Epoch: 100	Loss: 0.021103	Acc: 57.6% (5765/10000)
===========finish==========
['2024-08-19', '17:22:08.702633', '100', 'test', '0.02110286313891411', '57.65', '57.94']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.046087	Acc: 36.3% (3627/10000)
[Test]  Epoch: 2	Loss: 0.023209	Acc: 53.1% (5307/10000)
[Test]  Epoch: 3	Loss: 0.022237	Acc: 54.6% (5464/10000)
[Test]  Epoch: 4	Loss: 0.022015	Acc: 55.4% (5539/10000)
[Test]  Epoch: 5	Loss: 0.021918	Acc: 55.3% (5534/10000)
[Test]  Epoch: 6	Loss: 0.022159	Acc: 55.6% (5558/10000)
[Test]  Epoch: 7	Loss: 0.021930	Acc: 56.1% (5613/10000)
[Test]  Epoch: 8	Loss: 0.021836	Acc: 56.2% (5624/10000)
[Test]  Epoch: 9	Loss: 0.021720	Acc: 56.4% (5639/10000)
[Test]  Epoch: 10	Loss: 0.021909	Acc: 55.9% (5591/10000)
[Test]  Epoch: 11	Loss: 0.021761	Acc: 56.4% (5642/10000)
[Test]  Epoch: 12	Loss: 0.021745	Acc: 56.3% (5634/10000)
[Test]  Epoch: 13	Loss: 0.021483	Acc: 56.7% (5668/10000)
[Test]  Epoch: 14	Loss: 0.021692	Acc: 56.5% (5655/10000)
[Test]  Epoch: 15	Loss: 0.021569	Acc: 56.5% (5652/10000)
[Test]  Epoch: 16	Loss: 0.021621	Acc: 56.8% (5680/10000)
[Test]  Epoch: 17	Loss: 0.021534	Acc: 56.9% (5689/10000)
[Test]  Epoch: 18	Loss: 0.021389	Acc: 57.2% (5718/10000)
[Test]  Epoch: 19	Loss: 0.021543	Acc: 56.6% (5661/10000)
[Test]  Epoch: 20	Loss: 0.021383	Acc: 56.9% (5693/10000)
[Test]  Epoch: 21	Loss: 0.021394	Acc: 57.0% (5700/10000)
[Test]  Epoch: 22	Loss: 0.021289	Acc: 57.3% (5726/10000)
[Test]  Epoch: 23	Loss: 0.021313	Acc: 57.0% (5704/10000)
[Test]  Epoch: 24	Loss: 0.021237	Acc: 57.2% (5721/10000)
[Test]  Epoch: 25	Loss: 0.021150	Acc: 57.6% (5756/10000)
[Test]  Epoch: 26	Loss: 0.021111	Acc: 57.6% (5761/10000)
[Test]  Epoch: 27	Loss: 0.021174	Acc: 57.4% (5743/10000)
[Test]  Epoch: 28	Loss: 0.021154	Acc: 57.5% (5754/10000)
[Test]  Epoch: 29	Loss: 0.021182	Acc: 57.6% (5759/10000)
[Test]  Epoch: 30	Loss: 0.021063	Acc: 57.5% (5751/10000)
[Test]  Epoch: 31	Loss: 0.021065	Acc: 57.5% (5751/10000)
[Test]  Epoch: 32	Loss: 0.021004	Acc: 57.8% (5780/10000)
[Test]  Epoch: 33	Loss: 0.020998	Acc: 58.0% (5797/10000)
[Test]  Epoch: 34	Loss: 0.020965	Acc: 57.8% (5781/10000)
[Test]  Epoch: 35	Loss: 0.021028	Acc: 57.6% (5757/10000)
[Test]  Epoch: 36	Loss: 0.021047	Acc: 57.5% (5750/10000)
[Test]  Epoch: 37	Loss: 0.020934	Acc: 57.8% (5777/10000)
[Test]  Epoch: 38	Loss: 0.020967	Acc: 57.6% (5763/10000)
[Test]  Epoch: 39	Loss: 0.020964	Acc: 57.7% (5766/10000)
[Test]  Epoch: 40	Loss: 0.020935	Acc: 57.7% (5769/10000)
[Test]  Epoch: 41	Loss: 0.020871	Acc: 58.0% (5796/10000)
[Test]  Epoch: 42	Loss: 0.020873	Acc: 58.0% (5797/10000)
[Test]  Epoch: 43	Loss: 0.020926	Acc: 57.9% (5793/10000)
[Test]  Epoch: 44	Loss: 0.020720	Acc: 58.1% (5815/10000)
[Test]  Epoch: 45	Loss: 0.020818	Acc: 58.0% (5804/10000)
[Test]  Epoch: 46	Loss: 0.020829	Acc: 58.0% (5795/10000)
[Test]  Epoch: 47	Loss: 0.020705	Acc: 58.1% (5809/10000)
[Test]  Epoch: 48	Loss: 0.020831	Acc: 58.0% (5798/10000)
[Test]  Epoch: 49	Loss: 0.020810	Acc: 57.8% (5783/10000)
[Test]  Epoch: 50	Loss: 0.020769	Acc: 57.7% (5771/10000)
[Test]  Epoch: 51	Loss: 0.020685	Acc: 58.3% (5831/10000)
[Test]  Epoch: 52	Loss: 0.020729	Acc: 58.1% (5810/10000)
[Test]  Epoch: 53	Loss: 0.020704	Acc: 58.0% (5800/10000)
[Test]  Epoch: 54	Loss: 0.020708	Acc: 58.3% (5832/10000)
[Test]  Epoch: 55	Loss: 0.020649	Acc: 58.0% (5797/10000)
[Test]  Epoch: 56	Loss: 0.020678	Acc: 58.1% (5813/10000)
[Test]  Epoch: 57	Loss: 0.020582	Acc: 58.0% (5801/10000)
[Test]  Epoch: 58	Loss: 0.020617	Acc: 58.3% (5831/10000)
[Test]  Epoch: 59	Loss: 0.020618	Acc: 58.0% (5799/10000)
[Test]  Epoch: 60	Loss: 0.020737	Acc: 58.0% (5799/10000)
[Test]  Epoch: 61	Loss: 0.020670	Acc: 58.2% (5822/10000)
[Test]  Epoch: 62	Loss: 0.020659	Acc: 58.1% (5812/10000)
[Test]  Epoch: 63	Loss: 0.020641	Acc: 58.1% (5806/10000)
[Test]  Epoch: 64	Loss: 0.020595	Acc: 58.3% (5826/10000)
[Test]  Epoch: 65	Loss: 0.020561	Acc: 58.2% (5819/10000)
[Test]  Epoch: 66	Loss: 0.020569	Acc: 58.1% (5811/10000)
[Test]  Epoch: 67	Loss: 0.020617	Acc: 58.2% (5819/10000)
[Test]  Epoch: 68	Loss: 0.020590	Acc: 58.3% (5827/10000)
[Test]  Epoch: 69	Loss: 0.020578	Acc: 58.1% (5809/10000)
[Test]  Epoch: 70	Loss: 0.020514	Acc: 58.3% (5832/10000)
[Test]  Epoch: 71	Loss: 0.020543	Acc: 58.2% (5824/10000)
[Test]  Epoch: 72	Loss: 0.020551	Acc: 58.2% (5822/10000)
[Test]  Epoch: 73	Loss: 0.020607	Acc: 58.2% (5816/10000)
[Test]  Epoch: 74	Loss: 0.020561	Acc: 58.2% (5816/10000)
[Test]  Epoch: 75	Loss: 0.020582	Acc: 58.1% (5812/10000)
[Test]  Epoch: 76	Loss: 0.020535	Acc: 58.1% (5808/10000)
[Test]  Epoch: 77	Loss: 0.020575	Acc: 58.1% (5810/10000)
[Test]  Epoch: 78	Loss: 0.020581	Acc: 58.1% (5806/10000)
[Test]  Epoch: 79	Loss: 0.020559	Acc: 58.2% (5816/10000)
[Test]  Epoch: 80	Loss: 0.020534	Acc: 58.1% (5811/10000)
[Test]  Epoch: 81	Loss: 0.020541	Acc: 58.1% (5810/10000)
[Test]  Epoch: 82	Loss: 0.020563	Acc: 58.0% (5803/10000)
[Test]  Epoch: 83	Loss: 0.020501	Acc: 58.3% (5827/10000)
[Test]  Epoch: 84	Loss: 0.020569	Acc: 58.2% (5824/10000)
[Test]  Epoch: 85	Loss: 0.020551	Acc: 58.1% (5814/10000)
[Test]  Epoch: 86	Loss: 0.020522	Acc: 58.1% (5814/10000)
[Test]  Epoch: 87	Loss: 0.020566	Acc: 58.1% (5815/10000)
[Test]  Epoch: 88	Loss: 0.020557	Acc: 58.2% (5825/10000)
[Test]  Epoch: 89	Loss: 0.020554	Acc: 58.2% (5825/10000)
[Test]  Epoch: 90	Loss: 0.020546	Acc: 58.2% (5822/10000)
[Test]  Epoch: 91	Loss: 0.020616	Acc: 58.2% (5819/10000)
[Test]  Epoch: 92	Loss: 0.020547	Acc: 58.1% (5815/10000)
[Test]  Epoch: 93	Loss: 0.020526	Acc: 58.3% (5828/10000)
[Test]  Epoch: 94	Loss: 0.020564	Acc: 58.2% (5822/10000)
[Test]  Epoch: 95	Loss: 0.020488	Acc: 58.2% (5823/10000)
[Test]  Epoch: 96	Loss: 0.020500	Acc: 58.1% (5814/10000)
[Test]  Epoch: 97	Loss: 0.020549	Acc: 58.3% (5831/10000)
[Test]  Epoch: 98	Loss: 0.020606	Acc: 58.1% (5807/10000)
[Test]  Epoch: 99	Loss: 0.020524	Acc: 58.1% (5813/10000)
[Test]  Epoch: 100	Loss: 0.020585	Acc: 58.1% (5814/10000)
===========finish==========
['2024-08-19', '17:26:40.058995', '100', 'test', '0.02058493514060974', '58.14', '58.32']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.032891	Acc: 40.5% (4046/10000)
[Test]  Epoch: 2	Loss: 0.020889	Acc: 55.4% (5536/10000)
[Test]  Epoch: 3	Loss: 0.020157	Acc: 57.4% (5735/10000)
[Test]  Epoch: 4	Loss: 0.020102	Acc: 58.0% (5795/10000)
[Test]  Epoch: 5	Loss: 0.020117	Acc: 58.3% (5829/10000)
[Test]  Epoch: 6	Loss: 0.020107	Acc: 58.4% (5835/10000)
[Test]  Epoch: 7	Loss: 0.020163	Acc: 58.2% (5818/10000)
[Test]  Epoch: 8	Loss: 0.020068	Acc: 58.6% (5865/10000)
[Test]  Epoch: 9	Loss: 0.020094	Acc: 58.8% (5877/10000)
[Test]  Epoch: 10	Loss: 0.020040	Acc: 58.8% (5884/10000)
[Test]  Epoch: 11	Loss: 0.019935	Acc: 59.0% (5895/10000)
[Test]  Epoch: 12	Loss: 0.019974	Acc: 59.2% (5917/10000)
[Test]  Epoch: 13	Loss: 0.019909	Acc: 59.4% (5937/10000)
[Test]  Epoch: 14	Loss: 0.019947	Acc: 59.4% (5941/10000)
[Test]  Epoch: 15	Loss: 0.019986	Acc: 59.2% (5924/10000)
[Test]  Epoch: 16	Loss: 0.019847	Acc: 59.6% (5965/10000)
[Test]  Epoch: 17	Loss: 0.019809	Acc: 59.6% (5956/10000)
[Test]  Epoch: 18	Loss: 0.019817	Acc: 59.6% (5956/10000)
[Test]  Epoch: 19	Loss: 0.019884	Acc: 59.4% (5935/10000)
[Test]  Epoch: 20	Loss: 0.019843	Acc: 59.7% (5972/10000)
[Test]  Epoch: 21	Loss: 0.019860	Acc: 59.7% (5970/10000)
[Test]  Epoch: 22	Loss: 0.019781	Acc: 60.0% (6001/10000)
[Test]  Epoch: 23	Loss: 0.019761	Acc: 59.7% (5971/10000)
[Test]  Epoch: 24	Loss: 0.019832	Acc: 59.5% (5951/10000)
[Test]  Epoch: 25	Loss: 0.019809	Acc: 59.9% (5985/10000)
[Test]  Epoch: 26	Loss: 0.019781	Acc: 59.9% (5994/10000)
[Test]  Epoch: 27	Loss: 0.019698	Acc: 60.0% (6004/10000)
[Test]  Epoch: 28	Loss: 0.019843	Acc: 59.5% (5946/10000)
[Test]  Epoch: 29	Loss: 0.019708	Acc: 59.8% (5981/10000)
[Test]  Epoch: 30	Loss: 0.019712	Acc: 59.7% (5973/10000)
[Test]  Epoch: 31	Loss: 0.019662	Acc: 59.8% (5978/10000)
[Test]  Epoch: 32	Loss: 0.019719	Acc: 59.8% (5979/10000)
[Test]  Epoch: 33	Loss: 0.019601	Acc: 60.0% (5995/10000)
[Test]  Epoch: 34	Loss: 0.019635	Acc: 59.7% (5973/10000)
[Test]  Epoch: 35	Loss: 0.019576	Acc: 59.9% (5992/10000)
[Test]  Epoch: 36	Loss: 0.019623	Acc: 59.8% (5982/10000)
[Test]  Epoch: 37	Loss: 0.019547	Acc: 60.1% (6015/10000)
[Test]  Epoch: 38	Loss: 0.019499	Acc: 60.2% (6016/10000)
[Test]  Epoch: 39	Loss: 0.019550	Acc: 60.1% (6015/10000)
[Test]  Epoch: 40	Loss: 0.019488	Acc: 60.0% (6003/10000)
[Test]  Epoch: 41	Loss: 0.019392	Acc: 60.1% (6011/10000)
[Test]  Epoch: 42	Loss: 0.019441	Acc: 60.2% (6025/10000)
[Test]  Epoch: 43	Loss: 0.019468	Acc: 60.2% (6017/10000)
[Test]  Epoch: 44	Loss: 0.019399	Acc: 60.2% (6018/10000)
[Test]  Epoch: 45	Loss: 0.019421	Acc: 60.4% (6037/10000)
[Test]  Epoch: 46	Loss: 0.019474	Acc: 60.0% (6005/10000)
[Test]  Epoch: 47	Loss: 0.019435	Acc: 60.2% (6022/10000)
[Test]  Epoch: 48	Loss: 0.019437	Acc: 60.2% (6024/10000)
[Test]  Epoch: 49	Loss: 0.019409	Acc: 60.4% (6042/10000)
[Test]  Epoch: 50	Loss: 0.019363	Acc: 60.2% (6018/10000)
[Test]  Epoch: 51	Loss: 0.019358	Acc: 60.4% (6038/10000)
[Test]  Epoch: 52	Loss: 0.019369	Acc: 60.2% (6022/10000)
[Test]  Epoch: 53	Loss: 0.019399	Acc: 60.3% (6033/10000)
[Test]  Epoch: 54	Loss: 0.019340	Acc: 60.4% (6036/10000)
[Test]  Epoch: 55	Loss: 0.019308	Acc: 60.3% (6034/10000)
[Test]  Epoch: 56	Loss: 0.019240	Acc: 60.5% (6053/10000)
[Test]  Epoch: 57	Loss: 0.019274	Acc: 60.3% (6029/10000)
[Test]  Epoch: 58	Loss: 0.019308	Acc: 60.5% (6046/10000)
[Test]  Epoch: 59	Loss: 0.019291	Acc: 60.4% (6035/10000)
[Test]  Epoch: 60	Loss: 0.019353	Acc: 60.5% (6051/10000)
[Test]  Epoch: 61	Loss: 0.019301	Acc: 60.4% (6039/10000)
[Test]  Epoch: 62	Loss: 0.019305	Acc: 60.5% (6049/10000)
[Test]  Epoch: 63	Loss: 0.019251	Acc: 60.5% (6048/10000)
[Test]  Epoch: 64	Loss: 0.019264	Acc: 60.4% (6044/10000)
[Test]  Epoch: 65	Loss: 0.019254	Acc: 60.4% (6042/10000)
[Test]  Epoch: 66	Loss: 0.019244	Acc: 60.4% (6038/10000)
[Test]  Epoch: 67	Loss: 0.019225	Acc: 60.5% (6052/10000)
[Test]  Epoch: 68	Loss: 0.019274	Acc: 60.5% (6055/10000)
[Test]  Epoch: 69	Loss: 0.019266	Acc: 60.3% (6034/10000)
[Test]  Epoch: 70	Loss: 0.019219	Acc: 60.6% (6059/10000)
[Test]  Epoch: 71	Loss: 0.019252	Acc: 60.5% (6052/10000)
[Test]  Epoch: 72	Loss: 0.019200	Acc: 60.5% (6053/10000)
[Test]  Epoch: 73	Loss: 0.019243	Acc: 60.5% (6047/10000)
[Test]  Epoch: 74	Loss: 0.019254	Acc: 60.4% (6037/10000)
[Test]  Epoch: 75	Loss: 0.019259	Acc: 60.6% (6058/10000)
[Test]  Epoch: 76	Loss: 0.019230	Acc: 60.7% (6067/10000)
[Test]  Epoch: 77	Loss: 0.019251	Acc: 60.5% (6050/10000)
[Test]  Epoch: 78	Loss: 0.019227	Acc: 60.6% (6056/10000)
[Test]  Epoch: 79	Loss: 0.019189	Acc: 60.5% (6049/10000)
[Test]  Epoch: 80	Loss: 0.019210	Acc: 60.6% (6062/10000)
[Test]  Epoch: 81	Loss: 0.019220	Acc: 60.5% (6045/10000)
[Test]  Epoch: 82	Loss: 0.019261	Acc: 60.6% (6056/10000)
[Test]  Epoch: 83	Loss: 0.019233	Acc: 60.4% (6042/10000)
[Test]  Epoch: 84	Loss: 0.019213	Acc: 60.6% (6065/10000)
[Test]  Epoch: 85	Loss: 0.019218	Acc: 60.4% (6044/10000)
[Test]  Epoch: 86	Loss: 0.019187	Acc: 60.5% (6045/10000)
[Test]  Epoch: 87	Loss: 0.019228	Acc: 60.5% (6049/10000)
[Test]  Epoch: 88	Loss: 0.019237	Acc: 60.6% (6062/10000)
[Test]  Epoch: 89	Loss: 0.019261	Acc: 60.5% (6055/10000)
[Test]  Epoch: 90	Loss: 0.019242	Acc: 60.5% (6047/10000)
[Test]  Epoch: 91	Loss: 0.019255	Acc: 60.6% (6058/10000)
[Test]  Epoch: 92	Loss: 0.019233	Acc: 60.5% (6055/10000)
[Test]  Epoch: 93	Loss: 0.019211	Acc: 60.5% (6046/10000)
[Test]  Epoch: 94	Loss: 0.019239	Acc: 60.5% (6051/10000)
[Test]  Epoch: 95	Loss: 0.019226	Acc: 60.6% (6060/10000)
[Test]  Epoch: 96	Loss: 0.019216	Acc: 60.5% (6045/10000)
[Test]  Epoch: 97	Loss: 0.019255	Acc: 60.5% (6047/10000)
[Test]  Epoch: 98	Loss: 0.019269	Acc: 60.5% (6053/10000)
[Test]  Epoch: 99	Loss: 0.019267	Acc: 60.5% (6048/10000)
[Test]  Epoch: 100	Loss: 0.019270	Acc: 60.5% (6054/10000)
===========finish==========
['2024-08-19', '17:31:09.559889', '100', 'test', '0.01926958199739456', '60.54', '60.67']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036441	Acc: 36.9% (3690/10000)
[Test]  Epoch: 2	Loss: 0.019796	Acc: 57.9% (5786/10000)
[Test]  Epoch: 3	Loss: 0.019083	Acc: 59.6% (5958/10000)
[Test]  Epoch: 4	Loss: 0.018937	Acc: 59.9% (5985/10000)
[Test]  Epoch: 5	Loss: 0.018978	Acc: 59.9% (5989/10000)
[Test]  Epoch: 6	Loss: 0.018859	Acc: 60.0% (5999/10000)
[Test]  Epoch: 7	Loss: 0.018827	Acc: 60.4% (6043/10000)
[Test]  Epoch: 8	Loss: 0.018865	Acc: 60.3% (6028/10000)
[Test]  Epoch: 9	Loss: 0.018833	Acc: 60.4% (6039/10000)
[Test]  Epoch: 10	Loss: 0.018847	Acc: 60.6% (6057/10000)
[Test]  Epoch: 11	Loss: 0.018696	Acc: 61.0% (6096/10000)
[Test]  Epoch: 12	Loss: 0.018835	Acc: 60.8% (6083/10000)
[Test]  Epoch: 13	Loss: 0.018755	Acc: 60.6% (6062/10000)
[Test]  Epoch: 14	Loss: 0.018786	Acc: 60.8% (6077/10000)
[Test]  Epoch: 15	Loss: 0.018730	Acc: 60.8% (6076/10000)
[Test]  Epoch: 16	Loss: 0.018747	Acc: 60.7% (6067/10000)
[Test]  Epoch: 17	Loss: 0.018717	Acc: 60.9% (6094/10000)
[Test]  Epoch: 18	Loss: 0.018605	Acc: 61.0% (6101/10000)
[Test]  Epoch: 19	Loss: 0.018652	Acc: 61.0% (6095/10000)
[Test]  Epoch: 20	Loss: 0.018575	Acc: 60.9% (6087/10000)
[Test]  Epoch: 21	Loss: 0.018611	Acc: 60.8% (6077/10000)
[Test]  Epoch: 22	Loss: 0.018493	Acc: 60.8% (6079/10000)
[Test]  Epoch: 23	Loss: 0.018497	Acc: 60.9% (6087/10000)
[Test]  Epoch: 24	Loss: 0.018607	Acc: 61.1% (6109/10000)
[Test]  Epoch: 25	Loss: 0.018481	Acc: 61.3% (6128/10000)
[Test]  Epoch: 26	Loss: 0.018470	Acc: 61.2% (6121/10000)
[Test]  Epoch: 27	Loss: 0.018411	Acc: 61.2% (6123/10000)
[Test]  Epoch: 28	Loss: 0.018391	Acc: 61.1% (6115/10000)
[Test]  Epoch: 29	Loss: 0.018433	Acc: 61.2% (6118/10000)
[Test]  Epoch: 30	Loss: 0.018461	Acc: 61.3% (6126/10000)
[Test]  Epoch: 31	Loss: 0.018403	Acc: 61.5% (6148/10000)
[Test]  Epoch: 32	Loss: 0.018411	Acc: 61.2% (6121/10000)
[Test]  Epoch: 33	Loss: 0.018312	Acc: 61.6% (6156/10000)
[Test]  Epoch: 34	Loss: 0.018280	Acc: 61.5% (6145/10000)
[Test]  Epoch: 35	Loss: 0.018324	Acc: 61.7% (6167/10000)
[Test]  Epoch: 36	Loss: 0.018311	Acc: 61.5% (6148/10000)
[Test]  Epoch: 37	Loss: 0.018183	Acc: 61.6% (6156/10000)
[Test]  Epoch: 38	Loss: 0.018285	Acc: 61.4% (6142/10000)
[Test]  Epoch: 39	Loss: 0.018286	Acc: 61.6% (6162/10000)
[Test]  Epoch: 40	Loss: 0.018250	Acc: 61.5% (6151/10000)
[Test]  Epoch: 41	Loss: 0.018260	Acc: 61.5% (6154/10000)
[Test]  Epoch: 42	Loss: 0.018206	Acc: 61.5% (6151/10000)
[Test]  Epoch: 43	Loss: 0.018271	Acc: 61.5% (6154/10000)
[Test]  Epoch: 44	Loss: 0.018135	Acc: 61.9% (6185/10000)
[Test]  Epoch: 45	Loss: 0.018175	Acc: 61.7% (6168/10000)
[Test]  Epoch: 46	Loss: 0.018212	Acc: 61.5% (6151/10000)
[Test]  Epoch: 47	Loss: 0.018145	Acc: 61.6% (6163/10000)
[Test]  Epoch: 48	Loss: 0.018189	Acc: 61.8% (6178/10000)
[Test]  Epoch: 49	Loss: 0.018143	Acc: 61.6% (6157/10000)
[Test]  Epoch: 50	Loss: 0.018086	Acc: 61.6% (6162/10000)
[Test]  Epoch: 51	Loss: 0.018067	Acc: 61.7% (6174/10000)
[Test]  Epoch: 52	Loss: 0.018076	Acc: 61.9% (6189/10000)
[Test]  Epoch: 53	Loss: 0.018080	Acc: 61.7% (6166/10000)
[Test]  Epoch: 54	Loss: 0.018095	Acc: 61.7% (6170/10000)
[Test]  Epoch: 55	Loss: 0.018053	Acc: 61.8% (6182/10000)
[Test]  Epoch: 56	Loss: 0.018069	Acc: 61.9% (6187/10000)
[Test]  Epoch: 57	Loss: 0.018025	Acc: 61.7% (6173/10000)
[Test]  Epoch: 58	Loss: 0.018020	Acc: 61.9% (6188/10000)
[Test]  Epoch: 59	Loss: 0.018114	Acc: 61.7% (6170/10000)
[Test]  Epoch: 60	Loss: 0.018198	Acc: 61.5% (6153/10000)
[Test]  Epoch: 61	Loss: 0.018118	Acc: 61.7% (6169/10000)
[Test]  Epoch: 62	Loss: 0.018095	Acc: 61.8% (6181/10000)
[Test]  Epoch: 63	Loss: 0.018036	Acc: 61.8% (6175/10000)
[Test]  Epoch: 64	Loss: 0.018044	Acc: 61.7% (6172/10000)
[Test]  Epoch: 65	Loss: 0.018061	Acc: 61.6% (6164/10000)
[Test]  Epoch: 66	Loss: 0.018033	Acc: 61.9% (6190/10000)
[Test]  Epoch: 67	Loss: 0.018071	Acc: 61.9% (6188/10000)
[Test]  Epoch: 68	Loss: 0.018042	Acc: 61.9% (6193/10000)
[Test]  Epoch: 69	Loss: 0.018074	Acc: 61.9% (6185/10000)
[Test]  Epoch: 70	Loss: 0.018003	Acc: 61.9% (6191/10000)
[Test]  Epoch: 71	Loss: 0.018041	Acc: 61.9% (6189/10000)
[Test]  Epoch: 72	Loss: 0.018014	Acc: 61.9% (6188/10000)
[Test]  Epoch: 73	Loss: 0.018024	Acc: 62.0% (6196/10000)
[Test]  Epoch: 74	Loss: 0.018023	Acc: 62.1% (6210/10000)
[Test]  Epoch: 75	Loss: 0.018028	Acc: 62.0% (6200/10000)
[Test]  Epoch: 76	Loss: 0.017991	Acc: 62.0% (6200/10000)
[Test]  Epoch: 77	Loss: 0.018031	Acc: 62.0% (6195/10000)
[Test]  Epoch: 78	Loss: 0.018064	Acc: 61.8% (6179/10000)
[Test]  Epoch: 79	Loss: 0.018052	Acc: 61.9% (6191/10000)
[Test]  Epoch: 80	Loss: 0.018021	Acc: 62.1% (6210/10000)
[Test]  Epoch: 81	Loss: 0.018027	Acc: 62.0% (6199/10000)
[Test]  Epoch: 82	Loss: 0.018049	Acc: 62.0% (6199/10000)
[Test]  Epoch: 83	Loss: 0.018020	Acc: 61.9% (6191/10000)
[Test]  Epoch: 84	Loss: 0.018011	Acc: 62.0% (6196/10000)
[Test]  Epoch: 85	Loss: 0.018009	Acc: 62.0% (6199/10000)
[Test]  Epoch: 86	Loss: 0.018023	Acc: 62.0% (6197/10000)
[Test]  Epoch: 87	Loss: 0.017979	Acc: 62.1% (6210/10000)
[Test]  Epoch: 88	Loss: 0.017994	Acc: 62.1% (6209/10000)
[Test]  Epoch: 89	Loss: 0.018079	Acc: 61.9% (6193/10000)
[Test]  Epoch: 90	Loss: 0.018066	Acc: 61.9% (6192/10000)
[Test]  Epoch: 91	Loss: 0.018009	Acc: 61.9% (6194/10000)
[Test]  Epoch: 92	Loss: 0.018021	Acc: 62.0% (6198/10000)
[Test]  Epoch: 93	Loss: 0.018005	Acc: 62.0% (6195/10000)
[Test]  Epoch: 94	Loss: 0.017996	Acc: 62.0% (6203/10000)
[Test]  Epoch: 95	Loss: 0.017968	Acc: 62.0% (6197/10000)
[Test]  Epoch: 96	Loss: 0.017971	Acc: 62.0% (6196/10000)
[Test]  Epoch: 97	Loss: 0.018006	Acc: 62.1% (6208/10000)
[Test]  Epoch: 98	Loss: 0.018054	Acc: 61.9% (6186/10000)
[Test]  Epoch: 99	Loss: 0.018023	Acc: 61.9% (6188/10000)
[Test]  Epoch: 100	Loss: 0.018034	Acc: 61.9% (6189/10000)
===========finish==========
['2024-08-19', '17:35:30.696164', '100', 'test', '0.018034432518482208', '61.89', '62.1']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.031201	Acc: 40.9% (4088/10000)
[Test]  Epoch: 2	Loss: 0.019901	Acc: 57.5% (5752/10000)
[Test]  Epoch: 3	Loss: 0.018906	Acc: 59.5% (5949/10000)
[Test]  Epoch: 4	Loss: 0.018860	Acc: 59.6% (5957/10000)
[Test]  Epoch: 5	Loss: 0.018959	Acc: 60.0% (6001/10000)
[Test]  Epoch: 6	Loss: 0.018822	Acc: 60.2% (6024/10000)
[Test]  Epoch: 7	Loss: 0.018804	Acc: 60.2% (6017/10000)
[Test]  Epoch: 8	Loss: 0.018805	Acc: 60.4% (6040/10000)
[Test]  Epoch: 9	Loss: 0.018941	Acc: 60.3% (6033/10000)
[Test]  Epoch: 10	Loss: 0.018876	Acc: 60.2% (6018/10000)
[Test]  Epoch: 11	Loss: 0.018697	Acc: 60.7% (6067/10000)
[Test]  Epoch: 12	Loss: 0.018792	Acc: 60.5% (6049/10000)
[Test]  Epoch: 13	Loss: 0.018704	Acc: 60.6% (6065/10000)
[Test]  Epoch: 14	Loss: 0.018838	Acc: 60.5% (6049/10000)
[Test]  Epoch: 15	Loss: 0.018773	Acc: 60.5% (6045/10000)
[Test]  Epoch: 16	Loss: 0.018771	Acc: 60.5% (6054/10000)
[Test]  Epoch: 17	Loss: 0.018744	Acc: 60.8% (6077/10000)
[Test]  Epoch: 18	Loss: 0.018597	Acc: 60.6% (6062/10000)
[Test]  Epoch: 19	Loss: 0.018653	Acc: 60.7% (6068/10000)
[Test]  Epoch: 20	Loss: 0.018584	Acc: 60.6% (6057/10000)
[Test]  Epoch: 21	Loss: 0.018640	Acc: 60.6% (6065/10000)
[Test]  Epoch: 22	Loss: 0.018514	Acc: 60.8% (6082/10000)
[Test]  Epoch: 23	Loss: 0.018529	Acc: 60.5% (6047/10000)
[Test]  Epoch: 24	Loss: 0.018636	Acc: 60.8% (6080/10000)
[Test]  Epoch: 25	Loss: 0.018590	Acc: 60.9% (6088/10000)
[Test]  Epoch: 26	Loss: 0.018547	Acc: 60.7% (6072/10000)
[Test]  Epoch: 27	Loss: 0.018433	Acc: 61.0% (6103/10000)
[Test]  Epoch: 28	Loss: 0.018546	Acc: 60.9% (6088/10000)
[Test]  Epoch: 29	Loss: 0.018549	Acc: 60.6% (6057/10000)
[Test]  Epoch: 30	Loss: 0.018548	Acc: 60.9% (6092/10000)
[Test]  Epoch: 31	Loss: 0.018431	Acc: 61.4% (6140/10000)
[Test]  Epoch: 32	Loss: 0.018424	Acc: 61.1% (6109/10000)
[Test]  Epoch: 33	Loss: 0.018363	Acc: 61.0% (6102/10000)
[Test]  Epoch: 34	Loss: 0.018337	Acc: 61.2% (6124/10000)
[Test]  Epoch: 35	Loss: 0.018356	Acc: 61.4% (6135/10000)
[Test]  Epoch: 36	Loss: 0.018348	Acc: 60.9% (6093/10000)
[Test]  Epoch: 37	Loss: 0.018269	Acc: 61.3% (6127/10000)
[Test]  Epoch: 38	Loss: 0.018343	Acc: 61.4% (6143/10000)
[Test]  Epoch: 39	Loss: 0.018392	Acc: 61.4% (6144/10000)
[Test]  Epoch: 40	Loss: 0.018340	Acc: 61.4% (6135/10000)
[Test]  Epoch: 41	Loss: 0.018292	Acc: 61.6% (6157/10000)
[Test]  Epoch: 42	Loss: 0.018369	Acc: 61.3% (6132/10000)
[Test]  Epoch: 43	Loss: 0.018301	Acc: 61.6% (6161/10000)
[Test]  Epoch: 44	Loss: 0.018220	Acc: 61.5% (6151/10000)
[Test]  Epoch: 45	Loss: 0.018296	Acc: 61.4% (6139/10000)
[Test]  Epoch: 46	Loss: 0.018277	Acc: 61.4% (6137/10000)
[Test]  Epoch: 47	Loss: 0.018249	Acc: 61.5% (6148/10000)
[Test]  Epoch: 48	Loss: 0.018290	Acc: 61.4% (6143/10000)
[Test]  Epoch: 49	Loss: 0.018228	Acc: 61.5% (6151/10000)
[Test]  Epoch: 50	Loss: 0.018181	Acc: 61.2% (6120/10000)
[Test]  Epoch: 51	Loss: 0.018197	Acc: 61.2% (6123/10000)
[Test]  Epoch: 52	Loss: 0.018185	Acc: 61.6% (6165/10000)
[Test]  Epoch: 53	Loss: 0.018127	Acc: 61.4% (6136/10000)
[Test]  Epoch: 54	Loss: 0.018119	Acc: 61.8% (6180/10000)
[Test]  Epoch: 55	Loss: 0.018084	Acc: 61.6% (6156/10000)
[Test]  Epoch: 56	Loss: 0.018149	Acc: 61.7% (6170/10000)
[Test]  Epoch: 57	Loss: 0.018141	Acc: 61.3% (6134/10000)
[Test]  Epoch: 58	Loss: 0.018109	Acc: 61.7% (6170/10000)
[Test]  Epoch: 59	Loss: 0.018161	Acc: 61.7% (6173/10000)
[Test]  Epoch: 60	Loss: 0.018215	Acc: 61.2% (6125/10000)
[Test]  Epoch: 61	Loss: 0.018145	Acc: 61.6% (6164/10000)
[Test]  Epoch: 62	Loss: 0.018117	Acc: 61.7% (6170/10000)
[Test]  Epoch: 63	Loss: 0.018037	Acc: 61.6% (6160/10000)
[Test]  Epoch: 64	Loss: 0.018065	Acc: 61.7% (6167/10000)
[Test]  Epoch: 65	Loss: 0.018105	Acc: 61.6% (6161/10000)
[Test]  Epoch: 66	Loss: 0.018076	Acc: 61.7% (6166/10000)
[Test]  Epoch: 67	Loss: 0.018114	Acc: 61.6% (6164/10000)
[Test]  Epoch: 68	Loss: 0.018099	Acc: 61.6% (6165/10000)
[Test]  Epoch: 69	Loss: 0.018102	Acc: 61.6% (6160/10000)
[Test]  Epoch: 70	Loss: 0.018039	Acc: 61.6% (6160/10000)
[Test]  Epoch: 71	Loss: 0.018078	Acc: 61.4% (6139/10000)
[Test]  Epoch: 72	Loss: 0.018071	Acc: 61.6% (6165/10000)
[Test]  Epoch: 73	Loss: 0.018077	Acc: 61.8% (6179/10000)
[Test]  Epoch: 74	Loss: 0.018080	Acc: 61.6% (6160/10000)
[Test]  Epoch: 75	Loss: 0.018055	Acc: 61.8% (6177/10000)
[Test]  Epoch: 76	Loss: 0.018046	Acc: 61.6% (6163/10000)
[Test]  Epoch: 77	Loss: 0.018054	Acc: 61.8% (6176/10000)
[Test]  Epoch: 78	Loss: 0.018090	Acc: 61.5% (6152/10000)
[Test]  Epoch: 79	Loss: 0.018103	Acc: 61.6% (6159/10000)
[Test]  Epoch: 80	Loss: 0.018073	Acc: 61.6% (6162/10000)
[Test]  Epoch: 81	Loss: 0.018097	Acc: 61.6% (6156/10000)
[Test]  Epoch: 82	Loss: 0.018110	Acc: 61.6% (6158/10000)
[Test]  Epoch: 83	Loss: 0.018117	Acc: 61.5% (6150/10000)
[Test]  Epoch: 84	Loss: 0.018096	Acc: 61.7% (6166/10000)
[Test]  Epoch: 85	Loss: 0.018100	Acc: 61.4% (6138/10000)
[Test]  Epoch: 86	Loss: 0.018095	Acc: 61.6% (6158/10000)
[Test]  Epoch: 87	Loss: 0.018080	Acc: 61.8% (6181/10000)
[Test]  Epoch: 88	Loss: 0.018085	Acc: 61.5% (6155/10000)
[Test]  Epoch: 89	Loss: 0.018161	Acc: 61.5% (6148/10000)
[Test]  Epoch: 90	Loss: 0.018117	Acc: 61.5% (6155/10000)
[Test]  Epoch: 91	Loss: 0.018063	Acc: 61.7% (6167/10000)
[Test]  Epoch: 92	Loss: 0.018088	Acc: 61.7% (6169/10000)
[Test]  Epoch: 93	Loss: 0.018040	Acc: 61.6% (6165/10000)
[Test]  Epoch: 94	Loss: 0.018052	Acc: 61.7% (6173/10000)
[Test]  Epoch: 95	Loss: 0.018044	Acc: 61.7% (6174/10000)
[Test]  Epoch: 96	Loss: 0.018064	Acc: 61.7% (6167/10000)
[Test]  Epoch: 97	Loss: 0.018074	Acc: 61.6% (6160/10000)
[Test]  Epoch: 98	Loss: 0.018088	Acc: 61.6% (6165/10000)
[Test]  Epoch: 99	Loss: 0.018084	Acc: 61.6% (6157/10000)
[Test]  Epoch: 100	Loss: 0.018090	Acc: 61.6% (6157/10000)
===========finish==========
['2024-08-19', '17:39:49.479185', '100', 'test', '0.018089536088705064', '61.57', '61.81']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.033769	Acc: 36.5% (3648/10000)
[Test]  Epoch: 2	Loss: 0.019847	Acc: 57.7% (5768/10000)
[Test]  Epoch: 3	Loss: 0.019113	Acc: 59.0% (5900/10000)
[Test]  Epoch: 4	Loss: 0.019012	Acc: 59.5% (5945/10000)
[Test]  Epoch: 5	Loss: 0.019015	Acc: 60.0% (6004/10000)
[Test]  Epoch: 6	Loss: 0.018941	Acc: 60.0% (6001/10000)
[Test]  Epoch: 7	Loss: 0.018987	Acc: 60.0% (5999/10000)
[Test]  Epoch: 8	Loss: 0.018903	Acc: 60.2% (6023/10000)
[Test]  Epoch: 9	Loss: 0.018949	Acc: 60.3% (6031/10000)
[Test]  Epoch: 10	Loss: 0.018841	Acc: 60.7% (6067/10000)
[Test]  Epoch: 11	Loss: 0.018810	Acc: 60.8% (6075/10000)
[Test]  Epoch: 12	Loss: 0.018834	Acc: 60.7% (6066/10000)
[Test]  Epoch: 13	Loss: 0.018823	Acc: 60.5% (6045/10000)
[Test]  Epoch: 14	Loss: 0.018908	Acc: 60.9% (6087/10000)
[Test]  Epoch: 15	Loss: 0.018769	Acc: 60.9% (6087/10000)
[Test]  Epoch: 16	Loss: 0.018864	Acc: 60.9% (6089/10000)
[Test]  Epoch: 17	Loss: 0.018774	Acc: 60.5% (6051/10000)
[Test]  Epoch: 18	Loss: 0.018662	Acc: 60.9% (6094/10000)
[Test]  Epoch: 19	Loss: 0.018797	Acc: 60.5% (6053/10000)
[Test]  Epoch: 20	Loss: 0.018798	Acc: 60.7% (6066/10000)
[Test]  Epoch: 21	Loss: 0.018727	Acc: 61.1% (6109/10000)
[Test]  Epoch: 22	Loss: 0.018622	Acc: 61.2% (6125/10000)
[Test]  Epoch: 23	Loss: 0.018642	Acc: 61.1% (6112/10000)
[Test]  Epoch: 24	Loss: 0.018711	Acc: 61.0% (6099/10000)
[Test]  Epoch: 25	Loss: 0.018701	Acc: 61.1% (6109/10000)
[Test]  Epoch: 26	Loss: 0.018654	Acc: 61.1% (6113/10000)
[Test]  Epoch: 27	Loss: 0.018581	Acc: 61.4% (6136/10000)
[Test]  Epoch: 28	Loss: 0.018660	Acc: 61.2% (6117/10000)
[Test]  Epoch: 29	Loss: 0.018588	Acc: 61.2% (6124/10000)
[Test]  Epoch: 30	Loss: 0.018521	Acc: 61.2% (6123/10000)
[Test]  Epoch: 31	Loss: 0.018411	Acc: 61.5% (6152/10000)
[Test]  Epoch: 32	Loss: 0.018503	Acc: 61.5% (6153/10000)
[Test]  Epoch: 33	Loss: 0.018412	Acc: 61.6% (6162/10000)
[Test]  Epoch: 34	Loss: 0.018396	Acc: 61.6% (6162/10000)
[Test]  Epoch: 35	Loss: 0.018358	Acc: 61.6% (6162/10000)
[Test]  Epoch: 36	Loss: 0.018380	Acc: 61.5% (6155/10000)
[Test]  Epoch: 37	Loss: 0.018366	Acc: 61.6% (6163/10000)
[Test]  Epoch: 38	Loss: 0.018353	Acc: 61.5% (6152/10000)
[Test]  Epoch: 39	Loss: 0.018384	Acc: 61.6% (6164/10000)
[Test]  Epoch: 40	Loss: 0.018347	Acc: 61.8% (6183/10000)
[Test]  Epoch: 41	Loss: 0.018339	Acc: 61.9% (6193/10000)
[Test]  Epoch: 42	Loss: 0.018364	Acc: 61.6% (6158/10000)
[Test]  Epoch: 43	Loss: 0.018400	Acc: 61.8% (6177/10000)
[Test]  Epoch: 44	Loss: 0.018263	Acc: 61.7% (6170/10000)
[Test]  Epoch: 45	Loss: 0.018253	Acc: 61.9% (6194/10000)
[Test]  Epoch: 46	Loss: 0.018310	Acc: 61.8% (6183/10000)
[Test]  Epoch: 47	Loss: 0.018255	Acc: 61.7% (6168/10000)
[Test]  Epoch: 48	Loss: 0.018359	Acc: 61.7% (6172/10000)
[Test]  Epoch: 49	Loss: 0.018237	Acc: 61.7% (6174/10000)
[Test]  Epoch: 50	Loss: 0.018227	Acc: 61.8% (6180/10000)
[Test]  Epoch: 51	Loss: 0.018286	Acc: 61.9% (6192/10000)
[Test]  Epoch: 52	Loss: 0.018260	Acc: 61.9% (6187/10000)
[Test]  Epoch: 53	Loss: 0.018253	Acc: 61.9% (6187/10000)
[Test]  Epoch: 54	Loss: 0.018216	Acc: 61.8% (6177/10000)
[Test]  Epoch: 55	Loss: 0.018164	Acc: 61.8% (6183/10000)
[Test]  Epoch: 56	Loss: 0.018194	Acc: 61.6% (6163/10000)
[Test]  Epoch: 57	Loss: 0.018183	Acc: 61.6% (6161/10000)
[Test]  Epoch: 58	Loss: 0.018225	Acc: 61.7% (6170/10000)
[Test]  Epoch: 59	Loss: 0.018275	Acc: 61.7% (6173/10000)
[Test]  Epoch: 60	Loss: 0.018271	Acc: 61.7% (6166/10000)
[Test]  Epoch: 61	Loss: 0.018227	Acc: 61.8% (6181/10000)
[Test]  Epoch: 62	Loss: 0.018186	Acc: 61.9% (6185/10000)
[Test]  Epoch: 63	Loss: 0.018101	Acc: 62.0% (6200/10000)
[Test]  Epoch: 64	Loss: 0.018143	Acc: 62.0% (6203/10000)
[Test]  Epoch: 65	Loss: 0.018145	Acc: 61.9% (6192/10000)
[Test]  Epoch: 66	Loss: 0.018150	Acc: 61.8% (6184/10000)
[Test]  Epoch: 67	Loss: 0.018166	Acc: 61.9% (6185/10000)
[Test]  Epoch: 68	Loss: 0.018137	Acc: 62.1% (6209/10000)
[Test]  Epoch: 69	Loss: 0.018137	Acc: 62.0% (6201/10000)
[Test]  Epoch: 70	Loss: 0.018092	Acc: 62.1% (6208/10000)
[Test]  Epoch: 71	Loss: 0.018116	Acc: 61.8% (6179/10000)
[Test]  Epoch: 72	Loss: 0.018105	Acc: 62.2% (6217/10000)
[Test]  Epoch: 73	Loss: 0.018130	Acc: 61.9% (6190/10000)
[Test]  Epoch: 74	Loss: 0.018128	Acc: 62.0% (6204/10000)
[Test]  Epoch: 75	Loss: 0.018115	Acc: 62.1% (6208/10000)
[Test]  Epoch: 76	Loss: 0.018100	Acc: 62.1% (6208/10000)
[Test]  Epoch: 77	Loss: 0.018100	Acc: 62.1% (6206/10000)
[Test]  Epoch: 78	Loss: 0.018128	Acc: 62.0% (6196/10000)
[Test]  Epoch: 79	Loss: 0.018134	Acc: 62.0% (6201/10000)
[Test]  Epoch: 80	Loss: 0.018132	Acc: 61.9% (6190/10000)
[Test]  Epoch: 81	Loss: 0.018126	Acc: 61.8% (6179/10000)
[Test]  Epoch: 82	Loss: 0.018169	Acc: 61.8% (6179/10000)
[Test]  Epoch: 83	Loss: 0.018182	Acc: 61.8% (6182/10000)
[Test]  Epoch: 84	Loss: 0.018135	Acc: 62.0% (6197/10000)
[Test]  Epoch: 85	Loss: 0.018109	Acc: 61.9% (6189/10000)
[Test]  Epoch: 86	Loss: 0.018122	Acc: 62.1% (6207/10000)
[Test]  Epoch: 87	Loss: 0.018110	Acc: 62.0% (6199/10000)
[Test]  Epoch: 88	Loss: 0.018127	Acc: 61.9% (6187/10000)
[Test]  Epoch: 89	Loss: 0.018188	Acc: 61.9% (6188/10000)
[Test]  Epoch: 90	Loss: 0.018184	Acc: 61.9% (6185/10000)
[Test]  Epoch: 91	Loss: 0.018095	Acc: 62.0% (6196/10000)
[Test]  Epoch: 92	Loss: 0.018122	Acc: 62.1% (6206/10000)
[Test]  Epoch: 93	Loss: 0.018096	Acc: 62.0% (6204/10000)
[Test]  Epoch: 94	Loss: 0.018079	Acc: 62.1% (6206/10000)
[Test]  Epoch: 95	Loss: 0.018094	Acc: 62.1% (6207/10000)
[Test]  Epoch: 96	Loss: 0.018103	Acc: 62.1% (6210/10000)
[Test]  Epoch: 97	Loss: 0.018117	Acc: 61.9% (6186/10000)
[Test]  Epoch: 98	Loss: 0.018125	Acc: 62.0% (6197/10000)
[Test]  Epoch: 99	Loss: 0.018108	Acc: 61.9% (6193/10000)
[Test]  Epoch: 100	Loss: 0.018153	Acc: 61.9% (6190/10000)
===========finish==========
['2024-08-19', '17:44:10.386453', '100', 'test', '0.018153046548366546', '61.9', '62.17']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.030509	Acc: 35.0% (3503/10000)
[Test]  Epoch: 2	Loss: 0.020991	Acc: 52.7% (5271/10000)
[Test]  Epoch: 3	Loss: 0.020248	Acc: 54.8% (5483/10000)
[Test]  Epoch: 4	Loss: 0.020236	Acc: 55.8% (5582/10000)
[Test]  Epoch: 5	Loss: 0.020069	Acc: 56.6% (5656/10000)
[Test]  Epoch: 6	Loss: 0.020190	Acc: 56.1% (5606/10000)
[Test]  Epoch: 7	Loss: 0.020179	Acc: 56.4% (5635/10000)
[Test]  Epoch: 8	Loss: 0.020197	Acc: 56.7% (5666/10000)
[Test]  Epoch: 9	Loss: 0.020063	Acc: 56.7% (5669/10000)
[Test]  Epoch: 10	Loss: 0.020057	Acc: 57.1% (5714/10000)
[Test]  Epoch: 11	Loss: 0.019999	Acc: 57.1% (5711/10000)
[Test]  Epoch: 12	Loss: 0.020193	Acc: 56.8% (5679/10000)
[Test]  Epoch: 13	Loss: 0.020090	Acc: 56.9% (5694/10000)
[Test]  Epoch: 14	Loss: 0.020120	Acc: 56.8% (5682/10000)
[Test]  Epoch: 15	Loss: 0.020194	Acc: 56.9% (5692/10000)
[Test]  Epoch: 16	Loss: 0.020007	Acc: 57.1% (5714/10000)
[Test]  Epoch: 17	Loss: 0.020150	Acc: 57.2% (5725/10000)
[Test]  Epoch: 18	Loss: 0.019933	Acc: 57.5% (5748/10000)
[Test]  Epoch: 19	Loss: 0.020015	Acc: 57.2% (5719/10000)
[Test]  Epoch: 20	Loss: 0.019965	Acc: 57.6% (5758/10000)
[Test]  Epoch: 21	Loss: 0.020016	Acc: 57.6% (5759/10000)
[Test]  Epoch: 22	Loss: 0.019937	Acc: 57.7% (5769/10000)
[Test]  Epoch: 23	Loss: 0.019932	Acc: 58.0% (5795/10000)
[Test]  Epoch: 24	Loss: 0.019914	Acc: 57.7% (5772/10000)
[Test]  Epoch: 25	Loss: 0.019993	Acc: 57.8% (5777/10000)
[Test]  Epoch: 26	Loss: 0.019939	Acc: 57.8% (5782/10000)
[Test]  Epoch: 27	Loss: 0.019952	Acc: 57.6% (5764/10000)
[Test]  Epoch: 28	Loss: 0.020009	Acc: 57.4% (5739/10000)
[Test]  Epoch: 29	Loss: 0.020011	Acc: 57.3% (5734/10000)
[Test]  Epoch: 30	Loss: 0.020000	Acc: 57.6% (5761/10000)
[Test]  Epoch: 31	Loss: 0.019920	Acc: 57.5% (5753/10000)
[Test]  Epoch: 32	Loss: 0.019872	Acc: 57.8% (5776/10000)
[Test]  Epoch: 33	Loss: 0.019829	Acc: 57.6% (5756/10000)
[Test]  Epoch: 34	Loss: 0.019838	Acc: 57.8% (5777/10000)
[Test]  Epoch: 35	Loss: 0.019761	Acc: 57.9% (5786/10000)
[Test]  Epoch: 36	Loss: 0.019768	Acc: 58.1% (5807/10000)
[Test]  Epoch: 37	Loss: 0.019784	Acc: 57.8% (5777/10000)
[Test]  Epoch: 38	Loss: 0.019790	Acc: 57.8% (5780/10000)
[Test]  Epoch: 39	Loss: 0.019744	Acc: 57.9% (5787/10000)
[Test]  Epoch: 40	Loss: 0.019691	Acc: 58.1% (5806/10000)
[Test]  Epoch: 41	Loss: 0.019589	Acc: 58.0% (5799/10000)
[Test]  Epoch: 42	Loss: 0.019666	Acc: 58.2% (5825/10000)
[Test]  Epoch: 43	Loss: 0.019651	Acc: 58.0% (5803/10000)
[Test]  Epoch: 44	Loss: 0.019564	Acc: 58.5% (5845/10000)
[Test]  Epoch: 45	Loss: 0.019572	Acc: 58.3% (5832/10000)
[Test]  Epoch: 46	Loss: 0.019634	Acc: 58.5% (5845/10000)
[Test]  Epoch: 47	Loss: 0.019538	Acc: 58.3% (5831/10000)
[Test]  Epoch: 48	Loss: 0.019659	Acc: 58.2% (5821/10000)
[Test]  Epoch: 49	Loss: 0.019483	Acc: 58.4% (5844/10000)
[Test]  Epoch: 50	Loss: 0.019534	Acc: 58.5% (5853/10000)
[Test]  Epoch: 51	Loss: 0.019549	Acc: 58.5% (5845/10000)
[Test]  Epoch: 52	Loss: 0.019547	Acc: 58.5% (5849/10000)
[Test]  Epoch: 53	Loss: 0.019549	Acc: 58.5% (5855/10000)
[Test]  Epoch: 54	Loss: 0.019478	Acc: 58.6% (5857/10000)
[Test]  Epoch: 55	Loss: 0.019461	Acc: 58.6% (5859/10000)
[Test]  Epoch: 56	Loss: 0.019501	Acc: 58.7% (5870/10000)
[Test]  Epoch: 57	Loss: 0.019381	Acc: 58.6% (5865/10000)
[Test]  Epoch: 58	Loss: 0.019397	Acc: 58.7% (5873/10000)
[Test]  Epoch: 59	Loss: 0.019546	Acc: 58.5% (5845/10000)
[Test]  Epoch: 60	Loss: 0.019632	Acc: 58.3% (5830/10000)
[Test]  Epoch: 61	Loss: 0.019520	Acc: 58.5% (5855/10000)
[Test]  Epoch: 62	Loss: 0.019513	Acc: 58.5% (5850/10000)
[Test]  Epoch: 63	Loss: 0.019462	Acc: 58.6% (5863/10000)
[Test]  Epoch: 64	Loss: 0.019452	Acc: 58.6% (5859/10000)
[Test]  Epoch: 65	Loss: 0.019410	Acc: 58.5% (5855/10000)
[Test]  Epoch: 66	Loss: 0.019410	Acc: 58.6% (5863/10000)
[Test]  Epoch: 67	Loss: 0.019490	Acc: 58.4% (5838/10000)
[Test]  Epoch: 68	Loss: 0.019412	Acc: 58.7% (5867/10000)
[Test]  Epoch: 69	Loss: 0.019446	Acc: 58.6% (5856/10000)
[Test]  Epoch: 70	Loss: 0.019390	Acc: 58.7% (5869/10000)
[Test]  Epoch: 71	Loss: 0.019416	Acc: 58.6% (5865/10000)
[Test]  Epoch: 72	Loss: 0.019443	Acc: 58.6% (5859/10000)
[Test]  Epoch: 73	Loss: 0.019427	Acc: 58.5% (5855/10000)
[Test]  Epoch: 74	Loss: 0.019448	Acc: 58.7% (5867/10000)
[Test]  Epoch: 75	Loss: 0.019444	Acc: 58.6% (5860/10000)
[Test]  Epoch: 76	Loss: 0.019378	Acc: 58.7% (5868/10000)
[Test]  Epoch: 77	Loss: 0.019385	Acc: 58.6% (5856/10000)
[Test]  Epoch: 78	Loss: 0.019371	Acc: 58.7% (5872/10000)
[Test]  Epoch: 79	Loss: 0.019392	Acc: 58.8% (5877/10000)
[Test]  Epoch: 80	Loss: 0.019431	Acc: 58.5% (5847/10000)
[Test]  Epoch: 81	Loss: 0.019413	Acc: 58.6% (5856/10000)
[Test]  Epoch: 82	Loss: 0.019479	Acc: 58.4% (5839/10000)
[Test]  Epoch: 83	Loss: 0.019402	Acc: 58.6% (5860/10000)
[Test]  Epoch: 84	Loss: 0.019420	Acc: 58.6% (5859/10000)
[Test]  Epoch: 85	Loss: 0.019415	Acc: 58.5% (5850/10000)
[Test]  Epoch: 86	Loss: 0.019395	Acc: 58.5% (5849/10000)
[Test]  Epoch: 87	Loss: 0.019456	Acc: 58.5% (5855/10000)
[Test]  Epoch: 88	Loss: 0.019494	Acc: 58.5% (5850/10000)
[Test]  Epoch: 89	Loss: 0.019478	Acc: 58.5% (5852/10000)
[Test]  Epoch: 90	Loss: 0.019439	Acc: 58.6% (5857/10000)
[Test]  Epoch: 91	Loss: 0.019469	Acc: 58.7% (5867/10000)
[Test]  Epoch: 92	Loss: 0.019468	Acc: 58.6% (5863/10000)
[Test]  Epoch: 93	Loss: 0.019422	Acc: 58.7% (5872/10000)
[Test]  Epoch: 94	Loss: 0.019477	Acc: 58.5% (5855/10000)
[Test]  Epoch: 95	Loss: 0.019374	Acc: 58.7% (5874/10000)
[Test]  Epoch: 96	Loss: 0.019383	Acc: 58.8% (5884/10000)
[Test]  Epoch: 97	Loss: 0.019397	Acc: 58.7% (5871/10000)
[Test]  Epoch: 98	Loss: 0.019480	Acc: 58.6% (5859/10000)
[Test]  Epoch: 99	Loss: 0.019429	Acc: 58.7% (5868/10000)
[Test]  Epoch: 100	Loss: 0.019413	Acc: 58.6% (5863/10000)
===========finish==========
['2024-08-19', '17:48:39.446275', '100', 'test', '0.01941345449090004', '58.63', '58.84']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035636	Acc: 23.0% (2300/10000)
[Test]  Epoch: 2	Loss: 0.023618	Acc: 47.8% (4784/10000)
[Test]  Epoch: 3	Loss: 0.022048	Acc: 52.8% (5282/10000)
[Test]  Epoch: 4	Loss: 0.022059	Acc: 53.9% (5387/10000)
[Test]  Epoch: 5	Loss: 0.022312	Acc: 54.2% (5425/10000)
[Test]  Epoch: 6	Loss: 0.021836	Acc: 54.7% (5467/10000)
[Test]  Epoch: 7	Loss: 0.022054	Acc: 54.5% (5445/10000)
[Test]  Epoch: 8	Loss: 0.021802	Acc: 55.2% (5523/10000)
[Test]  Epoch: 9	Loss: 0.021934	Acc: 55.2% (5518/10000)
[Test]  Epoch: 10	Loss: 0.021977	Acc: 56.0% (5596/10000)
[Test]  Epoch: 11	Loss: 0.021803	Acc: 55.8% (5577/10000)
[Test]  Epoch: 12	Loss: 0.022169	Acc: 55.7% (5570/10000)
[Test]  Epoch: 13	Loss: 0.021632	Acc: 56.1% (5609/10000)
[Test]  Epoch: 14	Loss: 0.021569	Acc: 55.8% (5576/10000)
[Test]  Epoch: 15	Loss: 0.022452	Acc: 55.9% (5588/10000)
[Test]  Epoch: 16	Loss: 0.021698	Acc: 56.1% (5614/10000)
[Test]  Epoch: 17	Loss: 0.021677	Acc: 56.4% (5637/10000)
[Test]  Epoch: 18	Loss: 0.021295	Acc: 56.7% (5673/10000)
[Test]  Epoch: 19	Loss: 0.021360	Acc: 56.7% (5670/10000)
[Test]  Epoch: 20	Loss: 0.021398	Acc: 56.3% (5634/10000)
[Test]  Epoch: 21	Loss: 0.021297	Acc: 56.8% (5677/10000)
[Test]  Epoch: 22	Loss: 0.021252	Acc: 57.0% (5702/10000)
[Test]  Epoch: 23	Loss: 0.021174	Acc: 56.7% (5671/10000)
[Test]  Epoch: 24	Loss: 0.021259	Acc: 56.8% (5679/10000)
[Test]  Epoch: 25	Loss: 0.021373	Acc: 56.6% (5662/10000)
[Test]  Epoch: 26	Loss: 0.021246	Acc: 57.0% (5695/10000)
[Test]  Epoch: 27	Loss: 0.021029	Acc: 57.2% (5718/10000)
[Test]  Epoch: 28	Loss: 0.021146	Acc: 57.0% (5705/10000)
[Test]  Epoch: 29	Loss: 0.021148	Acc: 57.2% (5722/10000)
[Test]  Epoch: 30	Loss: 0.021268	Acc: 57.0% (5702/10000)
[Test]  Epoch: 31	Loss: 0.021110	Acc: 56.8% (5678/10000)
[Test]  Epoch: 32	Loss: 0.021028	Acc: 57.5% (5747/10000)
[Test]  Epoch: 33	Loss: 0.021003	Acc: 57.7% (5769/10000)
[Test]  Epoch: 34	Loss: 0.021009	Acc: 57.5% (5746/10000)
[Test]  Epoch: 35	Loss: 0.021019	Acc: 57.3% (5733/10000)
[Test]  Epoch: 36	Loss: 0.020985	Acc: 57.3% (5728/10000)
[Test]  Epoch: 37	Loss: 0.021117	Acc: 57.2% (5723/10000)
[Test]  Epoch: 38	Loss: 0.020979	Acc: 57.6% (5760/10000)
[Test]  Epoch: 39	Loss: 0.021369	Acc: 57.3% (5732/10000)
[Test]  Epoch: 40	Loss: 0.021105	Acc: 57.4% (5743/10000)
[Test]  Epoch: 41	Loss: 0.021054	Acc: 57.7% (5774/10000)
[Test]  Epoch: 42	Loss: 0.021135	Acc: 57.7% (5773/10000)
[Test]  Epoch: 43	Loss: 0.021020	Acc: 57.6% (5759/10000)
[Test]  Epoch: 44	Loss: 0.020902	Acc: 57.7% (5771/10000)
[Test]  Epoch: 45	Loss: 0.020977	Acc: 57.4% (5743/10000)
[Test]  Epoch: 46	Loss: 0.020950	Acc: 57.5% (5755/10000)
[Test]  Epoch: 47	Loss: 0.020792	Acc: 57.6% (5760/10000)
[Test]  Epoch: 48	Loss: 0.020966	Acc: 57.5% (5752/10000)
[Test]  Epoch: 49	Loss: 0.021049	Acc: 57.5% (5748/10000)
[Test]  Epoch: 50	Loss: 0.020948	Acc: 57.5% (5751/10000)
[Test]  Epoch: 51	Loss: 0.020889	Acc: 57.6% (5764/10000)
[Test]  Epoch: 52	Loss: 0.020962	Acc: 57.6% (5757/10000)
[Test]  Epoch: 53	Loss: 0.020892	Acc: 57.8% (5779/10000)
[Test]  Epoch: 54	Loss: 0.020840	Acc: 57.5% (5753/10000)
[Test]  Epoch: 55	Loss: 0.020907	Acc: 57.8% (5783/10000)
[Test]  Epoch: 56	Loss: 0.020922	Acc: 57.4% (5741/10000)
[Test]  Epoch: 57	Loss: 0.020820	Acc: 57.5% (5746/10000)
[Test]  Epoch: 58	Loss: 0.020900	Acc: 57.4% (5736/10000)
[Test]  Epoch: 59	Loss: 0.020951	Acc: 57.4% (5735/10000)
[Test]  Epoch: 60	Loss: 0.021063	Acc: 57.1% (5713/10000)
[Test]  Epoch: 61	Loss: 0.020908	Acc: 57.5% (5751/10000)
[Test]  Epoch: 62	Loss: 0.020901	Acc: 57.6% (5757/10000)
[Test]  Epoch: 63	Loss: 0.020827	Acc: 57.6% (5758/10000)
[Test]  Epoch: 64	Loss: 0.020836	Acc: 57.6% (5758/10000)
[Test]  Epoch: 65	Loss: 0.020897	Acc: 57.4% (5735/10000)
[Test]  Epoch: 66	Loss: 0.020849	Acc: 57.5% (5752/10000)
[Test]  Epoch: 67	Loss: 0.020934	Acc: 57.4% (5737/10000)
[Test]  Epoch: 68	Loss: 0.020893	Acc: 57.5% (5754/10000)
[Test]  Epoch: 69	Loss: 0.020890	Acc: 57.5% (5754/10000)
[Test]  Epoch: 70	Loss: 0.020849	Acc: 57.8% (5775/10000)
[Test]  Epoch: 71	Loss: 0.020889	Acc: 57.6% (5756/10000)
[Test]  Epoch: 72	Loss: 0.020817	Acc: 57.5% (5750/10000)
[Test]  Epoch: 73	Loss: 0.020887	Acc: 57.6% (5759/10000)
[Test]  Epoch: 74	Loss: 0.020867	Acc: 57.4% (5740/10000)
[Test]  Epoch: 75	Loss: 0.020868	Acc: 57.3% (5728/10000)
[Test]  Epoch: 76	Loss: 0.020874	Acc: 57.5% (5754/10000)
[Test]  Epoch: 77	Loss: 0.020818	Acc: 57.4% (5736/10000)
[Test]  Epoch: 78	Loss: 0.020860	Acc: 57.4% (5735/10000)
[Test]  Epoch: 79	Loss: 0.020894	Acc: 57.2% (5720/10000)
[Test]  Epoch: 80	Loss: 0.020900	Acc: 57.6% (5756/10000)
[Test]  Epoch: 81	Loss: 0.020859	Acc: 57.6% (5760/10000)
[Test]  Epoch: 82	Loss: 0.020870	Acc: 57.6% (5761/10000)
[Test]  Epoch: 83	Loss: 0.020878	Acc: 57.6% (5756/10000)
[Test]  Epoch: 84	Loss: 0.020860	Acc: 57.8% (5781/10000)
[Test]  Epoch: 85	Loss: 0.020850	Acc: 57.6% (5757/10000)
[Test]  Epoch: 86	Loss: 0.020871	Acc: 57.5% (5751/10000)
[Test]  Epoch: 87	Loss: 0.020925	Acc: 57.6% (5762/10000)
[Test]  Epoch: 88	Loss: 0.020892	Acc: 57.7% (5770/10000)
[Test]  Epoch: 89	Loss: 0.020932	Acc: 57.5% (5746/10000)
[Test]  Epoch: 90	Loss: 0.020888	Acc: 57.5% (5753/10000)
[Test]  Epoch: 91	Loss: 0.020834	Acc: 57.7% (5769/10000)
[Test]  Epoch: 92	Loss: 0.020838	Acc: 57.5% (5747/10000)
[Test]  Epoch: 93	Loss: 0.020865	Acc: 57.6% (5759/10000)
[Test]  Epoch: 94	Loss: 0.020827	Acc: 57.7% (5766/10000)
[Test]  Epoch: 95	Loss: 0.020814	Acc: 57.5% (5755/10000)
[Test]  Epoch: 96	Loss: 0.020869	Acc: 57.3% (5726/10000)
[Test]  Epoch: 97	Loss: 0.020830	Acc: 57.6% (5759/10000)
[Test]  Epoch: 98	Loss: 0.020849	Acc: 57.5% (5749/10000)
[Test]  Epoch: 99	Loss: 0.020836	Acc: 57.6% (5757/10000)
[Test]  Epoch: 100	Loss: 0.020789	Acc: 57.5% (5750/10000)
===========finish==========
['2024-08-19', '17:53:02.538887', '100', 'test', '0.020788802325725556', '57.5', '57.83']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.bias', 0.0), ('layer4.2.bn3.weight', -0.25532251596450806), ('layer4.1.bn3.weight', -0.431115984916687), ('layer4.1.bn2.weight', -0.8567735552787781), ('layer4.0.bn3.weight', -1.3869907855987549), ('layer4.2.bn2.weight', -1.542337417602539), ('layer4.1.bn1.weight', -1.7425603866577148), ('layer4.0.downsample.1.weight', -2.195530414581299), ('layer4.2.bn1.weight', -2.5162277221679688), ('layer3.4.bn1.weight', -2.5844790935516357), ('layer3.4.bn2.weight', -2.7462878227233887), ('layer3.3.bn1.weight', -2.8508617877960205), ('layer3.4.bn3.weight', -3.1942107677459717), ('layer3.3.bn2.weight', -3.362853527069092), ('layer3.2.bn1.weight', -3.663343906402588), ('layer2.3.bn1.weight', -3.79219388961792), ('layer3.5.bn1.weight', -3.819960117340088), ('layer3.5.bn2.weight', -3.9979937076568604), ('layer1.1.bn1.weight', -4.027764320373535), ('layer1.2.bn1.weight', -4.043910980224609), ('layer3.3.bn3.weight', -4.227269172668457), ('layer1.1.bn2.weight', -4.2447829246521), ('layer2.2.bn1.weight', -4.411649703979492), ('layer3.2.bn2.weight', -4.672630310058594), ('layer3.5.bn3.weight', -4.716547012329102), ('layer1.1.bn3.weight', -4.998157501220703), ('layer1.2.bn3.weight', -5.045965671539307), ('layer2.1.bn1.weight', -5.202062129974365), ('layer2.3.bn2.weight', -5.290611267089844), ('layer2.2.bn2.weight', -5.306205749511719), ('layer1.0.bn2.weight', -5.380007743835449), ('layer1.2.bn2.weight', -5.41752815246582), ('layer3.1.bn1.weight', -5.418455123901367), ('layer2.3.bn3.weight', -5.645761013031006), ('layer1.0.bn1.weight', -5.729146480560303), ('layer2.1.bn2.weight', -5.965054988861084), ('layer3.2.bn3.weight', -6.262253284454346), ('layer2.2.bn3.weight', -6.337114334106445), ('layer3.1.bn2.weight', -6.5522990226745605), ('layer2.1.bn3.weight', -6.934593200683594), ('layer1.0.bn3.weight', -7.005728244781494), ('layer4.1.conv2.weight', -7.278209209442139), ('layer3.1.bn3.weight', -7.325861930847168), ('layer4.1.conv3.weight', -8.544623374938965), ('layer4.2.conv3.weight', -11.002939224243164), ('layer2.0.bn3.weight', -11.051823616027832), ('layer2.0.bn2.weight', -11.165401458740234), ('layer4.2.conv2.weight', -11.807210922241211), ('layer4.1.conv1.weight', -11.861047744750977), ('layer2.0.bn1.weight', -11.861307144165039), ('layer1.1.conv1.weight', -13.87671947479248), ('layer3.0.bn2.weight', -15.205124855041504), ('layer1.1.conv3.weight', -15.546338081359863), ('layer3.4.conv3.weight', -16.294260025024414), ('layer3.0.bn3.weight', -16.69080352783203), ('layer2.1.conv1.weight', -16.819198608398438), ('layer4.2.conv1.weight', -17.586307525634766), ('layer3.0.bn1.weight', -17.696063995361328), ('layer1.0.downsample.1.weight', -18.30758285522461), ('layer1.2.conv1.weight', -18.360061645507812), ('layer3.4.conv1.weight', -19.01287078857422), ('layer2.0.downsample.1.weight', -19.285396575927734), ('layer1.2.conv3.weight', -19.640193939208984), ('layer3.0.downsample.1.weight', -19.692790985107422), ('layer3.3.conv1.weight', -20.358234405517578), ('layer3.5.conv3.weight', -20.422487258911133), ('layer3.3.conv3.weight', -21.06536865234375), ('layer1.1.conv2.weight', -21.982892990112305), ('layer3.2.conv1.weight', -22.805768966674805), ('layer3.4.conv2.weight', -23.27004051208496), ('layer1.0.conv3.weight', -23.563400268554688), ('layer2.2.conv1.weight', -24.407657623291016), ('layer4.0.bn2.weight', -25.537734985351562), ('layer1.0.conv1.weight', -25.58289909362793), ('layer3.5.conv1.weight', -25.884197235107422), ('layer2.3.conv1.weight', -25.946941375732422), ('layer4.0.bn1.weight', -26.226654052734375), ('layer2.3.conv3.weight', -27.469724655151367), ('layer3.5.conv2.weight', -27.73259162902832), ('layer3.3.conv2.weight', -28.112796783447266), ('layer3.1.conv1.weight', -30.71748161315918), ('layer1.0.conv2.weight', -31.06386947631836), ('layer3.2.conv3.weight', -31.61490821838379), ('layer2.2.conv3.weight', -32.868587493896484), ('layer2.1.conv3.weight', -34.047462463378906), ('bn1.weight', -35.242835998535156), ('layer3.2.conv2.weight', -37.78449249267578), ('layer2.1.conv2.weight', -39.10803985595703), ('layer3.1.conv3.weight', -39.89768981933594), ('layer1.2.conv2.weight', -43.034278869628906), ('layer2.2.conv2.weight', -43.9514045715332), ('layer2.3.conv2.weight', -47.77814865112305), ('layer3.1.conv2.weight', -50.042659759521484), ('layer2.0.conv3.weight', -61.167762756347656), ('layer2.0.conv1.weight', -66.61029052734375), ('layer2.0.conv2.weight', -79.30095672607422), ('layer2.0.downsample.0.weight', -108.00669860839844), ('layer3.0.conv1.weight', -119.5993423461914), ('layer3.0.conv3.weight', -124.65796661376953), ('last_linear.weight', -125.61329650878906), ('layer3.0.downsample.0.weight', -136.8106689453125), ('layer3.0.conv2.weight', -143.4822998046875), ('layer1.0.downsample.0.weight', -157.38128662109375), ('conv1.weight', -161.22019958496094), ('layer4.0.conv1.weight', -177.9443359375), ('layer4.0.downsample.0.weight', -183.99868774414062), ('layer4.0.conv3.weight', -189.902099609375), ('layer4.0.conv2.weight', -221.24530029296875)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('last_linear.bias', 0.0), ('layer4.1.conv2.weight', -7.278209209442139), ('layer4.1.conv3.weight', -8.544623374938965), ('layer4.2.conv3.weight', -11.002939224243164), ('layer4.2.conv2.weight', -11.807210922241211), ('layer4.1.conv1.weight', -11.861047744750977), ('layer1.1.conv1.weight', -13.87671947479248), ('layer1.1.conv3.weight', -15.546338081359863), ('layer3.4.conv3.weight', -16.294260025024414), ('layer2.1.conv1.weight', -16.819198608398438), ('layer4.2.conv1.weight', -17.586307525634766), ('layer1.2.conv1.weight', -18.360061645507812), ('layer3.4.conv1.weight', -19.01287078857422), ('layer1.2.conv3.weight', -19.640193939208984), ('layer3.3.conv1.weight', -20.358234405517578), ('layer3.5.conv3.weight', -20.422487258911133), ('layer3.3.conv3.weight', -21.06536865234375), ('layer1.1.conv2.weight', -21.982892990112305), ('layer3.2.conv1.weight', -22.805768966674805), ('layer3.4.conv2.weight', -23.27004051208496), ('layer1.0.conv3.weight', -23.563400268554688), ('layer2.2.conv1.weight', -24.407657623291016), ('layer1.0.conv1.weight', -25.58289909362793), ('layer3.5.conv1.weight', -25.884197235107422), ('layer2.3.conv1.weight', -25.946941375732422), ('layer2.3.conv3.weight', -27.469724655151367), ('layer3.5.conv2.weight', -27.73259162902832), ('layer3.3.conv2.weight', -28.112796783447266), ('layer3.1.conv1.weight', -30.71748161315918), ('layer1.0.conv2.weight', -31.06386947631836), ('layer3.2.conv3.weight', -31.61490821838379), ('layer2.2.conv3.weight', -32.868587493896484), ('layer2.1.conv3.weight', -34.047462463378906), ('layer3.2.conv2.weight', -37.78449249267578), ('layer2.1.conv2.weight', -39.10803985595703), ('layer3.1.conv3.weight', -39.89768981933594), ('layer1.2.conv2.weight', -43.034278869628906), ('layer2.2.conv2.weight', -43.9514045715332), ('layer2.3.conv2.weight', -47.77814865112305), ('layer3.1.conv2.weight', -50.042659759521484), ('layer2.0.conv3.weight', -61.167762756347656), ('layer2.0.conv1.weight', -66.61029052734375), ('layer2.0.conv2.weight', -79.30095672607422), ('layer3.0.conv1.weight', -119.5993423461914), ('layer3.0.conv3.weight', -124.65796661376953), ('last_linear.weight', -125.61329650878906), ('layer3.0.conv2.weight', -143.4822998046875), ('conv1.weight', -161.22019958496094), ('layer4.0.conv1.weight', -177.9443359375), ('layer4.0.conv3.weight', -189.902099609375), ('layer4.0.conv2.weight', -221.24530029296875)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.032275	Acc: 54.8% (5478/10000)
[Test]  Epoch: 2	Loss: 0.032159	Acc: 54.9% (5492/10000)
[Test]  Epoch: 3	Loss: 0.032260	Acc: 55.0% (5503/10000)
[Test]  Epoch: 4	Loss: 0.032232	Acc: 55.2% (5525/10000)
[Test]  Epoch: 5	Loss: 0.032361	Acc: 55.2% (5524/10000)
[Test]  Epoch: 6	Loss: 0.032440	Acc: 55.0% (5500/10000)
[Test]  Epoch: 7	Loss: 0.032413	Acc: 55.2% (5521/10000)
[Test]  Epoch: 8	Loss: 0.032482	Acc: 55.2% (5522/10000)
[Test]  Epoch: 9	Loss: 0.032505	Acc: 55.1% (5511/10000)
[Test]  Epoch: 10	Loss: 0.032563	Acc: 55.2% (5519/10000)
[Test]  Epoch: 11	Loss: 0.032618	Acc: 55.1% (5512/10000)
[Test]  Epoch: 12	Loss: 0.032545	Acc: 55.1% (5510/10000)
[Test]  Epoch: 13	Loss: 0.032654	Acc: 55.2% (5523/10000)
[Test]  Epoch: 14	Loss: 0.032692	Acc: 55.3% (5534/10000)
[Test]  Epoch: 15	Loss: 0.032761	Acc: 55.3% (5527/10000)
[Test]  Epoch: 16	Loss: 0.032714	Acc: 55.2% (5516/10000)
[Test]  Epoch: 17	Loss: 0.032699	Acc: 55.3% (5528/10000)
[Test]  Epoch: 18	Loss: 0.032737	Acc: 55.4% (5542/10000)
[Test]  Epoch: 19	Loss: 0.032780	Acc: 55.1% (5512/10000)
[Test]  Epoch: 20	Loss: 0.032809	Acc: 55.4% (5536/10000)
[Test]  Epoch: 21	Loss: 0.032767	Acc: 55.3% (5529/10000)
[Test]  Epoch: 22	Loss: 0.032838	Acc: 55.3% (5529/10000)
[Test]  Epoch: 23	Loss: 0.032846	Acc: 55.3% (5534/10000)
[Test]  Epoch: 24	Loss: 0.032934	Acc: 55.3% (5527/10000)
[Test]  Epoch: 25	Loss: 0.032847	Acc: 55.3% (5528/10000)
[Test]  Epoch: 26	Loss: 0.032821	Acc: 55.3% (5534/10000)
[Test]  Epoch: 27	Loss: 0.032872	Acc: 55.3% (5532/10000)
[Test]  Epoch: 28	Loss: 0.032971	Acc: 55.2% (5519/10000)
[Test]  Epoch: 29	Loss: 0.032956	Acc: 55.3% (5532/10000)
[Test]  Epoch: 30	Loss: 0.032924	Acc: 55.4% (5539/10000)
[Test]  Epoch: 31	Loss: 0.033063	Acc: 55.1% (5515/10000)
[Test]  Epoch: 32	Loss: 0.033066	Acc: 55.1% (5511/10000)
[Test]  Epoch: 33	Loss: 0.033039	Acc: 55.4% (5536/10000)
[Test]  Epoch: 34	Loss: 0.032960	Acc: 55.4% (5535/10000)
[Test]  Epoch: 35	Loss: 0.033026	Acc: 55.4% (5541/10000)
[Test]  Epoch: 36	Loss: 0.033061	Acc: 55.3% (5530/10000)
[Test]  Epoch: 37	Loss: 0.033092	Acc: 55.3% (5531/10000)
[Test]  Epoch: 38	Loss: 0.033153	Acc: 55.2% (5525/10000)
[Test]  Epoch: 39	Loss: 0.033153	Acc: 55.3% (5526/10000)
[Test]  Epoch: 40	Loss: 0.033178	Acc: 55.2% (5524/10000)
[Test]  Epoch: 41	Loss: 0.033129	Acc: 55.4% (5535/10000)
[Test]  Epoch: 42	Loss: 0.033211	Acc: 55.3% (5526/10000)
[Test]  Epoch: 43	Loss: 0.033197	Acc: 55.3% (5531/10000)
[Test]  Epoch: 44	Loss: 0.033203	Acc: 55.5% (5547/10000)
[Test]  Epoch: 45	Loss: 0.033209	Acc: 55.3% (5534/10000)
[Test]  Epoch: 46	Loss: 0.033270	Acc: 55.2% (5520/10000)
[Test]  Epoch: 47	Loss: 0.033202	Acc: 55.4% (5542/10000)
[Test]  Epoch: 48	Loss: 0.033199	Acc: 55.3% (5533/10000)
[Test]  Epoch: 49	Loss: 0.033260	Acc: 55.2% (5522/10000)
[Test]  Epoch: 50	Loss: 0.033348	Acc: 55.4% (5542/10000)
[Test]  Epoch: 51	Loss: 0.033330	Acc: 55.6% (5565/10000)
[Test]  Epoch: 52	Loss: 0.033293	Acc: 55.4% (5535/10000)
[Test]  Epoch: 53	Loss: 0.033356	Acc: 55.4% (5542/10000)
[Test]  Epoch: 54	Loss: 0.033354	Acc: 55.4% (5544/10000)
[Test]  Epoch: 55	Loss: 0.033344	Acc: 55.5% (5547/10000)
[Test]  Epoch: 56	Loss: 0.033338	Acc: 55.6% (5559/10000)
[Test]  Epoch: 57	Loss: 0.033347	Acc: 55.4% (5543/10000)
[Test]  Epoch: 58	Loss: 0.033341	Acc: 55.5% (5548/10000)
[Test]  Epoch: 59	Loss: 0.033415	Acc: 55.5% (5551/10000)
[Test]  Epoch: 60	Loss: 0.033484	Acc: 55.4% (5538/10000)
[Test]  Epoch: 61	Loss: 0.033492	Acc: 55.5% (5550/10000)
[Test]  Epoch: 62	Loss: 0.033437	Acc: 55.5% (5549/10000)
[Test]  Epoch: 63	Loss: 0.033411	Acc: 55.4% (5540/10000)
[Test]  Epoch: 64	Loss: 0.033433	Acc: 55.4% (5540/10000)
[Test]  Epoch: 65	Loss: 0.033422	Acc: 55.4% (5541/10000)
[Test]  Epoch: 66	Loss: 0.033451	Acc: 55.6% (5558/10000)
[Test]  Epoch: 67	Loss: 0.033412	Acc: 55.5% (5555/10000)
[Test]  Epoch: 68	Loss: 0.033469	Acc: 55.5% (5550/10000)
[Test]  Epoch: 69	Loss: 0.033450	Acc: 55.6% (5562/10000)
[Test]  Epoch: 70	Loss: 0.033388	Acc: 55.5% (5554/10000)
[Test]  Epoch: 71	Loss: 0.033479	Acc: 55.3% (5529/10000)
[Test]  Epoch: 72	Loss: 0.033445	Acc: 55.4% (5539/10000)
[Test]  Epoch: 73	Loss: 0.033374	Acc: 55.5% (5545/10000)
[Test]  Epoch: 74	Loss: 0.033355	Acc: 55.5% (5551/10000)
[Test]  Epoch: 75	Loss: 0.033385	Acc: 55.5% (5549/10000)
[Test]  Epoch: 76	Loss: 0.033342	Acc: 55.6% (5556/10000)
[Test]  Epoch: 77	Loss: 0.033388	Acc: 55.4% (5539/10000)
[Test]  Epoch: 78	Loss: 0.033437	Acc: 55.3% (5534/10000)
[Test]  Epoch: 79	Loss: 0.033427	Acc: 55.4% (5543/10000)
[Test]  Epoch: 80	Loss: 0.033441	Acc: 55.4% (5538/10000)
[Test]  Epoch: 81	Loss: 0.033478	Acc: 55.4% (5537/10000)
[Test]  Epoch: 82	Loss: 0.033496	Acc: 55.4% (5536/10000)
[Test]  Epoch: 83	Loss: 0.033441	Acc: 55.4% (5536/10000)
[Test]  Epoch: 84	Loss: 0.033480	Acc: 55.5% (5547/10000)
[Test]  Epoch: 85	Loss: 0.033443	Acc: 55.5% (5549/10000)
[Test]  Epoch: 86	Loss: 0.033498	Acc: 55.3% (5527/10000)
[Test]  Epoch: 87	Loss: 0.033444	Acc: 55.4% (5541/10000)
[Test]  Epoch: 88	Loss: 0.033442	Acc: 55.4% (5540/10000)
[Test]  Epoch: 89	Loss: 0.033473	Acc: 55.5% (5551/10000)
[Test]  Epoch: 90	Loss: 0.033479	Acc: 55.5% (5548/10000)
[Test]  Epoch: 91	Loss: 0.033468	Acc: 55.6% (5557/10000)
[Test]  Epoch: 92	Loss: 0.033379	Acc: 55.4% (5541/10000)
[Test]  Epoch: 93	Loss: 0.033459	Acc: 55.5% (5548/10000)
[Test]  Epoch: 94	Loss: 0.033441	Acc: 55.5% (5547/10000)
[Test]  Epoch: 95	Loss: 0.033441	Acc: 55.5% (5548/10000)
[Test]  Epoch: 96	Loss: 0.033424	Acc: 55.5% (5553/10000)
[Test]  Epoch: 97	Loss: 0.033462	Acc: 55.4% (5536/10000)
[Test]  Epoch: 98	Loss: 0.033451	Acc: 55.5% (5548/10000)
[Test]  Epoch: 99	Loss: 0.033437	Acc: 55.5% (5546/10000)
[Test]  Epoch: 100	Loss: 0.033429	Acc: 55.5% (5549/10000)
===========finish==========
['2024-08-19', '18:01:37.896022', '100', 'test', '0.0334293455183506', '55.49', '55.65']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042326	Acc: 40.1% (4015/10000)
[Test]  Epoch: 2	Loss: 0.034492	Acc: 49.4% (4936/10000)
[Test]  Epoch: 3	Loss: 0.033080	Acc: 51.4% (5140/10000)
[Test]  Epoch: 4	Loss: 0.032488	Acc: 52.4% (5238/10000)
[Test]  Epoch: 5	Loss: 0.032551	Acc: 52.7% (5273/10000)
[Test]  Epoch: 6	Loss: 0.032498	Acc: 53.3% (5330/10000)
[Test]  Epoch: 7	Loss: 0.032434	Acc: 53.3% (5327/10000)
[Test]  Epoch: 8	Loss: 0.032472	Acc: 53.4% (5335/10000)
[Test]  Epoch: 9	Loss: 0.032508	Acc: 53.6% (5358/10000)
[Test]  Epoch: 10	Loss: 0.032553	Acc: 53.7% (5369/10000)
[Test]  Epoch: 11	Loss: 0.032456	Acc: 53.8% (5378/10000)
[Test]  Epoch: 12	Loss: 0.032439	Acc: 53.7% (5372/10000)
[Test]  Epoch: 13	Loss: 0.032520	Acc: 54.0% (5395/10000)
[Test]  Epoch: 14	Loss: 0.032561	Acc: 53.9% (5388/10000)
[Test]  Epoch: 15	Loss: 0.032614	Acc: 53.9% (5390/10000)
[Test]  Epoch: 16	Loss: 0.032578	Acc: 53.8% (5378/10000)
[Test]  Epoch: 17	Loss: 0.032575	Acc: 54.1% (5407/10000)
[Test]  Epoch: 18	Loss: 0.032617	Acc: 54.1% (5407/10000)
[Test]  Epoch: 19	Loss: 0.032651	Acc: 54.2% (5423/10000)
[Test]  Epoch: 20	Loss: 0.032702	Acc: 54.1% (5414/10000)
[Test]  Epoch: 21	Loss: 0.032601	Acc: 54.1% (5410/10000)
[Test]  Epoch: 22	Loss: 0.032765	Acc: 54.2% (5425/10000)
[Test]  Epoch: 23	Loss: 0.032736	Acc: 54.1% (5409/10000)
[Test]  Epoch: 24	Loss: 0.032814	Acc: 54.1% (5408/10000)
[Test]  Epoch: 25	Loss: 0.032739	Acc: 54.4% (5437/10000)
[Test]  Epoch: 26	Loss: 0.032718	Acc: 54.2% (5424/10000)
[Test]  Epoch: 27	Loss: 0.032764	Acc: 54.4% (5438/10000)
[Test]  Epoch: 28	Loss: 0.032853	Acc: 54.2% (5418/10000)
[Test]  Epoch: 29	Loss: 0.032777	Acc: 54.4% (5438/10000)
[Test]  Epoch: 30	Loss: 0.032810	Acc: 54.3% (5433/10000)
[Test]  Epoch: 31	Loss: 0.033014	Acc: 54.2% (5423/10000)
[Test]  Epoch: 32	Loss: 0.032955	Acc: 54.2% (5417/10000)
[Test]  Epoch: 33	Loss: 0.033008	Acc: 53.9% (5389/10000)
[Test]  Epoch: 34	Loss: 0.032884	Acc: 54.1% (5414/10000)
[Test]  Epoch: 35	Loss: 0.032919	Acc: 54.2% (5424/10000)
[Test]  Epoch: 36	Loss: 0.033003	Acc: 54.3% (5427/10000)
[Test]  Epoch: 37	Loss: 0.033008	Acc: 54.3% (5434/10000)
[Test]  Epoch: 38	Loss: 0.033086	Acc: 54.4% (5436/10000)
[Test]  Epoch: 39	Loss: 0.033079	Acc: 54.1% (5414/10000)
[Test]  Epoch: 40	Loss: 0.033132	Acc: 54.1% (5409/10000)
[Test]  Epoch: 41	Loss: 0.033038	Acc: 54.4% (5439/10000)
[Test]  Epoch: 42	Loss: 0.033146	Acc: 54.2% (5421/10000)
[Test]  Epoch: 43	Loss: 0.033138	Acc: 54.3% (5434/10000)
[Test]  Epoch: 44	Loss: 0.033120	Acc: 54.3% (5434/10000)
[Test]  Epoch: 45	Loss: 0.033124	Acc: 54.3% (5426/10000)
[Test]  Epoch: 46	Loss: 0.033189	Acc: 54.4% (5442/10000)
[Test]  Epoch: 47	Loss: 0.033174	Acc: 54.5% (5451/10000)
[Test]  Epoch: 48	Loss: 0.033148	Acc: 54.2% (5425/10000)
[Test]  Epoch: 49	Loss: 0.033264	Acc: 54.3% (5426/10000)
[Test]  Epoch: 50	Loss: 0.033322	Acc: 54.5% (5455/10000)
[Test]  Epoch: 51	Loss: 0.033256	Acc: 54.5% (5451/10000)
[Test]  Epoch: 52	Loss: 0.033265	Acc: 54.2% (5416/10000)
[Test]  Epoch: 53	Loss: 0.033327	Acc: 54.4% (5438/10000)
[Test]  Epoch: 54	Loss: 0.033312	Acc: 54.4% (5440/10000)
[Test]  Epoch: 55	Loss: 0.033306	Acc: 54.5% (5455/10000)
[Test]  Epoch: 56	Loss: 0.033325	Acc: 54.4% (5436/10000)
[Test]  Epoch: 57	Loss: 0.033310	Acc: 54.5% (5446/10000)
[Test]  Epoch: 58	Loss: 0.033314	Acc: 54.6% (5460/10000)
[Test]  Epoch: 59	Loss: 0.033427	Acc: 54.3% (5431/10000)
[Test]  Epoch: 60	Loss: 0.033454	Acc: 54.4% (5436/10000)
[Test]  Epoch: 61	Loss: 0.033455	Acc: 54.4% (5436/10000)
[Test]  Epoch: 62	Loss: 0.033421	Acc: 54.4% (5443/10000)
[Test]  Epoch: 63	Loss: 0.033392	Acc: 54.5% (5455/10000)
[Test]  Epoch: 64	Loss: 0.033410	Acc: 54.5% (5454/10000)
[Test]  Epoch: 65	Loss: 0.033399	Acc: 54.4% (5436/10000)
[Test]  Epoch: 66	Loss: 0.033428	Acc: 54.5% (5448/10000)
[Test]  Epoch: 67	Loss: 0.033420	Acc: 54.4% (5441/10000)
[Test]  Epoch: 68	Loss: 0.033495	Acc: 54.3% (5426/10000)
[Test]  Epoch: 69	Loss: 0.033452	Acc: 54.5% (5450/10000)
[Test]  Epoch: 70	Loss: 0.033362	Acc: 54.5% (5452/10000)
[Test]  Epoch: 71	Loss: 0.033483	Acc: 54.3% (5427/10000)
[Test]  Epoch: 72	Loss: 0.033438	Acc: 54.5% (5451/10000)
[Test]  Epoch: 73	Loss: 0.033349	Acc: 54.7% (5469/10000)
[Test]  Epoch: 74	Loss: 0.033334	Acc: 54.4% (5441/10000)
[Test]  Epoch: 75	Loss: 0.033381	Acc: 54.6% (5458/10000)
[Test]  Epoch: 76	Loss: 0.033306	Acc: 54.5% (5453/10000)
[Test]  Epoch: 77	Loss: 0.033408	Acc: 54.5% (5455/10000)
[Test]  Epoch: 78	Loss: 0.033409	Acc: 54.3% (5434/10000)
[Test]  Epoch: 79	Loss: 0.033416	Acc: 54.5% (5445/10000)
[Test]  Epoch: 80	Loss: 0.033431	Acc: 54.2% (5424/10000)
[Test]  Epoch: 81	Loss: 0.033462	Acc: 54.4% (5437/10000)
[Test]  Epoch: 82	Loss: 0.033503	Acc: 54.1% (5411/10000)
[Test]  Epoch: 83	Loss: 0.033402	Acc: 54.5% (5451/10000)
[Test]  Epoch: 84	Loss: 0.033483	Acc: 54.4% (5438/10000)
[Test]  Epoch: 85	Loss: 0.033452	Acc: 54.4% (5441/10000)
[Test]  Epoch: 86	Loss: 0.033478	Acc: 54.3% (5431/10000)
[Test]  Epoch: 87	Loss: 0.033449	Acc: 54.4% (5443/10000)
[Test]  Epoch: 88	Loss: 0.033431	Acc: 54.4% (5443/10000)
[Test]  Epoch: 89	Loss: 0.033436	Acc: 54.5% (5448/10000)
[Test]  Epoch: 90	Loss: 0.033472	Acc: 54.5% (5451/10000)
[Test]  Epoch: 91	Loss: 0.033455	Acc: 54.5% (5447/10000)
[Test]  Epoch: 92	Loss: 0.033387	Acc: 54.5% (5448/10000)
[Test]  Epoch: 93	Loss: 0.033460	Acc: 54.3% (5431/10000)
[Test]  Epoch: 94	Loss: 0.033475	Acc: 54.4% (5436/10000)
[Test]  Epoch: 95	Loss: 0.033440	Acc: 54.2% (5425/10000)
[Test]  Epoch: 96	Loss: 0.033389	Acc: 54.4% (5442/10000)
[Test]  Epoch: 97	Loss: 0.033418	Acc: 54.4% (5438/10000)
[Test]  Epoch: 98	Loss: 0.033429	Acc: 54.3% (5434/10000)
[Test]  Epoch: 99	Loss: 0.033474	Acc: 54.2% (5425/10000)
[Test]  Epoch: 100	Loss: 0.033451	Acc: 54.4% (5436/10000)
===========finish==========
['2024-08-19', '18:10:18.953999', '100', 'test', '0.03345105406045914', '54.36', '54.69']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.046560	Acc: 36.5% (3647/10000)
[Test]  Epoch: 2	Loss: 0.036994	Acc: 47.0% (4702/10000)
[Test]  Epoch: 3	Loss: 0.034430	Acc: 50.7% (5067/10000)
[Test]  Epoch: 4	Loss: 0.033862	Acc: 51.9% (5192/10000)
[Test]  Epoch: 5	Loss: 0.034074	Acc: 51.8% (5183/10000)
[Test]  Epoch: 6	Loss: 0.033853	Acc: 52.3% (5227/10000)
[Test]  Epoch: 7	Loss: 0.033763	Acc: 52.6% (5263/10000)
[Test]  Epoch: 8	Loss: 0.033797	Acc: 52.6% (5257/10000)
[Test]  Epoch: 9	Loss: 0.033796	Acc: 52.5% (5254/10000)
[Test]  Epoch: 10	Loss: 0.033906	Acc: 52.8% (5281/10000)
[Test]  Epoch: 11	Loss: 0.033854	Acc: 52.8% (5277/10000)
[Test]  Epoch: 12	Loss: 0.033764	Acc: 53.0% (5304/10000)
[Test]  Epoch: 13	Loss: 0.033876	Acc: 52.9% (5294/10000)
[Test]  Epoch: 14	Loss: 0.033883	Acc: 52.8% (5284/10000)
[Test]  Epoch: 15	Loss: 0.033974	Acc: 52.7% (5272/10000)
[Test]  Epoch: 16	Loss: 0.034005	Acc: 52.9% (5290/10000)
[Test]  Epoch: 17	Loss: 0.033904	Acc: 53.1% (5314/10000)
[Test]  Epoch: 18	Loss: 0.034005	Acc: 53.1% (5313/10000)
[Test]  Epoch: 19	Loss: 0.033950	Acc: 53.1% (5309/10000)
[Test]  Epoch: 20	Loss: 0.034096	Acc: 53.0% (5298/10000)
[Test]  Epoch: 21	Loss: 0.034028	Acc: 53.3% (5328/10000)
[Test]  Epoch: 22	Loss: 0.034152	Acc: 53.2% (5321/10000)
[Test]  Epoch: 23	Loss: 0.034092	Acc: 53.1% (5310/10000)
[Test]  Epoch: 24	Loss: 0.034210	Acc: 53.3% (5333/10000)
[Test]  Epoch: 25	Loss: 0.034164	Acc: 53.1% (5311/10000)
[Test]  Epoch: 26	Loss: 0.034063	Acc: 53.7% (5371/10000)
[Test]  Epoch: 27	Loss: 0.034091	Acc: 53.3% (5334/10000)
[Test]  Epoch: 28	Loss: 0.034227	Acc: 53.4% (5336/10000)
[Test]  Epoch: 29	Loss: 0.034161	Acc: 53.6% (5358/10000)
[Test]  Epoch: 30	Loss: 0.034258	Acc: 53.5% (5349/10000)
[Test]  Epoch: 31	Loss: 0.034383	Acc: 53.3% (5328/10000)
[Test]  Epoch: 32	Loss: 0.034368	Acc: 53.4% (5341/10000)
[Test]  Epoch: 33	Loss: 0.034357	Acc: 53.5% (5351/10000)
[Test]  Epoch: 34	Loss: 0.034256	Acc: 53.5% (5345/10000)
[Test]  Epoch: 35	Loss: 0.034278	Acc: 53.6% (5361/10000)
[Test]  Epoch: 36	Loss: 0.034425	Acc: 53.5% (5348/10000)
[Test]  Epoch: 37	Loss: 0.034385	Acc: 53.6% (5362/10000)
[Test]  Epoch: 38	Loss: 0.034502	Acc: 53.6% (5364/10000)
[Test]  Epoch: 39	Loss: 0.034437	Acc: 53.3% (5329/10000)
[Test]  Epoch: 40	Loss: 0.034536	Acc: 53.4% (5343/10000)
[Test]  Epoch: 41	Loss: 0.034420	Acc: 53.4% (5342/10000)
[Test]  Epoch: 42	Loss: 0.034473	Acc: 53.5% (5348/10000)
[Test]  Epoch: 43	Loss: 0.034543	Acc: 53.6% (5360/10000)
[Test]  Epoch: 44	Loss: 0.034553	Acc: 53.3% (5332/10000)
[Test]  Epoch: 45	Loss: 0.034502	Acc: 53.5% (5346/10000)
[Test]  Epoch: 46	Loss: 0.034593	Acc: 53.3% (5328/10000)
[Test]  Epoch: 47	Loss: 0.034549	Acc: 53.7% (5374/10000)
[Test]  Epoch: 48	Loss: 0.034534	Acc: 53.5% (5346/10000)
[Test]  Epoch: 49	Loss: 0.034623	Acc: 53.3% (5327/10000)
[Test]  Epoch: 50	Loss: 0.034722	Acc: 53.5% (5355/10000)
[Test]  Epoch: 51	Loss: 0.034703	Acc: 53.6% (5356/10000)
[Test]  Epoch: 52	Loss: 0.034615	Acc: 53.2% (5323/10000)
[Test]  Epoch: 53	Loss: 0.034685	Acc: 53.4% (5341/10000)
[Test]  Epoch: 54	Loss: 0.034726	Acc: 53.3% (5332/10000)
[Test]  Epoch: 55	Loss: 0.034670	Acc: 53.5% (5351/10000)
[Test]  Epoch: 56	Loss: 0.034737	Acc: 53.5% (5349/10000)
[Test]  Epoch: 57	Loss: 0.034676	Acc: 53.7% (5368/10000)
[Test]  Epoch: 58	Loss: 0.034695	Acc: 53.5% (5355/10000)
[Test]  Epoch: 59	Loss: 0.034840	Acc: 53.3% (5329/10000)
[Test]  Epoch: 60	Loss: 0.034820	Acc: 53.5% (5348/10000)
[Test]  Epoch: 61	Loss: 0.034828	Acc: 53.4% (5337/10000)
[Test]  Epoch: 62	Loss: 0.034808	Acc: 53.5% (5353/10000)
[Test]  Epoch: 63	Loss: 0.034805	Acc: 53.5% (5350/10000)
[Test]  Epoch: 64	Loss: 0.034813	Acc: 53.4% (5342/10000)
[Test]  Epoch: 65	Loss: 0.034764	Acc: 53.5% (5345/10000)
[Test]  Epoch: 66	Loss: 0.034820	Acc: 53.3% (5332/10000)
[Test]  Epoch: 67	Loss: 0.034796	Acc: 53.4% (5342/10000)
[Test]  Epoch: 68	Loss: 0.034823	Acc: 53.4% (5339/10000)
[Test]  Epoch: 69	Loss: 0.034868	Acc: 53.3% (5333/10000)
[Test]  Epoch: 70	Loss: 0.034789	Acc: 53.4% (5343/10000)
[Test]  Epoch: 71	Loss: 0.034907	Acc: 53.3% (5328/10000)
[Test]  Epoch: 72	Loss: 0.034864	Acc: 53.4% (5338/10000)
[Test]  Epoch: 73	Loss: 0.034752	Acc: 53.5% (5347/10000)
[Test]  Epoch: 74	Loss: 0.034745	Acc: 53.5% (5353/10000)
[Test]  Epoch: 75	Loss: 0.034810	Acc: 53.5% (5353/10000)
[Test]  Epoch: 76	Loss: 0.034716	Acc: 53.7% (5368/10000)
[Test]  Epoch: 77	Loss: 0.034803	Acc: 53.6% (5365/10000)
[Test]  Epoch: 78	Loss: 0.034852	Acc: 53.5% (5355/10000)
[Test]  Epoch: 79	Loss: 0.034790	Acc: 53.6% (5360/10000)
[Test]  Epoch: 80	Loss: 0.034839	Acc: 53.2% (5325/10000)
[Test]  Epoch: 81	Loss: 0.034847	Acc: 53.3% (5327/10000)
[Test]  Epoch: 82	Loss: 0.034884	Acc: 53.3% (5331/10000)
[Test]  Epoch: 83	Loss: 0.034823	Acc: 53.4% (5335/10000)
[Test]  Epoch: 84	Loss: 0.034826	Acc: 53.5% (5347/10000)
[Test]  Epoch: 85	Loss: 0.034826	Acc: 53.4% (5344/10000)
[Test]  Epoch: 86	Loss: 0.034869	Acc: 53.3% (5328/10000)
[Test]  Epoch: 87	Loss: 0.034852	Acc: 53.5% (5353/10000)
[Test]  Epoch: 88	Loss: 0.034842	Acc: 53.4% (5341/10000)
[Test]  Epoch: 89	Loss: 0.034897	Acc: 53.4% (5338/10000)
[Test]  Epoch: 90	Loss: 0.034890	Acc: 53.5% (5351/10000)
[Test]  Epoch: 91	Loss: 0.034877	Acc: 53.5% (5351/10000)
[Test]  Epoch: 92	Loss: 0.034773	Acc: 53.6% (5357/10000)
[Test]  Epoch: 93	Loss: 0.034835	Acc: 53.5% (5349/10000)
[Test]  Epoch: 94	Loss: 0.034835	Acc: 53.5% (5345/10000)
[Test]  Epoch: 95	Loss: 0.034835	Acc: 53.4% (5340/10000)
[Test]  Epoch: 96	Loss: 0.034809	Acc: 53.7% (5370/10000)
[Test]  Epoch: 97	Loss: 0.034827	Acc: 53.3% (5333/10000)
[Test]  Epoch: 98	Loss: 0.034825	Acc: 53.5% (5345/10000)
[Test]  Epoch: 99	Loss: 0.034851	Acc: 53.4% (5341/10000)
[Test]  Epoch: 100	Loss: 0.034823	Acc: 53.5% (5349/10000)
===========finish==========
['2024-08-19', '18:17:30.154082', '100', 'test', '0.034823260700702664', '53.49', '53.74']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight', 'layer2.2.bn1.weight', 'layer3.2.bn2.weight', 'layer3.5.bn3.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.049542	Acc: 31.2% (3121/10000)
[Test]  Epoch: 2	Loss: 0.039386	Acc: 44.0% (4398/10000)
[Test]  Epoch: 3	Loss: 0.036066	Acc: 48.3% (4832/10000)
[Test]  Epoch: 4	Loss: 0.035402	Acc: 50.1% (5007/10000)
[Test]  Epoch: 5	Loss: 0.035596	Acc: 50.6% (5058/10000)
[Test]  Epoch: 6	Loss: 0.035232	Acc: 50.8% (5075/10000)
[Test]  Epoch: 7	Loss: 0.035246	Acc: 51.1% (5113/10000)
[Test]  Epoch: 8	Loss: 0.035146	Acc: 51.3% (5132/10000)
[Test]  Epoch: 9	Loss: 0.035193	Acc: 51.4% (5137/10000)
[Test]  Epoch: 10	Loss: 0.035110	Acc: 51.6% (5159/10000)
[Test]  Epoch: 11	Loss: 0.035241	Acc: 51.6% (5158/10000)
[Test]  Epoch: 12	Loss: 0.035063	Acc: 51.9% (5194/10000)
[Test]  Epoch: 13	Loss: 0.035250	Acc: 52.0% (5203/10000)
[Test]  Epoch: 14	Loss: 0.035113	Acc: 52.1% (5211/10000)
[Test]  Epoch: 15	Loss: 0.035400	Acc: 51.8% (5184/10000)
[Test]  Epoch: 16	Loss: 0.035227	Acc: 52.0% (5204/10000)
[Test]  Epoch: 17	Loss: 0.035192	Acc: 51.9% (5190/10000)
[Test]  Epoch: 18	Loss: 0.035208	Acc: 52.4% (5236/10000)
[Test]  Epoch: 19	Loss: 0.035249	Acc: 52.2% (5217/10000)
[Test]  Epoch: 20	Loss: 0.035291	Acc: 52.2% (5225/10000)
[Test]  Epoch: 21	Loss: 0.035232	Acc: 52.5% (5255/10000)
[Test]  Epoch: 22	Loss: 0.035331	Acc: 52.4% (5241/10000)
[Test]  Epoch: 23	Loss: 0.035266	Acc: 52.3% (5232/10000)
[Test]  Epoch: 24	Loss: 0.035390	Acc: 52.6% (5256/10000)
[Test]  Epoch: 25	Loss: 0.035386	Acc: 52.5% (5246/10000)
[Test]  Epoch: 26	Loss: 0.035337	Acc: 52.6% (5264/10000)
[Test]  Epoch: 27	Loss: 0.035257	Acc: 52.5% (5254/10000)
[Test]  Epoch: 28	Loss: 0.035410	Acc: 52.4% (5239/10000)
[Test]  Epoch: 29	Loss: 0.035313	Acc: 52.6% (5257/10000)
[Test]  Epoch: 30	Loss: 0.035441	Acc: 52.7% (5266/10000)
[Test]  Epoch: 31	Loss: 0.035587	Acc: 52.4% (5238/10000)
[Test]  Epoch: 32	Loss: 0.035591	Acc: 52.4% (5241/10000)
[Test]  Epoch: 33	Loss: 0.035522	Acc: 52.7% (5268/10000)
[Test]  Epoch: 34	Loss: 0.035406	Acc: 52.6% (5261/10000)
[Test]  Epoch: 35	Loss: 0.035391	Acc: 52.8% (5283/10000)
[Test]  Epoch: 36	Loss: 0.035505	Acc: 52.6% (5265/10000)
[Test]  Epoch: 37	Loss: 0.035524	Acc: 52.6% (5257/10000)
[Test]  Epoch: 38	Loss: 0.035625	Acc: 52.5% (5253/10000)
[Test]  Epoch: 39	Loss: 0.035632	Acc: 52.5% (5247/10000)
[Test]  Epoch: 40	Loss: 0.035658	Acc: 52.6% (5258/10000)
[Test]  Epoch: 41	Loss: 0.035524	Acc: 52.6% (5259/10000)
[Test]  Epoch: 42	Loss: 0.035608	Acc: 52.5% (5253/10000)
[Test]  Epoch: 43	Loss: 0.035674	Acc: 52.6% (5263/10000)
[Test]  Epoch: 44	Loss: 0.035641	Acc: 52.6% (5263/10000)
[Test]  Epoch: 45	Loss: 0.035615	Acc: 52.6% (5261/10000)
[Test]  Epoch: 46	Loss: 0.035703	Acc: 52.4% (5241/10000)
[Test]  Epoch: 47	Loss: 0.035675	Acc: 52.8% (5277/10000)
[Test]  Epoch: 48	Loss: 0.035649	Acc: 52.5% (5250/10000)
[Test]  Epoch: 49	Loss: 0.035740	Acc: 52.7% (5267/10000)
[Test]  Epoch: 50	Loss: 0.035832	Acc: 52.6% (5259/10000)
[Test]  Epoch: 51	Loss: 0.035804	Acc: 52.6% (5256/10000)
[Test]  Epoch: 52	Loss: 0.035738	Acc: 52.8% (5277/10000)
[Test]  Epoch: 53	Loss: 0.035800	Acc: 52.4% (5244/10000)
[Test]  Epoch: 54	Loss: 0.035818	Acc: 52.6% (5258/10000)
[Test]  Epoch: 55	Loss: 0.035716	Acc: 52.7% (5270/10000)
[Test]  Epoch: 56	Loss: 0.035802	Acc: 52.8% (5284/10000)
[Test]  Epoch: 57	Loss: 0.035761	Acc: 52.8% (5280/10000)
[Test]  Epoch: 58	Loss: 0.035797	Acc: 52.9% (5293/10000)
[Test]  Epoch: 59	Loss: 0.035912	Acc: 52.6% (5263/10000)
[Test]  Epoch: 60	Loss: 0.035937	Acc: 52.5% (5252/10000)
[Test]  Epoch: 61	Loss: 0.035957	Acc: 52.7% (5274/10000)
[Test]  Epoch: 62	Loss: 0.035919	Acc: 52.6% (5265/10000)
[Test]  Epoch: 63	Loss: 0.035872	Acc: 52.7% (5271/10000)
[Test]  Epoch: 64	Loss: 0.035852	Acc: 52.8% (5277/10000)
[Test]  Epoch: 65	Loss: 0.035862	Acc: 52.6% (5260/10000)
[Test]  Epoch: 66	Loss: 0.035925	Acc: 52.7% (5272/10000)
[Test]  Epoch: 67	Loss: 0.035865	Acc: 52.7% (5269/10000)
[Test]  Epoch: 68	Loss: 0.035906	Acc: 52.5% (5248/10000)
[Test]  Epoch: 69	Loss: 0.035916	Acc: 52.6% (5261/10000)
[Test]  Epoch: 70	Loss: 0.035880	Acc: 52.7% (5273/10000)
[Test]  Epoch: 71	Loss: 0.035951	Acc: 52.5% (5254/10000)
[Test]  Epoch: 72	Loss: 0.035905	Acc: 52.7% (5268/10000)
[Test]  Epoch: 73	Loss: 0.035821	Acc: 52.9% (5291/10000)
[Test]  Epoch: 74	Loss: 0.035844	Acc: 52.8% (5276/10000)
[Test]  Epoch: 75	Loss: 0.035926	Acc: 52.8% (5275/10000)
[Test]  Epoch: 76	Loss: 0.035812	Acc: 52.8% (5280/10000)
[Test]  Epoch: 77	Loss: 0.035869	Acc: 52.7% (5271/10000)
[Test]  Epoch: 78	Loss: 0.035895	Acc: 52.4% (5241/10000)
[Test]  Epoch: 79	Loss: 0.035898	Acc: 52.7% (5273/10000)
[Test]  Epoch: 80	Loss: 0.035940	Acc: 52.7% (5267/10000)
[Test]  Epoch: 81	Loss: 0.035973	Acc: 52.6% (5263/10000)
[Test]  Epoch: 82	Loss: 0.035962	Acc: 52.6% (5257/10000)
[Test]  Epoch: 83	Loss: 0.035902	Acc: 52.6% (5265/10000)
[Test]  Epoch: 84	Loss: 0.035911	Acc: 52.7% (5270/10000)
[Test]  Epoch: 85	Loss: 0.035894	Acc: 52.8% (5278/10000)
[Test]  Epoch: 86	Loss: 0.035943	Acc: 52.5% (5245/10000)
[Test]  Epoch: 87	Loss: 0.035892	Acc: 52.6% (5262/10000)
[Test]  Epoch: 88	Loss: 0.035907	Acc: 52.8% (5281/10000)
[Test]  Epoch: 89	Loss: 0.035909	Acc: 52.6% (5261/10000)
[Test]  Epoch: 90	Loss: 0.035960	Acc: 52.6% (5261/10000)
[Test]  Epoch: 91	Loss: 0.035953	Acc: 52.7% (5269/10000)
[Test]  Epoch: 92	Loss: 0.035849	Acc: 52.8% (5276/10000)
[Test]  Epoch: 93	Loss: 0.035932	Acc: 52.7% (5268/10000)
[Test]  Epoch: 94	Loss: 0.035869	Acc: 52.6% (5264/10000)
[Test]  Epoch: 95	Loss: 0.035873	Acc: 52.6% (5259/10000)
[Test]  Epoch: 96	Loss: 0.035844	Acc: 52.8% (5278/10000)
[Test]  Epoch: 97	Loss: 0.035906	Acc: 52.7% (5268/10000)
[Test]  Epoch: 98	Loss: 0.035915	Acc: 52.7% (5269/10000)
[Test]  Epoch: 99	Loss: 0.035917	Acc: 52.7% (5271/10000)
[Test]  Epoch: 100	Loss: 0.035884	Acc: 52.8% (5278/10000)
===========finish==========
['2024-08-19', '18:24:32.558601', '100', 'test', '0.03588362959623337', '52.78', '52.93']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight', 'layer2.2.bn1.weight', 'layer3.2.bn2.weight', 'layer3.5.bn3.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.1.bn1.weight', 'layer2.3.bn3.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn2.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer4.1.conv2.weight', 'layer3.1.bn3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.055603	Acc: 24.3% (2434/10000)
[Test]  Epoch: 2	Loss: 0.043029	Acc: 39.3% (3931/10000)
[Test]  Epoch: 3	Loss: 0.039387	Acc: 44.0% (4405/10000)
[Test]  Epoch: 4	Loss: 0.038349	Acc: 46.4% (4636/10000)
[Test]  Epoch: 5	Loss: 0.038580	Acc: 47.1% (4711/10000)
[Test]  Epoch: 6	Loss: 0.038079	Acc: 47.9% (4791/10000)
[Test]  Epoch: 7	Loss: 0.037974	Acc: 48.1% (4807/10000)
[Test]  Epoch: 8	Loss: 0.037833	Acc: 48.4% (4838/10000)
[Test]  Epoch: 9	Loss: 0.037838	Acc: 48.5% (4852/10000)
[Test]  Epoch: 10	Loss: 0.037633	Acc: 48.9% (4886/10000)
[Test]  Epoch: 11	Loss: 0.037768	Acc: 49.0% (4895/10000)
[Test]  Epoch: 12	Loss: 0.037491	Acc: 49.2% (4922/10000)
[Test]  Epoch: 13	Loss: 0.037772	Acc: 49.1% (4913/10000)
[Test]  Epoch: 14	Loss: 0.037624	Acc: 49.1% (4914/10000)
[Test]  Epoch: 15	Loss: 0.037638	Acc: 49.1% (4914/10000)
[Test]  Epoch: 16	Loss: 0.037624	Acc: 49.6% (4958/10000)
[Test]  Epoch: 17	Loss: 0.037430	Acc: 49.7% (4972/10000)
[Test]  Epoch: 18	Loss: 0.037558	Acc: 49.7% (4971/10000)
[Test]  Epoch: 19	Loss: 0.037494	Acc: 49.7% (4970/10000)
[Test]  Epoch: 20	Loss: 0.037616	Acc: 49.7% (4974/10000)
[Test]  Epoch: 21	Loss: 0.037485	Acc: 50.0% (5004/10000)
[Test]  Epoch: 22	Loss: 0.037541	Acc: 50.3% (5027/10000)
[Test]  Epoch: 23	Loss: 0.037464	Acc: 50.2% (5018/10000)
[Test]  Epoch: 24	Loss: 0.037675	Acc: 50.1% (5006/10000)
[Test]  Epoch: 25	Loss: 0.037460	Acc: 50.2% (5023/10000)
[Test]  Epoch: 26	Loss: 0.037391	Acc: 50.6% (5059/10000)
[Test]  Epoch: 27	Loss: 0.037358	Acc: 50.5% (5053/10000)
[Test]  Epoch: 28	Loss: 0.037504	Acc: 50.4% (5041/10000)
[Test]  Epoch: 29	Loss: 0.037389	Acc: 50.5% (5047/10000)
[Test]  Epoch: 30	Loss: 0.037450	Acc: 50.4% (5037/10000)
[Test]  Epoch: 31	Loss: 0.037646	Acc: 50.4% (5044/10000)
[Test]  Epoch: 32	Loss: 0.037564	Acc: 50.8% (5081/10000)
[Test]  Epoch: 33	Loss: 0.037584	Acc: 50.5% (5051/10000)
[Test]  Epoch: 34	Loss: 0.037415	Acc: 50.6% (5056/10000)
[Test]  Epoch: 35	Loss: 0.037391	Acc: 50.8% (5078/10000)
[Test]  Epoch: 36	Loss: 0.037557	Acc: 50.6% (5057/10000)
[Test]  Epoch: 37	Loss: 0.037556	Acc: 50.8% (5082/10000)
[Test]  Epoch: 38	Loss: 0.037613	Acc: 50.4% (5041/10000)
[Test]  Epoch: 39	Loss: 0.037553	Acc: 50.9% (5085/10000)
[Test]  Epoch: 40	Loss: 0.037648	Acc: 50.7% (5068/10000)
[Test]  Epoch: 41	Loss: 0.037506	Acc: 50.8% (5077/10000)
[Test]  Epoch: 42	Loss: 0.037589	Acc: 50.8% (5082/10000)
[Test]  Epoch: 43	Loss: 0.037558	Acc: 50.9% (5086/10000)
[Test]  Epoch: 44	Loss: 0.037660	Acc: 50.7% (5067/10000)
[Test]  Epoch: 45	Loss: 0.037609	Acc: 50.7% (5066/10000)
[Test]  Epoch: 46	Loss: 0.037670	Acc: 50.7% (5066/10000)
[Test]  Epoch: 47	Loss: 0.037504	Acc: 51.1% (5114/10000)
[Test]  Epoch: 48	Loss: 0.037554	Acc: 51.0% (5103/10000)
[Test]  Epoch: 49	Loss: 0.037612	Acc: 51.0% (5095/10000)
[Test]  Epoch: 50	Loss: 0.037695	Acc: 50.8% (5078/10000)
[Test]  Epoch: 51	Loss: 0.037671	Acc: 51.0% (5097/10000)
[Test]  Epoch: 52	Loss: 0.037600	Acc: 50.9% (5089/10000)
[Test]  Epoch: 53	Loss: 0.037689	Acc: 50.7% (5071/10000)
[Test]  Epoch: 54	Loss: 0.037725	Acc: 50.8% (5079/10000)
[Test]  Epoch: 55	Loss: 0.037616	Acc: 51.0% (5105/10000)
[Test]  Epoch: 56	Loss: 0.037644	Acc: 51.0% (5096/10000)
[Test]  Epoch: 57	Loss: 0.037654	Acc: 51.0% (5103/10000)
[Test]  Epoch: 58	Loss: 0.037628	Acc: 51.0% (5102/10000)
[Test]  Epoch: 59	Loss: 0.037762	Acc: 50.7% (5072/10000)
[Test]  Epoch: 60	Loss: 0.037750	Acc: 50.9% (5091/10000)
[Test]  Epoch: 61	Loss: 0.037818	Acc: 50.9% (5093/10000)
[Test]  Epoch: 62	Loss: 0.037766	Acc: 50.8% (5079/10000)
[Test]  Epoch: 63	Loss: 0.037737	Acc: 50.7% (5071/10000)
[Test]  Epoch: 64	Loss: 0.037747	Acc: 51.0% (5104/10000)
[Test]  Epoch: 65	Loss: 0.037739	Acc: 50.8% (5082/10000)
[Test]  Epoch: 66	Loss: 0.037748	Acc: 51.0% (5097/10000)
[Test]  Epoch: 67	Loss: 0.037731	Acc: 50.9% (5092/10000)
[Test]  Epoch: 68	Loss: 0.037783	Acc: 51.0% (5095/10000)
[Test]  Epoch: 69	Loss: 0.037759	Acc: 51.1% (5112/10000)
[Test]  Epoch: 70	Loss: 0.037699	Acc: 51.1% (5107/10000)
[Test]  Epoch: 71	Loss: 0.037814	Acc: 50.9% (5086/10000)
[Test]  Epoch: 72	Loss: 0.037782	Acc: 51.1% (5110/10000)
[Test]  Epoch: 73	Loss: 0.037695	Acc: 51.0% (5101/10000)
[Test]  Epoch: 74	Loss: 0.037669	Acc: 51.1% (5108/10000)
[Test]  Epoch: 75	Loss: 0.037764	Acc: 51.0% (5102/10000)
[Test]  Epoch: 76	Loss: 0.037676	Acc: 51.0% (5098/10000)
[Test]  Epoch: 77	Loss: 0.037701	Acc: 51.0% (5104/10000)
[Test]  Epoch: 78	Loss: 0.037762	Acc: 50.8% (5083/10000)
[Test]  Epoch: 79	Loss: 0.037727	Acc: 51.2% (5116/10000)
[Test]  Epoch: 80	Loss: 0.037811	Acc: 50.8% (5079/10000)
[Test]  Epoch: 81	Loss: 0.037765	Acc: 51.0% (5095/10000)
[Test]  Epoch: 82	Loss: 0.037790	Acc: 50.8% (5079/10000)
[Test]  Epoch: 83	Loss: 0.037724	Acc: 50.9% (5087/10000)
[Test]  Epoch: 84	Loss: 0.037751	Acc: 51.1% (5113/10000)
[Test]  Epoch: 85	Loss: 0.037748	Acc: 51.0% (5098/10000)
[Test]  Epoch: 86	Loss: 0.037772	Acc: 50.8% (5075/10000)
[Test]  Epoch: 87	Loss: 0.037772	Acc: 51.0% (5100/10000)
[Test]  Epoch: 88	Loss: 0.037732	Acc: 51.1% (5107/10000)
[Test]  Epoch: 89	Loss: 0.037771	Acc: 51.0% (5101/10000)
[Test]  Epoch: 90	Loss: 0.037761	Acc: 51.0% (5100/10000)
[Test]  Epoch: 91	Loss: 0.037788	Acc: 51.2% (5116/10000)
[Test]  Epoch: 92	Loss: 0.037699	Acc: 51.1% (5115/10000)
[Test]  Epoch: 93	Loss: 0.037761	Acc: 50.8% (5083/10000)
[Test]  Epoch: 94	Loss: 0.037772	Acc: 51.0% (5100/10000)
[Test]  Epoch: 95	Loss: 0.037752	Acc: 51.1% (5108/10000)
[Test]  Epoch: 96	Loss: 0.037686	Acc: 51.2% (5116/10000)
[Test]  Epoch: 97	Loss: 0.037759	Acc: 50.9% (5094/10000)
[Test]  Epoch: 98	Loss: 0.037749	Acc: 50.8% (5081/10000)
[Test]  Epoch: 99	Loss: 0.037714	Acc: 51.0% (5099/10000)
[Test]  Epoch: 100	Loss: 0.037718	Acc: 51.1% (5111/10000)
===========finish==========
['2024-08-19', '18:31:42.394722', '100', 'test', '0.03771772116422653', '51.11', '51.16']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight', 'layer2.2.bn1.weight', 'layer3.2.bn2.weight', 'layer3.5.bn3.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.1.bn1.weight', 'layer2.3.bn3.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn2.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer4.1.conv2.weight', 'layer3.1.bn3.weight', 'layer4.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn2.weight', 'layer4.2.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.bn1.weight', 'layer1.1.conv1.weight', 'layer3.0.bn2.weight', 'layer1.1.conv3.weight', 'layer3.4.conv3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.067462	Acc: 19.0% (1897/10000)
[Test]  Epoch: 2	Loss: 0.044976	Acc: 35.4% (3538/10000)
[Test]  Epoch: 3	Loss: 0.040681	Acc: 39.9% (3991/10000)
[Test]  Epoch: 4	Loss: 0.039185	Acc: 43.1% (4313/10000)
[Test]  Epoch: 5	Loss: 0.039157	Acc: 43.5% (4355/10000)
[Test]  Epoch: 6	Loss: 0.038458	Acc: 44.8% (4475/10000)
[Test]  Epoch: 7	Loss: 0.038187	Acc: 45.1% (4514/10000)
[Test]  Epoch: 8	Loss: 0.038087	Acc: 45.4% (4543/10000)
[Test]  Epoch: 9	Loss: 0.038315	Acc: 45.5% (4550/10000)
[Test]  Epoch: 10	Loss: 0.038011	Acc: 45.9% (4592/10000)
[Test]  Epoch: 11	Loss: 0.038071	Acc: 46.3% (4634/10000)
[Test]  Epoch: 12	Loss: 0.037981	Acc: 46.5% (4651/10000)
[Test]  Epoch: 13	Loss: 0.037910	Acc: 46.6% (4664/10000)
[Test]  Epoch: 14	Loss: 0.037943	Acc: 46.5% (4651/10000)
[Test]  Epoch: 15	Loss: 0.038028	Acc: 46.3% (4633/10000)
[Test]  Epoch: 16	Loss: 0.037841	Acc: 46.7% (4669/10000)
[Test]  Epoch: 17	Loss: 0.037852	Acc: 46.9% (4689/10000)
[Test]  Epoch: 18	Loss: 0.037834	Acc: 47.2% (4718/10000)
[Test]  Epoch: 19	Loss: 0.037826	Acc: 47.1% (4706/10000)
[Test]  Epoch: 20	Loss: 0.037990	Acc: 47.4% (4736/10000)
[Test]  Epoch: 21	Loss: 0.037718	Acc: 47.5% (4750/10000)
[Test]  Epoch: 22	Loss: 0.037822	Acc: 47.6% (4760/10000)
[Test]  Epoch: 23	Loss: 0.037795	Acc: 47.6% (4757/10000)
[Test]  Epoch: 24	Loss: 0.037845	Acc: 47.6% (4756/10000)
[Test]  Epoch: 25	Loss: 0.037877	Acc: 47.6% (4761/10000)
[Test]  Epoch: 26	Loss: 0.037706	Acc: 47.7% (4772/10000)
[Test]  Epoch: 27	Loss: 0.037786	Acc: 47.4% (4738/10000)
[Test]  Epoch: 28	Loss: 0.037807	Acc: 47.7% (4766/10000)
[Test]  Epoch: 29	Loss: 0.037800	Acc: 47.7% (4769/10000)
[Test]  Epoch: 30	Loss: 0.037800	Acc: 47.7% (4768/10000)
[Test]  Epoch: 31	Loss: 0.037948	Acc: 47.7% (4771/10000)
[Test]  Epoch: 32	Loss: 0.037943	Acc: 48.0% (4799/10000)
[Test]  Epoch: 33	Loss: 0.037904	Acc: 47.7% (4772/10000)
[Test]  Epoch: 34	Loss: 0.037779	Acc: 47.9% (4789/10000)
[Test]  Epoch: 35	Loss: 0.037834	Acc: 47.9% (4792/10000)
[Test]  Epoch: 36	Loss: 0.037853	Acc: 48.0% (4805/10000)
[Test]  Epoch: 37	Loss: 0.037983	Acc: 48.0% (4799/10000)
[Test]  Epoch: 38	Loss: 0.037967	Acc: 47.9% (4792/10000)
[Test]  Epoch: 39	Loss: 0.037953	Acc: 48.2% (4821/10000)
[Test]  Epoch: 40	Loss: 0.038073	Acc: 47.9% (4786/10000)
[Test]  Epoch: 41	Loss: 0.038000	Acc: 48.2% (4818/10000)
[Test]  Epoch: 42	Loss: 0.037974	Acc: 48.1% (4812/10000)
[Test]  Epoch: 43	Loss: 0.037984	Acc: 48.1% (4806/10000)
[Test]  Epoch: 44	Loss: 0.038023	Acc: 48.4% (4837/10000)
[Test]  Epoch: 45	Loss: 0.037988	Acc: 48.0% (4795/10000)
[Test]  Epoch: 46	Loss: 0.038020	Acc: 48.0% (4805/10000)
[Test]  Epoch: 47	Loss: 0.038064	Acc: 48.5% (4854/10000)
[Test]  Epoch: 48	Loss: 0.038044	Acc: 48.2% (4823/10000)
[Test]  Epoch: 49	Loss: 0.038056	Acc: 48.5% (4850/10000)
[Test]  Epoch: 50	Loss: 0.037986	Acc: 48.2% (4817/10000)
[Test]  Epoch: 51	Loss: 0.038013	Acc: 48.7% (4873/10000)
[Test]  Epoch: 52	Loss: 0.038037	Acc: 48.3% (4831/10000)
[Test]  Epoch: 53	Loss: 0.038152	Acc: 48.4% (4842/10000)
[Test]  Epoch: 54	Loss: 0.038090	Acc: 48.8% (4879/10000)
[Test]  Epoch: 55	Loss: 0.038062	Acc: 48.6% (4863/10000)
[Test]  Epoch: 56	Loss: 0.038115	Acc: 48.2% (4821/10000)
[Test]  Epoch: 57	Loss: 0.038093	Acc: 48.3% (4829/10000)
[Test]  Epoch: 58	Loss: 0.038079	Acc: 48.5% (4850/10000)
[Test]  Epoch: 59	Loss: 0.038290	Acc: 48.2% (4821/10000)
[Test]  Epoch: 60	Loss: 0.038149	Acc: 48.7% (4870/10000)
[Test]  Epoch: 61	Loss: 0.038232	Acc: 48.6% (4856/10000)
[Test]  Epoch: 62	Loss: 0.038156	Acc: 48.7% (4869/10000)
[Test]  Epoch: 63	Loss: 0.038168	Acc: 48.6% (4862/10000)
[Test]  Epoch: 64	Loss: 0.038160	Acc: 48.8% (4877/10000)
[Test]  Epoch: 65	Loss: 0.038156	Acc: 48.4% (4843/10000)
[Test]  Epoch: 66	Loss: 0.038142	Acc: 48.8% (4876/10000)
[Test]  Epoch: 67	Loss: 0.038148	Acc: 48.8% (4875/10000)
[Test]  Epoch: 68	Loss: 0.038234	Acc: 48.6% (4857/10000)
[Test]  Epoch: 69	Loss: 0.038178	Acc: 48.7% (4873/10000)
[Test]  Epoch: 70	Loss: 0.038109	Acc: 48.7% (4873/10000)
[Test]  Epoch: 71	Loss: 0.038153	Acc: 48.6% (4863/10000)
[Test]  Epoch: 72	Loss: 0.038205	Acc: 48.5% (4848/10000)
[Test]  Epoch: 73	Loss: 0.038135	Acc: 48.8% (4880/10000)
[Test]  Epoch: 74	Loss: 0.038065	Acc: 48.7% (4870/10000)
[Test]  Epoch: 75	Loss: 0.038155	Acc: 48.8% (4881/10000)
[Test]  Epoch: 76	Loss: 0.038083	Acc: 48.9% (4886/10000)
[Test]  Epoch: 77	Loss: 0.038148	Acc: 48.7% (4872/10000)
[Test]  Epoch: 78	Loss: 0.038189	Acc: 48.6% (4858/10000)
[Test]  Epoch: 79	Loss: 0.038166	Acc: 48.7% (4869/10000)
[Test]  Epoch: 80	Loss: 0.038179	Acc: 48.8% (4881/10000)
[Test]  Epoch: 81	Loss: 0.038204	Acc: 48.7% (4867/10000)
[Test]  Epoch: 82	Loss: 0.038200	Acc: 48.6% (4858/10000)
[Test]  Epoch: 83	Loss: 0.038150	Acc: 48.6% (4856/10000)
[Test]  Epoch: 84	Loss: 0.038202	Acc: 48.8% (4883/10000)
[Test]  Epoch: 85	Loss: 0.038190	Acc: 48.7% (4871/10000)
[Test]  Epoch: 86	Loss: 0.038255	Acc: 48.4% (4839/10000)
[Test]  Epoch: 87	Loss: 0.038218	Acc: 48.6% (4858/10000)
[Test]  Epoch: 88	Loss: 0.038227	Acc: 48.5% (4855/10000)
[Test]  Epoch: 89	Loss: 0.038252	Acc: 48.5% (4850/10000)
[Test]  Epoch: 90	Loss: 0.038232	Acc: 48.7% (4873/10000)
[Test]  Epoch: 91	Loss: 0.038164	Acc: 48.8% (4878/10000)
[Test]  Epoch: 92	Loss: 0.038106	Acc: 48.8% (4878/10000)
[Test]  Epoch: 93	Loss: 0.038188	Acc: 48.7% (4870/10000)
[Test]  Epoch: 94	Loss: 0.038220	Acc: 48.7% (4872/10000)
[Test]  Epoch: 95	Loss: 0.038201	Acc: 48.6% (4857/10000)
[Test]  Epoch: 96	Loss: 0.038135	Acc: 48.7% (4872/10000)
[Test]  Epoch: 97	Loss: 0.038181	Acc: 48.6% (4858/10000)
[Test]  Epoch: 98	Loss: 0.038166	Acc: 48.6% (4864/10000)
[Test]  Epoch: 99	Loss: 0.038204	Acc: 48.6% (4860/10000)
[Test]  Epoch: 100	Loss: 0.038160	Acc: 48.7% (4866/10000)
===========finish==========
['2024-08-19', '18:38:53.183460', '100', 'test', '0.038160083705186844', '48.66', '48.86']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight', 'layer2.2.bn1.weight', 'layer3.2.bn2.weight', 'layer3.5.bn3.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.1.bn1.weight', 'layer2.3.bn3.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn2.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer4.1.conv2.weight', 'layer3.1.bn3.weight', 'layer4.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn2.weight', 'layer4.2.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.bn1.weight', 'layer1.1.conv1.weight', 'layer3.0.bn2.weight', 'layer1.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.bn3.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.2.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.2.conv3.weight', 'layer3.0.downsample.1.weight', 'layer3.3.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.068054	Acc: 18.6% (1865/10000)
[Test]  Epoch: 2	Loss: 0.048206	Acc: 31.6% (3164/10000)
[Test]  Epoch: 3	Loss: 0.042511	Acc: 37.4% (3736/10000)
[Test]  Epoch: 4	Loss: 0.040659	Acc: 40.0% (3995/10000)
[Test]  Epoch: 5	Loss: 0.039896	Acc: 40.6% (4064/10000)
[Test]  Epoch: 6	Loss: 0.039248	Acc: 42.5% (4250/10000)
[Test]  Epoch: 7	Loss: 0.038941	Acc: 43.4% (4337/10000)
[Test]  Epoch: 8	Loss: 0.038870	Acc: 43.8% (4375/10000)
[Test]  Epoch: 9	Loss: 0.038952	Acc: 44.1% (4409/10000)
[Test]  Epoch: 10	Loss: 0.038750	Acc: 44.3% (4428/10000)
[Test]  Epoch: 11	Loss: 0.038837	Acc: 44.5% (4454/10000)
[Test]  Epoch: 12	Loss: 0.038477	Acc: 44.7% (4470/10000)
[Test]  Epoch: 13	Loss: 0.038692	Acc: 44.8% (4482/10000)
[Test]  Epoch: 14	Loss: 0.038499	Acc: 45.0% (4496/10000)
[Test]  Epoch: 15	Loss: 0.038745	Acc: 44.6% (4461/10000)
[Test]  Epoch: 16	Loss: 0.038519	Acc: 45.6% (4564/10000)
[Test]  Epoch: 17	Loss: 0.038456	Acc: 45.6% (4558/10000)
[Test]  Epoch: 18	Loss: 0.038518	Acc: 45.5% (4549/10000)
[Test]  Epoch: 19	Loss: 0.038458	Acc: 45.7% (4571/10000)
[Test]  Epoch: 20	Loss: 0.038506	Acc: 46.0% (4603/10000)
[Test]  Epoch: 21	Loss: 0.038341	Acc: 46.0% (4595/10000)
[Test]  Epoch: 22	Loss: 0.038572	Acc: 46.1% (4610/10000)
[Test]  Epoch: 23	Loss: 0.038417	Acc: 46.2% (4619/10000)
[Test]  Epoch: 24	Loss: 0.038553	Acc: 46.2% (4624/10000)
[Test]  Epoch: 25	Loss: 0.038602	Acc: 46.4% (4635/10000)
[Test]  Epoch: 26	Loss: 0.038480	Acc: 45.9% (4588/10000)
[Test]  Epoch: 27	Loss: 0.038445	Acc: 46.1% (4607/10000)
[Test]  Epoch: 28	Loss: 0.038436	Acc: 46.8% (4682/10000)
[Test]  Epoch: 29	Loss: 0.038428	Acc: 46.3% (4629/10000)
[Test]  Epoch: 30	Loss: 0.038416	Acc: 46.8% (4680/10000)
[Test]  Epoch: 31	Loss: 0.038550	Acc: 46.6% (4661/10000)
[Test]  Epoch: 32	Loss: 0.038684	Acc: 46.6% (4665/10000)
[Test]  Epoch: 33	Loss: 0.038490	Acc: 46.6% (4664/10000)
[Test]  Epoch: 34	Loss: 0.038507	Acc: 46.8% (4683/10000)
[Test]  Epoch: 35	Loss: 0.038486	Acc: 47.1% (4710/10000)
[Test]  Epoch: 36	Loss: 0.038579	Acc: 46.9% (4693/10000)
[Test]  Epoch: 37	Loss: 0.038618	Acc: 47.0% (4695/10000)
[Test]  Epoch: 38	Loss: 0.038570	Acc: 46.8% (4679/10000)
[Test]  Epoch: 39	Loss: 0.038605	Acc: 46.8% (4676/10000)
[Test]  Epoch: 40	Loss: 0.038672	Acc: 46.8% (4681/10000)
[Test]  Epoch: 41	Loss: 0.038510	Acc: 46.9% (4687/10000)
[Test]  Epoch: 42	Loss: 0.038636	Acc: 46.7% (4667/10000)
[Test]  Epoch: 43	Loss: 0.038569	Acc: 47.1% (4708/10000)
[Test]  Epoch: 44	Loss: 0.038599	Acc: 46.8% (4684/10000)
[Test]  Epoch: 45	Loss: 0.038607	Acc: 47.0% (4698/10000)
[Test]  Epoch: 46	Loss: 0.038568	Acc: 47.0% (4697/10000)
[Test]  Epoch: 47	Loss: 0.038653	Acc: 47.2% (4724/10000)
[Test]  Epoch: 48	Loss: 0.038663	Acc: 47.1% (4712/10000)
[Test]  Epoch: 49	Loss: 0.038777	Acc: 47.0% (4702/10000)
[Test]  Epoch: 50	Loss: 0.038717	Acc: 47.0% (4701/10000)
[Test]  Epoch: 51	Loss: 0.038703	Acc: 47.2% (4723/10000)
[Test]  Epoch: 52	Loss: 0.038744	Acc: 47.2% (4725/10000)
[Test]  Epoch: 53	Loss: 0.038745	Acc: 47.4% (4744/10000)
[Test]  Epoch: 54	Loss: 0.038767	Acc: 47.3% (4729/10000)
[Test]  Epoch: 55	Loss: 0.038737	Acc: 47.6% (4765/10000)
[Test]  Epoch: 56	Loss: 0.038743	Acc: 47.4% (4740/10000)
[Test]  Epoch: 57	Loss: 0.038783	Acc: 47.3% (4734/10000)
[Test]  Epoch: 58	Loss: 0.038791	Acc: 47.7% (4767/10000)
[Test]  Epoch: 59	Loss: 0.038835	Acc: 47.4% (4737/10000)
[Test]  Epoch: 60	Loss: 0.038843	Acc: 47.5% (4746/10000)
[Test]  Epoch: 61	Loss: 0.038845	Acc: 47.5% (4755/10000)
[Test]  Epoch: 62	Loss: 0.038810	Acc: 47.7% (4774/10000)
[Test]  Epoch: 63	Loss: 0.038767	Acc: 47.7% (4769/10000)
[Test]  Epoch: 64	Loss: 0.038785	Acc: 47.6% (4763/10000)
[Test]  Epoch: 65	Loss: 0.038762	Acc: 47.6% (4762/10000)
[Test]  Epoch: 66	Loss: 0.038834	Acc: 47.4% (4743/10000)
[Test]  Epoch: 67	Loss: 0.038803	Acc: 47.6% (4758/10000)
[Test]  Epoch: 68	Loss: 0.038838	Acc: 47.5% (4750/10000)
[Test]  Epoch: 69	Loss: 0.038814	Acc: 47.5% (4752/10000)
[Test]  Epoch: 70	Loss: 0.038790	Acc: 47.6% (4763/10000)
[Test]  Epoch: 71	Loss: 0.038888	Acc: 47.5% (4749/10000)
[Test]  Epoch: 72	Loss: 0.038877	Acc: 47.4% (4744/10000)
[Test]  Epoch: 73	Loss: 0.038757	Acc: 47.8% (4780/10000)
[Test]  Epoch: 74	Loss: 0.038716	Acc: 47.6% (4760/10000)
[Test]  Epoch: 75	Loss: 0.038766	Acc: 47.6% (4763/10000)
[Test]  Epoch: 76	Loss: 0.038730	Acc: 47.7% (4766/10000)
[Test]  Epoch: 77	Loss: 0.038771	Acc: 47.6% (4765/10000)
[Test]  Epoch: 78	Loss: 0.038817	Acc: 47.5% (4746/10000)
[Test]  Epoch: 79	Loss: 0.038772	Acc: 47.6% (4762/10000)
[Test]  Epoch: 80	Loss: 0.038875	Acc: 47.5% (4746/10000)
[Test]  Epoch: 81	Loss: 0.038832	Acc: 47.6% (4757/10000)
[Test]  Epoch: 82	Loss: 0.038879	Acc: 47.4% (4740/10000)
[Test]  Epoch: 83	Loss: 0.038791	Acc: 47.6% (4761/10000)
[Test]  Epoch: 84	Loss: 0.038807	Acc: 47.6% (4765/10000)
[Test]  Epoch: 85	Loss: 0.038828	Acc: 47.7% (4767/10000)
[Test]  Epoch: 86	Loss: 0.038864	Acc: 47.5% (4753/10000)
[Test]  Epoch: 87	Loss: 0.038777	Acc: 47.7% (4772/10000)
[Test]  Epoch: 88	Loss: 0.038803	Acc: 47.7% (4773/10000)
[Test]  Epoch: 89	Loss: 0.038853	Acc: 47.5% (4746/10000)
[Test]  Epoch: 90	Loss: 0.038834	Acc: 47.7% (4772/10000)
[Test]  Epoch: 91	Loss: 0.038796	Acc: 47.6% (4764/10000)
[Test]  Epoch: 92	Loss: 0.038749	Acc: 47.7% (4767/10000)
[Test]  Epoch: 93	Loss: 0.038840	Acc: 47.7% (4770/10000)
[Test]  Epoch: 94	Loss: 0.038857	Acc: 47.5% (4746/10000)
[Test]  Epoch: 95	Loss: 0.038880	Acc: 47.6% (4758/10000)
[Test]  Epoch: 96	Loss: 0.038786	Acc: 47.7% (4771/10000)
[Test]  Epoch: 97	Loss: 0.038841	Acc: 47.6% (4759/10000)
[Test]  Epoch: 98	Loss: 0.038839	Acc: 47.5% (4754/10000)
[Test]  Epoch: 99	Loss: 0.038798	Acc: 47.7% (4767/10000)
[Test]  Epoch: 100	Loss: 0.038786	Acc: 47.7% (4769/10000)
===========finish==========
['2024-08-19', '18:46:05.610372', '100', 'test', '0.03878575096130371', '47.69', '47.8']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight', 'layer2.2.bn1.weight', 'layer3.2.bn2.weight', 'layer3.5.bn3.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.1.bn1.weight', 'layer2.3.bn3.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn2.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer4.1.conv2.weight', 'layer3.1.bn3.weight', 'layer4.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn2.weight', 'layer4.2.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.bn1.weight', 'layer1.1.conv1.weight', 'layer3.0.bn2.weight', 'layer1.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.bn3.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.2.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.2.conv3.weight', 'layer3.0.downsample.1.weight', 'layer3.3.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv3.weight', 'layer1.1.conv2.weight', 'layer3.2.conv1.weight', 'layer3.4.conv2.weight', 'layer1.0.conv3.weight', 'layer2.2.conv1.weight', 'layer4.0.bn2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.084781	Acc: 15.5% (1552/10000)
[Test]  Epoch: 2	Loss: 0.049684	Acc: 30.3% (3033/10000)
[Test]  Epoch: 3	Loss: 0.043919	Acc: 35.4% (3543/10000)
[Test]  Epoch: 4	Loss: 0.041755	Acc: 38.4% (3839/10000)
[Test]  Epoch: 5	Loss: 0.041166	Acc: 39.8% (3976/10000)
[Test]  Epoch: 6	Loss: 0.040537	Acc: 40.8% (4076/10000)
[Test]  Epoch: 7	Loss: 0.040468	Acc: 40.9% (4091/10000)
[Test]  Epoch: 8	Loss: 0.040273	Acc: 41.1% (4110/10000)
[Test]  Epoch: 9	Loss: 0.040233	Acc: 41.9% (4186/10000)
[Test]  Epoch: 10	Loss: 0.040027	Acc: 42.5% (4245/10000)
[Test]  Epoch: 11	Loss: 0.040324	Acc: 42.1% (4211/10000)
[Test]  Epoch: 12	Loss: 0.039738	Acc: 42.7% (4267/10000)
[Test]  Epoch: 13	Loss: 0.039971	Acc: 42.7% (4269/10000)
[Test]  Epoch: 14	Loss: 0.039811	Acc: 43.0% (4299/10000)
[Test]  Epoch: 15	Loss: 0.039915	Acc: 43.0% (4302/10000)
[Test]  Epoch: 16	Loss: 0.039931	Acc: 42.8% (4277/10000)
[Test]  Epoch: 17	Loss: 0.039751	Acc: 42.9% (4293/10000)
[Test]  Epoch: 18	Loss: 0.039752	Acc: 43.1% (4309/10000)
[Test]  Epoch: 19	Loss: 0.039796	Acc: 43.6% (4356/10000)
[Test]  Epoch: 20	Loss: 0.039785	Acc: 43.5% (4354/10000)
[Test]  Epoch: 21	Loss: 0.039686	Acc: 43.8% (4376/10000)
[Test]  Epoch: 22	Loss: 0.039759	Acc: 43.6% (4364/10000)
[Test]  Epoch: 23	Loss: 0.039761	Acc: 43.6% (4358/10000)
[Test]  Epoch: 24	Loss: 0.039861	Acc: 43.4% (4339/10000)
[Test]  Epoch: 25	Loss: 0.039717	Acc: 43.8% (4376/10000)
[Test]  Epoch: 26	Loss: 0.039747	Acc: 43.9% (4385/10000)
[Test]  Epoch: 27	Loss: 0.039740	Acc: 43.7% (4374/10000)
[Test]  Epoch: 28	Loss: 0.039792	Acc: 43.9% (4387/10000)
[Test]  Epoch: 29	Loss: 0.039881	Acc: 43.7% (4369/10000)
[Test]  Epoch: 30	Loss: 0.039715	Acc: 43.9% (4394/10000)
[Test]  Epoch: 31	Loss: 0.039912	Acc: 44.0% (4404/10000)
[Test]  Epoch: 32	Loss: 0.039844	Acc: 44.6% (4457/10000)
[Test]  Epoch: 33	Loss: 0.039745	Acc: 44.0% (4404/10000)
[Test]  Epoch: 34	Loss: 0.039785	Acc: 44.1% (4415/10000)
[Test]  Epoch: 35	Loss: 0.039769	Acc: 44.3% (4434/10000)
[Test]  Epoch: 36	Loss: 0.039745	Acc: 44.4% (4439/10000)
[Test]  Epoch: 37	Loss: 0.039920	Acc: 43.9% (4394/10000)
[Test]  Epoch: 38	Loss: 0.039775	Acc: 44.5% (4455/10000)
[Test]  Epoch: 39	Loss: 0.039816	Acc: 44.5% (4449/10000)
[Test]  Epoch: 40	Loss: 0.039849	Acc: 44.3% (4433/10000)
[Test]  Epoch: 41	Loss: 0.039827	Acc: 44.5% (4451/10000)
[Test]  Epoch: 42	Loss: 0.039890	Acc: 44.4% (4442/10000)
[Test]  Epoch: 43	Loss: 0.039913	Acc: 44.7% (4468/10000)
[Test]  Epoch: 44	Loss: 0.039885	Acc: 44.6% (4459/10000)
[Test]  Epoch: 45	Loss: 0.039878	Acc: 44.7% (4471/10000)
[Test]  Epoch: 46	Loss: 0.039859	Acc: 44.8% (4477/10000)
[Test]  Epoch: 47	Loss: 0.039903	Acc: 45.0% (4503/10000)
[Test]  Epoch: 48	Loss: 0.039868	Acc: 44.7% (4468/10000)
[Test]  Epoch: 49	Loss: 0.039968	Acc: 44.8% (4475/10000)
[Test]  Epoch: 50	Loss: 0.039965	Acc: 45.0% (4504/10000)
[Test]  Epoch: 51	Loss: 0.039916	Acc: 45.1% (4506/10000)
[Test]  Epoch: 52	Loss: 0.039929	Acc: 44.8% (4476/10000)
[Test]  Epoch: 53	Loss: 0.040056	Acc: 44.8% (4479/10000)
[Test]  Epoch: 54	Loss: 0.040009	Acc: 45.1% (4514/10000)
[Test]  Epoch: 55	Loss: 0.040005	Acc: 45.0% (4498/10000)
[Test]  Epoch: 56	Loss: 0.040038	Acc: 45.1% (4510/10000)
[Test]  Epoch: 57	Loss: 0.039975	Acc: 45.1% (4508/10000)
[Test]  Epoch: 58	Loss: 0.039981	Acc: 45.1% (4509/10000)
[Test]  Epoch: 59	Loss: 0.040089	Acc: 44.7% (4467/10000)
[Test]  Epoch: 60	Loss: 0.040068	Acc: 45.4% (4543/10000)
[Test]  Epoch: 61	Loss: 0.040131	Acc: 45.1% (4513/10000)
[Test]  Epoch: 62	Loss: 0.040046	Acc: 45.3% (4530/10000)
[Test]  Epoch: 63	Loss: 0.039982	Acc: 45.2% (4519/10000)
[Test]  Epoch: 64	Loss: 0.040014	Acc: 45.1% (4514/10000)
[Test]  Epoch: 65	Loss: 0.039992	Acc: 45.3% (4529/10000)
[Test]  Epoch: 66	Loss: 0.039975	Acc: 45.3% (4530/10000)
[Test]  Epoch: 67	Loss: 0.040024	Acc: 45.2% (4523/10000)
[Test]  Epoch: 68	Loss: 0.040086	Acc: 45.0% (4499/10000)
[Test]  Epoch: 69	Loss: 0.040037	Acc: 45.2% (4525/10000)
[Test]  Epoch: 70	Loss: 0.040023	Acc: 45.1% (4515/10000)
[Test]  Epoch: 71	Loss: 0.040075	Acc: 45.0% (4499/10000)
[Test]  Epoch: 72	Loss: 0.040104	Acc: 44.9% (4491/10000)
[Test]  Epoch: 73	Loss: 0.040041	Acc: 45.1% (4515/10000)
[Test]  Epoch: 74	Loss: 0.039925	Acc: 45.3% (4528/10000)
[Test]  Epoch: 75	Loss: 0.039993	Acc: 45.2% (4523/10000)
[Test]  Epoch: 76	Loss: 0.039965	Acc: 45.2% (4524/10000)
[Test]  Epoch: 77	Loss: 0.039998	Acc: 45.2% (4520/10000)
[Test]  Epoch: 78	Loss: 0.040013	Acc: 45.1% (4510/10000)
[Test]  Epoch: 79	Loss: 0.040008	Acc: 45.3% (4529/10000)
[Test]  Epoch: 80	Loss: 0.040047	Acc: 45.1% (4510/10000)
[Test]  Epoch: 81	Loss: 0.040022	Acc: 45.0% (4505/10000)
[Test]  Epoch: 82	Loss: 0.040056	Acc: 45.1% (4511/10000)
[Test]  Epoch: 83	Loss: 0.040025	Acc: 45.3% (4529/10000)
[Test]  Epoch: 84	Loss: 0.040040	Acc: 45.3% (4531/10000)
[Test]  Epoch: 85	Loss: 0.040057	Acc: 45.2% (4524/10000)
[Test]  Epoch: 86	Loss: 0.040114	Acc: 45.0% (4499/10000)
[Test]  Epoch: 87	Loss: 0.040033	Acc: 45.1% (4509/10000)
[Test]  Epoch: 88	Loss: 0.040044	Acc: 45.2% (4519/10000)
[Test]  Epoch: 89	Loss: 0.040120	Acc: 45.0% (4502/10000)
[Test]  Epoch: 90	Loss: 0.040128	Acc: 45.0% (4500/10000)
[Test]  Epoch: 91	Loss: 0.040017	Acc: 45.3% (4531/10000)
[Test]  Epoch: 92	Loss: 0.039994	Acc: 45.0% (4496/10000)
[Test]  Epoch: 93	Loss: 0.040074	Acc: 45.0% (4500/10000)
[Test]  Epoch: 94	Loss: 0.040133	Acc: 45.0% (4498/10000)
[Test]  Epoch: 95	Loss: 0.040075	Acc: 44.8% (4483/10000)
[Test]  Epoch: 96	Loss: 0.040050	Acc: 45.2% (4517/10000)
[Test]  Epoch: 97	Loss: 0.040074	Acc: 45.1% (4506/10000)
[Test]  Epoch: 98	Loss: 0.040086	Acc: 45.1% (4509/10000)
[Test]  Epoch: 99	Loss: 0.040085	Acc: 45.1% (4508/10000)
[Test]  Epoch: 100	Loss: 0.040057	Acc: 45.1% (4512/10000)
===========finish==========
['2024-08-19', '18:53:17.441744', '100', 'test', '0.04005736092329025', '45.12', '45.43']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight', 'layer2.2.bn1.weight', 'layer3.2.bn2.weight', 'layer3.5.bn3.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.1.bn1.weight', 'layer2.3.bn3.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn2.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer4.1.conv2.weight', 'layer3.1.bn3.weight', 'layer4.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn2.weight', 'layer4.2.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.bn1.weight', 'layer1.1.conv1.weight', 'layer3.0.bn2.weight', 'layer1.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.bn3.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.2.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.2.conv3.weight', 'layer3.0.downsample.1.weight', 'layer3.3.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv3.weight', 'layer1.1.conv2.weight', 'layer3.2.conv1.weight', 'layer3.4.conv2.weight', 'layer1.0.conv3.weight', 'layer2.2.conv1.weight', 'layer4.0.bn2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'layer2.3.conv1.weight', 'layer4.0.bn1.weight', 'layer2.3.conv3.weight', 'layer3.5.conv2.weight', 'layer3.3.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer3.2.conv3.weight', 'layer2.2.conv3.weight', 'layer2.1.conv3.weight', 'bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.075392	Acc: 17.5% (1751/10000)
[Test]  Epoch: 2	Loss: 0.048794	Acc: 32.0% (3204/10000)
[Test]  Epoch: 3	Loss: 0.043755	Acc: 35.3% (3533/10000)
[Test]  Epoch: 4	Loss: 0.042467	Acc: 37.9% (3788/10000)
[Test]  Epoch: 5	Loss: 0.042043	Acc: 38.2% (3817/10000)
[Test]  Epoch: 6	Loss: 0.041022	Acc: 40.1% (4012/10000)
[Test]  Epoch: 7	Loss: 0.041090	Acc: 40.1% (4013/10000)
[Test]  Epoch: 8	Loss: 0.040776	Acc: 40.8% (4078/10000)
[Test]  Epoch: 9	Loss: 0.040587	Acc: 41.2% (4124/10000)
[Test]  Epoch: 10	Loss: 0.040227	Acc: 41.8% (4175/10000)
[Test]  Epoch: 11	Loss: 0.040629	Acc: 41.4% (4144/10000)
[Test]  Epoch: 12	Loss: 0.040324	Acc: 42.0% (4198/10000)
[Test]  Epoch: 13	Loss: 0.040472	Acc: 42.2% (4219/10000)
[Test]  Epoch: 14	Loss: 0.040431	Acc: 41.7% (4173/10000)
[Test]  Epoch: 15	Loss: 0.040476	Acc: 42.3% (4234/10000)
[Test]  Epoch: 16	Loss: 0.040478	Acc: 42.3% (4228/10000)
[Test]  Epoch: 17	Loss: 0.040247	Acc: 42.6% (4260/10000)
[Test]  Epoch: 18	Loss: 0.040280	Acc: 42.8% (4279/10000)
[Test]  Epoch: 19	Loss: 0.040229	Acc: 43.1% (4307/10000)
[Test]  Epoch: 20	Loss: 0.040299	Acc: 43.1% (4314/10000)
[Test]  Epoch: 21	Loss: 0.040199	Acc: 43.2% (4322/10000)
[Test]  Epoch: 22	Loss: 0.040395	Acc: 43.4% (4343/10000)
[Test]  Epoch: 23	Loss: 0.040155	Acc: 43.5% (4354/10000)
[Test]  Epoch: 24	Loss: 0.040300	Acc: 43.1% (4313/10000)
[Test]  Epoch: 25	Loss: 0.040301	Acc: 43.4% (4342/10000)
[Test]  Epoch: 26	Loss: 0.040287	Acc: 43.6% (4365/10000)
[Test]  Epoch: 27	Loss: 0.040323	Acc: 43.3% (4334/10000)
[Test]  Epoch: 28	Loss: 0.040215	Acc: 43.7% (4366/10000)
[Test]  Epoch: 29	Loss: 0.040281	Acc: 43.5% (4349/10000)
[Test]  Epoch: 30	Loss: 0.040224	Acc: 43.6% (4361/10000)
[Test]  Epoch: 31	Loss: 0.040377	Acc: 43.8% (4376/10000)
[Test]  Epoch: 32	Loss: 0.040342	Acc: 43.9% (4386/10000)
[Test]  Epoch: 33	Loss: 0.040372	Acc: 43.7% (4366/10000)
[Test]  Epoch: 34	Loss: 0.040305	Acc: 44.1% (4410/10000)
[Test]  Epoch: 35	Loss: 0.040179	Acc: 43.9% (4394/10000)
[Test]  Epoch: 36	Loss: 0.040431	Acc: 43.6% (4361/10000)
[Test]  Epoch: 37	Loss: 0.040426	Acc: 43.8% (4382/10000)
[Test]  Epoch: 38	Loss: 0.040379	Acc: 44.4% (4440/10000)
[Test]  Epoch: 39	Loss: 0.040347	Acc: 44.2% (4423/10000)
[Test]  Epoch: 40	Loss: 0.040456	Acc: 44.0% (4398/10000)
[Test]  Epoch: 41	Loss: 0.040232	Acc: 44.4% (4435/10000)
[Test]  Epoch: 42	Loss: 0.040384	Acc: 44.1% (4413/10000)
[Test]  Epoch: 43	Loss: 0.040432	Acc: 44.2% (4425/10000)
[Test]  Epoch: 44	Loss: 0.040500	Acc: 43.9% (4387/10000)
[Test]  Epoch: 45	Loss: 0.040412	Acc: 44.1% (4415/10000)
[Test]  Epoch: 46	Loss: 0.040388	Acc: 44.3% (4431/10000)
[Test]  Epoch: 47	Loss: 0.040434	Acc: 44.4% (4441/10000)
[Test]  Epoch: 48	Loss: 0.040449	Acc: 44.1% (4406/10000)
[Test]  Epoch: 49	Loss: 0.040492	Acc: 44.4% (4443/10000)
[Test]  Epoch: 50	Loss: 0.040457	Acc: 44.5% (4448/10000)
[Test]  Epoch: 51	Loss: 0.040527	Acc: 44.3% (4432/10000)
[Test]  Epoch: 52	Loss: 0.040394	Acc: 44.3% (4431/10000)
[Test]  Epoch: 53	Loss: 0.040640	Acc: 44.1% (4412/10000)
[Test]  Epoch: 54	Loss: 0.040448	Acc: 44.5% (4448/10000)
[Test]  Epoch: 55	Loss: 0.040580	Acc: 44.3% (4427/10000)
[Test]  Epoch: 56	Loss: 0.040542	Acc: 44.4% (4436/10000)
[Test]  Epoch: 57	Loss: 0.040507	Acc: 44.3% (4434/10000)
[Test]  Epoch: 58	Loss: 0.040585	Acc: 44.2% (4421/10000)
[Test]  Epoch: 59	Loss: 0.040673	Acc: 44.3% (4427/10000)
[Test]  Epoch: 60	Loss: 0.040628	Acc: 44.5% (4445/10000)
[Test]  Epoch: 61	Loss: 0.040614	Acc: 44.6% (4459/10000)
[Test]  Epoch: 62	Loss: 0.040523	Acc: 44.6% (4456/10000)
[Test]  Epoch: 63	Loss: 0.040505	Acc: 44.5% (4455/10000)
[Test]  Epoch: 64	Loss: 0.040585	Acc: 44.5% (4451/10000)
[Test]  Epoch: 65	Loss: 0.040539	Acc: 44.5% (4449/10000)
[Test]  Epoch: 66	Loss: 0.040513	Acc: 44.9% (4487/10000)
[Test]  Epoch: 67	Loss: 0.040559	Acc: 44.5% (4451/10000)
[Test]  Epoch: 68	Loss: 0.040639	Acc: 44.6% (4460/10000)
[Test]  Epoch: 69	Loss: 0.040542	Acc: 44.6% (4459/10000)
[Test]  Epoch: 70	Loss: 0.040535	Acc: 44.7% (4469/10000)
[Test]  Epoch: 71	Loss: 0.040639	Acc: 44.6% (4457/10000)
[Test]  Epoch: 72	Loss: 0.040652	Acc: 44.6% (4461/10000)
[Test]  Epoch: 73	Loss: 0.040617	Acc: 44.5% (4451/10000)
[Test]  Epoch: 74	Loss: 0.040483	Acc: 44.6% (4462/10000)
[Test]  Epoch: 75	Loss: 0.040545	Acc: 44.7% (4473/10000)
[Test]  Epoch: 76	Loss: 0.040514	Acc: 44.6% (4464/10000)
[Test]  Epoch: 77	Loss: 0.040548	Acc: 44.7% (4471/10000)
[Test]  Epoch: 78	Loss: 0.040555	Acc: 44.7% (4467/10000)
[Test]  Epoch: 79	Loss: 0.040577	Acc: 44.6% (4456/10000)
[Test]  Epoch: 80	Loss: 0.040616	Acc: 44.6% (4457/10000)
[Test]  Epoch: 81	Loss: 0.040593	Acc: 44.4% (4444/10000)
[Test]  Epoch: 82	Loss: 0.040628	Acc: 44.5% (4448/10000)
[Test]  Epoch: 83	Loss: 0.040586	Acc: 44.6% (4459/10000)
[Test]  Epoch: 84	Loss: 0.040632	Acc: 44.6% (4463/10000)
[Test]  Epoch: 85	Loss: 0.040610	Acc: 44.8% (4480/10000)
[Test]  Epoch: 86	Loss: 0.040655	Acc: 44.6% (4459/10000)
[Test]  Epoch: 87	Loss: 0.040578	Acc: 44.5% (4453/10000)
[Test]  Epoch: 88	Loss: 0.040625	Acc: 44.7% (4473/10000)
[Test]  Epoch: 89	Loss: 0.040613	Acc: 44.6% (4461/10000)
[Test]  Epoch: 90	Loss: 0.040619	Acc: 44.7% (4472/10000)
[Test]  Epoch: 91	Loss: 0.040563	Acc: 44.5% (4449/10000)
[Test]  Epoch: 92	Loss: 0.040553	Acc: 44.8% (4480/10000)
[Test]  Epoch: 93	Loss: 0.040606	Acc: 44.7% (4466/10000)
[Test]  Epoch: 94	Loss: 0.040605	Acc: 44.7% (4470/10000)
[Test]  Epoch: 95	Loss: 0.040601	Acc: 44.6% (4463/10000)
[Test]  Epoch: 96	Loss: 0.040547	Acc: 44.7% (4470/10000)
[Test]  Epoch: 97	Loss: 0.040641	Acc: 44.5% (4453/10000)
[Test]  Epoch: 98	Loss: 0.040595	Acc: 44.7% (4466/10000)
[Test]  Epoch: 99	Loss: 0.040606	Acc: 44.5% (4449/10000)
[Test]  Epoch: 100	Loss: 0.040587	Acc: 44.7% (4471/10000)
===========finish==========
['2024-08-19', '19:00:33.585085', '100', 'test', '0.040586770069599154', '44.71', '44.87']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight', 'layer2.2.bn1.weight', 'layer3.2.bn2.weight', 'layer3.5.bn3.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.1.bn1.weight', 'layer2.3.bn3.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn2.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer4.1.conv2.weight', 'layer3.1.bn3.weight', 'layer4.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn2.weight', 'layer4.2.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.bn1.weight', 'layer1.1.conv1.weight', 'layer3.0.bn2.weight', 'layer1.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.bn3.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.2.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.2.conv3.weight', 'layer3.0.downsample.1.weight', 'layer3.3.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv3.weight', 'layer1.1.conv2.weight', 'layer3.2.conv1.weight', 'layer3.4.conv2.weight', 'layer1.0.conv3.weight', 'layer2.2.conv1.weight', 'layer4.0.bn2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'layer2.3.conv1.weight', 'layer4.0.bn1.weight', 'layer2.3.conv3.weight', 'layer3.5.conv2.weight', 'layer3.3.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer3.2.conv3.weight', 'layer2.2.conv3.weight', 'layer2.1.conv3.weight', 'bn1.weight', 'layer3.2.conv2.weight', 'layer2.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv2.weight', 'layer2.3.conv2.weight', 'layer3.1.conv2.weight', 'layer2.0.conv3.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.downsample.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.075855	Acc: 15.9% (1588/10000)
[Test]  Epoch: 2	Loss: 0.051883	Acc: 28.4% (2838/10000)
[Test]  Epoch: 3	Loss: 0.044832	Acc: 34.4% (3439/10000)
[Test]  Epoch: 4	Loss: 0.044496	Acc: 35.0% (3496/10000)
[Test]  Epoch: 5	Loss: 0.043123	Acc: 36.5% (3652/10000)
[Test]  Epoch: 6	Loss: 0.042729	Acc: 37.4% (3736/10000)
[Test]  Epoch: 7	Loss: 0.042365	Acc: 38.0% (3805/10000)
[Test]  Epoch: 8	Loss: 0.042357	Acc: 38.4% (3843/10000)
[Test]  Epoch: 9	Loss: 0.042146	Acc: 38.8% (3875/10000)
[Test]  Epoch: 10	Loss: 0.041907	Acc: 39.5% (3949/10000)
[Test]  Epoch: 11	Loss: 0.042085	Acc: 39.2% (3921/10000)
[Test]  Epoch: 12	Loss: 0.041965	Acc: 39.2% (3918/10000)
[Test]  Epoch: 13	Loss: 0.041836	Acc: 40.1% (4006/10000)
[Test]  Epoch: 14	Loss: 0.041858	Acc: 40.0% (4000/10000)
[Test]  Epoch: 15	Loss: 0.041889	Acc: 39.7% (3967/10000)
[Test]  Epoch: 16	Loss: 0.041810	Acc: 40.1% (4010/10000)
[Test]  Epoch: 17	Loss: 0.041741	Acc: 40.3% (4028/10000)
[Test]  Epoch: 18	Loss: 0.041675	Acc: 40.5% (4051/10000)
[Test]  Epoch: 19	Loss: 0.041614	Acc: 40.7% (4070/10000)
[Test]  Epoch: 20	Loss: 0.041778	Acc: 40.5% (4053/10000)
[Test]  Epoch: 21	Loss: 0.041574	Acc: 41.0% (4101/10000)
[Test]  Epoch: 22	Loss: 0.041721	Acc: 40.9% (4086/10000)
[Test]  Epoch: 23	Loss: 0.041632	Acc: 40.8% (4080/10000)
[Test]  Epoch: 24	Loss: 0.041765	Acc: 41.0% (4095/10000)
[Test]  Epoch: 25	Loss: 0.041868	Acc: 40.6% (4058/10000)
[Test]  Epoch: 26	Loss: 0.041573	Acc: 41.0% (4100/10000)
[Test]  Epoch: 27	Loss: 0.041680	Acc: 41.0% (4095/10000)
[Test]  Epoch: 28	Loss: 0.041723	Acc: 41.0% (4103/10000)
[Test]  Epoch: 29	Loss: 0.041716	Acc: 41.1% (4107/10000)
[Test]  Epoch: 30	Loss: 0.041658	Acc: 41.4% (4138/10000)
[Test]  Epoch: 31	Loss: 0.041772	Acc: 41.2% (4124/10000)
[Test]  Epoch: 32	Loss: 0.041808	Acc: 41.0% (4099/10000)
[Test]  Epoch: 33	Loss: 0.041802	Acc: 41.2% (4123/10000)
[Test]  Epoch: 34	Loss: 0.041757	Acc: 41.1% (4115/10000)
[Test]  Epoch: 35	Loss: 0.041664	Acc: 41.5% (4149/10000)
[Test]  Epoch: 36	Loss: 0.041867	Acc: 41.4% (4135/10000)
[Test]  Epoch: 37	Loss: 0.041802	Acc: 41.4% (4137/10000)
[Test]  Epoch: 38	Loss: 0.041626	Acc: 41.7% (4166/10000)
[Test]  Epoch: 39	Loss: 0.041775	Acc: 41.6% (4162/10000)
[Test]  Epoch: 40	Loss: 0.041880	Acc: 41.5% (4152/10000)
[Test]  Epoch: 41	Loss: 0.041700	Acc: 41.8% (4180/10000)
[Test]  Epoch: 42	Loss: 0.041840	Acc: 41.4% (4136/10000)
[Test]  Epoch: 43	Loss: 0.041819	Acc: 41.6% (4163/10000)
[Test]  Epoch: 44	Loss: 0.041802	Acc: 41.8% (4179/10000)
[Test]  Epoch: 45	Loss: 0.041877	Acc: 41.6% (4156/10000)
[Test]  Epoch: 46	Loss: 0.041798	Acc: 41.7% (4171/10000)
[Test]  Epoch: 47	Loss: 0.041860	Acc: 42.0% (4196/10000)
[Test]  Epoch: 48	Loss: 0.041871	Acc: 42.0% (4198/10000)
[Test]  Epoch: 49	Loss: 0.041889	Acc: 42.0% (4205/10000)
[Test]  Epoch: 50	Loss: 0.041761	Acc: 42.1% (4210/10000)
[Test]  Epoch: 51	Loss: 0.041933	Acc: 41.9% (4190/10000)
[Test]  Epoch: 52	Loss: 0.041759	Acc: 42.2% (4224/10000)
[Test]  Epoch: 53	Loss: 0.041997	Acc: 41.8% (4180/10000)
[Test]  Epoch: 54	Loss: 0.041965	Acc: 41.9% (4185/10000)
[Test]  Epoch: 55	Loss: 0.041958	Acc: 42.1% (4213/10000)
[Test]  Epoch: 56	Loss: 0.041897	Acc: 42.1% (4210/10000)
[Test]  Epoch: 57	Loss: 0.041820	Acc: 42.0% (4198/10000)
[Test]  Epoch: 58	Loss: 0.041926	Acc: 42.1% (4214/10000)
[Test]  Epoch: 59	Loss: 0.042053	Acc: 42.0% (4201/10000)
[Test]  Epoch: 60	Loss: 0.042056	Acc: 42.1% (4208/10000)
[Test]  Epoch: 61	Loss: 0.042091	Acc: 42.1% (4210/10000)
[Test]  Epoch: 62	Loss: 0.041974	Acc: 42.3% (4233/10000)
[Test]  Epoch: 63	Loss: 0.041923	Acc: 42.2% (4222/10000)
[Test]  Epoch: 64	Loss: 0.041988	Acc: 42.1% (4214/10000)
[Test]  Epoch: 65	Loss: 0.041950	Acc: 42.3% (4233/10000)
[Test]  Epoch: 66	Loss: 0.041907	Acc: 42.2% (4216/10000)
[Test]  Epoch: 67	Loss: 0.041974	Acc: 42.1% (4206/10000)
[Test]  Epoch: 68	Loss: 0.042058	Acc: 42.0% (4200/10000)
[Test]  Epoch: 69	Loss: 0.041940	Acc: 42.3% (4230/10000)
[Test]  Epoch: 70	Loss: 0.041920	Acc: 42.1% (4214/10000)
[Test]  Epoch: 71	Loss: 0.042052	Acc: 42.0% (4198/10000)
[Test]  Epoch: 72	Loss: 0.042110	Acc: 41.8% (4181/10000)
[Test]  Epoch: 73	Loss: 0.041966	Acc: 42.0% (4197/10000)
[Test]  Epoch: 74	Loss: 0.041915	Acc: 42.1% (4214/10000)
[Test]  Epoch: 75	Loss: 0.041935	Acc: 42.0% (4204/10000)
[Test]  Epoch: 76	Loss: 0.041919	Acc: 42.1% (4214/10000)
[Test]  Epoch: 77	Loss: 0.041942	Acc: 42.2% (4222/10000)
[Test]  Epoch: 78	Loss: 0.041968	Acc: 42.2% (4218/10000)
[Test]  Epoch: 79	Loss: 0.041914	Acc: 42.2% (4217/10000)
[Test]  Epoch: 80	Loss: 0.041996	Acc: 42.2% (4219/10000)
[Test]  Epoch: 81	Loss: 0.041994	Acc: 42.0% (4196/10000)
[Test]  Epoch: 82	Loss: 0.042016	Acc: 42.4% (4241/10000)
[Test]  Epoch: 83	Loss: 0.041966	Acc: 42.2% (4220/10000)
[Test]  Epoch: 84	Loss: 0.041979	Acc: 42.0% (4204/10000)
[Test]  Epoch: 85	Loss: 0.041965	Acc: 42.1% (4209/10000)
[Test]  Epoch: 86	Loss: 0.042041	Acc: 41.9% (4194/10000)
[Test]  Epoch: 87	Loss: 0.042021	Acc: 41.9% (4190/10000)
[Test]  Epoch: 88	Loss: 0.042004	Acc: 42.1% (4210/10000)
[Test]  Epoch: 89	Loss: 0.042000	Acc: 42.0% (4204/10000)
[Test]  Epoch: 90	Loss: 0.042017	Acc: 42.1% (4213/10000)
[Test]  Epoch: 91	Loss: 0.041998	Acc: 42.2% (4216/10000)
[Test]  Epoch: 92	Loss: 0.041948	Acc: 42.3% (4232/10000)
[Test]  Epoch: 93	Loss: 0.042036	Acc: 42.0% (4198/10000)
[Test]  Epoch: 94	Loss: 0.042040	Acc: 42.1% (4215/10000)
[Test]  Epoch: 95	Loss: 0.042029	Acc: 42.1% (4207/10000)
[Test]  Epoch: 96	Loss: 0.041954	Acc: 42.2% (4221/10000)
[Test]  Epoch: 97	Loss: 0.042040	Acc: 42.0% (4195/10000)
[Test]  Epoch: 98	Loss: 0.042040	Acc: 41.8% (4184/10000)
[Test]  Epoch: 99	Loss: 0.041989	Acc: 42.0% (4204/10000)
[Test]  Epoch: 100	Loss: 0.041972	Acc: 42.3% (4229/10000)
===========finish==========
['2024-08-19', '19:07:49.828125', '100', 'test', '0.041971801722049715', '42.29', '42.41']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer4.2.bn1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn3.weight', 'layer3.3.bn2.weight', 'layer3.2.bn1.weight', 'layer2.3.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer1.1.bn1.weight', 'layer1.2.bn1.weight', 'layer3.3.bn3.weight', 'layer1.1.bn2.weight', 'layer2.2.bn1.weight', 'layer3.2.bn2.weight', 'layer3.5.bn3.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer3.1.bn1.weight', 'layer2.3.bn3.weight', 'layer1.0.bn1.weight', 'layer2.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.2.bn3.weight', 'layer3.1.bn2.weight', 'layer2.1.bn3.weight', 'layer1.0.bn3.weight', 'layer4.1.conv2.weight', 'layer3.1.bn3.weight', 'layer4.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn2.weight', 'layer4.2.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.bn1.weight', 'layer1.1.conv1.weight', 'layer3.0.bn2.weight', 'layer1.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.bn3.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.2.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.downsample.1.weight', 'layer1.2.conv3.weight', 'layer3.0.downsample.1.weight', 'layer3.3.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv3.weight', 'layer1.1.conv2.weight', 'layer3.2.conv1.weight', 'layer3.4.conv2.weight', 'layer1.0.conv3.weight', 'layer2.2.conv1.weight', 'layer4.0.bn2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'layer2.3.conv1.weight', 'layer4.0.bn1.weight', 'layer2.3.conv3.weight', 'layer3.5.conv2.weight', 'layer3.3.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer3.2.conv3.weight', 'layer2.2.conv3.weight', 'layer2.1.conv3.weight', 'bn1.weight', 'layer3.2.conv2.weight', 'layer2.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv2.weight', 'layer2.3.conv2.weight', 'layer3.1.conv2.weight', 'layer2.0.conv3.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer3.0.conv1.weight', 'layer3.0.conv3.weight', 'last_linear.weight', 'layer3.0.downsample.0.weight', 'layer3.0.conv2.weight', 'layer1.0.downsample.0.weight', 'conv1.weight', 'layer4.0.conv1.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv3.weight', 'layer4.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.083437	Acc: 1.1% (112/10000)
[Test]  Epoch: 2	Loss: 0.078984	Acc: 4.9% (488/10000)
[Test]  Epoch: 3	Loss: 0.075737	Acc: 8.3% (834/10000)
[Test]  Epoch: 4	Loss: 0.072665	Acc: 11.3% (1133/10000)
[Test]  Epoch: 5	Loss: 0.070038	Acc: 13.5% (1354/10000)
[Test]  Epoch: 6	Loss: 0.068044	Acc: 15.1% (1508/10000)
[Test]  Epoch: 7	Loss: 0.066435	Acc: 16.2% (1619/10000)
[Test]  Epoch: 8	Loss: 0.064841	Acc: 17.0% (1701/10000)
[Test]  Epoch: 9	Loss: 0.063540	Acc: 17.9% (1793/10000)
[Test]  Epoch: 10	Loss: 0.062380	Acc: 18.6% (1859/10000)
[Test]  Epoch: 11	Loss: 0.061541	Acc: 18.8% (1883/10000)
[Test]  Epoch: 12	Loss: 0.060642	Acc: 19.7% (1971/10000)
[Test]  Epoch: 13	Loss: 0.060127	Acc: 20.0% (1997/10000)
[Test]  Epoch: 14	Loss: 0.059669	Acc: 20.0% (2002/10000)
[Test]  Epoch: 15	Loss: 0.059064	Acc: 20.4% (2045/10000)
[Test]  Epoch: 16	Loss: 0.058603	Acc: 20.6% (2064/10000)
[Test]  Epoch: 17	Loss: 0.058158	Acc: 21.2% (2124/10000)
[Test]  Epoch: 18	Loss: 0.057997	Acc: 21.0% (2099/10000)
[Test]  Epoch: 19	Loss: 0.057647	Acc: 21.6% (2159/10000)
[Test]  Epoch: 20	Loss: 0.057256	Acc: 22.0% (2196/10000)
[Test]  Epoch: 21	Loss: 0.057013	Acc: 22.1% (2212/10000)
[Test]  Epoch: 22	Loss: 0.056723	Acc: 22.4% (2242/10000)
[Test]  Epoch: 23	Loss: 0.056547	Acc: 22.4% (2243/10000)
[Test]  Epoch: 24	Loss: 0.056370	Acc: 22.4% (2241/10000)
[Test]  Epoch: 25	Loss: 0.056296	Acc: 22.5% (2252/10000)
[Test]  Epoch: 26	Loss: 0.056061	Acc: 22.9% (2290/10000)
[Test]  Epoch: 27	Loss: 0.055870	Acc: 23.1% (2309/10000)
[Test]  Epoch: 28	Loss: 0.055714	Acc: 23.5% (2348/10000)
[Test]  Epoch: 29	Loss: 0.055551	Acc: 23.5% (2347/10000)
[Test]  Epoch: 30	Loss: 0.055365	Acc: 23.8% (2384/10000)
[Test]  Epoch: 31	Loss: 0.055365	Acc: 23.6% (2362/10000)
[Test]  Epoch: 32	Loss: 0.055308	Acc: 23.8% (2375/10000)
[Test]  Epoch: 33	Loss: 0.055219	Acc: 23.9% (2389/10000)
[Test]  Epoch: 34	Loss: 0.054980	Acc: 24.3% (2432/10000)
[Test]  Epoch: 35	Loss: 0.054843	Acc: 24.4% (2444/10000)
[Test]  Epoch: 36	Loss: 0.054927	Acc: 24.1% (2413/10000)
[Test]  Epoch: 37	Loss: 0.054844	Acc: 24.4% (2439/10000)
[Test]  Epoch: 38	Loss: 0.054582	Acc: 24.8% (2476/10000)
[Test]  Epoch: 39	Loss: 0.054617	Acc: 24.8% (2477/10000)
[Test]  Epoch: 40	Loss: 0.054534	Acc: 24.9% (2493/10000)
[Test]  Epoch: 41	Loss: 0.054360	Acc: 25.1% (2508/10000)
[Test]  Epoch: 42	Loss: 0.054354	Acc: 25.1% (2507/10000)
[Test]  Epoch: 43	Loss: 0.054207	Acc: 25.2% (2518/10000)
[Test]  Epoch: 44	Loss: 0.054347	Acc: 25.1% (2508/10000)
[Test]  Epoch: 45	Loss: 0.054113	Acc: 25.2% (2525/10000)
[Test]  Epoch: 46	Loss: 0.054187	Acc: 25.3% (2533/10000)
[Test]  Epoch: 47	Loss: 0.054009	Acc: 25.5% (2554/10000)
[Test]  Epoch: 48	Loss: 0.053937	Acc: 25.7% (2567/10000)
[Test]  Epoch: 49	Loss: 0.053910	Acc: 25.8% (2583/10000)
[Test]  Epoch: 50	Loss: 0.053875	Acc: 25.9% (2593/10000)
[Test]  Epoch: 51	Loss: 0.053824	Acc: 25.8% (2580/10000)
[Test]  Epoch: 52	Loss: 0.053800	Acc: 25.8% (2575/10000)
[Test]  Epoch: 53	Loss: 0.053729	Acc: 25.7% (2568/10000)
[Test]  Epoch: 54	Loss: 0.053764	Acc: 26.0% (2597/10000)
[Test]  Epoch: 55	Loss: 0.053686	Acc: 25.9% (2595/10000)
[Test]  Epoch: 56	Loss: 0.053634	Acc: 26.1% (2606/10000)
[Test]  Epoch: 57	Loss: 0.053615	Acc: 26.3% (2629/10000)
[Test]  Epoch: 58	Loss: 0.053378	Acc: 26.4% (2639/10000)
[Test]  Epoch: 59	Loss: 0.053468	Acc: 26.4% (2639/10000)
[Test]  Epoch: 60	Loss: 0.053471	Acc: 26.3% (2627/10000)
[Test]  Epoch: 61	Loss: 0.053510	Acc: 26.4% (2637/10000)
[Test]  Epoch: 62	Loss: 0.053473	Acc: 26.4% (2642/10000)
[Test]  Epoch: 63	Loss: 0.053449	Acc: 26.3% (2629/10000)
[Test]  Epoch: 64	Loss: 0.053392	Acc: 26.4% (2644/10000)
[Test]  Epoch: 65	Loss: 0.053480	Acc: 26.3% (2632/10000)
[Test]  Epoch: 66	Loss: 0.053435	Acc: 26.7% (2669/10000)
[Test]  Epoch: 67	Loss: 0.053446	Acc: 26.4% (2638/10000)
[Test]  Epoch: 68	Loss: 0.053471	Acc: 26.5% (2648/10000)
[Test]  Epoch: 69	Loss: 0.053343	Acc: 26.8% (2683/10000)
[Test]  Epoch: 70	Loss: 0.053377	Acc: 26.5% (2650/10000)
[Test]  Epoch: 71	Loss: 0.053496	Acc: 26.4% (2645/10000)
[Test]  Epoch: 72	Loss: 0.053491	Acc: 26.5% (2651/10000)
[Test]  Epoch: 73	Loss: 0.053369	Acc: 26.6% (2661/10000)
[Test]  Epoch: 74	Loss: 0.053320	Acc: 26.7% (2666/10000)
[Test]  Epoch: 75	Loss: 0.053349	Acc: 26.7% (2670/10000)
[Test]  Epoch: 76	Loss: 0.053387	Acc: 26.6% (2656/10000)
[Test]  Epoch: 77	Loss: 0.053326	Acc: 26.7% (2669/10000)
[Test]  Epoch: 78	Loss: 0.053388	Acc: 26.6% (2662/10000)
[Test]  Epoch: 79	Loss: 0.053285	Acc: 26.8% (2679/10000)
[Test]  Epoch: 80	Loss: 0.053420	Acc: 26.6% (2658/10000)
[Test]  Epoch: 81	Loss: 0.053390	Acc: 26.5% (2646/10000)
[Test]  Epoch: 82	Loss: 0.053397	Acc: 26.6% (2661/10000)
[Test]  Epoch: 83	Loss: 0.053330	Acc: 26.8% (2680/10000)
[Test]  Epoch: 84	Loss: 0.053349	Acc: 26.6% (2660/10000)
[Test]  Epoch: 85	Loss: 0.053353	Acc: 26.7% (2669/10000)
[Test]  Epoch: 86	Loss: 0.053349	Acc: 26.6% (2662/10000)
[Test]  Epoch: 87	Loss: 0.053379	Acc: 26.7% (2667/10000)
[Test]  Epoch: 88	Loss: 0.053369	Acc: 26.7% (2672/10000)
[Test]  Epoch: 89	Loss: 0.053375	Acc: 26.8% (2680/10000)
[Test]  Epoch: 90	Loss: 0.053331	Acc: 26.8% (2676/10000)
[Test]  Epoch: 91	Loss: 0.053280	Acc: 26.9% (2686/10000)
[Test]  Epoch: 92	Loss: 0.053288	Acc: 26.8% (2680/10000)
[Test]  Epoch: 93	Loss: 0.053316	Acc: 26.6% (2664/10000)
[Test]  Epoch: 94	Loss: 0.053317	Acc: 26.7% (2667/10000)
[Test]  Epoch: 95	Loss: 0.053356	Acc: 26.8% (2678/10000)
[Test]  Epoch: 96	Loss: 0.053257	Acc: 26.7% (2666/10000)
[Test]  Epoch: 97	Loss: 0.053360	Acc: 26.6% (2661/10000)
[Test]  Epoch: 98	Loss: 0.053401	Acc: 26.6% (2664/10000)
[Test]  Epoch: 99	Loss: 0.053229	Acc: 26.8% (2679/10000)
[Test]  Epoch: 100	Loss: 0.053251	Acc: 26.8% (2675/10000)
===========finish==========
['2024-08-19', '19:15:07.253816', '100', 'test', '0.05325089447498321', '26.75', '26.86']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/stl10-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.bias', 0.0), ('layer2.2.bn1.weight', -0.010447876527905464), ('layer3.2.bn1.weight', -0.013970321044325829), ('layer2.3.bn2.weight', -0.01429399847984314), ('layer3.3.bn1.weight', -0.016690541058778763), ('layer2.2.bn2.weight', -0.016847852617502213), ('layer3.2.bn2.weight', -0.02013958990573883), ('layer3.4.bn1.weight', -0.02432737872004509), ('layer2.3.bn1.weight', -0.024369338527321815), ('layer3.3.bn2.weight', -0.026474224403500557), ('layer4.2.bn3.weight', -0.034871265292167664), ('layer4.1.bn3.weight', -0.03496591001749039), ('layer3.1.bn1.weight', -0.03919896483421326), ('layer3.4.bn2.weight', -0.040124282240867615), ('layer3.1.bn2.weight', -0.04782972112298012), ('layer3.0.bn1.weight', -0.05207210034132004), ('layer3.0.bn2.weight', -0.06161466985940933), ('layer2.2.bn3.weight', -0.0714651495218277), ('layer1.2.bn1.weight', -0.07310782372951508), ('layer2.3.bn3.weight', -0.07582734525203705), ('layer2.1.bn1.weight', -0.08010005950927734), ('layer2.2.conv1.weight', -0.08565621823072433), ('layer2.2.conv2.weight', -0.08655427396297455), ('layer4.0.bn3.weight', -0.09778054803609848), ('layer1.2.bn2.weight', -0.10671118646860123), ('layer1.1.bn2.weight', -0.10916654765605927), ('layer2.0.bn2.weight', -0.11374861001968384), ('layer3.5.bn1.weight', -0.11512500792741776), ('layer1.1.bn1.weight', -0.1159583181142807), ('layer2.0.bn1.weight', -0.1186688244342804), ('layer4.0.downsample.1.weight', -0.1277942657470703), ('layer3.3.bn3.weight', -0.13239800930023193), ('layer2.1.bn2.weight', -0.13341102004051208), ('layer3.2.conv2.weight', -0.1422399878501892), ('layer2.3.conv2.weight', -0.14418552815914154), ('layer1.1.bn3.weight', -0.14487224817276), ('layer2.1.bn3.weight', -0.1486082524061203), ('layer3.2.bn3.weight', -0.1628154218196869), ('layer3.1.bn3.weight', -0.16423234343528748), ('layer3.2.conv1.weight', -0.17114993929862976), ('layer2.0.bn3.weight', -0.1736348271369934), ('layer1.2.bn3.weight', -0.17417562007904053), ('layer4.2.bn2.weight', -0.18895047903060913), ('layer2.0.downsample.1.weight', -0.19416072964668274), ('layer3.0.bn3.weight', -0.19985274970531464), ('layer3.4.bn3.weight', -0.20276272296905518), ('layer4.2.bn1.weight', -0.20777691900730133), ('layer3.5.bn2.weight', -0.21826055645942688), ('layer3.3.conv1.weight', -0.23337599635124207), ('layer1.0.bn1.weight', -0.25252702832221985), ('layer3.1.conv1.weight', -0.25581803917884827), ('layer2.3.conv3.weight', -0.25906336307525635), ('layer3.3.conv2.weight', -0.2639927864074707), ('layer1.0.downsample.1.weight', -0.26931947469711304), ('layer3.5.bn3.weight', -0.28672030568122864), ('layer3.4.conv1.weight', -0.32240810990333557), ('layer4.1.bn1.weight', -0.3241586685180664), ('layer1.0.bn2.weight', -0.3245476484298706), ('layer2.2.conv3.weight', -0.32725393772125244), ('layer1.0.bn3.weight', -0.35006022453308105), ('layer3.4.conv2.weight', -0.3696632981300354), ('layer3.0.conv2.weight', -0.37511488795280457), ('layer4.1.bn2.weight', -0.3851258158683777), ('layer3.0.downsample.1.weight', -0.43935665488243103), ('layer3.1.conv2.weight', -0.45080268383026123), ('layer2.3.conv1.weight', -0.46861007809638977), ('layer2.1.conv2.weight', -0.4793727397918701), ('layer2.1.conv1.weight', -0.48731938004493713), ('layer2.0.conv2.weight', -0.5249545574188232), ('layer1.2.conv2.weight', -0.5358426570892334), ('layer1.2.conv3.weight', -0.5779004693031311), ('layer3.3.conv3.weight', -0.6601115465164185), ('layer2.1.conv3.weight', -0.6937952041625977), ('layer1.1.conv3.weight', -0.6949151754379272), ('layer3.2.conv3.weight', -0.7062830328941345), ('layer2.0.conv3.weight', -0.83575439453125), ('layer1.1.conv1.weight', -0.8457716703414917), ('layer1.2.conv1.weight', -0.8484777212142944), ('layer1.1.conv2.weight', -0.9027978181838989), ('layer3.0.conv1.weight', -0.9991403818130493), ('layer4.0.bn2.weight', -1.046947717666626), ('layer3.1.conv3.weight', -1.0973482131958008), ('layer3.4.conv3.weight', -1.1185874938964844), ('bn1.weight', -1.1654080152511597), ('layer1.0.conv2.weight', -1.4381260871887207), ('layer2.0.conv1.weight', -1.489320158958435), ('layer3.0.conv3.weight', -1.5805532932281494), ('layer1.0.conv3.weight', -1.9659833908081055), ('layer4.0.bn1.weight', -2.0286383628845215), ('layer3.5.conv1.weight', -2.036158561706543), ('layer3.5.conv2.weight', -2.662327289581299), ('layer4.2.conv1.weight', -3.2338552474975586), ('layer3.5.conv3.weight', -3.338859796524048), ('layer4.2.conv2.weight', -3.7191967964172363), ('layer1.0.conv1.weight', -3.821730136871338), ('layer4.1.conv2.weight', -5.360354423522949), ('conv1.weight', -5.406210899353027), ('layer2.0.downsample.0.weight', -5.769124507904053), ('layer4.1.conv1.weight', -6.041759014129639), ('layer1.0.downsample.0.weight', -6.065620422363281), ('last_linear.weight', -6.293739318847656), ('layer4.1.conv3.weight', -6.4528279304504395), ('layer3.0.downsample.0.weight', -7.53508186340332), ('layer4.0.downsample.0.weight', -8.870584487915039), ('layer4.2.conv3.weight', -10.687698364257812), ('layer4.0.conv3.weight', -12.066431045532227), ('layer4.0.conv2.weight', -12.562368392944336), ('layer4.0.conv1.weight', -20.42839813232422)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('last_linear.bias', 0.0), ('layer2.2.conv1.weight', -0.08565621823072433), ('layer2.2.conv2.weight', -0.08655427396297455), ('layer3.2.conv2.weight', -0.1422399878501892), ('layer2.3.conv2.weight', -0.14418552815914154), ('layer3.2.conv1.weight', -0.17114993929862976), ('layer3.3.conv1.weight', -0.23337599635124207), ('layer3.1.conv1.weight', -0.25581803917884827), ('layer2.3.conv3.weight', -0.25906336307525635), ('layer3.3.conv2.weight', -0.2639927864074707), ('layer3.4.conv1.weight', -0.32240810990333557), ('layer2.2.conv3.weight', -0.32725393772125244), ('layer3.4.conv2.weight', -0.3696632981300354), ('layer3.0.conv2.weight', -0.37511488795280457), ('layer3.1.conv2.weight', -0.45080268383026123), ('layer2.3.conv1.weight', -0.46861007809638977), ('layer2.1.conv2.weight', -0.4793727397918701), ('layer2.1.conv1.weight', -0.48731938004493713), ('layer2.0.conv2.weight', -0.5249545574188232), ('layer1.2.conv2.weight', -0.5358426570892334), ('layer1.2.conv3.weight', -0.5779004693031311), ('layer3.3.conv3.weight', -0.6601115465164185), ('layer2.1.conv3.weight', -0.6937952041625977), ('layer1.1.conv3.weight', -0.6949151754379272), ('layer3.2.conv3.weight', -0.7062830328941345), ('layer2.0.conv3.weight', -0.83575439453125), ('layer1.1.conv1.weight', -0.8457716703414917), ('layer1.2.conv1.weight', -0.8484777212142944), ('layer1.1.conv2.weight', -0.9027978181838989), ('layer3.0.conv1.weight', -0.9991403818130493), ('layer3.1.conv3.weight', -1.0973482131958008), ('layer3.4.conv3.weight', -1.1185874938964844), ('layer1.0.conv2.weight', -1.4381260871887207), ('layer2.0.conv1.weight', -1.489320158958435), ('layer3.0.conv3.weight', -1.5805532932281494), ('layer1.0.conv3.weight', -1.9659833908081055), ('layer3.5.conv1.weight', -2.036158561706543), ('layer3.5.conv2.weight', -2.662327289581299), ('layer4.2.conv1.weight', -3.2338552474975586), ('layer3.5.conv3.weight', -3.338859796524048), ('layer4.2.conv2.weight', -3.7191967964172363), ('layer1.0.conv1.weight', -3.821730136871338), ('layer4.1.conv2.weight', -5.360354423522949), ('conv1.weight', -5.406210899353027), ('layer4.1.conv1.weight', -6.041759014129639), ('last_linear.weight', -6.293739318847656), ('layer4.1.conv3.weight', -6.4528279304504395), ('layer4.2.conv3.weight', -10.687698364257812), ('layer4.0.conv3.weight', -12.066431045532227), ('layer4.0.conv2.weight', -12.562368392944336), ('layer4.0.conv1.weight', -20.42839813232422)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.014417	Acc: 67.0% (5358/8000)
[Test]  Epoch: 2	Loss: 0.014409	Acc: 67.2% (5373/8000)
[Test]  Epoch: 3	Loss: 0.014490	Acc: 67.2% (5378/8000)
[Test]  Epoch: 4	Loss: 0.014471	Acc: 66.8% (5341/8000)
[Test]  Epoch: 5	Loss: 0.014453	Acc: 67.0% (5357/8000)
[Test]  Epoch: 6	Loss: 0.014466	Acc: 67.1% (5366/8000)
[Test]  Epoch: 7	Loss: 0.014401	Acc: 67.0% (5357/8000)
[Test]  Epoch: 8	Loss: 0.014355	Acc: 67.3% (5382/8000)
[Test]  Epoch: 9	Loss: 0.014384	Acc: 66.8% (5343/8000)
[Test]  Epoch: 10	Loss: 0.014339	Acc: 67.3% (5382/8000)
[Test]  Epoch: 11	Loss: 0.014436	Acc: 67.0% (5357/8000)
[Test]  Epoch: 12	Loss: 0.014291	Acc: 67.5% (5400/8000)
[Test]  Epoch: 13	Loss: 0.014323	Acc: 67.4% (5391/8000)
[Test]  Epoch: 14	Loss: 0.014310	Acc: 67.2% (5377/8000)
[Test]  Epoch: 15	Loss: 0.014385	Acc: 67.2% (5373/8000)
[Test]  Epoch: 16	Loss: 0.014336	Acc: 67.1% (5369/8000)
[Test]  Epoch: 17	Loss: 0.014300	Acc: 67.2% (5380/8000)
[Test]  Epoch: 18	Loss: 0.014374	Acc: 66.9% (5355/8000)
[Test]  Epoch: 19	Loss: 0.014366	Acc: 67.2% (5376/8000)
[Test]  Epoch: 20	Loss: 0.014450	Acc: 66.8% (5341/8000)
[Test]  Epoch: 21	Loss: 0.014349	Acc: 67.4% (5389/8000)
[Test]  Epoch: 22	Loss: 0.014378	Acc: 67.5% (5400/8000)
[Test]  Epoch: 23	Loss: 0.014273	Acc: 67.5% (5401/8000)
[Test]  Epoch: 24	Loss: 0.014323	Acc: 67.1% (5371/8000)
[Test]  Epoch: 25	Loss: 0.014419	Acc: 67.0% (5360/8000)
[Test]  Epoch: 26	Loss: 0.014325	Acc: 67.4% (5389/8000)
[Test]  Epoch: 27	Loss: 0.014314	Acc: 67.5% (5401/8000)
[Test]  Epoch: 28	Loss: 0.014321	Acc: 67.2% (5376/8000)
[Test]  Epoch: 29	Loss: 0.014321	Acc: 67.3% (5385/8000)
[Test]  Epoch: 30	Loss: 0.014340	Acc: 67.0% (5356/8000)
[Test]  Epoch: 31	Loss: 0.014312	Acc: 67.2% (5377/8000)
[Test]  Epoch: 32	Loss: 0.014282	Acc: 67.4% (5392/8000)
[Test]  Epoch: 33	Loss: 0.014323	Acc: 67.2% (5375/8000)
[Test]  Epoch: 34	Loss: 0.014253	Acc: 67.5% (5403/8000)
[Test]  Epoch: 35	Loss: 0.014269	Acc: 67.2% (5380/8000)
[Test]  Epoch: 36	Loss: 0.014243	Acc: 67.2% (5379/8000)
[Test]  Epoch: 37	Loss: 0.014269	Acc: 67.4% (5390/8000)
[Test]  Epoch: 38	Loss: 0.014342	Acc: 67.3% (5385/8000)
[Test]  Epoch: 39	Loss: 0.014280	Acc: 67.2% (5380/8000)
[Test]  Epoch: 40	Loss: 0.014299	Acc: 67.0% (5361/8000)
[Test]  Epoch: 41	Loss: 0.014246	Acc: 67.0% (5364/8000)
[Test]  Epoch: 42	Loss: 0.014342	Acc: 67.4% (5393/8000)
[Test]  Epoch: 43	Loss: 0.014300	Acc: 67.3% (5381/8000)
[Test]  Epoch: 44	Loss: 0.014276	Acc: 67.7% (5417/8000)
[Test]  Epoch: 45	Loss: 0.014282	Acc: 67.3% (5387/8000)
[Test]  Epoch: 46	Loss: 0.014304	Acc: 67.4% (5394/8000)
[Test]  Epoch: 47	Loss: 0.014294	Acc: 67.5% (5402/8000)
[Test]  Epoch: 48	Loss: 0.014324	Acc: 67.4% (5392/8000)
[Test]  Epoch: 49	Loss: 0.014281	Acc: 67.5% (5397/8000)
[Test]  Epoch: 50	Loss: 0.014269	Acc: 67.5% (5398/8000)
[Test]  Epoch: 51	Loss: 0.014294	Acc: 67.3% (5385/8000)
[Test]  Epoch: 52	Loss: 0.014304	Acc: 67.2% (5378/8000)
[Test]  Epoch: 53	Loss: 0.014277	Acc: 67.1% (5369/8000)
[Test]  Epoch: 54	Loss: 0.014312	Acc: 67.4% (5394/8000)
[Test]  Epoch: 55	Loss: 0.014306	Acc: 67.3% (5387/8000)
[Test]  Epoch: 56	Loss: 0.014291	Acc: 67.4% (5391/8000)
[Test]  Epoch: 57	Loss: 0.014244	Acc: 67.2% (5373/8000)
[Test]  Epoch: 58	Loss: 0.014280	Acc: 67.5% (5399/8000)
[Test]  Epoch: 59	Loss: 0.014314	Acc: 67.2% (5380/8000)
[Test]  Epoch: 60	Loss: 0.014325	Acc: 67.5% (5398/8000)
[Test]  Epoch: 61	Loss: 0.014312	Acc: 67.6% (5405/8000)
[Test]  Epoch: 62	Loss: 0.014310	Acc: 67.3% (5387/8000)
[Test]  Epoch: 63	Loss: 0.014289	Acc: 67.4% (5394/8000)
[Test]  Epoch: 64	Loss: 0.014291	Acc: 67.5% (5404/8000)
[Test]  Epoch: 65	Loss: 0.014280	Acc: 67.5% (5396/8000)
[Test]  Epoch: 66	Loss: 0.014291	Acc: 67.4% (5394/8000)
[Test]  Epoch: 67	Loss: 0.014293	Acc: 67.4% (5390/8000)
[Test]  Epoch: 68	Loss: 0.014292	Acc: 67.4% (5393/8000)
[Test]  Epoch: 69	Loss: 0.014272	Acc: 67.5% (5399/8000)
[Test]  Epoch: 70	Loss: 0.014300	Acc: 67.4% (5391/8000)
[Test]  Epoch: 71	Loss: 0.014281	Acc: 67.5% (5402/8000)
[Test]  Epoch: 72	Loss: 0.014278	Acc: 67.5% (5399/8000)
[Test]  Epoch: 73	Loss: 0.014277	Acc: 67.3% (5385/8000)
[Test]  Epoch: 74	Loss: 0.014281	Acc: 67.5% (5399/8000)
[Test]  Epoch: 75	Loss: 0.014280	Acc: 67.6% (5406/8000)
[Test]  Epoch: 76	Loss: 0.014294	Acc: 67.1% (5369/8000)
[Test]  Epoch: 77	Loss: 0.014276	Acc: 67.4% (5390/8000)
[Test]  Epoch: 78	Loss: 0.014290	Acc: 67.2% (5377/8000)
[Test]  Epoch: 79	Loss: 0.014282	Acc: 67.5% (5399/8000)
[Test]  Epoch: 80	Loss: 0.014275	Acc: 67.5% (5396/8000)
[Test]  Epoch: 81	Loss: 0.014278	Acc: 67.4% (5391/8000)
[Test]  Epoch: 82	Loss: 0.014350	Acc: 67.2% (5378/8000)
[Test]  Epoch: 83	Loss: 0.014275	Acc: 67.3% (5385/8000)
[Test]  Epoch: 84	Loss: 0.014285	Acc: 67.2% (5376/8000)
[Test]  Epoch: 85	Loss: 0.014283	Acc: 67.3% (5388/8000)
[Test]  Epoch: 86	Loss: 0.014291	Acc: 67.4% (5391/8000)
[Test]  Epoch: 87	Loss: 0.014291	Acc: 67.3% (5387/8000)
[Test]  Epoch: 88	Loss: 0.014285	Acc: 67.3% (5383/8000)
[Test]  Epoch: 89	Loss: 0.014282	Acc: 67.5% (5396/8000)
[Test]  Epoch: 90	Loss: 0.014284	Acc: 67.2% (5378/8000)
[Test]  Epoch: 91	Loss: 0.014301	Acc: 67.3% (5384/8000)
[Test]  Epoch: 92	Loss: 0.014289	Acc: 67.3% (5381/8000)
[Test]  Epoch: 93	Loss: 0.014269	Acc: 67.4% (5389/8000)
[Test]  Epoch: 94	Loss: 0.014276	Acc: 67.4% (5389/8000)
[Test]  Epoch: 95	Loss: 0.014273	Acc: 67.3% (5384/8000)
[Test]  Epoch: 96	Loss: 0.014258	Acc: 67.3% (5388/8000)
[Test]  Epoch: 97	Loss: 0.014288	Acc: 67.5% (5398/8000)
[Test]  Epoch: 98	Loss: 0.014282	Acc: 67.5% (5397/8000)
[Test]  Epoch: 99	Loss: 0.014285	Acc: 67.4% (5390/8000)
[Test]  Epoch: 100	Loss: 0.014283	Acc: 67.7% (5414/8000)
===========finish==========
['2024-08-19', '19:23:11.076499', '100', 'test', '0.01428286711871624', '67.675', '67.7125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.015480	Acc: 65.1% (5205/8000)
[Test]  Epoch: 2	Loss: 0.014765	Acc: 66.4% (5313/8000)
[Test]  Epoch: 3	Loss: 0.014752	Acc: 66.5% (5323/8000)
[Test]  Epoch: 4	Loss: 0.014698	Acc: 66.6% (5326/8000)
[Test]  Epoch: 5	Loss: 0.014638	Acc: 66.5% (5319/8000)
[Test]  Epoch: 6	Loss: 0.014697	Acc: 66.5% (5323/8000)
[Test]  Epoch: 7	Loss: 0.014558	Acc: 66.8% (5340/8000)
[Test]  Epoch: 8	Loss: 0.014498	Acc: 66.9% (5351/8000)
[Test]  Epoch: 9	Loss: 0.014541	Acc: 66.5% (5322/8000)
[Test]  Epoch: 10	Loss: 0.014474	Acc: 66.7% (5337/8000)
[Test]  Epoch: 11	Loss: 0.014563	Acc: 66.7% (5332/8000)
[Test]  Epoch: 12	Loss: 0.014428	Acc: 67.0% (5358/8000)
[Test]  Epoch: 13	Loss: 0.014441	Acc: 67.0% (5357/8000)
[Test]  Epoch: 14	Loss: 0.014446	Acc: 67.2% (5377/8000)
[Test]  Epoch: 15	Loss: 0.014477	Acc: 66.7% (5335/8000)
[Test]  Epoch: 16	Loss: 0.014469	Acc: 66.9% (5352/8000)
[Test]  Epoch: 17	Loss: 0.014415	Acc: 67.0% (5363/8000)
[Test]  Epoch: 18	Loss: 0.014496	Acc: 66.8% (5345/8000)
[Test]  Epoch: 19	Loss: 0.014479	Acc: 67.1% (5365/8000)
[Test]  Epoch: 20	Loss: 0.014586	Acc: 66.5% (5322/8000)
[Test]  Epoch: 21	Loss: 0.014452	Acc: 67.2% (5380/8000)
[Test]  Epoch: 22	Loss: 0.014484	Acc: 67.3% (5382/8000)
[Test]  Epoch: 23	Loss: 0.014382	Acc: 67.0% (5364/8000)
[Test]  Epoch: 24	Loss: 0.014411	Acc: 67.0% (5357/8000)
[Test]  Epoch: 25	Loss: 0.014530	Acc: 66.8% (5346/8000)
[Test]  Epoch: 26	Loss: 0.014415	Acc: 66.8% (5347/8000)
[Test]  Epoch: 27	Loss: 0.014395	Acc: 67.0% (5360/8000)
[Test]  Epoch: 28	Loss: 0.014413	Acc: 67.1% (5366/8000)
[Test]  Epoch: 29	Loss: 0.014418	Acc: 67.3% (5382/8000)
[Test]  Epoch: 30	Loss: 0.014433	Acc: 66.8% (5347/8000)
[Test]  Epoch: 31	Loss: 0.014406	Acc: 67.0% (5361/8000)
[Test]  Epoch: 32	Loss: 0.014380	Acc: 67.0% (5356/8000)
[Test]  Epoch: 33	Loss: 0.014434	Acc: 67.2% (5374/8000)
[Test]  Epoch: 34	Loss: 0.014351	Acc: 67.2% (5377/8000)
[Test]  Epoch: 35	Loss: 0.014358	Acc: 67.0% (5363/8000)
[Test]  Epoch: 36	Loss: 0.014329	Acc: 67.0% (5358/8000)
[Test]  Epoch: 37	Loss: 0.014371	Acc: 67.1% (5366/8000)
[Test]  Epoch: 38	Loss: 0.014431	Acc: 66.9% (5351/8000)
[Test]  Epoch: 39	Loss: 0.014354	Acc: 67.1% (5365/8000)
[Test]  Epoch: 40	Loss: 0.014396	Acc: 66.9% (5351/8000)
[Test]  Epoch: 41	Loss: 0.014332	Acc: 67.1% (5368/8000)
[Test]  Epoch: 42	Loss: 0.014424	Acc: 67.1% (5369/8000)
[Test]  Epoch: 43	Loss: 0.014384	Acc: 67.0% (5362/8000)
[Test]  Epoch: 44	Loss: 0.014367	Acc: 67.2% (5374/8000)
[Test]  Epoch: 45	Loss: 0.014379	Acc: 67.0% (5357/8000)
[Test]  Epoch: 46	Loss: 0.014399	Acc: 66.9% (5353/8000)
[Test]  Epoch: 47	Loss: 0.014362	Acc: 67.3% (5382/8000)
[Test]  Epoch: 48	Loss: 0.014420	Acc: 66.9% (5352/8000)
[Test]  Epoch: 49	Loss: 0.014375	Acc: 67.1% (5367/8000)
[Test]  Epoch: 50	Loss: 0.014345	Acc: 67.3% (5386/8000)
[Test]  Epoch: 51	Loss: 0.014374	Acc: 67.0% (5360/8000)
[Test]  Epoch: 52	Loss: 0.014399	Acc: 66.9% (5349/8000)
[Test]  Epoch: 53	Loss: 0.014361	Acc: 66.8% (5348/8000)
[Test]  Epoch: 54	Loss: 0.014400	Acc: 66.9% (5353/8000)
[Test]  Epoch: 55	Loss: 0.014371	Acc: 67.1% (5369/8000)
[Test]  Epoch: 56	Loss: 0.014370	Acc: 67.2% (5378/8000)
[Test]  Epoch: 57	Loss: 0.014329	Acc: 67.0% (5363/8000)
[Test]  Epoch: 58	Loss: 0.014377	Acc: 67.2% (5375/8000)
[Test]  Epoch: 59	Loss: 0.014402	Acc: 67.1% (5365/8000)
[Test]  Epoch: 60	Loss: 0.014405	Acc: 67.0% (5358/8000)
[Test]  Epoch: 61	Loss: 0.014392	Acc: 67.0% (5362/8000)
[Test]  Epoch: 62	Loss: 0.014388	Acc: 67.0% (5359/8000)
[Test]  Epoch: 63	Loss: 0.014369	Acc: 67.2% (5372/8000)
[Test]  Epoch: 64	Loss: 0.014371	Acc: 67.0% (5356/8000)
[Test]  Epoch: 65	Loss: 0.014357	Acc: 67.2% (5379/8000)
[Test]  Epoch: 66	Loss: 0.014377	Acc: 67.2% (5375/8000)
[Test]  Epoch: 67	Loss: 0.014371	Acc: 67.2% (5378/8000)
[Test]  Epoch: 68	Loss: 0.014371	Acc: 67.0% (5362/8000)
[Test]  Epoch: 69	Loss: 0.014349	Acc: 67.2% (5376/8000)
[Test]  Epoch: 70	Loss: 0.014374	Acc: 67.2% (5372/8000)
[Test]  Epoch: 71	Loss: 0.014361	Acc: 67.2% (5377/8000)
[Test]  Epoch: 72	Loss: 0.014357	Acc: 67.2% (5373/8000)
[Test]  Epoch: 73	Loss: 0.014360	Acc: 67.1% (5371/8000)
[Test]  Epoch: 74	Loss: 0.014363	Acc: 67.0% (5361/8000)
[Test]  Epoch: 75	Loss: 0.014356	Acc: 67.2% (5380/8000)
[Test]  Epoch: 76	Loss: 0.014370	Acc: 67.0% (5357/8000)
[Test]  Epoch: 77	Loss: 0.014355	Acc: 67.1% (5366/8000)
[Test]  Epoch: 78	Loss: 0.014372	Acc: 67.1% (5369/8000)
[Test]  Epoch: 79	Loss: 0.014358	Acc: 67.3% (5385/8000)
[Test]  Epoch: 80	Loss: 0.014354	Acc: 67.2% (5375/8000)
[Test]  Epoch: 81	Loss: 0.014358	Acc: 67.2% (5374/8000)
[Test]  Epoch: 82	Loss: 0.014423	Acc: 66.9% (5354/8000)
[Test]  Epoch: 83	Loss: 0.014355	Acc: 67.2% (5379/8000)
[Test]  Epoch: 84	Loss: 0.014367	Acc: 66.9% (5351/8000)
[Test]  Epoch: 85	Loss: 0.014365	Acc: 67.1% (5369/8000)
[Test]  Epoch: 86	Loss: 0.014370	Acc: 67.1% (5365/8000)
[Test]  Epoch: 87	Loss: 0.014364	Acc: 67.1% (5370/8000)
[Test]  Epoch: 88	Loss: 0.014363	Acc: 67.2% (5376/8000)
[Test]  Epoch: 89	Loss: 0.014357	Acc: 67.0% (5359/8000)
[Test]  Epoch: 90	Loss: 0.014366	Acc: 67.1% (5371/8000)
[Test]  Epoch: 91	Loss: 0.014383	Acc: 67.0% (5359/8000)
[Test]  Epoch: 92	Loss: 0.014374	Acc: 67.0% (5362/8000)
[Test]  Epoch: 93	Loss: 0.014351	Acc: 67.1% (5365/8000)
[Test]  Epoch: 94	Loss: 0.014353	Acc: 67.1% (5366/8000)
[Test]  Epoch: 95	Loss: 0.014352	Acc: 67.1% (5371/8000)
[Test]  Epoch: 96	Loss: 0.014338	Acc: 67.2% (5378/8000)
[Test]  Epoch: 97	Loss: 0.014372	Acc: 67.2% (5373/8000)
[Test]  Epoch: 98	Loss: 0.014361	Acc: 67.3% (5386/8000)
[Test]  Epoch: 99	Loss: 0.014355	Acc: 67.0% (5364/8000)
[Test]  Epoch: 100	Loss: 0.014359	Acc: 67.2% (5379/8000)
===========finish==========
['2024-08-19', '19:32:16.565127', '100', 'test', '0.014358901381492616', '67.2375', '67.325']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.016504	Acc: 62.9% (5029/8000)
[Test]  Epoch: 2	Loss: 0.015341	Acc: 65.3% (5223/8000)
[Test]  Epoch: 3	Loss: 0.015062	Acc: 66.0% (5282/8000)
[Test]  Epoch: 4	Loss: 0.014976	Acc: 66.2% (5295/8000)
[Test]  Epoch: 5	Loss: 0.014922	Acc: 66.0% (5277/8000)
[Test]  Epoch: 6	Loss: 0.014996	Acc: 65.7% (5253/8000)
[Test]  Epoch: 7	Loss: 0.014826	Acc: 66.4% (5310/8000)
[Test]  Epoch: 8	Loss: 0.014760	Acc: 66.3% (5303/8000)
[Test]  Epoch: 9	Loss: 0.014811	Acc: 66.1% (5287/8000)
[Test]  Epoch: 10	Loss: 0.014731	Acc: 66.4% (5312/8000)
[Test]  Epoch: 11	Loss: 0.014813	Acc: 66.2% (5295/8000)
[Test]  Epoch: 12	Loss: 0.014641	Acc: 66.6% (5325/8000)
[Test]  Epoch: 13	Loss: 0.014667	Acc: 66.2% (5297/8000)
[Test]  Epoch: 14	Loss: 0.014713	Acc: 66.2% (5299/8000)
[Test]  Epoch: 15	Loss: 0.014738	Acc: 66.5% (5318/8000)
[Test]  Epoch: 16	Loss: 0.014742	Acc: 66.3% (5306/8000)
[Test]  Epoch: 17	Loss: 0.014635	Acc: 66.7% (5335/8000)
[Test]  Epoch: 18	Loss: 0.014718	Acc: 66.3% (5304/8000)
[Test]  Epoch: 19	Loss: 0.014680	Acc: 66.6% (5328/8000)
[Test]  Epoch: 20	Loss: 0.014749	Acc: 66.4% (5315/8000)
[Test]  Epoch: 21	Loss: 0.014654	Acc: 66.7% (5337/8000)
[Test]  Epoch: 22	Loss: 0.014692	Acc: 66.7% (5333/8000)
[Test]  Epoch: 23	Loss: 0.014574	Acc: 66.8% (5341/8000)
[Test]  Epoch: 24	Loss: 0.014632	Acc: 66.5% (5317/8000)
[Test]  Epoch: 25	Loss: 0.014756	Acc: 66.6% (5331/8000)
[Test]  Epoch: 26	Loss: 0.014630	Acc: 66.7% (5337/8000)
[Test]  Epoch: 27	Loss: 0.014584	Acc: 66.8% (5340/8000)
[Test]  Epoch: 28	Loss: 0.014623	Acc: 66.9% (5351/8000)
[Test]  Epoch: 29	Loss: 0.014621	Acc: 66.8% (5345/8000)
[Test]  Epoch: 30	Loss: 0.014635	Acc: 66.6% (5330/8000)
[Test]  Epoch: 31	Loss: 0.014616	Acc: 66.8% (5345/8000)
[Test]  Epoch: 32	Loss: 0.014606	Acc: 66.6% (5331/8000)
[Test]  Epoch: 33	Loss: 0.014586	Acc: 67.0% (5361/8000)
[Test]  Epoch: 34	Loss: 0.014547	Acc: 66.9% (5353/8000)
[Test]  Epoch: 35	Loss: 0.014543	Acc: 67.1% (5368/8000)
[Test]  Epoch: 36	Loss: 0.014537	Acc: 67.0% (5364/8000)
[Test]  Epoch: 37	Loss: 0.014547	Acc: 66.9% (5350/8000)
[Test]  Epoch: 38	Loss: 0.014607	Acc: 66.7% (5332/8000)
[Test]  Epoch: 39	Loss: 0.014532	Acc: 66.8% (5342/8000)
[Test]  Epoch: 40	Loss: 0.014559	Acc: 66.7% (5333/8000)
[Test]  Epoch: 41	Loss: 0.014520	Acc: 66.9% (5355/8000)
[Test]  Epoch: 42	Loss: 0.014625	Acc: 66.6% (5329/8000)
[Test]  Epoch: 43	Loss: 0.014559	Acc: 67.0% (5358/8000)
[Test]  Epoch: 44	Loss: 0.014542	Acc: 67.0% (5360/8000)
[Test]  Epoch: 45	Loss: 0.014573	Acc: 67.0% (5359/8000)
[Test]  Epoch: 46	Loss: 0.014620	Acc: 66.9% (5354/8000)
[Test]  Epoch: 47	Loss: 0.014548	Acc: 67.2% (5372/8000)
[Test]  Epoch: 48	Loss: 0.014614	Acc: 66.6% (5325/8000)
[Test]  Epoch: 49	Loss: 0.014550	Acc: 66.9% (5353/8000)
[Test]  Epoch: 50	Loss: 0.014519	Acc: 67.0% (5360/8000)
[Test]  Epoch: 51	Loss: 0.014562	Acc: 66.9% (5350/8000)
[Test]  Epoch: 52	Loss: 0.014564	Acc: 66.9% (5351/8000)
[Test]  Epoch: 53	Loss: 0.014563	Acc: 66.8% (5342/8000)
[Test]  Epoch: 54	Loss: 0.014591	Acc: 67.0% (5358/8000)
[Test]  Epoch: 55	Loss: 0.014585	Acc: 66.9% (5353/8000)
[Test]  Epoch: 56	Loss: 0.014567	Acc: 66.9% (5355/8000)
[Test]  Epoch: 57	Loss: 0.014519	Acc: 67.1% (5365/8000)
[Test]  Epoch: 58	Loss: 0.014563	Acc: 66.9% (5354/8000)
[Test]  Epoch: 59	Loss: 0.014596	Acc: 66.8% (5347/8000)
[Test]  Epoch: 60	Loss: 0.014592	Acc: 66.9% (5352/8000)
[Test]  Epoch: 61	Loss: 0.014577	Acc: 66.9% (5351/8000)
[Test]  Epoch: 62	Loss: 0.014575	Acc: 66.8% (5344/8000)
[Test]  Epoch: 63	Loss: 0.014557	Acc: 67.0% (5361/8000)
[Test]  Epoch: 64	Loss: 0.014573	Acc: 66.9% (5355/8000)
[Test]  Epoch: 65	Loss: 0.014553	Acc: 66.8% (5346/8000)
[Test]  Epoch: 66	Loss: 0.014569	Acc: 66.9% (5349/8000)
[Test]  Epoch: 67	Loss: 0.014562	Acc: 66.9% (5352/8000)
[Test]  Epoch: 68	Loss: 0.014572	Acc: 66.8% (5348/8000)
[Test]  Epoch: 69	Loss: 0.014543	Acc: 67.1% (5369/8000)
[Test]  Epoch: 70	Loss: 0.014556	Acc: 66.8% (5347/8000)
[Test]  Epoch: 71	Loss: 0.014541	Acc: 67.0% (5363/8000)
[Test]  Epoch: 72	Loss: 0.014536	Acc: 66.9% (5350/8000)
[Test]  Epoch: 73	Loss: 0.014542	Acc: 66.9% (5353/8000)
[Test]  Epoch: 74	Loss: 0.014545	Acc: 66.8% (5341/8000)
[Test]  Epoch: 75	Loss: 0.014542	Acc: 67.0% (5362/8000)
[Test]  Epoch: 76	Loss: 0.014560	Acc: 66.7% (5338/8000)
[Test]  Epoch: 77	Loss: 0.014540	Acc: 67.0% (5359/8000)
[Test]  Epoch: 78	Loss: 0.014565	Acc: 66.8% (5342/8000)
[Test]  Epoch: 79	Loss: 0.014552	Acc: 67.0% (5356/8000)
[Test]  Epoch: 80	Loss: 0.014541	Acc: 67.0% (5356/8000)
[Test]  Epoch: 81	Loss: 0.014544	Acc: 67.0% (5364/8000)
[Test]  Epoch: 82	Loss: 0.014600	Acc: 66.9% (5352/8000)
[Test]  Epoch: 83	Loss: 0.014538	Acc: 66.8% (5345/8000)
[Test]  Epoch: 84	Loss: 0.014559	Acc: 66.9% (5354/8000)
[Test]  Epoch: 85	Loss: 0.014553	Acc: 66.8% (5346/8000)
[Test]  Epoch: 86	Loss: 0.014552	Acc: 66.9% (5349/8000)
[Test]  Epoch: 87	Loss: 0.014559	Acc: 66.8% (5348/8000)
[Test]  Epoch: 88	Loss: 0.014539	Acc: 66.9% (5349/8000)
[Test]  Epoch: 89	Loss: 0.014543	Acc: 66.7% (5339/8000)
[Test]  Epoch: 90	Loss: 0.014554	Acc: 67.0% (5357/8000)
[Test]  Epoch: 91	Loss: 0.014566	Acc: 66.7% (5337/8000)
[Test]  Epoch: 92	Loss: 0.014552	Acc: 66.9% (5353/8000)
[Test]  Epoch: 93	Loss: 0.014528	Acc: 67.1% (5369/8000)
[Test]  Epoch: 94	Loss: 0.014539	Acc: 67.0% (5357/8000)
[Test]  Epoch: 95	Loss: 0.014540	Acc: 67.0% (5359/8000)
[Test]  Epoch: 96	Loss: 0.014521	Acc: 66.9% (5350/8000)
[Test]  Epoch: 97	Loss: 0.014546	Acc: 67.0% (5362/8000)
[Test]  Epoch: 98	Loss: 0.014544	Acc: 66.9% (5353/8000)
[Test]  Epoch: 99	Loss: 0.014533	Acc: 67.0% (5361/8000)
[Test]  Epoch: 100	Loss: 0.014539	Acc: 67.0% (5361/8000)
===========finish==========
['2024-08-19', '19:41:40.378586', '100', 'test', '0.014538887687027454', '67.0125', '67.15']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer4.0.bn3.weight', 'layer1.2.bn2.weight', 'layer1.1.bn2.weight', 'layer2.0.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn1.weight', 'layer2.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer2.1.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.018793	Acc: 58.5% (4683/8000)
[Test]  Epoch: 2	Loss: 0.016499	Acc: 63.6% (5085/8000)
[Test]  Epoch: 3	Loss: 0.015789	Acc: 64.8% (5188/8000)
[Test]  Epoch: 4	Loss: 0.015801	Acc: 64.2% (5138/8000)
[Test]  Epoch: 5	Loss: 0.015613	Acc: 64.9% (5190/8000)
[Test]  Epoch: 6	Loss: 0.015606	Acc: 64.8% (5182/8000)
[Test]  Epoch: 7	Loss: 0.015390	Acc: 65.1% (5211/8000)
[Test]  Epoch: 8	Loss: 0.015376	Acc: 65.1% (5209/8000)
[Test]  Epoch: 9	Loss: 0.015358	Acc: 65.1% (5208/8000)
[Test]  Epoch: 10	Loss: 0.015277	Acc: 65.3% (5226/8000)
[Test]  Epoch: 11	Loss: 0.015383	Acc: 65.2% (5216/8000)
[Test]  Epoch: 12	Loss: 0.015166	Acc: 65.5% (5242/8000)
[Test]  Epoch: 13	Loss: 0.015184	Acc: 65.4% (5231/8000)
[Test]  Epoch: 14	Loss: 0.015217	Acc: 65.2% (5214/8000)
[Test]  Epoch: 15	Loss: 0.015264	Acc: 65.4% (5231/8000)
[Test]  Epoch: 16	Loss: 0.015192	Acc: 65.3% (5226/8000)
[Test]  Epoch: 17	Loss: 0.015092	Acc: 65.7% (5258/8000)
[Test]  Epoch: 18	Loss: 0.015210	Acc: 65.2% (5219/8000)
[Test]  Epoch: 19	Loss: 0.015132	Acc: 65.7% (5255/8000)
[Test]  Epoch: 20	Loss: 0.015267	Acc: 65.5% (5241/8000)
[Test]  Epoch: 21	Loss: 0.015088	Acc: 65.7% (5255/8000)
[Test]  Epoch: 22	Loss: 0.015143	Acc: 65.4% (5235/8000)
[Test]  Epoch: 23	Loss: 0.015026	Acc: 65.8% (5263/8000)
[Test]  Epoch: 24	Loss: 0.015059	Acc: 65.5% (5242/8000)
[Test]  Epoch: 25	Loss: 0.015240	Acc: 65.2% (5219/8000)
[Test]  Epoch: 26	Loss: 0.015066	Acc: 65.5% (5244/8000)
[Test]  Epoch: 27	Loss: 0.015018	Acc: 65.4% (5235/8000)
[Test]  Epoch: 28	Loss: 0.015039	Acc: 65.5% (5239/8000)
[Test]  Epoch: 29	Loss: 0.014971	Acc: 65.7% (5257/8000)
[Test]  Epoch: 30	Loss: 0.015003	Acc: 65.6% (5251/8000)
[Test]  Epoch: 31	Loss: 0.015008	Acc: 65.9% (5272/8000)
[Test]  Epoch: 32	Loss: 0.015019	Acc: 65.5% (5240/8000)
[Test]  Epoch: 33	Loss: 0.014974	Acc: 65.8% (5268/8000)
[Test]  Epoch: 34	Loss: 0.014896	Acc: 65.8% (5261/8000)
[Test]  Epoch: 35	Loss: 0.014977	Acc: 65.6% (5249/8000)
[Test]  Epoch: 36	Loss: 0.014971	Acc: 65.5% (5239/8000)
[Test]  Epoch: 37	Loss: 0.014915	Acc: 65.6% (5250/8000)
[Test]  Epoch: 38	Loss: 0.015029	Acc: 65.6% (5247/8000)
[Test]  Epoch: 39	Loss: 0.014881	Acc: 65.8% (5263/8000)
[Test]  Epoch: 40	Loss: 0.014905	Acc: 65.8% (5264/8000)
[Test]  Epoch: 41	Loss: 0.014926	Acc: 66.2% (5293/8000)
[Test]  Epoch: 42	Loss: 0.015004	Acc: 65.9% (5269/8000)
[Test]  Epoch: 43	Loss: 0.014898	Acc: 66.0% (5276/8000)
[Test]  Epoch: 44	Loss: 0.014930	Acc: 65.9% (5273/8000)
[Test]  Epoch: 45	Loss: 0.014897	Acc: 66.0% (5276/8000)
[Test]  Epoch: 46	Loss: 0.015002	Acc: 65.8% (5266/8000)
[Test]  Epoch: 47	Loss: 0.014920	Acc: 65.8% (5268/8000)
[Test]  Epoch: 48	Loss: 0.014983	Acc: 65.9% (5270/8000)
[Test]  Epoch: 49	Loss: 0.014938	Acc: 65.9% (5272/8000)
[Test]  Epoch: 50	Loss: 0.014882	Acc: 65.9% (5273/8000)
[Test]  Epoch: 51	Loss: 0.014915	Acc: 66.2% (5292/8000)
[Test]  Epoch: 52	Loss: 0.014929	Acc: 65.8% (5268/8000)
[Test]  Epoch: 53	Loss: 0.014905	Acc: 65.9% (5272/8000)
[Test]  Epoch: 54	Loss: 0.014945	Acc: 65.8% (5264/8000)
[Test]  Epoch: 55	Loss: 0.014931	Acc: 65.9% (5270/8000)
[Test]  Epoch: 56	Loss: 0.014937	Acc: 65.9% (5274/8000)
[Test]  Epoch: 57	Loss: 0.014842	Acc: 66.0% (5283/8000)
[Test]  Epoch: 58	Loss: 0.014899	Acc: 66.1% (5287/8000)
[Test]  Epoch: 59	Loss: 0.014946	Acc: 65.9% (5273/8000)
[Test]  Epoch: 60	Loss: 0.014908	Acc: 65.8% (5265/8000)
[Test]  Epoch: 61	Loss: 0.014905	Acc: 65.8% (5265/8000)
[Test]  Epoch: 62	Loss: 0.014903	Acc: 65.9% (5269/8000)
[Test]  Epoch: 63	Loss: 0.014885	Acc: 66.0% (5279/8000)
[Test]  Epoch: 64	Loss: 0.014884	Acc: 66.0% (5276/8000)
[Test]  Epoch: 65	Loss: 0.014873	Acc: 66.1% (5289/8000)
[Test]  Epoch: 66	Loss: 0.014900	Acc: 66.0% (5280/8000)
[Test]  Epoch: 67	Loss: 0.014898	Acc: 66.0% (5276/8000)
[Test]  Epoch: 68	Loss: 0.014889	Acc: 66.2% (5294/8000)
[Test]  Epoch: 69	Loss: 0.014869	Acc: 66.2% (5292/8000)
[Test]  Epoch: 70	Loss: 0.014898	Acc: 66.1% (5286/8000)
[Test]  Epoch: 71	Loss: 0.014880	Acc: 66.2% (5298/8000)
[Test]  Epoch: 72	Loss: 0.014861	Acc: 66.1% (5288/8000)
[Test]  Epoch: 73	Loss: 0.014871	Acc: 66.1% (5288/8000)
[Test]  Epoch: 74	Loss: 0.014880	Acc: 66.0% (5282/8000)
[Test]  Epoch: 75	Loss: 0.014874	Acc: 66.2% (5298/8000)
[Test]  Epoch: 76	Loss: 0.014897	Acc: 66.0% (5279/8000)
[Test]  Epoch: 77	Loss: 0.014871	Acc: 66.1% (5291/8000)
[Test]  Epoch: 78	Loss: 0.014890	Acc: 66.2% (5292/8000)
[Test]  Epoch: 79	Loss: 0.014882	Acc: 66.1% (5288/8000)
[Test]  Epoch: 80	Loss: 0.014873	Acc: 66.3% (5302/8000)
[Test]  Epoch: 81	Loss: 0.014874	Acc: 66.1% (5286/8000)
[Test]  Epoch: 82	Loss: 0.014933	Acc: 65.8% (5267/8000)
[Test]  Epoch: 83	Loss: 0.014871	Acc: 66.1% (5286/8000)
[Test]  Epoch: 84	Loss: 0.014877	Acc: 66.0% (5284/8000)
[Test]  Epoch: 85	Loss: 0.014882	Acc: 66.1% (5285/8000)
[Test]  Epoch: 86	Loss: 0.014877	Acc: 66.2% (5294/8000)
[Test]  Epoch: 87	Loss: 0.014883	Acc: 66.2% (5296/8000)
[Test]  Epoch: 88	Loss: 0.014859	Acc: 66.1% (5291/8000)
[Test]  Epoch: 89	Loss: 0.014879	Acc: 66.1% (5287/8000)
[Test]  Epoch: 90	Loss: 0.014873	Acc: 66.2% (5297/8000)
[Test]  Epoch: 91	Loss: 0.014887	Acc: 66.2% (5299/8000)
[Test]  Epoch: 92	Loss: 0.014870	Acc: 66.2% (5293/8000)
[Test]  Epoch: 93	Loss: 0.014867	Acc: 66.0% (5281/8000)
[Test]  Epoch: 94	Loss: 0.014875	Acc: 66.2% (5294/8000)
[Test]  Epoch: 95	Loss: 0.014850	Acc: 66.2% (5293/8000)
[Test]  Epoch: 96	Loss: 0.014842	Acc: 66.0% (5278/8000)
[Test]  Epoch: 97	Loss: 0.014874	Acc: 66.2% (5297/8000)
[Test]  Epoch: 98	Loss: 0.014872	Acc: 66.2% (5297/8000)
[Test]  Epoch: 99	Loss: 0.014860	Acc: 66.1% (5285/8000)
[Test]  Epoch: 100	Loss: 0.014870	Acc: 66.0% (5284/8000)
===========finish==========
['2024-08-19', '19:51:00.421310', '100', 'test', '0.014869856178760528', '66.05', '66.275']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer4.0.bn3.weight', 'layer1.2.bn2.weight', 'layer1.1.bn2.weight', 'layer2.0.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn1.weight', 'layer2.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer2.1.bn2.weight', 'layer3.2.conv2.weight', 'layer2.3.conv2.weight', 'layer1.1.bn3.weight', 'layer2.1.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer2.0.bn3.weight', 'layer1.2.bn3.weight', 'layer4.2.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.022465	Acc: 49.3% (3944/8000)
[Test]  Epoch: 2	Loss: 0.017845	Acc: 59.5% (4764/8000)
[Test]  Epoch: 3	Loss: 0.016879	Acc: 62.0% (4961/8000)
[Test]  Epoch: 4	Loss: 0.016672	Acc: 61.8% (4946/8000)
[Test]  Epoch: 5	Loss: 0.016513	Acc: 63.0% (5039/8000)
[Test]  Epoch: 6	Loss: 0.016585	Acc: 62.1% (4971/8000)
[Test]  Epoch: 7	Loss: 0.016236	Acc: 63.1% (5049/8000)
[Test]  Epoch: 8	Loss: 0.016359	Acc: 63.1% (5045/8000)
[Test]  Epoch: 9	Loss: 0.016187	Acc: 63.1% (5050/8000)
[Test]  Epoch: 10	Loss: 0.016113	Acc: 63.4% (5072/8000)
[Test]  Epoch: 11	Loss: 0.016174	Acc: 63.2% (5054/8000)
[Test]  Epoch: 12	Loss: 0.015977	Acc: 63.5% (5082/8000)
[Test]  Epoch: 13	Loss: 0.016004	Acc: 63.4% (5071/8000)
[Test]  Epoch: 14	Loss: 0.016019	Acc: 63.2% (5054/8000)
[Test]  Epoch: 15	Loss: 0.015993	Acc: 63.4% (5074/8000)
[Test]  Epoch: 16	Loss: 0.015946	Acc: 63.7% (5094/8000)
[Test]  Epoch: 17	Loss: 0.015915	Acc: 63.8% (5100/8000)
[Test]  Epoch: 18	Loss: 0.016037	Acc: 63.2% (5056/8000)
[Test]  Epoch: 19	Loss: 0.015889	Acc: 63.6% (5087/8000)
[Test]  Epoch: 20	Loss: 0.015963	Acc: 63.6% (5091/8000)
[Test]  Epoch: 21	Loss: 0.015824	Acc: 63.8% (5104/8000)
[Test]  Epoch: 22	Loss: 0.015872	Acc: 63.5% (5077/8000)
[Test]  Epoch: 23	Loss: 0.015809	Acc: 63.8% (5104/8000)
[Test]  Epoch: 24	Loss: 0.015800	Acc: 63.9% (5114/8000)
[Test]  Epoch: 25	Loss: 0.016016	Acc: 63.3% (5064/8000)
[Test]  Epoch: 26	Loss: 0.015765	Acc: 63.9% (5114/8000)
[Test]  Epoch: 27	Loss: 0.015724	Acc: 63.9% (5108/8000)
[Test]  Epoch: 28	Loss: 0.015755	Acc: 63.9% (5109/8000)
[Test]  Epoch: 29	Loss: 0.015746	Acc: 63.8% (5107/8000)
[Test]  Epoch: 30	Loss: 0.015802	Acc: 63.5% (5084/8000)
[Test]  Epoch: 31	Loss: 0.015784	Acc: 63.6% (5091/8000)
[Test]  Epoch: 32	Loss: 0.015778	Acc: 63.8% (5103/8000)
[Test]  Epoch: 33	Loss: 0.015756	Acc: 63.7% (5099/8000)
[Test]  Epoch: 34	Loss: 0.015685	Acc: 64.1% (5130/8000)
[Test]  Epoch: 35	Loss: 0.015727	Acc: 64.0% (5117/8000)
[Test]  Epoch: 36	Loss: 0.015666	Acc: 64.1% (5126/8000)
[Test]  Epoch: 37	Loss: 0.015661	Acc: 63.9% (5112/8000)
[Test]  Epoch: 38	Loss: 0.015712	Acc: 63.8% (5105/8000)
[Test]  Epoch: 39	Loss: 0.015613	Acc: 64.0% (5121/8000)
[Test]  Epoch: 40	Loss: 0.015639	Acc: 64.0% (5118/8000)
[Test]  Epoch: 41	Loss: 0.015658	Acc: 64.0% (5118/8000)
[Test]  Epoch: 42	Loss: 0.015679	Acc: 64.1% (5130/8000)
[Test]  Epoch: 43	Loss: 0.015600	Acc: 64.0% (5124/8000)
[Test]  Epoch: 44	Loss: 0.015660	Acc: 63.8% (5102/8000)
[Test]  Epoch: 45	Loss: 0.015561	Acc: 64.3% (5148/8000)
[Test]  Epoch: 46	Loss: 0.015700	Acc: 63.6% (5091/8000)
[Test]  Epoch: 47	Loss: 0.015647	Acc: 64.0% (5121/8000)
[Test]  Epoch: 48	Loss: 0.015682	Acc: 64.0% (5118/8000)
[Test]  Epoch: 49	Loss: 0.015643	Acc: 63.9% (5115/8000)
[Test]  Epoch: 50	Loss: 0.015619	Acc: 63.9% (5111/8000)
[Test]  Epoch: 51	Loss: 0.015609	Acc: 64.1% (5126/8000)
[Test]  Epoch: 52	Loss: 0.015608	Acc: 64.2% (5133/8000)
[Test]  Epoch: 53	Loss: 0.015602	Acc: 64.3% (5143/8000)
[Test]  Epoch: 54	Loss: 0.015606	Acc: 64.2% (5134/8000)
[Test]  Epoch: 55	Loss: 0.015593	Acc: 64.2% (5134/8000)
[Test]  Epoch: 56	Loss: 0.015590	Acc: 64.2% (5140/8000)
[Test]  Epoch: 57	Loss: 0.015544	Acc: 64.3% (5142/8000)
[Test]  Epoch: 58	Loss: 0.015563	Acc: 64.2% (5134/8000)
[Test]  Epoch: 59	Loss: 0.015631	Acc: 63.8% (5107/8000)
[Test]  Epoch: 60	Loss: 0.015647	Acc: 64.2% (5134/8000)
[Test]  Epoch: 61	Loss: 0.015607	Acc: 64.3% (5141/8000)
[Test]  Epoch: 62	Loss: 0.015583	Acc: 64.3% (5147/8000)
[Test]  Epoch: 63	Loss: 0.015579	Acc: 64.2% (5139/8000)
[Test]  Epoch: 64	Loss: 0.015576	Acc: 64.3% (5141/8000)
[Test]  Epoch: 65	Loss: 0.015553	Acc: 64.2% (5132/8000)
[Test]  Epoch: 66	Loss: 0.015583	Acc: 64.2% (5134/8000)
[Test]  Epoch: 67	Loss: 0.015583	Acc: 64.0% (5116/8000)
[Test]  Epoch: 68	Loss: 0.015579	Acc: 64.1% (5126/8000)
[Test]  Epoch: 69	Loss: 0.015560	Acc: 64.2% (5137/8000)
[Test]  Epoch: 70	Loss: 0.015566	Acc: 64.2% (5140/8000)
[Test]  Epoch: 71	Loss: 0.015562	Acc: 64.2% (5133/8000)
[Test]  Epoch: 72	Loss: 0.015550	Acc: 64.2% (5140/8000)
[Test]  Epoch: 73	Loss: 0.015554	Acc: 64.2% (5136/8000)
[Test]  Epoch: 74	Loss: 0.015564	Acc: 64.1% (5126/8000)
[Test]  Epoch: 75	Loss: 0.015558	Acc: 64.3% (5148/8000)
[Test]  Epoch: 76	Loss: 0.015554	Acc: 64.3% (5145/8000)
[Test]  Epoch: 77	Loss: 0.015545	Acc: 64.3% (5148/8000)
[Test]  Epoch: 78	Loss: 0.015569	Acc: 64.1% (5130/8000)
[Test]  Epoch: 79	Loss: 0.015565	Acc: 64.0% (5123/8000)
[Test]  Epoch: 80	Loss: 0.015552	Acc: 64.1% (5131/8000)
[Test]  Epoch: 81	Loss: 0.015568	Acc: 64.3% (5141/8000)
[Test]  Epoch: 82	Loss: 0.015609	Acc: 64.1% (5126/8000)
[Test]  Epoch: 83	Loss: 0.015550	Acc: 64.2% (5133/8000)
[Test]  Epoch: 84	Loss: 0.015571	Acc: 64.1% (5130/8000)
[Test]  Epoch: 85	Loss: 0.015552	Acc: 64.2% (5136/8000)
[Test]  Epoch: 86	Loss: 0.015558	Acc: 64.2% (5132/8000)
[Test]  Epoch: 87	Loss: 0.015549	Acc: 64.1% (5127/8000)
[Test]  Epoch: 88	Loss: 0.015557	Acc: 64.2% (5136/8000)
[Test]  Epoch: 89	Loss: 0.015560	Acc: 64.0% (5123/8000)
[Test]  Epoch: 90	Loss: 0.015565	Acc: 64.1% (5126/8000)
[Test]  Epoch: 91	Loss: 0.015564	Acc: 64.1% (5128/8000)
[Test]  Epoch: 92	Loss: 0.015554	Acc: 64.1% (5126/8000)
[Test]  Epoch: 93	Loss: 0.015550	Acc: 64.2% (5132/8000)
[Test]  Epoch: 94	Loss: 0.015583	Acc: 64.1% (5130/8000)
[Test]  Epoch: 95	Loss: 0.015547	Acc: 64.3% (5141/8000)
[Test]  Epoch: 96	Loss: 0.015531	Acc: 64.2% (5137/8000)
[Test]  Epoch: 97	Loss: 0.015555	Acc: 64.3% (5142/8000)
[Test]  Epoch: 98	Loss: 0.015553	Acc: 64.2% (5137/8000)
[Test]  Epoch: 99	Loss: 0.015547	Acc: 64.2% (5135/8000)
[Test]  Epoch: 100	Loss: 0.015560	Acc: 64.1% (5130/8000)
===========finish==========
['2024-08-19', '20:00:29.364612', '100', 'test', '0.015560017004609107', '64.125', '64.35']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer4.0.bn3.weight', 'layer1.2.bn2.weight', 'layer1.1.bn2.weight', 'layer2.0.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn1.weight', 'layer2.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer2.1.bn2.weight', 'layer3.2.conv2.weight', 'layer2.3.conv2.weight', 'layer1.1.bn3.weight', 'layer2.1.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer2.0.bn3.weight', 'layer1.2.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer3.4.bn3.weight', 'layer4.2.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.conv1.weight', 'layer1.0.bn1.weight', 'layer3.1.conv1.weight', 'layer2.3.conv3.weight', 'layer3.3.conv2.weight', 'layer1.0.downsample.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.031664	Acc: 26.2% (2097/8000)
[Test]  Epoch: 2	Loss: 0.020537	Acc: 52.9% (4231/8000)
[Test]  Epoch: 3	Loss: 0.017966	Acc: 58.6% (4692/8000)
[Test]  Epoch: 4	Loss: 0.017808	Acc: 59.3% (4747/8000)
[Test]  Epoch: 5	Loss: 0.017273	Acc: 61.2% (4899/8000)
[Test]  Epoch: 6	Loss: 0.017194	Acc: 60.7% (4854/8000)
[Test]  Epoch: 7	Loss: 0.016801	Acc: 61.8% (4945/8000)
[Test]  Epoch: 8	Loss: 0.016898	Acc: 61.9% (4953/8000)
[Test]  Epoch: 9	Loss: 0.016820	Acc: 61.7% (4934/8000)
[Test]  Epoch: 10	Loss: 0.016678	Acc: 61.9% (4954/8000)
[Test]  Epoch: 11	Loss: 0.016704	Acc: 62.0% (4959/8000)
[Test]  Epoch: 12	Loss: 0.016533	Acc: 62.3% (4986/8000)
[Test]  Epoch: 13	Loss: 0.016539	Acc: 61.9% (4950/8000)
[Test]  Epoch: 14	Loss: 0.016743	Acc: 61.6% (4926/8000)
[Test]  Epoch: 15	Loss: 0.016581	Acc: 62.2% (4973/8000)
[Test]  Epoch: 16	Loss: 0.016488	Acc: 62.5% (5002/8000)
[Test]  Epoch: 17	Loss: 0.016483	Acc: 62.6% (5007/8000)
[Test]  Epoch: 18	Loss: 0.016490	Acc: 61.8% (4947/8000)
[Test]  Epoch: 19	Loss: 0.016385	Acc: 62.8% (5020/8000)
[Test]  Epoch: 20	Loss: 0.016435	Acc: 62.5% (4996/8000)
[Test]  Epoch: 21	Loss: 0.016408	Acc: 62.5% (5004/8000)
[Test]  Epoch: 22	Loss: 0.016398	Acc: 62.2% (4974/8000)
[Test]  Epoch: 23	Loss: 0.016349	Acc: 62.8% (5024/8000)
[Test]  Epoch: 24	Loss: 0.016349	Acc: 62.4% (4993/8000)
[Test]  Epoch: 25	Loss: 0.016608	Acc: 62.2% (4976/8000)
[Test]  Epoch: 26	Loss: 0.016370	Acc: 62.4% (4991/8000)
[Test]  Epoch: 27	Loss: 0.016280	Acc: 62.5% (5004/8000)
[Test]  Epoch: 28	Loss: 0.016314	Acc: 62.3% (4982/8000)
[Test]  Epoch: 29	Loss: 0.016247	Acc: 62.6% (5010/8000)
[Test]  Epoch: 30	Loss: 0.016305	Acc: 62.9% (5035/8000)
[Test]  Epoch: 31	Loss: 0.016305	Acc: 62.7% (5014/8000)
[Test]  Epoch: 32	Loss: 0.016220	Acc: 62.9% (5033/8000)
[Test]  Epoch: 33	Loss: 0.016361	Acc: 62.1% (4965/8000)
[Test]  Epoch: 34	Loss: 0.016219	Acc: 62.7% (5013/8000)
[Test]  Epoch: 35	Loss: 0.016328	Acc: 62.5% (5001/8000)
[Test]  Epoch: 36	Loss: 0.016206	Acc: 62.6% (5007/8000)
[Test]  Epoch: 37	Loss: 0.016237	Acc: 62.7% (5013/8000)
[Test]  Epoch: 38	Loss: 0.016321	Acc: 62.4% (4991/8000)
[Test]  Epoch: 39	Loss: 0.016220	Acc: 62.9% (5034/8000)
[Test]  Epoch: 40	Loss: 0.016191	Acc: 62.9% (5033/8000)
[Test]  Epoch: 41	Loss: 0.016182	Acc: 62.5% (4999/8000)
[Test]  Epoch: 42	Loss: 0.016195	Acc: 62.6% (5009/8000)
[Test]  Epoch: 43	Loss: 0.016153	Acc: 62.9% (5030/8000)
[Test]  Epoch: 44	Loss: 0.016214	Acc: 62.6% (5005/8000)
[Test]  Epoch: 45	Loss: 0.016098	Acc: 62.9% (5029/8000)
[Test]  Epoch: 46	Loss: 0.016226	Acc: 62.4% (4995/8000)
[Test]  Epoch: 47	Loss: 0.016239	Acc: 62.8% (5024/8000)
[Test]  Epoch: 48	Loss: 0.016221	Acc: 62.5% (4999/8000)
[Test]  Epoch: 49	Loss: 0.016173	Acc: 62.4% (4992/8000)
[Test]  Epoch: 50	Loss: 0.016169	Acc: 62.8% (5023/8000)
[Test]  Epoch: 51	Loss: 0.016136	Acc: 62.6% (5009/8000)
[Test]  Epoch: 52	Loss: 0.016142	Acc: 62.7% (5016/8000)
[Test]  Epoch: 53	Loss: 0.016217	Acc: 62.5% (5001/8000)
[Test]  Epoch: 54	Loss: 0.016128	Acc: 62.9% (5028/8000)
[Test]  Epoch: 55	Loss: 0.016201	Acc: 63.1% (5047/8000)
[Test]  Epoch: 56	Loss: 0.016146	Acc: 63.1% (5049/8000)
[Test]  Epoch: 57	Loss: 0.016071	Acc: 63.2% (5053/8000)
[Test]  Epoch: 58	Loss: 0.016145	Acc: 62.7% (5017/8000)
[Test]  Epoch: 59	Loss: 0.016186	Acc: 63.0% (5043/8000)
[Test]  Epoch: 60	Loss: 0.016113	Acc: 62.9% (5029/8000)
[Test]  Epoch: 61	Loss: 0.016096	Acc: 63.0% (5040/8000)
[Test]  Epoch: 62	Loss: 0.016088	Acc: 63.1% (5047/8000)
[Test]  Epoch: 63	Loss: 0.016061	Acc: 63.1% (5052/8000)
[Test]  Epoch: 64	Loss: 0.016066	Acc: 62.9% (5034/8000)
[Test]  Epoch: 65	Loss: 0.016051	Acc: 63.0% (5044/8000)
[Test]  Epoch: 66	Loss: 0.016095	Acc: 62.9% (5033/8000)
[Test]  Epoch: 67	Loss: 0.016078	Acc: 63.0% (5038/8000)
[Test]  Epoch: 68	Loss: 0.016076	Acc: 62.9% (5034/8000)
[Test]  Epoch: 69	Loss: 0.016058	Acc: 63.0% (5038/8000)
[Test]  Epoch: 70	Loss: 0.016064	Acc: 63.2% (5054/8000)
[Test]  Epoch: 71	Loss: 0.016049	Acc: 63.0% (5043/8000)
[Test]  Epoch: 72	Loss: 0.016037	Acc: 63.0% (5037/8000)
[Test]  Epoch: 73	Loss: 0.016051	Acc: 63.0% (5040/8000)
[Test]  Epoch: 74	Loss: 0.016051	Acc: 63.0% (5043/8000)
[Test]  Epoch: 75	Loss: 0.016058	Acc: 63.1% (5048/8000)
[Test]  Epoch: 76	Loss: 0.016049	Acc: 62.9% (5032/8000)
[Test]  Epoch: 77	Loss: 0.016042	Acc: 63.0% (5036/8000)
[Test]  Epoch: 78	Loss: 0.016073	Acc: 63.0% (5040/8000)
[Test]  Epoch: 79	Loss: 0.016052	Acc: 63.1% (5050/8000)
[Test]  Epoch: 80	Loss: 0.016054	Acc: 62.9% (5034/8000)
[Test]  Epoch: 81	Loss: 0.016057	Acc: 63.0% (5039/8000)
[Test]  Epoch: 82	Loss: 0.016115	Acc: 63.0% (5036/8000)
[Test]  Epoch: 83	Loss: 0.016040	Acc: 63.0% (5041/8000)
[Test]  Epoch: 84	Loss: 0.016067	Acc: 63.0% (5040/8000)
[Test]  Epoch: 85	Loss: 0.016070	Acc: 63.0% (5038/8000)
[Test]  Epoch: 86	Loss: 0.016071	Acc: 63.0% (5036/8000)
[Test]  Epoch: 87	Loss: 0.016069	Acc: 63.1% (5047/8000)
[Test]  Epoch: 88	Loss: 0.016060	Acc: 62.9% (5030/8000)
[Test]  Epoch: 89	Loss: 0.016075	Acc: 63.0% (5039/8000)
[Test]  Epoch: 90	Loss: 0.016062	Acc: 62.8% (5023/8000)
[Test]  Epoch: 91	Loss: 0.016072	Acc: 63.0% (5044/8000)
[Test]  Epoch: 92	Loss: 0.016070	Acc: 62.9% (5031/8000)
[Test]  Epoch: 93	Loss: 0.016070	Acc: 62.8% (5026/8000)
[Test]  Epoch: 94	Loss: 0.016073	Acc: 62.9% (5029/8000)
[Test]  Epoch: 95	Loss: 0.016061	Acc: 62.9% (5030/8000)
[Test]  Epoch: 96	Loss: 0.016038	Acc: 62.9% (5028/8000)
[Test]  Epoch: 97	Loss: 0.016090	Acc: 62.9% (5032/8000)
[Test]  Epoch: 98	Loss: 0.016053	Acc: 63.0% (5042/8000)
[Test]  Epoch: 99	Loss: 0.016067	Acc: 63.0% (5038/8000)
[Test]  Epoch: 100	Loss: 0.016069	Acc: 63.0% (5039/8000)
===========finish==========
['2024-08-19', '20:09:54.479852', '100', 'test', '0.01606928226351738', '62.9875', '63.175']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer4.0.bn3.weight', 'layer1.2.bn2.weight', 'layer1.1.bn2.weight', 'layer2.0.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn1.weight', 'layer2.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer2.1.bn2.weight', 'layer3.2.conv2.weight', 'layer2.3.conv2.weight', 'layer1.1.bn3.weight', 'layer2.1.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer2.0.bn3.weight', 'layer1.2.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer3.4.bn3.weight', 'layer4.2.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.conv1.weight', 'layer1.0.bn1.weight', 'layer3.1.conv1.weight', 'layer2.3.conv3.weight', 'layer3.3.conv2.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer3.4.conv1.weight', 'layer4.1.bn1.weight', 'layer1.0.bn2.weight', 'layer2.2.conv3.weight', 'layer1.0.bn3.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer4.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.040897	Acc: 14.5% (1158/8000)
[Test]  Epoch: 2	Loss: 0.025575	Acc: 39.5% (3158/8000)
[Test]  Epoch: 3	Loss: 0.020125	Acc: 53.4% (4272/8000)
[Test]  Epoch: 4	Loss: 0.019554	Acc: 55.1% (4408/8000)
[Test]  Epoch: 5	Loss: 0.019647	Acc: 54.8% (4380/8000)
[Test]  Epoch: 6	Loss: 0.019572	Acc: 55.0% (4397/8000)
[Test]  Epoch: 7	Loss: 0.019031	Acc: 56.4% (4513/8000)
[Test]  Epoch: 8	Loss: 0.018982	Acc: 56.7% (4535/8000)
[Test]  Epoch: 9	Loss: 0.018992	Acc: 56.4% (4512/8000)
[Test]  Epoch: 10	Loss: 0.018725	Acc: 56.9% (4548/8000)
[Test]  Epoch: 11	Loss: 0.018755	Acc: 57.0% (4561/8000)
[Test]  Epoch: 12	Loss: 0.018603	Acc: 57.3% (4582/8000)
[Test]  Epoch: 13	Loss: 0.018585	Acc: 57.2% (4577/8000)
[Test]  Epoch: 14	Loss: 0.018515	Acc: 57.3% (4585/8000)
[Test]  Epoch: 15	Loss: 0.018572	Acc: 57.3% (4583/8000)
[Test]  Epoch: 16	Loss: 0.018449	Acc: 57.2% (4578/8000)
[Test]  Epoch: 17	Loss: 0.018367	Acc: 57.7% (4614/8000)
[Test]  Epoch: 18	Loss: 0.018414	Acc: 57.5% (4604/8000)
[Test]  Epoch: 19	Loss: 0.018331	Acc: 57.6% (4609/8000)
[Test]  Epoch: 20	Loss: 0.018307	Acc: 57.9% (4631/8000)
[Test]  Epoch: 21	Loss: 0.018290	Acc: 57.6% (4610/8000)
[Test]  Epoch: 22	Loss: 0.018151	Acc: 58.3% (4664/8000)
[Test]  Epoch: 23	Loss: 0.018189	Acc: 58.0% (4641/8000)
[Test]  Epoch: 24	Loss: 0.018129	Acc: 58.0% (4643/8000)
[Test]  Epoch: 25	Loss: 0.018480	Acc: 57.4% (4591/8000)
[Test]  Epoch: 26	Loss: 0.018245	Acc: 57.8% (4623/8000)
[Test]  Epoch: 27	Loss: 0.018199	Acc: 57.8% (4625/8000)
[Test]  Epoch: 28	Loss: 0.018086	Acc: 58.2% (4653/8000)
[Test]  Epoch: 29	Loss: 0.017978	Acc: 58.7% (4697/8000)
[Test]  Epoch: 30	Loss: 0.018020	Acc: 57.8% (4623/8000)
[Test]  Epoch: 31	Loss: 0.018042	Acc: 58.4% (4675/8000)
[Test]  Epoch: 32	Loss: 0.017923	Acc: 58.6% (4686/8000)
[Test]  Epoch: 33	Loss: 0.017996	Acc: 58.5% (4680/8000)
[Test]  Epoch: 34	Loss: 0.018177	Acc: 57.9% (4629/8000)
[Test]  Epoch: 35	Loss: 0.018209	Acc: 58.0% (4642/8000)
[Test]  Epoch: 36	Loss: 0.017913	Acc: 58.8% (4703/8000)
[Test]  Epoch: 37	Loss: 0.017885	Acc: 58.8% (4701/8000)
[Test]  Epoch: 38	Loss: 0.018004	Acc: 58.3% (4667/8000)
[Test]  Epoch: 39	Loss: 0.017792	Acc: 58.9% (4708/8000)
[Test]  Epoch: 40	Loss: 0.018072	Acc: 58.3% (4663/8000)
[Test]  Epoch: 41	Loss: 0.017829	Acc: 58.8% (4702/8000)
[Test]  Epoch: 42	Loss: 0.017906	Acc: 58.5% (4682/8000)
[Test]  Epoch: 43	Loss: 0.017710	Acc: 59.4% (4752/8000)
[Test]  Epoch: 44	Loss: 0.017891	Acc: 58.9% (4710/8000)
[Test]  Epoch: 45	Loss: 0.017748	Acc: 59.5% (4758/8000)
[Test]  Epoch: 46	Loss: 0.017803	Acc: 59.1% (4725/8000)
[Test]  Epoch: 47	Loss: 0.017738	Acc: 59.1% (4727/8000)
[Test]  Epoch: 48	Loss: 0.017866	Acc: 58.6% (4690/8000)
[Test]  Epoch: 49	Loss: 0.017852	Acc: 58.7% (4699/8000)
[Test]  Epoch: 50	Loss: 0.017705	Acc: 59.2% (4740/8000)
[Test]  Epoch: 51	Loss: 0.017707	Acc: 59.0% (4722/8000)
[Test]  Epoch: 52	Loss: 0.017716	Acc: 59.1% (4727/8000)
[Test]  Epoch: 53	Loss: 0.017882	Acc: 58.8% (4702/8000)
[Test]  Epoch: 54	Loss: 0.017772	Acc: 58.8% (4707/8000)
[Test]  Epoch: 55	Loss: 0.017738	Acc: 59.4% (4748/8000)
[Test]  Epoch: 56	Loss: 0.017647	Acc: 59.4% (4751/8000)
[Test]  Epoch: 57	Loss: 0.017732	Acc: 59.0% (4723/8000)
[Test]  Epoch: 58	Loss: 0.017654	Acc: 59.2% (4738/8000)
[Test]  Epoch: 59	Loss: 0.017839	Acc: 58.9% (4714/8000)
[Test]  Epoch: 60	Loss: 0.017777	Acc: 59.0% (4723/8000)
[Test]  Epoch: 61	Loss: 0.017732	Acc: 59.2% (4736/8000)
[Test]  Epoch: 62	Loss: 0.017737	Acc: 59.2% (4733/8000)
[Test]  Epoch: 63	Loss: 0.017689	Acc: 59.1% (4731/8000)
[Test]  Epoch: 64	Loss: 0.017650	Acc: 59.2% (4737/8000)
[Test]  Epoch: 65	Loss: 0.017662	Acc: 59.0% (4724/8000)
[Test]  Epoch: 66	Loss: 0.017693	Acc: 59.2% (4739/8000)
[Test]  Epoch: 67	Loss: 0.017698	Acc: 59.0% (4722/8000)
[Test]  Epoch: 68	Loss: 0.017676	Acc: 58.9% (4714/8000)
[Test]  Epoch: 69	Loss: 0.017665	Acc: 59.2% (4738/8000)
[Test]  Epoch: 70	Loss: 0.017652	Acc: 59.2% (4738/8000)
[Test]  Epoch: 71	Loss: 0.017658	Acc: 59.4% (4750/8000)
[Test]  Epoch: 72	Loss: 0.017633	Acc: 59.4% (4749/8000)
[Test]  Epoch: 73	Loss: 0.017667	Acc: 59.2% (4739/8000)
[Test]  Epoch: 74	Loss: 0.017683	Acc: 59.2% (4740/8000)
[Test]  Epoch: 75	Loss: 0.017659	Acc: 59.4% (4754/8000)
[Test]  Epoch: 76	Loss: 0.017666	Acc: 59.2% (4735/8000)
[Test]  Epoch: 77	Loss: 0.017637	Acc: 59.2% (4738/8000)
[Test]  Epoch: 78	Loss: 0.017657	Acc: 59.1% (4728/8000)
[Test]  Epoch: 79	Loss: 0.017643	Acc: 58.9% (4715/8000)
[Test]  Epoch: 80	Loss: 0.017630	Acc: 59.2% (4740/8000)
[Test]  Epoch: 81	Loss: 0.017641	Acc: 59.3% (4746/8000)
[Test]  Epoch: 82	Loss: 0.017711	Acc: 59.1% (4729/8000)
[Test]  Epoch: 83	Loss: 0.017620	Acc: 59.2% (4740/8000)
[Test]  Epoch: 84	Loss: 0.017655	Acc: 59.3% (4746/8000)
[Test]  Epoch: 85	Loss: 0.017645	Acc: 59.2% (4733/8000)
[Test]  Epoch: 86	Loss: 0.017638	Acc: 59.0% (4724/8000)
[Test]  Epoch: 87	Loss: 0.017624	Acc: 59.1% (4727/8000)
[Test]  Epoch: 88	Loss: 0.017645	Acc: 59.4% (4748/8000)
[Test]  Epoch: 89	Loss: 0.017631	Acc: 59.3% (4747/8000)
[Test]  Epoch: 90	Loss: 0.017635	Acc: 59.4% (4752/8000)
[Test]  Epoch: 91	Loss: 0.017646	Acc: 59.1% (4730/8000)
[Test]  Epoch: 92	Loss: 0.017631	Acc: 59.5% (4761/8000)
[Test]  Epoch: 93	Loss: 0.017641	Acc: 59.3% (4744/8000)
[Test]  Epoch: 94	Loss: 0.017661	Acc: 59.1% (4726/8000)
[Test]  Epoch: 95	Loss: 0.017667	Acc: 59.2% (4733/8000)
[Test]  Epoch: 96	Loss: 0.017616	Acc: 59.5% (4758/8000)
[Test]  Epoch: 97	Loss: 0.017666	Acc: 59.4% (4750/8000)
[Test]  Epoch: 98	Loss: 0.017626	Acc: 59.2% (4734/8000)
[Test]  Epoch: 99	Loss: 0.017647	Acc: 59.4% (4752/8000)
[Test]  Epoch: 100	Loss: 0.017647	Acc: 59.3% (4741/8000)
===========finish==========
['2024-08-19', '20:19:16.026700', '100', 'test', '0.01764674337953329', '59.2625', '59.5125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer4.0.bn3.weight', 'layer1.2.bn2.weight', 'layer1.1.bn2.weight', 'layer2.0.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn1.weight', 'layer2.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer2.1.bn2.weight', 'layer3.2.conv2.weight', 'layer2.3.conv2.weight', 'layer1.1.bn3.weight', 'layer2.1.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer2.0.bn3.weight', 'layer1.2.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer3.4.bn3.weight', 'layer4.2.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.conv1.weight', 'layer1.0.bn1.weight', 'layer3.1.conv1.weight', 'layer2.3.conv3.weight', 'layer3.3.conv2.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer3.4.conv1.weight', 'layer4.1.bn1.weight', 'layer1.0.bn2.weight', 'layer2.2.conv3.weight', 'layer1.0.bn3.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer4.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer2.3.conv1.weight', 'layer2.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.0.conv2.weight', 'layer1.2.conv2.weight', 'layer1.2.conv3.weight', 'layer3.3.conv3.weight', 'layer2.1.conv3.weight', 'layer1.1.conv3.weight', 'layer3.2.conv3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036112	Acc: 20.0% (1598/8000)
[Test]  Epoch: 2	Loss: 0.026758	Acc: 39.6% (3165/8000)
[Test]  Epoch: 3	Loss: 0.021173	Acc: 50.6% (4049/8000)
[Test]  Epoch: 4	Loss: 0.020056	Acc: 53.4% (4275/8000)
[Test]  Epoch: 5	Loss: 0.020002	Acc: 53.8% (4307/8000)
[Test]  Epoch: 6	Loss: 0.020112	Acc: 53.4% (4268/8000)
[Test]  Epoch: 7	Loss: 0.019996	Acc: 54.4% (4348/8000)
[Test]  Epoch: 8	Loss: 0.019747	Acc: 54.7% (4373/8000)
[Test]  Epoch: 9	Loss: 0.019862	Acc: 54.5% (4364/8000)
[Test]  Epoch: 10	Loss: 0.019350	Acc: 54.9% (4393/8000)
[Test]  Epoch: 11	Loss: 0.019371	Acc: 55.3% (4421/8000)
[Test]  Epoch: 12	Loss: 0.019391	Acc: 55.5% (4440/8000)
[Test]  Epoch: 13	Loss: 0.019268	Acc: 55.6% (4452/8000)
[Test]  Epoch: 14	Loss: 0.019375	Acc: 55.3% (4425/8000)
[Test]  Epoch: 15	Loss: 0.019300	Acc: 55.5% (4442/8000)
[Test]  Epoch: 16	Loss: 0.019199	Acc: 55.8% (4461/8000)
[Test]  Epoch: 17	Loss: 0.019110	Acc: 56.0% (4483/8000)
[Test]  Epoch: 18	Loss: 0.019052	Acc: 56.1% (4487/8000)
[Test]  Epoch: 19	Loss: 0.019030	Acc: 56.2% (4496/8000)
[Test]  Epoch: 20	Loss: 0.019054	Acc: 56.5% (4522/8000)
[Test]  Epoch: 21	Loss: 0.019103	Acc: 55.8% (4462/8000)
[Test]  Epoch: 22	Loss: 0.018885	Acc: 56.2% (4497/8000)
[Test]  Epoch: 23	Loss: 0.018934	Acc: 56.4% (4513/8000)
[Test]  Epoch: 24	Loss: 0.018954	Acc: 56.5% (4518/8000)
[Test]  Epoch: 25	Loss: 0.018986	Acc: 56.1% (4486/8000)
[Test]  Epoch: 26	Loss: 0.018860	Acc: 56.2% (4498/8000)
[Test]  Epoch: 27	Loss: 0.018844	Acc: 56.5% (4524/8000)
[Test]  Epoch: 28	Loss: 0.018966	Acc: 56.1% (4485/8000)
[Test]  Epoch: 29	Loss: 0.018871	Acc: 56.3% (4503/8000)
[Test]  Epoch: 30	Loss: 0.018839	Acc: 56.3% (4503/8000)
[Test]  Epoch: 31	Loss: 0.018786	Acc: 56.6% (4530/8000)
[Test]  Epoch: 32	Loss: 0.018705	Acc: 56.5% (4519/8000)
[Test]  Epoch: 33	Loss: 0.018774	Acc: 56.5% (4524/8000)
[Test]  Epoch: 34	Loss: 0.018793	Acc: 56.5% (4521/8000)
[Test]  Epoch: 35	Loss: 0.018751	Acc: 56.5% (4520/8000)
[Test]  Epoch: 36	Loss: 0.018741	Acc: 56.2% (4499/8000)
[Test]  Epoch: 37	Loss: 0.018650	Acc: 56.8% (4542/8000)
[Test]  Epoch: 38	Loss: 0.018817	Acc: 56.8% (4542/8000)
[Test]  Epoch: 39	Loss: 0.018659	Acc: 57.1% (4568/8000)
[Test]  Epoch: 40	Loss: 0.018678	Acc: 57.0% (4563/8000)
[Test]  Epoch: 41	Loss: 0.018540	Acc: 56.9% (4552/8000)
[Test]  Epoch: 42	Loss: 0.018672	Acc: 56.6% (4531/8000)
[Test]  Epoch: 43	Loss: 0.018459	Acc: 57.8% (4625/8000)
[Test]  Epoch: 44	Loss: 0.018618	Acc: 56.7% (4537/8000)
[Test]  Epoch: 45	Loss: 0.018580	Acc: 57.6% (4610/8000)
[Test]  Epoch: 46	Loss: 0.018553	Acc: 57.2% (4580/8000)
[Test]  Epoch: 47	Loss: 0.018496	Acc: 57.3% (4586/8000)
[Test]  Epoch: 48	Loss: 0.018595	Acc: 56.9% (4553/8000)
[Test]  Epoch: 49	Loss: 0.018509	Acc: 56.9% (4554/8000)
[Test]  Epoch: 50	Loss: 0.018471	Acc: 57.6% (4608/8000)
[Test]  Epoch: 51	Loss: 0.018518	Acc: 57.4% (4589/8000)
[Test]  Epoch: 52	Loss: 0.018526	Acc: 57.2% (4573/8000)
[Test]  Epoch: 53	Loss: 0.018660	Acc: 56.8% (4545/8000)
[Test]  Epoch: 54	Loss: 0.018454	Acc: 57.4% (4591/8000)
[Test]  Epoch: 55	Loss: 0.018559	Acc: 57.2% (4573/8000)
[Test]  Epoch: 56	Loss: 0.018457	Acc: 57.2% (4578/8000)
[Test]  Epoch: 57	Loss: 0.018496	Acc: 57.1% (4571/8000)
[Test]  Epoch: 58	Loss: 0.018606	Acc: 57.2% (4575/8000)
[Test]  Epoch: 59	Loss: 0.018576	Acc: 57.0% (4558/8000)
[Test]  Epoch: 60	Loss: 0.018555	Acc: 56.8% (4542/8000)
[Test]  Epoch: 61	Loss: 0.018546	Acc: 57.1% (4570/8000)
[Test]  Epoch: 62	Loss: 0.018535	Acc: 57.1% (4567/8000)
[Test]  Epoch: 63	Loss: 0.018514	Acc: 57.1% (4568/8000)
[Test]  Epoch: 64	Loss: 0.018475	Acc: 57.2% (4579/8000)
[Test]  Epoch: 65	Loss: 0.018469	Acc: 57.5% (4597/8000)
[Test]  Epoch: 66	Loss: 0.018489	Acc: 57.4% (4588/8000)
[Test]  Epoch: 67	Loss: 0.018467	Acc: 57.3% (4586/8000)
[Test]  Epoch: 68	Loss: 0.018455	Acc: 57.2% (4579/8000)
[Test]  Epoch: 69	Loss: 0.018458	Acc: 57.3% (4582/8000)
[Test]  Epoch: 70	Loss: 0.018447	Acc: 57.2% (4576/8000)
[Test]  Epoch: 71	Loss: 0.018441	Acc: 57.4% (4593/8000)
[Test]  Epoch: 72	Loss: 0.018412	Acc: 57.3% (4582/8000)
[Test]  Epoch: 73	Loss: 0.018445	Acc: 57.4% (4590/8000)
[Test]  Epoch: 74	Loss: 0.018459	Acc: 57.3% (4585/8000)
[Test]  Epoch: 75	Loss: 0.018454	Acc: 57.3% (4587/8000)
[Test]  Epoch: 76	Loss: 0.018461	Acc: 57.3% (4584/8000)
[Test]  Epoch: 77	Loss: 0.018458	Acc: 57.3% (4583/8000)
[Test]  Epoch: 78	Loss: 0.018463	Acc: 57.3% (4584/8000)
[Test]  Epoch: 79	Loss: 0.018435	Acc: 57.4% (4593/8000)
[Test]  Epoch: 80	Loss: 0.018438	Acc: 57.2% (4580/8000)
[Test]  Epoch: 81	Loss: 0.018426	Acc: 57.3% (4584/8000)
[Test]  Epoch: 82	Loss: 0.018499	Acc: 57.5% (4600/8000)
[Test]  Epoch: 83	Loss: 0.018436	Acc: 57.5% (4600/8000)
[Test]  Epoch: 84	Loss: 0.018446	Acc: 57.4% (4592/8000)
[Test]  Epoch: 85	Loss: 0.018446	Acc: 57.2% (4576/8000)
[Test]  Epoch: 86	Loss: 0.018410	Acc: 57.3% (4585/8000)
[Test]  Epoch: 87	Loss: 0.018404	Acc: 57.5% (4598/8000)
[Test]  Epoch: 88	Loss: 0.018435	Acc: 57.5% (4597/8000)
[Test]  Epoch: 89	Loss: 0.018429	Acc: 57.3% (4581/8000)
[Test]  Epoch: 90	Loss: 0.018419	Acc: 57.3% (4586/8000)
[Test]  Epoch: 91	Loss: 0.018418	Acc: 57.3% (4582/8000)
[Test]  Epoch: 92	Loss: 0.018434	Acc: 57.4% (4594/8000)
[Test]  Epoch: 93	Loss: 0.018442	Acc: 57.5% (4597/8000)
[Test]  Epoch: 94	Loss: 0.018461	Acc: 57.5% (4598/8000)
[Test]  Epoch: 95	Loss: 0.018475	Acc: 57.5% (4599/8000)
[Test]  Epoch: 96	Loss: 0.018405	Acc: 57.5% (4599/8000)
[Test]  Epoch: 97	Loss: 0.018449	Acc: 57.4% (4590/8000)
[Test]  Epoch: 98	Loss: 0.018434	Acc: 57.4% (4590/8000)
[Test]  Epoch: 99	Loss: 0.018448	Acc: 57.5% (4597/8000)
[Test]  Epoch: 100	Loss: 0.018446	Acc: 57.5% (4603/8000)
===========finish==========
['2024-08-19', '20:28:38.714612', '100', 'test', '0.01844632063806057', '57.5375', '57.8125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer4.0.bn3.weight', 'layer1.2.bn2.weight', 'layer1.1.bn2.weight', 'layer2.0.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn1.weight', 'layer2.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer2.1.bn2.weight', 'layer3.2.conv2.weight', 'layer2.3.conv2.weight', 'layer1.1.bn3.weight', 'layer2.1.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer2.0.bn3.weight', 'layer1.2.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer3.4.bn3.weight', 'layer4.2.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.conv1.weight', 'layer1.0.bn1.weight', 'layer3.1.conv1.weight', 'layer2.3.conv3.weight', 'layer3.3.conv2.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer3.4.conv1.weight', 'layer4.1.bn1.weight', 'layer1.0.bn2.weight', 'layer2.2.conv3.weight', 'layer1.0.bn3.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer4.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer2.3.conv1.weight', 'layer2.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.0.conv2.weight', 'layer1.2.conv2.weight', 'layer1.2.conv3.weight', 'layer3.3.conv3.weight', 'layer2.1.conv3.weight', 'layer1.1.conv3.weight', 'layer3.2.conv3.weight', 'layer2.0.conv3.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.conv1.weight', 'layer4.0.bn2.weight', 'layer3.1.conv3.weight', 'layer3.4.conv3.weight', 'bn1.weight', 'layer1.0.conv2.weight', 'layer2.0.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.041060	Acc: 11.5% (920/8000)
[Test]  Epoch: 2	Loss: 0.032727	Acc: 28.3% (2263/8000)
[Test]  Epoch: 3	Loss: 0.021957	Acc: 47.9% (3834/8000)
[Test]  Epoch: 4	Loss: 0.020645	Acc: 51.5% (4119/8000)
[Test]  Epoch: 5	Loss: 0.020531	Acc: 52.2% (4179/8000)
[Test]  Epoch: 6	Loss: 0.020329	Acc: 52.5% (4199/8000)
[Test]  Epoch: 7	Loss: 0.020190	Acc: 52.9% (4231/8000)
[Test]  Epoch: 8	Loss: 0.020029	Acc: 53.3% (4266/8000)
[Test]  Epoch: 9	Loss: 0.020003	Acc: 53.6% (4286/8000)
[Test]  Epoch: 10	Loss: 0.019797	Acc: 53.9% (4313/8000)
[Test]  Epoch: 11	Loss: 0.019771	Acc: 54.4% (4348/8000)
[Test]  Epoch: 12	Loss: 0.019617	Acc: 53.8% (4304/8000)
[Test]  Epoch: 13	Loss: 0.019687	Acc: 54.2% (4335/8000)
[Test]  Epoch: 14	Loss: 0.019741	Acc: 53.6% (4285/8000)
[Test]  Epoch: 15	Loss: 0.019532	Acc: 54.7% (4379/8000)
[Test]  Epoch: 16	Loss: 0.019441	Acc: 54.4% (4350/8000)
[Test]  Epoch: 17	Loss: 0.019502	Acc: 54.2% (4340/8000)
[Test]  Epoch: 18	Loss: 0.019522	Acc: 54.6% (4371/8000)
[Test]  Epoch: 19	Loss: 0.019410	Acc: 54.8% (4381/8000)
[Test]  Epoch: 20	Loss: 0.019404	Acc: 54.6% (4370/8000)
[Test]  Epoch: 21	Loss: 0.019422	Acc: 54.5% (4359/8000)
[Test]  Epoch: 22	Loss: 0.019306	Acc: 54.8% (4385/8000)
[Test]  Epoch: 23	Loss: 0.019244	Acc: 54.7% (4378/8000)
[Test]  Epoch: 24	Loss: 0.019329	Acc: 54.5% (4362/8000)
[Test]  Epoch: 25	Loss: 0.019329	Acc: 55.0% (4400/8000)
[Test]  Epoch: 26	Loss: 0.019231	Acc: 54.8% (4383/8000)
[Test]  Epoch: 27	Loss: 0.019126	Acc: 55.2% (4418/8000)
[Test]  Epoch: 28	Loss: 0.019355	Acc: 55.0% (4396/8000)
[Test]  Epoch: 29	Loss: 0.019184	Acc: 55.1% (4411/8000)
[Test]  Epoch: 30	Loss: 0.019126	Acc: 55.1% (4406/8000)
[Test]  Epoch: 31	Loss: 0.019197	Acc: 55.0% (4399/8000)
[Test]  Epoch: 32	Loss: 0.019119	Acc: 55.1% (4412/8000)
[Test]  Epoch: 33	Loss: 0.019114	Acc: 55.1% (4405/8000)
[Test]  Epoch: 34	Loss: 0.019075	Acc: 55.1% (4406/8000)
[Test]  Epoch: 35	Loss: 0.019096	Acc: 55.4% (4433/8000)
[Test]  Epoch: 36	Loss: 0.019129	Acc: 54.8% (4387/8000)
[Test]  Epoch: 37	Loss: 0.019026	Acc: 55.6% (4447/8000)
[Test]  Epoch: 38	Loss: 0.019042	Acc: 55.4% (4429/8000)
[Test]  Epoch: 39	Loss: 0.019086	Acc: 55.2% (4416/8000)
[Test]  Epoch: 40	Loss: 0.018939	Acc: 55.5% (4439/8000)
[Test]  Epoch: 41	Loss: 0.018989	Acc: 55.1% (4411/8000)
[Test]  Epoch: 42	Loss: 0.018942	Acc: 55.4% (4430/8000)
[Test]  Epoch: 43	Loss: 0.018933	Acc: 55.4% (4431/8000)
[Test]  Epoch: 44	Loss: 0.018925	Acc: 55.6% (4448/8000)
[Test]  Epoch: 45	Loss: 0.018937	Acc: 55.7% (4457/8000)
[Test]  Epoch: 46	Loss: 0.019040	Acc: 55.4% (4433/8000)
[Test]  Epoch: 47	Loss: 0.018902	Acc: 55.6% (4450/8000)
[Test]  Epoch: 48	Loss: 0.018946	Acc: 55.3% (4426/8000)
[Test]  Epoch: 49	Loss: 0.018954	Acc: 55.4% (4433/8000)
[Test]  Epoch: 50	Loss: 0.018899	Acc: 55.5% (4440/8000)
[Test]  Epoch: 51	Loss: 0.018975	Acc: 55.7% (4454/8000)
[Test]  Epoch: 52	Loss: 0.018917	Acc: 55.5% (4444/8000)
[Test]  Epoch: 53	Loss: 0.018922	Acc: 55.6% (4449/8000)
[Test]  Epoch: 54	Loss: 0.018874	Acc: 55.4% (4429/8000)
[Test]  Epoch: 55	Loss: 0.018861	Acc: 55.8% (4460/8000)
[Test]  Epoch: 56	Loss: 0.018888	Acc: 55.6% (4446/8000)
[Test]  Epoch: 57	Loss: 0.018879	Acc: 55.9% (4475/8000)
[Test]  Epoch: 58	Loss: 0.018919	Acc: 55.7% (4459/8000)
[Test]  Epoch: 59	Loss: 0.018955	Acc: 55.9% (4473/8000)
[Test]  Epoch: 60	Loss: 0.018907	Acc: 55.7% (4458/8000)
[Test]  Epoch: 61	Loss: 0.018912	Acc: 55.5% (4442/8000)
[Test]  Epoch: 62	Loss: 0.018900	Acc: 55.8% (4464/8000)
[Test]  Epoch: 63	Loss: 0.018884	Acc: 55.6% (4449/8000)
[Test]  Epoch: 64	Loss: 0.018864	Acc: 55.8% (4461/8000)
[Test]  Epoch: 65	Loss: 0.018859	Acc: 56.0% (4477/8000)
[Test]  Epoch: 66	Loss: 0.018852	Acc: 55.8% (4461/8000)
[Test]  Epoch: 67	Loss: 0.018849	Acc: 55.9% (4472/8000)
[Test]  Epoch: 68	Loss: 0.018857	Acc: 55.8% (4464/8000)
[Test]  Epoch: 69	Loss: 0.018849	Acc: 55.7% (4457/8000)
[Test]  Epoch: 70	Loss: 0.018846	Acc: 55.6% (4450/8000)
[Test]  Epoch: 71	Loss: 0.018837	Acc: 55.9% (4470/8000)
[Test]  Epoch: 72	Loss: 0.018819	Acc: 55.8% (4461/8000)
[Test]  Epoch: 73	Loss: 0.018817	Acc: 55.8% (4461/8000)
[Test]  Epoch: 74	Loss: 0.018824	Acc: 55.8% (4461/8000)
[Test]  Epoch: 75	Loss: 0.018836	Acc: 55.8% (4465/8000)
[Test]  Epoch: 76	Loss: 0.018863	Acc: 55.8% (4462/8000)
[Test]  Epoch: 77	Loss: 0.018824	Acc: 55.9% (4468/8000)
[Test]  Epoch: 78	Loss: 0.018833	Acc: 56.0% (4478/8000)
[Test]  Epoch: 79	Loss: 0.018814	Acc: 56.0% (4482/8000)
[Test]  Epoch: 80	Loss: 0.018818	Acc: 55.9% (4473/8000)
[Test]  Epoch: 81	Loss: 0.018812	Acc: 55.9% (4470/8000)
[Test]  Epoch: 82	Loss: 0.018858	Acc: 56.1% (4491/8000)
[Test]  Epoch: 83	Loss: 0.018805	Acc: 56.1% (4492/8000)
[Test]  Epoch: 84	Loss: 0.018829	Acc: 56.1% (4486/8000)
[Test]  Epoch: 85	Loss: 0.018818	Acc: 55.9% (4475/8000)
[Test]  Epoch: 86	Loss: 0.018824	Acc: 55.7% (4459/8000)
[Test]  Epoch: 87	Loss: 0.018808	Acc: 55.9% (4468/8000)
[Test]  Epoch: 88	Loss: 0.018835	Acc: 55.8% (4461/8000)
[Test]  Epoch: 89	Loss: 0.018809	Acc: 55.8% (4461/8000)
[Test]  Epoch: 90	Loss: 0.018814	Acc: 55.8% (4466/8000)
[Test]  Epoch: 91	Loss: 0.018795	Acc: 56.0% (4481/8000)
[Test]  Epoch: 92	Loss: 0.018826	Acc: 55.8% (4464/8000)
[Test]  Epoch: 93	Loss: 0.018805	Acc: 56.0% (4482/8000)
[Test]  Epoch: 94	Loss: 0.018820	Acc: 56.1% (4490/8000)
[Test]  Epoch: 95	Loss: 0.018819	Acc: 55.9% (4475/8000)
[Test]  Epoch: 96	Loss: 0.018776	Acc: 55.8% (4466/8000)
[Test]  Epoch: 97	Loss: 0.018816	Acc: 55.7% (4459/8000)
[Test]  Epoch: 98	Loss: 0.018791	Acc: 55.8% (4465/8000)
[Test]  Epoch: 99	Loss: 0.018806	Acc: 55.8% (4466/8000)
[Test]  Epoch: 100	Loss: 0.018796	Acc: 55.9% (4472/8000)
===========finish==========
['2024-08-19', '20:38:02.577791', '100', 'test', '0.018795855343341827', '55.9', '56.15']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer4.0.bn3.weight', 'layer1.2.bn2.weight', 'layer1.1.bn2.weight', 'layer2.0.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn1.weight', 'layer2.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer2.1.bn2.weight', 'layer3.2.conv2.weight', 'layer2.3.conv2.weight', 'layer1.1.bn3.weight', 'layer2.1.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer2.0.bn3.weight', 'layer1.2.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer3.4.bn3.weight', 'layer4.2.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.conv1.weight', 'layer1.0.bn1.weight', 'layer3.1.conv1.weight', 'layer2.3.conv3.weight', 'layer3.3.conv2.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer3.4.conv1.weight', 'layer4.1.bn1.weight', 'layer1.0.bn2.weight', 'layer2.2.conv3.weight', 'layer1.0.bn3.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer4.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer2.3.conv1.weight', 'layer2.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.0.conv2.weight', 'layer1.2.conv2.weight', 'layer1.2.conv3.weight', 'layer3.3.conv3.weight', 'layer2.1.conv3.weight', 'layer1.1.conv3.weight', 'layer3.2.conv3.weight', 'layer2.0.conv3.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.conv1.weight', 'layer4.0.bn2.weight', 'layer3.1.conv3.weight', 'layer3.4.conv3.weight', 'bn1.weight', 'layer1.0.conv2.weight', 'layer2.0.conv1.weight', 'layer3.0.conv3.weight', 'layer1.0.conv3.weight', 'layer4.0.bn1.weight', 'layer3.5.conv1.weight', 'layer3.5.conv2.weight', 'layer4.2.conv1.weight', 'layer3.5.conv3.weight', 'layer4.2.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv2.weight', 'conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.055079	Acc: 11.9% (949/8000)
[Test]  Epoch: 2	Loss: 0.033673	Acc: 30.1% (2410/8000)
[Test]  Epoch: 3	Loss: 0.019856	Acc: 54.0% (4319/8000)
[Test]  Epoch: 4	Loss: 0.018210	Acc: 58.0% (4640/8000)
[Test]  Epoch: 5	Loss: 0.017763	Acc: 58.9% (4709/8000)
[Test]  Epoch: 6	Loss: 0.017263	Acc: 60.0% (4799/8000)
[Test]  Epoch: 7	Loss: 0.017384	Acc: 60.0% (4801/8000)
[Test]  Epoch: 8	Loss: 0.017259	Acc: 59.9% (4792/8000)
[Test]  Epoch: 9	Loss: 0.017053	Acc: 60.5% (4837/8000)
[Test]  Epoch: 10	Loss: 0.017016	Acc: 60.2% (4820/8000)
[Test]  Epoch: 11	Loss: 0.017073	Acc: 60.3% (4823/8000)
[Test]  Epoch: 12	Loss: 0.017048	Acc: 60.5% (4843/8000)
[Test]  Epoch: 13	Loss: 0.016898	Acc: 60.9% (4870/8000)
[Test]  Epoch: 14	Loss: 0.017308	Acc: 59.8% (4782/8000)
[Test]  Epoch: 15	Loss: 0.016880	Acc: 60.7% (4859/8000)
[Test]  Epoch: 16	Loss: 0.016708	Acc: 60.9% (4872/8000)
[Test]  Epoch: 17	Loss: 0.016707	Acc: 61.4% (4913/8000)
[Test]  Epoch: 18	Loss: 0.016693	Acc: 61.2% (4900/8000)
[Test]  Epoch: 19	Loss: 0.016760	Acc: 61.3% (4903/8000)
[Test]  Epoch: 20	Loss: 0.016828	Acc: 60.9% (4868/8000)
[Test]  Epoch: 21	Loss: 0.016698	Acc: 61.2% (4900/8000)
[Test]  Epoch: 22	Loss: 0.016793	Acc: 60.7% (4854/8000)
[Test]  Epoch: 23	Loss: 0.016609	Acc: 61.5% (4921/8000)
[Test]  Epoch: 24	Loss: 0.016662	Acc: 61.5% (4923/8000)
[Test]  Epoch: 25	Loss: 0.016646	Acc: 61.4% (4914/8000)
[Test]  Epoch: 26	Loss: 0.016605	Acc: 61.4% (4914/8000)
[Test]  Epoch: 27	Loss: 0.016566	Acc: 61.5% (4924/8000)
[Test]  Epoch: 28	Loss: 0.016592	Acc: 61.2% (4893/8000)
[Test]  Epoch: 29	Loss: 0.016547	Acc: 61.6% (4930/8000)
[Test]  Epoch: 30	Loss: 0.016654	Acc: 61.1% (4887/8000)
[Test]  Epoch: 31	Loss: 0.016558	Acc: 61.5% (4921/8000)
[Test]  Epoch: 32	Loss: 0.016507	Acc: 61.7% (4938/8000)
[Test]  Epoch: 33	Loss: 0.016537	Acc: 61.8% (4946/8000)
[Test]  Epoch: 34	Loss: 0.016495	Acc: 61.9% (4948/8000)
[Test]  Epoch: 35	Loss: 0.016596	Acc: 61.6% (4932/8000)
[Test]  Epoch: 36	Loss: 0.016471	Acc: 61.5% (4920/8000)
[Test]  Epoch: 37	Loss: 0.016417	Acc: 61.8% (4941/8000)
[Test]  Epoch: 38	Loss: 0.016487	Acc: 61.9% (4948/8000)
[Test]  Epoch: 39	Loss: 0.016506	Acc: 61.7% (4934/8000)
[Test]  Epoch: 40	Loss: 0.016595	Acc: 60.9% (4875/8000)
[Test]  Epoch: 41	Loss: 0.016473	Acc: 61.6% (4932/8000)
[Test]  Epoch: 42	Loss: 0.016448	Acc: 61.6% (4929/8000)
[Test]  Epoch: 43	Loss: 0.016413	Acc: 61.8% (4942/8000)
[Test]  Epoch: 44	Loss: 0.016426	Acc: 61.5% (4924/8000)
[Test]  Epoch: 45	Loss: 0.016377	Acc: 61.9% (4948/8000)
[Test]  Epoch: 46	Loss: 0.016421	Acc: 61.8% (4943/8000)
[Test]  Epoch: 47	Loss: 0.016458	Acc: 61.6% (4931/8000)
[Test]  Epoch: 48	Loss: 0.016413	Acc: 61.5% (4922/8000)
[Test]  Epoch: 49	Loss: 0.016396	Acc: 61.8% (4947/8000)
[Test]  Epoch: 50	Loss: 0.016363	Acc: 61.8% (4940/8000)
[Test]  Epoch: 51	Loss: 0.016391	Acc: 61.9% (4950/8000)
[Test]  Epoch: 52	Loss: 0.016433	Acc: 61.4% (4910/8000)
[Test]  Epoch: 53	Loss: 0.016350	Acc: 62.0% (4961/8000)
[Test]  Epoch: 54	Loss: 0.016419	Acc: 61.6% (4928/8000)
[Test]  Epoch: 55	Loss: 0.016306	Acc: 62.0% (4956/8000)
[Test]  Epoch: 56	Loss: 0.016321	Acc: 61.9% (4951/8000)
[Test]  Epoch: 57	Loss: 0.016377	Acc: 61.9% (4952/8000)
[Test]  Epoch: 58	Loss: 0.016304	Acc: 62.2% (4973/8000)
[Test]  Epoch: 59	Loss: 0.016478	Acc: 61.5% (4922/8000)
[Test]  Epoch: 60	Loss: 0.016328	Acc: 62.1% (4970/8000)
[Test]  Epoch: 61	Loss: 0.016361	Acc: 62.0% (4960/8000)
[Test]  Epoch: 62	Loss: 0.016358	Acc: 61.9% (4952/8000)
[Test]  Epoch: 63	Loss: 0.016353	Acc: 62.1% (4966/8000)
[Test]  Epoch: 64	Loss: 0.016345	Acc: 61.9% (4954/8000)
[Test]  Epoch: 65	Loss: 0.016357	Acc: 61.9% (4954/8000)
[Test]  Epoch: 66	Loss: 0.016353	Acc: 61.8% (4941/8000)
[Test]  Epoch: 67	Loss: 0.016352	Acc: 62.0% (4957/8000)
[Test]  Epoch: 68	Loss: 0.016324	Acc: 62.1% (4971/8000)
[Test]  Epoch: 69	Loss: 0.016326	Acc: 61.9% (4954/8000)
[Test]  Epoch: 70	Loss: 0.016313	Acc: 62.1% (4971/8000)
[Test]  Epoch: 71	Loss: 0.016337	Acc: 61.9% (4948/8000)
[Test]  Epoch: 72	Loss: 0.016301	Acc: 62.0% (4963/8000)
[Test]  Epoch: 73	Loss: 0.016306	Acc: 62.1% (4967/8000)
[Test]  Epoch: 74	Loss: 0.016310	Acc: 62.0% (4960/8000)
[Test]  Epoch: 75	Loss: 0.016267	Acc: 62.0% (4956/8000)
[Test]  Epoch: 76	Loss: 0.016305	Acc: 62.2% (4974/8000)
[Test]  Epoch: 77	Loss: 0.016312	Acc: 61.9% (4955/8000)
[Test]  Epoch: 78	Loss: 0.016335	Acc: 62.1% (4968/8000)
[Test]  Epoch: 79	Loss: 0.016330	Acc: 61.8% (4947/8000)
[Test]  Epoch: 80	Loss: 0.016330	Acc: 61.9% (4948/8000)
[Test]  Epoch: 81	Loss: 0.016300	Acc: 61.9% (4953/8000)
[Test]  Epoch: 82	Loss: 0.016373	Acc: 62.1% (4967/8000)
[Test]  Epoch: 83	Loss: 0.016297	Acc: 62.0% (4958/8000)
[Test]  Epoch: 84	Loss: 0.016305	Acc: 62.2% (4975/8000)
[Test]  Epoch: 85	Loss: 0.016331	Acc: 62.0% (4957/8000)
[Test]  Epoch: 86	Loss: 0.016320	Acc: 61.9% (4950/8000)
[Test]  Epoch: 87	Loss: 0.016336	Acc: 62.0% (4963/8000)
[Test]  Epoch: 88	Loss: 0.016341	Acc: 62.0% (4962/8000)
[Test]  Epoch: 89	Loss: 0.016344	Acc: 61.9% (4948/8000)
[Test]  Epoch: 90	Loss: 0.016363	Acc: 61.9% (4948/8000)
[Test]  Epoch: 91	Loss: 0.016314	Acc: 62.0% (4962/8000)
[Test]  Epoch: 92	Loss: 0.016339	Acc: 61.9% (4949/8000)
[Test]  Epoch: 93	Loss: 0.016296	Acc: 61.9% (4949/8000)
[Test]  Epoch: 94	Loss: 0.016371	Acc: 61.9% (4954/8000)
[Test]  Epoch: 95	Loss: 0.016345	Acc: 61.9% (4954/8000)
[Test]  Epoch: 96	Loss: 0.016298	Acc: 62.0% (4960/8000)
[Test]  Epoch: 97	Loss: 0.016327	Acc: 61.8% (4942/8000)
[Test]  Epoch: 98	Loss: 0.016324	Acc: 61.9% (4954/8000)
[Test]  Epoch: 99	Loss: 0.016338	Acc: 62.0% (4956/8000)
[Test]  Epoch: 100	Loss: 0.016342	Acc: 61.9% (4955/8000)
===========finish==========
['2024-08-19', '20:47:24.702084', '100', 'test', '0.016341992363333703', '61.9375', '62.1875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.4.bn1.weight', 'layer2.3.bn1.weight', 'layer3.3.bn2.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'layer3.1.bn1.weight', 'layer3.4.bn2.weight', 'layer3.1.bn2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer1.2.bn1.weight', 'layer2.3.bn3.weight', 'layer2.1.bn1.weight', 'layer2.2.conv1.weight', 'layer2.2.conv2.weight', 'layer4.0.bn3.weight', 'layer1.2.bn2.weight', 'layer1.1.bn2.weight', 'layer2.0.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn1.weight', 'layer2.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer2.1.bn2.weight', 'layer3.2.conv2.weight', 'layer2.3.conv2.weight', 'layer1.1.bn3.weight', 'layer2.1.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer2.0.bn3.weight', 'layer1.2.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer3.4.bn3.weight', 'layer4.2.bn1.weight', 'layer3.5.bn2.weight', 'layer3.3.conv1.weight', 'layer1.0.bn1.weight', 'layer3.1.conv1.weight', 'layer2.3.conv3.weight', 'layer3.3.conv2.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer3.4.conv1.weight', 'layer4.1.bn1.weight', 'layer1.0.bn2.weight', 'layer2.2.conv3.weight', 'layer1.0.bn3.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer4.1.bn2.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer2.3.conv1.weight', 'layer2.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.0.conv2.weight', 'layer1.2.conv2.weight', 'layer1.2.conv3.weight', 'layer3.3.conv3.weight', 'layer2.1.conv3.weight', 'layer1.1.conv3.weight', 'layer3.2.conv3.weight', 'layer2.0.conv3.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.conv1.weight', 'layer4.0.bn2.weight', 'layer3.1.conv3.weight', 'layer3.4.conv3.weight', 'bn1.weight', 'layer1.0.conv2.weight', 'layer2.0.conv1.weight', 'layer3.0.conv3.weight', 'layer1.0.conv3.weight', 'layer4.0.bn1.weight', 'layer3.5.conv1.weight', 'layer3.5.conv2.weight', 'layer4.2.conv1.weight', 'layer3.5.conv3.weight', 'layer4.2.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv2.weight', 'conv1.weight', 'layer2.0.downsample.0.weight', 'layer4.1.conv1.weight', 'layer1.0.downsample.0.weight', 'last_linear.weight', 'layer4.1.conv3.weight', 'layer3.0.downsample.0.weight', 'layer4.0.downsample.0.weight', 'layer4.2.conv3.weight', 'layer4.0.conv3.weight', 'layer4.0.conv2.weight', 'layer4.0.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036675	Acc: 11.6% (927/8000)
[Test]  Epoch: 2	Loss: 0.024609	Acc: 48.3% (3863/8000)
[Test]  Epoch: 3	Loss: 0.014278	Acc: 71.7% (5733/8000)
[Test]  Epoch: 4	Loss: 0.012561	Acc: 75.6% (6051/8000)
[Test]  Epoch: 5	Loss: 0.011959	Acc: 76.4% (6110/8000)
[Test]  Epoch: 6	Loss: 0.011706	Acc: 76.6% (6125/8000)
[Test]  Epoch: 7	Loss: 0.011449	Acc: 77.4% (6191/8000)
[Test]  Epoch: 8	Loss: 0.011458	Acc: 76.8% (6147/8000)
[Test]  Epoch: 9	Loss: 0.011414	Acc: 76.9% (6153/8000)
[Test]  Epoch: 10	Loss: 0.011446	Acc: 76.9% (6153/8000)
[Test]  Epoch: 11	Loss: 0.011331	Acc: 77.3% (6187/8000)
[Test]  Epoch: 12	Loss: 0.011356	Acc: 77.0% (6156/8000)
[Test]  Epoch: 13	Loss: 0.011262	Acc: 77.1% (6168/8000)
[Test]  Epoch: 14	Loss: 0.011257	Acc: 77.2% (6178/8000)
[Test]  Epoch: 15	Loss: 0.011260	Acc: 76.9% (6150/8000)
[Test]  Epoch: 16	Loss: 0.011134	Acc: 77.1% (6169/8000)
[Test]  Epoch: 17	Loss: 0.011238	Acc: 77.5% (6200/8000)
[Test]  Epoch: 18	Loss: 0.011250	Acc: 76.9% (6154/8000)
[Test]  Epoch: 19	Loss: 0.011140	Acc: 77.5% (6201/8000)
[Test]  Epoch: 20	Loss: 0.011227	Acc: 77.3% (6188/8000)
[Test]  Epoch: 21	Loss: 0.011198	Acc: 77.4% (6190/8000)
[Test]  Epoch: 22	Loss: 0.011170	Acc: 77.5% (6198/8000)
[Test]  Epoch: 23	Loss: 0.011042	Acc: 77.5% (6204/8000)
[Test]  Epoch: 24	Loss: 0.011129	Acc: 77.5% (6204/8000)
[Test]  Epoch: 25	Loss: 0.011162	Acc: 77.1% (6171/8000)
[Test]  Epoch: 26	Loss: 0.011183	Acc: 77.2% (6180/8000)
[Test]  Epoch: 27	Loss: 0.011129	Acc: 77.2% (6176/8000)
[Test]  Epoch: 28	Loss: 0.011142	Acc: 77.5% (6197/8000)
[Test]  Epoch: 29	Loss: 0.011100	Acc: 77.5% (6196/8000)
[Test]  Epoch: 30	Loss: 0.011120	Acc: 77.2% (6178/8000)
[Test]  Epoch: 31	Loss: 0.011159	Acc: 77.0% (6163/8000)
[Test]  Epoch: 32	Loss: 0.011061	Acc: 77.5% (6199/8000)
[Test]  Epoch: 33	Loss: 0.011204	Acc: 77.2% (6173/8000)
[Test]  Epoch: 34	Loss: 0.011066	Acc: 77.8% (6221/8000)
[Test]  Epoch: 35	Loss: 0.011094	Acc: 77.4% (6191/8000)
[Test]  Epoch: 36	Loss: 0.011137	Acc: 77.6% (6205/8000)
[Test]  Epoch: 37	Loss: 0.011127	Acc: 77.3% (6188/8000)
[Test]  Epoch: 38	Loss: 0.011081	Acc: 77.1% (6167/8000)
[Test]  Epoch: 39	Loss: 0.011079	Acc: 77.3% (6184/8000)
[Test]  Epoch: 40	Loss: 0.011114	Acc: 77.3% (6186/8000)
[Test]  Epoch: 41	Loss: 0.011170	Acc: 77.1% (6171/8000)
[Test]  Epoch: 42	Loss: 0.011146	Acc: 77.2% (6175/8000)
[Test]  Epoch: 43	Loss: 0.011064	Acc: 77.2% (6180/8000)
[Test]  Epoch: 44	Loss: 0.011147	Acc: 77.1% (6166/8000)
[Test]  Epoch: 45	Loss: 0.011035	Acc: 77.4% (6194/8000)
[Test]  Epoch: 46	Loss: 0.011111	Acc: 77.7% (6214/8000)
[Test]  Epoch: 47	Loss: 0.011102	Acc: 77.1% (6165/8000)
[Test]  Epoch: 48	Loss: 0.011036	Acc: 77.2% (6180/8000)
[Test]  Epoch: 49	Loss: 0.011065	Acc: 77.5% (6201/8000)
[Test]  Epoch: 50	Loss: 0.011092	Acc: 77.7% (6214/8000)
[Test]  Epoch: 51	Loss: 0.011229	Acc: 77.0% (6161/8000)
[Test]  Epoch: 52	Loss: 0.011159	Acc: 77.3% (6184/8000)
[Test]  Epoch: 53	Loss: 0.011057	Acc: 77.6% (6208/8000)
[Test]  Epoch: 54	Loss: 0.011206	Acc: 77.0% (6160/8000)
[Test]  Epoch: 55	Loss: 0.011189	Acc: 76.6% (6131/8000)
[Test]  Epoch: 56	Loss: 0.011142	Acc: 77.3% (6184/8000)
[Test]  Epoch: 57	Loss: 0.011149	Acc: 77.6% (6210/8000)
[Test]  Epoch: 58	Loss: 0.011192	Acc: 77.3% (6186/8000)
[Test]  Epoch: 59	Loss: 0.011100	Acc: 77.3% (6186/8000)
[Test]  Epoch: 60	Loss: 0.011126	Acc: 77.3% (6182/8000)
[Test]  Epoch: 61	Loss: 0.011150	Acc: 77.2% (6175/8000)
[Test]  Epoch: 62	Loss: 0.011151	Acc: 77.2% (6179/8000)
[Test]  Epoch: 63	Loss: 0.011096	Acc: 77.4% (6195/8000)
[Test]  Epoch: 64	Loss: 0.011125	Acc: 77.3% (6183/8000)
[Test]  Epoch: 65	Loss: 0.011092	Acc: 77.2% (6175/8000)
[Test]  Epoch: 66	Loss: 0.011121	Acc: 77.3% (6184/8000)
[Test]  Epoch: 67	Loss: 0.011089	Acc: 77.3% (6187/8000)
[Test]  Epoch: 68	Loss: 0.011123	Acc: 77.4% (6193/8000)
[Test]  Epoch: 69	Loss: 0.011115	Acc: 77.4% (6193/8000)
[Test]  Epoch: 70	Loss: 0.011122	Acc: 77.3% (6187/8000)
[Test]  Epoch: 71	Loss: 0.011109	Acc: 77.3% (6188/8000)
[Test]  Epoch: 72	Loss: 0.011107	Acc: 77.5% (6200/8000)
[Test]  Epoch: 73	Loss: 0.011112	Acc: 77.3% (6182/8000)
[Test]  Epoch: 74	Loss: 0.011134	Acc: 77.3% (6186/8000)
[Test]  Epoch: 75	Loss: 0.011124	Acc: 77.3% (6183/8000)
[Test]  Epoch: 76	Loss: 0.011104	Acc: 77.4% (6191/8000)
[Test]  Epoch: 77	Loss: 0.011119	Acc: 77.4% (6189/8000)
[Test]  Epoch: 78	Loss: 0.011119	Acc: 77.4% (6194/8000)
[Test]  Epoch: 79	Loss: 0.011132	Acc: 77.5% (6200/8000)
[Test]  Epoch: 80	Loss: 0.011133	Acc: 77.4% (6189/8000)
[Test]  Epoch: 81	Loss: 0.011115	Acc: 77.5% (6198/8000)
[Test]  Epoch: 82	Loss: 0.011159	Acc: 77.3% (6182/8000)
[Test]  Epoch: 83	Loss: 0.011120	Acc: 77.5% (6197/8000)
[Test]  Epoch: 84	Loss: 0.011109	Acc: 77.5% (6199/8000)
[Test]  Epoch: 85	Loss: 0.011132	Acc: 77.0% (6163/8000)
[Test]  Epoch: 86	Loss: 0.011104	Acc: 77.5% (6204/8000)
[Test]  Epoch: 87	Loss: 0.011053	Acc: 77.5% (6201/8000)
[Test]  Epoch: 88	Loss: 0.011135	Acc: 77.5% (6199/8000)
[Test]  Epoch: 89	Loss: 0.011156	Acc: 77.4% (6189/8000)
[Test]  Epoch: 90	Loss: 0.011141	Acc: 77.4% (6194/8000)
[Test]  Epoch: 91	Loss: 0.011140	Acc: 77.2% (6179/8000)
[Test]  Epoch: 92	Loss: 0.011115	Acc: 77.5% (6196/8000)
[Test]  Epoch: 93	Loss: 0.011100	Acc: 77.5% (6198/8000)
[Test]  Epoch: 94	Loss: 0.011105	Acc: 77.3% (6182/8000)
[Test]  Epoch: 95	Loss: 0.011135	Acc: 77.3% (6184/8000)
[Test]  Epoch: 96	Loss: 0.011188	Acc: 77.4% (6190/8000)
[Test]  Epoch: 97	Loss: 0.011120	Acc: 77.4% (6190/8000)
[Test]  Epoch: 98	Loss: 0.011124	Acc: 77.6% (6209/8000)
[Test]  Epoch: 99	Loss: 0.011165	Acc: 77.4% (6189/8000)
[Test]  Epoch: 100	Loss: 0.011102	Acc: 77.5% (6197/8000)
===========finish==========
['2024-08-19', '20:56:56.493257', '100', 'test', '0.011101914208382368', '77.4625', '77.7625']
