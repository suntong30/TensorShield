nohup: ignoring input
result path:  /home/gpu2/jbw/other_XAI/knockoffnets/ms_elastictrainer_result_cifar100_stl10_resnet18.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/stl10-resnet18/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('last_linear.bias', 0.0), ('layer4.1.bn2.weight', -0.0015446108300238848), ('layer4.0.bn2.weight', -0.1158297136425972), ('layer1.0.bn1.weight', -0.16701605916023254), ('layer1.1.bn2.weight', -0.1743679642677307), ('layer4.0.downsample.1.weight', -0.1862175166606903), ('layer1.1.bn1.weight', -0.19466985762119293), ('layer4.1.bn1.weight', -0.21169783174991608), ('layer3.0.downsample.1.weight', -0.21330277621746063), ('layer1.0.bn2.weight', -0.2385367453098297), ('layer3.1.bn2.weight', -0.26860368251800537), ('layer4.0.bn1.weight', -0.4229624271392822), ('bn1.weight', -0.4740481972694397), ('layer2.0.downsample.1.weight', -0.5117068290710449), ('layer2.1.bn2.weight', -0.5175747871398926), ('layer3.1.bn1.weight', -0.585665225982666), ('layer3.0.bn2.weight', -0.6406246423721313), ('layer2.0.bn2.weight', -0.69770747423172), ('last_linear.weight', -0.9434133768081665), ('layer2.1.bn1.weight', -0.956588625907898), ('layer2.0.bn1.weight', -1.0398969650268555), ('layer3.0.bn1.weight', -1.1289247274398804), ('layer3.0.downsample.0.weight', -1.24802827835083), ('layer4.1.conv2.weight', -1.6556594371795654), ('layer4.1.conv1.weight', -2.099062442779541), ('layer4.0.downsample.0.weight', -2.3427560329437256), ('layer3.1.conv2.weight', -2.4908905029296875), ('layer1.0.conv2.weight', -3.1048150062561035), ('layer4.0.conv2.weight', -3.1428751945495605), ('layer1.1.conv1.weight', -3.217849016189575), ('conv1.weight', -3.2213196754455566), ('layer1.1.conv2.weight', -3.3522214889526367), ('layer1.0.conv1.weight', -3.5746350288391113), ('layer2.1.conv2.weight', -3.685030937194824), ('layer2.0.downsample.0.weight', -4.306734561920166), ('layer4.0.conv1.weight', -4.531484127044678), ('layer3.1.conv1.weight', -4.80584716796875), ('layer2.1.conv1.weight', -6.069857597351074), ('layer2.0.conv2.weight', -6.953187942504883), ('layer3.0.conv2.weight', -7.574256420135498), ('layer3.0.conv1.weight', -8.99406623840332), ('layer2.0.conv1.weight', -9.170661926269531)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('last_linear.bias', 0.0), ('last_linear.weight', -0.9434133768081665), ('layer4.1.conv2.weight', -1.6556594371795654), ('layer4.1.conv1.weight', -2.099062442779541), ('layer3.1.conv2.weight', -2.4908905029296875), ('layer1.0.conv2.weight', -3.1048150062561035), ('layer4.0.conv2.weight', -3.1428751945495605), ('layer1.1.conv1.weight', -3.217849016189575), ('conv1.weight', -3.2213196754455566), ('layer1.1.conv2.weight', -3.3522214889526367), ('layer1.0.conv1.weight', -3.5746350288391113), ('layer2.1.conv2.weight', -3.685030937194824), ('layer4.0.conv1.weight', -4.531484127044678), ('layer3.1.conv1.weight', -4.80584716796875), ('layer2.1.conv1.weight', -6.069857597351074), ('layer2.0.conv2.weight', -6.953187942504883), ('layer3.0.conv2.weight', -7.574256420135498), ('layer3.0.conv1.weight', -8.99406623840332), ('layer2.0.conv1.weight', -9.170661926269531)]
--------------------------------------------
get_sample_layers n=41  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.006648	Acc: 86.1% (6885/8000)
[Test]  Epoch: 2	Loss: 0.006633	Acc: 86.0% (6881/8000)
[Test]  Epoch: 3	Loss: 0.006705	Acc: 86.2% (6897/8000)
[Test]  Epoch: 4	Loss: 0.006691	Acc: 86.3% (6905/8000)
[Test]  Epoch: 5	Loss: 0.006683	Acc: 86.3% (6903/8000)
[Test]  Epoch: 6	Loss: 0.006667	Acc: 86.3% (6903/8000)
[Test]  Epoch: 7	Loss: 0.006689	Acc: 86.3% (6901/8000)
[Test]  Epoch: 8	Loss: 0.006646	Acc: 86.5% (6916/8000)
[Test]  Epoch: 9	Loss: 0.006622	Acc: 86.4% (6909/8000)
[Test]  Epoch: 10	Loss: 0.006656	Acc: 86.3% (6906/8000)
[Test]  Epoch: 11	Loss: 0.006682	Acc: 86.3% (6905/8000)
[Test]  Epoch: 12	Loss: 0.006657	Acc: 86.4% (6914/8000)
[Test]  Epoch: 13	Loss: 0.006629	Acc: 86.3% (6907/8000)
[Test]  Epoch: 14	Loss: 0.006628	Acc: 86.3% (6908/8000)
[Test]  Epoch: 15	Loss: 0.006637	Acc: 86.4% (6914/8000)
[Test]  Epoch: 16	Loss: 0.006645	Acc: 86.3% (6906/8000)
[Test]  Epoch: 17	Loss: 0.006640	Acc: 86.3% (6905/8000)
[Test]  Epoch: 18	Loss: 0.006666	Acc: 86.2% (6899/8000)
[Test]  Epoch: 19	Loss: 0.006616	Acc: 86.2% (6900/8000)
[Test]  Epoch: 20	Loss: 0.006617	Acc: 86.3% (6906/8000)
[Test]  Epoch: 21	Loss: 0.006651	Acc: 86.2% (6898/8000)
[Test]  Epoch: 22	Loss: 0.006612	Acc: 86.4% (6913/8000)
[Test]  Epoch: 23	Loss: 0.006670	Acc: 86.4% (6910/8000)
[Test]  Epoch: 24	Loss: 0.006614	Acc: 86.2% (6899/8000)
[Test]  Epoch: 25	Loss: 0.006684	Acc: 86.4% (6910/8000)
[Test]  Epoch: 26	Loss: 0.006581	Acc: 86.3% (6908/8000)
[Test]  Epoch: 27	Loss: 0.006597	Acc: 86.3% (6908/8000)
[Test]  Epoch: 28	Loss: 0.006615	Acc: 86.4% (6911/8000)
[Test]  Epoch: 29	Loss: 0.006552	Acc: 86.3% (6903/8000)
[Test]  Epoch: 30	Loss: 0.006635	Acc: 86.4% (6909/8000)
[Test]  Epoch: 31	Loss: 0.006563	Acc: 86.3% (6907/8000)
[Test]  Epoch: 32	Loss: 0.006565	Acc: 86.3% (6905/8000)
[Test]  Epoch: 33	Loss: 0.006609	Acc: 86.3% (6902/8000)
[Test]  Epoch: 34	Loss: 0.006618	Acc: 86.4% (6911/8000)
[Test]  Epoch: 35	Loss: 0.006567	Acc: 86.5% (6917/8000)
[Test]  Epoch: 36	Loss: 0.006559	Acc: 86.3% (6908/8000)
[Test]  Epoch: 37	Loss: 0.006587	Acc: 86.4% (6912/8000)
[Test]  Epoch: 38	Loss: 0.006564	Acc: 86.2% (6899/8000)
[Test]  Epoch: 39	Loss: 0.006555	Acc: 86.3% (6903/8000)
[Test]  Epoch: 40	Loss: 0.006560	Acc: 86.3% (6903/8000)
[Test]  Epoch: 41	Loss: 0.006590	Acc: 86.4% (6909/8000)
[Test]  Epoch: 42	Loss: 0.006559	Acc: 86.4% (6910/8000)
[Test]  Epoch: 43	Loss: 0.006548	Acc: 86.5% (6922/8000)
[Test]  Epoch: 44	Loss: 0.006540	Acc: 86.3% (6908/8000)
[Test]  Epoch: 45	Loss: 0.006545	Acc: 86.5% (6918/8000)
[Test]  Epoch: 46	Loss: 0.006557	Acc: 86.5% (6916/8000)
[Test]  Epoch: 47	Loss: 0.006526	Acc: 86.4% (6910/8000)
[Test]  Epoch: 48	Loss: 0.006574	Acc: 86.4% (6914/8000)
[Test]  Epoch: 49	Loss: 0.006574	Acc: 86.4% (6915/8000)
[Test]  Epoch: 50	Loss: 0.006604	Acc: 86.3% (6908/8000)
[Test]  Epoch: 51	Loss: 0.006569	Acc: 86.3% (6907/8000)
[Test]  Epoch: 52	Loss: 0.006564	Acc: 86.3% (6908/8000)
[Test]  Epoch: 53	Loss: 0.006546	Acc: 86.3% (6906/8000)
[Test]  Epoch: 54	Loss: 0.006546	Acc: 86.3% (6908/8000)
[Test]  Epoch: 55	Loss: 0.006580	Acc: 86.4% (6910/8000)
[Test]  Epoch: 56	Loss: 0.006566	Acc: 86.6% (6925/8000)
[Test]  Epoch: 57	Loss: 0.006519	Acc: 86.5% (6919/8000)
[Test]  Epoch: 58	Loss: 0.006524	Acc: 86.5% (6917/8000)
[Test]  Epoch: 59	Loss: 0.006518	Acc: 86.5% (6917/8000)
[Test]  Epoch: 60	Loss: 0.006538	Acc: 86.4% (6914/8000)
[Test]  Epoch: 61	Loss: 0.006497	Acc: 86.4% (6909/8000)
[Test]  Epoch: 62	Loss: 0.006535	Acc: 86.4% (6911/8000)
[Test]  Epoch: 63	Loss: 0.006559	Acc: 86.4% (6911/8000)
[Test]  Epoch: 64	Loss: 0.006514	Acc: 86.4% (6912/8000)
[Test]  Epoch: 65	Loss: 0.006521	Acc: 86.4% (6912/8000)
[Test]  Epoch: 66	Loss: 0.006535	Acc: 86.6% (6926/8000)
[Test]  Epoch: 67	Loss: 0.006578	Acc: 86.4% (6915/8000)
[Test]  Epoch: 68	Loss: 0.006561	Acc: 86.4% (6910/8000)
[Test]  Epoch: 69	Loss: 0.006510	Acc: 86.3% (6906/8000)
[Test]  Epoch: 70	Loss: 0.006511	Acc: 86.4% (6914/8000)
[Test]  Epoch: 71	Loss: 0.006541	Acc: 86.4% (6913/8000)
[Test]  Epoch: 72	Loss: 0.006539	Acc: 86.5% (6924/8000)
[Test]  Epoch: 73	Loss: 0.006562	Acc: 86.4% (6911/8000)
[Test]  Epoch: 74	Loss: 0.006507	Acc: 86.4% (6914/8000)
[Test]  Epoch: 75	Loss: 0.006516	Acc: 86.5% (6923/8000)
[Test]  Epoch: 76	Loss: 0.006552	Acc: 86.5% (6921/8000)
[Test]  Epoch: 77	Loss: 0.006479	Acc: 86.5% (6917/8000)
[Test]  Epoch: 78	Loss: 0.006501	Acc: 86.4% (6911/8000)
[Test]  Epoch: 79	Loss: 0.006543	Acc: 86.4% (6910/8000)
[Test]  Epoch: 80	Loss: 0.006520	Acc: 86.5% (6920/8000)
[Test]  Epoch: 81	Loss: 0.006497	Acc: 86.5% (6919/8000)
[Test]  Epoch: 82	Loss: 0.006509	Acc: 86.5% (6919/8000)
[Test]  Epoch: 83	Loss: 0.006523	Acc: 86.5% (6916/8000)
[Test]  Epoch: 84	Loss: 0.006558	Acc: 86.4% (6914/8000)
[Test]  Epoch: 85	Loss: 0.006516	Acc: 86.2% (6900/8000)
[Test]  Epoch: 86	Loss: 0.006547	Acc: 86.4% (6910/8000)
[Test]  Epoch: 87	Loss: 0.006548	Acc: 86.3% (6907/8000)
[Test]  Epoch: 88	Loss: 0.006509	Acc: 86.4% (6912/8000)
[Test]  Epoch: 89	Loss: 0.006505	Acc: 86.4% (6909/8000)
[Test]  Epoch: 90	Loss: 0.006548	Acc: 86.4% (6914/8000)
[Test]  Epoch: 91	Loss: 0.006534	Acc: 86.3% (6908/8000)
[Test]  Epoch: 92	Loss: 0.006524	Acc: 86.5% (6921/8000)
[Test]  Epoch: 93	Loss: 0.006525	Acc: 86.5% (6923/8000)
[Test]  Epoch: 94	Loss: 0.006500	Acc: 86.4% (6913/8000)
[Test]  Epoch: 95	Loss: 0.006506	Acc: 86.4% (6913/8000)
[Test]  Epoch: 96	Loss: 0.006525	Acc: 86.4% (6915/8000)
[Test]  Epoch: 97	Loss: 0.006510	Acc: 86.5% (6917/8000)
[Test]  Epoch: 98	Loss: 0.006503	Acc: 86.5% (6924/8000)
[Test]  Epoch: 99	Loss: 0.006520	Acc: 86.4% (6914/8000)
[Test]  Epoch: 100	Loss: 0.006526	Acc: 86.4% (6915/8000)
===========finish==========
['2024-09-04', '14:17:08.902077', '100', 'test', '0.006525898201391101', '86.4375', '86.575']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007763	Acc: 85.8% (6867/8000)
[Test]  Epoch: 2	Loss: 0.007505	Acc: 85.8% (6861/8000)
[Test]  Epoch: 3	Loss: 0.007523	Acc: 85.9% (6869/8000)
[Test]  Epoch: 4	Loss: 0.007395	Acc: 86.0% (6878/8000)
[Test]  Epoch: 5	Loss: 0.007365	Acc: 86.1% (6887/8000)
[Test]  Epoch: 6	Loss: 0.007307	Acc: 86.1% (6887/8000)
[Test]  Epoch: 7	Loss: 0.007297	Acc: 86.0% (6880/8000)
[Test]  Epoch: 8	Loss: 0.007221	Acc: 86.2% (6893/8000)
[Test]  Epoch: 9	Loss: 0.007147	Acc: 86.1% (6887/8000)
[Test]  Epoch: 10	Loss: 0.007196	Acc: 86.1% (6890/8000)
[Test]  Epoch: 11	Loss: 0.007150	Acc: 86.1% (6891/8000)
[Test]  Epoch: 12	Loss: 0.007111	Acc: 86.2% (6894/8000)
[Test]  Epoch: 13	Loss: 0.007073	Acc: 86.2% (6897/8000)
[Test]  Epoch: 14	Loss: 0.007052	Acc: 86.1% (6888/8000)
[Test]  Epoch: 15	Loss: 0.007079	Acc: 86.1% (6890/8000)
[Test]  Epoch: 16	Loss: 0.007038	Acc: 86.3% (6907/8000)
[Test]  Epoch: 17	Loss: 0.007025	Acc: 86.1% (6890/8000)
[Test]  Epoch: 18	Loss: 0.007066	Acc: 86.1% (6886/8000)
[Test]  Epoch: 19	Loss: 0.006998	Acc: 86.2% (6895/8000)
[Test]  Epoch: 20	Loss: 0.006966	Acc: 86.1% (6891/8000)
[Test]  Epoch: 21	Loss: 0.006982	Acc: 86.1% (6888/8000)
[Test]  Epoch: 22	Loss: 0.006929	Acc: 86.4% (6914/8000)
[Test]  Epoch: 23	Loss: 0.007008	Acc: 86.4% (6912/8000)
[Test]  Epoch: 24	Loss: 0.006911	Acc: 86.1% (6889/8000)
[Test]  Epoch: 25	Loss: 0.007022	Acc: 86.2% (6895/8000)
[Test]  Epoch: 26	Loss: 0.006883	Acc: 86.1% (6887/8000)
[Test]  Epoch: 27	Loss: 0.006919	Acc: 86.2% (6893/8000)
[Test]  Epoch: 28	Loss: 0.006905	Acc: 86.3% (6901/8000)
[Test]  Epoch: 29	Loss: 0.006851	Acc: 86.4% (6911/8000)
[Test]  Epoch: 30	Loss: 0.006941	Acc: 86.3% (6904/8000)
[Test]  Epoch: 31	Loss: 0.006840	Acc: 86.4% (6910/8000)
[Test]  Epoch: 32	Loss: 0.006849	Acc: 86.3% (6903/8000)
[Test]  Epoch: 33	Loss: 0.006891	Acc: 86.3% (6903/8000)
[Test]  Epoch: 34	Loss: 0.006886	Acc: 86.3% (6905/8000)
[Test]  Epoch: 35	Loss: 0.006812	Acc: 86.2% (6900/8000)
[Test]  Epoch: 36	Loss: 0.006805	Acc: 86.2% (6895/8000)
[Test]  Epoch: 37	Loss: 0.006830	Acc: 86.2% (6894/8000)
[Test]  Epoch: 38	Loss: 0.006794	Acc: 86.3% (6902/8000)
[Test]  Epoch: 39	Loss: 0.006810	Acc: 86.2% (6900/8000)
[Test]  Epoch: 40	Loss: 0.006799	Acc: 86.2% (6898/8000)
[Test]  Epoch: 41	Loss: 0.006864	Acc: 86.2% (6897/8000)
[Test]  Epoch: 42	Loss: 0.006810	Acc: 86.2% (6898/8000)
[Test]  Epoch: 43	Loss: 0.006816	Acc: 86.4% (6915/8000)
[Test]  Epoch: 44	Loss: 0.006791	Acc: 86.3% (6905/8000)
[Test]  Epoch: 45	Loss: 0.006786	Acc: 86.2% (6897/8000)
[Test]  Epoch: 46	Loss: 0.006780	Acc: 86.2% (6896/8000)
[Test]  Epoch: 47	Loss: 0.006749	Acc: 86.2% (6900/8000)
[Test]  Epoch: 48	Loss: 0.006802	Acc: 86.4% (6910/8000)
[Test]  Epoch: 49	Loss: 0.006798	Acc: 86.3% (6908/8000)
[Test]  Epoch: 50	Loss: 0.006829	Acc: 86.3% (6903/8000)
[Test]  Epoch: 51	Loss: 0.006793	Acc: 86.2% (6896/8000)
[Test]  Epoch: 52	Loss: 0.006771	Acc: 86.1% (6886/8000)
[Test]  Epoch: 53	Loss: 0.006744	Acc: 86.2% (6892/8000)
[Test]  Epoch: 54	Loss: 0.006784	Acc: 86.2% (6900/8000)
[Test]  Epoch: 55	Loss: 0.006806	Acc: 86.3% (6902/8000)
[Test]  Epoch: 56	Loss: 0.006834	Acc: 86.3% (6902/8000)
[Test]  Epoch: 57	Loss: 0.006732	Acc: 86.2% (6899/8000)
[Test]  Epoch: 58	Loss: 0.006751	Acc: 86.2% (6900/8000)
[Test]  Epoch: 59	Loss: 0.006750	Acc: 86.3% (6905/8000)
[Test]  Epoch: 60	Loss: 0.006774	Acc: 86.2% (6897/8000)
[Test]  Epoch: 61	Loss: 0.006690	Acc: 86.1% (6886/8000)
[Test]  Epoch: 62	Loss: 0.006724	Acc: 86.2% (6894/8000)
[Test]  Epoch: 63	Loss: 0.006760	Acc: 86.3% (6903/8000)
[Test]  Epoch: 64	Loss: 0.006705	Acc: 86.2% (6898/8000)
[Test]  Epoch: 65	Loss: 0.006740	Acc: 86.3% (6906/8000)
[Test]  Epoch: 66	Loss: 0.006760	Acc: 86.3% (6903/8000)
[Test]  Epoch: 67	Loss: 0.006801	Acc: 86.2% (6896/8000)
[Test]  Epoch: 68	Loss: 0.006750	Acc: 86.3% (6902/8000)
[Test]  Epoch: 69	Loss: 0.006695	Acc: 86.3% (6903/8000)
[Test]  Epoch: 70	Loss: 0.006717	Acc: 86.2% (6894/8000)
[Test]  Epoch: 71	Loss: 0.006735	Acc: 86.2% (6900/8000)
[Test]  Epoch: 72	Loss: 0.006730	Acc: 86.3% (6905/8000)
[Test]  Epoch: 73	Loss: 0.006759	Acc: 86.2% (6894/8000)
[Test]  Epoch: 74	Loss: 0.006689	Acc: 86.3% (6906/8000)
[Test]  Epoch: 75	Loss: 0.006711	Acc: 86.3% (6903/8000)
[Test]  Epoch: 76	Loss: 0.006741	Acc: 86.4% (6910/8000)
[Test]  Epoch: 77	Loss: 0.006663	Acc: 86.2% (6898/8000)
[Test]  Epoch: 78	Loss: 0.006707	Acc: 86.3% (6901/8000)
[Test]  Epoch: 79	Loss: 0.006754	Acc: 86.3% (6904/8000)
[Test]  Epoch: 80	Loss: 0.006716	Acc: 86.2% (6900/8000)
[Test]  Epoch: 81	Loss: 0.006680	Acc: 86.3% (6903/8000)
[Test]  Epoch: 82	Loss: 0.006671	Acc: 86.2% (6894/8000)
[Test]  Epoch: 83	Loss: 0.006719	Acc: 86.3% (6902/8000)
[Test]  Epoch: 84	Loss: 0.006752	Acc: 86.4% (6911/8000)
[Test]  Epoch: 85	Loss: 0.006734	Acc: 86.2% (6897/8000)
[Test]  Epoch: 86	Loss: 0.006741	Acc: 86.3% (6901/8000)
[Test]  Epoch: 87	Loss: 0.006737	Acc: 86.4% (6911/8000)
[Test]  Epoch: 88	Loss: 0.006705	Acc: 86.3% (6908/8000)
[Test]  Epoch: 89	Loss: 0.006718	Acc: 86.2% (6895/8000)
[Test]  Epoch: 90	Loss: 0.006739	Acc: 86.3% (6905/8000)
[Test]  Epoch: 91	Loss: 0.006745	Acc: 86.1% (6891/8000)
[Test]  Epoch: 92	Loss: 0.006765	Acc: 86.2% (6900/8000)
[Test]  Epoch: 93	Loss: 0.006731	Acc: 86.2% (6897/8000)
[Test]  Epoch: 94	Loss: 0.006688	Acc: 86.3% (6902/8000)
[Test]  Epoch: 95	Loss: 0.006705	Acc: 86.3% (6903/8000)
[Test]  Epoch: 96	Loss: 0.006732	Acc: 86.2% (6899/8000)
[Test]  Epoch: 97	Loss: 0.006694	Acc: 86.4% (6909/8000)
[Test]  Epoch: 98	Loss: 0.006699	Acc: 86.3% (6906/8000)
[Test]  Epoch: 99	Loss: 0.006722	Acc: 86.2% (6899/8000)
[Test]  Epoch: 100	Loss: 0.006719	Acc: 86.2% (6896/8000)
===========finish==========
['2024-09-04', '14:20:02.203198', '100', 'test', '0.006719158187508583', '86.2', '86.4375']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007556	Acc: 85.6% (6851/8000)
[Test]  Epoch: 2	Loss: 0.007355	Acc: 85.5% (6840/8000)
[Test]  Epoch: 3	Loss: 0.007375	Acc: 85.8% (6861/8000)
[Test]  Epoch: 4	Loss: 0.007266	Acc: 85.8% (6863/8000)
[Test]  Epoch: 5	Loss: 0.007216	Acc: 85.8% (6867/8000)
[Test]  Epoch: 6	Loss: 0.007198	Acc: 86.0% (6878/8000)
[Test]  Epoch: 7	Loss: 0.007192	Acc: 85.9% (6869/8000)
[Test]  Epoch: 8	Loss: 0.007097	Acc: 86.0% (6877/8000)
[Test]  Epoch: 9	Loss: 0.007052	Acc: 86.0% (6877/8000)
[Test]  Epoch: 10	Loss: 0.007120	Acc: 85.9% (6869/8000)
[Test]  Epoch: 11	Loss: 0.007083	Acc: 86.0% (6880/8000)
[Test]  Epoch: 12	Loss: 0.007042	Acc: 86.0% (6879/8000)
[Test]  Epoch: 13	Loss: 0.007007	Acc: 86.0% (6879/8000)
[Test]  Epoch: 14	Loss: 0.007001	Acc: 86.0% (6883/8000)
[Test]  Epoch: 15	Loss: 0.007043	Acc: 85.9% (6873/8000)
[Test]  Epoch: 16	Loss: 0.006987	Acc: 86.0% (6882/8000)
[Test]  Epoch: 17	Loss: 0.006986	Acc: 85.9% (6871/8000)
[Test]  Epoch: 18	Loss: 0.006994	Acc: 85.9% (6871/8000)
[Test]  Epoch: 19	Loss: 0.006946	Acc: 86.0% (6877/8000)
[Test]  Epoch: 20	Loss: 0.006935	Acc: 86.0% (6876/8000)
[Test]  Epoch: 21	Loss: 0.006939	Acc: 86.0% (6877/8000)
[Test]  Epoch: 22	Loss: 0.006894	Acc: 86.1% (6889/8000)
[Test]  Epoch: 23	Loss: 0.006963	Acc: 86.2% (6892/8000)
[Test]  Epoch: 24	Loss: 0.006875	Acc: 86.0% (6882/8000)
[Test]  Epoch: 25	Loss: 0.006966	Acc: 86.1% (6887/8000)
[Test]  Epoch: 26	Loss: 0.006851	Acc: 86.0% (6881/8000)
[Test]  Epoch: 27	Loss: 0.006879	Acc: 86.0% (6879/8000)
[Test]  Epoch: 28	Loss: 0.006891	Acc: 86.0% (6879/8000)
[Test]  Epoch: 29	Loss: 0.006831	Acc: 86.1% (6887/8000)
[Test]  Epoch: 30	Loss: 0.006926	Acc: 86.2% (6892/8000)
[Test]  Epoch: 31	Loss: 0.006820	Acc: 86.2% (6892/8000)
[Test]  Epoch: 32	Loss: 0.006832	Acc: 86.1% (6886/8000)
[Test]  Epoch: 33	Loss: 0.006859	Acc: 86.1% (6888/8000)
[Test]  Epoch: 34	Loss: 0.006867	Acc: 86.1% (6887/8000)
[Test]  Epoch: 35	Loss: 0.006813	Acc: 86.2% (6893/8000)
[Test]  Epoch: 36	Loss: 0.006789	Acc: 86.0% (6883/8000)
[Test]  Epoch: 37	Loss: 0.006810	Acc: 85.9% (6872/8000)
[Test]  Epoch: 38	Loss: 0.006791	Acc: 86.1% (6891/8000)
[Test]  Epoch: 39	Loss: 0.006802	Acc: 86.1% (6886/8000)
[Test]  Epoch: 40	Loss: 0.006788	Acc: 86.1% (6889/8000)
[Test]  Epoch: 41	Loss: 0.006828	Acc: 86.2% (6897/8000)
[Test]  Epoch: 42	Loss: 0.006787	Acc: 86.1% (6888/8000)
[Test]  Epoch: 43	Loss: 0.006794	Acc: 86.2% (6894/8000)
[Test]  Epoch: 44	Loss: 0.006774	Acc: 86.1% (6887/8000)
[Test]  Epoch: 45	Loss: 0.006774	Acc: 86.1% (6888/8000)
[Test]  Epoch: 46	Loss: 0.006768	Acc: 86.1% (6885/8000)
[Test]  Epoch: 47	Loss: 0.006737	Acc: 86.2% (6897/8000)
[Test]  Epoch: 48	Loss: 0.006790	Acc: 86.2% (6895/8000)
[Test]  Epoch: 49	Loss: 0.006783	Acc: 86.2% (6895/8000)
[Test]  Epoch: 50	Loss: 0.006824	Acc: 86.1% (6889/8000)
[Test]  Epoch: 51	Loss: 0.006777	Acc: 86.0% (6880/8000)
[Test]  Epoch: 52	Loss: 0.006770	Acc: 86.1% (6886/8000)
[Test]  Epoch: 53	Loss: 0.006734	Acc: 86.1% (6885/8000)
[Test]  Epoch: 54	Loss: 0.006777	Acc: 86.2% (6895/8000)
[Test]  Epoch: 55	Loss: 0.006796	Acc: 86.2% (6892/8000)
[Test]  Epoch: 56	Loss: 0.006811	Acc: 86.0% (6877/8000)
[Test]  Epoch: 57	Loss: 0.006726	Acc: 86.0% (6882/8000)
[Test]  Epoch: 58	Loss: 0.006748	Acc: 86.0% (6884/8000)
[Test]  Epoch: 59	Loss: 0.006747	Acc: 86.0% (6880/8000)
[Test]  Epoch: 60	Loss: 0.006768	Acc: 86.2% (6895/8000)
[Test]  Epoch: 61	Loss: 0.006689	Acc: 86.0% (6884/8000)
[Test]  Epoch: 62	Loss: 0.006721	Acc: 86.1% (6890/8000)
[Test]  Epoch: 63	Loss: 0.006755	Acc: 86.1% (6885/8000)
[Test]  Epoch: 64	Loss: 0.006716	Acc: 86.1% (6888/8000)
[Test]  Epoch: 65	Loss: 0.006730	Acc: 86.1% (6887/8000)
[Test]  Epoch: 66	Loss: 0.006746	Acc: 86.1% (6888/8000)
[Test]  Epoch: 67	Loss: 0.006796	Acc: 86.1% (6887/8000)
[Test]  Epoch: 68	Loss: 0.006728	Acc: 86.1% (6888/8000)
[Test]  Epoch: 69	Loss: 0.006687	Acc: 86.1% (6890/8000)
[Test]  Epoch: 70	Loss: 0.006696	Acc: 86.1% (6891/8000)
[Test]  Epoch: 71	Loss: 0.006725	Acc: 86.1% (6888/8000)
[Test]  Epoch: 72	Loss: 0.006733	Acc: 86.2% (6895/8000)
[Test]  Epoch: 73	Loss: 0.006748	Acc: 86.2% (6895/8000)
[Test]  Epoch: 74	Loss: 0.006692	Acc: 86.1% (6886/8000)
[Test]  Epoch: 75	Loss: 0.006704	Acc: 86.2% (6900/8000)
[Test]  Epoch: 76	Loss: 0.006727	Acc: 86.2% (6894/8000)
[Test]  Epoch: 77	Loss: 0.006676	Acc: 86.1% (6890/8000)
[Test]  Epoch: 78	Loss: 0.006704	Acc: 86.1% (6891/8000)
[Test]  Epoch: 79	Loss: 0.006745	Acc: 86.2% (6892/8000)
[Test]  Epoch: 80	Loss: 0.006711	Acc: 86.2% (6894/8000)
[Test]  Epoch: 81	Loss: 0.006682	Acc: 86.2% (6894/8000)
[Test]  Epoch: 82	Loss: 0.006680	Acc: 86.2% (6897/8000)
[Test]  Epoch: 83	Loss: 0.006719	Acc: 86.2% (6896/8000)
[Test]  Epoch: 84	Loss: 0.006761	Acc: 86.1% (6885/8000)
[Test]  Epoch: 85	Loss: 0.006737	Acc: 86.1% (6886/8000)
[Test]  Epoch: 86	Loss: 0.006744	Acc: 86.1% (6885/8000)
[Test]  Epoch: 87	Loss: 0.006744	Acc: 86.2% (6897/8000)
[Test]  Epoch: 88	Loss: 0.006706	Acc: 86.1% (6891/8000)
[Test]  Epoch: 89	Loss: 0.006711	Acc: 86.1% (6888/8000)
[Test]  Epoch: 90	Loss: 0.006738	Acc: 86.1% (6891/8000)
[Test]  Epoch: 91	Loss: 0.006726	Acc: 86.2% (6893/8000)
[Test]  Epoch: 92	Loss: 0.006749	Acc: 86.2% (6899/8000)
[Test]  Epoch: 93	Loss: 0.006712	Acc: 86.1% (6889/8000)
[Test]  Epoch: 94	Loss: 0.006683	Acc: 86.1% (6887/8000)
[Test]  Epoch: 95	Loss: 0.006700	Acc: 86.1% (6891/8000)
[Test]  Epoch: 96	Loss: 0.006717	Acc: 86.2% (6895/8000)
[Test]  Epoch: 97	Loss: 0.006685	Acc: 86.2% (6897/8000)
[Test]  Epoch: 98	Loss: 0.006689	Acc: 86.2% (6896/8000)
[Test]  Epoch: 99	Loss: 0.006711	Acc: 86.1% (6890/8000)
[Test]  Epoch: 100	Loss: 0.006717	Acc: 86.2% (6892/8000)
===========finish==========
['2024-09-04', '14:22:57.863776', '100', 'test', '0.0067167678121477364', '86.15', '86.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer3.1.bn2.weight', 'layer4.0.bn1.weight', 'bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007548	Acc: 85.5% (6843/8000)
[Test]  Epoch: 2	Loss: 0.007476	Acc: 85.5% (6840/8000)
[Test]  Epoch: 3	Loss: 0.007459	Acc: 85.6% (6850/8000)
[Test]  Epoch: 4	Loss: 0.007353	Acc: 85.9% (6871/8000)
[Test]  Epoch: 5	Loss: 0.007308	Acc: 85.9% (6869/8000)
[Test]  Epoch: 6	Loss: 0.007264	Acc: 86.0% (6880/8000)
[Test]  Epoch: 7	Loss: 0.007263	Acc: 86.0% (6883/8000)
[Test]  Epoch: 8	Loss: 0.007185	Acc: 85.9% (6874/8000)
[Test]  Epoch: 9	Loss: 0.007144	Acc: 85.9% (6875/8000)
[Test]  Epoch: 10	Loss: 0.007202	Acc: 86.0% (6876/8000)
[Test]  Epoch: 11	Loss: 0.007157	Acc: 86.0% (6878/8000)
[Test]  Epoch: 12	Loss: 0.007089	Acc: 86.2% (6894/8000)
[Test]  Epoch: 13	Loss: 0.007060	Acc: 86.1% (6886/8000)
[Test]  Epoch: 14	Loss: 0.007099	Acc: 86.0% (6877/8000)
[Test]  Epoch: 15	Loss: 0.007100	Acc: 85.9% (6875/8000)
[Test]  Epoch: 16	Loss: 0.007049	Acc: 86.2% (6894/8000)
[Test]  Epoch: 17	Loss: 0.007015	Acc: 86.1% (6889/8000)
[Test]  Epoch: 18	Loss: 0.007041	Acc: 86.0% (6878/8000)
[Test]  Epoch: 19	Loss: 0.006992	Acc: 86.0% (6883/8000)
[Test]  Epoch: 20	Loss: 0.006969	Acc: 86.1% (6887/8000)
[Test]  Epoch: 21	Loss: 0.007004	Acc: 86.0% (6882/8000)
[Test]  Epoch: 22	Loss: 0.006946	Acc: 86.1% (6891/8000)
[Test]  Epoch: 23	Loss: 0.007006	Acc: 86.0% (6878/8000)
[Test]  Epoch: 24	Loss: 0.006940	Acc: 86.1% (6888/8000)
[Test]  Epoch: 25	Loss: 0.006994	Acc: 86.1% (6888/8000)
[Test]  Epoch: 26	Loss: 0.006887	Acc: 86.1% (6888/8000)
[Test]  Epoch: 27	Loss: 0.006942	Acc: 86.1% (6888/8000)
[Test]  Epoch: 28	Loss: 0.006935	Acc: 86.2% (6892/8000)
[Test]  Epoch: 29	Loss: 0.006913	Acc: 86.1% (6887/8000)
[Test]  Epoch: 30	Loss: 0.006997	Acc: 86.1% (6886/8000)
[Test]  Epoch: 31	Loss: 0.006879	Acc: 86.1% (6890/8000)
[Test]  Epoch: 32	Loss: 0.006890	Acc: 86.0% (6882/8000)
[Test]  Epoch: 33	Loss: 0.006897	Acc: 86.0% (6884/8000)
[Test]  Epoch: 34	Loss: 0.006892	Acc: 86.2% (6892/8000)
[Test]  Epoch: 35	Loss: 0.006848	Acc: 86.3% (6902/8000)
[Test]  Epoch: 36	Loss: 0.006862	Acc: 86.2% (6893/8000)
[Test]  Epoch: 37	Loss: 0.006868	Acc: 86.2% (6898/8000)
[Test]  Epoch: 38	Loss: 0.006859	Acc: 86.1% (6890/8000)
[Test]  Epoch: 39	Loss: 0.006868	Acc: 86.1% (6891/8000)
[Test]  Epoch: 40	Loss: 0.006839	Acc: 86.2% (6892/8000)
[Test]  Epoch: 41	Loss: 0.006889	Acc: 86.1% (6889/8000)
[Test]  Epoch: 42	Loss: 0.006837	Acc: 86.2% (6898/8000)
[Test]  Epoch: 43	Loss: 0.006852	Acc: 86.2% (6892/8000)
[Test]  Epoch: 44	Loss: 0.006793	Acc: 86.1% (6887/8000)
[Test]  Epoch: 45	Loss: 0.006830	Acc: 86.1% (6891/8000)
[Test]  Epoch: 46	Loss: 0.006814	Acc: 86.2% (6892/8000)
[Test]  Epoch: 47	Loss: 0.006766	Acc: 86.2% (6895/8000)
[Test]  Epoch: 48	Loss: 0.006857	Acc: 86.1% (6889/8000)
[Test]  Epoch: 49	Loss: 0.006838	Acc: 86.1% (6889/8000)
[Test]  Epoch: 50	Loss: 0.006885	Acc: 86.1% (6889/8000)
[Test]  Epoch: 51	Loss: 0.006801	Acc: 86.1% (6890/8000)
[Test]  Epoch: 52	Loss: 0.006807	Acc: 86.1% (6888/8000)
[Test]  Epoch: 53	Loss: 0.006784	Acc: 86.1% (6890/8000)
[Test]  Epoch: 54	Loss: 0.006803	Acc: 86.2% (6895/8000)
[Test]  Epoch: 55	Loss: 0.006839	Acc: 86.2% (6892/8000)
[Test]  Epoch: 56	Loss: 0.006848	Acc: 86.1% (6891/8000)
[Test]  Epoch: 57	Loss: 0.006761	Acc: 86.3% (6902/8000)
[Test]  Epoch: 58	Loss: 0.006800	Acc: 86.1% (6886/8000)
[Test]  Epoch: 59	Loss: 0.006790	Acc: 86.2% (6897/8000)
[Test]  Epoch: 60	Loss: 0.006799	Acc: 86.1% (6889/8000)
[Test]  Epoch: 61	Loss: 0.006729	Acc: 86.2% (6896/8000)
[Test]  Epoch: 62	Loss: 0.006769	Acc: 86.1% (6890/8000)
[Test]  Epoch: 63	Loss: 0.006796	Acc: 86.1% (6887/8000)
[Test]  Epoch: 64	Loss: 0.006779	Acc: 86.0% (6884/8000)
[Test]  Epoch: 65	Loss: 0.006760	Acc: 86.2% (6895/8000)
[Test]  Epoch: 66	Loss: 0.006773	Acc: 86.0% (6884/8000)
[Test]  Epoch: 67	Loss: 0.006835	Acc: 86.2% (6893/8000)
[Test]  Epoch: 68	Loss: 0.006763	Acc: 86.2% (6895/8000)
[Test]  Epoch: 69	Loss: 0.006721	Acc: 86.2% (6895/8000)
[Test]  Epoch: 70	Loss: 0.006749	Acc: 86.1% (6890/8000)
[Test]  Epoch: 71	Loss: 0.006774	Acc: 86.3% (6903/8000)
[Test]  Epoch: 72	Loss: 0.006774	Acc: 86.2% (6893/8000)
[Test]  Epoch: 73	Loss: 0.006780	Acc: 86.1% (6891/8000)
[Test]  Epoch: 74	Loss: 0.006744	Acc: 86.1% (6891/8000)
[Test]  Epoch: 75	Loss: 0.006744	Acc: 86.1% (6886/8000)
[Test]  Epoch: 76	Loss: 0.006750	Acc: 86.1% (6888/8000)
[Test]  Epoch: 77	Loss: 0.006734	Acc: 86.1% (6888/8000)
[Test]  Epoch: 78	Loss: 0.006752	Acc: 86.2% (6894/8000)
[Test]  Epoch: 79	Loss: 0.006776	Acc: 86.1% (6890/8000)
[Test]  Epoch: 80	Loss: 0.006738	Acc: 86.2% (6892/8000)
[Test]  Epoch: 81	Loss: 0.006706	Acc: 86.1% (6887/8000)
[Test]  Epoch: 82	Loss: 0.006732	Acc: 86.1% (6891/8000)
[Test]  Epoch: 83	Loss: 0.006756	Acc: 86.2% (6895/8000)
[Test]  Epoch: 84	Loss: 0.006796	Acc: 86.2% (6892/8000)
[Test]  Epoch: 85	Loss: 0.006793	Acc: 86.2% (6897/8000)
[Test]  Epoch: 86	Loss: 0.006782	Acc: 86.1% (6889/8000)
[Test]  Epoch: 87	Loss: 0.006781	Acc: 86.0% (6882/8000)
[Test]  Epoch: 88	Loss: 0.006749	Acc: 86.2% (6899/8000)
[Test]  Epoch: 89	Loss: 0.006758	Acc: 86.1% (6891/8000)
[Test]  Epoch: 90	Loss: 0.006777	Acc: 86.2% (6895/8000)
[Test]  Epoch: 91	Loss: 0.006765	Acc: 86.2% (6896/8000)
[Test]  Epoch: 92	Loss: 0.006796	Acc: 86.2% (6899/8000)
[Test]  Epoch: 93	Loss: 0.006745	Acc: 86.2% (6897/8000)
[Test]  Epoch: 94	Loss: 0.006718	Acc: 86.1% (6891/8000)
[Test]  Epoch: 95	Loss: 0.006752	Acc: 86.2% (6895/8000)
[Test]  Epoch: 96	Loss: 0.006769	Acc: 86.0% (6884/8000)
[Test]  Epoch: 97	Loss: 0.006720	Acc: 86.2% (6893/8000)
[Test]  Epoch: 98	Loss: 0.006743	Acc: 85.9% (6875/8000)
[Test]  Epoch: 99	Loss: 0.006724	Acc: 86.2% (6898/8000)
[Test]  Epoch: 100	Loss: 0.006782	Acc: 86.2% (6894/8000)
===========finish==========
['2024-09-04', '14:25:55.554475', '100', 'test', '0.006782256756909192', '86.175', '86.2875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer3.1.bn2.weight', 'layer4.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.0.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007787	Acc: 85.1% (6810/8000)
[Test]  Epoch: 2	Loss: 0.007611	Acc: 85.2% (6819/8000)
[Test]  Epoch: 3	Loss: 0.007576	Acc: 85.4% (6832/8000)
[Test]  Epoch: 4	Loss: 0.007502	Acc: 85.4% (6835/8000)
[Test]  Epoch: 5	Loss: 0.007454	Acc: 85.5% (6836/8000)
[Test]  Epoch: 6	Loss: 0.007459	Acc: 85.7% (6853/8000)
[Test]  Epoch: 7	Loss: 0.007474	Acc: 85.3% (6828/8000)
[Test]  Epoch: 8	Loss: 0.007346	Acc: 85.7% (6852/8000)
[Test]  Epoch: 9	Loss: 0.007301	Acc: 85.8% (6860/8000)
[Test]  Epoch: 10	Loss: 0.007354	Acc: 85.6% (6847/8000)
[Test]  Epoch: 11	Loss: 0.007310	Acc: 85.8% (6861/8000)
[Test]  Epoch: 12	Loss: 0.007251	Acc: 85.7% (6854/8000)
[Test]  Epoch: 13	Loss: 0.007198	Acc: 85.7% (6856/8000)
[Test]  Epoch: 14	Loss: 0.007256	Acc: 85.7% (6852/8000)
[Test]  Epoch: 15	Loss: 0.007266	Acc: 85.7% (6855/8000)
[Test]  Epoch: 16	Loss: 0.007201	Acc: 85.8% (6862/8000)
[Test]  Epoch: 17	Loss: 0.007159	Acc: 85.7% (6854/8000)
[Test]  Epoch: 18	Loss: 0.007206	Acc: 85.6% (6849/8000)
[Test]  Epoch: 19	Loss: 0.007149	Acc: 85.7% (6857/8000)
[Test]  Epoch: 20	Loss: 0.007124	Acc: 85.6% (6846/8000)
[Test]  Epoch: 21	Loss: 0.007125	Acc: 85.5% (6842/8000)
[Test]  Epoch: 22	Loss: 0.007085	Acc: 85.8% (6860/8000)
[Test]  Epoch: 23	Loss: 0.007148	Acc: 85.6% (6846/8000)
[Test]  Epoch: 24	Loss: 0.007054	Acc: 85.6% (6850/8000)
[Test]  Epoch: 25	Loss: 0.007114	Acc: 85.7% (6859/8000)
[Test]  Epoch: 26	Loss: 0.007042	Acc: 85.7% (6856/8000)
[Test]  Epoch: 27	Loss: 0.007079	Acc: 85.7% (6854/8000)
[Test]  Epoch: 28	Loss: 0.007049	Acc: 85.7% (6856/8000)
[Test]  Epoch: 29	Loss: 0.007049	Acc: 85.7% (6857/8000)
[Test]  Epoch: 30	Loss: 0.007098	Acc: 85.8% (6860/8000)
[Test]  Epoch: 31	Loss: 0.007012	Acc: 85.6% (6846/8000)
[Test]  Epoch: 32	Loss: 0.007014	Acc: 85.7% (6856/8000)
[Test]  Epoch: 33	Loss: 0.007023	Acc: 85.8% (6862/8000)
[Test]  Epoch: 34	Loss: 0.007029	Acc: 85.5% (6842/8000)
[Test]  Epoch: 35	Loss: 0.006954	Acc: 85.7% (6857/8000)
[Test]  Epoch: 36	Loss: 0.006938	Acc: 85.6% (6851/8000)
[Test]  Epoch: 37	Loss: 0.006981	Acc: 85.6% (6850/8000)
[Test]  Epoch: 38	Loss: 0.006963	Acc: 85.6% (6848/8000)
[Test]  Epoch: 39	Loss: 0.006954	Acc: 85.7% (6858/8000)
[Test]  Epoch: 40	Loss: 0.006929	Acc: 85.5% (6843/8000)
[Test]  Epoch: 41	Loss: 0.006994	Acc: 85.7% (6853/8000)
[Test]  Epoch: 42	Loss: 0.006963	Acc: 85.6% (6851/8000)
[Test]  Epoch: 43	Loss: 0.006967	Acc: 85.8% (6862/8000)
[Test]  Epoch: 44	Loss: 0.006909	Acc: 85.7% (6854/8000)
[Test]  Epoch: 45	Loss: 0.006930	Acc: 85.6% (6849/8000)
[Test]  Epoch: 46	Loss: 0.006898	Acc: 85.8% (6864/8000)
[Test]  Epoch: 47	Loss: 0.006872	Acc: 85.9% (6871/8000)
[Test]  Epoch: 48	Loss: 0.006976	Acc: 85.7% (6859/8000)
[Test]  Epoch: 49	Loss: 0.006935	Acc: 85.9% (6869/8000)
[Test]  Epoch: 50	Loss: 0.007002	Acc: 85.5% (6840/8000)
[Test]  Epoch: 51	Loss: 0.006915	Acc: 85.6% (6848/8000)
[Test]  Epoch: 52	Loss: 0.006916	Acc: 85.6% (6849/8000)
[Test]  Epoch: 53	Loss: 0.006888	Acc: 85.7% (6854/8000)
[Test]  Epoch: 54	Loss: 0.006917	Acc: 85.7% (6853/8000)
[Test]  Epoch: 55	Loss: 0.006962	Acc: 85.7% (6854/8000)
[Test]  Epoch: 56	Loss: 0.006963	Acc: 85.7% (6852/8000)
[Test]  Epoch: 57	Loss: 0.006873	Acc: 85.8% (6863/8000)
[Test]  Epoch: 58	Loss: 0.006924	Acc: 85.6% (6851/8000)
[Test]  Epoch: 59	Loss: 0.006919	Acc: 85.7% (6856/8000)
[Test]  Epoch: 60	Loss: 0.006906	Acc: 85.7% (6853/8000)
[Test]  Epoch: 61	Loss: 0.006862	Acc: 85.8% (6861/8000)
[Test]  Epoch: 62	Loss: 0.006886	Acc: 85.7% (6857/8000)
[Test]  Epoch: 63	Loss: 0.006899	Acc: 85.5% (6841/8000)
[Test]  Epoch: 64	Loss: 0.006884	Acc: 85.6% (6848/8000)
[Test]  Epoch: 65	Loss: 0.006863	Acc: 85.7% (6854/8000)
[Test]  Epoch: 66	Loss: 0.006887	Acc: 85.7% (6856/8000)
[Test]  Epoch: 67	Loss: 0.006942	Acc: 85.5% (6842/8000)
[Test]  Epoch: 68	Loss: 0.006858	Acc: 85.8% (6862/8000)
[Test]  Epoch: 69	Loss: 0.006826	Acc: 85.7% (6858/8000)
[Test]  Epoch: 70	Loss: 0.006847	Acc: 85.8% (6862/8000)
[Test]  Epoch: 71	Loss: 0.006865	Acc: 85.7% (6857/8000)
[Test]  Epoch: 72	Loss: 0.006875	Acc: 85.8% (6863/8000)
[Test]  Epoch: 73	Loss: 0.006882	Acc: 85.8% (6861/8000)
[Test]  Epoch: 74	Loss: 0.006847	Acc: 85.6% (6846/8000)
[Test]  Epoch: 75	Loss: 0.006858	Acc: 85.8% (6861/8000)
[Test]  Epoch: 76	Loss: 0.006854	Acc: 85.9% (6869/8000)
[Test]  Epoch: 77	Loss: 0.006847	Acc: 85.7% (6858/8000)
[Test]  Epoch: 78	Loss: 0.006853	Acc: 85.6% (6851/8000)
[Test]  Epoch: 79	Loss: 0.006857	Acc: 85.7% (6854/8000)
[Test]  Epoch: 80	Loss: 0.006847	Acc: 85.7% (6854/8000)
[Test]  Epoch: 81	Loss: 0.006803	Acc: 85.7% (6856/8000)
[Test]  Epoch: 82	Loss: 0.006832	Acc: 85.7% (6858/8000)
[Test]  Epoch: 83	Loss: 0.006845	Acc: 85.8% (6868/8000)
[Test]  Epoch: 84	Loss: 0.006892	Acc: 85.7% (6859/8000)
[Test]  Epoch: 85	Loss: 0.006935	Acc: 85.7% (6856/8000)
[Test]  Epoch: 86	Loss: 0.006882	Acc: 85.6% (6851/8000)
[Test]  Epoch: 87	Loss: 0.006865	Acc: 85.6% (6851/8000)
[Test]  Epoch: 88	Loss: 0.006835	Acc: 85.6% (6851/8000)
[Test]  Epoch: 89	Loss: 0.006850	Acc: 85.7% (6858/8000)
[Test]  Epoch: 90	Loss: 0.006875	Acc: 85.8% (6864/8000)
[Test]  Epoch: 91	Loss: 0.006854	Acc: 85.7% (6858/8000)
[Test]  Epoch: 92	Loss: 0.006901	Acc: 85.8% (6863/8000)
[Test]  Epoch: 93	Loss: 0.006851	Acc: 85.8% (6863/8000)
[Test]  Epoch: 94	Loss: 0.006814	Acc: 85.8% (6865/8000)
[Test]  Epoch: 95	Loss: 0.006845	Acc: 85.8% (6866/8000)
[Test]  Epoch: 96	Loss: 0.006855	Acc: 85.7% (6859/8000)
[Test]  Epoch: 97	Loss: 0.006820	Acc: 85.8% (6864/8000)
[Test]  Epoch: 98	Loss: 0.006857	Acc: 85.8% (6860/8000)
[Test]  Epoch: 99	Loss: 0.006825	Acc: 85.8% (6865/8000)
[Test]  Epoch: 100	Loss: 0.006883	Acc: 85.8% (6864/8000)
===========finish==========
['2024-09-04', '14:28:53.115694', '100', 'test', '0.006882839687168598', '85.8', '85.8875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer3.1.bn2.weight', 'layer4.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.0.bn2.weight', 'layer2.0.bn2.weight', 'last_linear.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.012913	Acc: 83.5% (6682/8000)
[Test]  Epoch: 2	Loss: 0.008292	Acc: 85.0% (6802/8000)
[Test]  Epoch: 3	Loss: 0.007133	Acc: 85.3% (6821/8000)
[Test]  Epoch: 4	Loss: 0.006849	Acc: 85.5% (6840/8000)
[Test]  Epoch: 5	Loss: 0.006776	Acc: 85.6% (6849/8000)
[Test]  Epoch: 6	Loss: 0.006754	Acc: 85.6% (6848/8000)
[Test]  Epoch: 7	Loss: 0.006812	Acc: 85.4% (6833/8000)
[Test]  Epoch: 8	Loss: 0.006731	Acc: 85.7% (6855/8000)
[Test]  Epoch: 9	Loss: 0.006774	Acc: 85.7% (6852/8000)
[Test]  Epoch: 10	Loss: 0.006745	Acc: 85.7% (6857/8000)
[Test]  Epoch: 11	Loss: 0.006723	Acc: 85.8% (6867/8000)
[Test]  Epoch: 12	Loss: 0.006737	Acc: 85.4% (6834/8000)
[Test]  Epoch: 13	Loss: 0.006692	Acc: 85.7% (6859/8000)
[Test]  Epoch: 14	Loss: 0.006666	Acc: 85.9% (6869/8000)
[Test]  Epoch: 15	Loss: 0.006741	Acc: 85.6% (6845/8000)
[Test]  Epoch: 16	Loss: 0.006730	Acc: 85.6% (6850/8000)
[Test]  Epoch: 17	Loss: 0.006695	Acc: 85.8% (6861/8000)
[Test]  Epoch: 18	Loss: 0.006678	Acc: 85.7% (6859/8000)
[Test]  Epoch: 19	Loss: 0.006688	Acc: 85.7% (6856/8000)
[Test]  Epoch: 20	Loss: 0.006709	Acc: 85.8% (6864/8000)
[Test]  Epoch: 21	Loss: 0.006681	Acc: 85.8% (6860/8000)
[Test]  Epoch: 22	Loss: 0.006677	Acc: 85.8% (6861/8000)
[Test]  Epoch: 23	Loss: 0.006702	Acc: 85.7% (6858/8000)
[Test]  Epoch: 24	Loss: 0.006702	Acc: 85.7% (6859/8000)
[Test]  Epoch: 25	Loss: 0.006735	Acc: 85.8% (6860/8000)
[Test]  Epoch: 26	Loss: 0.006628	Acc: 85.9% (6869/8000)
[Test]  Epoch: 27	Loss: 0.006621	Acc: 86.0% (6884/8000)
[Test]  Epoch: 28	Loss: 0.006666	Acc: 85.7% (6858/8000)
[Test]  Epoch: 29	Loss: 0.006585	Acc: 85.9% (6869/8000)
[Test]  Epoch: 30	Loss: 0.006672	Acc: 85.8% (6862/8000)
[Test]  Epoch: 31	Loss: 0.006604	Acc: 85.8% (6861/8000)
[Test]  Epoch: 32	Loss: 0.006621	Acc: 85.8% (6861/8000)
[Test]  Epoch: 33	Loss: 0.006666	Acc: 85.8% (6866/8000)
[Test]  Epoch: 34	Loss: 0.006689	Acc: 85.6% (6848/8000)
[Test]  Epoch: 35	Loss: 0.006615	Acc: 85.6% (6851/8000)
[Test]  Epoch: 36	Loss: 0.006616	Acc: 85.7% (6857/8000)
[Test]  Epoch: 37	Loss: 0.006617	Acc: 85.8% (6860/8000)
[Test]  Epoch: 38	Loss: 0.006629	Acc: 85.8% (6866/8000)
[Test]  Epoch: 39	Loss: 0.006584	Acc: 85.8% (6864/8000)
[Test]  Epoch: 40	Loss: 0.006579	Acc: 85.9% (6872/8000)
[Test]  Epoch: 41	Loss: 0.006589	Acc: 85.9% (6875/8000)
[Test]  Epoch: 42	Loss: 0.006603	Acc: 85.7% (6855/8000)
[Test]  Epoch: 43	Loss: 0.006572	Acc: 85.9% (6873/8000)
[Test]  Epoch: 44	Loss: 0.006536	Acc: 86.0% (6881/8000)
[Test]  Epoch: 45	Loss: 0.006563	Acc: 85.9% (6872/8000)
[Test]  Epoch: 46	Loss: 0.006562	Acc: 86.0% (6880/8000)
[Test]  Epoch: 47	Loss: 0.006598	Acc: 85.8% (6868/8000)
[Test]  Epoch: 48	Loss: 0.006613	Acc: 85.8% (6865/8000)
[Test]  Epoch: 49	Loss: 0.006596	Acc: 85.9% (6874/8000)
[Test]  Epoch: 50	Loss: 0.006652	Acc: 85.5% (6844/8000)
[Test]  Epoch: 51	Loss: 0.006607	Acc: 85.9% (6874/8000)
[Test]  Epoch: 52	Loss: 0.006633	Acc: 85.7% (6852/8000)
[Test]  Epoch: 53	Loss: 0.006614	Acc: 85.8% (6862/8000)
[Test]  Epoch: 54	Loss: 0.006605	Acc: 85.7% (6854/8000)
[Test]  Epoch: 55	Loss: 0.006632	Acc: 86.0% (6876/8000)
[Test]  Epoch: 56	Loss: 0.006594	Acc: 85.8% (6866/8000)
[Test]  Epoch: 57	Loss: 0.006544	Acc: 85.9% (6870/8000)
[Test]  Epoch: 58	Loss: 0.006572	Acc: 86.0% (6879/8000)
[Test]  Epoch: 59	Loss: 0.006579	Acc: 85.9% (6869/8000)
[Test]  Epoch: 60	Loss: 0.006566	Acc: 85.9% (6869/8000)
[Test]  Epoch: 61	Loss: 0.006546	Acc: 85.9% (6875/8000)
[Test]  Epoch: 62	Loss: 0.006545	Acc: 86.0% (6881/8000)
[Test]  Epoch: 63	Loss: 0.006582	Acc: 85.9% (6869/8000)
[Test]  Epoch: 64	Loss: 0.006575	Acc: 86.0% (6880/8000)
[Test]  Epoch: 65	Loss: 0.006546	Acc: 85.9% (6873/8000)
[Test]  Epoch: 66	Loss: 0.006559	Acc: 86.0% (6878/8000)
[Test]  Epoch: 67	Loss: 0.006566	Acc: 85.9% (6871/8000)
[Test]  Epoch: 68	Loss: 0.006577	Acc: 85.8% (6865/8000)
[Test]  Epoch: 69	Loss: 0.006552	Acc: 85.9% (6869/8000)
[Test]  Epoch: 70	Loss: 0.006527	Acc: 85.9% (6874/8000)
[Test]  Epoch: 71	Loss: 0.006538	Acc: 86.0% (6878/8000)
[Test]  Epoch: 72	Loss: 0.006567	Acc: 86.0% (6882/8000)
[Test]  Epoch: 73	Loss: 0.006564	Acc: 85.9% (6870/8000)
[Test]  Epoch: 74	Loss: 0.006548	Acc: 86.0% (6877/8000)
[Test]  Epoch: 75	Loss: 0.006549	Acc: 85.9% (6875/8000)
[Test]  Epoch: 76	Loss: 0.006560	Acc: 86.0% (6876/8000)
[Test]  Epoch: 77	Loss: 0.006506	Acc: 86.0% (6879/8000)
[Test]  Epoch: 78	Loss: 0.006545	Acc: 85.8% (6868/8000)
[Test]  Epoch: 79	Loss: 0.006556	Acc: 85.9% (6871/8000)
[Test]  Epoch: 80	Loss: 0.006528	Acc: 86.0% (6876/8000)
[Test]  Epoch: 81	Loss: 0.006548	Acc: 86.0% (6879/8000)
[Test]  Epoch: 82	Loss: 0.006570	Acc: 85.8% (6864/8000)
[Test]  Epoch: 83	Loss: 0.006535	Acc: 86.0% (6880/8000)
[Test]  Epoch: 84	Loss: 0.006583	Acc: 85.8% (6868/8000)
[Test]  Epoch: 85	Loss: 0.006531	Acc: 85.9% (6869/8000)
[Test]  Epoch: 86	Loss: 0.006557	Acc: 85.9% (6873/8000)
[Test]  Epoch: 87	Loss: 0.006567	Acc: 85.8% (6865/8000)
[Test]  Epoch: 88	Loss: 0.006526	Acc: 85.9% (6873/8000)
[Test]  Epoch: 89	Loss: 0.006541	Acc: 85.9% (6869/8000)
[Test]  Epoch: 90	Loss: 0.006565	Acc: 85.8% (6864/8000)
[Test]  Epoch: 91	Loss: 0.006561	Acc: 85.8% (6862/8000)
[Test]  Epoch: 92	Loss: 0.006528	Acc: 85.9% (6874/8000)
[Test]  Epoch: 93	Loss: 0.006535	Acc: 85.9% (6875/8000)
[Test]  Epoch: 94	Loss: 0.006547	Acc: 85.8% (6868/8000)
[Test]  Epoch: 95	Loss: 0.006541	Acc: 85.8% (6865/8000)
[Test]  Epoch: 96	Loss: 0.006530	Acc: 85.9% (6874/8000)
[Test]  Epoch: 97	Loss: 0.006525	Acc: 86.1% (6885/8000)
[Test]  Epoch: 98	Loss: 0.006548	Acc: 86.0% (6882/8000)
[Test]  Epoch: 99	Loss: 0.006516	Acc: 85.8% (6867/8000)
[Test]  Epoch: 100	Loss: 0.006541	Acc: 86.0% (6881/8000)
===========finish==========
['2024-09-04', '14:31:35.866149', '100', 'test', '0.006541316130198539', '86.0125', '86.0625']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer3.1.bn2.weight', 'layer4.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.0.bn2.weight', 'layer2.0.bn2.weight', 'last_linear.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.013699	Acc: 82.4% (6589/8000)
[Test]  Epoch: 2	Loss: 0.008199	Acc: 84.3% (6741/8000)
[Test]  Epoch: 3	Loss: 0.007385	Acc: 84.5% (6758/8000)
[Test]  Epoch: 4	Loss: 0.007176	Acc: 84.8% (6786/8000)
[Test]  Epoch: 5	Loss: 0.007065	Acc: 84.9% (6792/8000)
[Test]  Epoch: 6	Loss: 0.007066	Acc: 85.3% (6821/8000)
[Test]  Epoch: 7	Loss: 0.007062	Acc: 85.2% (6813/8000)
[Test]  Epoch: 8	Loss: 0.006938	Acc: 85.2% (6817/8000)
[Test]  Epoch: 9	Loss: 0.006981	Acc: 85.4% (6835/8000)
[Test]  Epoch: 10	Loss: 0.006923	Acc: 85.4% (6830/8000)
[Test]  Epoch: 11	Loss: 0.006902	Acc: 85.4% (6832/8000)
[Test]  Epoch: 12	Loss: 0.006907	Acc: 85.4% (6835/8000)
[Test]  Epoch: 13	Loss: 0.006876	Acc: 85.5% (6836/8000)
[Test]  Epoch: 14	Loss: 0.006861	Acc: 85.4% (6833/8000)
[Test]  Epoch: 15	Loss: 0.006906	Acc: 85.3% (6826/8000)
[Test]  Epoch: 16	Loss: 0.006900	Acc: 85.3% (6822/8000)
[Test]  Epoch: 17	Loss: 0.006829	Acc: 85.5% (6844/8000)
[Test]  Epoch: 18	Loss: 0.006854	Acc: 85.5% (6836/8000)
[Test]  Epoch: 19	Loss: 0.006850	Acc: 85.3% (6824/8000)
[Test]  Epoch: 20	Loss: 0.006875	Acc: 85.4% (6832/8000)
[Test]  Epoch: 21	Loss: 0.006850	Acc: 85.6% (6847/8000)
[Test]  Epoch: 22	Loss: 0.006837	Acc: 85.5% (6842/8000)
[Test]  Epoch: 23	Loss: 0.006856	Acc: 85.4% (6834/8000)
[Test]  Epoch: 24	Loss: 0.006869	Acc: 85.4% (6831/8000)
[Test]  Epoch: 25	Loss: 0.006880	Acc: 85.3% (6821/8000)
[Test]  Epoch: 26	Loss: 0.006812	Acc: 85.3% (6824/8000)
[Test]  Epoch: 27	Loss: 0.006765	Acc: 85.6% (6848/8000)
[Test]  Epoch: 28	Loss: 0.006827	Acc: 85.4% (6833/8000)
[Test]  Epoch: 29	Loss: 0.006765	Acc: 85.5% (6840/8000)
[Test]  Epoch: 30	Loss: 0.006842	Acc: 85.4% (6833/8000)
[Test]  Epoch: 31	Loss: 0.006753	Acc: 85.4% (6830/8000)
[Test]  Epoch: 32	Loss: 0.006787	Acc: 85.5% (6842/8000)
[Test]  Epoch: 33	Loss: 0.006808	Acc: 85.4% (6830/8000)
[Test]  Epoch: 34	Loss: 0.006806	Acc: 85.5% (6842/8000)
[Test]  Epoch: 35	Loss: 0.006741	Acc: 85.6% (6845/8000)
[Test]  Epoch: 36	Loss: 0.006724	Acc: 85.5% (6844/8000)
[Test]  Epoch: 37	Loss: 0.006722	Acc: 85.6% (6851/8000)
[Test]  Epoch: 38	Loss: 0.006721	Acc: 85.6% (6850/8000)
[Test]  Epoch: 39	Loss: 0.006727	Acc: 85.4% (6829/8000)
[Test]  Epoch: 40	Loss: 0.006713	Acc: 85.4% (6833/8000)
[Test]  Epoch: 41	Loss: 0.006752	Acc: 85.4% (6832/8000)
[Test]  Epoch: 42	Loss: 0.006745	Acc: 85.5% (6842/8000)
[Test]  Epoch: 43	Loss: 0.006691	Acc: 85.5% (6842/8000)
[Test]  Epoch: 44	Loss: 0.006681	Acc: 85.5% (6843/8000)
[Test]  Epoch: 45	Loss: 0.006693	Acc: 85.5% (6837/8000)
[Test]  Epoch: 46	Loss: 0.006678	Acc: 85.7% (6856/8000)
[Test]  Epoch: 47	Loss: 0.006715	Acc: 85.5% (6841/8000)
[Test]  Epoch: 48	Loss: 0.006721	Acc: 85.6% (6848/8000)
[Test]  Epoch: 49	Loss: 0.006679	Acc: 85.6% (6848/8000)
[Test]  Epoch: 50	Loss: 0.006705	Acc: 85.5% (6842/8000)
[Test]  Epoch: 51	Loss: 0.006726	Acc: 85.5% (6839/8000)
[Test]  Epoch: 52	Loss: 0.006695	Acc: 85.5% (6841/8000)
[Test]  Epoch: 53	Loss: 0.006695	Acc: 85.4% (6835/8000)
[Test]  Epoch: 54	Loss: 0.006693	Acc: 85.4% (6835/8000)
[Test]  Epoch: 55	Loss: 0.006740	Acc: 85.5% (6841/8000)
[Test]  Epoch: 56	Loss: 0.006725	Acc: 85.6% (6847/8000)
[Test]  Epoch: 57	Loss: 0.006667	Acc: 85.8% (6861/8000)
[Test]  Epoch: 58	Loss: 0.006685	Acc: 85.8% (6862/8000)
[Test]  Epoch: 59	Loss: 0.006701	Acc: 85.6% (6847/8000)
[Test]  Epoch: 60	Loss: 0.006680	Acc: 85.7% (6854/8000)
[Test]  Epoch: 61	Loss: 0.006664	Acc: 85.7% (6855/8000)
[Test]  Epoch: 62	Loss: 0.006655	Acc: 85.8% (6863/8000)
[Test]  Epoch: 63	Loss: 0.006694	Acc: 85.6% (6848/8000)
[Test]  Epoch: 64	Loss: 0.006677	Acc: 85.7% (6852/8000)
[Test]  Epoch: 65	Loss: 0.006673	Acc: 85.9% (6869/8000)
[Test]  Epoch: 66	Loss: 0.006653	Acc: 85.7% (6859/8000)
[Test]  Epoch: 67	Loss: 0.006683	Acc: 85.7% (6856/8000)
[Test]  Epoch: 68	Loss: 0.006691	Acc: 85.8% (6865/8000)
[Test]  Epoch: 69	Loss: 0.006659	Acc: 85.7% (6852/8000)
[Test]  Epoch: 70	Loss: 0.006652	Acc: 85.6% (6849/8000)
[Test]  Epoch: 71	Loss: 0.006662	Acc: 85.7% (6859/8000)
[Test]  Epoch: 72	Loss: 0.006663	Acc: 85.8% (6865/8000)
[Test]  Epoch: 73	Loss: 0.006672	Acc: 85.6% (6847/8000)
[Test]  Epoch: 74	Loss: 0.006646	Acc: 85.7% (6858/8000)
[Test]  Epoch: 75	Loss: 0.006653	Acc: 85.7% (6856/8000)
[Test]  Epoch: 76	Loss: 0.006667	Acc: 85.6% (6846/8000)
[Test]  Epoch: 77	Loss: 0.006641	Acc: 85.7% (6858/8000)
[Test]  Epoch: 78	Loss: 0.006672	Acc: 85.7% (6854/8000)
[Test]  Epoch: 79	Loss: 0.006665	Acc: 85.7% (6859/8000)
[Test]  Epoch: 80	Loss: 0.006642	Acc: 85.6% (6851/8000)
[Test]  Epoch: 81	Loss: 0.006670	Acc: 85.6% (6850/8000)
[Test]  Epoch: 82	Loss: 0.006678	Acc: 85.6% (6849/8000)
[Test]  Epoch: 83	Loss: 0.006660	Acc: 85.8% (6863/8000)
[Test]  Epoch: 84	Loss: 0.006683	Acc: 85.7% (6852/8000)
[Test]  Epoch: 85	Loss: 0.006654	Acc: 85.8% (6867/8000)
[Test]  Epoch: 86	Loss: 0.006665	Acc: 85.8% (6868/8000)
[Test]  Epoch: 87	Loss: 0.006683	Acc: 85.8% (6864/8000)
[Test]  Epoch: 88	Loss: 0.006643	Acc: 85.6% (6847/8000)
[Test]  Epoch: 89	Loss: 0.006650	Acc: 85.8% (6863/8000)
[Test]  Epoch: 90	Loss: 0.006677	Acc: 85.7% (6858/8000)
[Test]  Epoch: 91	Loss: 0.006662	Acc: 85.7% (6855/8000)
[Test]  Epoch: 92	Loss: 0.006636	Acc: 85.7% (6856/8000)
[Test]  Epoch: 93	Loss: 0.006650	Acc: 85.7% (6856/8000)
[Test]  Epoch: 94	Loss: 0.006681	Acc: 85.7% (6855/8000)
[Test]  Epoch: 95	Loss: 0.006665	Acc: 85.7% (6856/8000)
[Test]  Epoch: 96	Loss: 0.006650	Acc: 85.8% (6861/8000)
[Test]  Epoch: 97	Loss: 0.006644	Acc: 85.6% (6849/8000)
[Test]  Epoch: 98	Loss: 0.006669	Acc: 85.5% (6842/8000)
[Test]  Epoch: 99	Loss: 0.006649	Acc: 85.9% (6869/8000)
[Test]  Epoch: 100	Loss: 0.006651	Acc: 85.9% (6871/8000)
===========finish==========
['2024-09-04', '14:33:51.448610', '100', 'test', '0.006650766206905246', '85.8875', '85.8875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer3.1.bn2.weight', 'layer4.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.0.bn2.weight', 'layer2.0.bn2.weight', 'last_linear.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer1.0.conv2.weight', 'layer4.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.017817	Acc: 75.0% (5996/8000)
[Test]  Epoch: 2	Loss: 0.010566	Acc: 79.5% (6363/8000)
[Test]  Epoch: 3	Loss: 0.009178	Acc: 81.2% (6495/8000)
[Test]  Epoch: 4	Loss: 0.008806	Acc: 81.0% (6480/8000)
[Test]  Epoch: 5	Loss: 0.008561	Acc: 81.7% (6537/8000)
[Test]  Epoch: 6	Loss: 0.008451	Acc: 82.0% (6561/8000)
[Test]  Epoch: 7	Loss: 0.008388	Acc: 82.1% (6571/8000)
[Test]  Epoch: 8	Loss: 0.008267	Acc: 82.3% (6585/8000)
[Test]  Epoch: 9	Loss: 0.008304	Acc: 82.1% (6571/8000)
[Test]  Epoch: 10	Loss: 0.008337	Acc: 82.3% (6587/8000)
[Test]  Epoch: 11	Loss: 0.008210	Acc: 82.6% (6608/8000)
[Test]  Epoch: 12	Loss: 0.008212	Acc: 82.7% (6618/8000)
[Test]  Epoch: 13	Loss: 0.008233	Acc: 82.7% (6613/8000)
[Test]  Epoch: 14	Loss: 0.008185	Acc: 82.8% (6623/8000)
[Test]  Epoch: 15	Loss: 0.008310	Acc: 82.6% (6611/8000)
[Test]  Epoch: 16	Loss: 0.008230	Acc: 82.8% (6628/8000)
[Test]  Epoch: 17	Loss: 0.008143	Acc: 82.8% (6625/8000)
[Test]  Epoch: 18	Loss: 0.008152	Acc: 82.5% (6604/8000)
[Test]  Epoch: 19	Loss: 0.008176	Acc: 82.6% (6607/8000)
[Test]  Epoch: 20	Loss: 0.008147	Acc: 82.7% (6616/8000)
[Test]  Epoch: 21	Loss: 0.008148	Acc: 82.7% (6614/8000)
[Test]  Epoch: 22	Loss: 0.008149	Acc: 82.7% (6614/8000)
[Test]  Epoch: 23	Loss: 0.008178	Acc: 82.8% (6621/8000)
[Test]  Epoch: 24	Loss: 0.008145	Acc: 83.0% (6636/8000)
[Test]  Epoch: 25	Loss: 0.008158	Acc: 82.8% (6620/8000)
[Test]  Epoch: 26	Loss: 0.008106	Acc: 82.8% (6624/8000)
[Test]  Epoch: 27	Loss: 0.008071	Acc: 83.0% (6643/8000)
[Test]  Epoch: 28	Loss: 0.008114	Acc: 82.8% (6626/8000)
[Test]  Epoch: 29	Loss: 0.008076	Acc: 83.1% (6647/8000)
[Test]  Epoch: 30	Loss: 0.008132	Acc: 82.8% (6625/8000)
[Test]  Epoch: 31	Loss: 0.008055	Acc: 83.0% (6641/8000)
[Test]  Epoch: 32	Loss: 0.008070	Acc: 83.1% (6649/8000)
[Test]  Epoch: 33	Loss: 0.008049	Acc: 83.2% (6655/8000)
[Test]  Epoch: 34	Loss: 0.008099	Acc: 82.8% (6622/8000)
[Test]  Epoch: 35	Loss: 0.008056	Acc: 83.0% (6643/8000)
[Test]  Epoch: 36	Loss: 0.008047	Acc: 83.0% (6637/8000)
[Test]  Epoch: 37	Loss: 0.008027	Acc: 82.8% (6620/8000)
[Test]  Epoch: 38	Loss: 0.008077	Acc: 82.8% (6622/8000)
[Test]  Epoch: 39	Loss: 0.008006	Acc: 82.8% (6628/8000)
[Test]  Epoch: 40	Loss: 0.007985	Acc: 83.0% (6641/8000)
[Test]  Epoch: 41	Loss: 0.008007	Acc: 83.0% (6638/8000)
[Test]  Epoch: 42	Loss: 0.008030	Acc: 83.0% (6644/8000)
[Test]  Epoch: 43	Loss: 0.008043	Acc: 82.9% (6630/8000)
[Test]  Epoch: 44	Loss: 0.008022	Acc: 83.0% (6640/8000)
[Test]  Epoch: 45	Loss: 0.007977	Acc: 83.0% (6644/8000)
[Test]  Epoch: 46	Loss: 0.008033	Acc: 82.8% (6624/8000)
[Test]  Epoch: 47	Loss: 0.008036	Acc: 83.0% (6637/8000)
[Test]  Epoch: 48	Loss: 0.008031	Acc: 83.0% (6636/8000)
[Test]  Epoch: 49	Loss: 0.007961	Acc: 83.1% (6647/8000)
[Test]  Epoch: 50	Loss: 0.008045	Acc: 83.0% (6640/8000)
[Test]  Epoch: 51	Loss: 0.008016	Acc: 83.0% (6638/8000)
[Test]  Epoch: 52	Loss: 0.008043	Acc: 82.9% (6631/8000)
[Test]  Epoch: 53	Loss: 0.008014	Acc: 83.0% (6642/8000)
[Test]  Epoch: 54	Loss: 0.008022	Acc: 83.0% (6642/8000)
[Test]  Epoch: 55	Loss: 0.008040	Acc: 83.0% (6640/8000)
[Test]  Epoch: 56	Loss: 0.008023	Acc: 83.0% (6638/8000)
[Test]  Epoch: 57	Loss: 0.007972	Acc: 83.1% (6649/8000)
[Test]  Epoch: 58	Loss: 0.007997	Acc: 83.0% (6636/8000)
[Test]  Epoch: 59	Loss: 0.008029	Acc: 82.9% (6630/8000)
[Test]  Epoch: 60	Loss: 0.008002	Acc: 82.9% (6629/8000)
[Test]  Epoch: 61	Loss: 0.007970	Acc: 83.1% (6648/8000)
[Test]  Epoch: 62	Loss: 0.007932	Acc: 83.1% (6651/8000)
[Test]  Epoch: 63	Loss: 0.007997	Acc: 82.9% (6633/8000)
[Test]  Epoch: 64	Loss: 0.007998	Acc: 82.8% (6625/8000)
[Test]  Epoch: 65	Loss: 0.008015	Acc: 82.8% (6625/8000)
[Test]  Epoch: 66	Loss: 0.007957	Acc: 83.0% (6637/8000)
[Test]  Epoch: 67	Loss: 0.008012	Acc: 82.8% (6628/8000)
[Test]  Epoch: 68	Loss: 0.007978	Acc: 83.0% (6643/8000)
[Test]  Epoch: 69	Loss: 0.007944	Acc: 83.1% (6648/8000)
[Test]  Epoch: 70	Loss: 0.007953	Acc: 83.0% (6643/8000)
[Test]  Epoch: 71	Loss: 0.007944	Acc: 83.0% (6644/8000)
[Test]  Epoch: 72	Loss: 0.007966	Acc: 83.0% (6638/8000)
[Test]  Epoch: 73	Loss: 0.007949	Acc: 83.1% (6646/8000)
[Test]  Epoch: 74	Loss: 0.007925	Acc: 83.0% (6642/8000)
[Test]  Epoch: 75	Loss: 0.007939	Acc: 83.1% (6649/8000)
[Test]  Epoch: 76	Loss: 0.007947	Acc: 83.2% (6653/8000)
[Test]  Epoch: 77	Loss: 0.007970	Acc: 83.0% (6637/8000)
[Test]  Epoch: 78	Loss: 0.007978	Acc: 83.0% (6637/8000)
[Test]  Epoch: 79	Loss: 0.007972	Acc: 83.1% (6645/8000)
[Test]  Epoch: 80	Loss: 0.007938	Acc: 83.1% (6648/8000)
[Test]  Epoch: 81	Loss: 0.007938	Acc: 83.1% (6650/8000)
[Test]  Epoch: 82	Loss: 0.007973	Acc: 83.1% (6648/8000)
[Test]  Epoch: 83	Loss: 0.007970	Acc: 83.0% (6638/8000)
[Test]  Epoch: 84	Loss: 0.007992	Acc: 83.1% (6645/8000)
[Test]  Epoch: 85	Loss: 0.007978	Acc: 83.0% (6639/8000)
[Test]  Epoch: 86	Loss: 0.007958	Acc: 83.0% (6642/8000)
[Test]  Epoch: 87	Loss: 0.008005	Acc: 82.9% (6634/8000)
[Test]  Epoch: 88	Loss: 0.007962	Acc: 83.0% (6640/8000)
[Test]  Epoch: 89	Loss: 0.007974	Acc: 83.1% (6645/8000)
[Test]  Epoch: 90	Loss: 0.007989	Acc: 82.9% (6635/8000)
[Test]  Epoch: 91	Loss: 0.007952	Acc: 83.0% (6638/8000)
[Test]  Epoch: 92	Loss: 0.007922	Acc: 83.2% (6653/8000)
[Test]  Epoch: 93	Loss: 0.007918	Acc: 83.2% (6653/8000)
[Test]  Epoch: 94	Loss: 0.007976	Acc: 83.1% (6650/8000)
[Test]  Epoch: 95	Loss: 0.007961	Acc: 83.1% (6647/8000)
[Test]  Epoch: 96	Loss: 0.007923	Acc: 83.0% (6639/8000)
[Test]  Epoch: 97	Loss: 0.007915	Acc: 83.2% (6655/8000)
[Test]  Epoch: 98	Loss: 0.007977	Acc: 83.0% (6636/8000)
[Test]  Epoch: 99	Loss: 0.007959	Acc: 83.0% (6639/8000)
[Test]  Epoch: 100	Loss: 0.007939	Acc: 83.1% (6650/8000)
===========finish==========
['2024-09-04', '14:36:07.992994', '100', 'test', '0.007938882993534208', '83.125', '83.1875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer3.1.bn2.weight', 'layer4.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.0.bn2.weight', 'layer2.0.bn2.weight', 'last_linear.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer1.0.conv2.weight', 'layer4.0.conv2.weight', 'layer1.1.conv1.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.017884	Acc: 75.2% (6012/8000)
[Test]  Epoch: 2	Loss: 0.010473	Acc: 80.0% (6399/8000)
[Test]  Epoch: 3	Loss: 0.008989	Acc: 81.3% (6506/8000)
[Test]  Epoch: 4	Loss: 0.008607	Acc: 81.9% (6551/8000)
[Test]  Epoch: 5	Loss: 0.008353	Acc: 82.4% (6590/8000)
[Test]  Epoch: 6	Loss: 0.008248	Acc: 82.7% (6612/8000)
[Test]  Epoch: 7	Loss: 0.008259	Acc: 82.4% (6589/8000)
[Test]  Epoch: 8	Loss: 0.008077	Acc: 82.9% (6629/8000)
[Test]  Epoch: 9	Loss: 0.008113	Acc: 82.8% (6623/8000)
[Test]  Epoch: 10	Loss: 0.008116	Acc: 82.8% (6622/8000)
[Test]  Epoch: 11	Loss: 0.008034	Acc: 83.1% (6649/8000)
[Test]  Epoch: 12	Loss: 0.008031	Acc: 83.1% (6646/8000)
[Test]  Epoch: 13	Loss: 0.008025	Acc: 83.0% (6644/8000)
[Test]  Epoch: 14	Loss: 0.008034	Acc: 83.0% (6638/8000)
[Test]  Epoch: 15	Loss: 0.008083	Acc: 83.0% (6641/8000)
[Test]  Epoch: 16	Loss: 0.007999	Acc: 83.1% (6649/8000)
[Test]  Epoch: 17	Loss: 0.007929	Acc: 83.2% (6654/8000)
[Test]  Epoch: 18	Loss: 0.007986	Acc: 83.2% (6652/8000)
[Test]  Epoch: 19	Loss: 0.007989	Acc: 83.1% (6649/8000)
[Test]  Epoch: 20	Loss: 0.007976	Acc: 83.2% (6659/8000)
[Test]  Epoch: 21	Loss: 0.007936	Acc: 83.3% (6664/8000)
[Test]  Epoch: 22	Loss: 0.007920	Acc: 83.3% (6661/8000)
[Test]  Epoch: 23	Loss: 0.008000	Acc: 83.2% (6654/8000)
[Test]  Epoch: 24	Loss: 0.007927	Acc: 83.3% (6668/8000)
[Test]  Epoch: 25	Loss: 0.007927	Acc: 83.4% (6675/8000)
[Test]  Epoch: 26	Loss: 0.007901	Acc: 83.3% (6667/8000)
[Test]  Epoch: 27	Loss: 0.007896	Acc: 83.4% (6671/8000)
[Test]  Epoch: 28	Loss: 0.007930	Acc: 83.2% (6659/8000)
[Test]  Epoch: 29	Loss: 0.007918	Acc: 83.4% (6671/8000)
[Test]  Epoch: 30	Loss: 0.007959	Acc: 83.2% (6657/8000)
[Test]  Epoch: 31	Loss: 0.007843	Acc: 83.6% (6691/8000)
[Test]  Epoch: 32	Loss: 0.007874	Acc: 83.3% (6668/8000)
[Test]  Epoch: 33	Loss: 0.007865	Acc: 83.7% (6693/8000)
[Test]  Epoch: 34	Loss: 0.007887	Acc: 83.6% (6691/8000)
[Test]  Epoch: 35	Loss: 0.007832	Acc: 83.6% (6689/8000)
[Test]  Epoch: 36	Loss: 0.007838	Acc: 83.6% (6688/8000)
[Test]  Epoch: 37	Loss: 0.007829	Acc: 83.6% (6688/8000)
[Test]  Epoch: 38	Loss: 0.007854	Acc: 83.6% (6689/8000)
[Test]  Epoch: 39	Loss: 0.007805	Acc: 83.6% (6687/8000)
[Test]  Epoch: 40	Loss: 0.007773	Acc: 83.6% (6688/8000)
[Test]  Epoch: 41	Loss: 0.007796	Acc: 83.6% (6687/8000)
[Test]  Epoch: 42	Loss: 0.007834	Acc: 83.6% (6691/8000)
[Test]  Epoch: 43	Loss: 0.007831	Acc: 83.7% (6696/8000)
[Test]  Epoch: 44	Loss: 0.007810	Acc: 83.6% (6685/8000)
[Test]  Epoch: 45	Loss: 0.007792	Acc: 83.8% (6702/8000)
[Test]  Epoch: 46	Loss: 0.007827	Acc: 83.5% (6678/8000)
[Test]  Epoch: 47	Loss: 0.007812	Acc: 83.8% (6700/8000)
[Test]  Epoch: 48	Loss: 0.007824	Acc: 83.6% (6689/8000)
[Test]  Epoch: 49	Loss: 0.007812	Acc: 83.6% (6687/8000)
[Test]  Epoch: 50	Loss: 0.007891	Acc: 83.3% (6661/8000)
[Test]  Epoch: 51	Loss: 0.007850	Acc: 83.5% (6677/8000)
[Test]  Epoch: 52	Loss: 0.007849	Acc: 83.7% (6695/8000)
[Test]  Epoch: 53	Loss: 0.007823	Acc: 83.7% (6693/8000)
[Test]  Epoch: 54	Loss: 0.007809	Acc: 83.7% (6693/8000)
[Test]  Epoch: 55	Loss: 0.007893	Acc: 83.5% (6681/8000)
[Test]  Epoch: 56	Loss: 0.007838	Acc: 83.5% (6682/8000)
[Test]  Epoch: 57	Loss: 0.007821	Acc: 83.6% (6690/8000)
[Test]  Epoch: 58	Loss: 0.007844	Acc: 83.2% (6656/8000)
[Test]  Epoch: 59	Loss: 0.007840	Acc: 83.5% (6682/8000)
[Test]  Epoch: 60	Loss: 0.007877	Acc: 83.3% (6668/8000)
[Test]  Epoch: 61	Loss: 0.007839	Acc: 83.3% (6666/8000)
[Test]  Epoch: 62	Loss: 0.007839	Acc: 83.4% (6670/8000)
[Test]  Epoch: 63	Loss: 0.007866	Acc: 83.4% (6674/8000)
[Test]  Epoch: 64	Loss: 0.007844	Acc: 83.5% (6678/8000)
[Test]  Epoch: 65	Loss: 0.007880	Acc: 83.5% (6682/8000)
[Test]  Epoch: 66	Loss: 0.007835	Acc: 83.3% (6665/8000)
[Test]  Epoch: 67	Loss: 0.007897	Acc: 83.4% (6674/8000)
[Test]  Epoch: 68	Loss: 0.007842	Acc: 83.5% (6676/8000)
[Test]  Epoch: 69	Loss: 0.007813	Acc: 83.5% (6683/8000)
[Test]  Epoch: 70	Loss: 0.007801	Acc: 83.5% (6678/8000)
[Test]  Epoch: 71	Loss: 0.007808	Acc: 83.5% (6677/8000)
[Test]  Epoch: 72	Loss: 0.007822	Acc: 83.5% (6684/8000)
[Test]  Epoch: 73	Loss: 0.007815	Acc: 83.6% (6689/8000)
[Test]  Epoch: 74	Loss: 0.007783	Acc: 83.6% (6691/8000)
[Test]  Epoch: 75	Loss: 0.007788	Acc: 83.6% (6691/8000)
[Test]  Epoch: 76	Loss: 0.007783	Acc: 83.6% (6689/8000)
[Test]  Epoch: 77	Loss: 0.007812	Acc: 83.6% (6687/8000)
[Test]  Epoch: 78	Loss: 0.007835	Acc: 83.5% (6678/8000)
[Test]  Epoch: 79	Loss: 0.007833	Acc: 83.5% (6678/8000)
[Test]  Epoch: 80	Loss: 0.007790	Acc: 83.5% (6679/8000)
[Test]  Epoch: 81	Loss: 0.007802	Acc: 83.6% (6689/8000)
[Test]  Epoch: 82	Loss: 0.007822	Acc: 83.6% (6689/8000)
[Test]  Epoch: 83	Loss: 0.007823	Acc: 83.6% (6687/8000)
[Test]  Epoch: 84	Loss: 0.007828	Acc: 83.5% (6682/8000)
[Test]  Epoch: 85	Loss: 0.007825	Acc: 83.5% (6680/8000)
[Test]  Epoch: 86	Loss: 0.007801	Acc: 83.5% (6676/8000)
[Test]  Epoch: 87	Loss: 0.007843	Acc: 83.3% (6667/8000)
[Test]  Epoch: 88	Loss: 0.007801	Acc: 83.5% (6677/8000)
[Test]  Epoch: 89	Loss: 0.007822	Acc: 83.4% (6674/8000)
[Test]  Epoch: 90	Loss: 0.007826	Acc: 83.5% (6682/8000)
[Test]  Epoch: 91	Loss: 0.007802	Acc: 83.6% (6685/8000)
[Test]  Epoch: 92	Loss: 0.007777	Acc: 83.6% (6691/8000)
[Test]  Epoch: 93	Loss: 0.007776	Acc: 83.7% (6697/8000)
[Test]  Epoch: 94	Loss: 0.007813	Acc: 83.5% (6677/8000)
[Test]  Epoch: 95	Loss: 0.007811	Acc: 83.5% (6679/8000)
[Test]  Epoch: 96	Loss: 0.007788	Acc: 83.6% (6690/8000)
[Test]  Epoch: 97	Loss: 0.007782	Acc: 83.7% (6697/8000)
[Test]  Epoch: 98	Loss: 0.007825	Acc: 83.6% (6690/8000)
[Test]  Epoch: 99	Loss: 0.007808	Acc: 83.5% (6678/8000)
[Test]  Epoch: 100	Loss: 0.007810	Acc: 83.4% (6674/8000)
===========finish==========
['2024-09-04', '14:38:23.858719', '100', 'test', '0.007810182087123394', '83.425', '83.775']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer3.1.bn2.weight', 'layer4.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.0.bn2.weight', 'layer2.0.bn2.weight', 'last_linear.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer1.0.conv2.weight', 'layer4.0.conv2.weight', 'layer1.1.conv1.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.conv1.weight', 'layer3.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.018513	Acc: 69.9% (5589/8000)
[Test]  Epoch: 2	Loss: 0.011981	Acc: 76.5% (6118/8000)
[Test]  Epoch: 3	Loss: 0.010412	Acc: 78.2% (6259/8000)
[Test]  Epoch: 4	Loss: 0.009937	Acc: 78.7% (6297/8000)
[Test]  Epoch: 5	Loss: 0.009676	Acc: 79.1% (6328/8000)
[Test]  Epoch: 6	Loss: 0.009553	Acc: 79.6% (6365/8000)
[Test]  Epoch: 7	Loss: 0.009419	Acc: 79.6% (6370/8000)
[Test]  Epoch: 8	Loss: 0.009277	Acc: 79.7% (6373/8000)
[Test]  Epoch: 9	Loss: 0.009254	Acc: 79.9% (6390/8000)
[Test]  Epoch: 10	Loss: 0.009192	Acc: 80.0% (6397/8000)
[Test]  Epoch: 11	Loss: 0.009142	Acc: 80.0% (6400/8000)
[Test]  Epoch: 12	Loss: 0.009169	Acc: 80.1% (6407/8000)
[Test]  Epoch: 13	Loss: 0.009167	Acc: 80.2% (6413/8000)
[Test]  Epoch: 14	Loss: 0.009121	Acc: 80.3% (6423/8000)
[Test]  Epoch: 15	Loss: 0.009135	Acc: 80.1% (6411/8000)
[Test]  Epoch: 16	Loss: 0.009082	Acc: 80.1% (6405/8000)
[Test]  Epoch: 17	Loss: 0.009121	Acc: 80.1% (6406/8000)
[Test]  Epoch: 18	Loss: 0.009092	Acc: 80.1% (6409/8000)
[Test]  Epoch: 19	Loss: 0.009132	Acc: 80.1% (6410/8000)
[Test]  Epoch: 20	Loss: 0.009120	Acc: 80.3% (6426/8000)
[Test]  Epoch: 21	Loss: 0.009058	Acc: 80.2% (6420/8000)
[Test]  Epoch: 22	Loss: 0.009058	Acc: 80.2% (6415/8000)
[Test]  Epoch: 23	Loss: 0.009120	Acc: 80.1% (6407/8000)
[Test]  Epoch: 24	Loss: 0.009115	Acc: 80.2% (6412/8000)
[Test]  Epoch: 25	Loss: 0.009126	Acc: 80.2% (6418/8000)
[Test]  Epoch: 26	Loss: 0.009061	Acc: 80.2% (6419/8000)
[Test]  Epoch: 27	Loss: 0.009039	Acc: 80.3% (6427/8000)
[Test]  Epoch: 28	Loss: 0.009029	Acc: 80.5% (6439/8000)
[Test]  Epoch: 29	Loss: 0.009003	Acc: 80.5% (6444/8000)
[Test]  Epoch: 30	Loss: 0.009023	Acc: 80.5% (6443/8000)
[Test]  Epoch: 31	Loss: 0.009034	Acc: 80.3% (6425/8000)
[Test]  Epoch: 32	Loss: 0.008956	Acc: 80.5% (6440/8000)
[Test]  Epoch: 33	Loss: 0.008907	Acc: 80.6% (6449/8000)
[Test]  Epoch: 34	Loss: 0.008990	Acc: 80.5% (6443/8000)
[Test]  Epoch: 35	Loss: 0.008926	Acc: 80.6% (6445/8000)
[Test]  Epoch: 36	Loss: 0.009011	Acc: 80.5% (6441/8000)
[Test]  Epoch: 37	Loss: 0.009048	Acc: 80.2% (6420/8000)
[Test]  Epoch: 38	Loss: 0.008942	Acc: 80.6% (6449/8000)
[Test]  Epoch: 39	Loss: 0.008914	Acc: 80.5% (6441/8000)
[Test]  Epoch: 40	Loss: 0.008909	Acc: 80.7% (6452/8000)
[Test]  Epoch: 41	Loss: 0.008923	Acc: 80.8% (6461/8000)
[Test]  Epoch: 42	Loss: 0.008922	Acc: 80.7% (6456/8000)
[Test]  Epoch: 43	Loss: 0.008911	Acc: 80.5% (6440/8000)
[Test]  Epoch: 44	Loss: 0.008922	Acc: 80.4% (6433/8000)
[Test]  Epoch: 45	Loss: 0.008933	Acc: 80.6% (6447/8000)
[Test]  Epoch: 46	Loss: 0.008936	Acc: 80.5% (6440/8000)
[Test]  Epoch: 47	Loss: 0.008918	Acc: 80.5% (6440/8000)
[Test]  Epoch: 48	Loss: 0.008880	Acc: 80.4% (6434/8000)
[Test]  Epoch: 49	Loss: 0.008884	Acc: 80.5% (6444/8000)
[Test]  Epoch: 50	Loss: 0.008983	Acc: 80.4% (6434/8000)
[Test]  Epoch: 51	Loss: 0.008974	Acc: 80.5% (6437/8000)
[Test]  Epoch: 52	Loss: 0.008919	Acc: 80.8% (6464/8000)
[Test]  Epoch: 53	Loss: 0.008888	Acc: 80.6% (6449/8000)
[Test]  Epoch: 54	Loss: 0.008913	Acc: 80.5% (6438/8000)
[Test]  Epoch: 55	Loss: 0.008962	Acc: 80.7% (6456/8000)
[Test]  Epoch: 56	Loss: 0.008868	Acc: 80.9% (6471/8000)
[Test]  Epoch: 57	Loss: 0.008872	Acc: 80.7% (6459/8000)
[Test]  Epoch: 58	Loss: 0.008931	Acc: 80.7% (6452/8000)
[Test]  Epoch: 59	Loss: 0.008888	Acc: 80.8% (6460/8000)
[Test]  Epoch: 60	Loss: 0.008870	Acc: 80.6% (6451/8000)
[Test]  Epoch: 61	Loss: 0.008896	Acc: 80.7% (6454/8000)
[Test]  Epoch: 62	Loss: 0.008926	Acc: 80.7% (6458/8000)
[Test]  Epoch: 63	Loss: 0.008899	Acc: 80.6% (6447/8000)
[Test]  Epoch: 64	Loss: 0.008896	Acc: 80.7% (6452/8000)
[Test]  Epoch: 65	Loss: 0.008904	Acc: 80.7% (6458/8000)
[Test]  Epoch: 66	Loss: 0.008892	Acc: 80.6% (6448/8000)
[Test]  Epoch: 67	Loss: 0.008907	Acc: 80.6% (6450/8000)
[Test]  Epoch: 68	Loss: 0.008862	Acc: 80.8% (6464/8000)
[Test]  Epoch: 69	Loss: 0.008842	Acc: 80.7% (6457/8000)
[Test]  Epoch: 70	Loss: 0.008836	Acc: 80.7% (6457/8000)
[Test]  Epoch: 71	Loss: 0.008831	Acc: 80.7% (6458/8000)
[Test]  Epoch: 72	Loss: 0.008837	Acc: 80.6% (6450/8000)
[Test]  Epoch: 73	Loss: 0.008845	Acc: 80.8% (6462/8000)
[Test]  Epoch: 74	Loss: 0.008840	Acc: 80.7% (6453/8000)
[Test]  Epoch: 75	Loss: 0.008842	Acc: 80.8% (6466/8000)
[Test]  Epoch: 76	Loss: 0.008818	Acc: 80.8% (6461/8000)
[Test]  Epoch: 77	Loss: 0.008866	Acc: 80.7% (6452/8000)
[Test]  Epoch: 78	Loss: 0.008855	Acc: 80.6% (6448/8000)
[Test]  Epoch: 79	Loss: 0.008880	Acc: 80.6% (6449/8000)
[Test]  Epoch: 80	Loss: 0.008858	Acc: 80.8% (6461/8000)
[Test]  Epoch: 81	Loss: 0.008863	Acc: 80.7% (6453/8000)
[Test]  Epoch: 82	Loss: 0.008870	Acc: 80.7% (6458/8000)
[Test]  Epoch: 83	Loss: 0.008837	Acc: 80.7% (6458/8000)
[Test]  Epoch: 84	Loss: 0.008833	Acc: 80.8% (6465/8000)
[Test]  Epoch: 85	Loss: 0.008864	Acc: 80.8% (6462/8000)
[Test]  Epoch: 86	Loss: 0.008835	Acc: 80.7% (6459/8000)
[Test]  Epoch: 87	Loss: 0.008848	Acc: 80.7% (6459/8000)
[Test]  Epoch: 88	Loss: 0.008847	Acc: 80.6% (6451/8000)
[Test]  Epoch: 89	Loss: 0.008885	Acc: 80.6% (6451/8000)
[Test]  Epoch: 90	Loss: 0.008866	Acc: 80.6% (6447/8000)
[Test]  Epoch: 91	Loss: 0.008876	Acc: 80.8% (6463/8000)
[Test]  Epoch: 92	Loss: 0.008827	Acc: 80.7% (6458/8000)
[Test]  Epoch: 93	Loss: 0.008842	Acc: 80.7% (6453/8000)
[Test]  Epoch: 94	Loss: 0.008820	Acc: 80.8% (6462/8000)
[Test]  Epoch: 95	Loss: 0.008853	Acc: 80.7% (6455/8000)
[Test]  Epoch: 96	Loss: 0.008867	Acc: 80.7% (6457/8000)
[Test]  Epoch: 97	Loss: 0.008852	Acc: 80.6% (6449/8000)
[Test]  Epoch: 98	Loss: 0.008872	Acc: 80.7% (6455/8000)
[Test]  Epoch: 99	Loss: 0.008855	Acc: 80.7% (6454/8000)
[Test]  Epoch: 100	Loss: 0.008900	Acc: 80.5% (6440/8000)
===========finish==========
['2024-09-04', '14:40:40.034727', '100', 'test', '0.008900147683918476', '80.5', '80.8875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet18-channel resnet18 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['layer4.1.bn2.weight', 'layer4.0.bn2.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer4.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer1.0.bn2.weight', 'layer3.1.bn2.weight', 'layer4.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.0.bn2.weight', 'layer2.0.bn2.weight', 'last_linear.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer1.0.conv2.weight', 'layer4.0.conv2.weight', 'layer1.1.conv1.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.conv1.weight', 'layer3.1.conv1.weight', 'layer2.1.conv1.weight', 'layer2.0.conv2.weight', 'layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'layer2.0.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.020849	Acc: 65.4% (5232/8000)
[Test]  Epoch: 2	Loss: 0.011594	Acc: 78.6% (6286/8000)
[Test]  Epoch: 3	Loss: 0.009418	Acc: 81.2% (6492/8000)
[Test]  Epoch: 4	Loss: 0.008708	Acc: 82.3% (6581/8000)
[Test]  Epoch: 5	Loss: 0.008231	Acc: 82.8% (6625/8000)
[Test]  Epoch: 6	Loss: 0.008056	Acc: 83.0% (6639/8000)
[Test]  Epoch: 7	Loss: 0.007818	Acc: 83.6% (6688/8000)
[Test]  Epoch: 8	Loss: 0.007743	Acc: 83.5% (6681/8000)
[Test]  Epoch: 9	Loss: 0.007736	Acc: 83.5% (6680/8000)
[Test]  Epoch: 10	Loss: 0.007642	Acc: 83.5% (6678/8000)
[Test]  Epoch: 11	Loss: 0.007521	Acc: 84.0% (6720/8000)
[Test]  Epoch: 12	Loss: 0.007512	Acc: 83.8% (6701/8000)
[Test]  Epoch: 13	Loss: 0.007512	Acc: 83.8% (6702/8000)
[Test]  Epoch: 14	Loss: 0.007433	Acc: 84.0% (6716/8000)
[Test]  Epoch: 15	Loss: 0.007441	Acc: 84.0% (6722/8000)
[Test]  Epoch: 16	Loss: 0.007387	Acc: 83.8% (6708/8000)
[Test]  Epoch: 17	Loss: 0.007456	Acc: 83.6% (6689/8000)
[Test]  Epoch: 18	Loss: 0.007447	Acc: 83.8% (6705/8000)
[Test]  Epoch: 19	Loss: 0.007429	Acc: 83.9% (6711/8000)
[Test]  Epoch: 20	Loss: 0.007422	Acc: 83.7% (6699/8000)
[Test]  Epoch: 21	Loss: 0.007406	Acc: 83.8% (6703/8000)
[Test]  Epoch: 22	Loss: 0.007389	Acc: 83.8% (6700/8000)
[Test]  Epoch: 23	Loss: 0.007431	Acc: 83.6% (6687/8000)
[Test]  Epoch: 24	Loss: 0.007380	Acc: 83.9% (6712/8000)
[Test]  Epoch: 25	Loss: 0.007385	Acc: 83.6% (6689/8000)
[Test]  Epoch: 26	Loss: 0.007315	Acc: 84.1% (6730/8000)
[Test]  Epoch: 27	Loss: 0.007300	Acc: 84.1% (6728/8000)
[Test]  Epoch: 28	Loss: 0.007321	Acc: 83.9% (6712/8000)
[Test]  Epoch: 29	Loss: 0.007220	Acc: 84.3% (6744/8000)
[Test]  Epoch: 30	Loss: 0.007219	Acc: 84.4% (6755/8000)
[Test]  Epoch: 31	Loss: 0.007240	Acc: 84.1% (6731/8000)
[Test]  Epoch: 32	Loss: 0.007217	Acc: 84.4% (6749/8000)
[Test]  Epoch: 33	Loss: 0.007252	Acc: 84.2% (6737/8000)
[Test]  Epoch: 34	Loss: 0.007370	Acc: 83.8% (6705/8000)
[Test]  Epoch: 35	Loss: 0.007307	Acc: 84.1% (6727/8000)
[Test]  Epoch: 36	Loss: 0.007396	Acc: 83.7% (6693/8000)
[Test]  Epoch: 37	Loss: 0.007388	Acc: 83.6% (6691/8000)
[Test]  Epoch: 38	Loss: 0.007286	Acc: 83.9% (6710/8000)
[Test]  Epoch: 39	Loss: 0.007244	Acc: 84.0% (6721/8000)
[Test]  Epoch: 40	Loss: 0.007282	Acc: 84.1% (6729/8000)
[Test]  Epoch: 41	Loss: 0.007287	Acc: 84.1% (6728/8000)
[Test]  Epoch: 42	Loss: 0.007257	Acc: 84.2% (6738/8000)
[Test]  Epoch: 43	Loss: 0.007258	Acc: 84.2% (6732/8000)
[Test]  Epoch: 44	Loss: 0.007174	Acc: 84.3% (6746/8000)
[Test]  Epoch: 45	Loss: 0.007243	Acc: 84.0% (6724/8000)
[Test]  Epoch: 46	Loss: 0.007162	Acc: 84.2% (6735/8000)
[Test]  Epoch: 47	Loss: 0.007214	Acc: 84.2% (6735/8000)
[Test]  Epoch: 48	Loss: 0.007138	Acc: 84.3% (6741/8000)
[Test]  Epoch: 49	Loss: 0.007166	Acc: 84.3% (6747/8000)
[Test]  Epoch: 50	Loss: 0.007242	Acc: 84.0% (6723/8000)
[Test]  Epoch: 51	Loss: 0.007259	Acc: 84.0% (6723/8000)
[Test]  Epoch: 52	Loss: 0.007167	Acc: 84.2% (6740/8000)
[Test]  Epoch: 53	Loss: 0.007166	Acc: 84.3% (6742/8000)
[Test]  Epoch: 54	Loss: 0.007190	Acc: 84.2% (6738/8000)
[Test]  Epoch: 55	Loss: 0.007316	Acc: 83.9% (6712/8000)
[Test]  Epoch: 56	Loss: 0.007272	Acc: 84.1% (6729/8000)
[Test]  Epoch: 57	Loss: 0.007201	Acc: 84.3% (6742/8000)
[Test]  Epoch: 58	Loss: 0.007256	Acc: 84.0% (6721/8000)
[Test]  Epoch: 59	Loss: 0.007157	Acc: 84.3% (6747/8000)
[Test]  Epoch: 60	Loss: 0.007223	Acc: 84.1% (6730/8000)
[Test]  Epoch: 61	Loss: 0.007235	Acc: 84.1% (6730/8000)
[Test]  Epoch: 62	Loss: 0.007222	Acc: 84.0% (6724/8000)
[Test]  Epoch: 63	Loss: 0.007262	Acc: 84.0% (6724/8000)
[Test]  Epoch: 64	Loss: 0.007267	Acc: 84.2% (6737/8000)
[Test]  Epoch: 65	Loss: 0.007212	Acc: 84.2% (6735/8000)
[Test]  Epoch: 66	Loss: 0.007225	Acc: 84.1% (6727/8000)
[Test]  Epoch: 67	Loss: 0.007239	Acc: 84.2% (6732/8000)
[Test]  Epoch: 68	Loss: 0.007193	Acc: 84.3% (6744/8000)
[Test]  Epoch: 69	Loss: 0.007189	Acc: 84.2% (6738/8000)
[Test]  Epoch: 70	Loss: 0.007184	Acc: 84.2% (6732/8000)
[Test]  Epoch: 71	Loss: 0.007210	Acc: 84.1% (6728/8000)
[Test]  Epoch: 72	Loss: 0.007202	Acc: 84.2% (6740/8000)
[Test]  Epoch: 73	Loss: 0.007188	Acc: 84.2% (6735/8000)
[Test]  Epoch: 74	Loss: 0.007173	Acc: 84.2% (6733/8000)
[Test]  Epoch: 75	Loss: 0.007188	Acc: 84.3% (6741/8000)
[Test]  Epoch: 76	Loss: 0.007126	Acc: 84.4% (6749/8000)
[Test]  Epoch: 77	Loss: 0.007186	Acc: 84.3% (6741/8000)
[Test]  Epoch: 78	Loss: 0.007155	Acc: 84.3% (6746/8000)
[Test]  Epoch: 79	Loss: 0.007208	Acc: 84.1% (6729/8000)
[Test]  Epoch: 80	Loss: 0.007171	Acc: 84.2% (6734/8000)
[Test]  Epoch: 81	Loss: 0.007186	Acc: 84.2% (6735/8000)
[Test]  Epoch: 82	Loss: 0.007114	Acc: 84.3% (6747/8000)
[Test]  Epoch: 83	Loss: 0.007175	Acc: 84.2% (6735/8000)
[Test]  Epoch: 84	Loss: 0.007148	Acc: 84.2% (6733/8000)
[Test]  Epoch: 85	Loss: 0.007180	Acc: 84.2% (6736/8000)
[Test]  Epoch: 86	Loss: 0.007192	Acc: 84.3% (6743/8000)
[Test]  Epoch: 87	Loss: 0.007176	Acc: 84.2% (6737/8000)
[Test]  Epoch: 88	Loss: 0.007189	Acc: 84.3% (6744/8000)
[Test]  Epoch: 89	Loss: 0.007194	Acc: 84.2% (6735/8000)
[Test]  Epoch: 90	Loss: 0.007173	Acc: 84.2% (6735/8000)
[Test]  Epoch: 91	Loss: 0.007204	Acc: 84.2% (6735/8000)
[Test]  Epoch: 92	Loss: 0.007156	Acc: 84.4% (6753/8000)
[Test]  Epoch: 93	Loss: 0.007140	Acc: 84.4% (6752/8000)
[Test]  Epoch: 94	Loss: 0.007123	Acc: 84.3% (6741/8000)
[Test]  Epoch: 95	Loss: 0.007183	Acc: 84.1% (6728/8000)
[Test]  Epoch: 96	Loss: 0.007207	Acc: 84.2% (6738/8000)
[Test]  Epoch: 97	Loss: 0.007123	Acc: 84.3% (6743/8000)
[Test]  Epoch: 98	Loss: 0.007216	Acc: 83.9% (6714/8000)
[Test]  Epoch: 99	Loss: 0.007170	Acc: 84.2% (6738/8000)
[Test]  Epoch: 100	Loss: 0.007253	Acc: 84.0% (6718/8000)
===========finish==========
['2024-09-04', '14:42:56.632048', '100', 'test', '0.007253421675413847', '83.975', '84.4375']
