result path:  /home/gpu2/jbw/other_XAI_grad/knockoffnets/ms_elastictrainer_result_resnet18_vgg16_mobilenetv2.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info does not exist. Calculating...
Load model from models/victim/cifar100-mobilenetv2/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('_features.0.0.weight', 0.0), ('_features.0.1.weight', 0.0), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.0.weight', 0.0), ('_features.1.conv.0.1.weight', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.1.weight', 0.0), ('_features.1.conv.2.weight', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.0.weight', 0.0), ('_features.2.conv.0.1.weight', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.0.weight', 0.0), ('_features.2.conv.1.1.weight', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.2.weight', 0.0), ('_features.2.conv.3.weight', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.0.weight', 0.0), ('_features.3.conv.0.1.weight', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.0.weight', 0.0), ('_features.3.conv.1.1.weight', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.2.weight', 0.0), ('_features.3.conv.3.weight', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.0.weight', 0.0), ('_features.4.conv.0.1.weight', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.0.weight', 0.0), ('_features.4.conv.1.1.weight', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.2.weight', 0.0), ('_features.4.conv.3.weight', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.0.weight', 0.0), ('_features.5.conv.0.1.weight', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.0.weight', 0.0), ('_features.5.conv.1.1.weight', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.2.weight', 0.0), ('_features.5.conv.3.weight', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.0.weight', 0.0), ('_features.6.conv.0.1.weight', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.0.weight', 0.0), ('_features.6.conv.1.1.weight', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.2.weight', 0.0), ('_features.6.conv.3.weight', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.0.weight', 0.0), ('_features.7.conv.0.1.weight', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.0.weight', 0.0), ('_features.7.conv.1.1.weight', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.2.weight', 0.0), ('_features.7.conv.3.weight', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.0.weight', 0.0), ('_features.8.conv.0.1.weight', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.0.weight', 0.0), ('_features.8.conv.1.1.weight', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.2.weight', 0.0), ('_features.8.conv.3.weight', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.0.weight', 0.0), ('_features.9.conv.0.1.weight', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.0.weight', 0.0), ('_features.9.conv.1.1.weight', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.2.weight', 0.0), ('_features.9.conv.3.weight', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.0.weight', 0.0), ('_features.10.conv.0.1.weight', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.0.weight', 0.0), ('_features.10.conv.1.1.weight', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.2.weight', 0.0), ('_features.10.conv.3.weight', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.0.weight', 0.0), ('_features.11.conv.0.1.weight', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.0.weight', 0.0), ('_features.11.conv.1.1.weight', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.2.weight', 0.0), ('_features.11.conv.3.weight', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.0.weight', 0.0), ('_features.12.conv.0.1.weight', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.0.weight', 0.0), ('_features.12.conv.1.1.weight', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.2.weight', 0.0), ('_features.12.conv.3.weight', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.0.weight', 0.0), ('_features.13.conv.0.1.weight', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.0.weight', 0.0), ('_features.13.conv.1.1.weight', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.2.weight', 0.0), ('_features.13.conv.3.weight', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.0.weight', 0.0), ('_features.14.conv.0.1.weight', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.0.weight', 0.0), ('_features.14.conv.1.1.weight', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.2.weight', 0.0), ('_features.14.conv.3.weight', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.0.weight', 0.0), ('_features.15.conv.0.1.weight', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.0.weight', 0.0), ('_features.15.conv.1.1.weight', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.2.weight', 0.0), ('_features.15.conv.3.weight', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.0.weight', 0.0), ('_features.16.conv.0.1.weight', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.0.weight', 0.0), ('_features.16.conv.1.1.weight', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.2.weight', 0.0), ('_features.16.conv.3.weight', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.0.weight', 0.0), ('_features.17.conv.0.1.weight', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.0.weight', 0.0), ('_features.17.conv.1.1.weight', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.2.weight', 0.0), ('_features.17.conv.3.weight', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.0.weight', 0.0), ('_features.18.1.weight', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('_features.0.0.weight', 0.0), ('_features.0.1.weight', 0.0), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.0.weight', 0.0), ('_features.1.conv.0.1.weight', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.1.weight', 0.0), ('_features.1.conv.2.weight', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.0.weight', 0.0), ('_features.2.conv.0.1.weight', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.0.weight', 0.0), ('_features.2.conv.1.1.weight', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.2.weight', 0.0), ('_features.2.conv.3.weight', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.0.weight', 0.0), ('_features.3.conv.0.1.weight', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.0.weight', 0.0), ('_features.3.conv.1.1.weight', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.2.weight', 0.0), ('_features.3.conv.3.weight', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.0.weight', 0.0), ('_features.4.conv.0.1.weight', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.0.weight', 0.0), ('_features.4.conv.1.1.weight', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.2.weight', 0.0), ('_features.4.conv.3.weight', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.0.weight', 0.0), ('_features.5.conv.0.1.weight', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.0.weight', 0.0), ('_features.5.conv.1.1.weight', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.2.weight', 0.0), ('_features.5.conv.3.weight', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.0.weight', 0.0), ('_features.6.conv.0.1.weight', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.0.weight', 0.0), ('_features.6.conv.1.1.weight', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.2.weight', 0.0), ('_features.6.conv.3.weight', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.0.weight', 0.0), ('_features.7.conv.0.1.weight', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.0.weight', 0.0), ('_features.7.conv.1.1.weight', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.2.weight', 0.0), ('_features.7.conv.3.weight', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.0.weight', 0.0), ('_features.8.conv.0.1.weight', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.0.weight', 0.0), ('_features.8.conv.1.1.weight', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.2.weight', 0.0), ('_features.8.conv.3.weight', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.0.weight', 0.0), ('_features.9.conv.0.1.weight', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.0.weight', 0.0), ('_features.9.conv.1.1.weight', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.2.weight', 0.0), ('_features.9.conv.3.weight', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.0.weight', 0.0), ('_features.10.conv.0.1.weight', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.0.weight', 0.0), ('_features.10.conv.1.1.weight', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.2.weight', 0.0), ('_features.10.conv.3.weight', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.0.weight', 0.0), ('_features.11.conv.0.1.weight', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.0.weight', 0.0), ('_features.11.conv.1.1.weight', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.2.weight', 0.0), ('_features.11.conv.3.weight', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.0.weight', 0.0), ('_features.12.conv.0.1.weight', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.0.weight', 0.0), ('_features.12.conv.1.1.weight', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.2.weight', 0.0), ('_features.12.conv.3.weight', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.0.weight', 0.0), ('_features.13.conv.0.1.weight', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.0.weight', 0.0), ('_features.13.conv.1.1.weight', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.2.weight', 0.0), ('_features.13.conv.3.weight', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.0.weight', 0.0), ('_features.14.conv.0.1.weight', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.0.weight', 0.0), ('_features.14.conv.1.1.weight', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.2.weight', 0.0), ('_features.14.conv.3.weight', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.0.weight', 0.0), ('_features.15.conv.0.1.weight', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.0.weight', 0.0), ('_features.15.conv.1.1.weight', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.2.weight', 0.0), ('_features.15.conv.3.weight', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.0.weight', 0.0), ('_features.16.conv.0.1.weight', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.0.weight', 0.0), ('_features.16.conv.1.1.weight', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.2.weight', 0.0), ('_features.16.conv.3.weight', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.0.weight', 0.0), ('_features.17.conv.0.1.weight', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.0.weight', 0.0), ('_features.17.conv.1.1.weight', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.2.weight', 0.0), ('_features.17.conv.3.weight', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.0.weight', 0.0), ('_features.18.1.weight', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=105  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.029810	Acc: 48.3% (4832/10000)
[Test]  Epoch: 2	Loss: 0.029693	Acc: 48.7% (4870/10000)
[Test]  Epoch: 3	Loss: 0.029694	Acc: 48.7% (4872/10000)
[Test]  Epoch: 4	Loss: 0.029700	Acc: 48.5% (4854/10000)
[Test]  Epoch: 5	Loss: 0.029715	Acc: 48.5% (4846/10000)
[Test]  Epoch: 6	Loss: 0.029710	Acc: 48.5% (4851/10000)
[Test]  Epoch: 7	Loss: 0.029590	Acc: 48.8% (4880/10000)
[Test]  Epoch: 8	Loss: 0.029616	Acc: 48.8% (4882/10000)
[Test]  Epoch: 9	Loss: 0.029549	Acc: 48.4% (4841/10000)
[Test]  Epoch: 10	Loss: 0.029681	Acc: 48.6% (4862/10000)
[Test]  Epoch: 11	Loss: 0.029587	Acc: 48.7% (4872/10000)
[Test]  Epoch: 12	Loss: 0.029541	Acc: 49.1% (4906/10000)
[Test]  Epoch: 13	Loss: 0.029553	Acc: 49.0% (4905/10000)
[Test]  Epoch: 14	Loss: 0.029509	Acc: 49.1% (4906/10000)
[Test]  Epoch: 15	Loss: 0.029652	Acc: 48.5% (4848/10000)
[Test]  Epoch: 16	Loss: 0.029468	Acc: 48.9% (4890/10000)
[Test]  Epoch: 17	Loss: 0.029572	Acc: 48.8% (4876/10000)
[Test]  Epoch: 18	Loss: 0.029632	Acc: 49.0% (4902/10000)
[Test]  Epoch: 19	Loss: 0.029568	Acc: 49.0% (4897/10000)
[Test]  Epoch: 20	Loss: 0.029555	Acc: 48.9% (4889/10000)
[Test]  Epoch: 21	Loss: 0.029518	Acc: 48.8% (4884/10000)
[Test]  Epoch: 22	Loss: 0.029460	Acc: 49.3% (4927/10000)
[Test]  Epoch: 23	Loss: 0.029562	Acc: 48.7% (4870/10000)
[Test]  Epoch: 24	Loss: 0.029555	Acc: 49.1% (4910/10000)
[Test]  Epoch: 25	Loss: 0.029511	Acc: 49.1% (4915/10000)
[Test]  Epoch: 26	Loss: 0.029487	Acc: 49.1% (4909/10000)
[Test]  Epoch: 27	Loss: 0.029574	Acc: 48.9% (4887/10000)
[Test]  Epoch: 28	Loss: 0.029676	Acc: 48.8% (4883/10000)
[Test]  Epoch: 29	Loss: 0.029601	Acc: 49.1% (4906/10000)
[Test]  Epoch: 30	Loss: 0.029527	Acc: 49.0% (4902/10000)
[Test]  Epoch: 31	Loss: 0.029483	Acc: 49.3% (4928/10000)
[Test]  Epoch: 32	Loss: 0.029608	Acc: 48.8% (4875/10000)
[Test]  Epoch: 33	Loss: 0.029534	Acc: 49.1% (4907/10000)
[Test]  Epoch: 34	Loss: 0.029564	Acc: 48.9% (4892/10000)
[Test]  Epoch: 35	Loss: 0.029592	Acc: 48.8% (4876/10000)
[Test]  Epoch: 36	Loss: 0.029684	Acc: 48.8% (4881/10000)
[Test]  Epoch: 37	Loss: 0.029557	Acc: 49.1% (4907/10000)
[Test]  Epoch: 38	Loss: 0.029465	Acc: 49.2% (4916/10000)
[Test]  Epoch: 39	Loss: 0.029560	Acc: 49.0% (4903/10000)
[Test]  Epoch: 40	Loss: 0.029531	Acc: 49.0% (4902/10000)
[Test]  Epoch: 41	Loss: 0.029520	Acc: 49.2% (4923/10000)
[Test]  Epoch: 42	Loss: 0.029685	Acc: 48.8% (4875/10000)
[Test]  Epoch: 43	Loss: 0.029649	Acc: 49.0% (4901/10000)
[Test]  Epoch: 44	Loss: 0.029550	Acc: 49.1% (4912/10000)
[Test]  Epoch: 45	Loss: 0.029715	Acc: 49.0% (4896/10000)
[Test]  Epoch: 46	Loss: 0.029550	Acc: 48.9% (4888/10000)
[Test]  Epoch: 47	Loss: 0.029567	Acc: 49.0% (4898/10000)
[Test]  Epoch: 48	Loss: 0.029578	Acc: 48.9% (4885/10000)
[Test]  Epoch: 49	Loss: 0.029596	Acc: 48.9% (4894/10000)
[Test]  Epoch: 50	Loss: 0.029596	Acc: 49.0% (4898/10000)
[Test]  Epoch: 51	Loss: 0.029617	Acc: 49.0% (4895/10000)
[Test]  Epoch: 52	Loss: 0.029522	Acc: 49.0% (4904/10000)
[Test]  Epoch: 53	Loss: 0.029622	Acc: 48.8% (4880/10000)
[Test]  Epoch: 54	Loss: 0.029665	Acc: 49.0% (4903/10000)
[Test]  Epoch: 55	Loss: 0.029581	Acc: 49.1% (4909/10000)
[Test]  Epoch: 56	Loss: 0.029641	Acc: 48.9% (4887/10000)
[Test]  Epoch: 57	Loss: 0.029658	Acc: 48.5% (4852/10000)
[Test]  Epoch: 58	Loss: 0.029512	Acc: 49.0% (4895/10000)
[Test]  Epoch: 59	Loss: 0.029612	Acc: 48.8% (4879/10000)
[Test]  Epoch: 60	Loss: 0.029795	Acc: 48.7% (4874/10000)
[Test]  Epoch: 61	Loss: 0.029741	Acc: 48.5% (4850/10000)
[Test]  Epoch: 62	Loss: 0.029660	Acc: 48.8% (4879/10000)
[Test]  Epoch: 63	Loss: 0.029652	Acc: 48.8% (4883/10000)
[Test]  Epoch: 64	Loss: 0.029636	Acc: 49.0% (4902/10000)
[Test]  Epoch: 65	Loss: 0.029567	Acc: 49.0% (4905/10000)
[Test]  Epoch: 66	Loss: 0.029587	Acc: 48.9% (4885/10000)
[Test]  Epoch: 67	Loss: 0.029666	Acc: 48.7% (4868/10000)
[Test]  Epoch: 68	Loss: 0.029582	Acc: 49.0% (4895/10000)
[Test]  Epoch: 69	Loss: 0.029595	Acc: 48.8% (4879/10000)
[Test]  Epoch: 70	Loss: 0.029615	Acc: 48.9% (4889/10000)
[Test]  Epoch: 71	Loss: 0.029571	Acc: 49.0% (4899/10000)
[Test]  Epoch: 72	Loss: 0.029605	Acc: 48.8% (4883/10000)
[Test]  Epoch: 73	Loss: 0.029537	Acc: 49.1% (4914/10000)
[Test]  Epoch: 74	Loss: 0.029539	Acc: 48.9% (4894/10000)
[Test]  Epoch: 75	Loss: 0.029559	Acc: 49.0% (4903/10000)
[Test]  Epoch: 76	Loss: 0.029636	Acc: 48.9% (4890/10000)
[Test]  Epoch: 77	Loss: 0.029571	Acc: 48.9% (4894/10000)
[Test]  Epoch: 78	Loss: 0.029630	Acc: 48.9% (4892/10000)
[Test]  Epoch: 79	Loss: 0.029605	Acc: 49.0% (4900/10000)
[Test]  Epoch: 80	Loss: 0.029580	Acc: 49.0% (4895/10000)
[Test]  Epoch: 81	Loss: 0.029523	Acc: 49.0% (4899/10000)
[Test]  Epoch: 82	Loss: 0.029632	Acc: 48.9% (4886/10000)
[Test]  Epoch: 83	Loss: 0.029566	Acc: 49.0% (4902/10000)
[Test]  Epoch: 84	Loss: 0.029569	Acc: 49.0% (4895/10000)
[Test]  Epoch: 85	Loss: 0.029533	Acc: 49.0% (4897/10000)
[Test]  Epoch: 86	Loss: 0.029596	Acc: 48.9% (4890/10000)
[Test]  Epoch: 87	Loss: 0.029555	Acc: 49.0% (4902/10000)
[Test]  Epoch: 88	Loss: 0.029638	Acc: 48.8% (4879/10000)
[Test]  Epoch: 89	Loss: 0.029620	Acc: 48.9% (4892/10000)
[Test]  Epoch: 90	Loss: 0.029545	Acc: 49.1% (4911/10000)
[Test]  Epoch: 91	Loss: 0.029531	Acc: 49.2% (4924/10000)
[Test]  Epoch: 92	Loss: 0.029616	Acc: 49.0% (4899/10000)
[Test]  Epoch: 93	Loss: 0.029572	Acc: 49.1% (4911/10000)
[Test]  Epoch: 94	Loss: 0.029554	Acc: 49.2% (4917/10000)
[Test]  Epoch: 95	Loss: 0.029622	Acc: 48.9% (4890/10000)
[Test]  Epoch: 96	Loss: 0.029537	Acc: 49.2% (4922/10000)
[Test]  Epoch: 97	Loss: 0.029670	Acc: 49.0% (4905/10000)
[Test]  Epoch: 98	Loss: 0.029602	Acc: 49.0% (4904/10000)
[Test]  Epoch: 99	Loss: 0.029631	Acc: 49.1% (4909/10000)
[Test]  Epoch: 100	Loss: 0.029639	Acc: 49.0% (4904/10000)
===========finish==========
['2024-08-19', '03:00:35.674261', '100', 'test', '0.02963896886110306', '49.04', '49.28']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.085312	Acc: 6.5% (653/10000)
[Test]  Epoch: 2	Loss: 0.059688	Acc: 15.8% (1576/10000)
[Test]  Epoch: 3	Loss: 0.055036	Acc: 19.1% (1911/10000)
[Test]  Epoch: 4	Loss: 0.053082	Acc: 20.8% (2079/10000)
[Test]  Epoch: 5	Loss: 0.052317	Acc: 20.9% (2086/10000)
[Test]  Epoch: 6	Loss: 0.052278	Acc: 21.2% (2122/10000)
[Test]  Epoch: 7	Loss: 0.051766	Acc: 21.7% (2170/10000)
[Test]  Epoch: 8	Loss: 0.050808	Acc: 22.5% (2248/10000)
[Test]  Epoch: 9	Loss: 0.050614	Acc: 22.6% (2263/10000)
[Test]  Epoch: 10	Loss: 0.049978	Acc: 22.8% (2277/10000)
[Test]  Epoch: 11	Loss: 0.050520	Acc: 22.5% (2247/10000)
[Test]  Epoch: 12	Loss: 0.049687	Acc: 23.5% (2353/10000)
[Test]  Epoch: 13	Loss: 0.050037	Acc: 23.3% (2326/10000)
[Test]  Epoch: 14	Loss: 0.050077	Acc: 23.6% (2359/10000)
[Test]  Epoch: 15	Loss: 0.049915	Acc: 23.5% (2351/10000)
[Test]  Epoch: 16	Loss: 0.049549	Acc: 23.7% (2371/10000)
[Test]  Epoch: 17	Loss: 0.050051	Acc: 23.3% (2331/10000)
[Test]  Epoch: 18	Loss: 0.049648	Acc: 23.6% (2363/10000)
[Test]  Epoch: 19	Loss: 0.049537	Acc: 23.6% (2360/10000)
[Test]  Epoch: 20	Loss: 0.049955	Acc: 23.2% (2324/10000)
[Test]  Epoch: 21	Loss: 0.049517	Acc: 23.7% (2372/10000)
[Test]  Epoch: 22	Loss: 0.049117	Acc: 24.3% (2428/10000)
[Test]  Epoch: 23	Loss: 0.048731	Acc: 24.4% (2444/10000)
[Test]  Epoch: 24	Loss: 0.048935	Acc: 24.3% (2430/10000)
[Test]  Epoch: 25	Loss: 0.049100	Acc: 24.4% (2442/10000)
[Test]  Epoch: 26	Loss: 0.048902	Acc: 24.4% (2437/10000)
[Test]  Epoch: 27	Loss: 0.048679	Acc: 24.6% (2457/10000)
[Test]  Epoch: 28	Loss: 0.049054	Acc: 23.9% (2395/10000)
[Test]  Epoch: 29	Loss: 0.048517	Acc: 25.1% (2514/10000)
[Test]  Epoch: 30	Loss: 0.048777	Acc: 24.5% (2447/10000)
[Test]  Epoch: 31	Loss: 0.048833	Acc: 24.6% (2462/10000)
[Test]  Epoch: 32	Loss: 0.048697	Acc: 24.8% (2483/10000)
[Test]  Epoch: 33	Loss: 0.048389	Acc: 25.0% (2497/10000)
[Test]  Epoch: 34	Loss: 0.048395	Acc: 24.8% (2478/10000)
[Test]  Epoch: 35	Loss: 0.048239	Acc: 25.2% (2521/10000)
[Test]  Epoch: 36	Loss: 0.048534	Acc: 25.0% (2501/10000)
[Test]  Epoch: 37	Loss: 0.048421	Acc: 24.6% (2462/10000)
[Test]  Epoch: 38	Loss: 0.048175	Acc: 25.1% (2508/10000)
[Test]  Epoch: 39	Loss: 0.048173	Acc: 25.4% (2537/10000)
[Test]  Epoch: 40	Loss: 0.048017	Acc: 25.4% (2540/10000)
[Test]  Epoch: 41	Loss: 0.048167	Acc: 25.2% (2520/10000)
[Test]  Epoch: 42	Loss: 0.048199	Acc: 24.7% (2472/10000)
[Test]  Epoch: 43	Loss: 0.048031	Acc: 25.0% (2496/10000)
[Test]  Epoch: 44	Loss: 0.047909	Acc: 25.3% (2527/10000)
[Test]  Epoch: 45	Loss: 0.047905	Acc: 25.6% (2558/10000)
[Test]  Epoch: 46	Loss: 0.048114	Acc: 25.4% (2542/10000)
[Test]  Epoch: 47	Loss: 0.047853	Acc: 25.6% (2555/10000)
[Test]  Epoch: 48	Loss: 0.047798	Acc: 25.5% (2553/10000)
[Test]  Epoch: 49	Loss: 0.047688	Acc: 25.7% (2566/10000)
[Test]  Epoch: 50	Loss: 0.047514	Acc: 25.8% (2584/10000)
[Test]  Epoch: 51	Loss: 0.047739	Acc: 25.5% (2552/10000)
[Test]  Epoch: 52	Loss: 0.047810	Acc: 25.6% (2561/10000)
[Test]  Epoch: 53	Loss: 0.047665	Acc: 25.4% (2539/10000)
[Test]  Epoch: 54	Loss: 0.047541	Acc: 25.7% (2568/10000)
[Test]  Epoch: 55	Loss: 0.047848	Acc: 25.3% (2531/10000)
[Test]  Epoch: 56	Loss: 0.047668	Acc: 25.6% (2565/10000)
[Test]  Epoch: 57	Loss: 0.047773	Acc: 25.5% (2550/10000)
[Test]  Epoch: 58	Loss: 0.047357	Acc: 26.3% (2630/10000)
[Test]  Epoch: 59	Loss: 0.047453	Acc: 26.0% (2596/10000)
[Test]  Epoch: 60	Loss: 0.047663	Acc: 25.8% (2579/10000)
[Test]  Epoch: 61	Loss: 0.047457	Acc: 26.0% (2600/10000)
[Test]  Epoch: 62	Loss: 0.047321	Acc: 26.1% (2605/10000)
[Test]  Epoch: 63	Loss: 0.047309	Acc: 26.3% (2632/10000)
[Test]  Epoch: 64	Loss: 0.047259	Acc: 26.4% (2641/10000)
[Test]  Epoch: 65	Loss: 0.047152	Acc: 26.6% (2656/10000)
[Test]  Epoch: 66	Loss: 0.047189	Acc: 26.3% (2628/10000)
[Test]  Epoch: 67	Loss: 0.047312	Acc: 26.2% (2617/10000)
[Test]  Epoch: 68	Loss: 0.047263	Acc: 26.2% (2623/10000)
[Test]  Epoch: 69	Loss: 0.047200	Acc: 26.4% (2636/10000)
[Test]  Epoch: 70	Loss: 0.047310	Acc: 26.3% (2629/10000)
[Test]  Epoch: 71	Loss: 0.047235	Acc: 26.2% (2623/10000)
[Test]  Epoch: 72	Loss: 0.047264	Acc: 26.3% (2632/10000)
[Test]  Epoch: 73	Loss: 0.047206	Acc: 26.2% (2616/10000)
[Test]  Epoch: 74	Loss: 0.047140	Acc: 26.6% (2655/10000)
[Test]  Epoch: 75	Loss: 0.047132	Acc: 26.6% (2657/10000)
[Test]  Epoch: 76	Loss: 0.047229	Acc: 26.0% (2604/10000)
[Test]  Epoch: 77	Loss: 0.047151	Acc: 26.3% (2627/10000)
[Test]  Epoch: 78	Loss: 0.047276	Acc: 26.0% (2598/10000)
[Test]  Epoch: 79	Loss: 0.047259	Acc: 25.9% (2593/10000)
[Test]  Epoch: 80	Loss: 0.047233	Acc: 26.1% (2614/10000)
[Test]  Epoch: 81	Loss: 0.047117	Acc: 26.3% (2631/10000)
[Test]  Epoch: 82	Loss: 0.047154	Acc: 26.2% (2619/10000)
[Test]  Epoch: 83	Loss: 0.047174	Acc: 26.2% (2619/10000)
[Test]  Epoch: 84	Loss: 0.047199	Acc: 26.2% (2618/10000)
[Test]  Epoch: 85	Loss: 0.047141	Acc: 26.2% (2616/10000)
[Test]  Epoch: 86	Loss: 0.047292	Acc: 26.0% (2602/10000)
[Test]  Epoch: 87	Loss: 0.047232	Acc: 26.0% (2603/10000)
[Test]  Epoch: 88	Loss: 0.047296	Acc: 26.0% (2598/10000)
[Test]  Epoch: 89	Loss: 0.047253	Acc: 26.1% (2613/10000)
[Test]  Epoch: 90	Loss: 0.047230	Acc: 26.2% (2620/10000)
[Test]  Epoch: 91	Loss: 0.047204	Acc: 26.1% (2611/10000)
[Test]  Epoch: 92	Loss: 0.047266	Acc: 26.1% (2607/10000)
[Test]  Epoch: 93	Loss: 0.047232	Acc: 26.2% (2620/10000)
[Test]  Epoch: 94	Loss: 0.047201	Acc: 26.1% (2613/10000)
[Test]  Epoch: 95	Loss: 0.047330	Acc: 25.8% (2577/10000)
[Test]  Epoch: 96	Loss: 0.047208	Acc: 26.1% (2612/10000)
[Test]  Epoch: 97	Loss: 0.047300	Acc: 26.1% (2614/10000)
[Test]  Epoch: 98	Loss: 0.047198	Acc: 26.0% (2599/10000)
[Test]  Epoch: 99	Loss: 0.047255	Acc: 26.0% (2598/10000)
[Test]  Epoch: 100	Loss: 0.047285	Acc: 26.1% (2610/10000)
===========finish==========
['2024-08-19', '03:04:30.206041', '100', 'test', '0.04728466260433197', '26.1', '26.57']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.091807	Acc: 5.3% (533/10000)
[Test]  Epoch: 2	Loss: 0.066524	Acc: 11.9% (1191/10000)
[Test]  Epoch: 3	Loss: 0.055263	Acc: 18.2% (1822/10000)
[Test]  Epoch: 4	Loss: 0.054113	Acc: 19.2% (1920/10000)
[Test]  Epoch: 5	Loss: 0.052573	Acc: 20.5% (2053/10000)
[Test]  Epoch: 6	Loss: 0.052578	Acc: 20.1% (2005/10000)
[Test]  Epoch: 7	Loss: 0.051999	Acc: 20.9% (2087/10000)
[Test]  Epoch: 8	Loss: 0.052126	Acc: 20.9% (2092/10000)
[Test]  Epoch: 9	Loss: 0.051442	Acc: 21.2% (2121/10000)
[Test]  Epoch: 10	Loss: 0.051261	Acc: 21.3% (2127/10000)
[Test]  Epoch: 11	Loss: 0.051289	Acc: 21.7% (2174/10000)
[Test]  Epoch: 12	Loss: 0.051257	Acc: 21.7% (2174/10000)
[Test]  Epoch: 13	Loss: 0.050981	Acc: 21.9% (2195/10000)
[Test]  Epoch: 14	Loss: 0.051245	Acc: 21.7% (2168/10000)
[Test]  Epoch: 15	Loss: 0.051022	Acc: 22.2% (2218/10000)
[Test]  Epoch: 16	Loss: 0.050835	Acc: 22.4% (2235/10000)
[Test]  Epoch: 17	Loss: 0.051019	Acc: 22.3% (2227/10000)
[Test]  Epoch: 18	Loss: 0.050778	Acc: 22.1% (2215/10000)
[Test]  Epoch: 19	Loss: 0.050749	Acc: 22.2% (2225/10000)
[Test]  Epoch: 20	Loss: 0.050577	Acc: 22.2% (2219/10000)
[Test]  Epoch: 21	Loss: 0.050358	Acc: 22.5% (2252/10000)
[Test]  Epoch: 22	Loss: 0.050315	Acc: 22.4% (2237/10000)
[Test]  Epoch: 23	Loss: 0.050470	Acc: 22.2% (2223/10000)
[Test]  Epoch: 24	Loss: 0.050416	Acc: 22.0% (2198/10000)
[Test]  Epoch: 25	Loss: 0.050547	Acc: 22.5% (2249/10000)
[Test]  Epoch: 26	Loss: 0.050214	Acc: 22.5% (2248/10000)
[Test]  Epoch: 27	Loss: 0.050150	Acc: 22.3% (2230/10000)
[Test]  Epoch: 28	Loss: 0.050014	Acc: 22.6% (2264/10000)
[Test]  Epoch: 29	Loss: 0.049920	Acc: 22.8% (2280/10000)
[Test]  Epoch: 30	Loss: 0.050145	Acc: 22.6% (2261/10000)
[Test]  Epoch: 31	Loss: 0.049895	Acc: 22.6% (2259/10000)
[Test]  Epoch: 32	Loss: 0.049914	Acc: 22.6% (2261/10000)
[Test]  Epoch: 33	Loss: 0.049964	Acc: 22.6% (2260/10000)
[Test]  Epoch: 34	Loss: 0.050042	Acc: 22.3% (2233/10000)
[Test]  Epoch: 35	Loss: 0.049808	Acc: 23.1% (2307/10000)
[Test]  Epoch: 36	Loss: 0.049923	Acc: 22.6% (2261/10000)
[Test]  Epoch: 37	Loss: 0.049585	Acc: 23.1% (2305/10000)
[Test]  Epoch: 38	Loss: 0.049570	Acc: 23.1% (2306/10000)
[Test]  Epoch: 39	Loss: 0.049659	Acc: 22.9% (2287/10000)
[Test]  Epoch: 40	Loss: 0.049536	Acc: 23.2% (2317/10000)
[Test]  Epoch: 41	Loss: 0.049542	Acc: 23.0% (2299/10000)
[Test]  Epoch: 42	Loss: 0.049791	Acc: 23.0% (2301/10000)
[Test]  Epoch: 43	Loss: 0.049478	Acc: 23.1% (2311/10000)
[Test]  Epoch: 44	Loss: 0.049407	Acc: 22.9% (2291/10000)
[Test]  Epoch: 45	Loss: 0.049596	Acc: 23.0% (2300/10000)
[Test]  Epoch: 46	Loss: 0.049456	Acc: 23.3% (2333/10000)
[Test]  Epoch: 47	Loss: 0.049233	Acc: 23.4% (2339/10000)
[Test]  Epoch: 48	Loss: 0.049164	Acc: 23.1% (2314/10000)
[Test]  Epoch: 49	Loss: 0.049219	Acc: 23.1% (2315/10000)
[Test]  Epoch: 50	Loss: 0.049491	Acc: 23.1% (2314/10000)
[Test]  Epoch: 51	Loss: 0.049372	Acc: 23.1% (2310/10000)
[Test]  Epoch: 52	Loss: 0.049194	Acc: 23.2% (2318/10000)
[Test]  Epoch: 53	Loss: 0.049304	Acc: 23.1% (2314/10000)
[Test]  Epoch: 54	Loss: 0.049239	Acc: 23.2% (2323/10000)
[Test]  Epoch: 55	Loss: 0.049342	Acc: 23.4% (2345/10000)
[Test]  Epoch: 56	Loss: 0.049062	Acc: 23.3% (2332/10000)
[Test]  Epoch: 57	Loss: 0.048973	Acc: 23.6% (2357/10000)
[Test]  Epoch: 58	Loss: 0.049092	Acc: 23.3% (2329/10000)
[Test]  Epoch: 59	Loss: 0.048696	Acc: 23.8% (2376/10000)
[Test]  Epoch: 60	Loss: 0.049278	Acc: 22.9% (2290/10000)
[Test]  Epoch: 61	Loss: 0.049024	Acc: 23.1% (2308/10000)
[Test]  Epoch: 62	Loss: 0.048902	Acc: 23.1% (2315/10000)
[Test]  Epoch: 63	Loss: 0.048895	Acc: 23.3% (2328/10000)
[Test]  Epoch: 64	Loss: 0.048811	Acc: 23.2% (2319/10000)
[Test]  Epoch: 65	Loss: 0.048722	Acc: 23.3% (2327/10000)
[Test]  Epoch: 66	Loss: 0.048754	Acc: 23.3% (2330/10000)
[Test]  Epoch: 67	Loss: 0.048849	Acc: 23.2% (2324/10000)
[Test]  Epoch: 68	Loss: 0.048810	Acc: 23.2% (2320/10000)
[Test]  Epoch: 69	Loss: 0.048767	Acc: 23.4% (2335/10000)
[Test]  Epoch: 70	Loss: 0.048874	Acc: 23.3% (2329/10000)
[Test]  Epoch: 71	Loss: 0.048815	Acc: 23.6% (2360/10000)
[Test]  Epoch: 72	Loss: 0.048802	Acc: 23.6% (2357/10000)
[Test]  Epoch: 73	Loss: 0.048798	Acc: 23.4% (2342/10000)
[Test]  Epoch: 74	Loss: 0.048743	Acc: 23.4% (2343/10000)
[Test]  Epoch: 75	Loss: 0.048734	Acc: 23.5% (2352/10000)
[Test]  Epoch: 76	Loss: 0.048811	Acc: 23.3% (2330/10000)
[Test]  Epoch: 77	Loss: 0.048765	Acc: 23.4% (2342/10000)
[Test]  Epoch: 78	Loss: 0.048769	Acc: 23.4% (2341/10000)
[Test]  Epoch: 79	Loss: 0.048776	Acc: 23.4% (2341/10000)
[Test]  Epoch: 80	Loss: 0.048762	Acc: 23.5% (2350/10000)
[Test]  Epoch: 81	Loss: 0.048694	Acc: 23.6% (2356/10000)
[Test]  Epoch: 82	Loss: 0.048709	Acc: 23.3% (2326/10000)
[Test]  Epoch: 83	Loss: 0.048764	Acc: 23.2% (2325/10000)
[Test]  Epoch: 84	Loss: 0.048773	Acc: 23.3% (2332/10000)
[Test]  Epoch: 85	Loss: 0.048722	Acc: 23.3% (2334/10000)
[Test]  Epoch: 86	Loss: 0.048818	Acc: 23.3% (2331/10000)
[Test]  Epoch: 87	Loss: 0.048863	Acc: 23.3% (2330/10000)
[Test]  Epoch: 88	Loss: 0.048845	Acc: 23.4% (2341/10000)
[Test]  Epoch: 89	Loss: 0.048792	Acc: 23.4% (2336/10000)
[Test]  Epoch: 90	Loss: 0.048752	Acc: 23.3% (2331/10000)
[Test]  Epoch: 91	Loss: 0.048737	Acc: 23.5% (2348/10000)
[Test]  Epoch: 92	Loss: 0.048737	Acc: 23.4% (2338/10000)
[Test]  Epoch: 93	Loss: 0.048776	Acc: 23.4% (2344/10000)
[Test]  Epoch: 94	Loss: 0.048785	Acc: 23.5% (2351/10000)
[Test]  Epoch: 95	Loss: 0.048869	Acc: 23.4% (2339/10000)
[Test]  Epoch: 96	Loss: 0.048730	Acc: 23.1% (2315/10000)
[Test]  Epoch: 97	Loss: 0.048834	Acc: 23.2% (2325/10000)
[Test]  Epoch: 98	Loss: 0.048723	Acc: 23.3% (2333/10000)
[Test]  Epoch: 99	Loss: 0.048743	Acc: 23.3% (2326/10000)
[Test]  Epoch: 100	Loss: 0.048799	Acc: 23.3% (2329/10000)
===========finish==========
['2024-08-19', '03:08:37.633861', '100', 'test', '0.04879856250286102', '23.29', '23.76']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.107836	Acc: 2.8% (283/10000)
[Test]  Epoch: 2	Loss: 0.061720	Acc: 13.5% (1351/10000)
[Test]  Epoch: 3	Loss: 0.057065	Acc: 16.7% (1669/10000)
[Test]  Epoch: 4	Loss: 0.055538	Acc: 17.4% (1743/10000)
[Test]  Epoch: 5	Loss: 0.055226	Acc: 17.9% (1785/10000)
[Test]  Epoch: 6	Loss: 0.055001	Acc: 17.9% (1795/10000)
[Test]  Epoch: 7	Loss: 0.053987	Acc: 18.8% (1878/10000)
[Test]  Epoch: 8	Loss: 0.053949	Acc: 19.1% (1914/10000)
[Test]  Epoch: 9	Loss: 0.054351	Acc: 19.0% (1900/10000)
[Test]  Epoch: 10	Loss: 0.053502	Acc: 19.4% (1938/10000)
[Test]  Epoch: 11	Loss: 0.053633	Acc: 19.5% (1953/10000)
[Test]  Epoch: 12	Loss: 0.053386	Acc: 19.6% (1957/10000)
[Test]  Epoch: 13	Loss: 0.053295	Acc: 19.9% (1986/10000)
[Test]  Epoch: 14	Loss: 0.053307	Acc: 19.9% (1995/10000)
[Test]  Epoch: 15	Loss: 0.053349	Acc: 19.6% (1955/10000)
[Test]  Epoch: 16	Loss: 0.053337	Acc: 19.9% (1991/10000)
[Test]  Epoch: 17	Loss: 0.052903	Acc: 19.8% (1984/10000)
[Test]  Epoch: 18	Loss: 0.053067	Acc: 20.4% (2045/10000)
[Test]  Epoch: 19	Loss: 0.053095	Acc: 20.0% (2002/10000)
[Test]  Epoch: 20	Loss: 0.052934	Acc: 19.8% (1978/10000)
[Test]  Epoch: 21	Loss: 0.052801	Acc: 20.2% (2021/10000)
[Test]  Epoch: 22	Loss: 0.052479	Acc: 20.6% (2058/10000)
[Test]  Epoch: 23	Loss: 0.052962	Acc: 20.1% (2007/10000)
[Test]  Epoch: 24	Loss: 0.052820	Acc: 20.5% (2047/10000)
[Test]  Epoch: 25	Loss: 0.052546	Acc: 20.0% (2000/10000)
[Test]  Epoch: 26	Loss: 0.052494	Acc: 20.3% (2031/10000)
[Test]  Epoch: 27	Loss: 0.052437	Acc: 20.3% (2030/10000)
[Test]  Epoch: 28	Loss: 0.052418	Acc: 20.6% (2056/10000)
[Test]  Epoch: 29	Loss: 0.052444	Acc: 20.6% (2057/10000)
[Test]  Epoch: 30	Loss: 0.052184	Acc: 20.7% (2072/10000)
[Test]  Epoch: 31	Loss: 0.052268	Acc: 20.9% (2087/10000)
[Test]  Epoch: 32	Loss: 0.052152	Acc: 20.5% (2050/10000)
[Test]  Epoch: 33	Loss: 0.052030	Acc: 21.1% (2110/10000)
[Test]  Epoch: 34	Loss: 0.051700	Acc: 21.2% (2121/10000)
[Test]  Epoch: 35	Loss: 0.051864	Acc: 20.8% (2077/10000)
[Test]  Epoch: 36	Loss: 0.051999	Acc: 20.7% (2070/10000)
[Test]  Epoch: 37	Loss: 0.051696	Acc: 21.2% (2123/10000)
[Test]  Epoch: 38	Loss: 0.051510	Acc: 20.9% (2094/10000)
[Test]  Epoch: 39	Loss: 0.051468	Acc: 21.4% (2136/10000)
[Test]  Epoch: 40	Loss: 0.051719	Acc: 20.9% (2087/10000)
[Test]  Epoch: 41	Loss: 0.051770	Acc: 20.6% (2059/10000)
[Test]  Epoch: 42	Loss: 0.051506	Acc: 21.1% (2114/10000)
[Test]  Epoch: 43	Loss: 0.051493	Acc: 21.1% (2113/10000)
[Test]  Epoch: 44	Loss: 0.051433	Acc: 21.1% (2109/10000)
[Test]  Epoch: 45	Loss: 0.051342	Acc: 21.1% (2105/10000)
[Test]  Epoch: 46	Loss: 0.051343	Acc: 21.5% (2148/10000)
[Test]  Epoch: 47	Loss: 0.051444	Acc: 21.2% (2120/10000)
[Test]  Epoch: 48	Loss: 0.051304	Acc: 21.1% (2108/10000)
[Test]  Epoch: 49	Loss: 0.051337	Acc: 21.2% (2121/10000)
[Test]  Epoch: 50	Loss: 0.051191	Acc: 21.4% (2143/10000)
[Test]  Epoch: 51	Loss: 0.051157	Acc: 21.1% (2109/10000)
[Test]  Epoch: 52	Loss: 0.051200	Acc: 21.6% (2156/10000)
[Test]  Epoch: 53	Loss: 0.050995	Acc: 21.6% (2159/10000)
[Test]  Epoch: 54	Loss: 0.050971	Acc: 21.6% (2164/10000)
[Test]  Epoch: 55	Loss: 0.051043	Acc: 21.6% (2165/10000)
[Test]  Epoch: 56	Loss: 0.050944	Acc: 21.3% (2126/10000)
[Test]  Epoch: 57	Loss: 0.051006	Acc: 21.4% (2137/10000)
[Test]  Epoch: 58	Loss: 0.050869	Acc: 21.6% (2162/10000)
[Test]  Epoch: 59	Loss: 0.050940	Acc: 21.5% (2149/10000)
[Test]  Epoch: 60	Loss: 0.050904	Acc: 21.6% (2161/10000)
[Test]  Epoch: 61	Loss: 0.050915	Acc: 21.6% (2155/10000)
[Test]  Epoch: 62	Loss: 0.050846	Acc: 21.5% (2149/10000)
[Test]  Epoch: 63	Loss: 0.050830	Acc: 21.4% (2138/10000)
[Test]  Epoch: 64	Loss: 0.050801	Acc: 21.5% (2151/10000)
[Test]  Epoch: 65	Loss: 0.050703	Acc: 21.6% (2162/10000)
[Test]  Epoch: 66	Loss: 0.050711	Acc: 21.5% (2153/10000)
[Test]  Epoch: 67	Loss: 0.050750	Acc: 21.8% (2175/10000)
[Test]  Epoch: 68	Loss: 0.050778	Acc: 21.7% (2170/10000)
[Test]  Epoch: 69	Loss: 0.050749	Acc: 21.6% (2157/10000)
[Test]  Epoch: 70	Loss: 0.050845	Acc: 21.7% (2170/10000)
[Test]  Epoch: 71	Loss: 0.050727	Acc: 21.7% (2166/10000)
[Test]  Epoch: 72	Loss: 0.050755	Acc: 21.7% (2166/10000)
[Test]  Epoch: 73	Loss: 0.050760	Acc: 21.7% (2170/10000)
[Test]  Epoch: 74	Loss: 0.050678	Acc: 21.7% (2167/10000)
[Test]  Epoch: 75	Loss: 0.050708	Acc: 21.9% (2189/10000)
[Test]  Epoch: 76	Loss: 0.050728	Acc: 21.8% (2176/10000)
[Test]  Epoch: 77	Loss: 0.050678	Acc: 21.9% (2187/10000)
[Test]  Epoch: 78	Loss: 0.050757	Acc: 21.8% (2177/10000)
[Test]  Epoch: 79	Loss: 0.050753	Acc: 21.8% (2181/10000)
[Test]  Epoch: 80	Loss: 0.050802	Acc: 21.8% (2178/10000)
[Test]  Epoch: 81	Loss: 0.050675	Acc: 21.9% (2190/10000)
[Test]  Epoch: 82	Loss: 0.050734	Acc: 21.6% (2159/10000)
[Test]  Epoch: 83	Loss: 0.050726	Acc: 21.5% (2153/10000)
[Test]  Epoch: 84	Loss: 0.050753	Acc: 21.7% (2174/10000)
[Test]  Epoch: 85	Loss: 0.050661	Acc: 21.8% (2179/10000)
[Test]  Epoch: 86	Loss: 0.050763	Acc: 21.6% (2160/10000)
[Test]  Epoch: 87	Loss: 0.050697	Acc: 21.7% (2174/10000)
[Test]  Epoch: 88	Loss: 0.050790	Acc: 21.8% (2183/10000)
[Test]  Epoch: 89	Loss: 0.050761	Acc: 21.8% (2179/10000)
[Test]  Epoch: 90	Loss: 0.050675	Acc: 21.9% (2185/10000)
[Test]  Epoch: 91	Loss: 0.050672	Acc: 21.7% (2173/10000)
[Test]  Epoch: 92	Loss: 0.050722	Acc: 21.8% (2176/10000)
[Test]  Epoch: 93	Loss: 0.050714	Acc: 21.8% (2182/10000)
[Test]  Epoch: 94	Loss: 0.050647	Acc: 22.0% (2200/10000)
[Test]  Epoch: 95	Loss: 0.050812	Acc: 21.6% (2164/10000)
[Test]  Epoch: 96	Loss: 0.050706	Acc: 21.7% (2172/10000)
[Test]  Epoch: 97	Loss: 0.050768	Acc: 21.9% (2190/10000)
[Test]  Epoch: 98	Loss: 0.050665	Acc: 21.9% (2187/10000)
[Test]  Epoch: 99	Loss: 0.050700	Acc: 21.9% (2189/10000)
[Test]  Epoch: 100	Loss: 0.050732	Acc: 22.0% (2204/10000)
===========finish==========
['2024-08-19', '03:12:36.765979', '100', 'test', '0.05073241267204285', '22.04', '22.04']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.155584	Acc: 2.8% (281/10000)
[Test]  Epoch: 2	Loss: 0.059187	Acc: 15.5% (1547/10000)
[Test]  Epoch: 3	Loss: 0.055018	Acc: 17.8% (1779/10000)
[Test]  Epoch: 4	Loss: 0.054273	Acc: 18.8% (1878/10000)
[Test]  Epoch: 5	Loss: 0.053170	Acc: 19.7% (1967/10000)
[Test]  Epoch: 6	Loss: 0.052836	Acc: 20.1% (2007/10000)
[Test]  Epoch: 7	Loss: 0.052658	Acc: 20.1% (2006/10000)
[Test]  Epoch: 8	Loss: 0.052164	Acc: 20.6% (2059/10000)
[Test]  Epoch: 9	Loss: 0.052195	Acc: 21.1% (2114/10000)
[Test]  Epoch: 10	Loss: 0.052239	Acc: 20.4% (2044/10000)
[Test]  Epoch: 11	Loss: 0.051510	Acc: 21.4% (2142/10000)
[Test]  Epoch: 12	Loss: 0.051250	Acc: 21.6% (2162/10000)
[Test]  Epoch: 13	Loss: 0.051432	Acc: 21.4% (2136/10000)
[Test]  Epoch: 14	Loss: 0.051308	Acc: 21.3% (2134/10000)
[Test]  Epoch: 15	Loss: 0.051020	Acc: 21.9% (2188/10000)
[Test]  Epoch: 16	Loss: 0.050908	Acc: 21.9% (2185/10000)
[Test]  Epoch: 17	Loss: 0.050652	Acc: 22.1% (2210/10000)
[Test]  Epoch: 18	Loss: 0.050707	Acc: 22.1% (2210/10000)
[Test]  Epoch: 19	Loss: 0.050940	Acc: 22.0% (2198/10000)
[Test]  Epoch: 20	Loss: 0.050649	Acc: 22.0% (2203/10000)
[Test]  Epoch: 21	Loss: 0.050416	Acc: 22.1% (2208/10000)
[Test]  Epoch: 22	Loss: 0.050367	Acc: 22.1% (2214/10000)
[Test]  Epoch: 23	Loss: 0.050128	Acc: 23.2% (2317/10000)
[Test]  Epoch: 24	Loss: 0.050059	Acc: 22.5% (2250/10000)
[Test]  Epoch: 25	Loss: 0.050163	Acc: 22.5% (2248/10000)
[Test]  Epoch: 26	Loss: 0.049813	Acc: 22.7% (2272/10000)
[Test]  Epoch: 27	Loss: 0.049819	Acc: 22.5% (2250/10000)
[Test]  Epoch: 28	Loss: 0.049796	Acc: 22.6% (2258/10000)
[Test]  Epoch: 29	Loss: 0.049767	Acc: 22.8% (2276/10000)
[Test]  Epoch: 30	Loss: 0.049875	Acc: 22.5% (2247/10000)
[Test]  Epoch: 31	Loss: 0.049702	Acc: 22.7% (2268/10000)
[Test]  Epoch: 32	Loss: 0.049563	Acc: 23.2% (2320/10000)
[Test]  Epoch: 33	Loss: 0.049945	Acc: 22.5% (2248/10000)
[Test]  Epoch: 34	Loss: 0.049410	Acc: 23.1% (2308/10000)
[Test]  Epoch: 35	Loss: 0.049307	Acc: 23.4% (2335/10000)
[Test]  Epoch: 36	Loss: 0.049396	Acc: 23.0% (2300/10000)
[Test]  Epoch: 37	Loss: 0.049491	Acc: 22.9% (2292/10000)
[Test]  Epoch: 38	Loss: 0.049386	Acc: 23.0% (2298/10000)
[Test]  Epoch: 39	Loss: 0.049425	Acc: 22.9% (2287/10000)
[Test]  Epoch: 40	Loss: 0.049196	Acc: 23.3% (2331/10000)
[Test]  Epoch: 41	Loss: 0.049145	Acc: 23.3% (2331/10000)
[Test]  Epoch: 42	Loss: 0.049148	Acc: 23.1% (2305/10000)
[Test]  Epoch: 43	Loss: 0.049037	Acc: 23.4% (2345/10000)
[Test]  Epoch: 44	Loss: 0.048954	Acc: 23.3% (2331/10000)
[Test]  Epoch: 45	Loss: 0.049153	Acc: 23.0% (2302/10000)
[Test]  Epoch: 46	Loss: 0.049127	Acc: 23.2% (2317/10000)
[Test]  Epoch: 47	Loss: 0.048852	Acc: 23.5% (2347/10000)
[Test]  Epoch: 48	Loss: 0.048864	Acc: 23.7% (2371/10000)
[Test]  Epoch: 49	Loss: 0.049052	Acc: 23.3% (2331/10000)
[Test]  Epoch: 50	Loss: 0.048985	Acc: 23.5% (2351/10000)
[Test]  Epoch: 51	Loss: 0.049140	Acc: 23.2% (2325/10000)
[Test]  Epoch: 52	Loss: 0.048925	Acc: 23.3% (2327/10000)
[Test]  Epoch: 53	Loss: 0.049115	Acc: 22.9% (2285/10000)
[Test]  Epoch: 54	Loss: 0.048642	Acc: 23.5% (2349/10000)
[Test]  Epoch: 55	Loss: 0.048969	Acc: 22.9% (2294/10000)
[Test]  Epoch: 56	Loss: 0.048792	Acc: 23.3% (2326/10000)
[Test]  Epoch: 57	Loss: 0.048885	Acc: 23.5% (2354/10000)
[Test]  Epoch: 58	Loss: 0.048596	Acc: 23.7% (2367/10000)
[Test]  Epoch: 59	Loss: 0.048712	Acc: 23.6% (2362/10000)
[Test]  Epoch: 60	Loss: 0.048696	Acc: 23.4% (2337/10000)
[Test]  Epoch: 61	Loss: 0.048596	Acc: 23.6% (2362/10000)
[Test]  Epoch: 62	Loss: 0.048501	Acc: 24.0% (2401/10000)
[Test]  Epoch: 63	Loss: 0.048476	Acc: 23.9% (2395/10000)
[Test]  Epoch: 64	Loss: 0.048435	Acc: 24.0% (2398/10000)
[Test]  Epoch: 65	Loss: 0.048346	Acc: 24.4% (2436/10000)
[Test]  Epoch: 66	Loss: 0.048372	Acc: 24.0% (2397/10000)
[Test]  Epoch: 67	Loss: 0.048412	Acc: 23.9% (2385/10000)
[Test]  Epoch: 68	Loss: 0.048420	Acc: 24.0% (2404/10000)
[Test]  Epoch: 69	Loss: 0.048384	Acc: 23.9% (2395/10000)
[Test]  Epoch: 70	Loss: 0.048413	Acc: 24.1% (2413/10000)
[Test]  Epoch: 71	Loss: 0.048343	Acc: 24.1% (2411/10000)
[Test]  Epoch: 72	Loss: 0.048409	Acc: 24.0% (2402/10000)
[Test]  Epoch: 73	Loss: 0.048355	Acc: 23.8% (2382/10000)
[Test]  Epoch: 74	Loss: 0.048291	Acc: 24.0% (2397/10000)
[Test]  Epoch: 75	Loss: 0.048334	Acc: 24.1% (2405/10000)
[Test]  Epoch: 76	Loss: 0.048403	Acc: 23.8% (2381/10000)
[Test]  Epoch: 77	Loss: 0.048329	Acc: 24.0% (2403/10000)
[Test]  Epoch: 78	Loss: 0.048435	Acc: 23.8% (2382/10000)
[Test]  Epoch: 79	Loss: 0.048413	Acc: 23.6% (2364/10000)
[Test]  Epoch: 80	Loss: 0.048395	Acc: 23.8% (2378/10000)
[Test]  Epoch: 81	Loss: 0.048248	Acc: 24.3% (2432/10000)
[Test]  Epoch: 82	Loss: 0.048278	Acc: 23.8% (2382/10000)
[Test]  Epoch: 83	Loss: 0.048308	Acc: 23.9% (2388/10000)
[Test]  Epoch: 84	Loss: 0.048437	Acc: 23.9% (2387/10000)
[Test]  Epoch: 85	Loss: 0.048285	Acc: 24.0% (2403/10000)
[Test]  Epoch: 86	Loss: 0.048441	Acc: 24.0% (2400/10000)
[Test]  Epoch: 87	Loss: 0.048381	Acc: 23.8% (2383/10000)
[Test]  Epoch: 88	Loss: 0.048428	Acc: 23.9% (2388/10000)
[Test]  Epoch: 89	Loss: 0.048367	Acc: 24.1% (2406/10000)
[Test]  Epoch: 90	Loss: 0.048373	Acc: 23.9% (2387/10000)
[Test]  Epoch: 91	Loss: 0.048356	Acc: 23.9% (2385/10000)
[Test]  Epoch: 92	Loss: 0.048390	Acc: 23.8% (2383/10000)
[Test]  Epoch: 93	Loss: 0.048401	Acc: 23.6% (2357/10000)
[Test]  Epoch: 94	Loss: 0.048301	Acc: 23.8% (2384/10000)
[Test]  Epoch: 95	Loss: 0.048460	Acc: 23.6% (2364/10000)
[Test]  Epoch: 96	Loss: 0.048328	Acc: 24.1% (2410/10000)
[Test]  Epoch: 97	Loss: 0.048417	Acc: 23.8% (2375/10000)
[Test]  Epoch: 98	Loss: 0.048362	Acc: 23.8% (2383/10000)
[Test]  Epoch: 99	Loss: 0.048406	Acc: 23.9% (2390/10000)
[Test]  Epoch: 100	Loss: 0.048388	Acc: 23.8% (2381/10000)
===========finish==========
['2024-08-19', '03:16:43.836296', '100', 'test', '0.048388153266906736', '23.81', '24.36']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.144169	Acc: 3.6% (356/10000)
[Test]  Epoch: 2	Loss: 0.061056	Acc: 14.0% (1402/10000)
[Test]  Epoch: 3	Loss: 0.055450	Acc: 16.9% (1694/10000)
[Test]  Epoch: 4	Loss: 0.054776	Acc: 17.3% (1727/10000)
[Test]  Epoch: 5	Loss: 0.053092	Acc: 18.9% (1888/10000)
[Test]  Epoch: 6	Loss: 0.053278	Acc: 19.0% (1901/10000)
[Test]  Epoch: 7	Loss: 0.052493	Acc: 19.3% (1929/10000)
[Test]  Epoch: 8	Loss: 0.052045	Acc: 20.4% (2042/10000)
[Test]  Epoch: 9	Loss: 0.051871	Acc: 20.9% (2093/10000)
[Test]  Epoch: 10	Loss: 0.051647	Acc: 20.9% (2085/10000)
[Test]  Epoch: 11	Loss: 0.051095	Acc: 21.0% (2101/10000)
[Test]  Epoch: 12	Loss: 0.051154	Acc: 21.5% (2147/10000)
[Test]  Epoch: 13	Loss: 0.050982	Acc: 21.4% (2142/10000)
[Test]  Epoch: 14	Loss: 0.050928	Acc: 21.5% (2149/10000)
[Test]  Epoch: 15	Loss: 0.050787	Acc: 21.4% (2143/10000)
[Test]  Epoch: 16	Loss: 0.050721	Acc: 21.6% (2158/10000)
[Test]  Epoch: 17	Loss: 0.050627	Acc: 21.7% (2170/10000)
[Test]  Epoch: 18	Loss: 0.050828	Acc: 21.6% (2157/10000)
[Test]  Epoch: 19	Loss: 0.050371	Acc: 21.7% (2172/10000)
[Test]  Epoch: 20	Loss: 0.050216	Acc: 21.9% (2186/10000)
[Test]  Epoch: 21	Loss: 0.050063	Acc: 22.0% (2203/10000)
[Test]  Epoch: 22	Loss: 0.050022	Acc: 21.8% (2183/10000)
[Test]  Epoch: 23	Loss: 0.049969	Acc: 22.3% (2226/10000)
[Test]  Epoch: 24	Loss: 0.049858	Acc: 21.9% (2194/10000)
[Test]  Epoch: 25	Loss: 0.050008	Acc: 22.0% (2200/10000)
[Test]  Epoch: 26	Loss: 0.049736	Acc: 22.1% (2214/10000)
[Test]  Epoch: 27	Loss: 0.049607	Acc: 22.3% (2226/10000)
[Test]  Epoch: 28	Loss: 0.049859	Acc: 22.1% (2207/10000)
[Test]  Epoch: 29	Loss: 0.049387	Acc: 22.4% (2239/10000)
[Test]  Epoch: 30	Loss: 0.049640	Acc: 22.2% (2222/10000)
[Test]  Epoch: 31	Loss: 0.049310	Acc: 22.4% (2244/10000)
[Test]  Epoch: 32	Loss: 0.049270	Acc: 22.9% (2291/10000)
[Test]  Epoch: 33	Loss: 0.049560	Acc: 22.3% (2227/10000)
[Test]  Epoch: 34	Loss: 0.049184	Acc: 22.5% (2249/10000)
[Test]  Epoch: 35	Loss: 0.049044	Acc: 22.8% (2278/10000)
[Test]  Epoch: 36	Loss: 0.049294	Acc: 22.4% (2243/10000)
[Test]  Epoch: 37	Loss: 0.049094	Acc: 22.6% (2262/10000)
[Test]  Epoch: 38	Loss: 0.049050	Acc: 22.5% (2252/10000)
[Test]  Epoch: 39	Loss: 0.049035	Acc: 22.4% (2245/10000)
[Test]  Epoch: 40	Loss: 0.048839	Acc: 22.7% (2272/10000)
[Test]  Epoch: 41	Loss: 0.048802	Acc: 23.0% (2303/10000)
[Test]  Epoch: 42	Loss: 0.048667	Acc: 22.7% (2271/10000)
[Test]  Epoch: 43	Loss: 0.048670	Acc: 23.0% (2297/10000)
[Test]  Epoch: 44	Loss: 0.048600	Acc: 22.9% (2292/10000)
[Test]  Epoch: 45	Loss: 0.048701	Acc: 22.6% (2265/10000)
[Test]  Epoch: 46	Loss: 0.048577	Acc: 23.3% (2331/10000)
[Test]  Epoch: 47	Loss: 0.048566	Acc: 23.1% (2305/10000)
[Test]  Epoch: 48	Loss: 0.048671	Acc: 23.1% (2308/10000)
[Test]  Epoch: 49	Loss: 0.048519	Acc: 22.9% (2293/10000)
[Test]  Epoch: 50	Loss: 0.048385	Acc: 23.6% (2356/10000)
[Test]  Epoch: 51	Loss: 0.048758	Acc: 23.1% (2311/10000)
[Test]  Epoch: 52	Loss: 0.048413	Acc: 23.4% (2345/10000)
[Test]  Epoch: 53	Loss: 0.048354	Acc: 23.0% (2298/10000)
[Test]  Epoch: 54	Loss: 0.048235	Acc: 23.0% (2304/10000)
[Test]  Epoch: 55	Loss: 0.048206	Acc: 23.6% (2361/10000)
[Test]  Epoch: 56	Loss: 0.048232	Acc: 23.4% (2335/10000)
[Test]  Epoch: 57	Loss: 0.048222	Acc: 23.4% (2336/10000)
[Test]  Epoch: 58	Loss: 0.048097	Acc: 23.8% (2382/10000)
[Test]  Epoch: 59	Loss: 0.048000	Acc: 24.0% (2399/10000)
[Test]  Epoch: 60	Loss: 0.048038	Acc: 23.6% (2359/10000)
[Test]  Epoch: 61	Loss: 0.047991	Acc: 23.4% (2342/10000)
[Test]  Epoch: 62	Loss: 0.047967	Acc: 23.5% (2352/10000)
[Test]  Epoch: 63	Loss: 0.047968	Acc: 23.6% (2355/10000)
[Test]  Epoch: 64	Loss: 0.047935	Acc: 23.5% (2350/10000)
[Test]  Epoch: 65	Loss: 0.047798	Acc: 23.8% (2376/10000)
[Test]  Epoch: 66	Loss: 0.047794	Acc: 23.8% (2380/10000)
[Test]  Epoch: 67	Loss: 0.047850	Acc: 23.8% (2378/10000)
[Test]  Epoch: 68	Loss: 0.047892	Acc: 23.6% (2361/10000)
[Test]  Epoch: 69	Loss: 0.047853	Acc: 23.8% (2375/10000)
[Test]  Epoch: 70	Loss: 0.047911	Acc: 23.6% (2365/10000)
[Test]  Epoch: 71	Loss: 0.047865	Acc: 23.7% (2369/10000)
[Test]  Epoch: 72	Loss: 0.047872	Acc: 23.6% (2361/10000)
[Test]  Epoch: 73	Loss: 0.047858	Acc: 23.8% (2379/10000)
[Test]  Epoch: 74	Loss: 0.047824	Acc: 23.8% (2384/10000)
[Test]  Epoch: 75	Loss: 0.047794	Acc: 23.7% (2370/10000)
[Test]  Epoch: 76	Loss: 0.047845	Acc: 23.9% (2385/10000)
[Test]  Epoch: 77	Loss: 0.047752	Acc: 23.9% (2385/10000)
[Test]  Epoch: 78	Loss: 0.047896	Acc: 23.7% (2370/10000)
[Test]  Epoch: 79	Loss: 0.047871	Acc: 23.8% (2383/10000)
[Test]  Epoch: 80	Loss: 0.047876	Acc: 23.6% (2359/10000)
[Test]  Epoch: 81	Loss: 0.047698	Acc: 24.0% (2400/10000)
[Test]  Epoch: 82	Loss: 0.047777	Acc: 23.8% (2382/10000)
[Test]  Epoch: 83	Loss: 0.047803	Acc: 23.8% (2379/10000)
[Test]  Epoch: 84	Loss: 0.047870	Acc: 23.8% (2375/10000)
[Test]  Epoch: 85	Loss: 0.047782	Acc: 23.5% (2351/10000)
[Test]  Epoch: 86	Loss: 0.047884	Acc: 23.6% (2361/10000)
[Test]  Epoch: 87	Loss: 0.047833	Acc: 23.8% (2379/10000)
[Test]  Epoch: 88	Loss: 0.047869	Acc: 23.8% (2380/10000)
[Test]  Epoch: 89	Loss: 0.047817	Acc: 23.7% (2369/10000)
[Test]  Epoch: 90	Loss: 0.047849	Acc: 23.7% (2368/10000)
[Test]  Epoch: 91	Loss: 0.047860	Acc: 23.5% (2350/10000)
[Test]  Epoch: 92	Loss: 0.047828	Acc: 23.7% (2372/10000)
[Test]  Epoch: 93	Loss: 0.047865	Acc: 23.5% (2350/10000)
[Test]  Epoch: 94	Loss: 0.047761	Acc: 23.7% (2370/10000)
[Test]  Epoch: 95	Loss: 0.047936	Acc: 23.6% (2355/10000)
[Test]  Epoch: 96	Loss: 0.047806	Acc: 23.7% (2373/10000)
[Test]  Epoch: 97	Loss: 0.047900	Acc: 23.8% (2376/10000)
[Test]  Epoch: 98	Loss: 0.047843	Acc: 23.7% (2368/10000)
[Test]  Epoch: 99	Loss: 0.047865	Acc: 23.5% (2354/10000)
[Test]  Epoch: 100	Loss: 0.047830	Acc: 23.6% (2363/10000)
===========finish==========
['2024-08-19', '03:20:39.186089', '100', 'test', '0.04783032517433167', '23.63', '24.0']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.139170	Acc: 3.5% (346/10000)
[Test]  Epoch: 2	Loss: 0.062086	Acc: 13.0% (1300/10000)
[Test]  Epoch: 3	Loss: 0.057314	Acc: 15.5% (1548/10000)
[Test]  Epoch: 4	Loss: 0.055259	Acc: 17.5% (1751/10000)
[Test]  Epoch: 5	Loss: 0.054497	Acc: 18.1% (1808/10000)
[Test]  Epoch: 6	Loss: 0.054251	Acc: 18.1% (1811/10000)
[Test]  Epoch: 7	Loss: 0.053940	Acc: 18.6% (1863/10000)
[Test]  Epoch: 8	Loss: 0.053689	Acc: 18.7% (1870/10000)
[Test]  Epoch: 9	Loss: 0.053550	Acc: 19.3% (1929/10000)
[Test]  Epoch: 10	Loss: 0.053702	Acc: 18.8% (1876/10000)
[Test]  Epoch: 11	Loss: 0.053421	Acc: 18.8% (1883/10000)
[Test]  Epoch: 12	Loss: 0.053052	Acc: 19.7% (1972/10000)
[Test]  Epoch: 13	Loss: 0.053183	Acc: 19.4% (1935/10000)
[Test]  Epoch: 14	Loss: 0.053120	Acc: 19.6% (1960/10000)
[Test]  Epoch: 15	Loss: 0.052825	Acc: 19.7% (1969/10000)
[Test]  Epoch: 16	Loss: 0.052721	Acc: 19.6% (1958/10000)
[Test]  Epoch: 17	Loss: 0.052499	Acc: 19.9% (1995/10000)
[Test]  Epoch: 18	Loss: 0.052608	Acc: 19.7% (1967/10000)
[Test]  Epoch: 19	Loss: 0.052287	Acc: 20.4% (2037/10000)
[Test]  Epoch: 20	Loss: 0.052377	Acc: 20.2% (2016/10000)
[Test]  Epoch: 21	Loss: 0.052111	Acc: 20.0% (1997/10000)
[Test]  Epoch: 22	Loss: 0.052205	Acc: 20.7% (2068/10000)
[Test]  Epoch: 23	Loss: 0.051723	Acc: 20.4% (2045/10000)
[Test]  Epoch: 24	Loss: 0.051998	Acc: 20.0% (2000/10000)
[Test]  Epoch: 25	Loss: 0.051974	Acc: 20.1% (2013/10000)
[Test]  Epoch: 26	Loss: 0.051767	Acc: 20.6% (2059/10000)
[Test]  Epoch: 27	Loss: 0.051613	Acc: 20.7% (2071/10000)
[Test]  Epoch: 28	Loss: 0.051770	Acc: 20.8% (2078/10000)
[Test]  Epoch: 29	Loss: 0.051551	Acc: 20.8% (2076/10000)
[Test]  Epoch: 30	Loss: 0.051662	Acc: 20.5% (2054/10000)
[Test]  Epoch: 31	Loss: 0.051670	Acc: 20.6% (2065/10000)
[Test]  Epoch: 32	Loss: 0.051333	Acc: 21.1% (2110/10000)
[Test]  Epoch: 33	Loss: 0.051269	Acc: 21.1% (2109/10000)
[Test]  Epoch: 34	Loss: 0.051202	Acc: 21.2% (2120/10000)
[Test]  Epoch: 35	Loss: 0.051196	Acc: 21.1% (2108/10000)
[Test]  Epoch: 36	Loss: 0.051174	Acc: 21.0% (2102/10000)
[Test]  Epoch: 37	Loss: 0.051572	Acc: 20.7% (2069/10000)
[Test]  Epoch: 38	Loss: 0.051252	Acc: 20.9% (2091/10000)
[Test]  Epoch: 39	Loss: 0.051223	Acc: 21.0% (2096/10000)
[Test]  Epoch: 40	Loss: 0.050882	Acc: 21.3% (2130/10000)
[Test]  Epoch: 41	Loss: 0.050857	Acc: 21.4% (2143/10000)
[Test]  Epoch: 42	Loss: 0.050968	Acc: 21.4% (2143/10000)
[Test]  Epoch: 43	Loss: 0.050950	Acc: 21.5% (2151/10000)
[Test]  Epoch: 44	Loss: 0.050926	Acc: 21.1% (2109/10000)
[Test]  Epoch: 45	Loss: 0.050829	Acc: 21.2% (2125/10000)
[Test]  Epoch: 46	Loss: 0.050871	Acc: 21.3% (2128/10000)
[Test]  Epoch: 47	Loss: 0.050832	Acc: 21.1% (2112/10000)
[Test]  Epoch: 48	Loss: 0.050725	Acc: 21.2% (2125/10000)
[Test]  Epoch: 49	Loss: 0.050616	Acc: 21.3% (2129/10000)
[Test]  Epoch: 50	Loss: 0.050688	Acc: 21.3% (2130/10000)
[Test]  Epoch: 51	Loss: 0.050587	Acc: 21.2% (2124/10000)
[Test]  Epoch: 52	Loss: 0.050711	Acc: 21.2% (2120/10000)
[Test]  Epoch: 53	Loss: 0.050632	Acc: 21.0% (2102/10000)
[Test]  Epoch: 54	Loss: 0.050570	Acc: 21.3% (2128/10000)
[Test]  Epoch: 55	Loss: 0.050467	Acc: 21.6% (2162/10000)
[Test]  Epoch: 56	Loss: 0.050505	Acc: 21.6% (2155/10000)
[Test]  Epoch: 57	Loss: 0.050517	Acc: 21.4% (2137/10000)
[Test]  Epoch: 58	Loss: 0.050385	Acc: 21.5% (2153/10000)
[Test]  Epoch: 59	Loss: 0.050355	Acc: 21.4% (2143/10000)
[Test]  Epoch: 60	Loss: 0.050589	Acc: 21.0% (2102/10000)
[Test]  Epoch: 61	Loss: 0.050546	Acc: 21.4% (2135/10000)
[Test]  Epoch: 62	Loss: 0.050441	Acc: 21.4% (2135/10000)
[Test]  Epoch: 63	Loss: 0.050452	Acc: 21.2% (2124/10000)
[Test]  Epoch: 64	Loss: 0.050478	Acc: 21.4% (2138/10000)
[Test]  Epoch: 65	Loss: 0.050306	Acc: 21.4% (2145/10000)
[Test]  Epoch: 66	Loss: 0.050283	Acc: 21.4% (2138/10000)
[Test]  Epoch: 67	Loss: 0.050320	Acc: 21.4% (2139/10000)
[Test]  Epoch: 68	Loss: 0.050334	Acc: 21.2% (2121/10000)
[Test]  Epoch: 69	Loss: 0.050293	Acc: 21.5% (2148/10000)
[Test]  Epoch: 70	Loss: 0.050295	Acc: 21.4% (2142/10000)
[Test]  Epoch: 71	Loss: 0.050236	Acc: 21.7% (2173/10000)
[Test]  Epoch: 72	Loss: 0.050284	Acc: 21.5% (2149/10000)
[Test]  Epoch: 73	Loss: 0.050247	Acc: 21.6% (2163/10000)
[Test]  Epoch: 74	Loss: 0.050183	Acc: 21.5% (2148/10000)
[Test]  Epoch: 75	Loss: 0.050212	Acc: 21.5% (2153/10000)
[Test]  Epoch: 76	Loss: 0.050230	Acc: 21.5% (2151/10000)
[Test]  Epoch: 77	Loss: 0.050141	Acc: 21.5% (2147/10000)
[Test]  Epoch: 78	Loss: 0.050238	Acc: 21.5% (2147/10000)
[Test]  Epoch: 79	Loss: 0.050267	Acc: 21.5% (2150/10000)
[Test]  Epoch: 80	Loss: 0.050239	Acc: 21.4% (2142/10000)
[Test]  Epoch: 81	Loss: 0.050114	Acc: 21.5% (2154/10000)
[Test]  Epoch: 82	Loss: 0.050101	Acc: 21.5% (2152/10000)
[Test]  Epoch: 83	Loss: 0.050114	Acc: 21.3% (2130/10000)
[Test]  Epoch: 84	Loss: 0.050276	Acc: 21.4% (2145/10000)
[Test]  Epoch: 85	Loss: 0.050180	Acc: 21.3% (2131/10000)
[Test]  Epoch: 86	Loss: 0.050265	Acc: 21.5% (2146/10000)
[Test]  Epoch: 87	Loss: 0.050123	Acc: 21.6% (2156/10000)
[Test]  Epoch: 88	Loss: 0.050238	Acc: 21.6% (2162/10000)
[Test]  Epoch: 89	Loss: 0.050142	Acc: 21.5% (2154/10000)
[Test]  Epoch: 90	Loss: 0.050195	Acc: 21.6% (2165/10000)
[Test]  Epoch: 91	Loss: 0.050202	Acc: 21.6% (2161/10000)
[Test]  Epoch: 92	Loss: 0.050131	Acc: 21.7% (2172/10000)
[Test]  Epoch: 93	Loss: 0.050155	Acc: 21.6% (2158/10000)
[Test]  Epoch: 94	Loss: 0.050091	Acc: 21.5% (2154/10000)
[Test]  Epoch: 95	Loss: 0.050209	Acc: 21.6% (2165/10000)
[Test]  Epoch: 96	Loss: 0.050111	Acc: 21.8% (2179/10000)
[Test]  Epoch: 97	Loss: 0.050169	Acc: 21.6% (2163/10000)
[Test]  Epoch: 98	Loss: 0.050148	Acc: 21.7% (2169/10000)
[Test]  Epoch: 99	Loss: 0.050180	Acc: 21.6% (2161/10000)
[Test]  Epoch: 100	Loss: 0.050237	Acc: 21.4% (2144/10000)
===========finish==========
['2024-08-19', '03:24:46.293308', '100', 'test', '0.05023709354400635', '21.44', '21.79']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.167898	Acc: 2.7% (270/10000)
[Test]  Epoch: 2	Loss: 0.064024	Acc: 13.9% (1389/10000)
[Test]  Epoch: 3	Loss: 0.057438	Acc: 16.0% (1603/10000)
[Test]  Epoch: 4	Loss: 0.055889	Acc: 16.9% (1686/10000)
[Test]  Epoch: 5	Loss: 0.055609	Acc: 16.1% (1605/10000)
[Test]  Epoch: 6	Loss: 0.055066	Acc: 17.8% (1782/10000)
[Test]  Epoch: 7	Loss: 0.054534	Acc: 17.8% (1782/10000)
[Test]  Epoch: 8	Loss: 0.054240	Acc: 18.4% (1838/10000)
[Test]  Epoch: 9	Loss: 0.054033	Acc: 18.4% (1838/10000)
[Test]  Epoch: 10	Loss: 0.053756	Acc: 18.9% (1893/10000)
[Test]  Epoch: 11	Loss: 0.053532	Acc: 18.8% (1880/10000)
[Test]  Epoch: 12	Loss: 0.053541	Acc: 19.1% (1907/10000)
[Test]  Epoch: 13	Loss: 0.053468	Acc: 19.1% (1909/10000)
[Test]  Epoch: 14	Loss: 0.053364	Acc: 19.1% (1910/10000)
[Test]  Epoch: 15	Loss: 0.053367	Acc: 19.3% (1930/10000)
[Test]  Epoch: 16	Loss: 0.053093	Acc: 19.3% (1926/10000)
[Test]  Epoch: 17	Loss: 0.053180	Acc: 19.5% (1951/10000)
[Test]  Epoch: 18	Loss: 0.052982	Acc: 19.6% (1958/10000)
[Test]  Epoch: 19	Loss: 0.053010	Acc: 19.3% (1934/10000)
[Test]  Epoch: 20	Loss: 0.052723	Acc: 19.5% (1947/10000)
[Test]  Epoch: 21	Loss: 0.052720	Acc: 19.4% (1935/10000)
[Test]  Epoch: 22	Loss: 0.052537	Acc: 19.9% (1991/10000)
[Test]  Epoch: 23	Loss: 0.052282	Acc: 20.1% (2010/10000)
[Test]  Epoch: 24	Loss: 0.052315	Acc: 20.1% (2014/10000)
[Test]  Epoch: 25	Loss: 0.051999	Acc: 20.4% (2038/10000)
[Test]  Epoch: 26	Loss: 0.051939	Acc: 20.4% (2043/10000)
[Test]  Epoch: 27	Loss: 0.051859	Acc: 20.2% (2025/10000)
[Test]  Epoch: 28	Loss: 0.052000	Acc: 20.2% (2019/10000)
[Test]  Epoch: 29	Loss: 0.051725	Acc: 20.5% (2049/10000)
[Test]  Epoch: 30	Loss: 0.051707	Acc: 20.7% (2070/10000)
[Test]  Epoch: 31	Loss: 0.051606	Acc: 20.5% (2052/10000)
[Test]  Epoch: 32	Loss: 0.051576	Acc: 20.5% (2052/10000)
[Test]  Epoch: 33	Loss: 0.051606	Acc: 20.7% (2070/10000)
[Test]  Epoch: 34	Loss: 0.051484	Acc: 20.6% (2058/10000)
[Test]  Epoch: 35	Loss: 0.051419	Acc: 21.1% (2108/10000)
[Test]  Epoch: 36	Loss: 0.051540	Acc: 20.7% (2070/10000)
[Test]  Epoch: 37	Loss: 0.051422	Acc: 20.8% (2078/10000)
[Test]  Epoch: 38	Loss: 0.051313	Acc: 21.1% (2112/10000)
[Test]  Epoch: 39	Loss: 0.051345	Acc: 21.0% (2102/10000)
[Test]  Epoch: 40	Loss: 0.051273	Acc: 21.1% (2107/10000)
[Test]  Epoch: 41	Loss: 0.051051	Acc: 21.0% (2103/10000)
[Test]  Epoch: 42	Loss: 0.051312	Acc: 20.6% (2064/10000)
[Test]  Epoch: 43	Loss: 0.051331	Acc: 20.7% (2072/10000)
[Test]  Epoch: 44	Loss: 0.051008	Acc: 21.2% (2123/10000)
[Test]  Epoch: 45	Loss: 0.050716	Acc: 21.3% (2131/10000)
[Test]  Epoch: 46	Loss: 0.050926	Acc: 20.8% (2082/10000)
[Test]  Epoch: 47	Loss: 0.051096	Acc: 20.7% (2073/10000)
[Test]  Epoch: 48	Loss: 0.050740	Acc: 20.9% (2092/10000)
[Test]  Epoch: 49	Loss: 0.050714	Acc: 21.1% (2109/10000)
[Test]  Epoch: 50	Loss: 0.050760	Acc: 21.0% (2102/10000)
[Test]  Epoch: 51	Loss: 0.050857	Acc: 20.9% (2093/10000)
[Test]  Epoch: 52	Loss: 0.050799	Acc: 20.8% (2082/10000)
[Test]  Epoch: 53	Loss: 0.050749	Acc: 21.3% (2132/10000)
[Test]  Epoch: 54	Loss: 0.050568	Acc: 21.0% (2103/10000)
[Test]  Epoch: 55	Loss: 0.050446	Acc: 21.4% (2135/10000)
[Test]  Epoch: 56	Loss: 0.050650	Acc: 20.9% (2092/10000)
[Test]  Epoch: 57	Loss: 0.050487	Acc: 21.0% (2102/10000)
[Test]  Epoch: 58	Loss: 0.050543	Acc: 20.9% (2094/10000)
[Test]  Epoch: 59	Loss: 0.050470	Acc: 21.3% (2126/10000)
[Test]  Epoch: 60	Loss: 0.050408	Acc: 21.2% (2122/10000)
[Test]  Epoch: 61	Loss: 0.050448	Acc: 21.3% (2131/10000)
[Test]  Epoch: 62	Loss: 0.050356	Acc: 21.5% (2147/10000)
[Test]  Epoch: 63	Loss: 0.050385	Acc: 21.6% (2159/10000)
[Test]  Epoch: 64	Loss: 0.050375	Acc: 21.5% (2153/10000)
[Test]  Epoch: 65	Loss: 0.050254	Acc: 21.9% (2188/10000)
[Test]  Epoch: 66	Loss: 0.050246	Acc: 21.6% (2157/10000)
[Test]  Epoch: 67	Loss: 0.050338	Acc: 21.7% (2166/10000)
[Test]  Epoch: 68	Loss: 0.050331	Acc: 21.6% (2155/10000)
[Test]  Epoch: 69	Loss: 0.050309	Acc: 21.7% (2169/10000)
[Test]  Epoch: 70	Loss: 0.050314	Acc: 21.9% (2186/10000)
[Test]  Epoch: 71	Loss: 0.050280	Acc: 21.7% (2167/10000)
[Test]  Epoch: 72	Loss: 0.050323	Acc: 21.7% (2171/10000)
[Test]  Epoch: 73	Loss: 0.050305	Acc: 21.8% (2182/10000)
[Test]  Epoch: 74	Loss: 0.050241	Acc: 21.8% (2181/10000)
[Test]  Epoch: 75	Loss: 0.050267	Acc: 21.7% (2171/10000)
[Test]  Epoch: 76	Loss: 0.050323	Acc: 21.6% (2162/10000)
[Test]  Epoch: 77	Loss: 0.050290	Acc: 21.6% (2157/10000)
[Test]  Epoch: 78	Loss: 0.050359	Acc: 21.4% (2144/10000)
[Test]  Epoch: 79	Loss: 0.050352	Acc: 21.5% (2146/10000)
[Test]  Epoch: 80	Loss: 0.050332	Acc: 21.5% (2154/10000)
[Test]  Epoch: 81	Loss: 0.050209	Acc: 21.6% (2161/10000)
[Test]  Epoch: 82	Loss: 0.050233	Acc: 21.6% (2162/10000)
[Test]  Epoch: 83	Loss: 0.050274	Acc: 21.5% (2152/10000)
[Test]  Epoch: 84	Loss: 0.050361	Acc: 21.5% (2153/10000)
[Test]  Epoch: 85	Loss: 0.050222	Acc: 21.7% (2170/10000)
[Test]  Epoch: 86	Loss: 0.050316	Acc: 21.4% (2140/10000)
[Test]  Epoch: 87	Loss: 0.050310	Acc: 21.4% (2145/10000)
[Test]  Epoch: 88	Loss: 0.050343	Acc: 21.4% (2137/10000)
[Test]  Epoch: 89	Loss: 0.050245	Acc: 21.4% (2137/10000)
[Test]  Epoch: 90	Loss: 0.050293	Acc: 21.5% (2150/10000)
[Test]  Epoch: 91	Loss: 0.050294	Acc: 21.6% (2163/10000)
[Test]  Epoch: 92	Loss: 0.050295	Acc: 21.5% (2149/10000)
[Test]  Epoch: 93	Loss: 0.050268	Acc: 21.4% (2143/10000)
[Test]  Epoch: 94	Loss: 0.050180	Acc: 21.4% (2145/10000)
[Test]  Epoch: 95	Loss: 0.050399	Acc: 21.3% (2127/10000)
[Test]  Epoch: 96	Loss: 0.050258	Acc: 21.4% (2139/10000)
[Test]  Epoch: 97	Loss: 0.050265	Acc: 21.4% (2135/10000)
[Test]  Epoch: 98	Loss: 0.050231	Acc: 21.3% (2132/10000)
[Test]  Epoch: 99	Loss: 0.050296	Acc: 21.2% (2125/10000)
[Test]  Epoch: 100	Loss: 0.050291	Acc: 21.4% (2135/10000)
===========finish==========
['2024-08-19', '03:28:39.428915', '100', 'test', '0.05029131426811218', '21.35', '21.88']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.442350	Acc: 1.0% (98/10000)
[Test]  Epoch: 2	Loss: 0.064437	Acc: 10.3% (1026/10000)
[Test]  Epoch: 3	Loss: 0.058003	Acc: 14.2% (1417/10000)
[Test]  Epoch: 4	Loss: 0.057342	Acc: 14.4% (1441/10000)
[Test]  Epoch: 5	Loss: 0.055779	Acc: 16.3% (1626/10000)
[Test]  Epoch: 6	Loss: 0.055256	Acc: 16.2% (1617/10000)
[Test]  Epoch: 7	Loss: 0.055264	Acc: 16.3% (1631/10000)
[Test]  Epoch: 8	Loss: 0.055945	Acc: 16.3% (1631/10000)
[Test]  Epoch: 9	Loss: 0.054752	Acc: 17.7% (1773/10000)
[Test]  Epoch: 10	Loss: 0.054510	Acc: 17.3% (1734/10000)
[Test]  Epoch: 11	Loss: 0.054723	Acc: 17.5% (1748/10000)
[Test]  Epoch: 12	Loss: 0.054201	Acc: 17.8% (1776/10000)
[Test]  Epoch: 13	Loss: 0.054341	Acc: 17.7% (1772/10000)
[Test]  Epoch: 14	Loss: 0.054215	Acc: 18.3% (1826/10000)
[Test]  Epoch: 15	Loss: 0.054438	Acc: 18.0% (1803/10000)
[Test]  Epoch: 16	Loss: 0.053777	Acc: 18.4% (1842/10000)
[Test]  Epoch: 17	Loss: 0.053721	Acc: 18.3% (1829/10000)
[Test]  Epoch: 18	Loss: 0.053679	Acc: 18.7% (1872/10000)
[Test]  Epoch: 19	Loss: 0.053544	Acc: 19.1% (1912/10000)
[Test]  Epoch: 20	Loss: 0.053627	Acc: 18.3% (1830/10000)
[Test]  Epoch: 21	Loss: 0.053434	Acc: 18.8% (1883/10000)
[Test]  Epoch: 22	Loss: 0.053164	Acc: 19.0% (1903/10000)
[Test]  Epoch: 23	Loss: 0.053170	Acc: 18.9% (1895/10000)
[Test]  Epoch: 24	Loss: 0.053047	Acc: 18.8% (1882/10000)
[Test]  Epoch: 25	Loss: 0.053027	Acc: 19.1% (1908/10000)
[Test]  Epoch: 26	Loss: 0.052707	Acc: 19.2% (1919/10000)
[Test]  Epoch: 27	Loss: 0.053074	Acc: 18.6% (1855/10000)
[Test]  Epoch: 28	Loss: 0.052762	Acc: 19.3% (1930/10000)
[Test]  Epoch: 29	Loss: 0.052742	Acc: 19.3% (1933/10000)
[Test]  Epoch: 30	Loss: 0.052877	Acc: 18.6% (1864/10000)
[Test]  Epoch: 31	Loss: 0.052552	Acc: 19.4% (1938/10000)
[Test]  Epoch: 32	Loss: 0.052792	Acc: 19.4% (1944/10000)
[Test]  Epoch: 33	Loss: 0.052570	Acc: 19.3% (1934/10000)
[Test]  Epoch: 34	Loss: 0.052685	Acc: 19.1% (1914/10000)
[Test]  Epoch: 35	Loss: 0.052616	Acc: 19.2% (1922/10000)
[Test]  Epoch: 36	Loss: 0.052581	Acc: 19.1% (1907/10000)
[Test]  Epoch: 37	Loss: 0.052088	Acc: 19.7% (1973/10000)
[Test]  Epoch: 38	Loss: 0.052157	Acc: 19.3% (1931/10000)
[Test]  Epoch: 39	Loss: 0.052323	Acc: 19.2% (1923/10000)
[Test]  Epoch: 40	Loss: 0.052337	Acc: 19.1% (1910/10000)
[Test]  Epoch: 41	Loss: 0.052028	Acc: 19.5% (1953/10000)
[Test]  Epoch: 42	Loss: 0.052045	Acc: 19.7% (1971/10000)
[Test]  Epoch: 43	Loss: 0.051896	Acc: 19.8% (1980/10000)
[Test]  Epoch: 44	Loss: 0.051925	Acc: 19.7% (1968/10000)
[Test]  Epoch: 45	Loss: 0.051847	Acc: 19.2% (1925/10000)
[Test]  Epoch: 46	Loss: 0.051542	Acc: 20.1% (2010/10000)
[Test]  Epoch: 47	Loss: 0.051707	Acc: 20.0% (2001/10000)
[Test]  Epoch: 48	Loss: 0.051501	Acc: 19.9% (1991/10000)
[Test]  Epoch: 49	Loss: 0.051504	Acc: 20.1% (2011/10000)
[Test]  Epoch: 50	Loss: 0.051502	Acc: 20.1% (2010/10000)
[Test]  Epoch: 51	Loss: 0.051570	Acc: 20.1% (2012/10000)
[Test]  Epoch: 52	Loss: 0.051583	Acc: 20.1% (2007/10000)
[Test]  Epoch: 53	Loss: 0.051439	Acc: 20.2% (2021/10000)
[Test]  Epoch: 54	Loss: 0.051604	Acc: 19.9% (1993/10000)
[Test]  Epoch: 55	Loss: 0.051452	Acc: 20.2% (2020/10000)
[Test]  Epoch: 56	Loss: 0.051367	Acc: 20.3% (2030/10000)
[Test]  Epoch: 57	Loss: 0.051354	Acc: 20.1% (2013/10000)
[Test]  Epoch: 58	Loss: 0.051250	Acc: 20.5% (2047/10000)
[Test]  Epoch: 59	Loss: 0.051344	Acc: 20.2% (2025/10000)
[Test]  Epoch: 60	Loss: 0.051562	Acc: 20.2% (2019/10000)
[Test]  Epoch: 61	Loss: 0.051428	Acc: 20.3% (2028/10000)
[Test]  Epoch: 62	Loss: 0.051284	Acc: 20.4% (2042/10000)
[Test]  Epoch: 63	Loss: 0.051354	Acc: 20.3% (2031/10000)
[Test]  Epoch: 64	Loss: 0.051262	Acc: 20.2% (2018/10000)
[Test]  Epoch: 65	Loss: 0.051122	Acc: 20.5% (2048/10000)
[Test]  Epoch: 66	Loss: 0.051133	Acc: 20.6% (2064/10000)
[Test]  Epoch: 67	Loss: 0.051161	Acc: 20.7% (2066/10000)
[Test]  Epoch: 68	Loss: 0.051143	Acc: 20.7% (2074/10000)
[Test]  Epoch: 69	Loss: 0.051108	Acc: 20.4% (2042/10000)
[Test]  Epoch: 70	Loss: 0.051261	Acc: 20.5% (2051/10000)
[Test]  Epoch: 71	Loss: 0.051077	Acc: 20.5% (2054/10000)
[Test]  Epoch: 72	Loss: 0.051132	Acc: 20.7% (2069/10000)
[Test]  Epoch: 73	Loss: 0.051106	Acc: 20.5% (2050/10000)
[Test]  Epoch: 74	Loss: 0.051090	Acc: 20.4% (2045/10000)
[Test]  Epoch: 75	Loss: 0.051079	Acc: 20.6% (2063/10000)
[Test]  Epoch: 76	Loss: 0.051067	Acc: 20.7% (2066/10000)
[Test]  Epoch: 77	Loss: 0.051075	Acc: 20.4% (2042/10000)
[Test]  Epoch: 78	Loss: 0.051161	Acc: 20.5% (2053/10000)
[Test]  Epoch: 79	Loss: 0.051246	Acc: 20.3% (2029/10000)
[Test]  Epoch: 80	Loss: 0.051198	Acc: 20.4% (2044/10000)
[Test]  Epoch: 81	Loss: 0.050985	Acc: 20.6% (2065/10000)
[Test]  Epoch: 82	Loss: 0.051027	Acc: 20.6% (2063/10000)
[Test]  Epoch: 83	Loss: 0.051123	Acc: 20.5% (2051/10000)
[Test]  Epoch: 84	Loss: 0.051103	Acc: 20.6% (2058/10000)
[Test]  Epoch: 85	Loss: 0.051026	Acc: 20.7% (2069/10000)
[Test]  Epoch: 86	Loss: 0.051184	Acc: 20.5% (2050/10000)
[Test]  Epoch: 87	Loss: 0.051186	Acc: 20.3% (2031/10000)
[Test]  Epoch: 88	Loss: 0.051100	Acc: 20.4% (2037/10000)
[Test]  Epoch: 89	Loss: 0.051151	Acc: 20.7% (2070/10000)
[Test]  Epoch: 90	Loss: 0.051089	Acc: 20.6% (2064/10000)
[Test]  Epoch: 91	Loss: 0.051091	Acc: 20.4% (2044/10000)
[Test]  Epoch: 92	Loss: 0.051116	Acc: 20.6% (2061/10000)
[Test]  Epoch: 93	Loss: 0.051178	Acc: 20.4% (2043/10000)
[Test]  Epoch: 94	Loss: 0.050949	Acc: 20.9% (2085/10000)
[Test]  Epoch: 95	Loss: 0.051217	Acc: 20.3% (2031/10000)
[Test]  Epoch: 96	Loss: 0.051122	Acc: 20.5% (2047/10000)
[Test]  Epoch: 97	Loss: 0.051009	Acc: 20.6% (2064/10000)
[Test]  Epoch: 98	Loss: 0.051043	Acc: 20.2% (2025/10000)
[Test]  Epoch: 99	Loss: 0.051102	Acc: 20.5% (2047/10000)
[Test]  Epoch: 100	Loss: 0.051130	Acc: 20.5% (2053/10000)
===========finish==========
['2024-08-19', '03:32:44.573103', '100', 'test', '0.051130397772789', '20.53', '20.85']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.15.conv.2.weight', '_features.15.conv.3.weight', '_features.16.conv.0.0.weight', '_features.16.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.349036	Acc: 1.4% (137/10000)
[Test]  Epoch: 2	Loss: 0.069493	Acc: 9.0% (896/10000)
[Test]  Epoch: 3	Loss: 0.062702	Acc: 11.3% (1131/10000)
[Test]  Epoch: 4	Loss: 0.058179	Acc: 14.0% (1397/10000)
[Test]  Epoch: 5	Loss: 0.057460	Acc: 14.8% (1478/10000)
[Test]  Epoch: 6	Loss: 0.057543	Acc: 14.2% (1423/10000)
[Test]  Epoch: 7	Loss: 0.056474	Acc: 15.5% (1551/10000)
[Test]  Epoch: 8	Loss: 0.056439	Acc: 15.3% (1529/10000)
[Test]  Epoch: 9	Loss: 0.055872	Acc: 16.2% (1619/10000)
[Test]  Epoch: 10	Loss: 0.055778	Acc: 15.9% (1592/10000)
[Test]  Epoch: 11	Loss: 0.055866	Acc: 16.5% (1647/10000)
[Test]  Epoch: 12	Loss: 0.055638	Acc: 16.4% (1645/10000)
[Test]  Epoch: 13	Loss: 0.056232	Acc: 16.2% (1620/10000)
[Test]  Epoch: 14	Loss: 0.055651	Acc: 16.5% (1653/10000)
[Test]  Epoch: 15	Loss: 0.055075	Acc: 16.9% (1695/10000)
[Test]  Epoch: 16	Loss: 0.055227	Acc: 16.8% (1677/10000)
[Test]  Epoch: 17	Loss: 0.054780	Acc: 17.3% (1730/10000)
[Test]  Epoch: 18	Loss: 0.054782	Acc: 17.3% (1727/10000)
[Test]  Epoch: 19	Loss: 0.054989	Acc: 17.2% (1721/10000)
[Test]  Epoch: 20	Loss: 0.054891	Acc: 17.1% (1706/10000)
[Test]  Epoch: 21	Loss: 0.054377	Acc: 17.3% (1734/10000)
[Test]  Epoch: 22	Loss: 0.054156	Acc: 17.6% (1756/10000)
[Test]  Epoch: 23	Loss: 0.054353	Acc: 17.6% (1762/10000)
[Test]  Epoch: 24	Loss: 0.054225	Acc: 17.8% (1780/10000)
[Test]  Epoch: 25	Loss: 0.053897	Acc: 18.3% (1832/10000)
[Test]  Epoch: 26	Loss: 0.054055	Acc: 18.0% (1803/10000)
[Test]  Epoch: 27	Loss: 0.054000	Acc: 18.2% (1821/10000)
[Test]  Epoch: 28	Loss: 0.053738	Acc: 18.4% (1840/10000)
[Test]  Epoch: 29	Loss: 0.053994	Acc: 17.9% (1795/10000)
[Test]  Epoch: 30	Loss: 0.053706	Acc: 18.2% (1817/10000)
[Test]  Epoch: 31	Loss: 0.053745	Acc: 18.2% (1817/10000)
[Test]  Epoch: 32	Loss: 0.053667	Acc: 18.3% (1828/10000)
[Test]  Epoch: 33	Loss: 0.053685	Acc: 18.4% (1835/10000)
[Test]  Epoch: 34	Loss: 0.053564	Acc: 17.9% (1785/10000)
[Test]  Epoch: 35	Loss: 0.053239	Acc: 18.8% (1880/10000)
[Test]  Epoch: 36	Loss: 0.053369	Acc: 18.5% (1852/10000)
[Test]  Epoch: 37	Loss: 0.053188	Acc: 18.8% (1878/10000)
[Test]  Epoch: 38	Loss: 0.053050	Acc: 19.0% (1899/10000)
[Test]  Epoch: 39	Loss: 0.052889	Acc: 19.0% (1904/10000)
[Test]  Epoch: 40	Loss: 0.053002	Acc: 19.1% (1910/10000)
[Test]  Epoch: 41	Loss: 0.052875	Acc: 19.2% (1921/10000)
[Test]  Epoch: 42	Loss: 0.052763	Acc: 18.9% (1890/10000)
[Test]  Epoch: 43	Loss: 0.052986	Acc: 19.1% (1907/10000)
[Test]  Epoch: 44	Loss: 0.052950	Acc: 18.9% (1891/10000)
[Test]  Epoch: 45	Loss: 0.052971	Acc: 19.0% (1896/10000)
[Test]  Epoch: 46	Loss: 0.052525	Acc: 19.2% (1924/10000)
[Test]  Epoch: 47	Loss: 0.052563	Acc: 19.3% (1928/10000)
[Test]  Epoch: 48	Loss: 0.052298	Acc: 19.5% (1953/10000)
[Test]  Epoch: 49	Loss: 0.052653	Acc: 18.9% (1890/10000)
[Test]  Epoch: 50	Loss: 0.052452	Acc: 19.1% (1911/10000)
[Test]  Epoch: 51	Loss: 0.052560	Acc: 19.4% (1937/10000)
[Test]  Epoch: 52	Loss: 0.052217	Acc: 19.3% (1931/10000)
[Test]  Epoch: 53	Loss: 0.052718	Acc: 18.7% (1868/10000)
[Test]  Epoch: 54	Loss: 0.052555	Acc: 19.3% (1934/10000)
[Test]  Epoch: 55	Loss: 0.052199	Acc: 19.4% (1938/10000)
[Test]  Epoch: 56	Loss: 0.052370	Acc: 19.4% (1935/10000)
[Test]  Epoch: 57	Loss: 0.052244	Acc: 19.1% (1915/10000)
[Test]  Epoch: 58	Loss: 0.052323	Acc: 19.2% (1925/10000)
[Test]  Epoch: 59	Loss: 0.052528	Acc: 19.1% (1911/10000)
[Test]  Epoch: 60	Loss: 0.052424	Acc: 19.4% (1937/10000)
[Test]  Epoch: 61	Loss: 0.052254	Acc: 19.4% (1936/10000)
[Test]  Epoch: 62	Loss: 0.052104	Acc: 19.5% (1950/10000)
[Test]  Epoch: 63	Loss: 0.052101	Acc: 19.6% (1963/10000)
[Test]  Epoch: 64	Loss: 0.052056	Acc: 19.5% (1954/10000)
[Test]  Epoch: 65	Loss: 0.051935	Acc: 19.6% (1958/10000)
[Test]  Epoch: 66	Loss: 0.052003	Acc: 19.5% (1947/10000)
[Test]  Epoch: 67	Loss: 0.052037	Acc: 19.5% (1954/10000)
[Test]  Epoch: 68	Loss: 0.051986	Acc: 19.7% (1967/10000)
[Test]  Epoch: 69	Loss: 0.052017	Acc: 19.8% (1976/10000)
[Test]  Epoch: 70	Loss: 0.052052	Acc: 19.7% (1966/10000)
[Test]  Epoch: 71	Loss: 0.051897	Acc: 19.8% (1977/10000)
[Test]  Epoch: 72	Loss: 0.051924	Acc: 19.6% (1960/10000)
[Test]  Epoch: 73	Loss: 0.051934	Acc: 19.6% (1957/10000)
[Test]  Epoch: 74	Loss: 0.051874	Acc: 19.8% (1975/10000)
[Test]  Epoch: 75	Loss: 0.051838	Acc: 19.7% (1972/10000)
[Test]  Epoch: 76	Loss: 0.051882	Acc: 19.7% (1974/10000)
[Test]  Epoch: 77	Loss: 0.051844	Acc: 19.8% (1979/10000)
[Test]  Epoch: 78	Loss: 0.051940	Acc: 19.8% (1975/10000)
[Test]  Epoch: 79	Loss: 0.052056	Acc: 19.6% (1963/10000)
[Test]  Epoch: 80	Loss: 0.051970	Acc: 19.7% (1966/10000)
[Test]  Epoch: 81	Loss: 0.051900	Acc: 19.7% (1970/10000)
[Test]  Epoch: 82	Loss: 0.051851	Acc: 19.8% (1979/10000)
[Test]  Epoch: 83	Loss: 0.051905	Acc: 19.6% (1964/10000)
[Test]  Epoch: 84	Loss: 0.051929	Acc: 19.7% (1968/10000)
[Test]  Epoch: 85	Loss: 0.051836	Acc: 19.8% (1984/10000)
[Test]  Epoch: 86	Loss: 0.051979	Acc: 19.6% (1962/10000)
[Test]  Epoch: 87	Loss: 0.051990	Acc: 19.7% (1967/10000)
[Test]  Epoch: 88	Loss: 0.051945	Acc: 19.4% (1942/10000)
[Test]  Epoch: 89	Loss: 0.052037	Acc: 19.5% (1953/10000)
[Test]  Epoch: 90	Loss: 0.051947	Acc: 19.6% (1962/10000)
[Test]  Epoch: 91	Loss: 0.051948	Acc: 19.7% (1971/10000)
[Test]  Epoch: 92	Loss: 0.051921	Acc: 19.9% (1986/10000)
[Test]  Epoch: 93	Loss: 0.051951	Acc: 19.6% (1964/10000)
[Test]  Epoch: 94	Loss: 0.051757	Acc: 19.8% (1979/10000)
[Test]  Epoch: 95	Loss: 0.052034	Acc: 19.6% (1958/10000)
[Test]  Epoch: 96	Loss: 0.051895	Acc: 19.6% (1958/10000)
[Test]  Epoch: 97	Loss: 0.051851	Acc: 19.8% (1982/10000)
[Test]  Epoch: 98	Loss: 0.051833	Acc: 19.5% (1950/10000)
[Test]  Epoch: 99	Loss: 0.051984	Acc: 19.4% (1942/10000)
[Test]  Epoch: 100	Loss: 0.051890	Acc: 19.6% (1957/10000)
===========finish==========
['2024-08-19', '03:36:46.580561', '100', 'test', '0.05188952186107636', '19.57', '19.86']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.15.conv.2.weight', '_features.15.conv.3.weight', '_features.16.conv.0.0.weight', '_features.16.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.2.weight', '_features.16.conv.3.weight', '_features.17.conv.0.0.weight', '_features.17.conv.0.1.weight', '_features.17.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.17.conv.2.weight', '_features.17.conv.3.weight', '_features.18.0.weight', '_features.18.1.weight', 'last_linear.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.090840	Acc: 1.6% (157/10000)
[Test]  Epoch: 2	Loss: 0.068118	Acc: 5.5% (545/10000)
[Test]  Epoch: 3	Loss: 0.065015	Acc: 8.3% (833/10000)
[Test]  Epoch: 4	Loss: 0.063174	Acc: 9.1% (914/10000)
[Test]  Epoch: 5	Loss: 0.061629	Acc: 10.0% (1002/10000)
[Test]  Epoch: 6	Loss: 0.060760	Acc: 11.3% (1133/10000)
[Test]  Epoch: 7	Loss: 0.059165	Acc: 12.4% (1245/10000)
[Test]  Epoch: 8	Loss: 0.058217	Acc: 13.2% (1317/10000)
[Test]  Epoch: 9	Loss: 0.057735	Acc: 13.8% (1385/10000)
[Test]  Epoch: 10	Loss: 0.057879	Acc: 13.6% (1357/10000)
[Test]  Epoch: 11	Loss: 0.056687	Acc: 14.7% (1465/10000)
[Test]  Epoch: 12	Loss: 0.056002	Acc: 14.7% (1469/10000)
[Test]  Epoch: 13	Loss: 0.057353	Acc: 13.4% (1343/10000)
[Test]  Epoch: 14	Loss: 0.056065	Acc: 14.5% (1454/10000)
[Test]  Epoch: 15	Loss: 0.055471	Acc: 15.2% (1524/10000)
[Test]  Epoch: 16	Loss: 0.055535	Acc: 15.1% (1512/10000)
[Test]  Epoch: 17	Loss: 0.054989	Acc: 15.8% (1576/10000)
[Test]  Epoch: 18	Loss: 0.055260	Acc: 15.8% (1575/10000)
[Test]  Epoch: 19	Loss: 0.054887	Acc: 16.1% (1606/10000)
[Test]  Epoch: 20	Loss: 0.054413	Acc: 16.6% (1662/10000)
[Test]  Epoch: 21	Loss: 0.054713	Acc: 16.3% (1629/10000)
[Test]  Epoch: 22	Loss: 0.054124	Acc: 16.8% (1684/10000)
[Test]  Epoch: 23	Loss: 0.054309	Acc: 16.8% (1679/10000)
[Test]  Epoch: 24	Loss: 0.054470	Acc: 16.4% (1645/10000)
[Test]  Epoch: 25	Loss: 0.054015	Acc: 17.5% (1749/10000)
[Test]  Epoch: 26	Loss: 0.054105	Acc: 17.2% (1716/10000)
[Test]  Epoch: 27	Loss: 0.054242	Acc: 16.8% (1678/10000)
[Test]  Epoch: 28	Loss: 0.053900	Acc: 17.1% (1708/10000)
[Test]  Epoch: 29	Loss: 0.054070	Acc: 17.1% (1711/10000)
[Test]  Epoch: 30	Loss: 0.054301	Acc: 17.2% (1720/10000)
[Test]  Epoch: 31	Loss: 0.053555	Acc: 17.7% (1774/10000)
[Test]  Epoch: 32	Loss: 0.053747	Acc: 17.4% (1745/10000)
[Test]  Epoch: 33	Loss: 0.053785	Acc: 17.0% (1701/10000)
[Test]  Epoch: 34	Loss: 0.053488	Acc: 17.5% (1751/10000)
[Test]  Epoch: 35	Loss: 0.053267	Acc: 17.6% (1765/10000)
[Test]  Epoch: 36	Loss: 0.053272	Acc: 18.0% (1797/10000)
[Test]  Epoch: 37	Loss: 0.053112	Acc: 17.7% (1769/10000)
[Test]  Epoch: 38	Loss: 0.053158	Acc: 18.1% (1805/10000)
[Test]  Epoch: 39	Loss: 0.052973	Acc: 18.2% (1824/10000)
[Test]  Epoch: 40	Loss: 0.052943	Acc: 18.2% (1816/10000)
[Test]  Epoch: 41	Loss: 0.052822	Acc: 18.3% (1831/10000)
[Test]  Epoch: 42	Loss: 0.053126	Acc: 18.0% (1796/10000)
[Test]  Epoch: 43	Loss: 0.054129	Acc: 17.4% (1742/10000)
[Test]  Epoch: 44	Loss: 0.053201	Acc: 18.3% (1831/10000)
[Test]  Epoch: 45	Loss: 0.052952	Acc: 18.2% (1818/10000)
[Test]  Epoch: 46	Loss: 0.052836	Acc: 18.2% (1817/10000)
[Test]  Epoch: 47	Loss: 0.052535	Acc: 18.8% (1877/10000)
[Test]  Epoch: 48	Loss: 0.052760	Acc: 18.7% (1867/10000)
[Test]  Epoch: 49	Loss: 0.052453	Acc: 18.7% (1867/10000)
[Test]  Epoch: 50	Loss: 0.052432	Acc: 18.3% (1832/10000)
[Test]  Epoch: 51	Loss: 0.052535	Acc: 18.7% (1867/10000)
[Test]  Epoch: 52	Loss: 0.052616	Acc: 18.3% (1832/10000)
[Test]  Epoch: 53	Loss: 0.052248	Acc: 18.6% (1865/10000)
[Test]  Epoch: 54	Loss: 0.054211	Acc: 17.2% (1723/10000)
[Test]  Epoch: 55	Loss: 0.053263	Acc: 18.1% (1811/10000)
[Test]  Epoch: 56	Loss: 0.053154	Acc: 18.4% (1843/10000)
[Test]  Epoch: 57	Loss: 0.052509	Acc: 18.7% (1868/10000)
[Test]  Epoch: 58	Loss: 0.052580	Acc: 19.0% (1902/10000)
[Test]  Epoch: 59	Loss: 0.052323	Acc: 18.9% (1895/10000)
[Test]  Epoch: 60	Loss: 0.052019	Acc: 19.2% (1918/10000)
[Test]  Epoch: 61	Loss: 0.052033	Acc: 19.2% (1922/10000)
[Test]  Epoch: 62	Loss: 0.051931	Acc: 19.7% (1968/10000)
[Test]  Epoch: 63	Loss: 0.051945	Acc: 19.4% (1943/10000)
[Test]  Epoch: 64	Loss: 0.051928	Acc: 19.5% (1947/10000)
[Test]  Epoch: 65	Loss: 0.051831	Acc: 19.7% (1972/10000)
[Test]  Epoch: 66	Loss: 0.051832	Acc: 19.5% (1951/10000)
[Test]  Epoch: 67	Loss: 0.051812	Acc: 19.6% (1961/10000)
[Test]  Epoch: 68	Loss: 0.051816	Acc: 19.7% (1966/10000)
[Test]  Epoch: 69	Loss: 0.051750	Acc: 19.8% (1981/10000)
[Test]  Epoch: 70	Loss: 0.051838	Acc: 19.8% (1977/10000)
[Test]  Epoch: 71	Loss: 0.051732	Acc: 19.8% (1977/10000)
[Test]  Epoch: 72	Loss: 0.051758	Acc: 19.7% (1967/10000)
[Test]  Epoch: 73	Loss: 0.051770	Acc: 19.9% (1993/10000)
[Test]  Epoch: 74	Loss: 0.051748	Acc: 19.6% (1960/10000)
[Test]  Epoch: 75	Loss: 0.051729	Acc: 19.5% (1946/10000)
[Test]  Epoch: 76	Loss: 0.051751	Acc: 19.8% (1980/10000)
[Test]  Epoch: 77	Loss: 0.051732	Acc: 19.6% (1963/10000)
[Test]  Epoch: 78	Loss: 0.051635	Acc: 19.7% (1966/10000)
[Test]  Epoch: 79	Loss: 0.051774	Acc: 19.5% (1949/10000)
[Test]  Epoch: 80	Loss: 0.051717	Acc: 19.6% (1958/10000)
[Test]  Epoch: 81	Loss: 0.051618	Acc: 19.8% (1978/10000)
[Test]  Epoch: 82	Loss: 0.051601	Acc: 19.9% (1994/10000)
[Test]  Epoch: 83	Loss: 0.051644	Acc: 19.7% (1968/10000)
[Test]  Epoch: 84	Loss: 0.051709	Acc: 19.6% (1959/10000)
[Test]  Epoch: 85	Loss: 0.051694	Acc: 19.7% (1970/10000)
[Test]  Epoch: 86	Loss: 0.051720	Acc: 19.9% (1989/10000)
[Test]  Epoch: 87	Loss: 0.051724	Acc: 19.8% (1978/10000)
[Test]  Epoch: 88	Loss: 0.051612	Acc: 19.9% (1991/10000)
[Test]  Epoch: 89	Loss: 0.051692	Acc: 19.8% (1982/10000)
[Test]  Epoch: 90	Loss: 0.051732	Acc: 19.8% (1979/10000)
[Test]  Epoch: 91	Loss: 0.051688	Acc: 19.7% (1968/10000)
[Test]  Epoch: 92	Loss: 0.051649	Acc: 19.7% (1973/10000)
[Test]  Epoch: 93	Loss: 0.051661	Acc: 19.4% (1943/10000)
[Test]  Epoch: 94	Loss: 0.051663	Acc: 19.7% (1967/10000)
[Test]  Epoch: 95	Loss: 0.051763	Acc: 19.6% (1962/10000)
[Test]  Epoch: 96	Loss: 0.051692	Acc: 19.8% (1976/10000)
[Test]  Epoch: 97	Loss: 0.051653	Acc: 19.7% (1972/10000)
[Test]  Epoch: 98	Loss: 0.051610	Acc: 20.0% (1999/10000)
[Test]  Epoch: 99	Loss: 0.051545	Acc: 20.0% (1999/10000)
[Test]  Epoch: 100	Loss: 0.051623	Acc: 19.9% (1990/10000)
===========finish==========
['2024-08-19', '03:40:46.799538', '100', 'test', '0.051622758269309996', '19.9', '19.99']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info does not exist. Calculating...
Load model from models/victim/cifar100-vgg16_bn/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('features.0.weight', 0.0), ('features.0.bias', 0.0), ('features.1.weight', 0.0), ('features.1.bias', 0.0), ('features.3.weight', 0.0), ('features.3.bias', 0.0), ('features.4.weight', 0.0), ('features.4.bias', 0.0), ('features.7.weight', 0.0), ('features.7.bias', 0.0), ('features.8.weight', 0.0), ('features.8.bias', 0.0), ('features.10.weight', 0.0), ('features.10.bias', 0.0), ('features.11.weight', 0.0), ('features.11.bias', 0.0), ('features.14.weight', 0.0), ('features.14.bias', 0.0), ('features.15.weight', 0.0), ('features.15.bias', 0.0), ('features.17.weight', 0.0), ('features.17.bias', 0.0), ('features.18.weight', 0.0), ('features.18.bias', 0.0), ('features.20.weight', 0.0), ('features.20.bias', 0.0), ('features.21.weight', 0.0), ('features.21.bias', 0.0), ('features.24.weight', 0.0), ('features.24.bias', 0.0), ('features.25.weight', 0.0), ('features.25.bias', 0.0), ('features.27.weight', 0.0), ('features.27.bias', 0.0), ('features.28.weight', 0.0), ('features.28.bias', 0.0), ('features.30.weight', 0.0), ('features.30.bias', 0.0), ('features.31.weight', 0.0), ('features.31.bias', 0.0), ('features.34.weight', 0.0), ('features.34.bias', 0.0), ('features.35.weight', 0.0), ('features.35.bias', 0.0), ('features.37.weight', 0.0), ('features.37.bias', 0.0), ('features.38.weight', 0.0), ('features.38.bias', 0.0), ('features.40.weight', 0.0), ('features.40.bias', 0.0), ('features.41.weight', 0.0), ('features.41.bias', 0.0), ('classifier.weight', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('features.0.weight', 0.0), ('features.0.bias', 0.0), ('features.1.weight', 0.0), ('features.1.bias', 0.0), ('features.3.weight', 0.0), ('features.3.bias', 0.0), ('features.4.weight', 0.0), ('features.4.bias', 0.0), ('features.7.weight', 0.0), ('features.7.bias', 0.0), ('features.8.weight', 0.0), ('features.8.bias', 0.0), ('features.10.weight', 0.0), ('features.10.bias', 0.0), ('features.11.weight', 0.0), ('features.11.bias', 0.0), ('features.14.weight', 0.0), ('features.14.bias', 0.0), ('features.15.weight', 0.0), ('features.15.bias', 0.0), ('features.17.weight', 0.0), ('features.17.bias', 0.0), ('features.18.weight', 0.0), ('features.18.bias', 0.0), ('features.20.weight', 0.0), ('features.20.bias', 0.0), ('features.21.weight', 0.0), ('features.21.bias', 0.0), ('features.24.weight', 0.0), ('features.24.bias', 0.0), ('features.25.weight', 0.0), ('features.25.bias', 0.0), ('features.27.weight', 0.0), ('features.27.bias', 0.0), ('features.28.weight', 0.0), ('features.28.bias', 0.0), ('features.30.weight', 0.0), ('features.30.bias', 0.0), ('features.31.weight', 0.0), ('features.31.bias', 0.0), ('features.34.weight', 0.0), ('features.34.bias', 0.0), ('features.35.weight', 0.0), ('features.35.bias', 0.0), ('features.37.weight', 0.0), ('features.37.bias', 0.0), ('features.38.weight', 0.0), ('features.38.bias', 0.0), ('features.40.weight', 0.0), ('features.40.bias', 0.0), ('features.41.weight', 0.0), ('features.41.bias', 0.0), ('classifier.weight', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
get_sample_layers n=27  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.022280	Acc: 70.9% (7093/10000)
[Test]  Epoch: 2	Loss: 0.022285	Acc: 70.9% (7088/10000)
[Test]  Epoch: 3	Loss: 0.022080	Acc: 70.8% (7085/10000)
[Test]  Epoch: 4	Loss: 0.022156	Acc: 70.7% (7070/10000)
[Test]  Epoch: 5	Loss: 0.022168	Acc: 70.9% (7089/10000)
[Test]  Epoch: 6	Loss: 0.022125	Acc: 70.9% (7087/10000)
[Test]  Epoch: 7	Loss: 0.021713	Acc: 71.0% (7098/10000)
[Test]  Epoch: 8	Loss: 0.021887	Acc: 71.0% (7103/10000)
[Test]  Epoch: 9	Loss: 0.021908	Acc: 71.3% (7131/10000)
[Test]  Epoch: 10	Loss: 0.021824	Acc: 71.0% (7101/10000)
[Test]  Epoch: 11	Loss: 0.022079	Acc: 71.0% (7097/10000)
[Test]  Epoch: 12	Loss: 0.022051	Acc: 70.6% (7056/10000)
[Test]  Epoch: 13	Loss: 0.022132	Acc: 70.5% (7045/10000)
[Test]  Epoch: 14	Loss: 0.022023	Acc: 70.7% (7073/10000)
[Test]  Epoch: 15	Loss: 0.022240	Acc: 70.6% (7056/10000)
[Test]  Epoch: 16	Loss: 0.022212	Acc: 70.7% (7065/10000)
[Test]  Epoch: 17	Loss: 0.021912	Acc: 70.8% (7078/10000)
[Test]  Epoch: 18	Loss: 0.021987	Acc: 70.7% (7073/10000)
[Test]  Epoch: 19	Loss: 0.022346	Acc: 70.7% (7073/10000)
[Test]  Epoch: 20	Loss: 0.021938	Acc: 70.9% (7094/10000)
[Test]  Epoch: 21	Loss: 0.022092	Acc: 70.3% (7028/10000)
[Test]  Epoch: 22	Loss: 0.022260	Acc: 70.6% (7063/10000)
[Test]  Epoch: 23	Loss: 0.021850	Acc: 70.4% (7041/10000)
[Test]  Epoch: 24	Loss: 0.022157	Acc: 70.8% (7078/10000)
[Test]  Epoch: 25	Loss: 0.022071	Acc: 70.4% (7036/10000)
[Test]  Epoch: 26	Loss: 0.021972	Acc: 70.7% (7066/10000)
[Test]  Epoch: 27	Loss: 0.022135	Acc: 70.7% (7069/10000)
[Test]  Epoch: 28	Loss: 0.022198	Acc: 70.5% (7051/10000)
[Test]  Epoch: 29	Loss: 0.021877	Acc: 70.3% (7030/10000)
[Test]  Epoch: 30	Loss: 0.022226	Acc: 70.7% (7067/10000)
[Test]  Epoch: 31	Loss: 0.022180	Acc: 70.5% (7047/10000)
[Test]  Epoch: 32	Loss: 0.021938	Acc: 70.8% (7077/10000)
[Test]  Epoch: 33	Loss: 0.022077	Acc: 71.0% (7100/10000)
[Test]  Epoch: 34	Loss: 0.021819	Acc: 71.1% (7113/10000)
[Test]  Epoch: 35	Loss: 0.022109	Acc: 70.7% (7071/10000)
[Test]  Epoch: 36	Loss: 0.021976	Acc: 70.7% (7070/10000)
[Test]  Epoch: 37	Loss: 0.021866	Acc: 70.7% (7071/10000)
[Test]  Epoch: 38	Loss: 0.022330	Acc: 70.0% (7000/10000)
[Test]  Epoch: 39	Loss: 0.022027	Acc: 70.5% (7048/10000)
[Test]  Epoch: 40	Loss: 0.021961	Acc: 70.8% (7077/10000)
[Test]  Epoch: 41	Loss: 0.022094	Acc: 70.5% (7050/10000)
[Test]  Epoch: 42	Loss: 0.021925	Acc: 70.4% (7039/10000)
[Test]  Epoch: 43	Loss: 0.022002	Acc: 70.3% (7026/10000)
[Test]  Epoch: 44	Loss: 0.021879	Acc: 71.0% (7102/10000)
[Test]  Epoch: 45	Loss: 0.021755	Acc: 70.8% (7085/10000)
[Test]  Epoch: 46	Loss: 0.021964	Acc: 70.8% (7080/10000)
[Test]  Epoch: 47	Loss: 0.021910	Acc: 70.5% (7054/10000)
[Test]  Epoch: 48	Loss: 0.021703	Acc: 70.7% (7071/10000)
[Test]  Epoch: 49	Loss: 0.021506	Acc: 71.2% (7117/10000)
[Test]  Epoch: 50	Loss: 0.021732	Acc: 70.5% (7055/10000)
[Test]  Epoch: 51	Loss: 0.021756	Acc: 70.9% (7091/10000)
[Test]  Epoch: 52	Loss: 0.021767	Acc: 70.6% (7057/10000)
[Test]  Epoch: 53	Loss: 0.021905	Acc: 70.2% (7018/10000)
[Test]  Epoch: 54	Loss: 0.021977	Acc: 70.7% (7066/10000)
[Test]  Epoch: 55	Loss: 0.021945	Acc: 70.7% (7074/10000)
[Test]  Epoch: 56	Loss: 0.021793	Acc: 70.6% (7056/10000)
[Test]  Epoch: 57	Loss: 0.021732	Acc: 70.7% (7065/10000)
[Test]  Epoch: 58	Loss: 0.022229	Acc: 70.5% (7053/10000)
[Test]  Epoch: 59	Loss: 0.021813	Acc: 71.2% (7116/10000)
[Test]  Epoch: 60	Loss: 0.022045	Acc: 70.5% (7053/10000)
[Test]  Epoch: 61	Loss: 0.021829	Acc: 70.4% (7044/10000)
[Test]  Epoch: 62	Loss: 0.021895	Acc: 70.6% (7062/10000)
[Test]  Epoch: 63	Loss: 0.021966	Acc: 70.6% (7057/10000)
[Test]  Epoch: 64	Loss: 0.021785	Acc: 70.5% (7045/10000)
[Test]  Epoch: 65	Loss: 0.022049	Acc: 70.4% (7040/10000)
[Test]  Epoch: 66	Loss: 0.021838	Acc: 70.7% (7074/10000)
[Test]  Epoch: 67	Loss: 0.021973	Acc: 70.9% (7092/10000)
[Test]  Epoch: 68	Loss: 0.021857	Acc: 70.9% (7094/10000)
[Test]  Epoch: 69	Loss: 0.021867	Acc: 70.7% (7069/10000)
[Test]  Epoch: 70	Loss: 0.022093	Acc: 70.4% (7038/10000)
[Test]  Epoch: 71	Loss: 0.021663	Acc: 70.2% (7022/10000)
[Test]  Epoch: 72	Loss: 0.022011	Acc: 70.8% (7078/10000)
[Test]  Epoch: 73	Loss: 0.021858	Acc: 70.6% (7059/10000)
[Test]  Epoch: 74	Loss: 0.021680	Acc: 70.9% (7091/10000)
[Test]  Epoch: 75	Loss: 0.021588	Acc: 71.3% (7132/10000)
[Test]  Epoch: 76	Loss: 0.022096	Acc: 70.0% (6996/10000)
[Test]  Epoch: 77	Loss: 0.021894	Acc: 70.6% (7061/10000)
[Test]  Epoch: 78	Loss: 0.022243	Acc: 70.1% (7010/10000)
[Test]  Epoch: 79	Loss: 0.021607	Acc: 71.0% (7098/10000)
[Test]  Epoch: 80	Loss: 0.021902	Acc: 70.6% (7056/10000)
[Test]  Epoch: 81	Loss: 0.021771	Acc: 70.5% (7049/10000)
[Test]  Epoch: 82	Loss: 0.021792	Acc: 70.8% (7082/10000)
[Test]  Epoch: 83	Loss: 0.021777	Acc: 70.8% (7082/10000)
[Test]  Epoch: 84	Loss: 0.021574	Acc: 70.8% (7085/10000)
[Test]  Epoch: 85	Loss: 0.021849	Acc: 70.9% (7090/10000)
[Test]  Epoch: 86	Loss: 0.021674	Acc: 70.6% (7059/10000)
[Test]  Epoch: 87	Loss: 0.021732	Acc: 70.5% (7054/10000)
[Test]  Epoch: 88	Loss: 0.021794	Acc: 70.7% (7071/10000)
[Test]  Epoch: 89	Loss: 0.021494	Acc: 70.7% (7068/10000)
[Test]  Epoch: 90	Loss: 0.021759	Acc: 70.8% (7078/10000)
[Test]  Epoch: 91	Loss: 0.021980	Acc: 70.6% (7064/10000)
[Test]  Epoch: 92	Loss: 0.021714	Acc: 70.7% (7073/10000)
[Test]  Epoch: 93	Loss: 0.021880	Acc: 70.7% (7072/10000)
[Test]  Epoch: 94	Loss: 0.021714	Acc: 70.8% (7083/10000)
[Test]  Epoch: 95	Loss: 0.021794	Acc: 70.4% (7038/10000)
[Test]  Epoch: 96	Loss: 0.021741	Acc: 70.7% (7069/10000)
[Test]  Epoch: 97	Loss: 0.021838	Acc: 70.6% (7063/10000)
[Test]  Epoch: 98	Loss: 0.021845	Acc: 70.5% (7048/10000)
[Test]  Epoch: 99	Loss: 0.021726	Acc: 71.1% (7111/10000)
[Test]  Epoch: 100	Loss: 0.021925	Acc: 70.5% (7052/10000)
===========finish==========
['2024-08-19', '03:47:04.676593', '100', 'test', '0.021925040543079376', '70.52', '71.32']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.0.weight', 'features.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.059317	Acc: 32.9% (3294/10000)
[Test]  Epoch: 2	Loss: 0.039110	Acc: 50.4% (5037/10000)
[Test]  Epoch: 3	Loss: 0.032782	Acc: 56.4% (5637/10000)
[Test]  Epoch: 4	Loss: 0.032155	Acc: 56.2% (5625/10000)
[Test]  Epoch: 5	Loss: 0.029927	Acc: 59.2% (5925/10000)
[Test]  Epoch: 6	Loss: 0.030106	Acc: 59.1% (5912/10000)
[Test]  Epoch: 7	Loss: 0.028594	Acc: 60.7% (6072/10000)
[Test]  Epoch: 8	Loss: 0.028724	Acc: 60.8% (6079/10000)
[Test]  Epoch: 9	Loss: 0.028972	Acc: 61.0% (6098/10000)
[Test]  Epoch: 10	Loss: 0.028184	Acc: 61.8% (6183/10000)
[Test]  Epoch: 11	Loss: 0.028033	Acc: 61.3% (6130/10000)
[Test]  Epoch: 12	Loss: 0.027765	Acc: 61.9% (6192/10000)
[Test]  Epoch: 13	Loss: 0.028480	Acc: 61.1% (6113/10000)
[Test]  Epoch: 14	Loss: 0.028058	Acc: 61.7% (6174/10000)
[Test]  Epoch: 15	Loss: 0.028392	Acc: 61.4% (6136/10000)
[Test]  Epoch: 16	Loss: 0.028293	Acc: 61.3% (6130/10000)
[Test]  Epoch: 17	Loss: 0.027831	Acc: 61.4% (6139/10000)
[Test]  Epoch: 18	Loss: 0.027649	Acc: 61.8% (6181/10000)
[Test]  Epoch: 19	Loss: 0.027564	Acc: 62.6% (6256/10000)
[Test]  Epoch: 20	Loss: 0.027656	Acc: 61.9% (6188/10000)
[Test]  Epoch: 21	Loss: 0.027995	Acc: 61.8% (6184/10000)
[Test]  Epoch: 22	Loss: 0.028090	Acc: 61.8% (6182/10000)
[Test]  Epoch: 23	Loss: 0.027910	Acc: 62.1% (6210/10000)
[Test]  Epoch: 24	Loss: 0.027579	Acc: 62.0% (6205/10000)
[Test]  Epoch: 25	Loss: 0.027199	Acc: 62.9% (6292/10000)
[Test]  Epoch: 26	Loss: 0.027352	Acc: 62.8% (6279/10000)
[Test]  Epoch: 27	Loss: 0.027220	Acc: 62.7% (6266/10000)
[Test]  Epoch: 28	Loss: 0.027057	Acc: 62.2% (6218/10000)
[Test]  Epoch: 29	Loss: 0.027964	Acc: 61.7% (6168/10000)
[Test]  Epoch: 30	Loss: 0.027313	Acc: 62.6% (6264/10000)
[Test]  Epoch: 31	Loss: 0.027364	Acc: 62.4% (6243/10000)
[Test]  Epoch: 32	Loss: 0.027140	Acc: 62.8% (6277/10000)
[Test]  Epoch: 33	Loss: 0.026604	Acc: 63.2% (6317/10000)
[Test]  Epoch: 34	Loss: 0.027137	Acc: 63.3% (6333/10000)
[Test]  Epoch: 35	Loss: 0.027050	Acc: 63.1% (6314/10000)
[Test]  Epoch: 36	Loss: 0.027026	Acc: 62.6% (6257/10000)
[Test]  Epoch: 37	Loss: 0.027117	Acc: 62.6% (6258/10000)
[Test]  Epoch: 38	Loss: 0.027241	Acc: 62.7% (6271/10000)
[Test]  Epoch: 39	Loss: 0.026718	Acc: 63.2% (6323/10000)
[Test]  Epoch: 40	Loss: 0.026839	Acc: 62.7% (6273/10000)
[Test]  Epoch: 41	Loss: 0.026736	Acc: 63.1% (6314/10000)
[Test]  Epoch: 42	Loss: 0.026555	Acc: 63.4% (6340/10000)
[Test]  Epoch: 43	Loss: 0.026468	Acc: 63.5% (6350/10000)
[Test]  Epoch: 44	Loss: 0.026739	Acc: 63.5% (6346/10000)
[Test]  Epoch: 45	Loss: 0.026508	Acc: 63.5% (6350/10000)
[Test]  Epoch: 46	Loss: 0.026483	Acc: 63.4% (6342/10000)
[Test]  Epoch: 47	Loss: 0.026392	Acc: 63.2% (6320/10000)
[Test]  Epoch: 48	Loss: 0.026347	Acc: 63.5% (6350/10000)
[Test]  Epoch: 49	Loss: 0.026565	Acc: 63.0% (6301/10000)
[Test]  Epoch: 50	Loss: 0.026872	Acc: 63.2% (6320/10000)
[Test]  Epoch: 51	Loss: 0.026687	Acc: 63.1% (6310/10000)
[Test]  Epoch: 52	Loss: 0.026529	Acc: 62.9% (6288/10000)
[Test]  Epoch: 53	Loss: 0.026556	Acc: 63.0% (6305/10000)
[Test]  Epoch: 54	Loss: 0.026595	Acc: 63.0% (6296/10000)
[Test]  Epoch: 55	Loss: 0.026468	Acc: 63.0% (6303/10000)
[Test]  Epoch: 56	Loss: 0.026571	Acc: 63.1% (6307/10000)
[Test]  Epoch: 57	Loss: 0.026246	Acc: 63.5% (6350/10000)
[Test]  Epoch: 58	Loss: 0.026652	Acc: 63.4% (6344/10000)
[Test]  Epoch: 59	Loss: 0.026375	Acc: 63.8% (6378/10000)
[Test]  Epoch: 60	Loss: 0.026709	Acc: 63.3% (6326/10000)
[Test]  Epoch: 61	Loss: 0.026448	Acc: 63.4% (6340/10000)
[Test]  Epoch: 62	Loss: 0.026499	Acc: 63.8% (6379/10000)
[Test]  Epoch: 63	Loss: 0.026371	Acc: 63.5% (6352/10000)
[Test]  Epoch: 64	Loss: 0.026343	Acc: 63.4% (6343/10000)
[Test]  Epoch: 65	Loss: 0.026413	Acc: 63.7% (6368/10000)
[Test]  Epoch: 66	Loss: 0.026440	Acc: 63.7% (6366/10000)
[Test]  Epoch: 67	Loss: 0.026399	Acc: 63.5% (6348/10000)
[Test]  Epoch: 68	Loss: 0.026245	Acc: 63.4% (6336/10000)
[Test]  Epoch: 69	Loss: 0.026239	Acc: 63.7% (6370/10000)
[Test]  Epoch: 70	Loss: 0.026478	Acc: 63.6% (6356/10000)
[Test]  Epoch: 71	Loss: 0.026152	Acc: 63.6% (6356/10000)
[Test]  Epoch: 72	Loss: 0.026285	Acc: 63.6% (6357/10000)
[Test]  Epoch: 73	Loss: 0.026371	Acc: 63.8% (6380/10000)
[Test]  Epoch: 74	Loss: 0.025941	Acc: 63.8% (6381/10000)
[Test]  Epoch: 75	Loss: 0.026227	Acc: 64.0% (6399/10000)
[Test]  Epoch: 76	Loss: 0.026378	Acc: 63.6% (6356/10000)
[Test]  Epoch: 77	Loss: 0.026444	Acc: 63.3% (6332/10000)
[Test]  Epoch: 78	Loss: 0.026618	Acc: 63.4% (6341/10000)
[Test]  Epoch: 79	Loss: 0.026232	Acc: 63.8% (6375/10000)
[Test]  Epoch: 80	Loss: 0.026351	Acc: 63.6% (6358/10000)
[Test]  Epoch: 81	Loss: 0.026334	Acc: 63.3% (6328/10000)
[Test]  Epoch: 82	Loss: 0.026301	Acc: 63.3% (6331/10000)
[Test]  Epoch: 83	Loss: 0.026177	Acc: 64.2% (6416/10000)
[Test]  Epoch: 84	Loss: 0.026305	Acc: 63.7% (6370/10000)
[Test]  Epoch: 85	Loss: 0.026305	Acc: 63.8% (6377/10000)
[Test]  Epoch: 86	Loss: 0.026111	Acc: 64.1% (6406/10000)
[Test]  Epoch: 87	Loss: 0.026025	Acc: 64.0% (6396/10000)
[Test]  Epoch: 88	Loss: 0.026410	Acc: 63.8% (6384/10000)
[Test]  Epoch: 89	Loss: 0.026293	Acc: 63.7% (6367/10000)
[Test]  Epoch: 90	Loss: 0.026393	Acc: 63.5% (6355/10000)
[Test]  Epoch: 91	Loss: 0.026291	Acc: 63.7% (6366/10000)
[Test]  Epoch: 92	Loss: 0.026289	Acc: 63.8% (6376/10000)
[Test]  Epoch: 93	Loss: 0.026229	Acc: 63.8% (6381/10000)
[Test]  Epoch: 94	Loss: 0.026019	Acc: 63.9% (6385/10000)
[Test]  Epoch: 95	Loss: 0.026079	Acc: 63.4% (6340/10000)
[Test]  Epoch: 96	Loss: 0.026224	Acc: 64.1% (6406/10000)
[Test]  Epoch: 97	Loss: 0.026577	Acc: 62.7% (6270/10000)
[Test]  Epoch: 98	Loss: 0.026351	Acc: 63.4% (6342/10000)
[Test]  Epoch: 99	Loss: 0.026070	Acc: 64.0% (6399/10000)
[Test]  Epoch: 100	Loss: 0.026248	Acc: 63.9% (6393/10000)
===========finish==========
['2024-08-19', '03:51:40.854183', '100', 'test', '0.026247516363859177', '63.93', '64.16']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.114126	Acc: 3.5% (347/10000)
[Test]  Epoch: 2	Loss: 0.069701	Acc: 7.3% (735/10000)
[Test]  Epoch: 3	Loss: 0.064371	Acc: 10.1% (1010/10000)
[Test]  Epoch: 4	Loss: 0.064229	Acc: 10.8% (1077/10000)
[Test]  Epoch: 5	Loss: 0.063058	Acc: 12.9% (1295/10000)
[Test]  Epoch: 6	Loss: 0.062019	Acc: 15.1% (1505/10000)
[Test]  Epoch: 7	Loss: 0.061853	Acc: 14.7% (1472/10000)
[Test]  Epoch: 8	Loss: 0.063845	Acc: 14.9% (1493/10000)
[Test]  Epoch: 9	Loss: 0.063056	Acc: 16.2% (1621/10000)
[Test]  Epoch: 10	Loss: 0.063998	Acc: 16.0% (1602/10000)
[Test]  Epoch: 11	Loss: 0.064719	Acc: 17.0% (1700/10000)
[Test]  Epoch: 12	Loss: 0.065298	Acc: 16.9% (1689/10000)
[Test]  Epoch: 13	Loss: 0.065294	Acc: 17.5% (1749/10000)
[Test]  Epoch: 14	Loss: 0.066732	Acc: 17.6% (1763/10000)
[Test]  Epoch: 15	Loss: 0.067239	Acc: 17.7% (1771/10000)
[Test]  Epoch: 16	Loss: 0.066399	Acc: 18.3% (1829/10000)
[Test]  Epoch: 17	Loss: 0.065900	Acc: 19.1% (1905/10000)
[Test]  Epoch: 18	Loss: 0.065756	Acc: 19.1% (1913/10000)
[Test]  Epoch: 19	Loss: 0.066886	Acc: 17.8% (1776/10000)
[Test]  Epoch: 20	Loss: 0.066716	Acc: 18.6% (1863/10000)
[Test]  Epoch: 21	Loss: 0.067430	Acc: 19.1% (1910/10000)
[Test]  Epoch: 22	Loss: 0.066510	Acc: 18.5% (1846/10000)
[Test]  Epoch: 23	Loss: 0.067297	Acc: 19.2% (1917/10000)
[Test]  Epoch: 24	Loss: 0.067298	Acc: 19.5% (1947/10000)
[Test]  Epoch: 25	Loss: 0.067096	Acc: 19.3% (1934/10000)
[Test]  Epoch: 26	Loss: 0.067069	Acc: 19.6% (1965/10000)
[Test]  Epoch: 27	Loss: 0.068244	Acc: 19.7% (1971/10000)
[Test]  Epoch: 28	Loss: 0.066954	Acc: 20.9% (2086/10000)
[Test]  Epoch: 29	Loss: 0.067397	Acc: 19.3% (1933/10000)
[Test]  Epoch: 30	Loss: 0.066747	Acc: 20.3% (2034/10000)
[Test]  Epoch: 31	Loss: 0.067314	Acc: 19.6% (1956/10000)
[Test]  Epoch: 32	Loss: 0.066755	Acc: 20.8% (2080/10000)
[Test]  Epoch: 33	Loss: 0.067593	Acc: 19.8% (1981/10000)
[Test]  Epoch: 34	Loss: 0.067323	Acc: 20.4% (2040/10000)
[Test]  Epoch: 35	Loss: 0.068326	Acc: 19.9% (1992/10000)
[Test]  Epoch: 36	Loss: 0.067267	Acc: 19.5% (1948/10000)
[Test]  Epoch: 37	Loss: 0.067217	Acc: 20.4% (2045/10000)
[Test]  Epoch: 38	Loss: 0.067451	Acc: 19.8% (1982/10000)
[Test]  Epoch: 39	Loss: 0.067547	Acc: 20.4% (2042/10000)
[Test]  Epoch: 40	Loss: 0.067817	Acc: 20.2% (2018/10000)
[Test]  Epoch: 41	Loss: 0.067683	Acc: 20.4% (2036/10000)
[Test]  Epoch: 42	Loss: 0.067561	Acc: 20.6% (2061/10000)
[Test]  Epoch: 43	Loss: 0.067069	Acc: 20.3% (2034/10000)
[Test]  Epoch: 44	Loss: 0.067524	Acc: 20.6% (2061/10000)
[Test]  Epoch: 45	Loss: 0.068196	Acc: 20.1% (2013/10000)
[Test]  Epoch: 46	Loss: 0.067737	Acc: 20.4% (2041/10000)
[Test]  Epoch: 47	Loss: 0.067849	Acc: 20.3% (2030/10000)
[Test]  Epoch: 48	Loss: 0.067874	Acc: 20.7% (2069/10000)
[Test]  Epoch: 49	Loss: 0.067367	Acc: 21.0% (2102/10000)
[Test]  Epoch: 50	Loss: 0.067309	Acc: 21.0% (2097/10000)
[Test]  Epoch: 51	Loss: 0.067343	Acc: 20.8% (2075/10000)
[Test]  Epoch: 52	Loss: 0.067179	Acc: 20.7% (2074/10000)
[Test]  Epoch: 53	Loss: 0.067836	Acc: 20.6% (2056/10000)
[Test]  Epoch: 54	Loss: 0.067961	Acc: 20.7% (2071/10000)
[Test]  Epoch: 55	Loss: 0.067472	Acc: 20.9% (2090/10000)
[Test]  Epoch: 56	Loss: 0.067047	Acc: 21.4% (2138/10000)
[Test]  Epoch: 57	Loss: 0.067095	Acc: 21.2% (2123/10000)
[Test]  Epoch: 58	Loss: 0.067048	Acc: 20.8% (2080/10000)
[Test]  Epoch: 59	Loss: 0.067537	Acc: 20.7% (2069/10000)
[Test]  Epoch: 60	Loss: 0.067951	Acc: 20.9% (2087/10000)
[Test]  Epoch: 61	Loss: 0.068110	Acc: 20.4% (2037/10000)
[Test]  Epoch: 62	Loss: 0.067571	Acc: 20.6% (2063/10000)
[Test]  Epoch: 63	Loss: 0.067465	Acc: 21.0% (2100/10000)
[Test]  Epoch: 64	Loss: 0.067651	Acc: 20.5% (2052/10000)
[Test]  Epoch: 65	Loss: 0.067553	Acc: 20.8% (2078/10000)
[Test]  Epoch: 66	Loss: 0.067370	Acc: 21.1% (2111/10000)
[Test]  Epoch: 67	Loss: 0.067354	Acc: 20.9% (2085/10000)
[Test]  Epoch: 68	Loss: 0.066982	Acc: 20.8% (2076/10000)
[Test]  Epoch: 69	Loss: 0.067645	Acc: 20.8% (2082/10000)
[Test]  Epoch: 70	Loss: 0.067386	Acc: 20.7% (2068/10000)
[Test]  Epoch: 71	Loss: 0.066856	Acc: 20.9% (2090/10000)
[Test]  Epoch: 72	Loss: 0.067128	Acc: 20.8% (2083/10000)
[Test]  Epoch: 73	Loss: 0.067038	Acc: 21.2% (2124/10000)
[Test]  Epoch: 74	Loss: 0.066831	Acc: 21.0% (2103/10000)
[Test]  Epoch: 75	Loss: 0.067015	Acc: 20.7% (2072/10000)
[Test]  Epoch: 76	Loss: 0.067336	Acc: 20.6% (2057/10000)
[Test]  Epoch: 77	Loss: 0.067623	Acc: 20.5% (2047/10000)
[Test]  Epoch: 78	Loss: 0.066929	Acc: 21.2% (2124/10000)
[Test]  Epoch: 79	Loss: 0.067268	Acc: 20.8% (2079/10000)
[Test]  Epoch: 80	Loss: 0.066856	Acc: 21.1% (2105/10000)
[Test]  Epoch: 81	Loss: 0.067211	Acc: 20.7% (2071/10000)
[Test]  Epoch: 82	Loss: 0.067136	Acc: 21.7% (2166/10000)
[Test]  Epoch: 83	Loss: 0.066980	Acc: 20.7% (2070/10000)
[Test]  Epoch: 84	Loss: 0.066916	Acc: 21.6% (2163/10000)
[Test]  Epoch: 85	Loss: 0.066797	Acc: 21.4% (2143/10000)
[Test]  Epoch: 86	Loss: 0.067207	Acc: 21.1% (2112/10000)
[Test]  Epoch: 87	Loss: 0.067254	Acc: 21.0% (2100/10000)
[Test]  Epoch: 88	Loss: 0.066863	Acc: 21.6% (2155/10000)
[Test]  Epoch: 89	Loss: 0.066989	Acc: 21.1% (2112/10000)
[Test]  Epoch: 90	Loss: 0.066978	Acc: 21.3% (2129/10000)
[Test]  Epoch: 91	Loss: 0.066937	Acc: 20.9% (2091/10000)
[Test]  Epoch: 92	Loss: 0.067380	Acc: 20.7% (2074/10000)
[Test]  Epoch: 93	Loss: 0.067254	Acc: 20.8% (2082/10000)
[Test]  Epoch: 94	Loss: 0.067435	Acc: 21.0% (2103/10000)
[Test]  Epoch: 95	Loss: 0.067204	Acc: 21.2% (2121/10000)
[Test]  Epoch: 96	Loss: 0.067139	Acc: 20.7% (2070/10000)
[Test]  Epoch: 97	Loss: 0.066904	Acc: 21.3% (2131/10000)
[Test]  Epoch: 98	Loss: 0.066950	Acc: 21.3% (2131/10000)
[Test]  Epoch: 99	Loss: 0.067115	Acc: 21.2% (2120/10000)
[Test]  Epoch: 100	Loss: 0.066785	Acc: 21.4% (2141/10000)
===========finish==========
['2024-08-19', '03:56:26.297990', '100', 'test', '0.06678464541435242', '21.41', '21.66']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.088642	Acc: 3.0% (295/10000)
[Test]  Epoch: 2	Loss: 0.069917	Acc: 7.8% (778/10000)
[Test]  Epoch: 3	Loss: 0.065881	Acc: 9.2% (921/10000)
[Test]  Epoch: 4	Loss: 0.065700	Acc: 9.8% (977/10000)
[Test]  Epoch: 5	Loss: 0.064402	Acc: 11.9% (1190/10000)
[Test]  Epoch: 6	Loss: 0.065708	Acc: 12.2% (1224/10000)
[Test]  Epoch: 7	Loss: 0.065569	Acc: 11.9% (1194/10000)
[Test]  Epoch: 8	Loss: 0.066217	Acc: 13.0% (1304/10000)
[Test]  Epoch: 9	Loss: 0.066739	Acc: 13.7% (1371/10000)
[Test]  Epoch: 10	Loss: 0.065902	Acc: 14.1% (1412/10000)
[Test]  Epoch: 11	Loss: 0.068928	Acc: 13.4% (1345/10000)
[Test]  Epoch: 12	Loss: 0.070307	Acc: 12.9% (1292/10000)
[Test]  Epoch: 13	Loss: 0.069113	Acc: 14.5% (1447/10000)
[Test]  Epoch: 14	Loss: 0.069665	Acc: 15.2% (1520/10000)
[Test]  Epoch: 15	Loss: 0.070646	Acc: 14.8% (1475/10000)
[Test]  Epoch: 16	Loss: 0.072617	Acc: 14.1% (1414/10000)
[Test]  Epoch: 17	Loss: 0.071057	Acc: 14.8% (1485/10000)
[Test]  Epoch: 18	Loss: 0.071992	Acc: 14.6% (1457/10000)
[Test]  Epoch: 19	Loss: 0.071300	Acc: 15.6% (1561/10000)
[Test]  Epoch: 20	Loss: 0.071975	Acc: 15.1% (1514/10000)
[Test]  Epoch: 21	Loss: 0.073234	Acc: 15.4% (1543/10000)
[Test]  Epoch: 22	Loss: 0.073358	Acc: 15.2% (1520/10000)
[Test]  Epoch: 23	Loss: 0.071700	Acc: 16.2% (1622/10000)
[Test]  Epoch: 24	Loss: 0.073878	Acc: 16.2% (1620/10000)
[Test]  Epoch: 25	Loss: 0.072069	Acc: 16.3% (1632/10000)
[Test]  Epoch: 26	Loss: 0.071826	Acc: 16.3% (1629/10000)
[Test]  Epoch: 27	Loss: 0.073912	Acc: 15.3% (1527/10000)
[Test]  Epoch: 28	Loss: 0.073174	Acc: 16.0% (1599/10000)
[Test]  Epoch: 29	Loss: 0.073539	Acc: 15.7% (1566/10000)
[Test]  Epoch: 30	Loss: 0.073173	Acc: 16.6% (1662/10000)
[Test]  Epoch: 31	Loss: 0.072803	Acc: 16.4% (1639/10000)
[Test]  Epoch: 32	Loss: 0.072749	Acc: 16.0% (1601/10000)
[Test]  Epoch: 33	Loss: 0.073286	Acc: 16.8% (1679/10000)
[Test]  Epoch: 34	Loss: 0.073336	Acc: 16.9% (1688/10000)
[Test]  Epoch: 35	Loss: 0.073688	Acc: 16.9% (1689/10000)
[Test]  Epoch: 36	Loss: 0.073565	Acc: 16.6% (1661/10000)
[Test]  Epoch: 37	Loss: 0.073069	Acc: 16.7% (1673/10000)
[Test]  Epoch: 38	Loss: 0.073920	Acc: 16.8% (1681/10000)
[Test]  Epoch: 39	Loss: 0.073102	Acc: 17.0% (1703/10000)
[Test]  Epoch: 40	Loss: 0.073907	Acc: 16.9% (1685/10000)
[Test]  Epoch: 41	Loss: 0.074495	Acc: 16.3% (1634/10000)
[Test]  Epoch: 42	Loss: 0.074924	Acc: 16.1% (1613/10000)
[Test]  Epoch: 43	Loss: 0.073264	Acc: 17.0% (1697/10000)
[Test]  Epoch: 44	Loss: 0.073892	Acc: 16.9% (1693/10000)
[Test]  Epoch: 45	Loss: 0.073497	Acc: 16.8% (1678/10000)
[Test]  Epoch: 46	Loss: 0.073734	Acc: 16.7% (1669/10000)
[Test]  Epoch: 47	Loss: 0.073638	Acc: 17.4% (1740/10000)
[Test]  Epoch: 48	Loss: 0.073844	Acc: 16.6% (1664/10000)
[Test]  Epoch: 49	Loss: 0.074277	Acc: 16.7% (1674/10000)
[Test]  Epoch: 50	Loss: 0.073788	Acc: 17.2% (1724/10000)
[Test]  Epoch: 51	Loss: 0.073710	Acc: 17.1% (1712/10000)
[Test]  Epoch: 52	Loss: 0.073805	Acc: 16.9% (1689/10000)
[Test]  Epoch: 53	Loss: 0.073896	Acc: 17.1% (1710/10000)
[Test]  Epoch: 54	Loss: 0.073703	Acc: 17.4% (1735/10000)
[Test]  Epoch: 55	Loss: 0.073497	Acc: 17.4% (1736/10000)
[Test]  Epoch: 56	Loss: 0.073205	Acc: 17.0% (1703/10000)
[Test]  Epoch: 57	Loss: 0.073993	Acc: 16.8% (1684/10000)
[Test]  Epoch: 58	Loss: 0.073688	Acc: 17.1% (1709/10000)
[Test]  Epoch: 59	Loss: 0.074036	Acc: 17.3% (1733/10000)
[Test]  Epoch: 60	Loss: 0.074377	Acc: 16.7% (1672/10000)
[Test]  Epoch: 61	Loss: 0.073778	Acc: 17.2% (1723/10000)
[Test]  Epoch: 62	Loss: 0.073617	Acc: 17.5% (1749/10000)
[Test]  Epoch: 63	Loss: 0.073600	Acc: 16.9% (1685/10000)
[Test]  Epoch: 64	Loss: 0.073719	Acc: 17.4% (1742/10000)
[Test]  Epoch: 65	Loss: 0.073611	Acc: 17.4% (1737/10000)
[Test]  Epoch: 66	Loss: 0.073702	Acc: 17.6% (1764/10000)
[Test]  Epoch: 67	Loss: 0.073557	Acc: 17.5% (1753/10000)
[Test]  Epoch: 68	Loss: 0.072960	Acc: 17.8% (1775/10000)
[Test]  Epoch: 69	Loss: 0.073536	Acc: 17.2% (1718/10000)
[Test]  Epoch: 70	Loss: 0.073457	Acc: 16.8% (1684/10000)
[Test]  Epoch: 71	Loss: 0.073146	Acc: 17.5% (1753/10000)
[Test]  Epoch: 72	Loss: 0.073304	Acc: 17.7% (1766/10000)
[Test]  Epoch: 73	Loss: 0.073469	Acc: 17.3% (1732/10000)
[Test]  Epoch: 74	Loss: 0.073379	Acc: 17.5% (1750/10000)
[Test]  Epoch: 75	Loss: 0.073008	Acc: 17.4% (1741/10000)
[Test]  Epoch: 76	Loss: 0.073520	Acc: 17.2% (1724/10000)
[Test]  Epoch: 77	Loss: 0.073834	Acc: 17.2% (1722/10000)
[Test]  Epoch: 78	Loss: 0.073241	Acc: 17.6% (1755/10000)
[Test]  Epoch: 79	Loss: 0.073160	Acc: 17.5% (1751/10000)
[Test]  Epoch: 80	Loss: 0.073043	Acc: 17.4% (1743/10000)
[Test]  Epoch: 81	Loss: 0.072935	Acc: 17.3% (1728/10000)
[Test]  Epoch: 82	Loss: 0.073158	Acc: 17.7% (1770/10000)
[Test]  Epoch: 83	Loss: 0.073380	Acc: 17.5% (1750/10000)
[Test]  Epoch: 84	Loss: 0.073007	Acc: 17.6% (1755/10000)
[Test]  Epoch: 85	Loss: 0.073353	Acc: 17.6% (1765/10000)
[Test]  Epoch: 86	Loss: 0.072725	Acc: 17.4% (1739/10000)
[Test]  Epoch: 87	Loss: 0.073310	Acc: 17.3% (1730/10000)
[Test]  Epoch: 88	Loss: 0.072826	Acc: 17.8% (1781/10000)
[Test]  Epoch: 89	Loss: 0.072584	Acc: 17.7% (1766/10000)
[Test]  Epoch: 90	Loss: 0.073283	Acc: 17.7% (1766/10000)
[Test]  Epoch: 91	Loss: 0.073383	Acc: 17.8% (1779/10000)
[Test]  Epoch: 92	Loss: 0.073256	Acc: 17.4% (1744/10000)
[Test]  Epoch: 93	Loss: 0.072804	Acc: 17.7% (1774/10000)
[Test]  Epoch: 94	Loss: 0.072608	Acc: 17.8% (1778/10000)
[Test]  Epoch: 95	Loss: 0.072880	Acc: 17.8% (1777/10000)
[Test]  Epoch: 96	Loss: 0.073382	Acc: 17.4% (1744/10000)
[Test]  Epoch: 97	Loss: 0.073327	Acc: 17.8% (1780/10000)
[Test]  Epoch: 98	Loss: 0.073266	Acc: 17.5% (1754/10000)
[Test]  Epoch: 99	Loss: 0.072983	Acc: 17.6% (1764/10000)
[Test]  Epoch: 100	Loss: 0.072796	Acc: 17.6% (1757/10000)
===========finish==========
['2024-08-19', '04:02:09.038213', '100', 'test', '0.07279566898345947', '17.57', '17.81']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.110345	Acc: 1.9% (188/10000)
[Test]  Epoch: 2	Loss: 0.072722	Acc: 6.3% (634/10000)
[Test]  Epoch: 3	Loss: 0.067955	Acc: 8.1% (805/10000)
[Test]  Epoch: 4	Loss: 0.068001	Acc: 8.4% (837/10000)
[Test]  Epoch: 5	Loss: 0.066462	Acc: 9.9% (989/10000)
[Test]  Epoch: 6	Loss: 0.065807	Acc: 9.8% (981/10000)
[Test]  Epoch: 7	Loss: 0.066759	Acc: 11.2% (1116/10000)
[Test]  Epoch: 8	Loss: 0.068338	Acc: 11.0% (1100/10000)
[Test]  Epoch: 9	Loss: 0.067098	Acc: 11.9% (1194/10000)
[Test]  Epoch: 10	Loss: 0.069804	Acc: 11.5% (1146/10000)
[Test]  Epoch: 11	Loss: 0.071298	Acc: 11.9% (1187/10000)
[Test]  Epoch: 12	Loss: 0.071363	Acc: 11.9% (1188/10000)
[Test]  Epoch: 13	Loss: 0.072486	Acc: 12.1% (1214/10000)
[Test]  Epoch: 14	Loss: 0.071354	Acc: 12.9% (1289/10000)
[Test]  Epoch: 15	Loss: 0.072601	Acc: 12.4% (1243/10000)
[Test]  Epoch: 16	Loss: 0.073987	Acc: 12.7% (1273/10000)
[Test]  Epoch: 17	Loss: 0.071662	Acc: 13.5% (1351/10000)
[Test]  Epoch: 18	Loss: 0.074116	Acc: 12.9% (1292/10000)
[Test]  Epoch: 19	Loss: 0.075344	Acc: 12.6% (1263/10000)
[Test]  Epoch: 20	Loss: 0.077387	Acc: 12.2% (1215/10000)
[Test]  Epoch: 21	Loss: 0.075605	Acc: 12.8% (1282/10000)
[Test]  Epoch: 22	Loss: 0.075336	Acc: 13.9% (1387/10000)
[Test]  Epoch: 23	Loss: 0.074698	Acc: 14.5% (1446/10000)
[Test]  Epoch: 24	Loss: 0.075451	Acc: 13.7% (1374/10000)
[Test]  Epoch: 25	Loss: 0.076559	Acc: 13.3% (1335/10000)
[Test]  Epoch: 26	Loss: 0.076018	Acc: 14.2% (1422/10000)
[Test]  Epoch: 27	Loss: 0.076532	Acc: 14.2% (1422/10000)
[Test]  Epoch: 28	Loss: 0.076086	Acc: 14.1% (1413/10000)
[Test]  Epoch: 29	Loss: 0.076749	Acc: 14.3% (1434/10000)
[Test]  Epoch: 30	Loss: 0.076484	Acc: 14.8% (1485/10000)
[Test]  Epoch: 31	Loss: 0.076256	Acc: 14.6% (1461/10000)
[Test]  Epoch: 32	Loss: 0.076810	Acc: 14.3% (1435/10000)
[Test]  Epoch: 33	Loss: 0.077062	Acc: 14.2% (1422/10000)
[Test]  Epoch: 34	Loss: 0.077640	Acc: 14.3% (1431/10000)
[Test]  Epoch: 35	Loss: 0.076743	Acc: 14.5% (1446/10000)
[Test]  Epoch: 36	Loss: 0.077366	Acc: 14.0% (1404/10000)
[Test]  Epoch: 37	Loss: 0.077281	Acc: 14.7% (1473/10000)
[Test]  Epoch: 38	Loss: 0.078657	Acc: 14.6% (1458/10000)
[Test]  Epoch: 39	Loss: 0.076215	Acc: 15.0% (1499/10000)
[Test]  Epoch: 40	Loss: 0.076324	Acc: 15.3% (1532/10000)
[Test]  Epoch: 41	Loss: 0.078815	Acc: 13.9% (1388/10000)
[Test]  Epoch: 42	Loss: 0.078483	Acc: 14.3% (1431/10000)
[Test]  Epoch: 43	Loss: 0.077150	Acc: 15.3% (1534/10000)
[Test]  Epoch: 44	Loss: 0.077500	Acc: 14.8% (1482/10000)
[Test]  Epoch: 45	Loss: 0.077630	Acc: 14.7% (1470/10000)
[Test]  Epoch: 46	Loss: 0.077163	Acc: 15.1% (1507/10000)
[Test]  Epoch: 47	Loss: 0.077066	Acc: 14.9% (1494/10000)
[Test]  Epoch: 48	Loss: 0.077414	Acc: 14.9% (1490/10000)
[Test]  Epoch: 49	Loss: 0.077216	Acc: 15.4% (1536/10000)
[Test]  Epoch: 50	Loss: 0.077665	Acc: 15.0% (1504/10000)
[Test]  Epoch: 51	Loss: 0.077477	Acc: 15.4% (1543/10000)
[Test]  Epoch: 52	Loss: 0.078059	Acc: 14.6% (1459/10000)
[Test]  Epoch: 53	Loss: 0.078069	Acc: 15.3% (1535/10000)
[Test]  Epoch: 54	Loss: 0.078621	Acc: 14.8% (1478/10000)
[Test]  Epoch: 55	Loss: 0.078520	Acc: 14.7% (1474/10000)
[Test]  Epoch: 56	Loss: 0.077825	Acc: 15.4% (1542/10000)
[Test]  Epoch: 57	Loss: 0.077590	Acc: 15.2% (1525/10000)
[Test]  Epoch: 58	Loss: 0.078105	Acc: 14.7% (1467/10000)
[Test]  Epoch: 59	Loss: 0.077542	Acc: 15.2% (1518/10000)
[Test]  Epoch: 60	Loss: 0.077425	Acc: 15.1% (1508/10000)
[Test]  Epoch: 61	Loss: 0.077705	Acc: 14.9% (1486/10000)
[Test]  Epoch: 62	Loss: 0.077684	Acc: 15.3% (1533/10000)
[Test]  Epoch: 63	Loss: 0.077950	Acc: 15.2% (1519/10000)
[Test]  Epoch: 64	Loss: 0.077740	Acc: 15.2% (1518/10000)
[Test]  Epoch: 65	Loss: 0.077168	Acc: 15.1% (1507/10000)
[Test]  Epoch: 66	Loss: 0.077311	Acc: 15.4% (1543/10000)
[Test]  Epoch: 67	Loss: 0.077139	Acc: 15.6% (1561/10000)
[Test]  Epoch: 68	Loss: 0.077434	Acc: 15.3% (1529/10000)
[Test]  Epoch: 69	Loss: 0.077688	Acc: 14.8% (1480/10000)
[Test]  Epoch: 70	Loss: 0.077195	Acc: 15.1% (1508/10000)
[Test]  Epoch: 71	Loss: 0.077037	Acc: 15.4% (1537/10000)
[Test]  Epoch: 72	Loss: 0.077204	Acc: 15.0% (1502/10000)
[Test]  Epoch: 73	Loss: 0.077459	Acc: 15.2% (1520/10000)
[Test]  Epoch: 74	Loss: 0.076940	Acc: 15.4% (1541/10000)
[Test]  Epoch: 75	Loss: 0.076937	Acc: 15.5% (1546/10000)
[Test]  Epoch: 76	Loss: 0.077365	Acc: 15.2% (1520/10000)
[Test]  Epoch: 77	Loss: 0.077662	Acc: 15.4% (1541/10000)
[Test]  Epoch: 78	Loss: 0.077173	Acc: 15.2% (1519/10000)
[Test]  Epoch: 79	Loss: 0.077223	Acc: 15.5% (1550/10000)
[Test]  Epoch: 80	Loss: 0.077111	Acc: 15.3% (1532/10000)
[Test]  Epoch: 81	Loss: 0.077415	Acc: 15.4% (1543/10000)
[Test]  Epoch: 82	Loss: 0.077173	Acc: 15.5% (1552/10000)
[Test]  Epoch: 83	Loss: 0.077753	Acc: 15.4% (1540/10000)
[Test]  Epoch: 84	Loss: 0.077257	Acc: 15.7% (1566/10000)
[Test]  Epoch: 85	Loss: 0.076790	Acc: 15.3% (1534/10000)
[Test]  Epoch: 86	Loss: 0.076617	Acc: 14.9% (1489/10000)
[Test]  Epoch: 87	Loss: 0.077221	Acc: 15.5% (1553/10000)
[Test]  Epoch: 88	Loss: 0.077498	Acc: 15.3% (1526/10000)
[Test]  Epoch: 89	Loss: 0.077077	Acc: 15.3% (1533/10000)
[Test]  Epoch: 90	Loss: 0.077198	Acc: 15.2% (1519/10000)
[Test]  Epoch: 91	Loss: 0.077220	Acc: 15.1% (1513/10000)
[Test]  Epoch: 92	Loss: 0.077027	Acc: 15.4% (1542/10000)
[Test]  Epoch: 93	Loss: 0.076994	Acc: 15.1% (1510/10000)
[Test]  Epoch: 94	Loss: 0.076874	Acc: 15.6% (1558/10000)
[Test]  Epoch: 95	Loss: 0.076495	Acc: 15.6% (1560/10000)
[Test]  Epoch: 96	Loss: 0.077021	Acc: 15.2% (1525/10000)
[Test]  Epoch: 97	Loss: 0.077109	Acc: 15.4% (1544/10000)
[Test]  Epoch: 98	Loss: 0.076947	Acc: 15.3% (1526/10000)
[Test]  Epoch: 99	Loss: 0.077120	Acc: 15.4% (1537/10000)
[Test]  Epoch: 100	Loss: 0.077521	Acc: 15.5% (1553/10000)
===========finish==========
['2024-08-19', '04:07:12.385223', '100', 'test', '0.07752077894210815', '15.53', '15.66']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.200484	Acc: 1.5% (147/10000)
[Test]  Epoch: 2	Loss: 0.075446	Acc: 1.3% (128/10000)
[Test]  Epoch: 3	Loss: 0.073735	Acc: 1.5% (154/10000)
[Test]  Epoch: 4	Loss: 0.074465	Acc: 2.2% (217/10000)
[Test]  Epoch: 5	Loss: 0.073152	Acc: 2.0% (198/10000)
[Test]  Epoch: 6	Loss: 0.073246	Acc: 2.4% (243/10000)
[Test]  Epoch: 7	Loss: 0.073967	Acc: 2.2% (216/10000)
[Test]  Epoch: 8	Loss: 0.071468	Acc: 1.7% (166/10000)
[Test]  Epoch: 9	Loss: 0.071279	Acc: 2.6% (264/10000)
[Test]  Epoch: 10	Loss: 0.070616	Acc: 2.8% (278/10000)
[Test]  Epoch: 11	Loss: 0.074531	Acc: 3.6% (356/10000)
[Test]  Epoch: 12	Loss: 0.074204	Acc: 3.4% (340/10000)
[Test]  Epoch: 13	Loss: 0.069897	Acc: 3.0% (304/10000)
[Test]  Epoch: 14	Loss: 0.078373	Acc: 4.2% (419/10000)
[Test]  Epoch: 15	Loss: 0.077189	Acc: 3.7% (373/10000)
[Test]  Epoch: 16	Loss: 0.069493	Acc: 3.7% (367/10000)
[Test]  Epoch: 17	Loss: 0.095333	Acc: 4.5% (450/10000)
[Test]  Epoch: 18	Loss: 0.067620	Acc: 4.5% (451/10000)
[Test]  Epoch: 19	Loss: 0.069485	Acc: 4.9% (489/10000)
[Test]  Epoch: 20	Loss: 0.069922	Acc: 7.0% (695/10000)
[Test]  Epoch: 21	Loss: 0.068517	Acc: 7.0% (700/10000)
[Test]  Epoch: 22	Loss: 0.070870	Acc: 6.8% (681/10000)
[Test]  Epoch: 23	Loss: 0.071812	Acc: 7.4% (741/10000)
[Test]  Epoch: 24	Loss: 0.069114	Acc: 8.0% (799/10000)
[Test]  Epoch: 25	Loss: 0.069836	Acc: 8.1% (806/10000)
[Test]  Epoch: 26	Loss: 0.069664	Acc: 8.7% (874/10000)
[Test]  Epoch: 27	Loss: 0.067874	Acc: 8.8% (883/10000)
[Test]  Epoch: 28	Loss: 0.070333	Acc: 7.2% (716/10000)
[Test]  Epoch: 29	Loss: 0.070821	Acc: 8.8% (884/10000)
[Test]  Epoch: 30	Loss: 0.071988	Acc: 9.7% (965/10000)
[Test]  Epoch: 31	Loss: 0.072128	Acc: 9.0% (904/10000)
[Test]  Epoch: 32	Loss: 0.072622	Acc: 9.2% (923/10000)
[Test]  Epoch: 33	Loss: 0.072751	Acc: 9.8% (975/10000)
[Test]  Epoch: 34	Loss: 0.076711	Acc: 8.7% (870/10000)
[Test]  Epoch: 35	Loss: 0.074663	Acc: 9.9% (988/10000)
[Test]  Epoch: 36	Loss: 0.074446	Acc: 10.3% (1030/10000)
[Test]  Epoch: 37	Loss: 0.075923	Acc: 10.2% (1015/10000)
[Test]  Epoch: 38	Loss: 0.074468	Acc: 10.5% (1049/10000)
[Test]  Epoch: 39	Loss: 0.074358	Acc: 11.3% (1131/10000)
[Test]  Epoch: 40	Loss: 0.077483	Acc: 9.8% (977/10000)
[Test]  Epoch: 41	Loss: 0.077294	Acc: 9.1% (914/10000)
[Test]  Epoch: 42	Loss: 0.077860	Acc: 9.9% (992/10000)
[Test]  Epoch: 43	Loss: 0.076495	Acc: 11.2% (1123/10000)
[Test]  Epoch: 44	Loss: 0.078911	Acc: 10.6% (1058/10000)
[Test]  Epoch: 45	Loss: 0.078604	Acc: 10.6% (1063/10000)
[Test]  Epoch: 46	Loss: 0.077587	Acc: 10.9% (1087/10000)
[Test]  Epoch: 47	Loss: 0.078515	Acc: 11.6% (1164/10000)
[Test]  Epoch: 48	Loss: 0.078471	Acc: 10.8% (1085/10000)
[Test]  Epoch: 49	Loss: 0.078559	Acc: 11.5% (1154/10000)
[Test]  Epoch: 50	Loss: 0.079027	Acc: 11.4% (1139/10000)
[Test]  Epoch: 51	Loss: 0.079021	Acc: 11.2% (1116/10000)
[Test]  Epoch: 52	Loss: 0.079180	Acc: 11.8% (1184/10000)
[Test]  Epoch: 53	Loss: 0.080561	Acc: 11.2% (1117/10000)
[Test]  Epoch: 54	Loss: 0.081456	Acc: 11.4% (1143/10000)
[Test]  Epoch: 55	Loss: 0.080174	Acc: 11.8% (1181/10000)
[Test]  Epoch: 56	Loss: 0.080689	Acc: 10.7% (1069/10000)
[Test]  Epoch: 57	Loss: 0.080555	Acc: 11.4% (1138/10000)
[Test]  Epoch: 58	Loss: 0.079906	Acc: 12.0% (1200/10000)
[Test]  Epoch: 59	Loss: 0.080346	Acc: 12.0% (1199/10000)
[Test]  Epoch: 60	Loss: 0.080183	Acc: 12.3% (1232/10000)
[Test]  Epoch: 61	Loss: 0.079423	Acc: 12.4% (1244/10000)
[Test]  Epoch: 62	Loss: 0.079412	Acc: 12.9% (1287/10000)
[Test]  Epoch: 63	Loss: 0.079515	Acc: 12.7% (1270/10000)
[Test]  Epoch: 64	Loss: 0.079011	Acc: 12.6% (1257/10000)
[Test]  Epoch: 65	Loss: 0.079234	Acc: 12.7% (1273/10000)
[Test]  Epoch: 66	Loss: 0.079561	Acc: 12.6% (1262/10000)
[Test]  Epoch: 67	Loss: 0.078735	Acc: 12.8% (1275/10000)
[Test]  Epoch: 68	Loss: 0.079226	Acc: 12.5% (1249/10000)
[Test]  Epoch: 69	Loss: 0.079196	Acc: 12.6% (1261/10000)
[Test]  Epoch: 70	Loss: 0.079225	Acc: 12.2% (1220/10000)
[Test]  Epoch: 71	Loss: 0.079292	Acc: 12.9% (1290/10000)
[Test]  Epoch: 72	Loss: 0.079103	Acc: 12.5% (1249/10000)
[Test]  Epoch: 73	Loss: 0.079544	Acc: 12.6% (1259/10000)
[Test]  Epoch: 74	Loss: 0.079161	Acc: 12.6% (1255/10000)
[Test]  Epoch: 75	Loss: 0.079009	Acc: 12.6% (1262/10000)
[Test]  Epoch: 76	Loss: 0.079073	Acc: 12.6% (1262/10000)
[Test]  Epoch: 77	Loss: 0.079119	Acc: 12.5% (1246/10000)
[Test]  Epoch: 78	Loss: 0.078782	Acc: 12.6% (1259/10000)
[Test]  Epoch: 79	Loss: 0.079089	Acc: 12.4% (1241/10000)
[Test]  Epoch: 80	Loss: 0.078990	Acc: 12.5% (1251/10000)
[Test]  Epoch: 81	Loss: 0.079257	Acc: 12.6% (1263/10000)
[Test]  Epoch: 82	Loss: 0.078929	Acc: 12.4% (1238/10000)
[Test]  Epoch: 83	Loss: 0.079364	Acc: 12.4% (1238/10000)
[Test]  Epoch: 84	Loss: 0.079000	Acc: 13.0% (1297/10000)
[Test]  Epoch: 85	Loss: 0.079484	Acc: 12.5% (1246/10000)
[Test]  Epoch: 86	Loss: 0.078765	Acc: 13.2% (1315/10000)
[Test]  Epoch: 87	Loss: 0.079698	Acc: 12.6% (1261/10000)
[Test]  Epoch: 88	Loss: 0.079278	Acc: 12.5% (1248/10000)
[Test]  Epoch: 89	Loss: 0.079223	Acc: 12.8% (1280/10000)
[Test]  Epoch: 90	Loss: 0.079162	Acc: 12.9% (1287/10000)
[Test]  Epoch: 91	Loss: 0.079661	Acc: 12.2% (1220/10000)
[Test]  Epoch: 92	Loss: 0.079559	Acc: 12.8% (1283/10000)
[Test]  Epoch: 93	Loss: 0.079386	Acc: 12.5% (1247/10000)
[Test]  Epoch: 94	Loss: 0.078855	Acc: 12.8% (1279/10000)
[Test]  Epoch: 95	Loss: 0.079165	Acc: 12.3% (1235/10000)
[Test]  Epoch: 96	Loss: 0.079354	Acc: 12.6% (1255/10000)
[Test]  Epoch: 97	Loss: 0.079109	Acc: 12.8% (1277/10000)
[Test]  Epoch: 98	Loss: 0.079160	Acc: 12.6% (1258/10000)
[Test]  Epoch: 99	Loss: 0.079041	Acc: 12.6% (1262/10000)
[Test]  Epoch: 100	Loss: 0.079382	Acc: 12.5% (1248/10000)
===========finish==========
['2024-08-19', '04:12:12.512939', '100', 'test', '0.0793816309928894', '12.48', '13.15']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.213760	Acc: 1.4% (135/10000)
[Test]  Epoch: 2	Loss: 0.074684	Acc: 1.5% (153/10000)
[Test]  Epoch: 3	Loss: 0.071801	Acc: 2.1% (215/10000)
[Test]  Epoch: 4	Loss: 0.071202	Acc: 2.2% (221/10000)
[Test]  Epoch: 5	Loss: 0.071037	Acc: 2.3% (229/10000)
[Test]  Epoch: 6	Loss: 0.070090	Acc: 3.2% (321/10000)
[Test]  Epoch: 7	Loss: 0.069953	Acc: 4.2% (417/10000)
[Test]  Epoch: 8	Loss: 0.066701	Acc: 5.0% (504/10000)
[Test]  Epoch: 9	Loss: 0.068274	Acc: 6.1% (614/10000)
[Test]  Epoch: 10	Loss: 0.065399	Acc: 7.2% (717/10000)
[Test]  Epoch: 11	Loss: 0.068173	Acc: 8.1% (806/10000)
[Test]  Epoch: 12	Loss: 0.068580	Acc: 8.5% (853/10000)
[Test]  Epoch: 13	Loss: 0.067758	Acc: 8.6% (864/10000)
[Test]  Epoch: 14	Loss: 0.069726	Acc: 9.1% (907/10000)
[Test]  Epoch: 15	Loss: 0.070027	Acc: 9.2% (916/10000)
[Test]  Epoch: 16	Loss: 0.070103	Acc: 9.4% (944/10000)
[Test]  Epoch: 17	Loss: 0.069497	Acc: 10.1% (1014/10000)
[Test]  Epoch: 18	Loss: 0.072708	Acc: 8.5% (853/10000)
[Test]  Epoch: 19	Loss: 0.072867	Acc: 9.4% (944/10000)
[Test]  Epoch: 20	Loss: 0.073819	Acc: 9.4% (943/10000)
[Test]  Epoch: 21	Loss: 0.075741	Acc: 9.5% (952/10000)
[Test]  Epoch: 22	Loss: 0.073753	Acc: 10.8% (1080/10000)
[Test]  Epoch: 23	Loss: 0.075325	Acc: 11.1% (1110/10000)
[Test]  Epoch: 24	Loss: 0.075568	Acc: 10.9% (1090/10000)
[Test]  Epoch: 25	Loss: 0.078451	Acc: 10.0% (999/10000)
[Test]  Epoch: 26	Loss: 0.075998	Acc: 11.1% (1108/10000)
[Test]  Epoch: 27	Loss: 0.077245	Acc: 11.2% (1117/10000)
[Test]  Epoch: 28	Loss: 0.077126	Acc: 11.3% (1129/10000)
[Test]  Epoch: 29	Loss: 0.079161	Acc: 11.0% (1101/10000)
[Test]  Epoch: 30	Loss: 0.077101	Acc: 11.3% (1133/10000)
[Test]  Epoch: 31	Loss: 0.080237	Acc: 10.9% (1090/10000)
[Test]  Epoch: 32	Loss: 0.076829	Acc: 12.4% (1237/10000)
[Test]  Epoch: 33	Loss: 0.078320	Acc: 11.4% (1137/10000)
[Test]  Epoch: 34	Loss: 0.079256	Acc: 11.5% (1152/10000)
[Test]  Epoch: 35	Loss: 0.079964	Acc: 12.1% (1207/10000)
[Test]  Epoch: 36	Loss: 0.080651	Acc: 11.5% (1148/10000)
[Test]  Epoch: 37	Loss: 0.079719	Acc: 11.9% (1191/10000)
[Test]  Epoch: 38	Loss: 0.080136	Acc: 11.3% (1130/10000)
[Test]  Epoch: 39	Loss: 0.079717	Acc: 12.5% (1248/10000)
[Test]  Epoch: 40	Loss: 0.080604	Acc: 12.3% (1232/10000)
[Test]  Epoch: 41	Loss: 0.079944	Acc: 12.4% (1238/10000)
[Test]  Epoch: 42	Loss: 0.078722	Acc: 13.4% (1344/10000)
[Test]  Epoch: 43	Loss: 0.079523	Acc: 12.9% (1286/10000)
[Test]  Epoch: 44	Loss: 0.080529	Acc: 12.5% (1254/10000)
[Test]  Epoch: 45	Loss: 0.081101	Acc: 12.2% (1219/10000)
[Test]  Epoch: 46	Loss: 0.079912	Acc: 13.1% (1310/10000)
[Test]  Epoch: 47	Loss: 0.080238	Acc: 13.9% (1393/10000)
[Test]  Epoch: 48	Loss: 0.080953	Acc: 13.3% (1334/10000)
[Test]  Epoch: 49	Loss: 0.080818	Acc: 12.7% (1273/10000)
[Test]  Epoch: 50	Loss: 0.081183	Acc: 12.6% (1259/10000)
[Test]  Epoch: 51	Loss: 0.080733	Acc: 13.2% (1318/10000)
[Test]  Epoch: 52	Loss: 0.081113	Acc: 13.5% (1346/10000)
[Test]  Epoch: 53	Loss: 0.080971	Acc: 13.3% (1327/10000)
[Test]  Epoch: 54	Loss: 0.080623	Acc: 13.2% (1319/10000)
[Test]  Epoch: 55	Loss: 0.080161	Acc: 13.8% (1381/10000)
[Test]  Epoch: 56	Loss: 0.081470	Acc: 13.6% (1357/10000)
[Test]  Epoch: 57	Loss: 0.079866	Acc: 14.3% (1434/10000)
[Test]  Epoch: 58	Loss: 0.080764	Acc: 13.8% (1376/10000)
[Test]  Epoch: 59	Loss: 0.079618	Acc: 13.9% (1391/10000)
[Test]  Epoch: 60	Loss: 0.081132	Acc: 13.5% (1352/10000)
[Test]  Epoch: 61	Loss: 0.079708	Acc: 14.0% (1402/10000)
[Test]  Epoch: 62	Loss: 0.079613	Acc: 14.2% (1419/10000)
[Test]  Epoch: 63	Loss: 0.079485	Acc: 14.3% (1431/10000)
[Test]  Epoch: 64	Loss: 0.079423	Acc: 14.0% (1401/10000)
[Test]  Epoch: 65	Loss: 0.079240	Acc: 14.2% (1418/10000)
[Test]  Epoch: 66	Loss: 0.079261	Acc: 14.3% (1429/10000)
[Test]  Epoch: 67	Loss: 0.079231	Acc: 13.9% (1392/10000)
[Test]  Epoch: 68	Loss: 0.079348	Acc: 14.3% (1435/10000)
[Test]  Epoch: 69	Loss: 0.078925	Acc: 14.4% (1437/10000)
[Test]  Epoch: 70	Loss: 0.079539	Acc: 14.3% (1434/10000)
[Test]  Epoch: 71	Loss: 0.079217	Acc: 14.5% (1447/10000)
[Test]  Epoch: 72	Loss: 0.079205	Acc: 14.2% (1415/10000)
[Test]  Epoch: 73	Loss: 0.079706	Acc: 14.1% (1413/10000)
[Test]  Epoch: 74	Loss: 0.078444	Acc: 14.4% (1437/10000)
[Test]  Epoch: 75	Loss: 0.078869	Acc: 14.1% (1412/10000)
[Test]  Epoch: 76	Loss: 0.078898	Acc: 14.4% (1440/10000)
[Test]  Epoch: 77	Loss: 0.079494	Acc: 13.6% (1358/10000)
[Test]  Epoch: 78	Loss: 0.079326	Acc: 14.4% (1439/10000)
[Test]  Epoch: 79	Loss: 0.079162	Acc: 14.3% (1427/10000)
[Test]  Epoch: 80	Loss: 0.079057	Acc: 14.3% (1432/10000)
[Test]  Epoch: 81	Loss: 0.079411	Acc: 14.5% (1451/10000)
[Test]  Epoch: 82	Loss: 0.079501	Acc: 14.3% (1429/10000)
[Test]  Epoch: 83	Loss: 0.079504	Acc: 13.8% (1385/10000)
[Test]  Epoch: 84	Loss: 0.078997	Acc: 14.0% (1402/10000)
[Test]  Epoch: 85	Loss: 0.078907	Acc: 14.4% (1441/10000)
[Test]  Epoch: 86	Loss: 0.078824	Acc: 14.0% (1402/10000)
[Test]  Epoch: 87	Loss: 0.079107	Acc: 14.4% (1444/10000)
[Test]  Epoch: 88	Loss: 0.079202	Acc: 14.3% (1434/10000)
[Test]  Epoch: 89	Loss: 0.078579	Acc: 14.7% (1466/10000)
[Test]  Epoch: 90	Loss: 0.078981	Acc: 14.3% (1432/10000)
[Test]  Epoch: 91	Loss: 0.079658	Acc: 14.2% (1416/10000)
[Test]  Epoch: 92	Loss: 0.078767	Acc: 14.3% (1432/10000)
[Test]  Epoch: 93	Loss: 0.079431	Acc: 14.1% (1407/10000)
[Test]  Epoch: 94	Loss: 0.078508	Acc: 14.4% (1437/10000)
[Test]  Epoch: 95	Loss: 0.078581	Acc: 13.8% (1382/10000)
[Test]  Epoch: 96	Loss: 0.078785	Acc: 14.3% (1431/10000)
[Test]  Epoch: 97	Loss: 0.079164	Acc: 14.0% (1401/10000)
[Test]  Epoch: 98	Loss: 0.078969	Acc: 14.4% (1444/10000)
[Test]  Epoch: 99	Loss: 0.079029	Acc: 14.5% (1451/10000)
[Test]  Epoch: 100	Loss: 0.079112	Acc: 14.3% (1426/10000)
===========finish==========
['2024-08-19', '04:17:08.756249', '100', 'test', '0.0791120994091034', '14.26', '14.66']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.150325	Acc: 1.9% (188/10000)
[Test]  Epoch: 2	Loss: 0.071899	Acc: 2.0% (198/10000)
[Test]  Epoch: 3	Loss: 0.071255	Acc: 2.5% (252/10000)
[Test]  Epoch: 4	Loss: 0.071157	Acc: 2.4% (236/10000)
[Test]  Epoch: 5	Loss: 0.070203	Acc: 4.1% (412/10000)
[Test]  Epoch: 6	Loss: 0.068437	Acc: 4.6% (460/10000)
[Test]  Epoch: 7	Loss: 0.068349	Acc: 5.8% (581/10000)
[Test]  Epoch: 8	Loss: 0.067647	Acc: 7.4% (744/10000)
[Test]  Epoch: 9	Loss: 0.067568	Acc: 7.2% (725/10000)
[Test]  Epoch: 10	Loss: 0.066599	Acc: 7.9% (786/10000)
[Test]  Epoch: 11	Loss: 0.069257	Acc: 8.4% (841/10000)
[Test]  Epoch: 12	Loss: 0.073032	Acc: 8.3% (826/10000)
[Test]  Epoch: 13	Loss: 0.068457	Acc: 8.7% (866/10000)
[Test]  Epoch: 14	Loss: 0.074010	Acc: 8.8% (880/10000)
[Test]  Epoch: 15	Loss: 0.072202	Acc: 9.2% (922/10000)
[Test]  Epoch: 16	Loss: 0.070361	Acc: 10.0% (999/10000)
[Test]  Epoch: 17	Loss: 0.074370	Acc: 9.7% (974/10000)
[Test]  Epoch: 18	Loss: 0.072418	Acc: 9.5% (952/10000)
[Test]  Epoch: 19	Loss: 0.071402	Acc: 10.8% (1078/10000)
[Test]  Epoch: 20	Loss: 0.074404	Acc: 10.0% (1001/10000)
[Test]  Epoch: 21	Loss: 0.071257	Acc: 11.6% (1157/10000)
[Test]  Epoch: 22	Loss: 0.074964	Acc: 10.6% (1058/10000)
[Test]  Epoch: 23	Loss: 0.072628	Acc: 11.6% (1155/10000)
[Test]  Epoch: 24	Loss: 0.077469	Acc: 10.0% (998/10000)
[Test]  Epoch: 25	Loss: 0.078495	Acc: 10.8% (1083/10000)
[Test]  Epoch: 26	Loss: 0.074579	Acc: 12.4% (1238/10000)
[Test]  Epoch: 27	Loss: 0.075512	Acc: 11.5% (1153/10000)
[Test]  Epoch: 28	Loss: 0.076322	Acc: 11.5% (1154/10000)
[Test]  Epoch: 29	Loss: 0.078412	Acc: 11.7% (1169/10000)
[Test]  Epoch: 30	Loss: 0.077823	Acc: 12.1% (1207/10000)
[Test]  Epoch: 31	Loss: 0.077877	Acc: 12.6% (1264/10000)
[Test]  Epoch: 32	Loss: 0.076724	Acc: 12.0% (1198/10000)
[Test]  Epoch: 33	Loss: 0.077537	Acc: 12.1% (1212/10000)
[Test]  Epoch: 34	Loss: 0.078141	Acc: 12.4% (1243/10000)
[Test]  Epoch: 35	Loss: 0.078084	Acc: 12.9% (1287/10000)
[Test]  Epoch: 36	Loss: 0.079812	Acc: 12.9% (1288/10000)
[Test]  Epoch: 37	Loss: 0.078395	Acc: 12.4% (1239/10000)
[Test]  Epoch: 38	Loss: 0.078981	Acc: 13.0% (1300/10000)
[Test]  Epoch: 39	Loss: 0.077736	Acc: 13.3% (1331/10000)
[Test]  Epoch: 40	Loss: 0.079230	Acc: 12.7% (1266/10000)
[Test]  Epoch: 41	Loss: 0.080273	Acc: 12.9% (1294/10000)
[Test]  Epoch: 42	Loss: 0.078403	Acc: 12.6% (1263/10000)
[Test]  Epoch: 43	Loss: 0.079511	Acc: 13.2% (1325/10000)
[Test]  Epoch: 44	Loss: 0.079807	Acc: 12.8% (1283/10000)
[Test]  Epoch: 45	Loss: 0.079011	Acc: 12.9% (1286/10000)
[Test]  Epoch: 46	Loss: 0.078446	Acc: 13.3% (1331/10000)
[Test]  Epoch: 47	Loss: 0.079098	Acc: 14.2% (1415/10000)
[Test]  Epoch: 48	Loss: 0.078893	Acc: 13.3% (1329/10000)
[Test]  Epoch: 49	Loss: 0.078874	Acc: 13.3% (1330/10000)
[Test]  Epoch: 50	Loss: 0.077826	Acc: 13.2% (1321/10000)
[Test]  Epoch: 51	Loss: 0.080268	Acc: 13.4% (1339/10000)
[Test]  Epoch: 52	Loss: 0.080154	Acc: 13.3% (1328/10000)
[Test]  Epoch: 53	Loss: 0.080530	Acc: 13.1% (1312/10000)
[Test]  Epoch: 54	Loss: 0.079525	Acc: 13.3% (1334/10000)
[Test]  Epoch: 55	Loss: 0.079012	Acc: 13.1% (1311/10000)
[Test]  Epoch: 56	Loss: 0.079081	Acc: 13.5% (1351/10000)
[Test]  Epoch: 57	Loss: 0.078388	Acc: 13.7% (1374/10000)
[Test]  Epoch: 58	Loss: 0.079677	Acc: 13.8% (1379/10000)
[Test]  Epoch: 59	Loss: 0.079892	Acc: 13.8% (1381/10000)
[Test]  Epoch: 60	Loss: 0.078924	Acc: 14.1% (1412/10000)
[Test]  Epoch: 61	Loss: 0.077834	Acc: 14.5% (1454/10000)
[Test]  Epoch: 62	Loss: 0.077819	Acc: 14.2% (1424/10000)
[Test]  Epoch: 63	Loss: 0.077772	Acc: 14.3% (1428/10000)
[Test]  Epoch: 64	Loss: 0.078008	Acc: 14.5% (1447/10000)
[Test]  Epoch: 65	Loss: 0.077418	Acc: 14.3% (1435/10000)
[Test]  Epoch: 66	Loss: 0.077975	Acc: 14.3% (1434/10000)
[Test]  Epoch: 67	Loss: 0.077402	Acc: 14.2% (1423/10000)
[Test]  Epoch: 68	Loss: 0.077654	Acc: 14.2% (1417/10000)
[Test]  Epoch: 69	Loss: 0.077541	Acc: 14.1% (1410/10000)
[Test]  Epoch: 70	Loss: 0.077515	Acc: 14.6% (1455/10000)
[Test]  Epoch: 71	Loss: 0.077706	Acc: 14.7% (1471/10000)
[Test]  Epoch: 72	Loss: 0.077366	Acc: 14.5% (1452/10000)
[Test]  Epoch: 73	Loss: 0.077111	Acc: 14.6% (1463/10000)
[Test]  Epoch: 74	Loss: 0.077353	Acc: 14.4% (1438/10000)
[Test]  Epoch: 75	Loss: 0.077417	Acc: 14.6% (1461/10000)
[Test]  Epoch: 76	Loss: 0.077707	Acc: 14.5% (1449/10000)
[Test]  Epoch: 77	Loss: 0.077439	Acc: 14.2% (1422/10000)
[Test]  Epoch: 78	Loss: 0.077494	Acc: 14.3% (1426/10000)
[Test]  Epoch: 79	Loss: 0.077167	Acc: 14.3% (1432/10000)
[Test]  Epoch: 80	Loss: 0.076883	Acc: 14.3% (1429/10000)
[Test]  Epoch: 81	Loss: 0.077009	Acc: 14.4% (1436/10000)
[Test]  Epoch: 82	Loss: 0.077408	Acc: 14.5% (1446/10000)
[Test]  Epoch: 83	Loss: 0.077642	Acc: 14.2% (1421/10000)
[Test]  Epoch: 84	Loss: 0.077252	Acc: 14.6% (1462/10000)
[Test]  Epoch: 85	Loss: 0.077199	Acc: 14.7% (1465/10000)
[Test]  Epoch: 86	Loss: 0.077197	Acc: 14.2% (1422/10000)
[Test]  Epoch: 87	Loss: 0.077370	Acc: 14.8% (1484/10000)
[Test]  Epoch: 88	Loss: 0.077225	Acc: 14.4% (1440/10000)
[Test]  Epoch: 89	Loss: 0.077211	Acc: 14.4% (1440/10000)
[Test]  Epoch: 90	Loss: 0.077699	Acc: 14.6% (1464/10000)
[Test]  Epoch: 91	Loss: 0.077611	Acc: 14.3% (1428/10000)
[Test]  Epoch: 92	Loss: 0.077396	Acc: 14.3% (1432/10000)
[Test]  Epoch: 93	Loss: 0.077447	Acc: 14.5% (1451/10000)
[Test]  Epoch: 94	Loss: 0.076970	Acc: 14.7% (1466/10000)
[Test]  Epoch: 95	Loss: 0.077160	Acc: 14.4% (1439/10000)
[Test]  Epoch: 96	Loss: 0.077638	Acc: 14.4% (1436/10000)
[Test]  Epoch: 97	Loss: 0.077549	Acc: 14.3% (1428/10000)
[Test]  Epoch: 98	Loss: 0.077091	Acc: 14.8% (1483/10000)
[Test]  Epoch: 99	Loss: 0.077240	Acc: 15.0% (1496/10000)
[Test]  Epoch: 100	Loss: 0.077476	Acc: 14.8% (1479/10000)
===========finish==========
['2024-08-19', '04:22:06.208307', '100', 'test', '0.07747631509304047', '14.79', '14.96']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.077992	Acc: 1.2% (125/10000)
[Test]  Epoch: 2	Loss: 0.071406	Acc: 2.0% (199/10000)
[Test]  Epoch: 3	Loss: 0.070306	Acc: 2.5% (251/10000)
[Test]  Epoch: 4	Loss: 0.068352	Acc: 4.0% (398/10000)
[Test]  Epoch: 5	Loss: 0.069018	Acc: 4.0% (400/10000)
[Test]  Epoch: 6	Loss: 0.070371	Acc: 3.2% (318/10000)
[Test]  Epoch: 7	Loss: 0.068697	Acc: 4.6% (460/10000)
[Test]  Epoch: 8	Loss: 0.069946	Acc: 3.6% (365/10000)
[Test]  Epoch: 9	Loss: 0.068155	Acc: 5.4% (544/10000)
[Test]  Epoch: 10	Loss: 0.065027	Acc: 6.9% (693/10000)
[Test]  Epoch: 11	Loss: 0.068670	Acc: 5.9% (588/10000)
[Test]  Epoch: 12	Loss: 0.068604	Acc: 6.2% (620/10000)
[Test]  Epoch: 13	Loss: 0.068194	Acc: 6.4% (641/10000)
[Test]  Epoch: 14	Loss: 0.064623	Acc: 9.0% (896/10000)
[Test]  Epoch: 15	Loss: 0.067050	Acc: 7.6% (764/10000)
[Test]  Epoch: 16	Loss: 0.068179	Acc: 8.0% (799/10000)
[Test]  Epoch: 17	Loss: 0.066386	Acc: 8.2% (822/10000)
[Test]  Epoch: 18	Loss: 0.066828	Acc: 7.9% (788/10000)
[Test]  Epoch: 19	Loss: 0.065099	Acc: 9.1% (907/10000)
[Test]  Epoch: 20	Loss: 0.068306	Acc: 9.2% (923/10000)
[Test]  Epoch: 21	Loss: 0.068562	Acc: 9.2% (921/10000)
[Test]  Epoch: 22	Loss: 0.069725	Acc: 8.4% (842/10000)
[Test]  Epoch: 23	Loss: 0.066714	Acc: 10.5% (1049/10000)
[Test]  Epoch: 24	Loss: 0.069371	Acc: 9.4% (943/10000)
[Test]  Epoch: 25	Loss: 0.070496	Acc: 9.4% (938/10000)
[Test]  Epoch: 26	Loss: 0.068613	Acc: 10.1% (1012/10000)
[Test]  Epoch: 27	Loss: 0.069700	Acc: 10.6% (1060/10000)
[Test]  Epoch: 28	Loss: 0.072561	Acc: 9.1% (912/10000)
[Test]  Epoch: 29	Loss: 0.070242	Acc: 10.8% (1082/10000)
[Test]  Epoch: 30	Loss: 0.071832	Acc: 11.0% (1099/10000)
[Test]  Epoch: 31	Loss: 0.073426	Acc: 10.1% (1007/10000)
[Test]  Epoch: 32	Loss: 0.073400	Acc: 9.9% (991/10000)
[Test]  Epoch: 33	Loss: 0.073189	Acc: 10.2% (1019/10000)
[Test]  Epoch: 34	Loss: 0.073352	Acc: 10.0% (1003/10000)
[Test]  Epoch: 35	Loss: 0.075709	Acc: 10.8% (1077/10000)
[Test]  Epoch: 36	Loss: 0.076017	Acc: 10.4% (1038/10000)
[Test]  Epoch: 37	Loss: 0.076499	Acc: 10.5% (1050/10000)
[Test]  Epoch: 38	Loss: 0.074828	Acc: 11.2% (1121/10000)
[Test]  Epoch: 39	Loss: 0.074923	Acc: 11.4% (1138/10000)
[Test]  Epoch: 40	Loss: 0.074256	Acc: 11.8% (1183/10000)
[Test]  Epoch: 41	Loss: 0.076693	Acc: 11.1% (1112/10000)
[Test]  Epoch: 42	Loss: 0.077067	Acc: 11.9% (1187/10000)
[Test]  Epoch: 43	Loss: 0.075577	Acc: 11.5% (1148/10000)
[Test]  Epoch: 44	Loss: 0.075108	Acc: 11.8% (1182/10000)
[Test]  Epoch: 45	Loss: 0.076513	Acc: 11.2% (1119/10000)
[Test]  Epoch: 46	Loss: 0.076549	Acc: 12.0% (1196/10000)
[Test]  Epoch: 47	Loss: 0.076570	Acc: 12.4% (1240/10000)
[Test]  Epoch: 48	Loss: 0.077853	Acc: 12.3% (1226/10000)
[Test]  Epoch: 49	Loss: 0.079200	Acc: 10.9% (1095/10000)
[Test]  Epoch: 50	Loss: 0.076072	Acc: 12.5% (1246/10000)
[Test]  Epoch: 51	Loss: 0.077172	Acc: 12.4% (1236/10000)
[Test]  Epoch: 52	Loss: 0.078006	Acc: 13.0% (1301/10000)
[Test]  Epoch: 53	Loss: 0.077672	Acc: 12.5% (1253/10000)
[Test]  Epoch: 54	Loss: 0.077883	Acc: 12.1% (1214/10000)
[Test]  Epoch: 55	Loss: 0.078499	Acc: 12.2% (1220/10000)
[Test]  Epoch: 56	Loss: 0.078288	Acc: 12.8% (1282/10000)
[Test]  Epoch: 57	Loss: 0.077131	Acc: 12.5% (1254/10000)
[Test]  Epoch: 58	Loss: 0.078378	Acc: 12.4% (1242/10000)
[Test]  Epoch: 59	Loss: 0.078209	Acc: 12.7% (1271/10000)
[Test]  Epoch: 60	Loss: 0.078659	Acc: 13.2% (1319/10000)
[Test]  Epoch: 61	Loss: 0.077595	Acc: 13.2% (1316/10000)
[Test]  Epoch: 62	Loss: 0.076613	Acc: 13.2% (1319/10000)
[Test]  Epoch: 63	Loss: 0.076850	Acc: 13.4% (1338/10000)
[Test]  Epoch: 64	Loss: 0.076690	Acc: 13.7% (1367/10000)
[Test]  Epoch: 65	Loss: 0.076402	Acc: 13.9% (1386/10000)
[Test]  Epoch: 66	Loss: 0.076761	Acc: 13.8% (1381/10000)
[Test]  Epoch: 67	Loss: 0.075820	Acc: 13.7% (1372/10000)
[Test]  Epoch: 68	Loss: 0.076638	Acc: 13.7% (1369/10000)
[Test]  Epoch: 69	Loss: 0.076500	Acc: 13.6% (1356/10000)
[Test]  Epoch: 70	Loss: 0.076668	Acc: 13.7% (1366/10000)
[Test]  Epoch: 71	Loss: 0.076402	Acc: 13.4% (1344/10000)
[Test]  Epoch: 72	Loss: 0.076664	Acc: 13.9% (1391/10000)
[Test]  Epoch: 73	Loss: 0.076299	Acc: 13.8% (1383/10000)
[Test]  Epoch: 74	Loss: 0.076570	Acc: 13.2% (1323/10000)
[Test]  Epoch: 75	Loss: 0.076644	Acc: 13.6% (1357/10000)
[Test]  Epoch: 76	Loss: 0.076573	Acc: 13.7% (1367/10000)
[Test]  Epoch: 77	Loss: 0.076352	Acc: 13.2% (1319/10000)
[Test]  Epoch: 78	Loss: 0.076431	Acc: 13.2% (1323/10000)
[Test]  Epoch: 79	Loss: 0.076756	Acc: 13.3% (1334/10000)
[Test]  Epoch: 80	Loss: 0.075935	Acc: 13.7% (1367/10000)
[Test]  Epoch: 81	Loss: 0.076171	Acc: 13.7% (1373/10000)
[Test]  Epoch: 82	Loss: 0.076193	Acc: 13.7% (1372/10000)
[Test]  Epoch: 83	Loss: 0.076705	Acc: 13.6% (1361/10000)
[Test]  Epoch: 84	Loss: 0.076412	Acc: 13.6% (1356/10000)
[Test]  Epoch: 85	Loss: 0.076832	Acc: 13.4% (1343/10000)
[Test]  Epoch: 86	Loss: 0.076184	Acc: 13.5% (1349/10000)
[Test]  Epoch: 87	Loss: 0.076408	Acc: 13.8% (1383/10000)
[Test]  Epoch: 88	Loss: 0.076069	Acc: 13.9% (1389/10000)
[Test]  Epoch: 89	Loss: 0.075976	Acc: 13.8% (1384/10000)
[Test]  Epoch: 90	Loss: 0.076193	Acc: 13.7% (1374/10000)
[Test]  Epoch: 91	Loss: 0.076062	Acc: 13.9% (1393/10000)
[Test]  Epoch: 92	Loss: 0.076222	Acc: 13.5% (1351/10000)
[Test]  Epoch: 93	Loss: 0.076264	Acc: 13.6% (1360/10000)
[Test]  Epoch: 94	Loss: 0.076208	Acc: 14.0% (1401/10000)
[Test]  Epoch: 95	Loss: 0.075996	Acc: 14.1% (1409/10000)
[Test]  Epoch: 96	Loss: 0.076252	Acc: 13.8% (1382/10000)
[Test]  Epoch: 97	Loss: 0.076081	Acc: 13.9% (1391/10000)
[Test]  Epoch: 98	Loss: 0.076201	Acc: 13.9% (1388/10000)
[Test]  Epoch: 99	Loss: 0.076430	Acc: 13.9% (1389/10000)
[Test]  Epoch: 100	Loss: 0.076884	Acc: 13.3% (1334/10000)
===========finish==========
['2024-08-19', '04:26:51.010744', '100', 'test', '0.07688369798660279', '13.34', '14.09']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.475004	Acc: 2.0% (202/10000)
[Test]  Epoch: 2	Loss: 0.076991	Acc: 4.9% (487/10000)
[Test]  Epoch: 3	Loss: 0.072580	Acc: 5.6% (563/10000)
[Test]  Epoch: 4	Loss: 0.073272	Acc: 5.2% (524/10000)
[Test]  Epoch: 5	Loss: 0.072126	Acc: 6.7% (670/10000)
[Test]  Epoch: 6	Loss: 0.069735	Acc: 6.5% (648/10000)
[Test]  Epoch: 7	Loss: 0.069354	Acc: 6.9% (693/10000)
[Test]  Epoch: 8	Loss: 0.067672	Acc: 8.0% (801/10000)
[Test]  Epoch: 9	Loss: 0.066743	Acc: 8.9% (888/10000)
[Test]  Epoch: 10	Loss: 0.070311	Acc: 8.9% (894/10000)
[Test]  Epoch: 11	Loss: 0.069625	Acc: 9.1% (913/10000)
[Test]  Epoch: 12	Loss: 0.070551	Acc: 9.6% (964/10000)
[Test]  Epoch: 13	Loss: 0.070392	Acc: 9.0% (903/10000)
[Test]  Epoch: 14	Loss: 0.070344	Acc: 10.1% (1010/10000)
[Test]  Epoch: 15	Loss: 0.070977	Acc: 10.0% (999/10000)
[Test]  Epoch: 16	Loss: 0.073154	Acc: 9.1% (912/10000)
[Test]  Epoch: 17	Loss: 0.073045	Acc: 11.2% (1116/10000)
[Test]  Epoch: 18	Loss: 0.073814	Acc: 9.7% (969/10000)
[Test]  Epoch: 19	Loss: 0.074266	Acc: 9.8% (980/10000)
[Test]  Epoch: 20	Loss: 0.072965	Acc: 11.0% (1098/10000)
[Test]  Epoch: 21	Loss: 0.076909	Acc: 10.3% (1031/10000)
[Test]  Epoch: 22	Loss: 0.072827	Acc: 10.4% (1043/10000)
[Test]  Epoch: 23	Loss: 0.073795	Acc: 12.4% (1245/10000)
[Test]  Epoch: 24	Loss: 0.073779	Acc: 11.3% (1132/10000)
[Test]  Epoch: 25	Loss: 0.075839	Acc: 11.5% (1146/10000)
[Test]  Epoch: 26	Loss: 0.074413	Acc: 11.6% (1159/10000)
[Test]  Epoch: 27	Loss: 0.075337	Acc: 11.3% (1127/10000)
[Test]  Epoch: 28	Loss: 0.079102	Acc: 11.3% (1130/10000)
[Test]  Epoch: 29	Loss: 0.077064	Acc: 12.1% (1213/10000)
[Test]  Epoch: 30	Loss: 0.078146	Acc: 10.9% (1089/10000)
[Test]  Epoch: 31	Loss: 0.075670	Acc: 12.2% (1223/10000)
[Test]  Epoch: 32	Loss: 0.075620	Acc: 12.1% (1214/10000)
[Test]  Epoch: 33	Loss: 0.077661	Acc: 12.1% (1208/10000)
[Test]  Epoch: 34	Loss: 0.076129	Acc: 12.4% (1243/10000)
[Test]  Epoch: 35	Loss: 0.077190	Acc: 12.2% (1220/10000)
[Test]  Epoch: 36	Loss: 0.076542	Acc: 12.6% (1263/10000)
[Test]  Epoch: 37	Loss: 0.076642	Acc: 12.5% (1253/10000)
[Test]  Epoch: 38	Loss: 0.076897	Acc: 12.7% (1265/10000)
[Test]  Epoch: 39	Loss: 0.076091	Acc: 12.1% (1213/10000)
[Test]  Epoch: 40	Loss: 0.076582	Acc: 12.6% (1261/10000)
[Test]  Epoch: 41	Loss: 0.078393	Acc: 12.6% (1255/10000)
[Test]  Epoch: 42	Loss: 0.078681	Acc: 12.9% (1289/10000)
[Test]  Epoch: 43	Loss: 0.078682	Acc: 12.5% (1246/10000)
[Test]  Epoch: 44	Loss: 0.077181	Acc: 13.0% (1302/10000)
[Test]  Epoch: 45	Loss: 0.076572	Acc: 12.2% (1220/10000)
[Test]  Epoch: 46	Loss: 0.076600	Acc: 13.6% (1361/10000)
[Test]  Epoch: 47	Loss: 0.077084	Acc: 12.9% (1289/10000)
[Test]  Epoch: 48	Loss: 0.077304	Acc: 13.2% (1319/10000)
[Test]  Epoch: 49	Loss: 0.077689	Acc: 13.1% (1308/10000)
[Test]  Epoch: 50	Loss: 0.076710	Acc: 13.1% (1311/10000)
[Test]  Epoch: 51	Loss: 0.077651	Acc: 12.8% (1283/10000)
[Test]  Epoch: 52	Loss: 0.077109	Acc: 13.7% (1369/10000)
[Test]  Epoch: 53	Loss: 0.079001	Acc: 13.2% (1320/10000)
[Test]  Epoch: 54	Loss: 0.077213	Acc: 13.4% (1345/10000)
[Test]  Epoch: 55	Loss: 0.077143	Acc: 13.8% (1384/10000)
[Test]  Epoch: 56	Loss: 0.077571	Acc: 13.0% (1304/10000)
[Test]  Epoch: 57	Loss: 0.077930	Acc: 13.2% (1324/10000)
[Test]  Epoch: 58	Loss: 0.076913	Acc: 13.6% (1356/10000)
[Test]  Epoch: 59	Loss: 0.077294	Acc: 13.4% (1344/10000)
[Test]  Epoch: 60	Loss: 0.077699	Acc: 13.4% (1342/10000)
[Test]  Epoch: 61	Loss: 0.077172	Acc: 13.6% (1364/10000)
[Test]  Epoch: 62	Loss: 0.077278	Acc: 13.8% (1385/10000)
[Test]  Epoch: 63	Loss: 0.077071	Acc: 13.8% (1376/10000)
[Test]  Epoch: 64	Loss: 0.076632	Acc: 14.1% (1413/10000)
[Test]  Epoch: 65	Loss: 0.076535	Acc: 14.0% (1400/10000)
[Test]  Epoch: 66	Loss: 0.076967	Acc: 14.0% (1402/10000)
[Test]  Epoch: 67	Loss: 0.076570	Acc: 13.9% (1391/10000)
[Test]  Epoch: 68	Loss: 0.076674	Acc: 13.8% (1375/10000)
[Test]  Epoch: 69	Loss: 0.076481	Acc: 13.6% (1362/10000)
[Test]  Epoch: 70	Loss: 0.076920	Acc: 13.6% (1363/10000)
[Test]  Epoch: 71	Loss: 0.076760	Acc: 14.0% (1403/10000)
[Test]  Epoch: 72	Loss: 0.076408	Acc: 13.9% (1390/10000)
[Test]  Epoch: 73	Loss: 0.076636	Acc: 13.8% (1384/10000)
[Test]  Epoch: 74	Loss: 0.076590	Acc: 13.7% (1374/10000)
[Test]  Epoch: 75	Loss: 0.076431	Acc: 14.2% (1415/10000)
[Test]  Epoch: 76	Loss: 0.076647	Acc: 14.2% (1416/10000)
[Test]  Epoch: 77	Loss: 0.076715	Acc: 13.8% (1377/10000)
[Test]  Epoch: 78	Loss: 0.076771	Acc: 14.0% (1397/10000)
[Test]  Epoch: 79	Loss: 0.076591	Acc: 14.0% (1396/10000)
[Test]  Epoch: 80	Loss: 0.076480	Acc: 13.6% (1360/10000)
[Test]  Epoch: 81	Loss: 0.076489	Acc: 13.9% (1390/10000)
[Test]  Epoch: 82	Loss: 0.076510	Acc: 13.9% (1391/10000)
[Test]  Epoch: 83	Loss: 0.076530	Acc: 14.0% (1396/10000)
[Test]  Epoch: 84	Loss: 0.076284	Acc: 14.0% (1398/10000)
[Test]  Epoch: 85	Loss: 0.076407	Acc: 13.9% (1392/10000)
[Test]  Epoch: 86	Loss: 0.076345	Acc: 13.8% (1378/10000)
[Test]  Epoch: 87	Loss: 0.076301	Acc: 14.1% (1409/10000)
[Test]  Epoch: 88	Loss: 0.076576	Acc: 14.4% (1440/10000)
[Test]  Epoch: 89	Loss: 0.076499	Acc: 13.9% (1395/10000)
[Test]  Epoch: 90	Loss: 0.076605	Acc: 14.0% (1398/10000)
[Test]  Epoch: 91	Loss: 0.076837	Acc: 13.4% (1344/10000)
[Test]  Epoch: 92	Loss: 0.076496	Acc: 14.2% (1419/10000)
[Test]  Epoch: 93	Loss: 0.076376	Acc: 14.2% (1420/10000)
[Test]  Epoch: 94	Loss: 0.076263	Acc: 14.2% (1421/10000)
[Test]  Epoch: 95	Loss: 0.076319	Acc: 14.2% (1420/10000)
[Test]  Epoch: 96	Loss: 0.076296	Acc: 14.2% (1418/10000)
[Test]  Epoch: 97	Loss: 0.076636	Acc: 14.1% (1413/10000)
[Test]  Epoch: 98	Loss: 0.076117	Acc: 14.0% (1404/10000)
[Test]  Epoch: 99	Loss: 0.076885	Acc: 14.1% (1411/10000)
[Test]  Epoch: 100	Loss: 0.076348	Acc: 14.1% (1414/10000)
===========finish==========
['2024-08-19', '04:31:47.007396', '100', 'test', '0.07634763469696045', '14.14', '14.4']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight', 'features.40.weight', 'features.41.weight', 'classifier.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.137319	Acc: 1.0% (100/10000)
[Test]  Epoch: 2	Loss: 0.072166	Acc: 2.5% (245/10000)
[Test]  Epoch: 3	Loss: 0.071731	Acc: 3.0% (303/10000)
[Test]  Epoch: 4	Loss: 0.071309	Acc: 3.3% (333/10000)
[Test]  Epoch: 5	Loss: 0.070301	Acc: 3.5% (352/10000)
[Test]  Epoch: 6	Loss: 0.069602	Acc: 4.4% (440/10000)
[Test]  Epoch: 7	Loss: 0.069374	Acc: 3.3% (326/10000)
[Test]  Epoch: 8	Loss: 0.068664	Acc: 4.9% (493/10000)
[Test]  Epoch: 9	Loss: 0.068290	Acc: 5.4% (543/10000)
[Test]  Epoch: 10	Loss: 0.067842	Acc: 5.3% (526/10000)
[Test]  Epoch: 11	Loss: 0.066953	Acc: 6.1% (613/10000)
[Test]  Epoch: 12	Loss: 0.066715	Acc: 6.6% (664/10000)
[Test]  Epoch: 13	Loss: 0.066173	Acc: 6.4% (642/10000)
[Test]  Epoch: 14	Loss: 0.066044	Acc: 6.8% (682/10000)
[Test]  Epoch: 15	Loss: 0.065036	Acc: 7.9% (786/10000)
[Test]  Epoch: 16	Loss: 0.064626	Acc: 7.7% (770/10000)
[Test]  Epoch: 17	Loss: 0.064958	Acc: 8.1% (814/10000)
[Test]  Epoch: 18	Loss: 0.064339	Acc: 8.0% (795/10000)
[Test]  Epoch: 19	Loss: 0.063532	Acc: 9.5% (949/10000)
[Test]  Epoch: 20	Loss: 0.063695	Acc: 8.6% (864/10000)
[Test]  Epoch: 21	Loss: 0.063422	Acc: 9.1% (914/10000)
[Test]  Epoch: 22	Loss: 0.063247	Acc: 9.4% (942/10000)
[Test]  Epoch: 23	Loss: 0.064010	Acc: 10.1% (1008/10000)
[Test]  Epoch: 24	Loss: 0.063161	Acc: 9.7% (967/10000)
[Test]  Epoch: 25	Loss: 0.063234	Acc: 9.9% (986/10000)
[Test]  Epoch: 26	Loss: 0.063348	Acc: 11.2% (1116/10000)
[Test]  Epoch: 27	Loss: 0.062557	Acc: 10.8% (1079/10000)
[Test]  Epoch: 28	Loss: 0.063488	Acc: 10.3% (1028/10000)
[Test]  Epoch: 29	Loss: 0.064440	Acc: 10.6% (1064/10000)
[Test]  Epoch: 30	Loss: 0.063503	Acc: 10.4% (1039/10000)
[Test]  Epoch: 31	Loss: 0.063365	Acc: 10.7% (1069/10000)
[Test]  Epoch: 32	Loss: 0.062797	Acc: 11.0% (1103/10000)
[Test]  Epoch: 33	Loss: 0.063402	Acc: 10.5% (1047/10000)
[Test]  Epoch: 34	Loss: 0.062949	Acc: 11.8% (1175/10000)
[Test]  Epoch: 35	Loss: 0.065363	Acc: 11.4% (1140/10000)
[Test]  Epoch: 36	Loss: 0.063669	Acc: 10.9% (1087/10000)
[Test]  Epoch: 37	Loss: 0.063948	Acc: 11.4% (1145/10000)
[Test]  Epoch: 38	Loss: 0.063456	Acc: 11.8% (1180/10000)
[Test]  Epoch: 39	Loss: 0.065326	Acc: 12.5% (1246/10000)
[Test]  Epoch: 40	Loss: 0.064532	Acc: 11.9% (1195/10000)
[Test]  Epoch: 41	Loss: 0.064881	Acc: 11.4% (1136/10000)
[Test]  Epoch: 42	Loss: 0.065863	Acc: 11.2% (1118/10000)
[Test]  Epoch: 43	Loss: 0.066357	Acc: 11.7% (1172/10000)
[Test]  Epoch: 44	Loss: 0.065006	Acc: 12.1% (1208/10000)
[Test]  Epoch: 45	Loss: 0.065801	Acc: 11.6% (1157/10000)
[Test]  Epoch: 46	Loss: 0.064858	Acc: 12.4% (1243/10000)
[Test]  Epoch: 47	Loss: 0.066024	Acc: 12.1% (1207/10000)
[Test]  Epoch: 48	Loss: 0.066140	Acc: 12.2% (1225/10000)
[Test]  Epoch: 49	Loss: 0.067326	Acc: 11.8% (1179/10000)
[Test]  Epoch: 50	Loss: 0.066832	Acc: 12.8% (1278/10000)
[Test]  Epoch: 51	Loss: 0.067122	Acc: 12.2% (1224/10000)
[Test]  Epoch: 52	Loss: 0.069821	Acc: 12.2% (1222/10000)
[Test]  Epoch: 53	Loss: 0.068914	Acc: 12.6% (1264/10000)
[Test]  Epoch: 54	Loss: 0.069447	Acc: 12.2% (1218/10000)
[Test]  Epoch: 55	Loss: 0.068651	Acc: 11.7% (1166/10000)
[Test]  Epoch: 56	Loss: 0.070471	Acc: 12.5% (1249/10000)
[Test]  Epoch: 57	Loss: 0.069144	Acc: 12.1% (1213/10000)
[Test]  Epoch: 58	Loss: 0.069855	Acc: 12.6% (1261/10000)
[Test]  Epoch: 59	Loss: 0.069573	Acc: 12.4% (1243/10000)
[Test]  Epoch: 60	Loss: 0.070293	Acc: 11.9% (1186/10000)
[Test]  Epoch: 61	Loss: 0.069116	Acc: 13.0% (1303/10000)
[Test]  Epoch: 62	Loss: 0.069256	Acc: 13.3% (1326/10000)
[Test]  Epoch: 63	Loss: 0.069188	Acc: 13.2% (1324/10000)
[Test]  Epoch: 64	Loss: 0.069176	Acc: 13.2% (1324/10000)
[Test]  Epoch: 65	Loss: 0.069175	Acc: 13.4% (1343/10000)
[Test]  Epoch: 66	Loss: 0.069232	Acc: 13.3% (1331/10000)
[Test]  Epoch: 67	Loss: 0.069081	Acc: 13.4% (1344/10000)
[Test]  Epoch: 68	Loss: 0.069298	Acc: 13.3% (1327/10000)
[Test]  Epoch: 69	Loss: 0.069459	Acc: 13.4% (1338/10000)
[Test]  Epoch: 70	Loss: 0.069187	Acc: 13.1% (1311/10000)
[Test]  Epoch: 71	Loss: 0.069348	Acc: 13.2% (1319/10000)
[Test]  Epoch: 72	Loss: 0.069343	Acc: 13.4% (1336/10000)
[Test]  Epoch: 73	Loss: 0.069104	Acc: 13.8% (1375/10000)
[Test]  Epoch: 74	Loss: 0.069152	Acc: 13.3% (1334/10000)
[Test]  Epoch: 75	Loss: 0.069272	Acc: 13.6% (1363/10000)
[Test]  Epoch: 76	Loss: 0.069197	Acc: 13.5% (1350/10000)
[Test]  Epoch: 77	Loss: 0.069284	Acc: 13.3% (1334/10000)
[Test]  Epoch: 78	Loss: 0.069265	Acc: 13.5% (1349/10000)
[Test]  Epoch: 79	Loss: 0.069420	Acc: 13.4% (1338/10000)
[Test]  Epoch: 80	Loss: 0.069389	Acc: 13.2% (1322/10000)
[Test]  Epoch: 81	Loss: 0.069716	Acc: 13.3% (1329/10000)
[Test]  Epoch: 82	Loss: 0.069373	Acc: 13.2% (1316/10000)
[Test]  Epoch: 83	Loss: 0.069650	Acc: 13.4% (1341/10000)
[Test]  Epoch: 84	Loss: 0.069705	Acc: 13.4% (1344/10000)
[Test]  Epoch: 85	Loss: 0.069472	Acc: 13.5% (1352/10000)
[Test]  Epoch: 86	Loss: 0.069840	Acc: 13.1% (1311/10000)
[Test]  Epoch: 87	Loss: 0.069587	Acc: 13.5% (1353/10000)
[Test]  Epoch: 88	Loss: 0.069722	Acc: 13.6% (1363/10000)
[Test]  Epoch: 89	Loss: 0.069432	Acc: 13.5% (1349/10000)
[Test]  Epoch: 90	Loss: 0.069969	Acc: 13.4% (1345/10000)
[Test]  Epoch: 91	Loss: 0.069767	Acc: 13.3% (1328/10000)
[Test]  Epoch: 92	Loss: 0.069738	Acc: 13.2% (1317/10000)
[Test]  Epoch: 93	Loss: 0.070005	Acc: 13.1% (1314/10000)
[Test]  Epoch: 94	Loss: 0.069681	Acc: 13.4% (1338/10000)
[Test]  Epoch: 95	Loss: 0.069890	Acc: 13.4% (1342/10000)
[Test]  Epoch: 96	Loss: 0.070207	Acc: 13.6% (1361/10000)
[Test]  Epoch: 97	Loss: 0.069932	Acc: 13.6% (1361/10000)
[Test]  Epoch: 98	Loss: 0.069734	Acc: 13.3% (1333/10000)
[Test]  Epoch: 99	Loss: 0.070231	Acc: 13.1% (1309/10000)
[Test]  Epoch: 100	Loss: 0.070093	Acc: 13.5% (1352/10000)
===========finish==========
['2024-08-19', '04:36:43.100589', '100', 'test', '0.07009291908740997', '13.52', '13.75']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-resnet18/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('conv1.weight', 0.0), ('bn1.weight', 0.0), ('bn1.bias', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.bn1.weight', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.bn2.weight', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.bn1.weight', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.bn2.weight', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.bn1.weight', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.bn2.weight', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.downsample.0.weight', 0.0), ('layer2.0.downsample.1.weight', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.bn1.weight', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.bn2.weight', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.bn1.weight', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.bn2.weight', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.downsample.0.weight', 0.0), ('layer3.0.downsample.1.weight', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.bn1.weight', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.bn2.weight', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.bn1.weight', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.bn2.weight', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.downsample.0.weight', 0.0), ('layer4.0.downsample.1.weight', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.bn1.weight', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.bn2.weight', 0.0), ('layer4.1.bn2.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('conv1.weight', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.conv2.weight', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=41  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007631	Acc: 85.2% (8523/10000)
[Test]  Epoch: 2	Loss: 0.007449	Acc: 85.4% (8540/10000)
[Test]  Epoch: 3	Loss: 0.007349	Acc: 85.7% (8570/10000)
[Test]  Epoch: 4	Loss: 0.007242	Acc: 85.8% (8578/10000)
[Test]  Epoch: 5	Loss: 0.007205	Acc: 85.7% (8568/10000)
[Test]  Epoch: 6	Loss: 0.007147	Acc: 85.8% (8581/10000)
[Test]  Epoch: 7	Loss: 0.007090	Acc: 85.8% (8577/10000)
[Test]  Epoch: 8	Loss: 0.007055	Acc: 85.8% (8578/10000)
[Test]  Epoch: 9	Loss: 0.007016	Acc: 85.8% (8575/10000)
[Test]  Epoch: 10	Loss: 0.006982	Acc: 85.9% (8592/10000)
[Test]  Epoch: 11	Loss: 0.006983	Acc: 85.9% (8588/10000)
[Test]  Epoch: 12	Loss: 0.006962	Acc: 85.8% (8585/10000)
[Test]  Epoch: 13	Loss: 0.006898	Acc: 86.0% (8597/10000)
[Test]  Epoch: 14	Loss: 0.006888	Acc: 86.0% (8601/10000)
[Test]  Epoch: 15	Loss: 0.006893	Acc: 86.1% (8609/10000)
[Test]  Epoch: 16	Loss: 0.006864	Acc: 86.0% (8596/10000)
[Test]  Epoch: 17	Loss: 0.006839	Acc: 86.0% (8603/10000)
[Test]  Epoch: 18	Loss: 0.006839	Acc: 86.2% (8618/10000)
[Test]  Epoch: 19	Loss: 0.006824	Acc: 86.1% (8613/10000)
[Test]  Epoch: 20	Loss: 0.006830	Acc: 86.0% (8604/10000)
[Test]  Epoch: 21	Loss: 0.006832	Acc: 85.9% (8592/10000)
[Test]  Epoch: 22	Loss: 0.006834	Acc: 86.0% (8605/10000)
[Test]  Epoch: 23	Loss: 0.006803	Acc: 86.1% (8610/10000)
[Test]  Epoch: 24	Loss: 0.006783	Acc: 86.2% (8619/10000)
[Test]  Epoch: 25	Loss: 0.006787	Acc: 86.2% (8620/10000)
[Test]  Epoch: 26	Loss: 0.006766	Acc: 86.2% (8618/10000)
[Test]  Epoch: 27	Loss: 0.006781	Acc: 86.2% (8615/10000)
[Test]  Epoch: 28	Loss: 0.006743	Acc: 86.2% (8616/10000)
[Test]  Epoch: 29	Loss: 0.006742	Acc: 86.3% (8626/10000)
[Test]  Epoch: 30	Loss: 0.006767	Acc: 86.1% (8612/10000)
[Test]  Epoch: 31	Loss: 0.006741	Acc: 86.2% (8620/10000)
[Test]  Epoch: 32	Loss: 0.006748	Acc: 86.2% (8617/10000)
[Test]  Epoch: 33	Loss: 0.006749	Acc: 86.2% (8619/10000)
[Test]  Epoch: 34	Loss: 0.006722	Acc: 86.2% (8617/10000)
[Test]  Epoch: 35	Loss: 0.006708	Acc: 86.2% (8621/10000)
[Test]  Epoch: 36	Loss: 0.006703	Acc: 86.2% (8616/10000)
[Test]  Epoch: 37	Loss: 0.006734	Acc: 86.1% (8614/10000)
[Test]  Epoch: 38	Loss: 0.006718	Acc: 86.2% (8619/10000)
[Test]  Epoch: 39	Loss: 0.006711	Acc: 86.1% (8611/10000)
[Test]  Epoch: 40	Loss: 0.006706	Acc: 86.1% (8612/10000)
[Test]  Epoch: 41	Loss: 0.006712	Acc: 86.2% (8615/10000)
[Test]  Epoch: 42	Loss: 0.006723	Acc: 86.0% (8604/10000)
[Test]  Epoch: 43	Loss: 0.006701	Acc: 86.2% (8619/10000)
[Test]  Epoch: 44	Loss: 0.006707	Acc: 86.1% (8610/10000)
[Test]  Epoch: 45	Loss: 0.006708	Acc: 86.2% (8618/10000)
[Test]  Epoch: 46	Loss: 0.006688	Acc: 86.2% (8624/10000)
[Test]  Epoch: 47	Loss: 0.006714	Acc: 86.2% (8616/10000)
[Test]  Epoch: 48	Loss: 0.006664	Acc: 86.1% (8613/10000)
[Test]  Epoch: 49	Loss: 0.006704	Acc: 86.2% (8619/10000)
[Test]  Epoch: 50	Loss: 0.006700	Acc: 86.1% (8606/10000)
[Test]  Epoch: 51	Loss: 0.006664	Acc: 86.0% (8603/10000)
[Test]  Epoch: 52	Loss: 0.006654	Acc: 86.2% (8617/10000)
[Test]  Epoch: 53	Loss: 0.006651	Acc: 86.1% (8611/10000)
[Test]  Epoch: 54	Loss: 0.006656	Acc: 86.1% (8607/10000)
[Test]  Epoch: 55	Loss: 0.006655	Acc: 86.1% (8606/10000)
[Test]  Epoch: 56	Loss: 0.006643	Acc: 86.1% (8611/10000)
[Test]  Epoch: 57	Loss: 0.006670	Acc: 86.1% (8609/10000)
[Test]  Epoch: 58	Loss: 0.006673	Acc: 86.1% (8611/10000)
[Test]  Epoch: 59	Loss: 0.006671	Acc: 86.2% (8623/10000)
[Test]  Epoch: 60	Loss: 0.006687	Acc: 86.2% (8619/10000)
[Test]  Epoch: 61	Loss: 0.006671	Acc: 86.1% (8612/10000)
[Test]  Epoch: 62	Loss: 0.006671	Acc: 86.2% (8621/10000)
[Test]  Epoch: 63	Loss: 0.006661	Acc: 86.1% (8614/10000)
[Test]  Epoch: 64	Loss: 0.006645	Acc: 86.1% (8614/10000)
[Test]  Epoch: 65	Loss: 0.006646	Acc: 86.2% (8615/10000)
[Test]  Epoch: 66	Loss: 0.006650	Acc: 86.2% (8616/10000)
[Test]  Epoch: 67	Loss: 0.006641	Acc: 86.2% (8618/10000)
[Test]  Epoch: 68	Loss: 0.006665	Acc: 86.2% (8616/10000)
[Test]  Epoch: 69	Loss: 0.006666	Acc: 86.1% (8614/10000)
[Test]  Epoch: 70	Loss: 0.006656	Acc: 86.2% (8615/10000)
[Test]  Epoch: 71	Loss: 0.006639	Acc: 86.1% (8613/10000)
[Test]  Epoch: 72	Loss: 0.006619	Acc: 86.2% (8623/10000)
[Test]  Epoch: 73	Loss: 0.006636	Acc: 86.0% (8602/10000)
[Test]  Epoch: 74	Loss: 0.006633	Acc: 86.1% (8611/10000)
[Test]  Epoch: 75	Loss: 0.006636	Acc: 86.1% (8614/10000)
[Test]  Epoch: 76	Loss: 0.006645	Acc: 86.1% (8608/10000)
[Test]  Epoch: 77	Loss: 0.006646	Acc: 86.1% (8614/10000)
[Test]  Epoch: 78	Loss: 0.006645	Acc: 86.2% (8615/10000)
[Test]  Epoch: 79	Loss: 0.006657	Acc: 86.1% (8613/10000)
[Test]  Epoch: 80	Loss: 0.006645	Acc: 86.1% (8607/10000)
[Test]  Epoch: 81	Loss: 0.006660	Acc: 86.2% (8615/10000)
[Test]  Epoch: 82	Loss: 0.006657	Acc: 86.1% (8612/10000)
[Test]  Epoch: 83	Loss: 0.006633	Acc: 86.2% (8615/10000)
[Test]  Epoch: 84	Loss: 0.006639	Acc: 86.2% (8616/10000)
[Test]  Epoch: 85	Loss: 0.006646	Acc: 86.1% (8614/10000)
[Test]  Epoch: 86	Loss: 0.006642	Acc: 86.2% (8621/10000)
[Test]  Epoch: 87	Loss: 0.006640	Acc: 86.1% (8607/10000)
[Test]  Epoch: 88	Loss: 0.006632	Acc: 86.1% (8613/10000)
[Test]  Epoch: 89	Loss: 0.006630	Acc: 86.2% (8616/10000)
[Test]  Epoch: 90	Loss: 0.006632	Acc: 86.2% (8616/10000)
[Test]  Epoch: 91	Loss: 0.006638	Acc: 86.1% (8614/10000)
[Test]  Epoch: 92	Loss: 0.006629	Acc: 86.3% (8627/10000)
[Test]  Epoch: 93	Loss: 0.006631	Acc: 86.1% (8613/10000)
[Test]  Epoch: 94	Loss: 0.006633	Acc: 86.2% (8619/10000)
[Test]  Epoch: 95	Loss: 0.006625	Acc: 86.2% (8616/10000)
[Test]  Epoch: 96	Loss: 0.006624	Acc: 86.2% (8624/10000)
[Test]  Epoch: 97	Loss: 0.006639	Acc: 86.2% (8615/10000)
[Test]  Epoch: 98	Loss: 0.006652	Acc: 86.0% (8601/10000)
[Test]  Epoch: 99	Loss: 0.006630	Acc: 86.1% (8611/10000)
[Test]  Epoch: 100	Loss: 0.006650	Acc: 86.2% (8616/10000)
===========finish==========
['2024-08-19', '04:42:23.717794', '100', 'test', '0.006649573869258165', '86.16', '86.27']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.015373	Acc: 71.7% (7172/10000)
[Test]  Epoch: 2	Loss: 0.012871	Acc: 75.0% (7502/10000)
[Test]  Epoch: 3	Loss: 0.012370	Acc: 75.9% (7587/10000)
[Test]  Epoch: 4	Loss: 0.012234	Acc: 76.4% (7639/10000)
[Test]  Epoch: 5	Loss: 0.012081	Acc: 76.5% (7647/10000)
[Test]  Epoch: 6	Loss: 0.012039	Acc: 76.6% (7656/10000)
[Test]  Epoch: 7	Loss: 0.011964	Acc: 76.7% (7673/10000)
[Test]  Epoch: 8	Loss: 0.011886	Acc: 76.8% (7677/10000)
[Test]  Epoch: 9	Loss: 0.011838	Acc: 76.8% (7681/10000)
[Test]  Epoch: 10	Loss: 0.011841	Acc: 77.0% (7696/10000)
[Test]  Epoch: 11	Loss: 0.011764	Acc: 77.1% (7712/10000)
[Test]  Epoch: 12	Loss: 0.011716	Acc: 77.1% (7706/10000)
[Test]  Epoch: 13	Loss: 0.011590	Acc: 77.1% (7711/10000)
[Test]  Epoch: 14	Loss: 0.011678	Acc: 77.0% (7697/10000)
[Test]  Epoch: 15	Loss: 0.011732	Acc: 76.9% (7693/10000)
[Test]  Epoch: 16	Loss: 0.011626	Acc: 77.3% (7731/10000)
[Test]  Epoch: 17	Loss: 0.011540	Acc: 77.3% (7735/10000)
[Test]  Epoch: 18	Loss: 0.011528	Acc: 77.3% (7729/10000)
[Test]  Epoch: 19	Loss: 0.011411	Acc: 77.5% (7750/10000)
[Test]  Epoch: 20	Loss: 0.011418	Acc: 77.2% (7717/10000)
[Test]  Epoch: 21	Loss: 0.011405	Acc: 77.4% (7738/10000)
[Test]  Epoch: 22	Loss: 0.011414	Acc: 77.2% (7720/10000)
[Test]  Epoch: 23	Loss: 0.011331	Acc: 77.6% (7760/10000)
[Test]  Epoch: 24	Loss: 0.011364	Acc: 77.3% (7731/10000)
[Test]  Epoch: 25	Loss: 0.011464	Acc: 77.3% (7727/10000)
[Test]  Epoch: 26	Loss: 0.011330	Acc: 77.6% (7757/10000)
[Test]  Epoch: 27	Loss: 0.011341	Acc: 77.3% (7733/10000)
[Test]  Epoch: 28	Loss: 0.011260	Acc: 77.7% (7767/10000)
[Test]  Epoch: 29	Loss: 0.011258	Acc: 77.6% (7764/10000)
[Test]  Epoch: 30	Loss: 0.011258	Acc: 77.2% (7722/10000)
[Test]  Epoch: 31	Loss: 0.011233	Acc: 77.6% (7762/10000)
[Test]  Epoch: 32	Loss: 0.011238	Acc: 77.7% (7767/10000)
[Test]  Epoch: 33	Loss: 0.011210	Acc: 77.5% (7751/10000)
[Test]  Epoch: 34	Loss: 0.011205	Acc: 77.6% (7759/10000)
[Test]  Epoch: 35	Loss: 0.011165	Acc: 77.8% (7776/10000)
[Test]  Epoch: 36	Loss: 0.011139	Acc: 77.6% (7758/10000)
[Test]  Epoch: 37	Loss: 0.011160	Acc: 77.6% (7762/10000)
[Test]  Epoch: 38	Loss: 0.011249	Acc: 77.5% (7748/10000)
[Test]  Epoch: 39	Loss: 0.011116	Acc: 77.5% (7750/10000)
[Test]  Epoch: 40	Loss: 0.011117	Acc: 77.7% (7765/10000)
[Test]  Epoch: 41	Loss: 0.011139	Acc: 77.5% (7754/10000)
[Test]  Epoch: 42	Loss: 0.011139	Acc: 77.5% (7753/10000)
[Test]  Epoch: 43	Loss: 0.011053	Acc: 77.6% (7763/10000)
[Test]  Epoch: 44	Loss: 0.011057	Acc: 77.6% (7760/10000)
[Test]  Epoch: 45	Loss: 0.011111	Acc: 77.7% (7765/10000)
[Test]  Epoch: 46	Loss: 0.011043	Acc: 77.6% (7758/10000)
[Test]  Epoch: 47	Loss: 0.011031	Acc: 77.7% (7767/10000)
[Test]  Epoch: 48	Loss: 0.011008	Acc: 77.8% (7782/10000)
[Test]  Epoch: 49	Loss: 0.011098	Acc: 77.7% (7766/10000)
[Test]  Epoch: 50	Loss: 0.011068	Acc: 77.6% (7761/10000)
[Test]  Epoch: 51	Loss: 0.011018	Acc: 77.8% (7776/10000)
[Test]  Epoch: 52	Loss: 0.010992	Acc: 77.8% (7780/10000)
[Test]  Epoch: 53	Loss: 0.010959	Acc: 77.8% (7778/10000)
[Test]  Epoch: 54	Loss: 0.010997	Acc: 77.7% (7770/10000)
[Test]  Epoch: 55	Loss: 0.010991	Acc: 77.9% (7790/10000)
[Test]  Epoch: 56	Loss: 0.010947	Acc: 77.9% (7790/10000)
[Test]  Epoch: 57	Loss: 0.010963	Acc: 77.8% (7776/10000)
[Test]  Epoch: 58	Loss: 0.010932	Acc: 78.0% (7796/10000)
[Test]  Epoch: 59	Loss: 0.010932	Acc: 77.8% (7779/10000)
[Test]  Epoch: 60	Loss: 0.010976	Acc: 77.9% (7790/10000)
[Test]  Epoch: 61	Loss: 0.010969	Acc: 77.8% (7783/10000)
[Test]  Epoch: 62	Loss: 0.010989	Acc: 77.9% (7787/10000)
[Test]  Epoch: 63	Loss: 0.010953	Acc: 77.9% (7788/10000)
[Test]  Epoch: 64	Loss: 0.010937	Acc: 77.9% (7788/10000)
[Test]  Epoch: 65	Loss: 0.010944	Acc: 77.9% (7786/10000)
[Test]  Epoch: 66	Loss: 0.010922	Acc: 77.9% (7793/10000)
[Test]  Epoch: 67	Loss: 0.010928	Acc: 77.8% (7776/10000)
[Test]  Epoch: 68	Loss: 0.010940	Acc: 78.0% (7797/10000)
[Test]  Epoch: 69	Loss: 0.010967	Acc: 78.0% (7798/10000)
[Test]  Epoch: 70	Loss: 0.010933	Acc: 77.8% (7781/10000)
[Test]  Epoch: 71	Loss: 0.010901	Acc: 77.9% (7791/10000)
[Test]  Epoch: 72	Loss: 0.010912	Acc: 77.9% (7788/10000)
[Test]  Epoch: 73	Loss: 0.010889	Acc: 77.8% (7784/10000)
[Test]  Epoch: 74	Loss: 0.010899	Acc: 77.8% (7783/10000)
[Test]  Epoch: 75	Loss: 0.010908	Acc: 78.0% (7801/10000)
[Test]  Epoch: 76	Loss: 0.010881	Acc: 77.9% (7791/10000)
[Test]  Epoch: 77	Loss: 0.010905	Acc: 77.9% (7794/10000)
[Test]  Epoch: 78	Loss: 0.010934	Acc: 77.9% (7788/10000)
[Test]  Epoch: 79	Loss: 0.010895	Acc: 77.9% (7792/10000)
[Test]  Epoch: 80	Loss: 0.010912	Acc: 77.8% (7785/10000)
[Test]  Epoch: 81	Loss: 0.010930	Acc: 77.9% (7791/10000)
[Test]  Epoch: 82	Loss: 0.010930	Acc: 77.9% (7786/10000)
[Test]  Epoch: 83	Loss: 0.010917	Acc: 77.8% (7782/10000)
[Test]  Epoch: 84	Loss: 0.010934	Acc: 77.9% (7788/10000)
[Test]  Epoch: 85	Loss: 0.010904	Acc: 77.8% (7785/10000)
[Test]  Epoch: 86	Loss: 0.010910	Acc: 77.8% (7783/10000)
[Test]  Epoch: 87	Loss: 0.010876	Acc: 77.9% (7787/10000)
[Test]  Epoch: 88	Loss: 0.010896	Acc: 77.9% (7792/10000)
[Test]  Epoch: 89	Loss: 0.010927	Acc: 77.9% (7787/10000)
[Test]  Epoch: 90	Loss: 0.010905	Acc: 77.9% (7786/10000)
[Test]  Epoch: 91	Loss: 0.010894	Acc: 78.0% (7801/10000)
[Test]  Epoch: 92	Loss: 0.010896	Acc: 77.9% (7794/10000)
[Test]  Epoch: 93	Loss: 0.010896	Acc: 77.9% (7789/10000)
[Test]  Epoch: 94	Loss: 0.010904	Acc: 77.9% (7786/10000)
[Test]  Epoch: 95	Loss: 0.010900	Acc: 77.9% (7786/10000)
[Test]  Epoch: 96	Loss: 0.010895	Acc: 77.8% (7785/10000)
[Test]  Epoch: 97	Loss: 0.010914	Acc: 77.8% (7782/10000)
[Test]  Epoch: 98	Loss: 0.010925	Acc: 77.9% (7790/10000)
[Test]  Epoch: 99	Loss: 0.010885	Acc: 78.0% (7799/10000)
[Test]  Epoch: 100	Loss: 0.010891	Acc: 77.9% (7794/10000)
===========finish==========
['2024-08-19', '04:46:13.138800', '100', 'test', '0.010891017922759056', '77.94', '78.01']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.018448	Acc: 64.9% (6488/10000)
[Test]  Epoch: 2	Loss: 0.014756	Acc: 70.5% (7052/10000)
[Test]  Epoch: 3	Loss: 0.014152	Acc: 71.3% (7132/10000)
[Test]  Epoch: 4	Loss: 0.014020	Acc: 72.0% (7205/10000)
[Test]  Epoch: 5	Loss: 0.014126	Acc: 72.2% (7220/10000)
[Test]  Epoch: 6	Loss: 0.013968	Acc: 72.3% (7230/10000)
[Test]  Epoch: 7	Loss: 0.013948	Acc: 72.4% (7243/10000)
[Test]  Epoch: 8	Loss: 0.013925	Acc: 72.7% (7265/10000)
[Test]  Epoch: 9	Loss: 0.013894	Acc: 72.7% (7269/10000)
[Test]  Epoch: 10	Loss: 0.013852	Acc: 72.8% (7279/10000)
[Test]  Epoch: 11	Loss: 0.013783	Acc: 72.8% (7278/10000)
[Test]  Epoch: 12	Loss: 0.013670	Acc: 72.7% (7267/10000)
[Test]  Epoch: 13	Loss: 0.013705	Acc: 72.9% (7290/10000)
[Test]  Epoch: 14	Loss: 0.013677	Acc: 72.8% (7283/10000)
[Test]  Epoch: 15	Loss: 0.013605	Acc: 73.1% (7306/10000)
[Test]  Epoch: 16	Loss: 0.013528	Acc: 73.2% (7315/10000)
[Test]  Epoch: 17	Loss: 0.013518	Acc: 73.2% (7323/10000)
[Test]  Epoch: 18	Loss: 0.013520	Acc: 73.2% (7320/10000)
[Test]  Epoch: 19	Loss: 0.013416	Acc: 73.3% (7334/10000)
[Test]  Epoch: 20	Loss: 0.013452	Acc: 73.4% (7344/10000)
[Test]  Epoch: 21	Loss: 0.013406	Acc: 73.4% (7339/10000)
[Test]  Epoch: 22	Loss: 0.013405	Acc: 73.4% (7341/10000)
[Test]  Epoch: 23	Loss: 0.013320	Acc: 73.4% (7344/10000)
[Test]  Epoch: 24	Loss: 0.013307	Acc: 73.6% (7362/10000)
[Test]  Epoch: 25	Loss: 0.013488	Acc: 73.3% (7332/10000)
[Test]  Epoch: 26	Loss: 0.013333	Acc: 73.4% (7343/10000)
[Test]  Epoch: 27	Loss: 0.013299	Acc: 73.5% (7346/10000)
[Test]  Epoch: 28	Loss: 0.013223	Acc: 73.7% (7370/10000)
[Test]  Epoch: 29	Loss: 0.013185	Acc: 73.8% (7375/10000)
[Test]  Epoch: 30	Loss: 0.013252	Acc: 73.7% (7370/10000)
[Test]  Epoch: 31	Loss: 0.013205	Acc: 73.5% (7355/10000)
[Test]  Epoch: 32	Loss: 0.013184	Acc: 73.9% (7387/10000)
[Test]  Epoch: 33	Loss: 0.013155	Acc: 73.8% (7375/10000)
[Test]  Epoch: 34	Loss: 0.013239	Acc: 73.7% (7370/10000)
[Test]  Epoch: 35	Loss: 0.013171	Acc: 73.7% (7370/10000)
[Test]  Epoch: 36	Loss: 0.013117	Acc: 73.7% (7373/10000)
[Test]  Epoch: 37	Loss: 0.013078	Acc: 73.7% (7366/10000)
[Test]  Epoch: 38	Loss: 0.013121	Acc: 73.6% (7363/10000)
[Test]  Epoch: 39	Loss: 0.013087	Acc: 73.8% (7375/10000)
[Test]  Epoch: 40	Loss: 0.013035	Acc: 73.9% (7390/10000)
[Test]  Epoch: 41	Loss: 0.013030	Acc: 73.8% (7381/10000)
[Test]  Epoch: 42	Loss: 0.013076	Acc: 73.8% (7375/10000)
[Test]  Epoch: 43	Loss: 0.012949	Acc: 73.9% (7394/10000)
[Test]  Epoch: 44	Loss: 0.012944	Acc: 73.9% (7390/10000)
[Test]  Epoch: 45	Loss: 0.013002	Acc: 74.0% (7401/10000)
[Test]  Epoch: 46	Loss: 0.012949	Acc: 73.8% (7381/10000)
[Test]  Epoch: 47	Loss: 0.012900	Acc: 74.0% (7405/10000)
[Test]  Epoch: 48	Loss: 0.012919	Acc: 74.0% (7402/10000)
[Test]  Epoch: 49	Loss: 0.013011	Acc: 73.9% (7391/10000)
[Test]  Epoch: 50	Loss: 0.012891	Acc: 74.0% (7398/10000)
[Test]  Epoch: 51	Loss: 0.012899	Acc: 73.9% (7386/10000)
[Test]  Epoch: 52	Loss: 0.012946	Acc: 74.2% (7417/10000)
[Test]  Epoch: 53	Loss: 0.012873	Acc: 74.0% (7401/10000)
[Test]  Epoch: 54	Loss: 0.012834	Acc: 74.2% (7415/10000)
[Test]  Epoch: 55	Loss: 0.012925	Acc: 74.0% (7399/10000)
[Test]  Epoch: 56	Loss: 0.012899	Acc: 74.1% (7410/10000)
[Test]  Epoch: 57	Loss: 0.012854	Acc: 74.0% (7398/10000)
[Test]  Epoch: 58	Loss: 0.012828	Acc: 74.1% (7412/10000)
[Test]  Epoch: 59	Loss: 0.012836	Acc: 74.1% (7412/10000)
[Test]  Epoch: 60	Loss: 0.012863	Acc: 74.0% (7403/10000)
[Test]  Epoch: 61	Loss: 0.012816	Acc: 74.2% (7418/10000)
[Test]  Epoch: 62	Loss: 0.012848	Acc: 74.1% (7414/10000)
[Test]  Epoch: 63	Loss: 0.012796	Acc: 74.0% (7397/10000)
[Test]  Epoch: 64	Loss: 0.012802	Acc: 74.2% (7418/10000)
[Test]  Epoch: 65	Loss: 0.012788	Acc: 74.1% (7413/10000)
[Test]  Epoch: 66	Loss: 0.012781	Acc: 74.2% (7415/10000)
[Test]  Epoch: 67	Loss: 0.012813	Acc: 74.1% (7406/10000)
[Test]  Epoch: 68	Loss: 0.012812	Acc: 74.1% (7409/10000)
[Test]  Epoch: 69	Loss: 0.012822	Acc: 74.2% (7422/10000)
[Test]  Epoch: 70	Loss: 0.012798	Acc: 74.0% (7401/10000)
[Test]  Epoch: 71	Loss: 0.012796	Acc: 74.0% (7402/10000)
[Test]  Epoch: 72	Loss: 0.012750	Acc: 74.2% (7416/10000)
[Test]  Epoch: 73	Loss: 0.012745	Acc: 74.1% (7412/10000)
[Test]  Epoch: 74	Loss: 0.012788	Acc: 74.0% (7400/10000)
[Test]  Epoch: 75	Loss: 0.012775	Acc: 74.1% (7412/10000)
[Test]  Epoch: 76	Loss: 0.012763	Acc: 74.0% (7405/10000)
[Test]  Epoch: 77	Loss: 0.012789	Acc: 74.2% (7417/10000)
[Test]  Epoch: 78	Loss: 0.012809	Acc: 74.1% (7406/10000)
[Test]  Epoch: 79	Loss: 0.012765	Acc: 74.1% (7411/10000)
[Test]  Epoch: 80	Loss: 0.012765	Acc: 74.1% (7410/10000)
[Test]  Epoch: 81	Loss: 0.012786	Acc: 74.2% (7419/10000)
[Test]  Epoch: 82	Loss: 0.012823	Acc: 74.2% (7418/10000)
[Test]  Epoch: 83	Loss: 0.012768	Acc: 74.0% (7402/10000)
[Test]  Epoch: 84	Loss: 0.012786	Acc: 74.2% (7416/10000)
[Test]  Epoch: 85	Loss: 0.012786	Acc: 74.1% (7411/10000)
[Test]  Epoch: 86	Loss: 0.012784	Acc: 74.1% (7410/10000)
[Test]  Epoch: 87	Loss: 0.012745	Acc: 74.2% (7415/10000)
[Test]  Epoch: 88	Loss: 0.012791	Acc: 74.1% (7411/10000)
[Test]  Epoch: 89	Loss: 0.012842	Acc: 74.2% (7417/10000)
[Test]  Epoch: 90	Loss: 0.012787	Acc: 74.2% (7417/10000)
[Test]  Epoch: 91	Loss: 0.012770	Acc: 74.1% (7412/10000)
[Test]  Epoch: 92	Loss: 0.012777	Acc: 74.0% (7404/10000)
[Test]  Epoch: 93	Loss: 0.012779	Acc: 74.1% (7411/10000)
[Test]  Epoch: 94	Loss: 0.012779	Acc: 74.2% (7425/10000)
[Test]  Epoch: 95	Loss: 0.012784	Acc: 74.2% (7425/10000)
[Test]  Epoch: 96	Loss: 0.012780	Acc: 74.2% (7418/10000)
[Test]  Epoch: 97	Loss: 0.012805	Acc: 74.1% (7413/10000)
[Test]  Epoch: 98	Loss: 0.012804	Acc: 74.1% (7413/10000)
[Test]  Epoch: 99	Loss: 0.012775	Acc: 74.2% (7423/10000)
[Test]  Epoch: 100	Loss: 0.012772	Acc: 74.2% (7421/10000)
===========finish==========
['2024-08-19', '04:49:56.350662', '100', 'test', '0.01277185760140419', '74.21', '74.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.023032	Acc: 55.0% (5502/10000)
[Test]  Epoch: 2	Loss: 0.020190	Acc: 59.0% (5898/10000)
[Test]  Epoch: 3	Loss: 0.019630	Acc: 60.2% (6019/10000)
[Test]  Epoch: 4	Loss: 0.019797	Acc: 60.6% (6056/10000)
[Test]  Epoch: 5	Loss: 0.019919	Acc: 60.8% (6081/10000)
[Test]  Epoch: 6	Loss: 0.019976	Acc: 61.0% (6097/10000)
[Test]  Epoch: 7	Loss: 0.020020	Acc: 61.0% (6097/10000)
[Test]  Epoch: 8	Loss: 0.020026	Acc: 61.2% (6117/10000)
[Test]  Epoch: 9	Loss: 0.019977	Acc: 61.1% (6106/10000)
[Test]  Epoch: 10	Loss: 0.019856	Acc: 61.5% (6145/10000)
[Test]  Epoch: 11	Loss: 0.019917	Acc: 61.5% (6145/10000)
[Test]  Epoch: 12	Loss: 0.019773	Acc: 61.4% (6141/10000)
[Test]  Epoch: 13	Loss: 0.019750	Acc: 61.5% (6154/10000)
[Test]  Epoch: 14	Loss: 0.019716	Acc: 61.6% (6163/10000)
[Test]  Epoch: 15	Loss: 0.019712	Acc: 61.6% (6162/10000)
[Test]  Epoch: 16	Loss: 0.019725	Acc: 61.6% (6157/10000)
[Test]  Epoch: 17	Loss: 0.019734	Acc: 62.0% (6203/10000)
[Test]  Epoch: 18	Loss: 0.019709	Acc: 61.7% (6167/10000)
[Test]  Epoch: 19	Loss: 0.019602	Acc: 61.9% (6188/10000)
[Test]  Epoch: 20	Loss: 0.019568	Acc: 61.8% (6183/10000)
[Test]  Epoch: 21	Loss: 0.019708	Acc: 61.7% (6172/10000)
[Test]  Epoch: 22	Loss: 0.019506	Acc: 62.0% (6205/10000)
[Test]  Epoch: 23	Loss: 0.019586	Acc: 62.1% (6215/10000)
[Test]  Epoch: 24	Loss: 0.019609	Acc: 62.1% (6208/10000)
[Test]  Epoch: 25	Loss: 0.019590	Acc: 62.0% (6205/10000)
[Test]  Epoch: 26	Loss: 0.019531	Acc: 62.0% (6196/10000)
[Test]  Epoch: 27	Loss: 0.019543	Acc: 62.0% (6204/10000)
[Test]  Epoch: 28	Loss: 0.019444	Acc: 62.2% (6217/10000)
[Test]  Epoch: 29	Loss: 0.019373	Acc: 62.2% (6222/10000)
[Test]  Epoch: 30	Loss: 0.019404	Acc: 62.3% (6233/10000)
[Test]  Epoch: 31	Loss: 0.019395	Acc: 62.2% (6225/10000)
[Test]  Epoch: 32	Loss: 0.019487	Acc: 62.4% (6237/10000)
[Test]  Epoch: 33	Loss: 0.019350	Acc: 62.6% (6256/10000)
[Test]  Epoch: 34	Loss: 0.019377	Acc: 62.4% (6235/10000)
[Test]  Epoch: 35	Loss: 0.019401	Acc: 62.4% (6242/10000)
[Test]  Epoch: 36	Loss: 0.019372	Acc: 62.4% (6239/10000)
[Test]  Epoch: 37	Loss: 0.019316	Acc: 62.6% (6262/10000)
[Test]  Epoch: 38	Loss: 0.019379	Acc: 62.2% (6221/10000)
[Test]  Epoch: 39	Loss: 0.019249	Acc: 62.5% (6250/10000)
[Test]  Epoch: 40	Loss: 0.019162	Acc: 62.6% (6257/10000)
[Test]  Epoch: 41	Loss: 0.019231	Acc: 62.6% (6258/10000)
[Test]  Epoch: 42	Loss: 0.019203	Acc: 62.6% (6259/10000)
[Test]  Epoch: 43	Loss: 0.019163	Acc: 62.6% (6260/10000)
[Test]  Epoch: 44	Loss: 0.019174	Acc: 62.5% (6251/10000)
[Test]  Epoch: 45	Loss: 0.019275	Acc: 62.5% (6254/10000)
[Test]  Epoch: 46	Loss: 0.019157	Acc: 62.6% (6264/10000)
[Test]  Epoch: 47	Loss: 0.019232	Acc: 62.8% (6277/10000)
[Test]  Epoch: 48	Loss: 0.019200	Acc: 62.7% (6272/10000)
[Test]  Epoch: 49	Loss: 0.019161	Acc: 62.7% (6267/10000)
[Test]  Epoch: 50	Loss: 0.019172	Acc: 62.5% (6245/10000)
[Test]  Epoch: 51	Loss: 0.019190	Acc: 62.6% (6265/10000)
[Test]  Epoch: 52	Loss: 0.019091	Acc: 62.7% (6273/10000)
[Test]  Epoch: 53	Loss: 0.019155	Acc: 62.6% (6263/10000)
[Test]  Epoch: 54	Loss: 0.019163	Acc: 62.7% (6267/10000)
[Test]  Epoch: 55	Loss: 0.019146	Acc: 62.5% (6255/10000)
[Test]  Epoch: 56	Loss: 0.019191	Acc: 62.5% (6250/10000)
[Test]  Epoch: 57	Loss: 0.019157	Acc: 62.6% (6259/10000)
[Test]  Epoch: 58	Loss: 0.019141	Acc: 62.6% (6262/10000)
[Test]  Epoch: 59	Loss: 0.019121	Acc: 62.8% (6284/10000)
[Test]  Epoch: 60	Loss: 0.019176	Acc: 62.6% (6261/10000)
[Test]  Epoch: 61	Loss: 0.019172	Acc: 62.5% (6247/10000)
[Test]  Epoch: 62	Loss: 0.019153	Acc: 62.5% (6251/10000)
[Test]  Epoch: 63	Loss: 0.019158	Acc: 62.5% (6251/10000)
[Test]  Epoch: 64	Loss: 0.019085	Acc: 62.5% (6251/10000)
[Test]  Epoch: 65	Loss: 0.019079	Acc: 62.7% (6268/10000)
[Test]  Epoch: 66	Loss: 0.019095	Acc: 62.6% (6263/10000)
[Test]  Epoch: 67	Loss: 0.019152	Acc: 62.5% (6247/10000)
[Test]  Epoch: 68	Loss: 0.019113	Acc: 62.7% (6266/10000)
[Test]  Epoch: 69	Loss: 0.019110	Acc: 62.7% (6267/10000)
[Test]  Epoch: 70	Loss: 0.019129	Acc: 62.7% (6267/10000)
[Test]  Epoch: 71	Loss: 0.019082	Acc: 62.5% (6250/10000)
[Test]  Epoch: 72	Loss: 0.019080	Acc: 62.6% (6256/10000)
[Test]  Epoch: 73	Loss: 0.019052	Acc: 62.6% (6260/10000)
[Test]  Epoch: 74	Loss: 0.019066	Acc: 62.7% (6266/10000)
[Test]  Epoch: 75	Loss: 0.019020	Acc: 62.7% (6271/10000)
[Test]  Epoch: 76	Loss: 0.019046	Acc: 62.8% (6279/10000)
[Test]  Epoch: 77	Loss: 0.019116	Acc: 62.6% (6256/10000)
[Test]  Epoch: 78	Loss: 0.019084	Acc: 62.8% (6279/10000)
[Test]  Epoch: 79	Loss: 0.019057	Acc: 62.9% (6286/10000)
[Test]  Epoch: 80	Loss: 0.019049	Acc: 62.8% (6277/10000)
[Test]  Epoch: 81	Loss: 0.019127	Acc: 62.7% (6267/10000)
[Test]  Epoch: 82	Loss: 0.019121	Acc: 62.6% (6261/10000)
[Test]  Epoch: 83	Loss: 0.019042	Acc: 62.8% (6275/10000)
[Test]  Epoch: 84	Loss: 0.019101	Acc: 62.7% (6270/10000)
[Test]  Epoch: 85	Loss: 0.019102	Acc: 62.8% (6280/10000)
[Test]  Epoch: 86	Loss: 0.019074	Acc: 62.9% (6289/10000)
[Test]  Epoch: 87	Loss: 0.019074	Acc: 62.9% (6287/10000)
[Test]  Epoch: 88	Loss: 0.019106	Acc: 62.7% (6271/10000)
[Test]  Epoch: 89	Loss: 0.019133	Acc: 62.8% (6281/10000)
[Test]  Epoch: 90	Loss: 0.019075	Acc: 62.8% (6280/10000)
[Test]  Epoch: 91	Loss: 0.019098	Acc: 62.7% (6272/10000)
[Test]  Epoch: 92	Loss: 0.019089	Acc: 62.8% (6280/10000)
[Test]  Epoch: 93	Loss: 0.019076	Acc: 62.7% (6274/10000)
[Test]  Epoch: 94	Loss: 0.019074	Acc: 62.8% (6282/10000)
[Test]  Epoch: 95	Loss: 0.019103	Acc: 62.9% (6287/10000)
[Test]  Epoch: 96	Loss: 0.019095	Acc: 62.6% (6264/10000)
[Test]  Epoch: 97	Loss: 0.019115	Acc: 62.8% (6277/10000)
[Test]  Epoch: 98	Loss: 0.019170	Acc: 62.6% (6260/10000)
[Test]  Epoch: 99	Loss: 0.019076	Acc: 62.7% (6273/10000)
[Test]  Epoch: 100	Loss: 0.019104	Acc: 62.7% (6269/10000)
===========finish==========
['2024-08-19', '04:53:41.826305', '100', 'test', '0.01910444920063019', '62.69', '62.89']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.032349	Acc: 42.4% (4235/10000)
[Test]  Epoch: 2	Loss: 0.022149	Acc: 53.7% (5366/10000)
[Test]  Epoch: 3	Loss: 0.022103	Acc: 55.5% (5546/10000)
[Test]  Epoch: 4	Loss: 0.022733	Acc: 55.7% (5571/10000)
[Test]  Epoch: 5	Loss: 0.023085	Acc: 55.2% (5518/10000)
[Test]  Epoch: 6	Loss: 0.023056	Acc: 55.6% (5559/10000)
[Test]  Epoch: 7	Loss: 0.023119	Acc: 55.8% (5584/10000)
[Test]  Epoch: 8	Loss: 0.023340	Acc: 55.5% (5546/10000)
[Test]  Epoch: 9	Loss: 0.023276	Acc: 55.9% (5586/10000)
[Test]  Epoch: 10	Loss: 0.023233	Acc: 56.3% (5631/10000)
[Test]  Epoch: 11	Loss: 0.023060	Acc: 56.2% (5623/10000)
[Test]  Epoch: 12	Loss: 0.023182	Acc: 56.1% (5615/10000)
[Test]  Epoch: 13	Loss: 0.023080	Acc: 56.5% (5653/10000)
[Test]  Epoch: 14	Loss: 0.022946	Acc: 56.5% (5651/10000)
[Test]  Epoch: 15	Loss: 0.023007	Acc: 56.3% (5627/10000)
[Test]  Epoch: 16	Loss: 0.023043	Acc: 56.4% (5636/10000)
[Test]  Epoch: 17	Loss: 0.022965	Acc: 56.8% (5683/10000)
[Test]  Epoch: 18	Loss: 0.022933	Acc: 57.0% (5697/10000)
[Test]  Epoch: 19	Loss: 0.022700	Acc: 56.8% (5683/10000)
[Test]  Epoch: 20	Loss: 0.022716	Acc: 57.0% (5696/10000)
[Test]  Epoch: 21	Loss: 0.022886	Acc: 56.6% (5663/10000)
[Test]  Epoch: 22	Loss: 0.022776	Acc: 56.9% (5686/10000)
[Test]  Epoch: 23	Loss: 0.022776	Acc: 57.2% (5716/10000)
[Test]  Epoch: 24	Loss: 0.022715	Acc: 56.9% (5691/10000)
[Test]  Epoch: 25	Loss: 0.022741	Acc: 57.2% (5720/10000)
[Test]  Epoch: 26	Loss: 0.022617	Acc: 56.9% (5693/10000)
[Test]  Epoch: 27	Loss: 0.022653	Acc: 57.2% (5719/10000)
[Test]  Epoch: 28	Loss: 0.022654	Acc: 57.0% (5705/10000)
[Test]  Epoch: 29	Loss: 0.022580	Acc: 57.1% (5713/10000)
[Test]  Epoch: 30	Loss: 0.022648	Acc: 57.2% (5716/10000)
[Test]  Epoch: 31	Loss: 0.022582	Acc: 57.3% (5728/10000)
[Test]  Epoch: 32	Loss: 0.022586	Acc: 57.1% (5715/10000)
[Test]  Epoch: 33	Loss: 0.022539	Acc: 57.2% (5720/10000)
[Test]  Epoch: 34	Loss: 0.022534	Acc: 57.3% (5726/10000)
[Test]  Epoch: 35	Loss: 0.022529	Acc: 57.1% (5712/10000)
[Test]  Epoch: 36	Loss: 0.022505	Acc: 57.3% (5734/10000)
[Test]  Epoch: 37	Loss: 0.022441	Acc: 57.4% (5737/10000)
[Test]  Epoch: 38	Loss: 0.022563	Acc: 57.4% (5736/10000)
[Test]  Epoch: 39	Loss: 0.022491	Acc: 57.5% (5749/10000)
[Test]  Epoch: 40	Loss: 0.022426	Acc: 57.4% (5742/10000)
[Test]  Epoch: 41	Loss: 0.022402	Acc: 57.5% (5750/10000)
[Test]  Epoch: 42	Loss: 0.022289	Acc: 57.6% (5763/10000)
[Test]  Epoch: 43	Loss: 0.022298	Acc: 57.5% (5749/10000)
[Test]  Epoch: 44	Loss: 0.022325	Acc: 57.5% (5755/10000)
[Test]  Epoch: 45	Loss: 0.022311	Acc: 57.7% (5773/10000)
[Test]  Epoch: 46	Loss: 0.022243	Acc: 57.7% (5771/10000)
[Test]  Epoch: 47	Loss: 0.022287	Acc: 57.7% (5774/10000)
[Test]  Epoch: 48	Loss: 0.022288	Acc: 57.9% (5790/10000)
[Test]  Epoch: 49	Loss: 0.022314	Acc: 57.8% (5777/10000)
[Test]  Epoch: 50	Loss: 0.022250	Acc: 57.7% (5774/10000)
[Test]  Epoch: 51	Loss: 0.022179	Acc: 58.0% (5804/10000)
[Test]  Epoch: 52	Loss: 0.022158	Acc: 57.9% (5785/10000)
[Test]  Epoch: 53	Loss: 0.022183	Acc: 57.9% (5794/10000)
[Test]  Epoch: 54	Loss: 0.022146	Acc: 57.9% (5786/10000)
[Test]  Epoch: 55	Loss: 0.022220	Acc: 58.1% (5807/10000)
[Test]  Epoch: 56	Loss: 0.022136	Acc: 57.9% (5793/10000)
[Test]  Epoch: 57	Loss: 0.022112	Acc: 58.1% (5810/10000)
[Test]  Epoch: 58	Loss: 0.022131	Acc: 57.9% (5794/10000)
[Test]  Epoch: 59	Loss: 0.022066	Acc: 58.1% (5807/10000)
[Test]  Epoch: 60	Loss: 0.022254	Acc: 57.9% (5789/10000)
[Test]  Epoch: 61	Loss: 0.022189	Acc: 58.0% (5799/10000)
[Test]  Epoch: 62	Loss: 0.022183	Acc: 57.9% (5791/10000)
[Test]  Epoch: 63	Loss: 0.022145	Acc: 58.1% (5806/10000)
[Test]  Epoch: 64	Loss: 0.022056	Acc: 58.1% (5808/10000)
[Test]  Epoch: 65	Loss: 0.022076	Acc: 58.1% (5808/10000)
[Test]  Epoch: 66	Loss: 0.022039	Acc: 58.0% (5803/10000)
[Test]  Epoch: 67	Loss: 0.022110	Acc: 58.0% (5800/10000)
[Test]  Epoch: 68	Loss: 0.022107	Acc: 58.1% (5806/10000)
[Test]  Epoch: 69	Loss: 0.022115	Acc: 58.1% (5809/10000)
[Test]  Epoch: 70	Loss: 0.022117	Acc: 58.0% (5798/10000)
[Test]  Epoch: 71	Loss: 0.022001	Acc: 57.9% (5794/10000)
[Test]  Epoch: 72	Loss: 0.022052	Acc: 58.2% (5817/10000)
[Test]  Epoch: 73	Loss: 0.022028	Acc: 58.1% (5809/10000)
[Test]  Epoch: 74	Loss: 0.022002	Acc: 58.1% (5807/10000)
[Test]  Epoch: 75	Loss: 0.022038	Acc: 58.1% (5807/10000)
[Test]  Epoch: 76	Loss: 0.021993	Acc: 58.0% (5804/10000)
[Test]  Epoch: 77	Loss: 0.022110	Acc: 58.0% (5801/10000)
[Test]  Epoch: 78	Loss: 0.022003	Acc: 58.1% (5807/10000)
[Test]  Epoch: 79	Loss: 0.022000	Acc: 58.1% (5815/10000)
[Test]  Epoch: 80	Loss: 0.022001	Acc: 58.1% (5815/10000)
[Test]  Epoch: 81	Loss: 0.022094	Acc: 57.9% (5788/10000)
[Test]  Epoch: 82	Loss: 0.022157	Acc: 58.0% (5799/10000)
[Test]  Epoch: 83	Loss: 0.021998	Acc: 58.1% (5806/10000)
[Test]  Epoch: 84	Loss: 0.022078	Acc: 58.0% (5804/10000)
[Test]  Epoch: 85	Loss: 0.022069	Acc: 57.9% (5787/10000)
[Test]  Epoch: 86	Loss: 0.022008	Acc: 58.1% (5815/10000)
[Test]  Epoch: 87	Loss: 0.022027	Acc: 58.0% (5799/10000)
[Test]  Epoch: 88	Loss: 0.022014	Acc: 58.1% (5813/10000)
[Test]  Epoch: 89	Loss: 0.022055	Acc: 58.0% (5803/10000)
[Test]  Epoch: 90	Loss: 0.022041	Acc: 58.0% (5800/10000)
[Test]  Epoch: 91	Loss: 0.022086	Acc: 58.0% (5804/10000)
[Test]  Epoch: 92	Loss: 0.022128	Acc: 57.8% (5779/10000)
[Test]  Epoch: 93	Loss: 0.022073	Acc: 58.2% (5823/10000)
[Test]  Epoch: 94	Loss: 0.022088	Acc: 58.2% (5821/10000)
[Test]  Epoch: 95	Loss: 0.022028	Acc: 58.0% (5799/10000)
[Test]  Epoch: 96	Loss: 0.022004	Acc: 58.1% (5812/10000)
[Test]  Epoch: 97	Loss: 0.022060	Acc: 58.1% (5812/10000)
[Test]  Epoch: 98	Loss: 0.022138	Acc: 57.9% (5786/10000)
[Test]  Epoch: 99	Loss: 0.022081	Acc: 58.0% (5804/10000)
[Test]  Epoch: 100	Loss: 0.022107	Acc: 58.2% (5821/10000)
===========finish==========
['2024-08-19', '04:56:56.802496', '100', 'test', '0.022107394534349442', '58.21', '58.23']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.037367	Acc: 39.7% (3974/10000)
[Test]  Epoch: 2	Loss: 0.022319	Acc: 54.5% (5446/10000)
[Test]  Epoch: 3	Loss: 0.022421	Acc: 55.2% (5519/10000)
[Test]  Epoch: 4	Loss: 0.022915	Acc: 54.9% (5493/10000)
[Test]  Epoch: 5	Loss: 0.023209	Acc: 55.5% (5545/10000)
[Test]  Epoch: 6	Loss: 0.023254	Acc: 55.4% (5541/10000)
[Test]  Epoch: 7	Loss: 0.023320	Acc: 55.6% (5557/10000)
[Test]  Epoch: 8	Loss: 0.023445	Acc: 55.6% (5556/10000)
[Test]  Epoch: 9	Loss: 0.023512	Acc: 55.5% (5547/10000)
[Test]  Epoch: 10	Loss: 0.023516	Acc: 55.5% (5552/10000)
[Test]  Epoch: 11	Loss: 0.023398	Acc: 55.9% (5593/10000)
[Test]  Epoch: 12	Loss: 0.023355	Acc: 55.8% (5577/10000)
[Test]  Epoch: 13	Loss: 0.023293	Acc: 55.9% (5594/10000)
[Test]  Epoch: 14	Loss: 0.023295	Acc: 55.7% (5571/10000)
[Test]  Epoch: 15	Loss: 0.023162	Acc: 56.1% (5611/10000)
[Test]  Epoch: 16	Loss: 0.023042	Acc: 56.0% (5605/10000)
[Test]  Epoch: 17	Loss: 0.023201	Acc: 56.0% (5605/10000)
[Test]  Epoch: 18	Loss: 0.023174	Acc: 55.9% (5593/10000)
[Test]  Epoch: 19	Loss: 0.023035	Acc: 56.5% (5645/10000)
[Test]  Epoch: 20	Loss: 0.023059	Acc: 56.5% (5647/10000)
[Test]  Epoch: 21	Loss: 0.023094	Acc: 56.3% (5634/10000)
[Test]  Epoch: 22	Loss: 0.023077	Acc: 56.2% (5624/10000)
[Test]  Epoch: 23	Loss: 0.023019	Acc: 56.4% (5640/10000)
[Test]  Epoch: 24	Loss: 0.022969	Acc: 56.6% (5664/10000)
[Test]  Epoch: 25	Loss: 0.022986	Acc: 56.7% (5667/10000)
[Test]  Epoch: 26	Loss: 0.022888	Acc: 56.6% (5657/10000)
[Test]  Epoch: 27	Loss: 0.022860	Acc: 56.7% (5672/10000)
[Test]  Epoch: 28	Loss: 0.022874	Acc: 56.4% (5644/10000)
[Test]  Epoch: 29	Loss: 0.022771	Acc: 56.4% (5640/10000)
[Test]  Epoch: 30	Loss: 0.022720	Acc: 56.6% (5663/10000)
[Test]  Epoch: 31	Loss: 0.022684	Acc: 56.8% (5679/10000)
[Test]  Epoch: 32	Loss: 0.022731	Acc: 56.7% (5673/10000)
[Test]  Epoch: 33	Loss: 0.022607	Acc: 56.9% (5690/10000)
[Test]  Epoch: 34	Loss: 0.022616	Acc: 56.9% (5690/10000)
[Test]  Epoch: 35	Loss: 0.022641	Acc: 57.1% (5708/10000)
[Test]  Epoch: 36	Loss: 0.022586	Acc: 56.9% (5685/10000)
[Test]  Epoch: 37	Loss: 0.022535	Acc: 57.1% (5708/10000)
[Test]  Epoch: 38	Loss: 0.022510	Acc: 56.8% (5677/10000)
[Test]  Epoch: 39	Loss: 0.022540	Acc: 57.0% (5699/10000)
[Test]  Epoch: 40	Loss: 0.022521	Acc: 57.2% (5717/10000)
[Test]  Epoch: 41	Loss: 0.022370	Acc: 57.4% (5735/10000)
[Test]  Epoch: 42	Loss: 0.022415	Acc: 57.2% (5724/10000)
[Test]  Epoch: 43	Loss: 0.022387	Acc: 57.2% (5725/10000)
[Test]  Epoch: 44	Loss: 0.022375	Acc: 57.3% (5731/10000)
[Test]  Epoch: 45	Loss: 0.022383	Acc: 57.4% (5739/10000)
[Test]  Epoch: 46	Loss: 0.022339	Acc: 57.2% (5725/10000)
[Test]  Epoch: 47	Loss: 0.022402	Acc: 57.4% (5737/10000)
[Test]  Epoch: 48	Loss: 0.022351	Acc: 57.4% (5741/10000)
[Test]  Epoch: 49	Loss: 0.022373	Acc: 57.2% (5722/10000)
[Test]  Epoch: 50	Loss: 0.022383	Acc: 57.4% (5738/10000)
[Test]  Epoch: 51	Loss: 0.022339	Acc: 57.4% (5736/10000)
[Test]  Epoch: 52	Loss: 0.022260	Acc: 57.1% (5707/10000)
[Test]  Epoch: 53	Loss: 0.022224	Acc: 57.3% (5728/10000)
[Test]  Epoch: 54	Loss: 0.022139	Acc: 57.2% (5725/10000)
[Test]  Epoch: 55	Loss: 0.022146	Acc: 57.6% (5762/10000)
[Test]  Epoch: 56	Loss: 0.022184	Acc: 57.3% (5732/10000)
[Test]  Epoch: 57	Loss: 0.022194	Acc: 57.5% (5748/10000)
[Test]  Epoch: 58	Loss: 0.022211	Acc: 57.4% (5736/10000)
[Test]  Epoch: 59	Loss: 0.022119	Acc: 57.6% (5757/10000)
[Test]  Epoch: 60	Loss: 0.022195	Acc: 57.2% (5720/10000)
[Test]  Epoch: 61	Loss: 0.022164	Acc: 57.2% (5725/10000)
[Test]  Epoch: 62	Loss: 0.022127	Acc: 57.3% (5731/10000)
[Test]  Epoch: 63	Loss: 0.022143	Acc: 57.3% (5729/10000)
[Test]  Epoch: 64	Loss: 0.022161	Acc: 57.5% (5755/10000)
[Test]  Epoch: 65	Loss: 0.022091	Acc: 57.4% (5736/10000)
[Test]  Epoch: 66	Loss: 0.022058	Acc: 57.4% (5741/10000)
[Test]  Epoch: 67	Loss: 0.022097	Acc: 57.3% (5728/10000)
[Test]  Epoch: 68	Loss: 0.022114	Acc: 57.3% (5734/10000)
[Test]  Epoch: 69	Loss: 0.022104	Acc: 57.3% (5728/10000)
[Test]  Epoch: 70	Loss: 0.022092	Acc: 57.5% (5749/10000)
[Test]  Epoch: 71	Loss: 0.022040	Acc: 57.3% (5726/10000)
[Test]  Epoch: 72	Loss: 0.022146	Acc: 57.3% (5730/10000)
[Test]  Epoch: 73	Loss: 0.022056	Acc: 57.4% (5742/10000)
[Test]  Epoch: 74	Loss: 0.022062	Acc: 57.2% (5718/10000)
[Test]  Epoch: 75	Loss: 0.022132	Acc: 57.3% (5728/10000)
[Test]  Epoch: 76	Loss: 0.022039	Acc: 57.5% (5751/10000)
[Test]  Epoch: 77	Loss: 0.022132	Acc: 57.5% (5745/10000)
[Test]  Epoch: 78	Loss: 0.022088	Acc: 57.5% (5749/10000)
[Test]  Epoch: 79	Loss: 0.022088	Acc: 57.5% (5749/10000)
[Test]  Epoch: 80	Loss: 0.022091	Acc: 57.4% (5738/10000)
[Test]  Epoch: 81	Loss: 0.022157	Acc: 57.3% (5728/10000)
[Test]  Epoch: 82	Loss: 0.022149	Acc: 57.4% (5740/10000)
[Test]  Epoch: 83	Loss: 0.022107	Acc: 57.4% (5740/10000)
[Test]  Epoch: 84	Loss: 0.022100	Acc: 57.5% (5749/10000)
[Test]  Epoch: 85	Loss: 0.022098	Acc: 57.3% (5730/10000)
[Test]  Epoch: 86	Loss: 0.022102	Acc: 57.4% (5735/10000)
[Test]  Epoch: 87	Loss: 0.022074	Acc: 57.5% (5747/10000)
[Test]  Epoch: 88	Loss: 0.022081	Acc: 57.4% (5744/10000)
[Test]  Epoch: 89	Loss: 0.022112	Acc: 57.4% (5742/10000)
[Test]  Epoch: 90	Loss: 0.022094	Acc: 57.5% (5749/10000)
[Test]  Epoch: 91	Loss: 0.022107	Acc: 57.4% (5737/10000)
[Test]  Epoch: 92	Loss: 0.022128	Acc: 57.4% (5735/10000)
[Test]  Epoch: 93	Loss: 0.022121	Acc: 57.4% (5742/10000)
[Test]  Epoch: 94	Loss: 0.022120	Acc: 57.5% (5751/10000)
[Test]  Epoch: 95	Loss: 0.022087	Acc: 57.5% (5749/10000)
[Test]  Epoch: 96	Loss: 0.022112	Acc: 57.4% (5742/10000)
[Test]  Epoch: 97	Loss: 0.022153	Acc: 57.4% (5741/10000)
[Test]  Epoch: 98	Loss: 0.022202	Acc: 57.4% (5740/10000)
[Test]  Epoch: 99	Loss: 0.022085	Acc: 57.4% (5743/10000)
[Test]  Epoch: 100	Loss: 0.022118	Acc: 57.6% (5756/10000)
===========finish==========
['2024-08-19', '04:59:27.328812', '100', 'test', '0.02211837663650513', '57.56', '57.62']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.040178	Acc: 43.6% (4362/10000)
[Test]  Epoch: 2	Loss: 0.022442	Acc: 52.9% (5286/10000)
[Test]  Epoch: 3	Loss: 0.022273	Acc: 53.9% (5394/10000)
[Test]  Epoch: 4	Loss: 0.022692	Acc: 53.9% (5393/10000)
[Test]  Epoch: 5	Loss: 0.022867	Acc: 54.3% (5430/10000)
[Test]  Epoch: 6	Loss: 0.023132	Acc: 54.5% (5445/10000)
[Test]  Epoch: 7	Loss: 0.023279	Acc: 54.7% (5472/10000)
[Test]  Epoch: 8	Loss: 0.023096	Acc: 54.9% (5492/10000)
[Test]  Epoch: 9	Loss: 0.023364	Acc: 54.7% (5471/10000)
[Test]  Epoch: 10	Loss: 0.023144	Acc: 55.1% (5506/10000)
[Test]  Epoch: 11	Loss: 0.023150	Acc: 55.3% (5531/10000)
[Test]  Epoch: 12	Loss: 0.023163	Acc: 55.4% (5538/10000)
[Test]  Epoch: 13	Loss: 0.023165	Acc: 55.4% (5544/10000)
[Test]  Epoch: 14	Loss: 0.023129	Acc: 55.6% (5559/10000)
[Test]  Epoch: 15	Loss: 0.023240	Acc: 55.6% (5562/10000)
[Test]  Epoch: 16	Loss: 0.022979	Acc: 56.0% (5599/10000)
[Test]  Epoch: 17	Loss: 0.022936	Acc: 56.0% (5595/10000)
[Test]  Epoch: 18	Loss: 0.023100	Acc: 55.9% (5594/10000)
[Test]  Epoch: 19	Loss: 0.023008	Acc: 55.9% (5587/10000)
[Test]  Epoch: 20	Loss: 0.022854	Acc: 56.1% (5611/10000)
[Test]  Epoch: 21	Loss: 0.023112	Acc: 56.1% (5612/10000)
[Test]  Epoch: 22	Loss: 0.022948	Acc: 56.1% (5610/10000)
[Test]  Epoch: 23	Loss: 0.022697	Acc: 56.3% (5630/10000)
[Test]  Epoch: 24	Loss: 0.022839	Acc: 56.4% (5642/10000)
[Test]  Epoch: 25	Loss: 0.022853	Acc: 56.4% (5639/10000)
[Test]  Epoch: 26	Loss: 0.022811	Acc: 56.2% (5622/10000)
[Test]  Epoch: 27	Loss: 0.022719	Acc: 56.4% (5642/10000)
[Test]  Epoch: 28	Loss: 0.022697	Acc: 56.4% (5642/10000)
[Test]  Epoch: 29	Loss: 0.022730	Acc: 56.2% (5617/10000)
[Test]  Epoch: 30	Loss: 0.022599	Acc: 56.2% (5621/10000)
[Test]  Epoch: 31	Loss: 0.022565	Acc: 56.5% (5652/10000)
[Test]  Epoch: 32	Loss: 0.022511	Acc: 56.5% (5655/10000)
[Test]  Epoch: 33	Loss: 0.022433	Acc: 56.7% (5674/10000)
[Test]  Epoch: 34	Loss: 0.022422	Acc: 56.6% (5663/10000)
[Test]  Epoch: 35	Loss: 0.022497	Acc: 56.5% (5645/10000)
[Test]  Epoch: 36	Loss: 0.022565	Acc: 56.4% (5637/10000)
[Test]  Epoch: 37	Loss: 0.022520	Acc: 56.7% (5670/10000)
[Test]  Epoch: 38	Loss: 0.022526	Acc: 56.2% (5621/10000)
[Test]  Epoch: 39	Loss: 0.022567	Acc: 56.4% (5637/10000)
[Test]  Epoch: 40	Loss: 0.022365	Acc: 56.5% (5645/10000)
[Test]  Epoch: 41	Loss: 0.022469	Acc: 56.5% (5651/10000)
[Test]  Epoch: 42	Loss: 0.022303	Acc: 57.0% (5695/10000)
[Test]  Epoch: 43	Loss: 0.022441	Acc: 56.5% (5653/10000)
[Test]  Epoch: 44	Loss: 0.022296	Acc: 56.6% (5660/10000)
[Test]  Epoch: 45	Loss: 0.022392	Acc: 56.5% (5649/10000)
[Test]  Epoch: 46	Loss: 0.022288	Acc: 56.8% (5676/10000)
[Test]  Epoch: 47	Loss: 0.022233	Acc: 56.8% (5677/10000)
[Test]  Epoch: 48	Loss: 0.022456	Acc: 56.4% (5638/10000)
[Test]  Epoch: 49	Loss: 0.022254	Acc: 56.7% (5674/10000)
[Test]  Epoch: 50	Loss: 0.022360	Acc: 56.4% (5640/10000)
[Test]  Epoch: 51	Loss: 0.022334	Acc: 56.5% (5650/10000)
[Test]  Epoch: 52	Loss: 0.022191	Acc: 56.8% (5676/10000)
[Test]  Epoch: 53	Loss: 0.022366	Acc: 56.6% (5661/10000)
[Test]  Epoch: 54	Loss: 0.022167	Acc: 56.9% (5686/10000)
[Test]  Epoch: 55	Loss: 0.022128	Acc: 57.0% (5699/10000)
[Test]  Epoch: 56	Loss: 0.022200	Acc: 56.8% (5679/10000)
[Test]  Epoch: 57	Loss: 0.022061	Acc: 56.9% (5692/10000)
[Test]  Epoch: 58	Loss: 0.022104	Acc: 57.1% (5706/10000)
[Test]  Epoch: 59	Loss: 0.022162	Acc: 56.9% (5694/10000)
[Test]  Epoch: 60	Loss: 0.022335	Acc: 56.7% (5671/10000)
[Test]  Epoch: 61	Loss: 0.022220	Acc: 56.6% (5659/10000)
[Test]  Epoch: 62	Loss: 0.022182	Acc: 56.7% (5671/10000)
[Test]  Epoch: 63	Loss: 0.022202	Acc: 56.6% (5665/10000)
[Test]  Epoch: 64	Loss: 0.022185	Acc: 56.6% (5662/10000)
[Test]  Epoch: 65	Loss: 0.022147	Acc: 56.9% (5685/10000)
[Test]  Epoch: 66	Loss: 0.022052	Acc: 56.8% (5684/10000)
[Test]  Epoch: 67	Loss: 0.022096	Acc: 56.7% (5673/10000)
[Test]  Epoch: 68	Loss: 0.022059	Acc: 56.9% (5687/10000)
[Test]  Epoch: 69	Loss: 0.022095	Acc: 57.0% (5696/10000)
[Test]  Epoch: 70	Loss: 0.022090	Acc: 56.9% (5689/10000)
[Test]  Epoch: 71	Loss: 0.022024	Acc: 56.8% (5675/10000)
[Test]  Epoch: 72	Loss: 0.022162	Acc: 56.9% (5686/10000)
[Test]  Epoch: 73	Loss: 0.022089	Acc: 56.8% (5675/10000)
[Test]  Epoch: 74	Loss: 0.022019	Acc: 56.9% (5689/10000)
[Test]  Epoch: 75	Loss: 0.022108	Acc: 57.0% (5698/10000)
[Test]  Epoch: 76	Loss: 0.022008	Acc: 56.9% (5691/10000)
[Test]  Epoch: 77	Loss: 0.022048	Acc: 56.9% (5691/10000)
[Test]  Epoch: 78	Loss: 0.022079	Acc: 56.9% (5686/10000)
[Test]  Epoch: 79	Loss: 0.022035	Acc: 56.9% (5694/10000)
[Test]  Epoch: 80	Loss: 0.022064	Acc: 56.7% (5671/10000)
[Test]  Epoch: 81	Loss: 0.022047	Acc: 57.0% (5698/10000)
[Test]  Epoch: 82	Loss: 0.022122	Acc: 56.9% (5687/10000)
[Test]  Epoch: 83	Loss: 0.022037	Acc: 57.0% (5703/10000)
[Test]  Epoch: 84	Loss: 0.022075	Acc: 57.0% (5700/10000)
[Test]  Epoch: 85	Loss: 0.022037	Acc: 56.9% (5685/10000)
[Test]  Epoch: 86	Loss: 0.022053	Acc: 57.1% (5714/10000)
[Test]  Epoch: 87	Loss: 0.022034	Acc: 57.0% (5699/10000)
[Test]  Epoch: 88	Loss: 0.022088	Acc: 56.8% (5676/10000)
[Test]  Epoch: 89	Loss: 0.022101	Acc: 56.8% (5683/10000)
[Test]  Epoch: 90	Loss: 0.022091	Acc: 56.8% (5679/10000)
[Test]  Epoch: 91	Loss: 0.022072	Acc: 57.0% (5700/10000)
[Test]  Epoch: 92	Loss: 0.022113	Acc: 56.9% (5688/10000)
[Test]  Epoch: 93	Loss: 0.022041	Acc: 56.9% (5687/10000)
[Test]  Epoch: 94	Loss: 0.022067	Acc: 57.0% (5704/10000)
[Test]  Epoch: 95	Loss: 0.022113	Acc: 57.0% (5696/10000)
[Test]  Epoch: 96	Loss: 0.022066	Acc: 56.9% (5692/10000)
[Test]  Epoch: 97	Loss: 0.022066	Acc: 57.0% (5695/10000)
[Test]  Epoch: 98	Loss: 0.022116	Acc: 57.0% (5696/10000)
[Test]  Epoch: 99	Loss: 0.022046	Acc: 56.9% (5686/10000)
[Test]  Epoch: 100	Loss: 0.022034	Acc: 57.0% (5697/10000)
===========finish==========
['2024-08-19', '05:01:54.104827', '100', 'test', '0.022034277987480165', '56.97', '57.14']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042660	Acc: 40.7% (4073/10000)
[Test]  Epoch: 2	Loss: 0.023032	Acc: 52.3% (5228/10000)
[Test]  Epoch: 3	Loss: 0.022178	Acc: 54.6% (5456/10000)
[Test]  Epoch: 4	Loss: 0.022701	Acc: 54.7% (5468/10000)
[Test]  Epoch: 5	Loss: 0.022688	Acc: 55.4% (5541/10000)
[Test]  Epoch: 6	Loss: 0.022821	Acc: 55.0% (5500/10000)
[Test]  Epoch: 7	Loss: 0.023006	Acc: 55.4% (5543/10000)
[Test]  Epoch: 8	Loss: 0.022810	Acc: 55.6% (5558/10000)
[Test]  Epoch: 9	Loss: 0.023079	Acc: 55.5% (5550/10000)
[Test]  Epoch: 10	Loss: 0.022861	Acc: 55.9% (5588/10000)
[Test]  Epoch: 11	Loss: 0.022949	Acc: 55.6% (5560/10000)
[Test]  Epoch: 12	Loss: 0.022982	Acc: 55.6% (5562/10000)
[Test]  Epoch: 13	Loss: 0.022847	Acc: 55.9% (5586/10000)
[Test]  Epoch: 14	Loss: 0.023043	Acc: 55.7% (5568/10000)
[Test]  Epoch: 15	Loss: 0.022978	Acc: 55.8% (5581/10000)
[Test]  Epoch: 16	Loss: 0.022702	Acc: 56.3% (5631/10000)
[Test]  Epoch: 17	Loss: 0.022669	Acc: 56.0% (5604/10000)
[Test]  Epoch: 18	Loss: 0.022873	Acc: 56.1% (5613/10000)
[Test]  Epoch: 19	Loss: 0.022689	Acc: 56.0% (5596/10000)
[Test]  Epoch: 20	Loss: 0.022638	Acc: 56.3% (5629/10000)
[Test]  Epoch: 21	Loss: 0.022736	Acc: 56.2% (5625/10000)
[Test]  Epoch: 22	Loss: 0.022525	Acc: 56.5% (5650/10000)
[Test]  Epoch: 23	Loss: 0.022385	Acc: 56.7% (5670/10000)
[Test]  Epoch: 24	Loss: 0.022504	Acc: 56.4% (5636/10000)
[Test]  Epoch: 25	Loss: 0.022695	Acc: 56.5% (5654/10000)
[Test]  Epoch: 26	Loss: 0.022446	Acc: 56.3% (5626/10000)
[Test]  Epoch: 27	Loss: 0.022547	Acc: 56.5% (5652/10000)
[Test]  Epoch: 28	Loss: 0.022370	Acc: 56.9% (5685/10000)
[Test]  Epoch: 29	Loss: 0.022504	Acc: 56.8% (5676/10000)
[Test]  Epoch: 30	Loss: 0.022319	Acc: 56.9% (5690/10000)
[Test]  Epoch: 31	Loss: 0.022296	Acc: 57.0% (5698/10000)
[Test]  Epoch: 32	Loss: 0.022260	Acc: 57.3% (5728/10000)
[Test]  Epoch: 33	Loss: 0.022248	Acc: 57.3% (5726/10000)
[Test]  Epoch: 34	Loss: 0.022165	Acc: 57.1% (5715/10000)
[Test]  Epoch: 35	Loss: 0.022122	Acc: 57.3% (5730/10000)
[Test]  Epoch: 36	Loss: 0.022159	Acc: 57.2% (5722/10000)
[Test]  Epoch: 37	Loss: 0.022138	Acc: 57.4% (5739/10000)
[Test]  Epoch: 38	Loss: 0.022224	Acc: 57.0% (5697/10000)
[Test]  Epoch: 39	Loss: 0.022295	Acc: 57.0% (5697/10000)
[Test]  Epoch: 40	Loss: 0.022053	Acc: 56.9% (5690/10000)
[Test]  Epoch: 41	Loss: 0.022084	Acc: 57.2% (5722/10000)
[Test]  Epoch: 42	Loss: 0.021993	Acc: 57.2% (5716/10000)
[Test]  Epoch: 43	Loss: 0.022165	Acc: 57.1% (5714/10000)
[Test]  Epoch: 44	Loss: 0.021855	Acc: 57.3% (5733/10000)
[Test]  Epoch: 45	Loss: 0.022015	Acc: 57.2% (5723/10000)
[Test]  Epoch: 46	Loss: 0.021924	Acc: 57.4% (5744/10000)
[Test]  Epoch: 47	Loss: 0.021869	Acc: 57.5% (5754/10000)
[Test]  Epoch: 48	Loss: 0.021996	Acc: 57.4% (5739/10000)
[Test]  Epoch: 49	Loss: 0.021759	Acc: 57.6% (5764/10000)
[Test]  Epoch: 50	Loss: 0.021910	Acc: 57.4% (5744/10000)
[Test]  Epoch: 51	Loss: 0.021939	Acc: 57.2% (5724/10000)
[Test]  Epoch: 52	Loss: 0.021807	Acc: 57.4% (5739/10000)
[Test]  Epoch: 53	Loss: 0.021922	Acc: 57.3% (5732/10000)
[Test]  Epoch: 54	Loss: 0.021774	Acc: 57.6% (5757/10000)
[Test]  Epoch: 55	Loss: 0.021674	Acc: 57.5% (5753/10000)
[Test]  Epoch: 56	Loss: 0.021839	Acc: 57.5% (5753/10000)
[Test]  Epoch: 57	Loss: 0.021654	Acc: 57.7% (5770/10000)
[Test]  Epoch: 58	Loss: 0.021645	Acc: 57.7% (5767/10000)
[Test]  Epoch: 59	Loss: 0.021739	Acc: 57.6% (5761/10000)
[Test]  Epoch: 60	Loss: 0.021938	Acc: 57.1% (5711/10000)
[Test]  Epoch: 61	Loss: 0.021854	Acc: 57.4% (5744/10000)
[Test]  Epoch: 62	Loss: 0.021780	Acc: 57.5% (5755/10000)
[Test]  Epoch: 63	Loss: 0.021800	Acc: 57.6% (5765/10000)
[Test]  Epoch: 64	Loss: 0.021794	Acc: 57.6% (5757/10000)
[Test]  Epoch: 65	Loss: 0.021746	Acc: 57.5% (5755/10000)
[Test]  Epoch: 66	Loss: 0.021631	Acc: 57.7% (5772/10000)
[Test]  Epoch: 67	Loss: 0.021721	Acc: 57.5% (5753/10000)
[Test]  Epoch: 68	Loss: 0.021654	Acc: 57.7% (5769/10000)
[Test]  Epoch: 69	Loss: 0.021742	Acc: 57.7% (5770/10000)
[Test]  Epoch: 70	Loss: 0.021703	Acc: 57.7% (5768/10000)
[Test]  Epoch: 71	Loss: 0.021674	Acc: 57.5% (5748/10000)
[Test]  Epoch: 72	Loss: 0.021763	Acc: 57.5% (5751/10000)
[Test]  Epoch: 73	Loss: 0.021683	Acc: 57.5% (5752/10000)
[Test]  Epoch: 74	Loss: 0.021603	Acc: 57.7% (5772/10000)
[Test]  Epoch: 75	Loss: 0.021700	Acc: 57.4% (5744/10000)
[Test]  Epoch: 76	Loss: 0.021628	Acc: 57.4% (5742/10000)
[Test]  Epoch: 77	Loss: 0.021679	Acc: 57.8% (5777/10000)
[Test]  Epoch: 78	Loss: 0.021673	Acc: 57.5% (5751/10000)
[Test]  Epoch: 79	Loss: 0.021596	Acc: 57.9% (5785/10000)
[Test]  Epoch: 80	Loss: 0.021665	Acc: 57.8% (5777/10000)
[Test]  Epoch: 81	Loss: 0.021653	Acc: 57.6% (5761/10000)
[Test]  Epoch: 82	Loss: 0.021720	Acc: 57.7% (5766/10000)
[Test]  Epoch: 83	Loss: 0.021626	Acc: 57.6% (5764/10000)
[Test]  Epoch: 84	Loss: 0.021644	Acc: 57.6% (5756/10000)
[Test]  Epoch: 85	Loss: 0.021610	Acc: 57.8% (5779/10000)
[Test]  Epoch: 86	Loss: 0.021647	Acc: 57.8% (5777/10000)
[Test]  Epoch: 87	Loss: 0.021618	Acc: 57.6% (5763/10000)
[Test]  Epoch: 88	Loss: 0.021669	Acc: 57.5% (5749/10000)
[Test]  Epoch: 89	Loss: 0.021719	Acc: 57.6% (5757/10000)
[Test]  Epoch: 90	Loss: 0.021672	Acc: 57.5% (5753/10000)
[Test]  Epoch: 91	Loss: 0.021680	Acc: 57.6% (5758/10000)
[Test]  Epoch: 92	Loss: 0.021688	Acc: 57.6% (5759/10000)
[Test]  Epoch: 93	Loss: 0.021640	Acc: 57.6% (5764/10000)
[Test]  Epoch: 94	Loss: 0.021644	Acc: 57.5% (5753/10000)
[Test]  Epoch: 95	Loss: 0.021711	Acc: 57.6% (5759/10000)
[Test]  Epoch: 96	Loss: 0.021655	Acc: 57.8% (5782/10000)
[Test]  Epoch: 97	Loss: 0.021681	Acc: 57.7% (5772/10000)
[Test]  Epoch: 98	Loss: 0.021777	Acc: 57.7% (5770/10000)
[Test]  Epoch: 99	Loss: 0.021692	Acc: 57.7% (5769/10000)
[Test]  Epoch: 100	Loss: 0.021657	Acc: 57.9% (5789/10000)
===========finish==========
['2024-08-19', '05:04:29.115748', '100', 'test', '0.021657262808084488', '57.89', '57.89']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.041487	Acc: 40.4% (4043/10000)
[Test]  Epoch: 2	Loss: 0.022778	Acc: 53.0% (5296/10000)
[Test]  Epoch: 3	Loss: 0.022631	Acc: 53.5% (5349/10000)
[Test]  Epoch: 4	Loss: 0.022623	Acc: 54.0% (5403/10000)
[Test]  Epoch: 5	Loss: 0.023052	Acc: 54.3% (5432/10000)
[Test]  Epoch: 6	Loss: 0.023067	Acc: 54.4% (5444/10000)
[Test]  Epoch: 7	Loss: 0.023197	Acc: 54.4% (5443/10000)
[Test]  Epoch: 8	Loss: 0.023261	Acc: 54.9% (5492/10000)
[Test]  Epoch: 9	Loss: 0.023360	Acc: 54.7% (5467/10000)
[Test]  Epoch: 10	Loss: 0.023193	Acc: 55.0% (5500/10000)
[Test]  Epoch: 11	Loss: 0.023401	Acc: 54.5% (5453/10000)
[Test]  Epoch: 12	Loss: 0.023408	Acc: 55.0% (5497/10000)
[Test]  Epoch: 13	Loss: 0.023237	Acc: 54.9% (5490/10000)
[Test]  Epoch: 14	Loss: 0.023556	Acc: 54.4% (5444/10000)
[Test]  Epoch: 15	Loss: 0.023484	Acc: 55.0% (5496/10000)
[Test]  Epoch: 16	Loss: 0.023250	Acc: 55.3% (5533/10000)
[Test]  Epoch: 17	Loss: 0.023254	Acc: 54.8% (5481/10000)
[Test]  Epoch: 18	Loss: 0.023397	Acc: 55.1% (5513/10000)
[Test]  Epoch: 19	Loss: 0.023335	Acc: 55.2% (5518/10000)
[Test]  Epoch: 20	Loss: 0.023104	Acc: 55.4% (5538/10000)
[Test]  Epoch: 21	Loss: 0.023098	Acc: 55.6% (5559/10000)
[Test]  Epoch: 22	Loss: 0.023050	Acc: 55.5% (5553/10000)
[Test]  Epoch: 23	Loss: 0.022937	Acc: 55.8% (5577/10000)
[Test]  Epoch: 24	Loss: 0.022962	Acc: 55.7% (5569/10000)
[Test]  Epoch: 25	Loss: 0.023019	Acc: 55.9% (5586/10000)
[Test]  Epoch: 26	Loss: 0.022854	Acc: 55.7% (5569/10000)
[Test]  Epoch: 27	Loss: 0.022827	Acc: 55.9% (5585/10000)
[Test]  Epoch: 28	Loss: 0.022838	Acc: 56.0% (5602/10000)
[Test]  Epoch: 29	Loss: 0.022883	Acc: 56.0% (5604/10000)
[Test]  Epoch: 30	Loss: 0.022658	Acc: 56.2% (5623/10000)
[Test]  Epoch: 31	Loss: 0.022702	Acc: 56.1% (5611/10000)
[Test]  Epoch: 32	Loss: 0.022761	Acc: 56.0% (5604/10000)
[Test]  Epoch: 33	Loss: 0.022559	Acc: 56.0% (5599/10000)
[Test]  Epoch: 34	Loss: 0.022632	Acc: 56.3% (5627/10000)
[Test]  Epoch: 35	Loss: 0.022627	Acc: 56.2% (5622/10000)
[Test]  Epoch: 36	Loss: 0.022597	Acc: 56.3% (5629/10000)
[Test]  Epoch: 37	Loss: 0.022724	Acc: 56.1% (5614/10000)
[Test]  Epoch: 38	Loss: 0.022777	Acc: 56.1% (5613/10000)
[Test]  Epoch: 39	Loss: 0.022668	Acc: 56.0% (5602/10000)
[Test]  Epoch: 40	Loss: 0.022462	Acc: 56.3% (5634/10000)
[Test]  Epoch: 41	Loss: 0.022534	Acc: 56.3% (5626/10000)
[Test]  Epoch: 42	Loss: 0.022416	Acc: 56.5% (5652/10000)
[Test]  Epoch: 43	Loss: 0.022629	Acc: 56.3% (5633/10000)
[Test]  Epoch: 44	Loss: 0.022513	Acc: 56.0% (5598/10000)
[Test]  Epoch: 45	Loss: 0.022559	Acc: 56.1% (5607/10000)
[Test]  Epoch: 46	Loss: 0.022431	Acc: 55.9% (5594/10000)
[Test]  Epoch: 47	Loss: 0.022464	Acc: 56.4% (5642/10000)
[Test]  Epoch: 48	Loss: 0.022492	Acc: 56.1% (5615/10000)
[Test]  Epoch: 49	Loss: 0.022378	Acc: 56.6% (5662/10000)
[Test]  Epoch: 50	Loss: 0.022476	Acc: 56.1% (5614/10000)
[Test]  Epoch: 51	Loss: 0.022523	Acc: 56.2% (5625/10000)
[Test]  Epoch: 52	Loss: 0.022346	Acc: 56.2% (5621/10000)
[Test]  Epoch: 53	Loss: 0.022465	Acc: 56.1% (5607/10000)
[Test]  Epoch: 54	Loss: 0.022298	Acc: 56.3% (5632/10000)
[Test]  Epoch: 55	Loss: 0.022245	Acc: 56.2% (5621/10000)
[Test]  Epoch: 56	Loss: 0.022286	Acc: 56.6% (5661/10000)
[Test]  Epoch: 57	Loss: 0.022215	Acc: 56.5% (5646/10000)
[Test]  Epoch: 58	Loss: 0.022259	Acc: 56.1% (5609/10000)
[Test]  Epoch: 59	Loss: 0.022246	Acc: 56.1% (5613/10000)
[Test]  Epoch: 60	Loss: 0.022364	Acc: 56.2% (5620/10000)
[Test]  Epoch: 61	Loss: 0.022263	Acc: 56.5% (5648/10000)
[Test]  Epoch: 62	Loss: 0.022246	Acc: 56.4% (5644/10000)
[Test]  Epoch: 63	Loss: 0.022281	Acc: 56.2% (5621/10000)
[Test]  Epoch: 64	Loss: 0.022264	Acc: 56.3% (5634/10000)
[Test]  Epoch: 65	Loss: 0.022252	Acc: 56.4% (5639/10000)
[Test]  Epoch: 66	Loss: 0.022139	Acc: 56.5% (5648/10000)
[Test]  Epoch: 67	Loss: 0.022238	Acc: 56.5% (5646/10000)
[Test]  Epoch: 68	Loss: 0.022154	Acc: 56.6% (5662/10000)
[Test]  Epoch: 69	Loss: 0.022184	Acc: 56.5% (5655/10000)
[Test]  Epoch: 70	Loss: 0.022212	Acc: 56.5% (5647/10000)
[Test]  Epoch: 71	Loss: 0.022200	Acc: 56.2% (5623/10000)
[Test]  Epoch: 72	Loss: 0.022265	Acc: 56.5% (5654/10000)
[Test]  Epoch: 73	Loss: 0.022200	Acc: 56.5% (5648/10000)
[Test]  Epoch: 74	Loss: 0.022182	Acc: 56.4% (5641/10000)
[Test]  Epoch: 75	Loss: 0.022173	Acc: 56.6% (5662/10000)
[Test]  Epoch: 76	Loss: 0.022109	Acc: 56.5% (5655/10000)
[Test]  Epoch: 77	Loss: 0.022191	Acc: 56.7% (5666/10000)
[Test]  Epoch: 78	Loss: 0.022182	Acc: 56.5% (5654/10000)
[Test]  Epoch: 79	Loss: 0.022135	Acc: 56.7% (5669/10000)
[Test]  Epoch: 80	Loss: 0.022160	Acc: 56.5% (5650/10000)
[Test]  Epoch: 81	Loss: 0.022151	Acc: 56.5% (5650/10000)
[Test]  Epoch: 82	Loss: 0.022208	Acc: 56.4% (5640/10000)
[Test]  Epoch: 83	Loss: 0.022173	Acc: 56.6% (5665/10000)
[Test]  Epoch: 84	Loss: 0.022164	Acc: 56.7% (5670/10000)
[Test]  Epoch: 85	Loss: 0.022096	Acc: 56.6% (5658/10000)
[Test]  Epoch: 86	Loss: 0.022152	Acc: 56.6% (5657/10000)
[Test]  Epoch: 87	Loss: 0.022130	Acc: 56.7% (5667/10000)
[Test]  Epoch: 88	Loss: 0.022156	Acc: 56.7% (5673/10000)
[Test]  Epoch: 89	Loss: 0.022212	Acc: 56.7% (5670/10000)
[Test]  Epoch: 90	Loss: 0.022177	Acc: 56.5% (5648/10000)
[Test]  Epoch: 91	Loss: 0.022197	Acc: 56.5% (5651/10000)
[Test]  Epoch: 92	Loss: 0.022165	Acc: 56.4% (5644/10000)
[Test]  Epoch: 93	Loss: 0.022142	Acc: 56.6% (5661/10000)
[Test]  Epoch: 94	Loss: 0.022137	Acc: 56.5% (5654/10000)
[Test]  Epoch: 95	Loss: 0.022205	Acc: 56.5% (5655/10000)
[Test]  Epoch: 96	Loss: 0.022143	Acc: 56.7% (5673/10000)
[Test]  Epoch: 97	Loss: 0.022158	Acc: 56.7% (5671/10000)
[Test]  Epoch: 98	Loss: 0.022244	Acc: 56.5% (5653/10000)
[Test]  Epoch: 99	Loss: 0.022211	Acc: 56.4% (5638/10000)
[Test]  Epoch: 100	Loss: 0.022200	Acc: 56.6% (5659/10000)
===========finish==========
['2024-08-19', '05:06:58.315254', '100', 'test', '0.022199773168563842', '56.59', '56.73']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.032097	Acc: 34.7% (3471/10000)
[Test]  Epoch: 2	Loss: 0.022501	Acc: 49.0% (4904/10000)
[Test]  Epoch: 3	Loss: 0.021615	Acc: 51.5% (5149/10000)
[Test]  Epoch: 4	Loss: 0.021746	Acc: 52.6% (5263/10000)
[Test]  Epoch: 5	Loss: 0.022127	Acc: 52.4% (5240/10000)
[Test]  Epoch: 6	Loss: 0.022061	Acc: 53.2% (5323/10000)
[Test]  Epoch: 7	Loss: 0.022097	Acc: 53.3% (5328/10000)
[Test]  Epoch: 8	Loss: 0.022163	Acc: 53.5% (5354/10000)
[Test]  Epoch: 9	Loss: 0.022322	Acc: 53.9% (5391/10000)
[Test]  Epoch: 10	Loss: 0.022213	Acc: 54.0% (5400/10000)
[Test]  Epoch: 11	Loss: 0.022335	Acc: 54.3% (5427/10000)
[Test]  Epoch: 12	Loss: 0.022496	Acc: 54.1% (5406/10000)
[Test]  Epoch: 13	Loss: 0.022332	Acc: 54.1% (5409/10000)
[Test]  Epoch: 14	Loss: 0.022510	Acc: 54.0% (5401/10000)
[Test]  Epoch: 15	Loss: 0.022535	Acc: 54.1% (5414/10000)
[Test]  Epoch: 16	Loss: 0.022365	Acc: 54.6% (5456/10000)
[Test]  Epoch: 17	Loss: 0.022477	Acc: 54.5% (5447/10000)
[Test]  Epoch: 18	Loss: 0.022672	Acc: 54.2% (5419/10000)
[Test]  Epoch: 19	Loss: 0.022536	Acc: 54.3% (5432/10000)
[Test]  Epoch: 20	Loss: 0.022395	Acc: 54.9% (5487/10000)
[Test]  Epoch: 21	Loss: 0.022407	Acc: 54.7% (5473/10000)
[Test]  Epoch: 22	Loss: 0.022345	Acc: 54.7% (5468/10000)
[Test]  Epoch: 23	Loss: 0.022274	Acc: 54.9% (5493/10000)
[Test]  Epoch: 24	Loss: 0.022458	Acc: 54.9% (5488/10000)
[Test]  Epoch: 25	Loss: 0.022466	Acc: 54.8% (5481/10000)
[Test]  Epoch: 26	Loss: 0.022410	Acc: 54.7% (5469/10000)
[Test]  Epoch: 27	Loss: 0.022493	Acc: 54.8% (5484/10000)
[Test]  Epoch: 28	Loss: 0.022424	Acc: 55.0% (5500/10000)
[Test]  Epoch: 29	Loss: 0.022567	Acc: 54.7% (5470/10000)
[Test]  Epoch: 30	Loss: 0.022454	Acc: 54.6% (5465/10000)
[Test]  Epoch: 31	Loss: 0.022456	Acc: 54.7% (5468/10000)
[Test]  Epoch: 32	Loss: 0.022343	Acc: 55.1% (5514/10000)
[Test]  Epoch: 33	Loss: 0.022256	Acc: 55.2% (5519/10000)
[Test]  Epoch: 34	Loss: 0.022459	Acc: 55.0% (5496/10000)
[Test]  Epoch: 35	Loss: 0.022405	Acc: 55.1% (5506/10000)
[Test]  Epoch: 36	Loss: 0.022293	Acc: 55.2% (5523/10000)
[Test]  Epoch: 37	Loss: 0.022519	Acc: 55.1% (5509/10000)
[Test]  Epoch: 38	Loss: 0.022373	Acc: 55.3% (5532/10000)
[Test]  Epoch: 39	Loss: 0.022414	Acc: 55.1% (5507/10000)
[Test]  Epoch: 40	Loss: 0.022457	Acc: 55.4% (5539/10000)
[Test]  Epoch: 41	Loss: 0.022398	Acc: 55.2% (5517/10000)
[Test]  Epoch: 42	Loss: 0.022270	Acc: 55.3% (5529/10000)
[Test]  Epoch: 43	Loss: 0.022531	Acc: 55.2% (5523/10000)
[Test]  Epoch: 44	Loss: 0.022423	Acc: 55.2% (5525/10000)
[Test]  Epoch: 45	Loss: 0.022449	Acc: 55.1% (5509/10000)
[Test]  Epoch: 46	Loss: 0.022333	Acc: 55.3% (5533/10000)
[Test]  Epoch: 47	Loss: 0.022395	Acc: 55.1% (5513/10000)
[Test]  Epoch: 48	Loss: 0.022419	Acc: 55.1% (5515/10000)
[Test]  Epoch: 49	Loss: 0.022349	Acc: 55.4% (5542/10000)
[Test]  Epoch: 50	Loss: 0.022433	Acc: 55.3% (5534/10000)
[Test]  Epoch: 51	Loss: 0.022332	Acc: 55.3% (5534/10000)
[Test]  Epoch: 52	Loss: 0.022259	Acc: 55.2% (5516/10000)
[Test]  Epoch: 53	Loss: 0.022383	Acc: 55.4% (5540/10000)
[Test]  Epoch: 54	Loss: 0.022261	Acc: 55.3% (5534/10000)
[Test]  Epoch: 55	Loss: 0.022159	Acc: 55.4% (5543/10000)
[Test]  Epoch: 56	Loss: 0.022246	Acc: 55.4% (5542/10000)
[Test]  Epoch: 57	Loss: 0.022136	Acc: 55.7% (5571/10000)
[Test]  Epoch: 58	Loss: 0.022232	Acc: 55.5% (5549/10000)
[Test]  Epoch: 59	Loss: 0.022398	Acc: 55.2% (5525/10000)
[Test]  Epoch: 60	Loss: 0.022499	Acc: 55.1% (5514/10000)
[Test]  Epoch: 61	Loss: 0.022381	Acc: 55.4% (5535/10000)
[Test]  Epoch: 62	Loss: 0.022384	Acc: 55.4% (5535/10000)
[Test]  Epoch: 63	Loss: 0.022321	Acc: 55.3% (5531/10000)
[Test]  Epoch: 64	Loss: 0.022337	Acc: 55.6% (5557/10000)
[Test]  Epoch: 65	Loss: 0.022270	Acc: 55.6% (5563/10000)
[Test]  Epoch: 66	Loss: 0.022249	Acc: 55.5% (5553/10000)
[Test]  Epoch: 67	Loss: 0.022369	Acc: 55.3% (5532/10000)
[Test]  Epoch: 68	Loss: 0.022237	Acc: 55.6% (5561/10000)
[Test]  Epoch: 69	Loss: 0.022265	Acc: 55.7% (5569/10000)
[Test]  Epoch: 70	Loss: 0.022263	Acc: 55.4% (5535/10000)
[Test]  Epoch: 71	Loss: 0.022297	Acc: 55.3% (5534/10000)
[Test]  Epoch: 72	Loss: 0.022327	Acc: 55.2% (5521/10000)
[Test]  Epoch: 73	Loss: 0.022271	Acc: 55.4% (5535/10000)
[Test]  Epoch: 74	Loss: 0.022242	Acc: 55.4% (5539/10000)
[Test]  Epoch: 75	Loss: 0.022309	Acc: 55.2% (5521/10000)
[Test]  Epoch: 76	Loss: 0.022269	Acc: 55.4% (5540/10000)
[Test]  Epoch: 77	Loss: 0.022289	Acc: 55.4% (5538/10000)
[Test]  Epoch: 78	Loss: 0.022318	Acc: 55.5% (5549/10000)
[Test]  Epoch: 79	Loss: 0.022252	Acc: 55.6% (5556/10000)
[Test]  Epoch: 80	Loss: 0.022293	Acc: 55.4% (5541/10000)
[Test]  Epoch: 81	Loss: 0.022259	Acc: 55.4% (5540/10000)
[Test]  Epoch: 82	Loss: 0.022294	Acc: 55.4% (5541/10000)
[Test]  Epoch: 83	Loss: 0.022282	Acc: 55.5% (5552/10000)
[Test]  Epoch: 84	Loss: 0.022253	Acc: 55.4% (5537/10000)
[Test]  Epoch: 85	Loss: 0.022251	Acc: 55.5% (5554/10000)
[Test]  Epoch: 86	Loss: 0.022251	Acc: 55.6% (5561/10000)
[Test]  Epoch: 87	Loss: 0.022267	Acc: 55.5% (5552/10000)
[Test]  Epoch: 88	Loss: 0.022327	Acc: 55.6% (5562/10000)
[Test]  Epoch: 89	Loss: 0.022299	Acc: 55.6% (5557/10000)
[Test]  Epoch: 90	Loss: 0.022287	Acc: 55.6% (5560/10000)
[Test]  Epoch: 91	Loss: 0.022301	Acc: 55.5% (5546/10000)
[Test]  Epoch: 92	Loss: 0.022325	Acc: 55.3% (5530/10000)
[Test]  Epoch: 93	Loss: 0.022268	Acc: 55.4% (5544/10000)
[Test]  Epoch: 94	Loss: 0.022257	Acc: 55.5% (5550/10000)
[Test]  Epoch: 95	Loss: 0.022299	Acc: 55.3% (5534/10000)
[Test]  Epoch: 96	Loss: 0.022216	Acc: 55.5% (5552/10000)
[Test]  Epoch: 97	Loss: 0.022243	Acc: 55.5% (5547/10000)
[Test]  Epoch: 98	Loss: 0.022318	Acc: 55.3% (5531/10000)
[Test]  Epoch: 99	Loss: 0.022259	Acc: 55.3% (5530/10000)
[Test]  Epoch: 100	Loss: 0.022291	Acc: 55.5% (5555/10000)
===========finish==========
['2024-08-19', '05:09:34.308983', '100', 'test', '0.02229109762907028', '55.55', '55.71']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'last_linear.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'last_linear.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.040141	Acc: 30.1% (3008/10000)
[Test]  Epoch: 2	Loss: 0.024230	Acc: 47.1% (4709/10000)
[Test]  Epoch: 3	Loss: 0.023273	Acc: 51.0% (5100/10000)
[Test]  Epoch: 4	Loss: 0.023692	Acc: 51.5% (5153/10000)
[Test]  Epoch: 5	Loss: 0.023948	Acc: 51.3% (5128/10000)
[Test]  Epoch: 6	Loss: 0.024309	Acc: 51.6% (5159/10000)
[Test]  Epoch: 7	Loss: 0.024040	Acc: 52.2% (5222/10000)
[Test]  Epoch: 8	Loss: 0.024426	Acc: 51.8% (5182/10000)
[Test]  Epoch: 9	Loss: 0.024192	Acc: 52.3% (5227/10000)
[Test]  Epoch: 10	Loss: 0.024437	Acc: 52.5% (5251/10000)
[Test]  Epoch: 11	Loss: 0.024202	Acc: 52.8% (5282/10000)
[Test]  Epoch: 12	Loss: 0.024283	Acc: 52.6% (5260/10000)
[Test]  Epoch: 13	Loss: 0.024154	Acc: 52.9% (5288/10000)
[Test]  Epoch: 14	Loss: 0.024138	Acc: 52.7% (5267/10000)
[Test]  Epoch: 15	Loss: 0.024266	Acc: 52.6% (5259/10000)
[Test]  Epoch: 16	Loss: 0.024298	Acc: 52.8% (5280/10000)
[Test]  Epoch: 17	Loss: 0.024090	Acc: 53.2% (5319/10000)
[Test]  Epoch: 18	Loss: 0.024187	Acc: 53.0% (5299/10000)
[Test]  Epoch: 19	Loss: 0.024133	Acc: 53.0% (5295/10000)
[Test]  Epoch: 20	Loss: 0.023899	Acc: 53.8% (5384/10000)
[Test]  Epoch: 21	Loss: 0.023920	Acc: 53.7% (5372/10000)
[Test]  Epoch: 22	Loss: 0.023810	Acc: 53.6% (5357/10000)
[Test]  Epoch: 23	Loss: 0.023790	Acc: 53.8% (5379/10000)
[Test]  Epoch: 24	Loss: 0.023846	Acc: 54.0% (5404/10000)
[Test]  Epoch: 25	Loss: 0.023829	Acc: 54.0% (5404/10000)
[Test]  Epoch: 26	Loss: 0.023751	Acc: 54.0% (5397/10000)
[Test]  Epoch: 27	Loss: 0.023732	Acc: 54.3% (5433/10000)
[Test]  Epoch: 28	Loss: 0.023853	Acc: 54.0% (5404/10000)
[Test]  Epoch: 29	Loss: 0.023966	Acc: 54.1% (5410/10000)
[Test]  Epoch: 30	Loss: 0.023858	Acc: 54.4% (5439/10000)
[Test]  Epoch: 31	Loss: 0.023619	Acc: 54.5% (5448/10000)
[Test]  Epoch: 32	Loss: 0.023693	Acc: 54.3% (5428/10000)
[Test]  Epoch: 33	Loss: 0.023766	Acc: 54.3% (5433/10000)
[Test]  Epoch: 34	Loss: 0.023918	Acc: 54.0% (5404/10000)
[Test]  Epoch: 35	Loss: 0.023730	Acc: 54.4% (5438/10000)
[Test]  Epoch: 36	Loss: 0.023742	Acc: 54.3% (5426/10000)
[Test]  Epoch: 37	Loss: 0.023789	Acc: 54.3% (5432/10000)
[Test]  Epoch: 38	Loss: 0.023833	Acc: 54.1% (5415/10000)
[Test]  Epoch: 39	Loss: 0.023875	Acc: 54.0% (5399/10000)
[Test]  Epoch: 40	Loss: 0.023908	Acc: 53.9% (5392/10000)
[Test]  Epoch: 41	Loss: 0.023727	Acc: 54.3% (5432/10000)
[Test]  Epoch: 42	Loss: 0.023707	Acc: 54.4% (5444/10000)
[Test]  Epoch: 43	Loss: 0.023764	Acc: 54.6% (5458/10000)
[Test]  Epoch: 44	Loss: 0.023718	Acc: 54.3% (5427/10000)
[Test]  Epoch: 45	Loss: 0.023784	Acc: 54.4% (5437/10000)
[Test]  Epoch: 46	Loss: 0.023717	Acc: 54.7% (5473/10000)
[Test]  Epoch: 47	Loss: 0.023738	Acc: 54.6% (5459/10000)
[Test]  Epoch: 48	Loss: 0.023799	Acc: 54.4% (5435/10000)
[Test]  Epoch: 49	Loss: 0.023865	Acc: 54.5% (5445/10000)
[Test]  Epoch: 50	Loss: 0.023770	Acc: 54.6% (5461/10000)
[Test]  Epoch: 51	Loss: 0.023631	Acc: 54.6% (5464/10000)
[Test]  Epoch: 52	Loss: 0.023673	Acc: 54.9% (5485/10000)
[Test]  Epoch: 53	Loss: 0.023649	Acc: 54.9% (5489/10000)
[Test]  Epoch: 54	Loss: 0.023739	Acc: 54.6% (5459/10000)
[Test]  Epoch: 55	Loss: 0.023480	Acc: 54.8% (5482/10000)
[Test]  Epoch: 56	Loss: 0.023666	Acc: 54.2% (5422/10000)
[Test]  Epoch: 57	Loss: 0.023508	Acc: 55.0% (5497/10000)
[Test]  Epoch: 58	Loss: 0.023564	Acc: 54.9% (5490/10000)
[Test]  Epoch: 59	Loss: 0.023680	Acc: 54.6% (5462/10000)
[Test]  Epoch: 60	Loss: 0.023652	Acc: 54.5% (5452/10000)
[Test]  Epoch: 61	Loss: 0.023597	Acc: 54.7% (5471/10000)
[Test]  Epoch: 62	Loss: 0.023558	Acc: 54.9% (5489/10000)
[Test]  Epoch: 63	Loss: 0.023605	Acc: 54.8% (5481/10000)
[Test]  Epoch: 64	Loss: 0.023641	Acc: 54.6% (5461/10000)
[Test]  Epoch: 65	Loss: 0.023572	Acc: 54.9% (5491/10000)
[Test]  Epoch: 66	Loss: 0.023485	Acc: 55.0% (5496/10000)
[Test]  Epoch: 67	Loss: 0.023513	Acc: 54.5% (5452/10000)
[Test]  Epoch: 68	Loss: 0.023503	Acc: 54.9% (5488/10000)
[Test]  Epoch: 69	Loss: 0.023562	Acc: 54.9% (5494/10000)
[Test]  Epoch: 70	Loss: 0.023525	Acc: 54.9% (5492/10000)
[Test]  Epoch: 71	Loss: 0.023551	Acc: 54.8% (5481/10000)
[Test]  Epoch: 72	Loss: 0.023563	Acc: 54.8% (5477/10000)
[Test]  Epoch: 73	Loss: 0.023511	Acc: 54.9% (5487/10000)
[Test]  Epoch: 74	Loss: 0.023413	Acc: 55.0% (5495/10000)
[Test]  Epoch: 75	Loss: 0.023446	Acc: 55.0% (5498/10000)
[Test]  Epoch: 76	Loss: 0.023426	Acc: 54.9% (5491/10000)
[Test]  Epoch: 77	Loss: 0.023472	Acc: 55.0% (5499/10000)
[Test]  Epoch: 78	Loss: 0.023461	Acc: 54.9% (5486/10000)
[Test]  Epoch: 79	Loss: 0.023451	Acc: 55.1% (5510/10000)
[Test]  Epoch: 80	Loss: 0.023460	Acc: 55.0% (5495/10000)
[Test]  Epoch: 81	Loss: 0.023477	Acc: 55.1% (5507/10000)
[Test]  Epoch: 82	Loss: 0.023465	Acc: 55.1% (5514/10000)
[Test]  Epoch: 83	Loss: 0.023470	Acc: 55.1% (5506/10000)
[Test]  Epoch: 84	Loss: 0.023435	Acc: 55.2% (5518/10000)
[Test]  Epoch: 85	Loss: 0.023443	Acc: 55.1% (5506/10000)
[Test]  Epoch: 86	Loss: 0.023453	Acc: 55.1% (5506/10000)
[Test]  Epoch: 87	Loss: 0.023457	Acc: 54.9% (5492/10000)
[Test]  Epoch: 88	Loss: 0.023512	Acc: 55.0% (5505/10000)
[Test]  Epoch: 89	Loss: 0.023481	Acc: 54.9% (5486/10000)
[Test]  Epoch: 90	Loss: 0.023505	Acc: 54.9% (5485/10000)
[Test]  Epoch: 91	Loss: 0.023538	Acc: 55.0% (5496/10000)
[Test]  Epoch: 92	Loss: 0.023468	Acc: 54.7% (5474/10000)
[Test]  Epoch: 93	Loss: 0.023472	Acc: 55.0% (5498/10000)
[Test]  Epoch: 94	Loss: 0.023469	Acc: 54.9% (5485/10000)
[Test]  Epoch: 95	Loss: 0.023436	Acc: 55.0% (5495/10000)
[Test]  Epoch: 96	Loss: 0.023378	Acc: 54.9% (5491/10000)
[Test]  Epoch: 97	Loss: 0.023491	Acc: 55.0% (5497/10000)
[Test]  Epoch: 98	Loss: 0.023560	Acc: 54.9% (5491/10000)
[Test]  Epoch: 99	Loss: 0.023498	Acc: 55.0% (5499/10000)
[Test]  Epoch: 100	Loss: 0.023437	Acc: 55.1% (5512/10000)
===========finish==========
['2024-08-19', '05:12:01.388729', '100', 'test', '0.023437311094999313', '55.12', '55.18']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-mobilenetv2/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('_features.0.0.weight', 0.0), ('_features.0.1.weight', 0.0), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.0.weight', 0.0), ('_features.1.conv.0.1.weight', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.1.weight', 0.0), ('_features.1.conv.2.weight', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.0.weight', 0.0), ('_features.2.conv.0.1.weight', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.0.weight', 0.0), ('_features.2.conv.1.1.weight', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.2.weight', 0.0), ('_features.2.conv.3.weight', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.0.weight', 0.0), ('_features.3.conv.0.1.weight', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.0.weight', 0.0), ('_features.3.conv.1.1.weight', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.2.weight', 0.0), ('_features.3.conv.3.weight', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.0.weight', 0.0), ('_features.4.conv.0.1.weight', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.0.weight', 0.0), ('_features.4.conv.1.1.weight', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.2.weight', 0.0), ('_features.4.conv.3.weight', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.0.weight', 0.0), ('_features.5.conv.0.1.weight', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.0.weight', 0.0), ('_features.5.conv.1.1.weight', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.2.weight', 0.0), ('_features.5.conv.3.weight', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.0.weight', 0.0), ('_features.6.conv.0.1.weight', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.0.weight', 0.0), ('_features.6.conv.1.1.weight', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.2.weight', 0.0), ('_features.6.conv.3.weight', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.0.weight', 0.0), ('_features.7.conv.0.1.weight', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.0.weight', 0.0), ('_features.7.conv.1.1.weight', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.2.weight', 0.0), ('_features.7.conv.3.weight', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.0.weight', 0.0), ('_features.8.conv.0.1.weight', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.0.weight', 0.0), ('_features.8.conv.1.1.weight', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.2.weight', 0.0), ('_features.8.conv.3.weight', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.0.weight', 0.0), ('_features.9.conv.0.1.weight', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.0.weight', 0.0), ('_features.9.conv.1.1.weight', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.2.weight', 0.0), ('_features.9.conv.3.weight', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.0.weight', 0.0), ('_features.10.conv.0.1.weight', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.0.weight', 0.0), ('_features.10.conv.1.1.weight', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.2.weight', 0.0), ('_features.10.conv.3.weight', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.0.weight', 0.0), ('_features.11.conv.0.1.weight', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.0.weight', 0.0), ('_features.11.conv.1.1.weight', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.2.weight', 0.0), ('_features.11.conv.3.weight', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.0.weight', 0.0), ('_features.12.conv.0.1.weight', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.0.weight', 0.0), ('_features.12.conv.1.1.weight', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.2.weight', 0.0), ('_features.12.conv.3.weight', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.0.weight', 0.0), ('_features.13.conv.0.1.weight', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.0.weight', 0.0), ('_features.13.conv.1.1.weight', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.2.weight', 0.0), ('_features.13.conv.3.weight', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.0.weight', 0.0), ('_features.14.conv.0.1.weight', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.0.weight', 0.0), ('_features.14.conv.1.1.weight', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.2.weight', 0.0), ('_features.14.conv.3.weight', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.0.weight', 0.0), ('_features.15.conv.0.1.weight', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.0.weight', 0.0), ('_features.15.conv.1.1.weight', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.2.weight', 0.0), ('_features.15.conv.3.weight', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.0.weight', 0.0), ('_features.16.conv.0.1.weight', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.0.weight', 0.0), ('_features.16.conv.1.1.weight', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.2.weight', 0.0), ('_features.16.conv.3.weight', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.0.weight', 0.0), ('_features.17.conv.0.1.weight', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.0.weight', 0.0), ('_features.17.conv.1.1.weight', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.2.weight', 0.0), ('_features.17.conv.3.weight', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.0.weight', 0.0), ('_features.18.1.weight', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('_features.0.0.weight', 0.0), ('_features.0.1.weight', 0.0), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.0.weight', 0.0), ('_features.1.conv.0.1.weight', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.1.weight', 0.0), ('_features.1.conv.2.weight', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.0.weight', 0.0), ('_features.2.conv.0.1.weight', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.0.weight', 0.0), ('_features.2.conv.1.1.weight', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.2.weight', 0.0), ('_features.2.conv.3.weight', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.0.weight', 0.0), ('_features.3.conv.0.1.weight', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.0.weight', 0.0), ('_features.3.conv.1.1.weight', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.2.weight', 0.0), ('_features.3.conv.3.weight', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.0.weight', 0.0), ('_features.4.conv.0.1.weight', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.0.weight', 0.0), ('_features.4.conv.1.1.weight', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.2.weight', 0.0), ('_features.4.conv.3.weight', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.0.weight', 0.0), ('_features.5.conv.0.1.weight', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.0.weight', 0.0), ('_features.5.conv.1.1.weight', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.2.weight', 0.0), ('_features.5.conv.3.weight', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.0.weight', 0.0), ('_features.6.conv.0.1.weight', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.0.weight', 0.0), ('_features.6.conv.1.1.weight', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.2.weight', 0.0), ('_features.6.conv.3.weight', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.0.weight', 0.0), ('_features.7.conv.0.1.weight', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.0.weight', 0.0), ('_features.7.conv.1.1.weight', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.2.weight', 0.0), ('_features.7.conv.3.weight', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.0.weight', 0.0), ('_features.8.conv.0.1.weight', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.0.weight', 0.0), ('_features.8.conv.1.1.weight', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.2.weight', 0.0), ('_features.8.conv.3.weight', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.0.weight', 0.0), ('_features.9.conv.0.1.weight', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.0.weight', 0.0), ('_features.9.conv.1.1.weight', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.2.weight', 0.0), ('_features.9.conv.3.weight', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.0.weight', 0.0), ('_features.10.conv.0.1.weight', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.0.weight', 0.0), ('_features.10.conv.1.1.weight', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.2.weight', 0.0), ('_features.10.conv.3.weight', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.0.weight', 0.0), ('_features.11.conv.0.1.weight', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.0.weight', 0.0), ('_features.11.conv.1.1.weight', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.2.weight', 0.0), ('_features.11.conv.3.weight', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.0.weight', 0.0), ('_features.12.conv.0.1.weight', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.0.weight', 0.0), ('_features.12.conv.1.1.weight', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.2.weight', 0.0), ('_features.12.conv.3.weight', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.0.weight', 0.0), ('_features.13.conv.0.1.weight', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.0.weight', 0.0), ('_features.13.conv.1.1.weight', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.2.weight', 0.0), ('_features.13.conv.3.weight', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.0.weight', 0.0), ('_features.14.conv.0.1.weight', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.0.weight', 0.0), ('_features.14.conv.1.1.weight', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.2.weight', 0.0), ('_features.14.conv.3.weight', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.0.weight', 0.0), ('_features.15.conv.0.1.weight', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.0.weight', 0.0), ('_features.15.conv.1.1.weight', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.2.weight', 0.0), ('_features.15.conv.3.weight', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.0.weight', 0.0), ('_features.16.conv.0.1.weight', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.0.weight', 0.0), ('_features.16.conv.1.1.weight', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.2.weight', 0.0), ('_features.16.conv.3.weight', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.0.weight', 0.0), ('_features.17.conv.0.1.weight', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.0.weight', 0.0), ('_features.17.conv.1.1.weight', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.2.weight', 0.0), ('_features.17.conv.3.weight', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.0.weight', 0.0), ('_features.18.1.weight', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=105  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.008447	Acc: 81.5% (8155/10000)
[Test]  Epoch: 2	Loss: 0.008332	Acc: 81.8% (8185/10000)
[Test]  Epoch: 3	Loss: 0.008337	Acc: 81.9% (8188/10000)
[Test]  Epoch: 4	Loss: 0.008349	Acc: 81.9% (8194/10000)
[Test]  Epoch: 5	Loss: 0.008300	Acc: 82.2% (8217/10000)
[Test]  Epoch: 6	Loss: 0.008307	Acc: 82.2% (8216/10000)
[Test]  Epoch: 7	Loss: 0.008332	Acc: 82.1% (8211/10000)
[Test]  Epoch: 8	Loss: 0.008320	Acc: 82.0% (8199/10000)
[Test]  Epoch: 9	Loss: 0.008361	Acc: 82.0% (8204/10000)
[Test]  Epoch: 10	Loss: 0.008258	Acc: 82.2% (8224/10000)
[Test]  Epoch: 11	Loss: 0.008260	Acc: 82.2% (8225/10000)
[Test]  Epoch: 12	Loss: 0.008229	Acc: 82.3% (8234/10000)
[Test]  Epoch: 13	Loss: 0.008224	Acc: 82.4% (8236/10000)
[Test]  Epoch: 14	Loss: 0.008217	Acc: 82.4% (8242/10000)
[Test]  Epoch: 15	Loss: 0.008205	Acc: 82.6% (8259/10000)
[Test]  Epoch: 16	Loss: 0.008292	Acc: 82.2% (8216/10000)
[Test]  Epoch: 17	Loss: 0.008201	Acc: 82.2% (8220/10000)
[Test]  Epoch: 18	Loss: 0.008258	Acc: 82.0% (8201/10000)
[Test]  Epoch: 19	Loss: 0.008196	Acc: 82.3% (8232/10000)
[Test]  Epoch: 20	Loss: 0.008181	Acc: 82.3% (8233/10000)
[Test]  Epoch: 21	Loss: 0.008198	Acc: 82.4% (8240/10000)
[Test]  Epoch: 22	Loss: 0.008198	Acc: 82.4% (8236/10000)
[Test]  Epoch: 23	Loss: 0.008231	Acc: 82.3% (8232/10000)
[Test]  Epoch: 24	Loss: 0.008238	Acc: 82.4% (8238/10000)
[Test]  Epoch: 25	Loss: 0.008180	Acc: 82.5% (8247/10000)
[Test]  Epoch: 26	Loss: 0.008214	Acc: 82.4% (8239/10000)
[Test]  Epoch: 27	Loss: 0.008268	Acc: 82.3% (8228/10000)
[Test]  Epoch: 28	Loss: 0.008252	Acc: 82.2% (8222/10000)
[Test]  Epoch: 29	Loss: 0.008249	Acc: 82.3% (8234/10000)
[Test]  Epoch: 30	Loss: 0.008215	Acc: 82.5% (8245/10000)
[Test]  Epoch: 31	Loss: 0.008177	Acc: 82.5% (8245/10000)
[Test]  Epoch: 32	Loss: 0.008218	Acc: 82.4% (8237/10000)
[Test]  Epoch: 33	Loss: 0.008248	Acc: 82.3% (8233/10000)
[Test]  Epoch: 34	Loss: 0.008175	Acc: 82.5% (8252/10000)
[Test]  Epoch: 35	Loss: 0.008199	Acc: 82.3% (8233/10000)
[Test]  Epoch: 36	Loss: 0.008220	Acc: 82.4% (8243/10000)
[Test]  Epoch: 37	Loss: 0.008211	Acc: 82.3% (8234/10000)
[Test]  Epoch: 38	Loss: 0.008251	Acc: 82.3% (8234/10000)
[Test]  Epoch: 39	Loss: 0.008249	Acc: 82.1% (8212/10000)
[Test]  Epoch: 40	Loss: 0.008229	Acc: 82.2% (8222/10000)
[Test]  Epoch: 41	Loss: 0.008203	Acc: 82.3% (8228/10000)
[Test]  Epoch: 42	Loss: 0.008257	Acc: 82.2% (8218/10000)
[Test]  Epoch: 43	Loss: 0.008253	Acc: 82.3% (8226/10000)
[Test]  Epoch: 44	Loss: 0.008265	Acc: 82.2% (8223/10000)
[Test]  Epoch: 45	Loss: 0.008248	Acc: 82.4% (8236/10000)
[Test]  Epoch: 46	Loss: 0.008258	Acc: 82.2% (8219/10000)
[Test]  Epoch: 47	Loss: 0.008232	Acc: 82.5% (8246/10000)
[Test]  Epoch: 48	Loss: 0.008227	Acc: 82.4% (8241/10000)
[Test]  Epoch: 49	Loss: 0.008206	Acc: 82.3% (8235/10000)
[Test]  Epoch: 50	Loss: 0.008223	Acc: 82.4% (8237/10000)
[Test]  Epoch: 51	Loss: 0.008237	Acc: 82.4% (8236/10000)
[Test]  Epoch: 52	Loss: 0.008236	Acc: 82.3% (8229/10000)
[Test]  Epoch: 53	Loss: 0.008261	Acc: 82.3% (8233/10000)
[Test]  Epoch: 54	Loss: 0.008220	Acc: 82.4% (8241/10000)
[Test]  Epoch: 55	Loss: 0.008267	Acc: 82.2% (8221/10000)
[Test]  Epoch: 56	Loss: 0.008252	Acc: 82.3% (8235/10000)
[Test]  Epoch: 57	Loss: 0.008280	Acc: 82.5% (8249/10000)
[Test]  Epoch: 58	Loss: 0.008267	Acc: 82.4% (8242/10000)
[Test]  Epoch: 59	Loss: 0.008280	Acc: 82.2% (8215/10000)
[Test]  Epoch: 60	Loss: 0.008271	Acc: 82.2% (8216/10000)
[Test]  Epoch: 61	Loss: 0.008262	Acc: 82.2% (8224/10000)
[Test]  Epoch: 62	Loss: 0.008274	Acc: 82.2% (8223/10000)
[Test]  Epoch: 63	Loss: 0.008284	Acc: 82.2% (8222/10000)
[Test]  Epoch: 64	Loss: 0.008234	Acc: 82.3% (8226/10000)
[Test]  Epoch: 65	Loss: 0.008242	Acc: 82.3% (8235/10000)
[Test]  Epoch: 66	Loss: 0.008249	Acc: 82.3% (8234/10000)
[Test]  Epoch: 67	Loss: 0.008240	Acc: 82.3% (8232/10000)
[Test]  Epoch: 68	Loss: 0.008242	Acc: 82.2% (8223/10000)
[Test]  Epoch: 69	Loss: 0.008266	Acc: 82.2% (8218/10000)
[Test]  Epoch: 70	Loss: 0.008238	Acc: 82.3% (8228/10000)
[Test]  Epoch: 71	Loss: 0.008234	Acc: 82.3% (8226/10000)
[Test]  Epoch: 72	Loss: 0.008221	Acc: 82.2% (8224/10000)
[Test]  Epoch: 73	Loss: 0.008252	Acc: 82.4% (8236/10000)
[Test]  Epoch: 74	Loss: 0.008246	Acc: 82.3% (8234/10000)
[Test]  Epoch: 75	Loss: 0.008246	Acc: 82.2% (8220/10000)
[Test]  Epoch: 76	Loss: 0.008233	Acc: 82.3% (8228/10000)
[Test]  Epoch: 77	Loss: 0.008240	Acc: 82.3% (8226/10000)
[Test]  Epoch: 78	Loss: 0.008220	Acc: 82.3% (8229/10000)
[Test]  Epoch: 79	Loss: 0.008239	Acc: 82.3% (8232/10000)
[Test]  Epoch: 80	Loss: 0.008250	Acc: 82.3% (8233/10000)
[Test]  Epoch: 81	Loss: 0.008229	Acc: 82.4% (8238/10000)
[Test]  Epoch: 82	Loss: 0.008245	Acc: 82.3% (8229/10000)
[Test]  Epoch: 83	Loss: 0.008231	Acc: 82.3% (8232/10000)
[Test]  Epoch: 84	Loss: 0.008235	Acc: 82.3% (8229/10000)
[Test]  Epoch: 85	Loss: 0.008263	Acc: 82.3% (8227/10000)
[Test]  Epoch: 86	Loss: 0.008235	Acc: 82.3% (8231/10000)
[Test]  Epoch: 87	Loss: 0.008254	Acc: 82.2% (8222/10000)
[Test]  Epoch: 88	Loss: 0.008242	Acc: 82.3% (8232/10000)
[Test]  Epoch: 89	Loss: 0.008233	Acc: 82.3% (8229/10000)
[Test]  Epoch: 90	Loss: 0.008241	Acc: 82.3% (8227/10000)
[Test]  Epoch: 91	Loss: 0.008243	Acc: 82.2% (8222/10000)
[Test]  Epoch: 92	Loss: 0.008227	Acc: 82.3% (8231/10000)
[Test]  Epoch: 93	Loss: 0.008242	Acc: 82.3% (8226/10000)
[Test]  Epoch: 94	Loss: 0.008255	Acc: 82.3% (8226/10000)
[Test]  Epoch: 95	Loss: 0.008240	Acc: 82.3% (8229/10000)
[Test]  Epoch: 96	Loss: 0.008231	Acc: 82.3% (8233/10000)
[Test]  Epoch: 97	Loss: 0.008209	Acc: 82.4% (8237/10000)
[Test]  Epoch: 98	Loss: 0.008244	Acc: 82.1% (8210/10000)
[Test]  Epoch: 99	Loss: 0.008251	Acc: 82.3% (8234/10000)
[Test]  Epoch: 100	Loss: 0.008265	Acc: 82.3% (8229/10000)
===========finish==========
['2024-08-19', '05:15:36.266669', '100', 'test', '0.008265061378479004', '82.29', '82.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.031448	Acc: 39.2% (3922/10000)
[Test]  Epoch: 2	Loss: 0.022979	Acc: 52.1% (5207/10000)
[Test]  Epoch: 3	Loss: 0.021590	Acc: 54.4% (5439/10000)
[Test]  Epoch: 4	Loss: 0.020549	Acc: 56.2% (5616/10000)
[Test]  Epoch: 5	Loss: 0.020149	Acc: 57.3% (5733/10000)
[Test]  Epoch: 6	Loss: 0.020011	Acc: 57.5% (5749/10000)
[Test]  Epoch: 7	Loss: 0.019962	Acc: 57.6% (5764/10000)
[Test]  Epoch: 8	Loss: 0.020101	Acc: 58.1% (5810/10000)
[Test]  Epoch: 9	Loss: 0.019906	Acc: 58.0% (5800/10000)
[Test]  Epoch: 10	Loss: 0.019882	Acc: 58.3% (5834/10000)
[Test]  Epoch: 11	Loss: 0.020027	Acc: 58.2% (5823/10000)
[Test]  Epoch: 12	Loss: 0.019744	Acc: 58.6% (5858/10000)
[Test]  Epoch: 13	Loss: 0.019895	Acc: 58.4% (5835/10000)
[Test]  Epoch: 14	Loss: 0.019809	Acc: 58.5% (5852/10000)
[Test]  Epoch: 15	Loss: 0.019758	Acc: 58.6% (5864/10000)
[Test]  Epoch: 16	Loss: 0.019908	Acc: 58.4% (5836/10000)
[Test]  Epoch: 17	Loss: 0.019694	Acc: 58.7% (5870/10000)
[Test]  Epoch: 18	Loss: 0.019782	Acc: 58.7% (5870/10000)
[Test]  Epoch: 19	Loss: 0.019700	Acc: 59.1% (5906/10000)
[Test]  Epoch: 20	Loss: 0.019919	Acc: 58.6% (5860/10000)
[Test]  Epoch: 21	Loss: 0.019716	Acc: 58.7% (5874/10000)
[Test]  Epoch: 22	Loss: 0.019532	Acc: 58.8% (5883/10000)
[Test]  Epoch: 23	Loss: 0.019509	Acc: 59.2% (5922/10000)
[Test]  Epoch: 24	Loss: 0.019440	Acc: 59.5% (5946/10000)
[Test]  Epoch: 25	Loss: 0.019656	Acc: 58.9% (5887/10000)
[Test]  Epoch: 26	Loss: 0.019341	Acc: 59.4% (5944/10000)
[Test]  Epoch: 27	Loss: 0.019458	Acc: 59.2% (5917/10000)
[Test]  Epoch: 28	Loss: 0.019274	Acc: 59.3% (5933/10000)
[Test]  Epoch: 29	Loss: 0.019441	Acc: 59.2% (5923/10000)
[Test]  Epoch: 30	Loss: 0.019351	Acc: 59.5% (5945/10000)
[Test]  Epoch: 31	Loss: 0.019272	Acc: 59.4% (5937/10000)
[Test]  Epoch: 32	Loss: 0.019114	Acc: 60.0% (6001/10000)
[Test]  Epoch: 33	Loss: 0.019145	Acc: 59.5% (5952/10000)
[Test]  Epoch: 34	Loss: 0.019350	Acc: 58.9% (5885/10000)
[Test]  Epoch: 35	Loss: 0.018994	Acc: 60.2% (6017/10000)
[Test]  Epoch: 36	Loss: 0.019208	Acc: 59.6% (5960/10000)
[Test]  Epoch: 37	Loss: 0.018968	Acc: 59.8% (5977/10000)
[Test]  Epoch: 38	Loss: 0.019275	Acc: 59.8% (5978/10000)
[Test]  Epoch: 39	Loss: 0.018918	Acc: 60.2% (6016/10000)
[Test]  Epoch: 40	Loss: 0.019148	Acc: 59.3% (5928/10000)
[Test]  Epoch: 41	Loss: 0.019055	Acc: 59.6% (5960/10000)
[Test]  Epoch: 42	Loss: 0.018963	Acc: 59.5% (5952/10000)
[Test]  Epoch: 43	Loss: 0.018976	Acc: 59.7% (5970/10000)
[Test]  Epoch: 44	Loss: 0.019001	Acc: 59.9% (5985/10000)
[Test]  Epoch: 45	Loss: 0.018875	Acc: 59.9% (5987/10000)
[Test]  Epoch: 46	Loss: 0.018903	Acc: 59.9% (5985/10000)
[Test]  Epoch: 47	Loss: 0.018884	Acc: 59.9% (5991/10000)
[Test]  Epoch: 48	Loss: 0.018657	Acc: 60.4% (6044/10000)
[Test]  Epoch: 49	Loss: 0.018788	Acc: 60.4% (6041/10000)
[Test]  Epoch: 50	Loss: 0.018787	Acc: 59.8% (5980/10000)
[Test]  Epoch: 51	Loss: 0.018841	Acc: 60.0% (5995/10000)
[Test]  Epoch: 52	Loss: 0.018847	Acc: 60.0% (5996/10000)
[Test]  Epoch: 53	Loss: 0.018798	Acc: 60.2% (6023/10000)
[Test]  Epoch: 54	Loss: 0.018733	Acc: 60.1% (6013/10000)
[Test]  Epoch: 55	Loss: 0.018623	Acc: 60.2% (6021/10000)
[Test]  Epoch: 56	Loss: 0.018943	Acc: 60.0% (6002/10000)
[Test]  Epoch: 57	Loss: 0.018661	Acc: 60.6% (6058/10000)
[Test]  Epoch: 58	Loss: 0.018739	Acc: 60.2% (6017/10000)
[Test]  Epoch: 59	Loss: 0.018639	Acc: 60.3% (6032/10000)
[Test]  Epoch: 60	Loss: 0.018656	Acc: 60.4% (6038/10000)
[Test]  Epoch: 61	Loss: 0.018629	Acc: 60.5% (6050/10000)
[Test]  Epoch: 62	Loss: 0.018593	Acc: 60.8% (6079/10000)
[Test]  Epoch: 63	Loss: 0.018554	Acc: 60.7% (6074/10000)
[Test]  Epoch: 64	Loss: 0.018593	Acc: 60.7% (6067/10000)
[Test]  Epoch: 65	Loss: 0.018578	Acc: 60.8% (6076/10000)
[Test]  Epoch: 66	Loss: 0.018585	Acc: 60.7% (6069/10000)
[Test]  Epoch: 67	Loss: 0.018605	Acc: 60.5% (6051/10000)
[Test]  Epoch: 68	Loss: 0.018572	Acc: 60.5% (6055/10000)
[Test]  Epoch: 69	Loss: 0.018648	Acc: 60.7% (6066/10000)
[Test]  Epoch: 70	Loss: 0.018612	Acc: 60.6% (6064/10000)
[Test]  Epoch: 71	Loss: 0.018597	Acc: 60.5% (6050/10000)
[Test]  Epoch: 72	Loss: 0.018516	Acc: 60.6% (6060/10000)
[Test]  Epoch: 73	Loss: 0.018549	Acc: 60.6% (6059/10000)
[Test]  Epoch: 74	Loss: 0.018517	Acc: 60.5% (6049/10000)
[Test]  Epoch: 75	Loss: 0.018545	Acc: 60.6% (6058/10000)
[Test]  Epoch: 76	Loss: 0.018576	Acc: 60.6% (6065/10000)
[Test]  Epoch: 77	Loss: 0.018585	Acc: 60.6% (6058/10000)
[Test]  Epoch: 78	Loss: 0.018553	Acc: 60.6% (6058/10000)
[Test]  Epoch: 79	Loss: 0.018507	Acc: 60.7% (6069/10000)
[Test]  Epoch: 80	Loss: 0.018573	Acc: 60.8% (6076/10000)
[Test]  Epoch: 81	Loss: 0.018622	Acc: 60.7% (6067/10000)
[Test]  Epoch: 82	Loss: 0.018566	Acc: 60.6% (6056/10000)
[Test]  Epoch: 83	Loss: 0.018593	Acc: 60.7% (6070/10000)
[Test]  Epoch: 84	Loss: 0.018540	Acc: 60.7% (6067/10000)
[Test]  Epoch: 85	Loss: 0.018623	Acc: 60.6% (6056/10000)
[Test]  Epoch: 86	Loss: 0.018540	Acc: 60.7% (6066/10000)
[Test]  Epoch: 87	Loss: 0.018558	Acc: 60.4% (6035/10000)
[Test]  Epoch: 88	Loss: 0.018555	Acc: 60.6% (6058/10000)
[Test]  Epoch: 89	Loss: 0.018628	Acc: 60.6% (6059/10000)
[Test]  Epoch: 90	Loss: 0.018614	Acc: 60.6% (6059/10000)
[Test]  Epoch: 91	Loss: 0.018582	Acc: 60.5% (6045/10000)
[Test]  Epoch: 92	Loss: 0.018553	Acc: 60.6% (6063/10000)
[Test]  Epoch: 93	Loss: 0.018544	Acc: 60.6% (6062/10000)
[Test]  Epoch: 94	Loss: 0.018583	Acc: 60.5% (6052/10000)
[Test]  Epoch: 95	Loss: 0.018543	Acc: 60.5% (6051/10000)
[Test]  Epoch: 96	Loss: 0.018546	Acc: 60.6% (6065/10000)
[Test]  Epoch: 97	Loss: 0.018536	Acc: 60.7% (6066/10000)
[Test]  Epoch: 98	Loss: 0.018537	Acc: 60.5% (6055/10000)
[Test]  Epoch: 99	Loss: 0.018559	Acc: 60.5% (6049/10000)
[Test]  Epoch: 100	Loss: 0.018587	Acc: 60.5% (6052/10000)
===========finish==========
['2024-08-19', '05:18:21.479072', '100', 'test', '0.01858749848008156', '60.52', '60.79']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.067365	Acc: 22.6% (2260/10000)
[Test]  Epoch: 2	Loss: 0.024293	Acc: 47.3% (4730/10000)
[Test]  Epoch: 3	Loss: 0.023286	Acc: 49.8% (4976/10000)
[Test]  Epoch: 4	Loss: 0.023055	Acc: 50.4% (5044/10000)
[Test]  Epoch: 5	Loss: 0.022981	Acc: 50.8% (5077/10000)
[Test]  Epoch: 6	Loss: 0.022863	Acc: 51.6% (5156/10000)
[Test]  Epoch: 7	Loss: 0.022789	Acc: 51.5% (5152/10000)
[Test]  Epoch: 8	Loss: 0.023062	Acc: 51.5% (5153/10000)
[Test]  Epoch: 9	Loss: 0.023164	Acc: 51.1% (5112/10000)
[Test]  Epoch: 10	Loss: 0.022987	Acc: 52.3% (5231/10000)
[Test]  Epoch: 11	Loss: 0.023093	Acc: 51.7% (5167/10000)
[Test]  Epoch: 12	Loss: 0.022966	Acc: 52.1% (5214/10000)
[Test]  Epoch: 13	Loss: 0.022932	Acc: 52.4% (5237/10000)
[Test]  Epoch: 14	Loss: 0.022905	Acc: 52.4% (5239/10000)
[Test]  Epoch: 15	Loss: 0.023043	Acc: 52.9% (5286/10000)
[Test]  Epoch: 16	Loss: 0.022973	Acc: 52.2% (5217/10000)
[Test]  Epoch: 17	Loss: 0.022655	Acc: 52.9% (5291/10000)
[Test]  Epoch: 18	Loss: 0.022717	Acc: 52.8% (5283/10000)
[Test]  Epoch: 19	Loss: 0.022894	Acc: 52.4% (5240/10000)
[Test]  Epoch: 20	Loss: 0.022822	Acc: 53.5% (5351/10000)
[Test]  Epoch: 21	Loss: 0.022836	Acc: 53.3% (5331/10000)
[Test]  Epoch: 22	Loss: 0.022809	Acc: 53.1% (5313/10000)
[Test]  Epoch: 23	Loss: 0.022701	Acc: 53.2% (5317/10000)
[Test]  Epoch: 24	Loss: 0.022712	Acc: 52.8% (5281/10000)
[Test]  Epoch: 25	Loss: 0.022587	Acc: 53.2% (5325/10000)
[Test]  Epoch: 26	Loss: 0.022503	Acc: 53.6% (5359/10000)
[Test]  Epoch: 27	Loss: 0.022223	Acc: 53.1% (5311/10000)
[Test]  Epoch: 28	Loss: 0.022322	Acc: 53.4% (5341/10000)
[Test]  Epoch: 29	Loss: 0.022409	Acc: 53.5% (5348/10000)
[Test]  Epoch: 30	Loss: 0.022252	Acc: 54.0% (5401/10000)
[Test]  Epoch: 31	Loss: 0.022067	Acc: 53.6% (5359/10000)
[Test]  Epoch: 32	Loss: 0.022258	Acc: 53.8% (5377/10000)
[Test]  Epoch: 33	Loss: 0.022088	Acc: 53.8% (5379/10000)
[Test]  Epoch: 34	Loss: 0.022084	Acc: 53.6% (5365/10000)
[Test]  Epoch: 35	Loss: 0.022109	Acc: 53.6% (5363/10000)
[Test]  Epoch: 36	Loss: 0.022237	Acc: 53.8% (5384/10000)
[Test]  Epoch: 37	Loss: 0.022099	Acc: 53.7% (5368/10000)
[Test]  Epoch: 38	Loss: 0.022035	Acc: 54.0% (5404/10000)
[Test]  Epoch: 39	Loss: 0.021864	Acc: 54.2% (5418/10000)
[Test]  Epoch: 40	Loss: 0.021673	Acc: 54.5% (5451/10000)
[Test]  Epoch: 41	Loss: 0.022034	Acc: 54.1% (5414/10000)
[Test]  Epoch: 42	Loss: 0.021783	Acc: 54.3% (5433/10000)
[Test]  Epoch: 43	Loss: 0.021836	Acc: 53.8% (5377/10000)
[Test]  Epoch: 44	Loss: 0.021735	Acc: 54.3% (5431/10000)
[Test]  Epoch: 45	Loss: 0.021722	Acc: 54.3% (5431/10000)
[Test]  Epoch: 46	Loss: 0.021805	Acc: 54.1% (5407/10000)
[Test]  Epoch: 47	Loss: 0.021703	Acc: 54.4% (5437/10000)
[Test]  Epoch: 48	Loss: 0.021791	Acc: 54.2% (5421/10000)
[Test]  Epoch: 49	Loss: 0.021839	Acc: 54.1% (5407/10000)
[Test]  Epoch: 50	Loss: 0.021681	Acc: 54.1% (5414/10000)
[Test]  Epoch: 51	Loss: 0.021763	Acc: 54.2% (5416/10000)
[Test]  Epoch: 52	Loss: 0.021698	Acc: 53.8% (5380/10000)
[Test]  Epoch: 53	Loss: 0.021650	Acc: 54.1% (5406/10000)
[Test]  Epoch: 54	Loss: 0.021609	Acc: 54.1% (5407/10000)
[Test]  Epoch: 55	Loss: 0.021754	Acc: 54.0% (5397/10000)
[Test]  Epoch: 56	Loss: 0.021881	Acc: 53.9% (5386/10000)
[Test]  Epoch: 57	Loss: 0.021755	Acc: 53.7% (5374/10000)
[Test]  Epoch: 58	Loss: 0.021686	Acc: 54.6% (5464/10000)
[Test]  Epoch: 59	Loss: 0.021746	Acc: 54.2% (5416/10000)
[Test]  Epoch: 60	Loss: 0.021567	Acc: 54.5% (5449/10000)
[Test]  Epoch: 61	Loss: 0.021522	Acc: 54.5% (5455/10000)
[Test]  Epoch: 62	Loss: 0.021487	Acc: 54.6% (5459/10000)
[Test]  Epoch: 63	Loss: 0.021419	Acc: 54.6% (5459/10000)
[Test]  Epoch: 64	Loss: 0.021477	Acc: 54.5% (5450/10000)
[Test]  Epoch: 65	Loss: 0.021463	Acc: 54.5% (5451/10000)
[Test]  Epoch: 66	Loss: 0.021497	Acc: 54.3% (5429/10000)
[Test]  Epoch: 67	Loss: 0.021543	Acc: 54.3% (5433/10000)
[Test]  Epoch: 68	Loss: 0.021446	Acc: 54.5% (5446/10000)
[Test]  Epoch: 69	Loss: 0.021501	Acc: 54.4% (5438/10000)
[Test]  Epoch: 70	Loss: 0.021468	Acc: 54.7% (5466/10000)
[Test]  Epoch: 71	Loss: 0.021502	Acc: 54.3% (5431/10000)
[Test]  Epoch: 72	Loss: 0.021467	Acc: 54.7% (5469/10000)
[Test]  Epoch: 73	Loss: 0.021409	Acc: 54.5% (5454/10000)
[Test]  Epoch: 74	Loss: 0.021444	Acc: 54.4% (5439/10000)
[Test]  Epoch: 75	Loss: 0.021420	Acc: 54.3% (5429/10000)
[Test]  Epoch: 76	Loss: 0.021459	Acc: 54.5% (5449/10000)
[Test]  Epoch: 77	Loss: 0.021465	Acc: 54.5% (5451/10000)
[Test]  Epoch: 78	Loss: 0.021430	Acc: 54.6% (5460/10000)
[Test]  Epoch: 79	Loss: 0.021432	Acc: 54.6% (5463/10000)
[Test]  Epoch: 80	Loss: 0.021457	Acc: 54.2% (5422/10000)
[Test]  Epoch: 81	Loss: 0.021524	Acc: 54.3% (5434/10000)
[Test]  Epoch: 82	Loss: 0.021537	Acc: 54.4% (5444/10000)
[Test]  Epoch: 83	Loss: 0.021470	Acc: 54.4% (5439/10000)
[Test]  Epoch: 84	Loss: 0.021457	Acc: 54.4% (5436/10000)
[Test]  Epoch: 85	Loss: 0.021487	Acc: 54.5% (5446/10000)
[Test]  Epoch: 86	Loss: 0.021439	Acc: 54.6% (5458/10000)
[Test]  Epoch: 87	Loss: 0.021414	Acc: 54.5% (5445/10000)
[Test]  Epoch: 88	Loss: 0.021402	Acc: 54.4% (5438/10000)
[Test]  Epoch: 89	Loss: 0.021447	Acc: 54.4% (5443/10000)
[Test]  Epoch: 90	Loss: 0.021439	Acc: 54.3% (5434/10000)
[Test]  Epoch: 91	Loss: 0.021397	Acc: 54.6% (5459/10000)
[Test]  Epoch: 92	Loss: 0.021438	Acc: 54.5% (5448/10000)
[Test]  Epoch: 93	Loss: 0.021436	Acc: 54.6% (5461/10000)
[Test]  Epoch: 94	Loss: 0.021447	Acc: 54.5% (5447/10000)
[Test]  Epoch: 95	Loss: 0.021429	Acc: 54.5% (5446/10000)
[Test]  Epoch: 96	Loss: 0.021464	Acc: 54.4% (5439/10000)
[Test]  Epoch: 97	Loss: 0.021439	Acc: 54.6% (5462/10000)
[Test]  Epoch: 98	Loss: 0.021467	Acc: 54.5% (5446/10000)
[Test]  Epoch: 99	Loss: 0.021464	Acc: 54.4% (5442/10000)
[Test]  Epoch: 100	Loss: 0.021477	Acc: 54.5% (5449/10000)
===========finish==========
['2024-08-19', '05:21:06.653202', '100', 'test', '0.021477039456367492', '54.49', '54.69']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.075784	Acc: 21.8% (2178/10000)
[Test]  Epoch: 2	Loss: 0.026373	Acc: 42.1% (4211/10000)
[Test]  Epoch: 3	Loss: 0.024955	Acc: 45.6% (4562/10000)
[Test]  Epoch: 4	Loss: 0.024926	Acc: 45.6% (4563/10000)
[Test]  Epoch: 5	Loss: 0.024316	Acc: 47.3% (4728/10000)
[Test]  Epoch: 6	Loss: 0.024156	Acc: 48.1% (4814/10000)
[Test]  Epoch: 7	Loss: 0.024109	Acc: 48.5% (4852/10000)
[Test]  Epoch: 8	Loss: 0.024123	Acc: 48.8% (4875/10000)
[Test]  Epoch: 9	Loss: 0.024174	Acc: 49.0% (4896/10000)
[Test]  Epoch: 10	Loss: 0.024293	Acc: 49.3% (4927/10000)
[Test]  Epoch: 11	Loss: 0.024399	Acc: 49.0% (4902/10000)
[Test]  Epoch: 12	Loss: 0.024559	Acc: 49.0% (4903/10000)
[Test]  Epoch: 13	Loss: 0.023873	Acc: 49.7% (4971/10000)
[Test]  Epoch: 14	Loss: 0.023822	Acc: 49.2% (4918/10000)
[Test]  Epoch: 15	Loss: 0.024011	Acc: 50.2% (5024/10000)
[Test]  Epoch: 16	Loss: 0.024148	Acc: 49.8% (4980/10000)
[Test]  Epoch: 17	Loss: 0.023807	Acc: 50.0% (5004/10000)
[Test]  Epoch: 18	Loss: 0.023777	Acc: 50.0% (5001/10000)
[Test]  Epoch: 19	Loss: 0.023753	Acc: 49.7% (4967/10000)
[Test]  Epoch: 20	Loss: 0.023805	Acc: 49.9% (4988/10000)
[Test]  Epoch: 21	Loss: 0.023983	Acc: 50.3% (5026/10000)
[Test]  Epoch: 22	Loss: 0.023597	Acc: 50.1% (5013/10000)
[Test]  Epoch: 23	Loss: 0.023597	Acc: 50.4% (5044/10000)
[Test]  Epoch: 24	Loss: 0.023655	Acc: 50.1% (5011/10000)
[Test]  Epoch: 25	Loss: 0.023654	Acc: 50.4% (5040/10000)
[Test]  Epoch: 26	Loss: 0.023311	Acc: 50.3% (5030/10000)
[Test]  Epoch: 27	Loss: 0.023634	Acc: 50.0% (4995/10000)
[Test]  Epoch: 28	Loss: 0.023513	Acc: 50.2% (5023/10000)
[Test]  Epoch: 29	Loss: 0.023257	Acc: 50.5% (5053/10000)
[Test]  Epoch: 30	Loss: 0.023195	Acc: 50.8% (5077/10000)
[Test]  Epoch: 31	Loss: 0.023085	Acc: 51.0% (5097/10000)
[Test]  Epoch: 32	Loss: 0.023111	Acc: 51.0% (5099/10000)
[Test]  Epoch: 33	Loss: 0.022946	Acc: 51.1% (5109/10000)
[Test]  Epoch: 34	Loss: 0.022959	Acc: 50.9% (5086/10000)
[Test]  Epoch: 35	Loss: 0.022904	Acc: 51.0% (5100/10000)
[Test]  Epoch: 36	Loss: 0.023089	Acc: 50.9% (5090/10000)
[Test]  Epoch: 37	Loss: 0.022844	Acc: 51.5% (5148/10000)
[Test]  Epoch: 38	Loss: 0.022831	Acc: 51.8% (5176/10000)
[Test]  Epoch: 39	Loss: 0.022836	Acc: 51.6% (5157/10000)
[Test]  Epoch: 40	Loss: 0.022715	Acc: 51.8% (5182/10000)
[Test]  Epoch: 41	Loss: 0.022801	Acc: 51.4% (5142/10000)
[Test]  Epoch: 42	Loss: 0.022721	Acc: 51.6% (5157/10000)
[Test]  Epoch: 43	Loss: 0.022607	Acc: 51.8% (5181/10000)
[Test]  Epoch: 44	Loss: 0.022648	Acc: 51.8% (5183/10000)
[Test]  Epoch: 45	Loss: 0.022686	Acc: 51.4% (5144/10000)
[Test]  Epoch: 46	Loss: 0.022723	Acc: 51.4% (5140/10000)
[Test]  Epoch: 47	Loss: 0.022559	Acc: 51.8% (5181/10000)
[Test]  Epoch: 48	Loss: 0.022525	Acc: 52.1% (5209/10000)
[Test]  Epoch: 49	Loss: 0.022554	Acc: 51.7% (5171/10000)
[Test]  Epoch: 50	Loss: 0.022622	Acc: 52.0% (5195/10000)
[Test]  Epoch: 51	Loss: 0.022595	Acc: 52.1% (5207/10000)
[Test]  Epoch: 52	Loss: 0.022522	Acc: 52.2% (5219/10000)
[Test]  Epoch: 53	Loss: 0.022549	Acc: 52.2% (5220/10000)
[Test]  Epoch: 54	Loss: 0.022296	Acc: 52.1% (5209/10000)
[Test]  Epoch: 55	Loss: 0.022439	Acc: 51.9% (5189/10000)
[Test]  Epoch: 56	Loss: 0.022281	Acc: 52.3% (5226/10000)
[Test]  Epoch: 57	Loss: 0.022372	Acc: 52.1% (5207/10000)
[Test]  Epoch: 58	Loss: 0.022338	Acc: 52.2% (5221/10000)
[Test]  Epoch: 59	Loss: 0.022269	Acc: 52.4% (5243/10000)
[Test]  Epoch: 60	Loss: 0.022336	Acc: 52.3% (5228/10000)
[Test]  Epoch: 61	Loss: 0.022272	Acc: 52.2% (5221/10000)
[Test]  Epoch: 62	Loss: 0.022219	Acc: 52.2% (5219/10000)
[Test]  Epoch: 63	Loss: 0.022173	Acc: 52.3% (5231/10000)
[Test]  Epoch: 64	Loss: 0.022231	Acc: 52.2% (5221/10000)
[Test]  Epoch: 65	Loss: 0.022170	Acc: 52.3% (5230/10000)
[Test]  Epoch: 66	Loss: 0.022172	Acc: 52.2% (5222/10000)
[Test]  Epoch: 67	Loss: 0.022197	Acc: 52.1% (5210/10000)
[Test]  Epoch: 68	Loss: 0.022204	Acc: 52.3% (5229/10000)
[Test]  Epoch: 69	Loss: 0.022157	Acc: 52.4% (5237/10000)
[Test]  Epoch: 70	Loss: 0.022158	Acc: 52.2% (5223/10000)
[Test]  Epoch: 71	Loss: 0.022170	Acc: 52.3% (5227/10000)
[Test]  Epoch: 72	Loss: 0.022191	Acc: 52.4% (5240/10000)
[Test]  Epoch: 73	Loss: 0.022164	Acc: 52.2% (5217/10000)
[Test]  Epoch: 74	Loss: 0.022140	Acc: 52.4% (5237/10000)
[Test]  Epoch: 75	Loss: 0.022100	Acc: 52.3% (5230/10000)
[Test]  Epoch: 76	Loss: 0.022086	Acc: 52.2% (5224/10000)
[Test]  Epoch: 77	Loss: 0.022088	Acc: 52.4% (5241/10000)
[Test]  Epoch: 78	Loss: 0.022133	Acc: 52.4% (5243/10000)
[Test]  Epoch: 79	Loss: 0.022115	Acc: 52.5% (5252/10000)
[Test]  Epoch: 80	Loss: 0.022130	Acc: 52.3% (5226/10000)
[Test]  Epoch: 81	Loss: 0.022190	Acc: 52.3% (5233/10000)
[Test]  Epoch: 82	Loss: 0.022170	Acc: 52.4% (5237/10000)
[Test]  Epoch: 83	Loss: 0.022165	Acc: 52.5% (5248/10000)
[Test]  Epoch: 84	Loss: 0.022156	Acc: 52.5% (5248/10000)
[Test]  Epoch: 85	Loss: 0.022108	Acc: 52.2% (5225/10000)
[Test]  Epoch: 86	Loss: 0.022104	Acc: 52.3% (5234/10000)
[Test]  Epoch: 87	Loss: 0.022118	Acc: 52.3% (5229/10000)
[Test]  Epoch: 88	Loss: 0.022105	Acc: 52.4% (5238/10000)
[Test]  Epoch: 89	Loss: 0.022208	Acc: 52.4% (5238/10000)
[Test]  Epoch: 90	Loss: 0.022161	Acc: 52.4% (5237/10000)
[Test]  Epoch: 91	Loss: 0.022103	Acc: 52.2% (5220/10000)
[Test]  Epoch: 92	Loss: 0.022137	Acc: 52.2% (5225/10000)
[Test]  Epoch: 93	Loss: 0.022144	Acc: 52.3% (5234/10000)
[Test]  Epoch: 94	Loss: 0.022151	Acc: 52.3% (5234/10000)
[Test]  Epoch: 95	Loss: 0.022129	Acc: 52.4% (5244/10000)
[Test]  Epoch: 96	Loss: 0.022116	Acc: 52.5% (5249/10000)
[Test]  Epoch: 97	Loss: 0.022109	Acc: 52.4% (5243/10000)
[Test]  Epoch: 98	Loss: 0.022132	Acc: 52.4% (5236/10000)
[Test]  Epoch: 99	Loss: 0.022083	Acc: 52.3% (5233/10000)
[Test]  Epoch: 100	Loss: 0.022103	Acc: 52.3% (5233/10000)
===========finish==========
['2024-08-19', '05:23:48.551522', '100', 'test', '0.022102917098999024', '52.33', '52.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.055432	Acc: 26.7% (2667/10000)
[Test]  Epoch: 2	Loss: 0.026178	Acc: 44.5% (4447/10000)
[Test]  Epoch: 3	Loss: 0.023357	Acc: 47.9% (4789/10000)
[Test]  Epoch: 4	Loss: 0.022884	Acc: 49.4% (4940/10000)
[Test]  Epoch: 5	Loss: 0.022928	Acc: 50.2% (5016/10000)
[Test]  Epoch: 6	Loss: 0.022728	Acc: 50.3% (5028/10000)
[Test]  Epoch: 7	Loss: 0.022514	Acc: 51.1% (5110/10000)
[Test]  Epoch: 8	Loss: 0.022558	Acc: 51.1% (5107/10000)
[Test]  Epoch: 9	Loss: 0.022673	Acc: 50.9% (5093/10000)
[Test]  Epoch: 10	Loss: 0.022548	Acc: 51.1% (5114/10000)
[Test]  Epoch: 11	Loss: 0.022869	Acc: 50.9% (5088/10000)
[Test]  Epoch: 12	Loss: 0.022645	Acc: 51.4% (5140/10000)
[Test]  Epoch: 13	Loss: 0.022549	Acc: 51.3% (5128/10000)
[Test]  Epoch: 14	Loss: 0.022441	Acc: 51.5% (5155/10000)
[Test]  Epoch: 15	Loss: 0.022413	Acc: 51.9% (5186/10000)
[Test]  Epoch: 16	Loss: 0.022574	Acc: 51.3% (5128/10000)
[Test]  Epoch: 17	Loss: 0.022431	Acc: 52.3% (5230/10000)
[Test]  Epoch: 18	Loss: 0.022388	Acc: 52.0% (5205/10000)
[Test]  Epoch: 19	Loss: 0.022190	Acc: 52.5% (5246/10000)
[Test]  Epoch: 20	Loss: 0.022171	Acc: 52.5% (5254/10000)
[Test]  Epoch: 21	Loss: 0.022011	Acc: 52.9% (5286/10000)
[Test]  Epoch: 22	Loss: 0.021873	Acc: 52.7% (5273/10000)
[Test]  Epoch: 23	Loss: 0.022115	Acc: 52.4% (5238/10000)
[Test]  Epoch: 24	Loss: 0.021953	Acc: 53.0% (5303/10000)
[Test]  Epoch: 25	Loss: 0.021977	Acc: 52.8% (5275/10000)
[Test]  Epoch: 26	Loss: 0.021916	Acc: 52.6% (5263/10000)
[Test]  Epoch: 27	Loss: 0.022019	Acc: 52.4% (5244/10000)
[Test]  Epoch: 28	Loss: 0.021968	Acc: 53.1% (5306/10000)
[Test]  Epoch: 29	Loss: 0.022006	Acc: 52.4% (5243/10000)
[Test]  Epoch: 30	Loss: 0.021820	Acc: 52.9% (5291/10000)
[Test]  Epoch: 31	Loss: 0.021786	Acc: 52.7% (5268/10000)
[Test]  Epoch: 32	Loss: 0.021560	Acc: 53.0% (5295/10000)
[Test]  Epoch: 33	Loss: 0.021769	Acc: 53.0% (5295/10000)
[Test]  Epoch: 34	Loss: 0.021682	Acc: 52.9% (5286/10000)
[Test]  Epoch: 35	Loss: 0.021880	Acc: 52.6% (5265/10000)
[Test]  Epoch: 36	Loss: 0.021587	Acc: 53.1% (5312/10000)
[Test]  Epoch: 37	Loss: 0.021673	Acc: 52.8% (5280/10000)
[Test]  Epoch: 38	Loss: 0.021795	Acc: 52.9% (5292/10000)
[Test]  Epoch: 39	Loss: 0.021490	Acc: 53.2% (5316/10000)
[Test]  Epoch: 40	Loss: 0.021500	Acc: 53.1% (5308/10000)
[Test]  Epoch: 41	Loss: 0.021424	Acc: 53.6% (5360/10000)
[Test]  Epoch: 42	Loss: 0.021439	Acc: 53.2% (5319/10000)
[Test]  Epoch: 43	Loss: 0.021439	Acc: 53.4% (5338/10000)
[Test]  Epoch: 44	Loss: 0.021385	Acc: 53.3% (5334/10000)
[Test]  Epoch: 45	Loss: 0.021561	Acc: 53.2% (5321/10000)
[Test]  Epoch: 46	Loss: 0.021556	Acc: 53.2% (5319/10000)
[Test]  Epoch: 47	Loss: 0.021514	Acc: 53.2% (5316/10000)
[Test]  Epoch: 48	Loss: 0.021337	Acc: 53.7% (5366/10000)
[Test]  Epoch: 49	Loss: 0.021405	Acc: 53.5% (5351/10000)
[Test]  Epoch: 50	Loss: 0.021297	Acc: 53.6% (5363/10000)
[Test]  Epoch: 51	Loss: 0.021326	Acc: 53.4% (5336/10000)
[Test]  Epoch: 52	Loss: 0.021281	Acc: 53.8% (5378/10000)
[Test]  Epoch: 53	Loss: 0.021153	Acc: 53.6% (5362/10000)
[Test]  Epoch: 54	Loss: 0.021217	Acc: 53.4% (5344/10000)
[Test]  Epoch: 55	Loss: 0.021150	Acc: 53.6% (5359/10000)
[Test]  Epoch: 56	Loss: 0.021084	Acc: 54.1% (5406/10000)
[Test]  Epoch: 57	Loss: 0.021190	Acc: 53.6% (5360/10000)
[Test]  Epoch: 58	Loss: 0.021153	Acc: 53.9% (5388/10000)
[Test]  Epoch: 59	Loss: 0.021155	Acc: 53.8% (5379/10000)
[Test]  Epoch: 60	Loss: 0.021324	Acc: 53.3% (5330/10000)
[Test]  Epoch: 61	Loss: 0.021208	Acc: 53.7% (5371/10000)
[Test]  Epoch: 62	Loss: 0.021166	Acc: 53.8% (5380/10000)
[Test]  Epoch: 63	Loss: 0.021128	Acc: 53.9% (5385/10000)
[Test]  Epoch: 64	Loss: 0.021154	Acc: 53.7% (5366/10000)
[Test]  Epoch: 65	Loss: 0.021150	Acc: 53.8% (5381/10000)
[Test]  Epoch: 66	Loss: 0.021118	Acc: 53.7% (5371/10000)
[Test]  Epoch: 67	Loss: 0.021179	Acc: 53.7% (5374/10000)
[Test]  Epoch: 68	Loss: 0.021141	Acc: 53.8% (5377/10000)
[Test]  Epoch: 69	Loss: 0.021117	Acc: 53.8% (5378/10000)
[Test]  Epoch: 70	Loss: 0.021132	Acc: 53.8% (5382/10000)
[Test]  Epoch: 71	Loss: 0.021200	Acc: 53.6% (5357/10000)
[Test]  Epoch: 72	Loss: 0.021181	Acc: 53.8% (5375/10000)
[Test]  Epoch: 73	Loss: 0.021141	Acc: 53.9% (5391/10000)
[Test]  Epoch: 74	Loss: 0.021134	Acc: 53.8% (5375/10000)
[Test]  Epoch: 75	Loss: 0.021114	Acc: 53.8% (5380/10000)
[Test]  Epoch: 76	Loss: 0.021147	Acc: 53.7% (5374/10000)
[Test]  Epoch: 77	Loss: 0.021106	Acc: 53.9% (5394/10000)
[Test]  Epoch: 78	Loss: 0.021142	Acc: 53.9% (5387/10000)
[Test]  Epoch: 79	Loss: 0.021136	Acc: 53.9% (5385/10000)
[Test]  Epoch: 80	Loss: 0.021099	Acc: 53.7% (5371/10000)
[Test]  Epoch: 81	Loss: 0.021133	Acc: 53.9% (5391/10000)
[Test]  Epoch: 82	Loss: 0.021132	Acc: 53.7% (5368/10000)
[Test]  Epoch: 83	Loss: 0.021113	Acc: 53.8% (5384/10000)
[Test]  Epoch: 84	Loss: 0.021093	Acc: 54.0% (5402/10000)
[Test]  Epoch: 85	Loss: 0.021176	Acc: 53.8% (5378/10000)
[Test]  Epoch: 86	Loss: 0.021147	Acc: 53.9% (5387/10000)
[Test]  Epoch: 87	Loss: 0.021108	Acc: 53.9% (5391/10000)
[Test]  Epoch: 88	Loss: 0.021050	Acc: 53.9% (5392/10000)
[Test]  Epoch: 89	Loss: 0.021112	Acc: 54.0% (5396/10000)
[Test]  Epoch: 90	Loss: 0.021058	Acc: 54.0% (5401/10000)
[Test]  Epoch: 91	Loss: 0.021061	Acc: 54.0% (5399/10000)
[Test]  Epoch: 92	Loss: 0.021131	Acc: 53.9% (5393/10000)
[Test]  Epoch: 93	Loss: 0.021121	Acc: 54.1% (5406/10000)
[Test]  Epoch: 94	Loss: 0.021096	Acc: 53.9% (5388/10000)
[Test]  Epoch: 95	Loss: 0.021114	Acc: 53.8% (5377/10000)
[Test]  Epoch: 96	Loss: 0.021066	Acc: 53.7% (5369/10000)
[Test]  Epoch: 97	Loss: 0.021063	Acc: 53.9% (5388/10000)
[Test]  Epoch: 98	Loss: 0.021162	Acc: 53.6% (5364/10000)
[Test]  Epoch: 99	Loss: 0.021098	Acc: 53.9% (5393/10000)
[Test]  Epoch: 100	Loss: 0.021065	Acc: 53.9% (5388/10000)
===========finish==========
['2024-08-19', '05:26:34.761467', '100', 'test', '0.021064901161193847', '53.88', '54.06']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.076816	Acc: 21.4% (2139/10000)
[Test]  Epoch: 2	Loss: 0.025390	Acc: 44.8% (4482/10000)
[Test]  Epoch: 3	Loss: 0.023100	Acc: 48.9% (4891/10000)
[Test]  Epoch: 4	Loss: 0.022655	Acc: 49.8% (4981/10000)
[Test]  Epoch: 5	Loss: 0.022559	Acc: 50.0% (5001/10000)
[Test]  Epoch: 6	Loss: 0.022553	Acc: 51.1% (5108/10000)
[Test]  Epoch: 7	Loss: 0.022400	Acc: 51.2% (5123/10000)
[Test]  Epoch: 8	Loss: 0.022692	Acc: 50.7% (5074/10000)
[Test]  Epoch: 9	Loss: 0.022572	Acc: 51.6% (5163/10000)
[Test]  Epoch: 10	Loss: 0.022407	Acc: 52.1% (5207/10000)
[Test]  Epoch: 11	Loss: 0.022493	Acc: 52.1% (5214/10000)
[Test]  Epoch: 12	Loss: 0.022490	Acc: 51.9% (5187/10000)
[Test]  Epoch: 13	Loss: 0.022804	Acc: 51.3% (5130/10000)
[Test]  Epoch: 14	Loss: 0.022520	Acc: 52.0% (5198/10000)
[Test]  Epoch: 15	Loss: 0.022610	Acc: 51.9% (5185/10000)
[Test]  Epoch: 16	Loss: 0.022495	Acc: 51.4% (5141/10000)
[Test]  Epoch: 17	Loss: 0.022422	Acc: 51.7% (5172/10000)
[Test]  Epoch: 18	Loss: 0.022375	Acc: 52.1% (5209/10000)
[Test]  Epoch: 19	Loss: 0.022304	Acc: 52.1% (5213/10000)
[Test]  Epoch: 20	Loss: 0.022212	Acc: 52.3% (5226/10000)
[Test]  Epoch: 21	Loss: 0.022265	Acc: 52.3% (5229/10000)
[Test]  Epoch: 22	Loss: 0.022166	Acc: 52.5% (5253/10000)
[Test]  Epoch: 23	Loss: 0.022168	Acc: 52.2% (5224/10000)
[Test]  Epoch: 24	Loss: 0.022139	Acc: 52.3% (5230/10000)
[Test]  Epoch: 25	Loss: 0.022020	Acc: 52.5% (5255/10000)
[Test]  Epoch: 26	Loss: 0.021953	Acc: 52.4% (5243/10000)
[Test]  Epoch: 27	Loss: 0.022128	Acc: 52.4% (5238/10000)
[Test]  Epoch: 28	Loss: 0.021907	Acc: 52.8% (5279/10000)
[Test]  Epoch: 29	Loss: 0.021938	Acc: 52.6% (5262/10000)
[Test]  Epoch: 30	Loss: 0.021692	Acc: 53.0% (5297/10000)
[Test]  Epoch: 31	Loss: 0.021745	Acc: 52.8% (5278/10000)
[Test]  Epoch: 32	Loss: 0.021669	Acc: 52.8% (5282/10000)
[Test]  Epoch: 33	Loss: 0.021654	Acc: 53.0% (5303/10000)
[Test]  Epoch: 34	Loss: 0.021588	Acc: 53.3% (5327/10000)
[Test]  Epoch: 35	Loss: 0.021648	Acc: 53.2% (5319/10000)
[Test]  Epoch: 36	Loss: 0.021471	Acc: 53.7% (5366/10000)
[Test]  Epoch: 37	Loss: 0.021536	Acc: 53.4% (5342/10000)
[Test]  Epoch: 38	Loss: 0.021443	Acc: 53.6% (5358/10000)
[Test]  Epoch: 39	Loss: 0.021289	Acc: 54.0% (5397/10000)
[Test]  Epoch: 40	Loss: 0.021264	Acc: 53.8% (5383/10000)
[Test]  Epoch: 41	Loss: 0.021407	Acc: 53.5% (5353/10000)
[Test]  Epoch: 42	Loss: 0.021394	Acc: 54.0% (5403/10000)
[Test]  Epoch: 43	Loss: 0.021484	Acc: 53.5% (5349/10000)
[Test]  Epoch: 44	Loss: 0.021253	Acc: 54.0% (5403/10000)
[Test]  Epoch: 45	Loss: 0.021477	Acc: 53.5% (5348/10000)
[Test]  Epoch: 46	Loss: 0.021493	Acc: 53.4% (5338/10000)
[Test]  Epoch: 47	Loss: 0.021373	Acc: 53.6% (5356/10000)
[Test]  Epoch: 48	Loss: 0.021215	Acc: 54.1% (5415/10000)
[Test]  Epoch: 49	Loss: 0.021122	Acc: 54.1% (5410/10000)
[Test]  Epoch: 50	Loss: 0.021283	Acc: 53.8% (5376/10000)
[Test]  Epoch: 51	Loss: 0.021208	Acc: 54.1% (5406/10000)
[Test]  Epoch: 52	Loss: 0.021020	Acc: 53.8% (5379/10000)
[Test]  Epoch: 53	Loss: 0.021059	Acc: 54.3% (5428/10000)
[Test]  Epoch: 54	Loss: 0.021085	Acc: 54.0% (5399/10000)
[Test]  Epoch: 55	Loss: 0.021091	Acc: 54.1% (5413/10000)
[Test]  Epoch: 56	Loss: 0.021081	Acc: 54.2% (5416/10000)
[Test]  Epoch: 57	Loss: 0.021065	Acc: 54.1% (5407/10000)
[Test]  Epoch: 58	Loss: 0.020860	Acc: 54.8% (5483/10000)
[Test]  Epoch: 59	Loss: 0.020946	Acc: 54.4% (5435/10000)
[Test]  Epoch: 60	Loss: 0.021041	Acc: 54.2% (5418/10000)
[Test]  Epoch: 61	Loss: 0.021040	Acc: 54.1% (5410/10000)
[Test]  Epoch: 62	Loss: 0.020995	Acc: 54.0% (5403/10000)
[Test]  Epoch: 63	Loss: 0.020934	Acc: 54.1% (5414/10000)
[Test]  Epoch: 64	Loss: 0.020964	Acc: 54.4% (5441/10000)
[Test]  Epoch: 65	Loss: 0.020898	Acc: 54.3% (5427/10000)
[Test]  Epoch: 66	Loss: 0.020891	Acc: 54.2% (5423/10000)
[Test]  Epoch: 67	Loss: 0.020969	Acc: 54.0% (5402/10000)
[Test]  Epoch: 68	Loss: 0.020957	Acc: 54.1% (5412/10000)
[Test]  Epoch: 69	Loss: 0.020935	Acc: 54.2% (5425/10000)
[Test]  Epoch: 70	Loss: 0.020978	Acc: 54.3% (5431/10000)
[Test]  Epoch: 71	Loss: 0.020979	Acc: 54.1% (5413/10000)
[Test]  Epoch: 72	Loss: 0.020981	Acc: 54.1% (5409/10000)
[Test]  Epoch: 73	Loss: 0.020973	Acc: 54.0% (5402/10000)
[Test]  Epoch: 74	Loss: 0.020861	Acc: 54.3% (5431/10000)
[Test]  Epoch: 75	Loss: 0.020899	Acc: 54.3% (5429/10000)
[Test]  Epoch: 76	Loss: 0.020958	Acc: 54.3% (5429/10000)
[Test]  Epoch: 77	Loss: 0.020941	Acc: 54.2% (5423/10000)
[Test]  Epoch: 78	Loss: 0.020983	Acc: 54.3% (5427/10000)
[Test]  Epoch: 79	Loss: 0.020903	Acc: 54.3% (5427/10000)
[Test]  Epoch: 80	Loss: 0.020891	Acc: 54.2% (5417/10000)
[Test]  Epoch: 81	Loss: 0.020928	Acc: 54.2% (5424/10000)
[Test]  Epoch: 82	Loss: 0.020958	Acc: 54.1% (5414/10000)
[Test]  Epoch: 83	Loss: 0.020944	Acc: 54.1% (5411/10000)
[Test]  Epoch: 84	Loss: 0.020918	Acc: 54.1% (5415/10000)
[Test]  Epoch: 85	Loss: 0.020972	Acc: 54.1% (5409/10000)
[Test]  Epoch: 86	Loss: 0.020925	Acc: 54.2% (5423/10000)
[Test]  Epoch: 87	Loss: 0.020930	Acc: 54.0% (5404/10000)
[Test]  Epoch: 88	Loss: 0.020853	Acc: 54.2% (5423/10000)
[Test]  Epoch: 89	Loss: 0.020898	Acc: 54.4% (5441/10000)
[Test]  Epoch: 90	Loss: 0.020895	Acc: 54.4% (5439/10000)
[Test]  Epoch: 91	Loss: 0.020908	Acc: 54.3% (5434/10000)
[Test]  Epoch: 92	Loss: 0.020949	Acc: 54.1% (5407/10000)
[Test]  Epoch: 93	Loss: 0.020927	Acc: 54.3% (5430/10000)
[Test]  Epoch: 94	Loss: 0.020959	Acc: 54.1% (5406/10000)
[Test]  Epoch: 95	Loss: 0.020963	Acc: 54.3% (5432/10000)
[Test]  Epoch: 96	Loss: 0.020915	Acc: 54.2% (5420/10000)
[Test]  Epoch: 97	Loss: 0.020877	Acc: 54.2% (5423/10000)
[Test]  Epoch: 98	Loss: 0.020960	Acc: 54.2% (5420/10000)
[Test]  Epoch: 99	Loss: 0.020953	Acc: 54.3% (5426/10000)
[Test]  Epoch: 100	Loss: 0.020914	Acc: 54.3% (5431/10000)
===========finish==========
['2024-08-19', '05:29:15.007360', '100', 'test', '0.020913926178216934', '54.31', '54.83']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.049052	Acc: 26.9% (2686/10000)
[Test]  Epoch: 2	Loss: 0.025871	Acc: 44.6% (4457/10000)
[Test]  Epoch: 3	Loss: 0.024457	Acc: 46.6% (4656/10000)
[Test]  Epoch: 4	Loss: 0.024322	Acc: 47.7% (4771/10000)
[Test]  Epoch: 5	Loss: 0.024335	Acc: 48.0% (4802/10000)
[Test]  Epoch: 6	Loss: 0.024507	Acc: 48.4% (4838/10000)
[Test]  Epoch: 7	Loss: 0.024674	Acc: 48.0% (4803/10000)
[Test]  Epoch: 8	Loss: 0.024339	Acc: 48.7% (4866/10000)
[Test]  Epoch: 9	Loss: 0.024439	Acc: 48.5% (4852/10000)
[Test]  Epoch: 10	Loss: 0.024353	Acc: 49.0% (4899/10000)
[Test]  Epoch: 11	Loss: 0.024442	Acc: 49.0% (4902/10000)
[Test]  Epoch: 12	Loss: 0.024297	Acc: 49.0% (4904/10000)
[Test]  Epoch: 13	Loss: 0.024159	Acc: 49.5% (4947/10000)
[Test]  Epoch: 14	Loss: 0.024063	Acc: 49.5% (4948/10000)
[Test]  Epoch: 15	Loss: 0.023951	Acc: 50.1% (5006/10000)
[Test]  Epoch: 16	Loss: 0.024021	Acc: 49.7% (4969/10000)
[Test]  Epoch: 17	Loss: 0.023971	Acc: 49.8% (4984/10000)
[Test]  Epoch: 18	Loss: 0.023897	Acc: 50.1% (5006/10000)
[Test]  Epoch: 19	Loss: 0.023667	Acc: 50.2% (5025/10000)
[Test]  Epoch: 20	Loss: 0.023892	Acc: 50.5% (5049/10000)
[Test]  Epoch: 21	Loss: 0.023798	Acc: 50.3% (5026/10000)
[Test]  Epoch: 22	Loss: 0.023558	Acc: 50.5% (5045/10000)
[Test]  Epoch: 23	Loss: 0.023519	Acc: 50.6% (5057/10000)
[Test]  Epoch: 24	Loss: 0.023396	Acc: 50.3% (5028/10000)
[Test]  Epoch: 25	Loss: 0.023505	Acc: 50.5% (5046/10000)
[Test]  Epoch: 26	Loss: 0.023347	Acc: 50.6% (5060/10000)
[Test]  Epoch: 27	Loss: 0.023386	Acc: 50.3% (5031/10000)
[Test]  Epoch: 28	Loss: 0.023142	Acc: 50.8% (5076/10000)
[Test]  Epoch: 29	Loss: 0.023072	Acc: 50.9% (5094/10000)
[Test]  Epoch: 30	Loss: 0.023174	Acc: 50.9% (5087/10000)
[Test]  Epoch: 31	Loss: 0.023094	Acc: 51.1% (5115/10000)
[Test]  Epoch: 32	Loss: 0.023012	Acc: 50.9% (5091/10000)
[Test]  Epoch: 33	Loss: 0.023054	Acc: 51.2% (5124/10000)
[Test]  Epoch: 34	Loss: 0.023022	Acc: 51.2% (5123/10000)
[Test]  Epoch: 35	Loss: 0.023038	Acc: 50.9% (5092/10000)
[Test]  Epoch: 36	Loss: 0.022934	Acc: 51.2% (5116/10000)
[Test]  Epoch: 37	Loss: 0.022812	Acc: 51.7% (5171/10000)
[Test]  Epoch: 38	Loss: 0.022992	Acc: 51.0% (5098/10000)
[Test]  Epoch: 39	Loss: 0.022686	Acc: 51.6% (5164/10000)
[Test]  Epoch: 40	Loss: 0.022716	Acc: 51.5% (5153/10000)
[Test]  Epoch: 41	Loss: 0.022524	Acc: 51.7% (5172/10000)
[Test]  Epoch: 42	Loss: 0.022545	Acc: 51.7% (5174/10000)
[Test]  Epoch: 43	Loss: 0.022571	Acc: 51.4% (5136/10000)
[Test]  Epoch: 44	Loss: 0.022648	Acc: 52.0% (5197/10000)
[Test]  Epoch: 45	Loss: 0.022564	Acc: 51.8% (5183/10000)
[Test]  Epoch: 46	Loss: 0.022559	Acc: 51.6% (5165/10000)
[Test]  Epoch: 47	Loss: 0.022561	Acc: 51.7% (5172/10000)
[Test]  Epoch: 48	Loss: 0.022589	Acc: 51.7% (5173/10000)
[Test]  Epoch: 49	Loss: 0.022422	Acc: 51.7% (5174/10000)
[Test]  Epoch: 50	Loss: 0.022422	Acc: 52.2% (5217/10000)
[Test]  Epoch: 51	Loss: 0.022434	Acc: 52.3% (5226/10000)
[Test]  Epoch: 52	Loss: 0.022435	Acc: 51.8% (5182/10000)
[Test]  Epoch: 53	Loss: 0.022367	Acc: 52.1% (5208/10000)
[Test]  Epoch: 54	Loss: 0.022449	Acc: 51.9% (5190/10000)
[Test]  Epoch: 55	Loss: 0.022355	Acc: 51.9% (5191/10000)
[Test]  Epoch: 56	Loss: 0.022345	Acc: 52.0% (5197/10000)
[Test]  Epoch: 57	Loss: 0.022306	Acc: 52.1% (5210/10000)
[Test]  Epoch: 58	Loss: 0.022370	Acc: 52.3% (5228/10000)
[Test]  Epoch: 59	Loss: 0.022463	Acc: 51.9% (5185/10000)
[Test]  Epoch: 60	Loss: 0.022402	Acc: 51.9% (5192/10000)
[Test]  Epoch: 61	Loss: 0.022306	Acc: 52.1% (5207/10000)
[Test]  Epoch: 62	Loss: 0.022308	Acc: 52.3% (5226/10000)
[Test]  Epoch: 63	Loss: 0.022185	Acc: 52.4% (5236/10000)
[Test]  Epoch: 64	Loss: 0.022276	Acc: 52.2% (5223/10000)
[Test]  Epoch: 65	Loss: 0.022226	Acc: 52.4% (5241/10000)
[Test]  Epoch: 66	Loss: 0.022250	Acc: 52.4% (5237/10000)
[Test]  Epoch: 67	Loss: 0.022263	Acc: 52.1% (5211/10000)
[Test]  Epoch: 68	Loss: 0.022243	Acc: 52.5% (5251/10000)
[Test]  Epoch: 69	Loss: 0.022271	Acc: 52.3% (5228/10000)
[Test]  Epoch: 70	Loss: 0.022241	Acc: 52.3% (5232/10000)
[Test]  Epoch: 71	Loss: 0.022230	Acc: 52.4% (5236/10000)
[Test]  Epoch: 72	Loss: 0.022244	Acc: 52.3% (5227/10000)
[Test]  Epoch: 73	Loss: 0.022226	Acc: 52.4% (5244/10000)
[Test]  Epoch: 74	Loss: 0.022187	Acc: 52.4% (5241/10000)
[Test]  Epoch: 75	Loss: 0.022144	Acc: 52.5% (5255/10000)
[Test]  Epoch: 76	Loss: 0.022201	Acc: 52.4% (5241/10000)
[Test]  Epoch: 77	Loss: 0.022208	Acc: 52.4% (5241/10000)
[Test]  Epoch: 78	Loss: 0.022218	Acc: 52.6% (5257/10000)
[Test]  Epoch: 79	Loss: 0.022202	Acc: 52.7% (5266/10000)
[Test]  Epoch: 80	Loss: 0.022198	Acc: 52.2% (5218/10000)
[Test]  Epoch: 81	Loss: 0.022310	Acc: 52.2% (5224/10000)
[Test]  Epoch: 82	Loss: 0.022244	Acc: 52.3% (5226/10000)
[Test]  Epoch: 83	Loss: 0.022223	Acc: 52.3% (5233/10000)
[Test]  Epoch: 84	Loss: 0.022219	Acc: 52.5% (5254/10000)
[Test]  Epoch: 85	Loss: 0.022243	Acc: 52.1% (5212/10000)
[Test]  Epoch: 86	Loss: 0.022218	Acc: 52.6% (5259/10000)
[Test]  Epoch: 87	Loss: 0.022205	Acc: 52.2% (5225/10000)
[Test]  Epoch: 88	Loss: 0.022116	Acc: 52.5% (5250/10000)
[Test]  Epoch: 89	Loss: 0.022204	Acc: 52.4% (5240/10000)
[Test]  Epoch: 90	Loss: 0.022191	Acc: 52.4% (5244/10000)
[Test]  Epoch: 91	Loss: 0.022128	Acc: 52.5% (5247/10000)
[Test]  Epoch: 92	Loss: 0.022203	Acc: 52.4% (5242/10000)
[Test]  Epoch: 93	Loss: 0.022161	Acc: 52.2% (5225/10000)
[Test]  Epoch: 94	Loss: 0.022159	Acc: 52.4% (5244/10000)
[Test]  Epoch: 95	Loss: 0.022127	Acc: 52.4% (5237/10000)
[Test]  Epoch: 96	Loss: 0.022203	Acc: 52.5% (5245/10000)
[Test]  Epoch: 97	Loss: 0.022176	Acc: 52.4% (5238/10000)
[Test]  Epoch: 98	Loss: 0.022256	Acc: 52.1% (5213/10000)
[Test]  Epoch: 99	Loss: 0.022187	Acc: 52.6% (5258/10000)
[Test]  Epoch: 100	Loss: 0.022156	Acc: 52.4% (5240/10000)
===========finish==========
['2024-08-19', '05:31:56.330180', '100', 'test', '0.02215589234828949', '52.4', '52.66']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.083208	Acc: 22.1% (2210/10000)
[Test]  Epoch: 2	Loss: 0.026813	Acc: 43.6% (4363/10000)
[Test]  Epoch: 3	Loss: 0.024459	Acc: 47.1% (4706/10000)
[Test]  Epoch: 4	Loss: 0.024386	Acc: 47.3% (4734/10000)
[Test]  Epoch: 5	Loss: 0.024367	Acc: 47.3% (4733/10000)
[Test]  Epoch: 6	Loss: 0.024353	Acc: 47.8% (4776/10000)
[Test]  Epoch: 7	Loss: 0.024051	Acc: 48.2% (4824/10000)
[Test]  Epoch: 8	Loss: 0.024156	Acc: 48.4% (4838/10000)
[Test]  Epoch: 9	Loss: 0.023877	Acc: 49.1% (4911/10000)
[Test]  Epoch: 10	Loss: 0.024146	Acc: 49.0% (4904/10000)
[Test]  Epoch: 11	Loss: 0.024035	Acc: 48.6% (4861/10000)
[Test]  Epoch: 12	Loss: 0.024490	Acc: 49.4% (4938/10000)
[Test]  Epoch: 13	Loss: 0.023822	Acc: 49.4% (4943/10000)
[Test]  Epoch: 14	Loss: 0.023538	Acc: 50.0% (5002/10000)
[Test]  Epoch: 15	Loss: 0.023691	Acc: 49.6% (4956/10000)
[Test]  Epoch: 16	Loss: 0.023511	Acc: 49.8% (4983/10000)
[Test]  Epoch: 17	Loss: 0.023710	Acc: 50.1% (5007/10000)
[Test]  Epoch: 18	Loss: 0.023624	Acc: 49.9% (4988/10000)
[Test]  Epoch: 19	Loss: 0.023479	Acc: 50.1% (5009/10000)
[Test]  Epoch: 20	Loss: 0.023394	Acc: 50.5% (5045/10000)
[Test]  Epoch: 21	Loss: 0.023318	Acc: 50.5% (5049/10000)
[Test]  Epoch: 22	Loss: 0.023339	Acc: 50.8% (5080/10000)
[Test]  Epoch: 23	Loss: 0.023274	Acc: 50.1% (5013/10000)
[Test]  Epoch: 24	Loss: 0.023188	Acc: 50.6% (5059/10000)
[Test]  Epoch: 25	Loss: 0.023225	Acc: 50.7% (5073/10000)
[Test]  Epoch: 26	Loss: 0.023198	Acc: 50.7% (5067/10000)
[Test]  Epoch: 27	Loss: 0.023072	Acc: 50.5% (5046/10000)
[Test]  Epoch: 28	Loss: 0.022978	Acc: 50.7% (5066/10000)
[Test]  Epoch: 29	Loss: 0.023227	Acc: 50.6% (5058/10000)
[Test]  Epoch: 30	Loss: 0.023007	Acc: 51.1% (5107/10000)
[Test]  Epoch: 31	Loss: 0.023000	Acc: 50.7% (5070/10000)
[Test]  Epoch: 32	Loss: 0.022926	Acc: 50.4% (5037/10000)
[Test]  Epoch: 33	Loss: 0.022906	Acc: 51.1% (5106/10000)
[Test]  Epoch: 34	Loss: 0.022847	Acc: 50.8% (5081/10000)
[Test]  Epoch: 35	Loss: 0.022846	Acc: 51.0% (5096/10000)
[Test]  Epoch: 36	Loss: 0.022963	Acc: 50.8% (5083/10000)
[Test]  Epoch: 37	Loss: 0.022779	Acc: 50.9% (5085/10000)
[Test]  Epoch: 38	Loss: 0.022900	Acc: 50.9% (5085/10000)
[Test]  Epoch: 39	Loss: 0.022876	Acc: 51.2% (5117/10000)
[Test]  Epoch: 40	Loss: 0.022769	Acc: 51.2% (5120/10000)
[Test]  Epoch: 41	Loss: 0.022838	Acc: 51.1% (5115/10000)
[Test]  Epoch: 42	Loss: 0.022688	Acc: 51.7% (5168/10000)
[Test]  Epoch: 43	Loss: 0.022698	Acc: 50.9% (5094/10000)
[Test]  Epoch: 44	Loss: 0.022682	Acc: 51.2% (5123/10000)
[Test]  Epoch: 45	Loss: 0.022697	Acc: 51.0% (5105/10000)
[Test]  Epoch: 46	Loss: 0.022714	Acc: 51.4% (5135/10000)
[Test]  Epoch: 47	Loss: 0.022682	Acc: 51.3% (5133/10000)
[Test]  Epoch: 48	Loss: 0.022709	Acc: 51.5% (5145/10000)
[Test]  Epoch: 49	Loss: 0.022579	Acc: 51.6% (5156/10000)
[Test]  Epoch: 50	Loss: 0.022537	Acc: 52.0% (5203/10000)
[Test]  Epoch: 51	Loss: 0.022484	Acc: 51.6% (5159/10000)
[Test]  Epoch: 52	Loss: 0.022411	Acc: 51.9% (5188/10000)
[Test]  Epoch: 53	Loss: 0.022545	Acc: 51.4% (5141/10000)
[Test]  Epoch: 54	Loss: 0.022383	Acc: 51.7% (5173/10000)
[Test]  Epoch: 55	Loss: 0.022478	Acc: 51.7% (5169/10000)
[Test]  Epoch: 56	Loss: 0.022538	Acc: 51.7% (5166/10000)
[Test]  Epoch: 57	Loss: 0.022371	Acc: 51.7% (5170/10000)
[Test]  Epoch: 58	Loss: 0.022300	Acc: 52.1% (5210/10000)
[Test]  Epoch: 59	Loss: 0.022341	Acc: 52.0% (5199/10000)
[Test]  Epoch: 60	Loss: 0.022354	Acc: 52.0% (5197/10000)
[Test]  Epoch: 61	Loss: 0.022374	Acc: 51.9% (5188/10000)
[Test]  Epoch: 62	Loss: 0.022313	Acc: 51.8% (5184/10000)
[Test]  Epoch: 63	Loss: 0.022202	Acc: 52.1% (5213/10000)
[Test]  Epoch: 64	Loss: 0.022269	Acc: 52.2% (5217/10000)
[Test]  Epoch: 65	Loss: 0.022238	Acc: 52.1% (5207/10000)
[Test]  Epoch: 66	Loss: 0.022261	Acc: 52.1% (5211/10000)
[Test]  Epoch: 67	Loss: 0.022259	Acc: 52.0% (5204/10000)
[Test]  Epoch: 68	Loss: 0.022222	Acc: 52.1% (5215/10000)
[Test]  Epoch: 69	Loss: 0.022208	Acc: 52.4% (5237/10000)
[Test]  Epoch: 70	Loss: 0.022222	Acc: 52.1% (5214/10000)
[Test]  Epoch: 71	Loss: 0.022297	Acc: 52.0% (5205/10000)
[Test]  Epoch: 72	Loss: 0.022310	Acc: 52.0% (5203/10000)
[Test]  Epoch: 73	Loss: 0.022276	Acc: 52.1% (5213/10000)
[Test]  Epoch: 74	Loss: 0.022234	Acc: 52.2% (5218/10000)
[Test]  Epoch: 75	Loss: 0.022208	Acc: 52.2% (5219/10000)
[Test]  Epoch: 76	Loss: 0.022221	Acc: 52.2% (5223/10000)
[Test]  Epoch: 77	Loss: 0.022274	Acc: 52.3% (5230/10000)
[Test]  Epoch: 78	Loss: 0.022324	Acc: 52.2% (5220/10000)
[Test]  Epoch: 79	Loss: 0.022282	Acc: 51.8% (5177/10000)
[Test]  Epoch: 80	Loss: 0.022239	Acc: 52.0% (5205/10000)
[Test]  Epoch: 81	Loss: 0.022304	Acc: 52.3% (5227/10000)
[Test]  Epoch: 82	Loss: 0.022317	Acc: 51.9% (5192/10000)
[Test]  Epoch: 83	Loss: 0.022231	Acc: 52.2% (5221/10000)
[Test]  Epoch: 84	Loss: 0.022223	Acc: 52.1% (5213/10000)
[Test]  Epoch: 85	Loss: 0.022282	Acc: 52.0% (5197/10000)
[Test]  Epoch: 86	Loss: 0.022206	Acc: 52.2% (5218/10000)
[Test]  Epoch: 87	Loss: 0.022222	Acc: 52.2% (5223/10000)
[Test]  Epoch: 88	Loss: 0.022209	Acc: 52.4% (5242/10000)
[Test]  Epoch: 89	Loss: 0.022242	Acc: 52.2% (5222/10000)
[Test]  Epoch: 90	Loss: 0.022211	Acc: 52.2% (5221/10000)
[Test]  Epoch: 91	Loss: 0.022233	Acc: 52.2% (5222/10000)
[Test]  Epoch: 92	Loss: 0.022240	Acc: 52.3% (5229/10000)
[Test]  Epoch: 93	Loss: 0.022206	Acc: 52.2% (5221/10000)
[Test]  Epoch: 94	Loss: 0.022213	Acc: 52.0% (5204/10000)
[Test]  Epoch: 95	Loss: 0.022213	Acc: 52.1% (5213/10000)
[Test]  Epoch: 96	Loss: 0.022220	Acc: 52.1% (5215/10000)
[Test]  Epoch: 97	Loss: 0.022188	Acc: 52.0% (5203/10000)
[Test]  Epoch: 98	Loss: 0.022264	Acc: 52.1% (5212/10000)
[Test]  Epoch: 99	Loss: 0.022263	Acc: 52.2% (5217/10000)
[Test]  Epoch: 100	Loss: 0.022212	Acc: 52.0% (5199/10000)
===========finish==========
['2024-08-19', '05:34:36.163929', '100', 'test', '0.02221173872947693', '51.99', '52.42']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.248258	Acc: 14.6% (1459/10000)
[Test]  Epoch: 2	Loss: 0.029551	Acc: 38.5% (3854/10000)
[Test]  Epoch: 3	Loss: 0.025652	Acc: 43.2% (4319/10000)
[Test]  Epoch: 4	Loss: 0.025002	Acc: 44.1% (4415/10000)
[Test]  Epoch: 5	Loss: 0.024958	Acc: 45.5% (4547/10000)
[Test]  Epoch: 6	Loss: 0.025370	Acc: 46.1% (4607/10000)
[Test]  Epoch: 7	Loss: 0.025456	Acc: 45.6% (4559/10000)
[Test]  Epoch: 8	Loss: 0.025279	Acc: 46.1% (4609/10000)
[Test]  Epoch: 9	Loss: 0.025351	Acc: 45.3% (4531/10000)
[Test]  Epoch: 10	Loss: 0.025750	Acc: 45.8% (4580/10000)
[Test]  Epoch: 11	Loss: 0.025477	Acc: 46.2% (4621/10000)
[Test]  Epoch: 12	Loss: 0.025389	Acc: 46.2% (4623/10000)
[Test]  Epoch: 13	Loss: 0.025230	Acc: 46.6% (4663/10000)
[Test]  Epoch: 14	Loss: 0.025151	Acc: 46.9% (4690/10000)
[Test]  Epoch: 15	Loss: 0.024965	Acc: 47.6% (4757/10000)
[Test]  Epoch: 16	Loss: 0.024874	Acc: 47.0% (4702/10000)
[Test]  Epoch: 17	Loss: 0.024791	Acc: 47.4% (4740/10000)
[Test]  Epoch: 18	Loss: 0.024808	Acc: 47.6% (4760/10000)
[Test]  Epoch: 19	Loss: 0.024848	Acc: 47.8% (4781/10000)
[Test]  Epoch: 20	Loss: 0.024426	Acc: 48.5% (4851/10000)
[Test]  Epoch: 21	Loss: 0.024561	Acc: 47.9% (4788/10000)
[Test]  Epoch: 22	Loss: 0.024371	Acc: 48.5% (4850/10000)
[Test]  Epoch: 23	Loss: 0.024441	Acc: 48.1% (4815/10000)
[Test]  Epoch: 24	Loss: 0.024394	Acc: 48.4% (4839/10000)
[Test]  Epoch: 25	Loss: 0.024438	Acc: 48.1% (4810/10000)
[Test]  Epoch: 26	Loss: 0.024211	Acc: 48.4% (4837/10000)
[Test]  Epoch: 27	Loss: 0.024279	Acc: 48.7% (4868/10000)
[Test]  Epoch: 28	Loss: 0.024166	Acc: 48.5% (4847/10000)
[Test]  Epoch: 29	Loss: 0.024034	Acc: 48.3% (4831/10000)
[Test]  Epoch: 30	Loss: 0.024036	Acc: 48.6% (4861/10000)
[Test]  Epoch: 31	Loss: 0.023908	Acc: 48.9% (4888/10000)
[Test]  Epoch: 32	Loss: 0.023831	Acc: 49.0% (4900/10000)
[Test]  Epoch: 33	Loss: 0.023851	Acc: 49.1% (4912/10000)
[Test]  Epoch: 34	Loss: 0.023854	Acc: 49.2% (4925/10000)
[Test]  Epoch: 35	Loss: 0.023767	Acc: 49.1% (4909/10000)
[Test]  Epoch: 36	Loss: 0.023682	Acc: 49.0% (4895/10000)
[Test]  Epoch: 37	Loss: 0.023879	Acc: 48.9% (4889/10000)
[Test]  Epoch: 38	Loss: 0.024064	Acc: 48.5% (4851/10000)
[Test]  Epoch: 39	Loss: 0.023784	Acc: 48.7% (4869/10000)
[Test]  Epoch: 40	Loss: 0.023738	Acc: 49.0% (4898/10000)
[Test]  Epoch: 41	Loss: 0.023562	Acc: 49.2% (4917/10000)
[Test]  Epoch: 42	Loss: 0.023494	Acc: 49.6% (4956/10000)
[Test]  Epoch: 43	Loss: 0.023639	Acc: 49.2% (4919/10000)
[Test]  Epoch: 44	Loss: 0.023549	Acc: 49.2% (4917/10000)
[Test]  Epoch: 45	Loss: 0.023597	Acc: 49.1% (4909/10000)
[Test]  Epoch: 46	Loss: 0.023669	Acc: 49.2% (4919/10000)
[Test]  Epoch: 47	Loss: 0.023508	Acc: 49.2% (4925/10000)
[Test]  Epoch: 48	Loss: 0.023666	Acc: 49.4% (4938/10000)
[Test]  Epoch: 49	Loss: 0.023567	Acc: 49.4% (4939/10000)
[Test]  Epoch: 50	Loss: 0.023422	Acc: 49.5% (4951/10000)
[Test]  Epoch: 51	Loss: 0.023488	Acc: 49.2% (4921/10000)
[Test]  Epoch: 52	Loss: 0.023368	Acc: 48.9% (4890/10000)
[Test]  Epoch: 53	Loss: 0.023431	Acc: 49.3% (4927/10000)
[Test]  Epoch: 54	Loss: 0.023286	Acc: 49.5% (4953/10000)
[Test]  Epoch: 55	Loss: 0.023313	Acc: 49.3% (4933/10000)
[Test]  Epoch: 56	Loss: 0.023408	Acc: 49.2% (4922/10000)
[Test]  Epoch: 57	Loss: 0.023323	Acc: 49.3% (4928/10000)
[Test]  Epoch: 58	Loss: 0.023225	Acc: 49.9% (4990/10000)
[Test]  Epoch: 59	Loss: 0.023230	Acc: 49.6% (4960/10000)
[Test]  Epoch: 60	Loss: 0.023222	Acc: 50.0% (4998/10000)
[Test]  Epoch: 61	Loss: 0.023186	Acc: 50.0% (5003/10000)
[Test]  Epoch: 62	Loss: 0.023126	Acc: 49.9% (4985/10000)
[Test]  Epoch: 63	Loss: 0.023132	Acc: 49.8% (4983/10000)
[Test]  Epoch: 64	Loss: 0.023157	Acc: 49.9% (4990/10000)
[Test]  Epoch: 65	Loss: 0.023129	Acc: 49.7% (4971/10000)
[Test]  Epoch: 66	Loss: 0.023085	Acc: 49.8% (4976/10000)
[Test]  Epoch: 67	Loss: 0.023128	Acc: 49.7% (4967/10000)
[Test]  Epoch: 68	Loss: 0.023077	Acc: 50.0% (5001/10000)
[Test]  Epoch: 69	Loss: 0.023144	Acc: 49.9% (4993/10000)
[Test]  Epoch: 70	Loss: 0.023063	Acc: 49.9% (4992/10000)
[Test]  Epoch: 71	Loss: 0.023054	Acc: 49.8% (4975/10000)
[Test]  Epoch: 72	Loss: 0.023185	Acc: 49.9% (4987/10000)
[Test]  Epoch: 73	Loss: 0.023099	Acc: 50.0% (5003/10000)
[Test]  Epoch: 74	Loss: 0.023028	Acc: 50.0% (4995/10000)
[Test]  Epoch: 75	Loss: 0.023033	Acc: 50.4% (5036/10000)
[Test]  Epoch: 76	Loss: 0.023122	Acc: 49.8% (4977/10000)
[Test]  Epoch: 77	Loss: 0.023041	Acc: 49.9% (4987/10000)
[Test]  Epoch: 78	Loss: 0.023090	Acc: 50.0% (5002/10000)
[Test]  Epoch: 79	Loss: 0.023095	Acc: 50.1% (5008/10000)
[Test]  Epoch: 80	Loss: 0.023108	Acc: 49.9% (4990/10000)
[Test]  Epoch: 81	Loss: 0.023117	Acc: 49.8% (4983/10000)
[Test]  Epoch: 82	Loss: 0.023127	Acc: 49.8% (4980/10000)
[Test]  Epoch: 83	Loss: 0.023117	Acc: 49.6% (4962/10000)
[Test]  Epoch: 84	Loss: 0.023050	Acc: 50.0% (4999/10000)
[Test]  Epoch: 85	Loss: 0.023117	Acc: 49.6% (4962/10000)
[Test]  Epoch: 86	Loss: 0.023047	Acc: 49.8% (4978/10000)
[Test]  Epoch: 87	Loss: 0.023083	Acc: 49.9% (4988/10000)
[Test]  Epoch: 88	Loss: 0.022997	Acc: 49.9% (4991/10000)
[Test]  Epoch: 89	Loss: 0.023143	Acc: 49.8% (4982/10000)
[Test]  Epoch: 90	Loss: 0.023080	Acc: 49.9% (4985/10000)
[Test]  Epoch: 91	Loss: 0.023044	Acc: 49.8% (4982/10000)
[Test]  Epoch: 92	Loss: 0.023113	Acc: 49.9% (4994/10000)
[Test]  Epoch: 93	Loss: 0.023073	Acc: 49.8% (4975/10000)
[Test]  Epoch: 94	Loss: 0.023105	Acc: 49.7% (4966/10000)
[Test]  Epoch: 95	Loss: 0.023141	Acc: 50.0% (4999/10000)
[Test]  Epoch: 96	Loss: 0.023072	Acc: 49.9% (4988/10000)
[Test]  Epoch: 97	Loss: 0.023077	Acc: 49.9% (4985/10000)
[Test]  Epoch: 98	Loss: 0.023221	Acc: 49.8% (4975/10000)
[Test]  Epoch: 99	Loss: 0.023116	Acc: 49.8% (4979/10000)
[Test]  Epoch: 100	Loss: 0.023087	Acc: 49.9% (4985/10000)
===========finish==========
['2024-08-19', '05:37:23.442136', '100', 'test', '0.023086848998069762', '49.85', '50.36']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.15.conv.2.weight', '_features.15.conv.3.weight', '_features.16.conv.0.0.weight', '_features.16.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.118698	Acc: 16.7% (1667/10000)
[Test]  Epoch: 2	Loss: 0.029211	Acc: 38.5% (3847/10000)
[Test]  Epoch: 3	Loss: 0.026169	Acc: 41.9% (4188/10000)
[Test]  Epoch: 4	Loss: 0.026721	Acc: 42.5% (4247/10000)
[Test]  Epoch: 5	Loss: 0.026486	Acc: 43.8% (4375/10000)
[Test]  Epoch: 6	Loss: 0.026666	Acc: 44.1% (4410/10000)
[Test]  Epoch: 7	Loss: 0.026551	Acc: 43.6% (4360/10000)
[Test]  Epoch: 8	Loss: 0.026844	Acc: 44.2% (4417/10000)
[Test]  Epoch: 9	Loss: 0.026577	Acc: 45.0% (4498/10000)
[Test]  Epoch: 10	Loss: 0.026733	Acc: 44.2% (4417/10000)
[Test]  Epoch: 11	Loss: 0.026203	Acc: 45.6% (4564/10000)
[Test]  Epoch: 12	Loss: 0.026568	Acc: 45.8% (4579/10000)
[Test]  Epoch: 13	Loss: 0.026066	Acc: 45.8% (4577/10000)
[Test]  Epoch: 14	Loss: 0.025775	Acc: 46.7% (4668/10000)
[Test]  Epoch: 15	Loss: 0.025883	Acc: 45.9% (4593/10000)
[Test]  Epoch: 16	Loss: 0.025768	Acc: 46.3% (4633/10000)
[Test]  Epoch: 17	Loss: 0.025544	Acc: 46.9% (4689/10000)
[Test]  Epoch: 18	Loss: 0.025601	Acc: 46.6% (4662/10000)
[Test]  Epoch: 19	Loss: 0.026067	Acc: 46.0% (4598/10000)
[Test]  Epoch: 20	Loss: 0.025459	Acc: 47.2% (4725/10000)
[Test]  Epoch: 21	Loss: 0.025566	Acc: 47.0% (4702/10000)
[Test]  Epoch: 22	Loss: 0.025513	Acc: 46.9% (4694/10000)
[Test]  Epoch: 23	Loss: 0.025394	Acc: 47.0% (4703/10000)
[Test]  Epoch: 24	Loss: 0.025220	Acc: 47.2% (4723/10000)
[Test]  Epoch: 25	Loss: 0.025137	Acc: 47.2% (4721/10000)
[Test]  Epoch: 26	Loss: 0.024942	Acc: 47.6% (4761/10000)
[Test]  Epoch: 27	Loss: 0.025199	Acc: 47.1% (4714/10000)
[Test]  Epoch: 28	Loss: 0.024802	Acc: 47.9% (4790/10000)
[Test]  Epoch: 29	Loss: 0.024739	Acc: 47.9% (4791/10000)
[Test]  Epoch: 30	Loss: 0.024891	Acc: 48.0% (4795/10000)
[Test]  Epoch: 31	Loss: 0.024839	Acc: 47.7% (4767/10000)
[Test]  Epoch: 32	Loss: 0.024610	Acc: 48.2% (4825/10000)
[Test]  Epoch: 33	Loss: 0.024833	Acc: 48.0% (4795/10000)
[Test]  Epoch: 34	Loss: 0.024681	Acc: 48.2% (4819/10000)
[Test]  Epoch: 35	Loss: 0.024419	Acc: 48.2% (4819/10000)
[Test]  Epoch: 36	Loss: 0.024676	Acc: 48.2% (4821/10000)
[Test]  Epoch: 37	Loss: 0.024425	Acc: 48.5% (4854/10000)
[Test]  Epoch: 38	Loss: 0.024619	Acc: 47.6% (4764/10000)
[Test]  Epoch: 39	Loss: 0.024502	Acc: 47.8% (4780/10000)
[Test]  Epoch: 40	Loss: 0.024316	Acc: 47.9% (4788/10000)
[Test]  Epoch: 41	Loss: 0.024385	Acc: 48.1% (4815/10000)
[Test]  Epoch: 42	Loss: 0.024461	Acc: 48.1% (4813/10000)
[Test]  Epoch: 43	Loss: 0.024304	Acc: 48.3% (4826/10000)
[Test]  Epoch: 44	Loss: 0.024249	Acc: 48.4% (4840/10000)
[Test]  Epoch: 45	Loss: 0.024336	Acc: 48.2% (4817/10000)
[Test]  Epoch: 46	Loss: 0.024235	Acc: 48.0% (4805/10000)
[Test]  Epoch: 47	Loss: 0.024192	Acc: 48.7% (4867/10000)
[Test]  Epoch: 48	Loss: 0.024130	Acc: 48.8% (4878/10000)
[Test]  Epoch: 49	Loss: 0.024158	Acc: 48.7% (4874/10000)
[Test]  Epoch: 50	Loss: 0.024150	Acc: 48.8% (4876/10000)
[Test]  Epoch: 51	Loss: 0.024303	Acc: 48.6% (4858/10000)
[Test]  Epoch: 52	Loss: 0.024148	Acc: 48.4% (4843/10000)
[Test]  Epoch: 53	Loss: 0.024123	Acc: 48.3% (4832/10000)
[Test]  Epoch: 54	Loss: 0.024069	Acc: 48.6% (4858/10000)
[Test]  Epoch: 55	Loss: 0.024017	Acc: 48.7% (4869/10000)
[Test]  Epoch: 56	Loss: 0.024147	Acc: 48.5% (4849/10000)
[Test]  Epoch: 57	Loss: 0.023967	Acc: 48.9% (4888/10000)
[Test]  Epoch: 58	Loss: 0.024018	Acc: 48.6% (4865/10000)
[Test]  Epoch: 59	Loss: 0.023975	Acc: 48.4% (4840/10000)
[Test]  Epoch: 60	Loss: 0.024068	Acc: 48.4% (4837/10000)
[Test]  Epoch: 61	Loss: 0.024021	Acc: 48.5% (4848/10000)
[Test]  Epoch: 62	Loss: 0.024041	Acc: 48.5% (4853/10000)
[Test]  Epoch: 63	Loss: 0.023925	Acc: 48.7% (4872/10000)
[Test]  Epoch: 64	Loss: 0.023947	Acc: 48.9% (4892/10000)
[Test]  Epoch: 65	Loss: 0.023897	Acc: 48.8% (4878/10000)
[Test]  Epoch: 66	Loss: 0.023854	Acc: 48.5% (4849/10000)
[Test]  Epoch: 67	Loss: 0.023990	Acc: 48.7% (4868/10000)
[Test]  Epoch: 68	Loss: 0.023849	Acc: 48.9% (4890/10000)
[Test]  Epoch: 69	Loss: 0.023824	Acc: 48.8% (4877/10000)
[Test]  Epoch: 70	Loss: 0.023815	Acc: 48.6% (4858/10000)
[Test]  Epoch: 71	Loss: 0.023919	Acc: 48.7% (4870/10000)
[Test]  Epoch: 72	Loss: 0.023984	Acc: 48.8% (4878/10000)
[Test]  Epoch: 73	Loss: 0.023915	Acc: 48.8% (4881/10000)
[Test]  Epoch: 74	Loss: 0.023865	Acc: 48.7% (4872/10000)
[Test]  Epoch: 75	Loss: 0.023830	Acc: 48.9% (4890/10000)
[Test]  Epoch: 76	Loss: 0.023922	Acc: 48.6% (4863/10000)
[Test]  Epoch: 77	Loss: 0.023893	Acc: 48.7% (4868/10000)
[Test]  Epoch: 78	Loss: 0.023884	Acc: 48.8% (4878/10000)
[Test]  Epoch: 79	Loss: 0.023826	Acc: 48.9% (4888/10000)
[Test]  Epoch: 80	Loss: 0.023876	Acc: 48.7% (4874/10000)
[Test]  Epoch: 81	Loss: 0.023920	Acc: 48.6% (4862/10000)
[Test]  Epoch: 82	Loss: 0.023906	Acc: 48.9% (4889/10000)
[Test]  Epoch: 83	Loss: 0.023953	Acc: 49.0% (4895/10000)
[Test]  Epoch: 84	Loss: 0.023953	Acc: 48.7% (4873/10000)
[Test]  Epoch: 85	Loss: 0.023925	Acc: 48.9% (4885/10000)
[Test]  Epoch: 86	Loss: 0.023835	Acc: 48.9% (4894/10000)
[Test]  Epoch: 87	Loss: 0.023864	Acc: 48.8% (4877/10000)
[Test]  Epoch: 88	Loss: 0.023770	Acc: 49.0% (4897/10000)
[Test]  Epoch: 89	Loss: 0.023901	Acc: 48.7% (4867/10000)
[Test]  Epoch: 90	Loss: 0.023773	Acc: 48.5% (4853/10000)
[Test]  Epoch: 91	Loss: 0.023819	Acc: 48.8% (4883/10000)
[Test]  Epoch: 92	Loss: 0.023840	Acc: 48.6% (4863/10000)
[Test]  Epoch: 93	Loss: 0.023815	Acc: 48.9% (4886/10000)
[Test]  Epoch: 94	Loss: 0.023906	Acc: 48.8% (4881/10000)
[Test]  Epoch: 95	Loss: 0.023823	Acc: 48.8% (4879/10000)
[Test]  Epoch: 96	Loss: 0.023884	Acc: 48.9% (4885/10000)
[Test]  Epoch: 97	Loss: 0.023878	Acc: 48.7% (4869/10000)
[Test]  Epoch: 98	Loss: 0.023904	Acc: 48.8% (4881/10000)
[Test]  Epoch: 99	Loss: 0.023790	Acc: 48.7% (4866/10000)
[Test]  Epoch: 100	Loss: 0.023820	Acc: 48.6% (4858/10000)
===========finish==========
['2024-08-19', '05:40:06.208573', '100', 'test', '0.02381975041627884', '48.58', '48.97']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.15.conv.2.weight', '_features.15.conv.3.weight', '_features.16.conv.0.0.weight', '_features.16.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.2.weight', '_features.16.conv.3.weight', '_features.17.conv.0.0.weight', '_features.17.conv.0.1.weight', '_features.17.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.17.conv.2.weight', '_features.17.conv.3.weight', '_features.18.0.weight', '_features.18.1.weight', 'last_linear.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.057568	Acc: 14.5% (1454/10000)
[Test]  Epoch: 2	Loss: 0.031288	Acc: 32.3% (3227/10000)
[Test]  Epoch: 3	Loss: 0.026944	Acc: 39.6% (3959/10000)
[Test]  Epoch: 4	Loss: 0.026842	Acc: 41.0% (4102/10000)
[Test]  Epoch: 5	Loss: 0.024759	Acc: 45.8% (4583/10000)
[Test]  Epoch: 6	Loss: 0.025611	Acc: 44.8% (4479/10000)
[Test]  Epoch: 7	Loss: 0.025693	Acc: 44.7% (4468/10000)
[Test]  Epoch: 8	Loss: 0.024610	Acc: 47.3% (4728/10000)
[Test]  Epoch: 9	Loss: 0.025536	Acc: 47.2% (4716/10000)
[Test]  Epoch: 10	Loss: 0.025144	Acc: 47.6% (4761/10000)
[Test]  Epoch: 11	Loss: 0.024752	Acc: 47.9% (4792/10000)
[Test]  Epoch: 12	Loss: 0.024358	Acc: 49.2% (4919/10000)
[Test]  Epoch: 13	Loss: 0.023921	Acc: 49.8% (4976/10000)
[Test]  Epoch: 14	Loss: 0.024001	Acc: 49.2% (4923/10000)
[Test]  Epoch: 15	Loss: 0.024227	Acc: 49.6% (4963/10000)
[Test]  Epoch: 16	Loss: 0.023983	Acc: 49.4% (4937/10000)
[Test]  Epoch: 17	Loss: 0.024166	Acc: 49.5% (4954/10000)
[Test]  Epoch: 18	Loss: 0.023997	Acc: 49.6% (4959/10000)
[Test]  Epoch: 19	Loss: 0.023870	Acc: 50.2% (5016/10000)
[Test]  Epoch: 20	Loss: 0.023476	Acc: 50.8% (5082/10000)
[Test]  Epoch: 21	Loss: 0.023574	Acc: 50.7% (5067/10000)
[Test]  Epoch: 22	Loss: 0.023164	Acc: 51.3% (5130/10000)
[Test]  Epoch: 23	Loss: 0.023117	Acc: 51.2% (5125/10000)
[Test]  Epoch: 24	Loss: 0.023220	Acc: 50.6% (5063/10000)
[Test]  Epoch: 25	Loss: 0.023122	Acc: 51.0% (5098/10000)
[Test]  Epoch: 26	Loss: 0.022914	Acc: 51.7% (5173/10000)
[Test]  Epoch: 27	Loss: 0.022869	Acc: 51.3% (5126/10000)
[Test]  Epoch: 28	Loss: 0.022855	Acc: 51.5% (5149/10000)
[Test]  Epoch: 29	Loss: 0.022862	Acc: 51.5% (5154/10000)
[Test]  Epoch: 30	Loss: 0.022785	Acc: 51.7% (5174/10000)
[Test]  Epoch: 31	Loss: 0.022717	Acc: 51.4% (5144/10000)
[Test]  Epoch: 32	Loss: 0.022699	Acc: 52.0% (5204/10000)
[Test]  Epoch: 33	Loss: 0.022484	Acc: 51.7% (5173/10000)
[Test]  Epoch: 34	Loss: 0.022661	Acc: 51.0% (5098/10000)
[Test]  Epoch: 35	Loss: 0.022442	Acc: 51.9% (5188/10000)
[Test]  Epoch: 36	Loss: 0.022485	Acc: 51.6% (5163/10000)
[Test]  Epoch: 37	Loss: 0.022523	Acc: 51.8% (5176/10000)
[Test]  Epoch: 38	Loss: 0.023041	Acc: 51.0% (5103/10000)
[Test]  Epoch: 39	Loss: 0.023108	Acc: 51.5% (5151/10000)
[Test]  Epoch: 40	Loss: 0.022493	Acc: 51.9% (5191/10000)
[Test]  Epoch: 41	Loss: 0.022586	Acc: 52.2% (5221/10000)
[Test]  Epoch: 42	Loss: 0.022534	Acc: 51.7% (5169/10000)
[Test]  Epoch: 43	Loss: 0.022563	Acc: 51.5% (5155/10000)
[Test]  Epoch: 44	Loss: 0.022423	Acc: 52.1% (5210/10000)
[Test]  Epoch: 45	Loss: 0.022505	Acc: 52.4% (5240/10000)
[Test]  Epoch: 46	Loss: 0.022364	Acc: 52.5% (5245/10000)
[Test]  Epoch: 47	Loss: 0.022315	Acc: 52.2% (5223/10000)
[Test]  Epoch: 48	Loss: 0.022327	Acc: 52.4% (5237/10000)
[Test]  Epoch: 49	Loss: 0.022441	Acc: 52.0% (5203/10000)
[Test]  Epoch: 50	Loss: 0.022107	Acc: 52.4% (5243/10000)
[Test]  Epoch: 51	Loss: 0.022023	Acc: 52.6% (5264/10000)
[Test]  Epoch: 52	Loss: 0.022225	Acc: 52.6% (5264/10000)
[Test]  Epoch: 53	Loss: 0.022197	Acc: 52.5% (5248/10000)
[Test]  Epoch: 54	Loss: 0.022050	Acc: 52.5% (5250/10000)
[Test]  Epoch: 55	Loss: 0.022247	Acc: 52.4% (5242/10000)
[Test]  Epoch: 56	Loss: 0.022332	Acc: 52.5% (5254/10000)
[Test]  Epoch: 57	Loss: 0.022282	Acc: 52.2% (5222/10000)
[Test]  Epoch: 58	Loss: 0.022130	Acc: 51.9% (5187/10000)
[Test]  Epoch: 59	Loss: 0.022140	Acc: 52.6% (5261/10000)
[Test]  Epoch: 60	Loss: 0.022063	Acc: 52.6% (5256/10000)
[Test]  Epoch: 61	Loss: 0.021978	Acc: 52.7% (5271/10000)
[Test]  Epoch: 62	Loss: 0.021915	Acc: 52.8% (5278/10000)
[Test]  Epoch: 63	Loss: 0.021878	Acc: 52.8% (5279/10000)
[Test]  Epoch: 64	Loss: 0.021879	Acc: 53.0% (5303/10000)
[Test]  Epoch: 65	Loss: 0.021920	Acc: 52.7% (5269/10000)
[Test]  Epoch: 66	Loss: 0.021888	Acc: 52.8% (5282/10000)
[Test]  Epoch: 67	Loss: 0.021928	Acc: 52.5% (5253/10000)
[Test]  Epoch: 68	Loss: 0.021843	Acc: 52.8% (5282/10000)
[Test]  Epoch: 69	Loss: 0.021866	Acc: 53.0% (5303/10000)
[Test]  Epoch: 70	Loss: 0.021818	Acc: 52.6% (5262/10000)
[Test]  Epoch: 71	Loss: 0.021883	Acc: 52.7% (5272/10000)
[Test]  Epoch: 72	Loss: 0.021870	Acc: 52.7% (5267/10000)
[Test]  Epoch: 73	Loss: 0.021853	Acc: 52.7% (5274/10000)
[Test]  Epoch: 74	Loss: 0.021838	Acc: 52.8% (5283/10000)
[Test]  Epoch: 75	Loss: 0.021865	Acc: 52.8% (5279/10000)
[Test]  Epoch: 76	Loss: 0.021784	Acc: 52.7% (5272/10000)
[Test]  Epoch: 77	Loss: 0.021759	Acc: 53.0% (5297/10000)
[Test]  Epoch: 78	Loss: 0.021870	Acc: 53.0% (5301/10000)
[Test]  Epoch: 79	Loss: 0.021919	Acc: 52.9% (5286/10000)
[Test]  Epoch: 80	Loss: 0.021914	Acc: 52.7% (5271/10000)
[Test]  Epoch: 81	Loss: 0.021902	Acc: 53.1% (5310/10000)
[Test]  Epoch: 82	Loss: 0.021901	Acc: 52.8% (5281/10000)
[Test]  Epoch: 83	Loss: 0.021872	Acc: 52.8% (5278/10000)
[Test]  Epoch: 84	Loss: 0.021807	Acc: 52.9% (5288/10000)
[Test]  Epoch: 85	Loss: 0.021823	Acc: 52.8% (5280/10000)
[Test]  Epoch: 86	Loss: 0.021834	Acc: 52.9% (5290/10000)
[Test]  Epoch: 87	Loss: 0.021760	Acc: 52.9% (5285/10000)
[Test]  Epoch: 88	Loss: 0.021770	Acc: 53.0% (5300/10000)
[Test]  Epoch: 89	Loss: 0.021818	Acc: 53.0% (5299/10000)
[Test]  Epoch: 90	Loss: 0.021813	Acc: 53.1% (5306/10000)
[Test]  Epoch: 91	Loss: 0.021788	Acc: 52.7% (5271/10000)
[Test]  Epoch: 92	Loss: 0.021810	Acc: 52.8% (5281/10000)
[Test]  Epoch: 93	Loss: 0.021756	Acc: 52.7% (5267/10000)
[Test]  Epoch: 94	Loss: 0.021776	Acc: 53.1% (5306/10000)
[Test]  Epoch: 95	Loss: 0.021696	Acc: 53.0% (5295/10000)
[Test]  Epoch: 96	Loss: 0.021765	Acc: 53.1% (5307/10000)
[Test]  Epoch: 97	Loss: 0.021778	Acc: 52.9% (5294/10000)
[Test]  Epoch: 98	Loss: 0.021801	Acc: 52.8% (5281/10000)
[Test]  Epoch: 99	Loss: 0.021775	Acc: 52.8% (5281/10000)
[Test]  Epoch: 100	Loss: 0.021733	Acc: 52.8% (5275/10000)
===========finish==========
['2024-08-19', '05:42:50.211910', '100', 'test', '0.021733196711540222', '52.75', '53.1']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-vgg16_bn/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('features.0.weight', 0.0), ('features.0.bias', 0.0), ('features.1.weight', 0.0), ('features.1.bias', 0.0), ('features.3.weight', 0.0), ('features.3.bias', 0.0), ('features.4.weight', 0.0), ('features.4.bias', 0.0), ('features.7.weight', 0.0), ('features.7.bias', 0.0), ('features.8.weight', 0.0), ('features.8.bias', 0.0), ('features.10.weight', 0.0), ('features.10.bias', 0.0), ('features.11.weight', 0.0), ('features.11.bias', 0.0), ('features.14.weight', 0.0), ('features.14.bias', 0.0), ('features.15.weight', 0.0), ('features.15.bias', 0.0), ('features.17.weight', 0.0), ('features.17.bias', 0.0), ('features.18.weight', 0.0), ('features.18.bias', 0.0), ('features.20.weight', 0.0), ('features.20.bias', 0.0), ('features.21.weight', 0.0), ('features.21.bias', 0.0), ('features.24.weight', 0.0), ('features.24.bias', 0.0), ('features.25.weight', 0.0), ('features.25.bias', 0.0), ('features.27.weight', 0.0), ('features.27.bias', 0.0), ('features.28.weight', 0.0), ('features.28.bias', 0.0), ('features.30.weight', 0.0), ('features.30.bias', 0.0), ('features.31.weight', 0.0), ('features.31.bias', 0.0), ('features.34.weight', 0.0), ('features.34.bias', 0.0), ('features.35.weight', 0.0), ('features.35.bias', 0.0), ('features.37.weight', 0.0), ('features.37.bias', 0.0), ('features.38.weight', 0.0), ('features.38.bias', 0.0), ('features.40.weight', 0.0), ('features.40.bias', 0.0), ('features.41.weight', 0.0), ('features.41.bias', 0.0), ('classifier.weight', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('features.0.weight', 0.0), ('features.0.bias', 0.0), ('features.1.weight', 0.0), ('features.1.bias', 0.0), ('features.3.weight', 0.0), ('features.3.bias', 0.0), ('features.4.weight', 0.0), ('features.4.bias', 0.0), ('features.7.weight', 0.0), ('features.7.bias', 0.0), ('features.8.weight', 0.0), ('features.8.bias', 0.0), ('features.10.weight', 0.0), ('features.10.bias', 0.0), ('features.11.weight', 0.0), ('features.11.bias', 0.0), ('features.14.weight', 0.0), ('features.14.bias', 0.0), ('features.15.weight', 0.0), ('features.15.bias', 0.0), ('features.17.weight', 0.0), ('features.17.bias', 0.0), ('features.18.weight', 0.0), ('features.18.bias', 0.0), ('features.20.weight', 0.0), ('features.20.bias', 0.0), ('features.21.weight', 0.0), ('features.21.bias', 0.0), ('features.24.weight', 0.0), ('features.24.bias', 0.0), ('features.25.weight', 0.0), ('features.25.bias', 0.0), ('features.27.weight', 0.0), ('features.27.bias', 0.0), ('features.28.weight', 0.0), ('features.28.bias', 0.0), ('features.30.weight', 0.0), ('features.30.bias', 0.0), ('features.31.weight', 0.0), ('features.31.bias', 0.0), ('features.34.weight', 0.0), ('features.34.bias', 0.0), ('features.35.weight', 0.0), ('features.35.bias', 0.0), ('features.37.weight', 0.0), ('features.37.bias', 0.0), ('features.38.weight', 0.0), ('features.38.bias', 0.0), ('features.40.weight', 0.0), ('features.40.bias', 0.0), ('features.41.weight', 0.0), ('features.41.bias', 0.0), ('classifier.weight', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
get_sample_layers n=27  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.005736	Acc: 87.7% (8765/10000)
[Test]  Epoch: 2	Loss: 0.005697	Acc: 87.8% (8779/10000)
[Test]  Epoch: 3	Loss: 0.005521	Acc: 88.3% (8830/10000)
[Test]  Epoch: 4	Loss: 0.005555	Acc: 88.2% (8815/10000)
[Test]  Epoch: 5	Loss: 0.005538	Acc: 88.1% (8810/10000)
[Test]  Epoch: 6	Loss: 0.005408	Acc: 88.5% (8855/10000)
[Test]  Epoch: 7	Loss: 0.005465	Acc: 88.5% (8845/10000)
[Test]  Epoch: 8	Loss: 0.005392	Acc: 88.8% (8877/10000)
[Test]  Epoch: 9	Loss: 0.005351	Acc: 88.6% (8863/10000)
[Test]  Epoch: 10	Loss: 0.005436	Acc: 88.6% (8862/10000)
[Test]  Epoch: 11	Loss: 0.005518	Acc: 88.3% (8828/10000)
[Test]  Epoch: 12	Loss: 0.005339	Acc: 88.9% (8890/10000)
[Test]  Epoch: 13	Loss: 0.005631	Acc: 88.2% (8819/10000)
[Test]  Epoch: 14	Loss: 0.005470	Acc: 88.7% (8868/10000)
[Test]  Epoch: 15	Loss: 0.005476	Acc: 88.5% (8855/10000)
[Test]  Epoch: 16	Loss: 0.005514	Acc: 88.7% (8873/10000)
[Test]  Epoch: 17	Loss: 0.005491	Acc: 88.7% (8866/10000)
[Test]  Epoch: 18	Loss: 0.005402	Acc: 88.9% (8892/10000)
[Test]  Epoch: 19	Loss: 0.005602	Acc: 88.8% (8878/10000)
[Test]  Epoch: 20	Loss: 0.005478	Acc: 89.1% (8906/10000)
[Test]  Epoch: 21	Loss: 0.005580	Acc: 88.8% (8882/10000)
[Test]  Epoch: 22	Loss: 0.005530	Acc: 89.0% (8897/10000)
[Test]  Epoch: 23	Loss: 0.005632	Acc: 88.7% (8869/10000)
[Test]  Epoch: 24	Loss: 0.005692	Acc: 88.6% (8864/10000)
[Test]  Epoch: 25	Loss: 0.005726	Acc: 88.6% (8858/10000)
[Test]  Epoch: 26	Loss: 0.005636	Acc: 89.1% (8913/10000)
[Test]  Epoch: 27	Loss: 0.005719	Acc: 88.7% (8865/10000)
[Test]  Epoch: 28	Loss: 0.005709	Acc: 88.9% (8889/10000)
[Test]  Epoch: 29	Loss: 0.005841	Acc: 88.6% (8858/10000)
[Test]  Epoch: 30	Loss: 0.005679	Acc: 88.8% (8885/10000)
[Test]  Epoch: 31	Loss: 0.005613	Acc: 88.9% (8892/10000)
[Test]  Epoch: 32	Loss: 0.005726	Acc: 88.6% (8864/10000)
[Test]  Epoch: 33	Loss: 0.005757	Acc: 88.6% (8859/10000)
[Test]  Epoch: 34	Loss: 0.005951	Acc: 88.3% (8829/10000)
[Test]  Epoch: 35	Loss: 0.005816	Acc: 88.9% (8887/10000)
[Test]  Epoch: 36	Loss: 0.005815	Acc: 88.5% (8846/10000)
[Test]  Epoch: 37	Loss: 0.005942	Acc: 88.3% (8835/10000)
[Test]  Epoch: 38	Loss: 0.005879	Acc: 88.6% (8864/10000)
[Test]  Epoch: 39	Loss: 0.005895	Acc: 88.5% (8850/10000)
[Test]  Epoch: 40	Loss: 0.005906	Acc: 88.6% (8856/10000)
[Test]  Epoch: 41	Loss: 0.005933	Acc: 88.4% (8842/10000)
[Test]  Epoch: 42	Loss: 0.005918	Acc: 88.6% (8856/10000)
[Test]  Epoch: 43	Loss: 0.005889	Acc: 88.8% (8878/10000)
[Test]  Epoch: 44	Loss: 0.006026	Acc: 88.9% (8889/10000)
[Test]  Epoch: 45	Loss: 0.005815	Acc: 88.6% (8861/10000)
[Test]  Epoch: 46	Loss: 0.006023	Acc: 88.4% (8837/10000)
[Test]  Epoch: 47	Loss: 0.006192	Acc: 88.4% (8844/10000)
[Test]  Epoch: 48	Loss: 0.006047	Acc: 88.7% (8865/10000)
[Test]  Epoch: 49	Loss: 0.006013	Acc: 88.7% (8866/10000)
[Test]  Epoch: 50	Loss: 0.006035	Acc: 88.6% (8857/10000)
[Test]  Epoch: 51	Loss: 0.005930	Acc: 88.8% (8884/10000)
[Test]  Epoch: 52	Loss: 0.005982	Acc: 88.7% (8871/10000)
[Test]  Epoch: 53	Loss: 0.005986	Acc: 88.5% (8845/10000)
[Test]  Epoch: 54	Loss: 0.006148	Acc: 88.6% (8860/10000)
[Test]  Epoch: 55	Loss: 0.006105	Acc: 88.6% (8862/10000)
[Test]  Epoch: 56	Loss: 0.006060	Acc: 88.3% (8832/10000)
[Test]  Epoch: 57	Loss: 0.006061	Acc: 88.5% (8845/10000)
[Test]  Epoch: 58	Loss: 0.006165	Acc: 88.2% (8823/10000)
[Test]  Epoch: 59	Loss: 0.006222	Acc: 88.4% (8838/10000)
[Test]  Epoch: 60	Loss: 0.006219	Acc: 88.6% (8862/10000)
[Test]  Epoch: 61	Loss: 0.005986	Acc: 88.7% (8869/10000)
[Test]  Epoch: 62	Loss: 0.006210	Acc: 88.5% (8855/10000)
[Test]  Epoch: 63	Loss: 0.006066	Acc: 88.9% (8892/10000)
[Test]  Epoch: 64	Loss: 0.006209	Acc: 88.3% (8833/10000)
[Test]  Epoch: 65	Loss: 0.006007	Acc: 88.8% (8882/10000)
[Test]  Epoch: 66	Loss: 0.006095	Acc: 88.5% (8850/10000)
[Test]  Epoch: 67	Loss: 0.006099	Acc: 88.7% (8870/10000)
[Test]  Epoch: 68	Loss: 0.006033	Acc: 88.7% (8868/10000)
[Test]  Epoch: 69	Loss: 0.005973	Acc: 88.9% (8889/10000)
[Test]  Epoch: 70	Loss: 0.006139	Acc: 89.0% (8896/10000)
[Test]  Epoch: 71	Loss: 0.006116	Acc: 88.5% (8852/10000)
[Test]  Epoch: 72	Loss: 0.006140	Acc: 88.5% (8855/10000)
[Test]  Epoch: 73	Loss: 0.006179	Acc: 88.7% (8872/10000)
[Test]  Epoch: 74	Loss: 0.006223	Acc: 88.4% (8840/10000)
[Test]  Epoch: 75	Loss: 0.006082	Acc: 88.7% (8874/10000)
[Test]  Epoch: 76	Loss: 0.006090	Acc: 88.4% (8844/10000)
[Test]  Epoch: 77	Loss: 0.006311	Acc: 88.5% (8850/10000)
[Test]  Epoch: 78	Loss: 0.006109	Acc: 88.8% (8883/10000)
[Test]  Epoch: 79	Loss: 0.006139	Acc: 88.6% (8857/10000)
[Test]  Epoch: 80	Loss: 0.006197	Acc: 88.4% (8842/10000)
[Test]  Epoch: 81	Loss: 0.006265	Acc: 88.6% (8861/10000)
[Test]  Epoch: 82	Loss: 0.006088	Acc: 88.4% (8838/10000)
[Test]  Epoch: 83	Loss: 0.006107	Acc: 88.8% (8877/10000)
[Test]  Epoch: 84	Loss: 0.006189	Acc: 88.8% (8884/10000)
[Test]  Epoch: 85	Loss: 0.006156	Acc: 88.7% (8868/10000)
[Test]  Epoch: 86	Loss: 0.006181	Acc: 88.4% (8839/10000)
[Test]  Epoch: 87	Loss: 0.006213	Acc: 88.5% (8848/10000)
[Test]  Epoch: 88	Loss: 0.006269	Acc: 88.5% (8848/10000)
[Test]  Epoch: 89	Loss: 0.006105	Acc: 88.7% (8873/10000)
[Test]  Epoch: 90	Loss: 0.006116	Acc: 88.8% (8882/10000)
[Test]  Epoch: 91	Loss: 0.006133	Acc: 88.9% (8890/10000)
[Test]  Epoch: 92	Loss: 0.006146	Acc: 88.6% (8860/10000)
[Test]  Epoch: 93	Loss: 0.006050	Acc: 88.6% (8859/10000)
[Test]  Epoch: 94	Loss: 0.006052	Acc: 88.8% (8878/10000)
[Test]  Epoch: 95	Loss: 0.006118	Acc: 88.8% (8877/10000)
[Test]  Epoch: 96	Loss: 0.005983	Acc: 88.7% (8870/10000)
[Test]  Epoch: 97	Loss: 0.006090	Acc: 88.7% (8866/10000)
[Test]  Epoch: 98	Loss: 0.006265	Acc: 88.7% (8873/10000)
[Test]  Epoch: 99	Loss: 0.006200	Acc: 88.7% (8872/10000)
[Test]  Epoch: 100	Loss: 0.006112	Acc: 88.7% (8869/10000)
===========finish==========
['2024-08-19', '05:47:04.395968', '100', 'test', '0.006111869164556265', '88.69', '89.13']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.0.weight', 'features.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.027136	Acc: 52.5% (5253/10000)
[Test]  Epoch: 2	Loss: 0.012528	Acc: 74.2% (7421/10000)
[Test]  Epoch: 3	Loss: 0.010001	Acc: 79.0% (7898/10000)
[Test]  Epoch: 4	Loss: 0.009668	Acc: 79.5% (7946/10000)
[Test]  Epoch: 5	Loss: 0.008869	Acc: 81.1% (8109/10000)
[Test]  Epoch: 6	Loss: 0.008771	Acc: 81.4% (8142/10000)
[Test]  Epoch: 7	Loss: 0.008537	Acc: 81.9% (8191/10000)
[Test]  Epoch: 8	Loss: 0.008087	Acc: 82.7% (8266/10000)
[Test]  Epoch: 9	Loss: 0.007931	Acc: 83.2% (8316/10000)
[Test]  Epoch: 10	Loss: 0.008315	Acc: 82.3% (8235/10000)
[Test]  Epoch: 11	Loss: 0.007849	Acc: 83.3% (8331/10000)
[Test]  Epoch: 12	Loss: 0.007794	Acc: 83.7% (8366/10000)
[Test]  Epoch: 13	Loss: 0.007773	Acc: 83.9% (8391/10000)
[Test]  Epoch: 14	Loss: 0.007829	Acc: 83.7% (8368/10000)
[Test]  Epoch: 15	Loss: 0.007739	Acc: 84.1% (8406/10000)
[Test]  Epoch: 16	Loss: 0.007576	Acc: 84.4% (8442/10000)
[Test]  Epoch: 17	Loss: 0.007753	Acc: 83.7% (8370/10000)
[Test]  Epoch: 18	Loss: 0.007549	Acc: 84.3% (8433/10000)
[Test]  Epoch: 19	Loss: 0.007883	Acc: 84.2% (8420/10000)
[Test]  Epoch: 20	Loss: 0.007549	Acc: 84.7% (8471/10000)
[Test]  Epoch: 21	Loss: 0.007624	Acc: 84.7% (8474/10000)
[Test]  Epoch: 22	Loss: 0.007575	Acc: 84.9% (8488/10000)
[Test]  Epoch: 23	Loss: 0.007705	Acc: 84.7% (8471/10000)
[Test]  Epoch: 24	Loss: 0.007685	Acc: 84.7% (8468/10000)
[Test]  Epoch: 25	Loss: 0.007843	Acc: 84.2% (8418/10000)
[Test]  Epoch: 26	Loss: 0.007811	Acc: 84.4% (8437/10000)
[Test]  Epoch: 27	Loss: 0.007950	Acc: 84.2% (8420/10000)
[Test]  Epoch: 28	Loss: 0.007696	Acc: 84.7% (8472/10000)
[Test]  Epoch: 29	Loss: 0.007964	Acc: 84.4% (8436/10000)
[Test]  Epoch: 30	Loss: 0.007719	Acc: 84.8% (8476/10000)
[Test]  Epoch: 31	Loss: 0.007755	Acc: 84.7% (8474/10000)
[Test]  Epoch: 32	Loss: 0.007718	Acc: 85.0% (8498/10000)
[Test]  Epoch: 33	Loss: 0.007846	Acc: 84.7% (8465/10000)
[Test]  Epoch: 34	Loss: 0.007952	Acc: 85.0% (8500/10000)
[Test]  Epoch: 35	Loss: 0.008079	Acc: 84.3% (8427/10000)
[Test]  Epoch: 36	Loss: 0.007944	Acc: 84.6% (8460/10000)
[Test]  Epoch: 37	Loss: 0.008186	Acc: 84.7% (8470/10000)
[Test]  Epoch: 38	Loss: 0.008127	Acc: 84.5% (8445/10000)
[Test]  Epoch: 39	Loss: 0.008178	Acc: 84.4% (8444/10000)
[Test]  Epoch: 40	Loss: 0.008041	Acc: 84.8% (8475/10000)
[Test]  Epoch: 41	Loss: 0.008032	Acc: 84.9% (8488/10000)
[Test]  Epoch: 42	Loss: 0.008217	Acc: 84.7% (8472/10000)
[Test]  Epoch: 43	Loss: 0.008082	Acc: 85.2% (8515/10000)
[Test]  Epoch: 44	Loss: 0.008058	Acc: 84.5% (8454/10000)
[Test]  Epoch: 45	Loss: 0.007840	Acc: 85.5% (8552/10000)
[Test]  Epoch: 46	Loss: 0.008107	Acc: 84.7% (8473/10000)
[Test]  Epoch: 47	Loss: 0.008341	Acc: 84.7% (8473/10000)
[Test]  Epoch: 48	Loss: 0.007952	Acc: 85.3% (8531/10000)
[Test]  Epoch: 49	Loss: 0.008109	Acc: 84.8% (8485/10000)
[Test]  Epoch: 50	Loss: 0.008145	Acc: 85.0% (8499/10000)
[Test]  Epoch: 51	Loss: 0.007928	Acc: 85.2% (8518/10000)
[Test]  Epoch: 52	Loss: 0.008048	Acc: 84.8% (8478/10000)
[Test]  Epoch: 53	Loss: 0.007955	Acc: 85.2% (8522/10000)
[Test]  Epoch: 54	Loss: 0.008088	Acc: 85.1% (8509/10000)
[Test]  Epoch: 55	Loss: 0.008248	Acc: 84.8% (8483/10000)
[Test]  Epoch: 56	Loss: 0.008285	Acc: 84.8% (8476/10000)
[Test]  Epoch: 57	Loss: 0.008133	Acc: 85.0% (8496/10000)
[Test]  Epoch: 58	Loss: 0.008232	Acc: 84.9% (8493/10000)
[Test]  Epoch: 59	Loss: 0.008429	Acc: 84.7% (8473/10000)
[Test]  Epoch: 60	Loss: 0.008263	Acc: 85.3% (8531/10000)
[Test]  Epoch: 61	Loss: 0.008131	Acc: 85.1% (8508/10000)
[Test]  Epoch: 62	Loss: 0.008267	Acc: 85.1% (8511/10000)
[Test]  Epoch: 63	Loss: 0.008229	Acc: 84.8% (8485/10000)
[Test]  Epoch: 64	Loss: 0.008280	Acc: 85.0% (8498/10000)
[Test]  Epoch: 65	Loss: 0.008029	Acc: 85.4% (8541/10000)
[Test]  Epoch: 66	Loss: 0.008045	Acc: 85.3% (8532/10000)
[Test]  Epoch: 67	Loss: 0.007983	Acc: 85.6% (8561/10000)
[Test]  Epoch: 68	Loss: 0.008263	Acc: 85.0% (8500/10000)
[Test]  Epoch: 69	Loss: 0.007926	Acc: 85.3% (8528/10000)
[Test]  Epoch: 70	Loss: 0.008233	Acc: 85.1% (8511/10000)
[Test]  Epoch: 71	Loss: 0.008162	Acc: 85.2% (8519/10000)
[Test]  Epoch: 72	Loss: 0.008164	Acc: 85.1% (8512/10000)
[Test]  Epoch: 73	Loss: 0.008276	Acc: 85.1% (8509/10000)
[Test]  Epoch: 74	Loss: 0.008101	Acc: 85.6% (8556/10000)
[Test]  Epoch: 75	Loss: 0.008089	Acc: 85.2% (8517/10000)
[Test]  Epoch: 76	Loss: 0.008028	Acc: 85.5% (8546/10000)
[Test]  Epoch: 77	Loss: 0.008276	Acc: 85.3% (8533/10000)
[Test]  Epoch: 78	Loss: 0.008157	Acc: 85.4% (8542/10000)
[Test]  Epoch: 79	Loss: 0.008256	Acc: 84.7% (8472/10000)
[Test]  Epoch: 80	Loss: 0.008228	Acc: 85.2% (8516/10000)
[Test]  Epoch: 81	Loss: 0.007974	Acc: 85.3% (8534/10000)
[Test]  Epoch: 82	Loss: 0.008192	Acc: 85.0% (8497/10000)
[Test]  Epoch: 83	Loss: 0.008178	Acc: 84.9% (8494/10000)
[Test]  Epoch: 84	Loss: 0.008267	Acc: 85.1% (8509/10000)
[Test]  Epoch: 85	Loss: 0.008243	Acc: 85.3% (8528/10000)
[Test]  Epoch: 86	Loss: 0.008220	Acc: 85.2% (8523/10000)
[Test]  Epoch: 87	Loss: 0.008248	Acc: 85.3% (8527/10000)
[Test]  Epoch: 88	Loss: 0.008294	Acc: 85.4% (8539/10000)
[Test]  Epoch: 89	Loss: 0.008196	Acc: 85.2% (8522/10000)
[Test]  Epoch: 90	Loss: 0.008257	Acc: 85.3% (8535/10000)
[Test]  Epoch: 91	Loss: 0.008185	Acc: 84.9% (8489/10000)
[Test]  Epoch: 92	Loss: 0.008172	Acc: 85.1% (8507/10000)
[Test]  Epoch: 93	Loss: 0.008176	Acc: 85.2% (8522/10000)
[Test]  Epoch: 94	Loss: 0.008109	Acc: 85.0% (8505/10000)
[Test]  Epoch: 95	Loss: 0.008048	Acc: 85.4% (8541/10000)
[Test]  Epoch: 96	Loss: 0.008095	Acc: 85.3% (8526/10000)
[Test]  Epoch: 97	Loss: 0.008163	Acc: 85.3% (8529/10000)
[Test]  Epoch: 98	Loss: 0.008250	Acc: 85.2% (8517/10000)
[Test]  Epoch: 99	Loss: 0.008226	Acc: 85.1% (8513/10000)
[Test]  Epoch: 100	Loss: 0.008134	Acc: 85.0% (8503/10000)
===========finish==========
['2024-08-19', '05:50:06.269184', '100', 'test', '0.008133578868210315', '85.03', '85.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039027	Acc: 30.0% (3004/10000)
[Test]  Epoch: 2	Loss: 0.031134	Acc: 40.6% (4065/10000)
[Test]  Epoch: 3	Loss: 0.027362	Acc: 45.0% (4500/10000)
[Test]  Epoch: 4	Loss: 0.023737	Acc: 49.4% (4944/10000)
[Test]  Epoch: 5	Loss: 0.022431	Acc: 51.8% (5176/10000)
[Test]  Epoch: 6	Loss: 0.021149	Acc: 54.8% (5477/10000)
[Test]  Epoch: 7	Loss: 0.021540	Acc: 54.4% (5442/10000)
[Test]  Epoch: 8	Loss: 0.020045	Acc: 57.2% (5722/10000)
[Test]  Epoch: 9	Loss: 0.020145	Acc: 56.9% (5689/10000)
[Test]  Epoch: 10	Loss: 0.019699	Acc: 58.2% (5823/10000)
[Test]  Epoch: 11	Loss: 0.018993	Acc: 60.3% (6029/10000)
[Test]  Epoch: 12	Loss: 0.018618	Acc: 60.9% (6088/10000)
[Test]  Epoch: 13	Loss: 0.019046	Acc: 60.3% (6034/10000)
[Test]  Epoch: 14	Loss: 0.019845	Acc: 60.1% (6006/10000)
[Test]  Epoch: 15	Loss: 0.018126	Acc: 62.3% (6231/10000)
[Test]  Epoch: 16	Loss: 0.018165	Acc: 63.1% (6314/10000)
[Test]  Epoch: 17	Loss: 0.019246	Acc: 60.3% (6033/10000)
[Test]  Epoch: 18	Loss: 0.018198	Acc: 62.7% (6269/10000)
[Test]  Epoch: 19	Loss: 0.018149	Acc: 63.4% (6344/10000)
[Test]  Epoch: 20	Loss: 0.018440	Acc: 63.5% (6354/10000)
[Test]  Epoch: 21	Loss: 0.018407	Acc: 64.1% (6406/10000)
[Test]  Epoch: 22	Loss: 0.017826	Acc: 64.4% (6443/10000)
[Test]  Epoch: 23	Loss: 0.018126	Acc: 64.8% (6480/10000)
[Test]  Epoch: 24	Loss: 0.018669	Acc: 64.4% (6440/10000)
[Test]  Epoch: 25	Loss: 0.019465	Acc: 63.4% (6339/10000)
[Test]  Epoch: 26	Loss: 0.018995	Acc: 64.1% (6411/10000)
[Test]  Epoch: 27	Loss: 0.018753	Acc: 64.5% (6449/10000)
[Test]  Epoch: 28	Loss: 0.019432	Acc: 63.7% (6374/10000)
[Test]  Epoch: 29	Loss: 0.018729	Acc: 65.3% (6531/10000)
[Test]  Epoch: 30	Loss: 0.020155	Acc: 63.4% (6337/10000)
[Test]  Epoch: 31	Loss: 0.019675	Acc: 63.2% (6316/10000)
[Test]  Epoch: 32	Loss: 0.018877	Acc: 64.9% (6494/10000)
[Test]  Epoch: 33	Loss: 0.019152	Acc: 65.0% (6499/10000)
[Test]  Epoch: 34	Loss: 0.019174	Acc: 64.9% (6490/10000)
[Test]  Epoch: 35	Loss: 0.020421	Acc: 63.8% (6384/10000)
[Test]  Epoch: 36	Loss: 0.018688	Acc: 66.5% (6650/10000)
[Test]  Epoch: 37	Loss: 0.019010	Acc: 65.7% (6572/10000)
[Test]  Epoch: 38	Loss: 0.020774	Acc: 63.6% (6356/10000)
[Test]  Epoch: 39	Loss: 0.019494	Acc: 64.8% (6480/10000)
[Test]  Epoch: 40	Loss: 0.019147	Acc: 65.6% (6560/10000)
[Test]  Epoch: 41	Loss: 0.019034	Acc: 66.0% (6602/10000)
[Test]  Epoch: 42	Loss: 0.019205	Acc: 66.1% (6611/10000)
[Test]  Epoch: 43	Loss: 0.019628	Acc: 65.5% (6550/10000)
[Test]  Epoch: 44	Loss: 0.019067	Acc: 66.5% (6645/10000)
[Test]  Epoch: 45	Loss: 0.019504	Acc: 66.1% (6610/10000)
[Test]  Epoch: 46	Loss: 0.020145	Acc: 65.4% (6542/10000)
[Test]  Epoch: 47	Loss: 0.019721	Acc: 65.6% (6562/10000)
[Test]  Epoch: 48	Loss: 0.020685	Acc: 64.7% (6469/10000)
[Test]  Epoch: 49	Loss: 0.019823	Acc: 66.1% (6607/10000)
[Test]  Epoch: 50	Loss: 0.019283	Acc: 66.5% (6655/10000)
[Test]  Epoch: 51	Loss: 0.019227	Acc: 66.7% (6665/10000)
[Test]  Epoch: 52	Loss: 0.019272	Acc: 67.5% (6753/10000)
[Test]  Epoch: 53	Loss: 0.019683	Acc: 66.6% (6661/10000)
[Test]  Epoch: 54	Loss: 0.019459	Acc: 67.4% (6737/10000)
[Test]  Epoch: 55	Loss: 0.020040	Acc: 66.0% (6598/10000)
[Test]  Epoch: 56	Loss: 0.020289	Acc: 66.3% (6626/10000)
[Test]  Epoch: 57	Loss: 0.019568	Acc: 66.6% (6660/10000)
[Test]  Epoch: 58	Loss: 0.019390	Acc: 67.0% (6702/10000)
[Test]  Epoch: 59	Loss: 0.019590	Acc: 66.8% (6685/10000)
[Test]  Epoch: 60	Loss: 0.019847	Acc: 67.1% (6707/10000)
[Test]  Epoch: 61	Loss: 0.019362	Acc: 67.8% (6779/10000)
[Test]  Epoch: 62	Loss: 0.019474	Acc: 67.5% (6751/10000)
[Test]  Epoch: 63	Loss: 0.019502	Acc: 67.3% (6726/10000)
[Test]  Epoch: 64	Loss: 0.019539	Acc: 67.2% (6721/10000)
[Test]  Epoch: 65	Loss: 0.019166	Acc: 67.5% (6749/10000)
[Test]  Epoch: 66	Loss: 0.019493	Acc: 67.5% (6754/10000)
[Test]  Epoch: 67	Loss: 0.019437	Acc: 67.3% (6728/10000)
[Test]  Epoch: 68	Loss: 0.019379	Acc: 67.6% (6758/10000)
[Test]  Epoch: 69	Loss: 0.019243	Acc: 67.1% (6710/10000)
[Test]  Epoch: 70	Loss: 0.019374	Acc: 67.3% (6735/10000)
[Test]  Epoch: 71	Loss: 0.019390	Acc: 67.6% (6758/10000)
[Test]  Epoch: 72	Loss: 0.019132	Acc: 67.3% (6732/10000)
[Test]  Epoch: 73	Loss: 0.019491	Acc: 67.3% (6732/10000)
[Test]  Epoch: 74	Loss: 0.019061	Acc: 67.7% (6766/10000)
[Test]  Epoch: 75	Loss: 0.018842	Acc: 67.7% (6767/10000)
[Test]  Epoch: 76	Loss: 0.019069	Acc: 67.8% (6782/10000)
[Test]  Epoch: 77	Loss: 0.019546	Acc: 67.6% (6760/10000)
[Test]  Epoch: 78	Loss: 0.019097	Acc: 67.8% (6781/10000)
[Test]  Epoch: 79	Loss: 0.019363	Acc: 67.2% (6719/10000)
[Test]  Epoch: 80	Loss: 0.019110	Acc: 67.5% (6746/10000)
[Test]  Epoch: 81	Loss: 0.019160	Acc: 67.8% (6779/10000)
[Test]  Epoch: 82	Loss: 0.019402	Acc: 67.6% (6759/10000)
[Test]  Epoch: 83	Loss: 0.019194	Acc: 67.9% (6786/10000)
[Test]  Epoch: 84	Loss: 0.019309	Acc: 67.9% (6789/10000)
[Test]  Epoch: 85	Loss: 0.019202	Acc: 67.3% (6729/10000)
[Test]  Epoch: 86	Loss: 0.019169	Acc: 67.8% (6777/10000)
[Test]  Epoch: 87	Loss: 0.019347	Acc: 67.7% (6772/10000)
[Test]  Epoch: 88	Loss: 0.019476	Acc: 67.4% (6742/10000)
[Test]  Epoch: 89	Loss: 0.019498	Acc: 67.5% (6746/10000)
[Test]  Epoch: 90	Loss: 0.019323	Acc: 67.9% (6790/10000)
[Test]  Epoch: 91	Loss: 0.019228	Acc: 67.7% (6767/10000)
[Test]  Epoch: 92	Loss: 0.019388	Acc: 67.7% (6769/10000)
[Test]  Epoch: 93	Loss: 0.019122	Acc: 67.9% (6791/10000)
[Test]  Epoch: 94	Loss: 0.019158	Acc: 67.7% (6766/10000)
[Test]  Epoch: 95	Loss: 0.019092	Acc: 67.6% (6756/10000)
[Test]  Epoch: 96	Loss: 0.019261	Acc: 67.8% (6775/10000)
[Test]  Epoch: 97	Loss: 0.019010	Acc: 68.1% (6813/10000)
[Test]  Epoch: 98	Loss: 0.019294	Acc: 67.8% (6783/10000)
[Test]  Epoch: 99	Loss: 0.019155	Acc: 67.9% (6787/10000)
[Test]  Epoch: 100	Loss: 0.019311	Acc: 67.8% (6785/10000)
===========finish==========
['2024-08-19', '05:53:14.265638', '100', 'test', '0.019310506922006608', '67.85', '68.13']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.058401	Acc: 22.3% (2231/10000)
[Test]  Epoch: 2	Loss: 0.034569	Acc: 37.1% (3707/10000)
[Test]  Epoch: 3	Loss: 0.029449	Acc: 40.1% (4013/10000)
[Test]  Epoch: 4	Loss: 0.025465	Acc: 46.4% (4635/10000)
[Test]  Epoch: 5	Loss: 0.025858	Acc: 44.8% (4477/10000)
[Test]  Epoch: 6	Loss: 0.025045	Acc: 47.2% (4716/10000)
[Test]  Epoch: 7	Loss: 0.024649	Acc: 48.4% (4837/10000)
[Test]  Epoch: 8	Loss: 0.023008	Acc: 51.2% (5116/10000)
[Test]  Epoch: 9	Loss: 0.022362	Acc: 51.7% (5174/10000)
[Test]  Epoch: 10	Loss: 0.024343	Acc: 50.4% (5035/10000)
[Test]  Epoch: 11	Loss: 0.022257	Acc: 53.5% (5345/10000)
[Test]  Epoch: 12	Loss: 0.022261	Acc: 53.3% (5328/10000)
[Test]  Epoch: 13	Loss: 0.021295	Acc: 55.4% (5540/10000)
[Test]  Epoch: 14	Loss: 0.021872	Acc: 54.4% (5444/10000)
[Test]  Epoch: 15	Loss: 0.022486	Acc: 54.5% (5450/10000)
[Test]  Epoch: 16	Loss: 0.022153	Acc: 54.2% (5417/10000)
[Test]  Epoch: 17	Loss: 0.020891	Acc: 57.4% (5741/10000)
[Test]  Epoch: 18	Loss: 0.020462	Acc: 58.1% (5810/10000)
[Test]  Epoch: 19	Loss: 0.022493	Acc: 55.6% (5559/10000)
[Test]  Epoch: 20	Loss: 0.021040	Acc: 58.2% (5821/10000)
[Test]  Epoch: 21	Loss: 0.024600	Acc: 53.0% (5301/10000)
[Test]  Epoch: 22	Loss: 0.022506	Acc: 56.8% (5681/10000)
[Test]  Epoch: 23	Loss: 0.021535	Acc: 59.1% (5907/10000)
[Test]  Epoch: 24	Loss: 0.022359	Acc: 58.3% (5830/10000)
[Test]  Epoch: 25	Loss: 0.021298	Acc: 59.4% (5935/10000)
[Test]  Epoch: 26	Loss: 0.021177	Acc: 60.0% (5996/10000)
[Test]  Epoch: 27	Loss: 0.024155	Acc: 55.6% (5560/10000)
[Test]  Epoch: 28	Loss: 0.023872	Acc: 57.3% (5730/10000)
[Test]  Epoch: 29	Loss: 0.022474	Acc: 58.6% (5864/10000)
[Test]  Epoch: 30	Loss: 0.022846	Acc: 58.3% (5833/10000)
[Test]  Epoch: 31	Loss: 0.023621	Acc: 58.1% (5806/10000)
[Test]  Epoch: 32	Loss: 0.022217	Acc: 59.9% (5988/10000)
[Test]  Epoch: 33	Loss: 0.022051	Acc: 60.0% (6001/10000)
[Test]  Epoch: 34	Loss: 0.023802	Acc: 58.2% (5822/10000)
[Test]  Epoch: 35	Loss: 0.022729	Acc: 59.5% (5952/10000)
[Test]  Epoch: 36	Loss: 0.022434	Acc: 59.8% (5975/10000)
[Test]  Epoch: 37	Loss: 0.022465	Acc: 60.5% (6049/10000)
[Test]  Epoch: 38	Loss: 0.024307	Acc: 58.0% (5798/10000)
[Test]  Epoch: 39	Loss: 0.023806	Acc: 58.9% (5887/10000)
[Test]  Epoch: 40	Loss: 0.022925	Acc: 60.3% (6033/10000)
[Test]  Epoch: 41	Loss: 0.023194	Acc: 60.2% (6018/10000)
[Test]  Epoch: 42	Loss: 0.023214	Acc: 60.3% (6029/10000)
[Test]  Epoch: 43	Loss: 0.022680	Acc: 61.2% (6125/10000)
[Test]  Epoch: 44	Loss: 0.022533	Acc: 61.6% (6158/10000)
[Test]  Epoch: 45	Loss: 0.022624	Acc: 61.3% (6131/10000)
[Test]  Epoch: 46	Loss: 0.022314	Acc: 61.7% (6174/10000)
[Test]  Epoch: 47	Loss: 0.022707	Acc: 62.1% (6208/10000)
[Test]  Epoch: 48	Loss: 0.022884	Acc: 61.0% (6102/10000)
[Test]  Epoch: 49	Loss: 0.023291	Acc: 61.0% (6105/10000)
[Test]  Epoch: 50	Loss: 0.024016	Acc: 59.8% (5981/10000)
[Test]  Epoch: 51	Loss: 0.022756	Acc: 61.3% (6127/10000)
[Test]  Epoch: 52	Loss: 0.023240	Acc: 61.4% (6140/10000)
[Test]  Epoch: 53	Loss: 0.025350	Acc: 59.4% (5942/10000)
[Test]  Epoch: 54	Loss: 0.023307	Acc: 60.9% (6085/10000)
[Test]  Epoch: 55	Loss: 0.023593	Acc: 61.7% (6167/10000)
[Test]  Epoch: 56	Loss: 0.023469	Acc: 61.0% (6095/10000)
[Test]  Epoch: 57	Loss: 0.023649	Acc: 61.6% (6156/10000)
[Test]  Epoch: 58	Loss: 0.023153	Acc: 61.3% (6130/10000)
[Test]  Epoch: 59	Loss: 0.023951	Acc: 60.7% (6069/10000)
[Test]  Epoch: 60	Loss: 0.024327	Acc: 60.6% (6058/10000)
[Test]  Epoch: 61	Loss: 0.023520	Acc: 61.7% (6173/10000)
[Test]  Epoch: 62	Loss: 0.023436	Acc: 61.9% (6193/10000)
[Test]  Epoch: 63	Loss: 0.023048	Acc: 62.0% (6198/10000)
[Test]  Epoch: 64	Loss: 0.023192	Acc: 62.3% (6230/10000)
[Test]  Epoch: 65	Loss: 0.022972	Acc: 62.3% (6230/10000)
[Test]  Epoch: 66	Loss: 0.022873	Acc: 62.1% (6213/10000)
[Test]  Epoch: 67	Loss: 0.023088	Acc: 62.3% (6231/10000)
[Test]  Epoch: 68	Loss: 0.022985	Acc: 62.3% (6232/10000)
[Test]  Epoch: 69	Loss: 0.022779	Acc: 62.3% (6229/10000)
[Test]  Epoch: 70	Loss: 0.022900	Acc: 62.0% (6201/10000)
[Test]  Epoch: 71	Loss: 0.022997	Acc: 62.1% (6206/10000)
[Test]  Epoch: 72	Loss: 0.023007	Acc: 62.1% (6212/10000)
[Test]  Epoch: 73	Loss: 0.022630	Acc: 62.6% (6259/10000)
[Test]  Epoch: 74	Loss: 0.023050	Acc: 62.1% (6214/10000)
[Test]  Epoch: 75	Loss: 0.022786	Acc: 62.6% (6256/10000)
[Test]  Epoch: 76	Loss: 0.022575	Acc: 63.2% (6316/10000)
[Test]  Epoch: 77	Loss: 0.022812	Acc: 62.9% (6286/10000)
[Test]  Epoch: 78	Loss: 0.023116	Acc: 62.3% (6226/10000)
[Test]  Epoch: 79	Loss: 0.022665	Acc: 62.8% (6277/10000)
[Test]  Epoch: 80	Loss: 0.022502	Acc: 63.1% (6308/10000)
[Test]  Epoch: 81	Loss: 0.022756	Acc: 62.7% (6269/10000)
[Test]  Epoch: 82	Loss: 0.022965	Acc: 61.8% (6181/10000)
[Test]  Epoch: 83	Loss: 0.022550	Acc: 63.0% (6301/10000)
[Test]  Epoch: 84	Loss: 0.022796	Acc: 62.5% (6254/10000)
[Test]  Epoch: 85	Loss: 0.022883	Acc: 62.2% (6223/10000)
[Test]  Epoch: 86	Loss: 0.022788	Acc: 62.6% (6261/10000)
[Test]  Epoch: 87	Loss: 0.022929	Acc: 62.3% (6233/10000)
[Test]  Epoch: 88	Loss: 0.022863	Acc: 62.6% (6263/10000)
[Test]  Epoch: 89	Loss: 0.022912	Acc: 62.6% (6263/10000)
[Test]  Epoch: 90	Loss: 0.022606	Acc: 62.4% (6236/10000)
[Test]  Epoch: 91	Loss: 0.022572	Acc: 62.8% (6275/10000)
[Test]  Epoch: 92	Loss: 0.022857	Acc: 62.6% (6260/10000)
[Test]  Epoch: 93	Loss: 0.022495	Acc: 63.1% (6312/10000)
[Test]  Epoch: 94	Loss: 0.022604	Acc: 62.5% (6252/10000)
[Test]  Epoch: 95	Loss: 0.022773	Acc: 62.9% (6292/10000)
[Test]  Epoch: 96	Loss: 0.022455	Acc: 63.3% (6327/10000)
[Test]  Epoch: 97	Loss: 0.022838	Acc: 62.8% (6276/10000)
[Test]  Epoch: 98	Loss: 0.022934	Acc: 62.7% (6266/10000)
[Test]  Epoch: 99	Loss: 0.022633	Acc: 63.3% (6331/10000)
[Test]  Epoch: 100	Loss: 0.022670	Acc: 62.9% (6288/10000)
===========finish==========
['2024-08-19', '05:56:21.165117', '100', 'test', '0.022669797587394715', '62.88', '63.31']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039666	Acc: 23.0% (2303/10000)
[Test]  Epoch: 2	Loss: 0.033863	Acc: 32.0% (3198/10000)
[Test]  Epoch: 3	Loss: 0.028418	Acc: 36.1% (3611/10000)
[Test]  Epoch: 4	Loss: 0.027377	Acc: 39.0% (3905/10000)
[Test]  Epoch: 5	Loss: 0.027723	Acc: 38.9% (3888/10000)
[Test]  Epoch: 6	Loss: 0.028997	Acc: 39.2% (3925/10000)
[Test]  Epoch: 7	Loss: 0.027697	Acc: 40.0% (3996/10000)
[Test]  Epoch: 8	Loss: 0.026462	Acc: 43.4% (4336/10000)
[Test]  Epoch: 9	Loss: 0.025871	Acc: 44.2% (4423/10000)
[Test]  Epoch: 10	Loss: 0.029008	Acc: 40.6% (4059/10000)
[Test]  Epoch: 11	Loss: 0.028931	Acc: 40.1% (4010/10000)
[Test]  Epoch: 12	Loss: 0.028485	Acc: 41.6% (4162/10000)
[Test]  Epoch: 13	Loss: 0.029668	Acc: 40.1% (4006/10000)
[Test]  Epoch: 14	Loss: 0.028215	Acc: 43.7% (4369/10000)
[Test]  Epoch: 15	Loss: 0.029272	Acc: 43.6% (4359/10000)
[Test]  Epoch: 16	Loss: 0.027204	Acc: 46.1% (4612/10000)
[Test]  Epoch: 17	Loss: 0.026812	Acc: 45.5% (4550/10000)
[Test]  Epoch: 18	Loss: 0.028155	Acc: 46.0% (4600/10000)
[Test]  Epoch: 19	Loss: 0.028162	Acc: 46.0% (4600/10000)
[Test]  Epoch: 20	Loss: 0.028207	Acc: 45.1% (4515/10000)
[Test]  Epoch: 21	Loss: 0.028768	Acc: 46.1% (4614/10000)
[Test]  Epoch: 22	Loss: 0.029236	Acc: 45.6% (4559/10000)
[Test]  Epoch: 23	Loss: 0.028779	Acc: 46.5% (4653/10000)
[Test]  Epoch: 24	Loss: 0.028082	Acc: 48.7% (4866/10000)
[Test]  Epoch: 25	Loss: 0.033982	Acc: 43.5% (4353/10000)
[Test]  Epoch: 26	Loss: 0.028641	Acc: 49.1% (4913/10000)
[Test]  Epoch: 27	Loss: 0.032003	Acc: 44.9% (4485/10000)
[Test]  Epoch: 28	Loss: 0.030207	Acc: 47.9% (4793/10000)
[Test]  Epoch: 29	Loss: 0.031645	Acc: 46.6% (4660/10000)
[Test]  Epoch: 30	Loss: 0.031217	Acc: 46.8% (4679/10000)
[Test]  Epoch: 31	Loss: 0.030136	Acc: 48.0% (4795/10000)
[Test]  Epoch: 32	Loss: 0.031662	Acc: 47.4% (4740/10000)
[Test]  Epoch: 33	Loss: 0.032377	Acc: 46.5% (4645/10000)
[Test]  Epoch: 34	Loss: 0.030836	Acc: 47.8% (4775/10000)
[Test]  Epoch: 35	Loss: 0.032933	Acc: 47.3% (4726/10000)
[Test]  Epoch: 36	Loss: 0.033133	Acc: 45.4% (4544/10000)
[Test]  Epoch: 37	Loss: 0.032000	Acc: 47.5% (4754/10000)
[Test]  Epoch: 38	Loss: 0.032743	Acc: 47.5% (4755/10000)
[Test]  Epoch: 39	Loss: 0.032681	Acc: 47.2% (4721/10000)
[Test]  Epoch: 40	Loss: 0.031106	Acc: 48.7% (4867/10000)
[Test]  Epoch: 41	Loss: 0.034694	Acc: 46.4% (4635/10000)
[Test]  Epoch: 42	Loss: 0.033237	Acc: 47.5% (4752/10000)
[Test]  Epoch: 43	Loss: 0.031867	Acc: 49.5% (4946/10000)
[Test]  Epoch: 44	Loss: 0.031082	Acc: 49.4% (4940/10000)
[Test]  Epoch: 45	Loss: 0.033290	Acc: 49.0% (4905/10000)
[Test]  Epoch: 46	Loss: 0.031772	Acc: 49.2% (4917/10000)
[Test]  Epoch: 47	Loss: 0.031815	Acc: 50.0% (4998/10000)
[Test]  Epoch: 48	Loss: 0.031305	Acc: 50.2% (5016/10000)
[Test]  Epoch: 49	Loss: 0.032395	Acc: 49.4% (4935/10000)
[Test]  Epoch: 50	Loss: 0.032615	Acc: 49.4% (4942/10000)
[Test]  Epoch: 51	Loss: 0.033079	Acc: 50.3% (5028/10000)
[Test]  Epoch: 52	Loss: 0.031522	Acc: 50.5% (5052/10000)
[Test]  Epoch: 53	Loss: 0.035083	Acc: 47.9% (4793/10000)
[Test]  Epoch: 54	Loss: 0.031666	Acc: 49.6% (4965/10000)
[Test]  Epoch: 55	Loss: 0.032507	Acc: 49.8% (4982/10000)
[Test]  Epoch: 56	Loss: 0.033086	Acc: 49.8% (4976/10000)
[Test]  Epoch: 57	Loss: 0.032744	Acc: 49.9% (4987/10000)
[Test]  Epoch: 58	Loss: 0.032528	Acc: 50.6% (5061/10000)
[Test]  Epoch: 59	Loss: 0.034283	Acc: 49.4% (4940/10000)
[Test]  Epoch: 60	Loss: 0.033129	Acc: 49.3% (4931/10000)
[Test]  Epoch: 61	Loss: 0.032287	Acc: 50.4% (5039/10000)
[Test]  Epoch: 62	Loss: 0.032039	Acc: 50.5% (5050/10000)
[Test]  Epoch: 63	Loss: 0.032085	Acc: 50.9% (5094/10000)
[Test]  Epoch: 64	Loss: 0.031516	Acc: 51.3% (5131/10000)
[Test]  Epoch: 65	Loss: 0.031600	Acc: 51.2% (5119/10000)
[Test]  Epoch: 66	Loss: 0.031720	Acc: 51.1% (5111/10000)
[Test]  Epoch: 67	Loss: 0.031764	Acc: 50.8% (5076/10000)
[Test]  Epoch: 68	Loss: 0.031731	Acc: 50.9% (5090/10000)
[Test]  Epoch: 69	Loss: 0.031935	Acc: 51.0% (5105/10000)
[Test]  Epoch: 70	Loss: 0.031648	Acc: 51.0% (5105/10000)
[Test]  Epoch: 71	Loss: 0.031546	Acc: 51.5% (5147/10000)
[Test]  Epoch: 72	Loss: 0.031240	Acc: 51.8% (5182/10000)
[Test]  Epoch: 73	Loss: 0.031135	Acc: 52.1% (5207/10000)
[Test]  Epoch: 74	Loss: 0.031825	Acc: 51.2% (5117/10000)
[Test]  Epoch: 75	Loss: 0.031215	Acc: 51.1% (5110/10000)
[Test]  Epoch: 76	Loss: 0.031557	Acc: 52.0% (5201/10000)
[Test]  Epoch: 77	Loss: 0.031363	Acc: 51.8% (5175/10000)
[Test]  Epoch: 78	Loss: 0.031535	Acc: 51.0% (5101/10000)
[Test]  Epoch: 79	Loss: 0.031289	Acc: 51.6% (5157/10000)
[Test]  Epoch: 80	Loss: 0.031336	Acc: 50.7% (5069/10000)
[Test]  Epoch: 81	Loss: 0.031455	Acc: 51.2% (5124/10000)
[Test]  Epoch: 82	Loss: 0.031359	Acc: 51.2% (5125/10000)
[Test]  Epoch: 83	Loss: 0.030922	Acc: 51.8% (5184/10000)
[Test]  Epoch: 84	Loss: 0.031122	Acc: 51.7% (5174/10000)
[Test]  Epoch: 85	Loss: 0.031135	Acc: 51.7% (5174/10000)
[Test]  Epoch: 86	Loss: 0.031392	Acc: 51.4% (5141/10000)
[Test]  Epoch: 87	Loss: 0.031470	Acc: 51.6% (5161/10000)
[Test]  Epoch: 88	Loss: 0.031471	Acc: 51.7% (5174/10000)
[Test]  Epoch: 89	Loss: 0.031306	Acc: 51.9% (5187/10000)
[Test]  Epoch: 90	Loss: 0.031211	Acc: 51.9% (5186/10000)
[Test]  Epoch: 91	Loss: 0.031233	Acc: 51.1% (5110/10000)
[Test]  Epoch: 92	Loss: 0.031255	Acc: 51.9% (5187/10000)
[Test]  Epoch: 93	Loss: 0.030992	Acc: 52.0% (5199/10000)
[Test]  Epoch: 94	Loss: 0.031117	Acc: 51.6% (5160/10000)
[Test]  Epoch: 95	Loss: 0.030897	Acc: 51.9% (5192/10000)
[Test]  Epoch: 96	Loss: 0.030879	Acc: 52.0% (5203/10000)
[Test]  Epoch: 97	Loss: 0.031368	Acc: 51.2% (5119/10000)
[Test]  Epoch: 98	Loss: 0.030889	Acc: 51.6% (5158/10000)
[Test]  Epoch: 99	Loss: 0.031053	Acc: 51.7% (5170/10000)
[Test]  Epoch: 100	Loss: 0.031188	Acc: 51.3% (5133/10000)
===========finish==========
['2024-08-19', '05:59:32.226937', '100', 'test', '0.031188178646564483', '51.33', '52.07']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.095812	Acc: 16.8% (1676/10000)
[Test]  Epoch: 2	Loss: 0.041525	Acc: 30.3% (3031/10000)
[Test]  Epoch: 3	Loss: 0.029240	Acc: 35.2% (3520/10000)
[Test]  Epoch: 4	Loss: 0.028841	Acc: 36.8% (3678/10000)
[Test]  Epoch: 5	Loss: 0.029543	Acc: 36.4% (3638/10000)
[Test]  Epoch: 6	Loss: 0.028266	Acc: 39.3% (3926/10000)
[Test]  Epoch: 7	Loss: 0.028969	Acc: 37.9% (3793/10000)
[Test]  Epoch: 8	Loss: 0.028201	Acc: 39.6% (3965/10000)
[Test]  Epoch: 9	Loss: 0.028847	Acc: 39.1% (3912/10000)
[Test]  Epoch: 10	Loss: 0.031118	Acc: 38.5% (3855/10000)
[Test]  Epoch: 11	Loss: 0.029904	Acc: 38.9% (3892/10000)
[Test]  Epoch: 12	Loss: 0.029672	Acc: 39.7% (3970/10000)
[Test]  Epoch: 13	Loss: 0.029542	Acc: 40.9% (4086/10000)
[Test]  Epoch: 14	Loss: 0.032515	Acc: 35.8% (3576/10000)
[Test]  Epoch: 15	Loss: 0.032179	Acc: 40.1% (4011/10000)
[Test]  Epoch: 16	Loss: 0.029606	Acc: 42.9% (4288/10000)
[Test]  Epoch: 17	Loss: 0.029489	Acc: 42.7% (4272/10000)
[Test]  Epoch: 18	Loss: 0.030369	Acc: 41.4% (4136/10000)
[Test]  Epoch: 19	Loss: 0.030844	Acc: 41.8% (4175/10000)
[Test]  Epoch: 20	Loss: 0.030773	Acc: 43.1% (4307/10000)
[Test]  Epoch: 21	Loss: 0.031025	Acc: 41.2% (4123/10000)
[Test]  Epoch: 22	Loss: 0.031801	Acc: 42.4% (4244/10000)
[Test]  Epoch: 23	Loss: 0.031351	Acc: 43.2% (4319/10000)
[Test]  Epoch: 24	Loss: 0.031426	Acc: 42.9% (4288/10000)
[Test]  Epoch: 25	Loss: 0.033818	Acc: 42.1% (4210/10000)
[Test]  Epoch: 26	Loss: 0.033970	Acc: 42.5% (4246/10000)
[Test]  Epoch: 27	Loss: 0.033112	Acc: 43.1% (4306/10000)
[Test]  Epoch: 28	Loss: 0.032913	Acc: 43.3% (4326/10000)
[Test]  Epoch: 29	Loss: 0.032385	Acc: 43.8% (4383/10000)
[Test]  Epoch: 30	Loss: 0.033585	Acc: 44.0% (4398/10000)
[Test]  Epoch: 31	Loss: 0.032810	Acc: 44.7% (4474/10000)
[Test]  Epoch: 32	Loss: 0.033925	Acc: 44.4% (4439/10000)
[Test]  Epoch: 33	Loss: 0.033759	Acc: 43.2% (4322/10000)
[Test]  Epoch: 34	Loss: 0.033160	Acc: 44.2% (4422/10000)
[Test]  Epoch: 35	Loss: 0.034218	Acc: 45.0% (4503/10000)
[Test]  Epoch: 36	Loss: 0.035221	Acc: 43.4% (4337/10000)
[Test]  Epoch: 37	Loss: 0.034510	Acc: 44.6% (4456/10000)
[Test]  Epoch: 38	Loss: 0.035216	Acc: 45.6% (4557/10000)
[Test]  Epoch: 39	Loss: 0.033867	Acc: 45.2% (4523/10000)
[Test]  Epoch: 40	Loss: 0.035588	Acc: 44.0% (4396/10000)
[Test]  Epoch: 41	Loss: 0.035815	Acc: 44.6% (4460/10000)
[Test]  Epoch: 42	Loss: 0.037471	Acc: 43.7% (4371/10000)
[Test]  Epoch: 43	Loss: 0.034555	Acc: 45.1% (4515/10000)
[Test]  Epoch: 44	Loss: 0.035802	Acc: 44.7% (4467/10000)
[Test]  Epoch: 45	Loss: 0.035483	Acc: 44.6% (4459/10000)
[Test]  Epoch: 46	Loss: 0.034681	Acc: 45.2% (4522/10000)
[Test]  Epoch: 47	Loss: 0.033912	Acc: 46.7% (4670/10000)
[Test]  Epoch: 48	Loss: 0.033876	Acc: 46.7% (4669/10000)
[Test]  Epoch: 49	Loss: 0.035226	Acc: 45.2% (4518/10000)
[Test]  Epoch: 50	Loss: 0.037507	Acc: 44.2% (4425/10000)
[Test]  Epoch: 51	Loss: 0.035213	Acc: 46.2% (4623/10000)
[Test]  Epoch: 52	Loss: 0.034184	Acc: 47.0% (4699/10000)
[Test]  Epoch: 53	Loss: 0.035734	Acc: 45.5% (4545/10000)
[Test]  Epoch: 54	Loss: 0.033970	Acc: 46.9% (4685/10000)
[Test]  Epoch: 55	Loss: 0.033873	Acc: 47.0% (4700/10000)
[Test]  Epoch: 56	Loss: 0.034760	Acc: 47.4% (4744/10000)
[Test]  Epoch: 57	Loss: 0.034556	Acc: 47.6% (4756/10000)
[Test]  Epoch: 58	Loss: 0.036033	Acc: 45.4% (4539/10000)
[Test]  Epoch: 59	Loss: 0.034712	Acc: 47.5% (4750/10000)
[Test]  Epoch: 60	Loss: 0.036412	Acc: 45.3% (4528/10000)
[Test]  Epoch: 61	Loss: 0.033875	Acc: 46.8% (4680/10000)
[Test]  Epoch: 62	Loss: 0.033863	Acc: 47.6% (4757/10000)
[Test]  Epoch: 63	Loss: 0.033906	Acc: 48.0% (4804/10000)
[Test]  Epoch: 64	Loss: 0.034105	Acc: 47.6% (4758/10000)
[Test]  Epoch: 65	Loss: 0.033369	Acc: 47.8% (4780/10000)
[Test]  Epoch: 66	Loss: 0.033517	Acc: 47.5% (4746/10000)
[Test]  Epoch: 67	Loss: 0.033974	Acc: 47.3% (4733/10000)
[Test]  Epoch: 68	Loss: 0.033622	Acc: 47.5% (4755/10000)
[Test]  Epoch: 69	Loss: 0.033677	Acc: 47.2% (4720/10000)
[Test]  Epoch: 70	Loss: 0.033431	Acc: 47.5% (4745/10000)
[Test]  Epoch: 71	Loss: 0.033303	Acc: 48.2% (4818/10000)
[Test]  Epoch: 72	Loss: 0.033543	Acc: 48.1% (4814/10000)
[Test]  Epoch: 73	Loss: 0.033619	Acc: 47.5% (4755/10000)
[Test]  Epoch: 74	Loss: 0.033804	Acc: 47.9% (4787/10000)
[Test]  Epoch: 75	Loss: 0.033428	Acc: 47.4% (4742/10000)
[Test]  Epoch: 76	Loss: 0.033510	Acc: 47.7% (4770/10000)
[Test]  Epoch: 77	Loss: 0.033442	Acc: 47.9% (4788/10000)
[Test]  Epoch: 78	Loss: 0.033568	Acc: 47.9% (4787/10000)
[Test]  Epoch: 79	Loss: 0.033309	Acc: 47.9% (4787/10000)
[Test]  Epoch: 80	Loss: 0.033443	Acc: 48.2% (4822/10000)
[Test]  Epoch: 81	Loss: 0.033237	Acc: 48.6% (4861/10000)
[Test]  Epoch: 82	Loss: 0.033843	Acc: 47.7% (4774/10000)
[Test]  Epoch: 83	Loss: 0.033445	Acc: 48.3% (4829/10000)
[Test]  Epoch: 84	Loss: 0.033464	Acc: 47.8% (4781/10000)
[Test]  Epoch: 85	Loss: 0.033706	Acc: 47.5% (4748/10000)
[Test]  Epoch: 86	Loss: 0.033769	Acc: 47.8% (4778/10000)
[Test]  Epoch: 87	Loss: 0.033567	Acc: 48.2% (4818/10000)
[Test]  Epoch: 88	Loss: 0.033550	Acc: 48.1% (4806/10000)
[Test]  Epoch: 89	Loss: 0.033574	Acc: 47.8% (4783/10000)
[Test]  Epoch: 90	Loss: 0.033618	Acc: 47.8% (4776/10000)
[Test]  Epoch: 91	Loss: 0.033500	Acc: 48.0% (4804/10000)
[Test]  Epoch: 92	Loss: 0.033748	Acc: 48.3% (4830/10000)
[Test]  Epoch: 93	Loss: 0.033411	Acc: 48.0% (4800/10000)
[Test]  Epoch: 94	Loss: 0.033238	Acc: 48.2% (4818/10000)
[Test]  Epoch: 95	Loss: 0.032945	Acc: 48.3% (4829/10000)
[Test]  Epoch: 96	Loss: 0.033448	Acc: 48.1% (4809/10000)
[Test]  Epoch: 97	Loss: 0.033432	Acc: 48.1% (4806/10000)
[Test]  Epoch: 98	Loss: 0.033504	Acc: 48.0% (4799/10000)
[Test]  Epoch: 99	Loss: 0.033431	Acc: 48.2% (4822/10000)
[Test]  Epoch: 100	Loss: 0.033158	Acc: 48.4% (4835/10000)
===========finish==========
['2024-08-19', '06:02:34.915156', '100', 'test', '0.03315774083137512', '48.35', '48.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.059985	Acc: 18.9% (1886/10000)
[Test]  Epoch: 2	Loss: 0.030917	Acc: 33.0% (3297/10000)
[Test]  Epoch: 3	Loss: 0.028531	Acc: 35.2% (3524/10000)
[Test]  Epoch: 4	Loss: 0.028333	Acc: 35.8% (3581/10000)
[Test]  Epoch: 5	Loss: 0.029043	Acc: 35.1% (3513/10000)
[Test]  Epoch: 6	Loss: 0.031989	Acc: 35.4% (3542/10000)
[Test]  Epoch: 7	Loss: 0.030781	Acc: 34.7% (3474/10000)
[Test]  Epoch: 8	Loss: 0.028596	Acc: 39.0% (3903/10000)
[Test]  Epoch: 9	Loss: 0.029882	Acc: 37.5% (3748/10000)
[Test]  Epoch: 10	Loss: 0.032560	Acc: 36.9% (3688/10000)
[Test]  Epoch: 11	Loss: 0.031097	Acc: 38.7% (3871/10000)
[Test]  Epoch: 12	Loss: 0.032423	Acc: 38.4% (3842/10000)
[Test]  Epoch: 13	Loss: 0.028931	Acc: 41.8% (4180/10000)
[Test]  Epoch: 14	Loss: 0.033016	Acc: 37.2% (3725/10000)
[Test]  Epoch: 15	Loss: 0.035139	Acc: 40.2% (4020/10000)
[Test]  Epoch: 16	Loss: 0.032383	Acc: 39.1% (3908/10000)
[Test]  Epoch: 17	Loss: 0.029843	Acc: 42.5% (4249/10000)
[Test]  Epoch: 18	Loss: 0.034865	Acc: 39.0% (3902/10000)
[Test]  Epoch: 19	Loss: 0.031081	Acc: 41.6% (4159/10000)
[Test]  Epoch: 20	Loss: 0.031588	Acc: 42.5% (4251/10000)
[Test]  Epoch: 21	Loss: 0.034517	Acc: 39.4% (3935/10000)
[Test]  Epoch: 22	Loss: 0.030457	Acc: 43.7% (4374/10000)
[Test]  Epoch: 23	Loss: 0.032083	Acc: 44.2% (4425/10000)
[Test]  Epoch: 24	Loss: 0.032985	Acc: 41.5% (4145/10000)
[Test]  Epoch: 25	Loss: 0.033079	Acc: 43.5% (4345/10000)
[Test]  Epoch: 26	Loss: 0.033001	Acc: 43.0% (4296/10000)
[Test]  Epoch: 27	Loss: 0.033865	Acc: 43.2% (4324/10000)
[Test]  Epoch: 28	Loss: 0.033481	Acc: 43.8% (4380/10000)
[Test]  Epoch: 29	Loss: 0.033429	Acc: 42.9% (4287/10000)
[Test]  Epoch: 30	Loss: 0.031716	Acc: 45.6% (4557/10000)
[Test]  Epoch: 31	Loss: 0.033771	Acc: 42.7% (4267/10000)
[Test]  Epoch: 32	Loss: 0.032767	Acc: 45.3% (4529/10000)
[Test]  Epoch: 33	Loss: 0.034370	Acc: 44.1% (4415/10000)
[Test]  Epoch: 34	Loss: 0.031498	Acc: 46.8% (4680/10000)
[Test]  Epoch: 35	Loss: 0.035739	Acc: 43.5% (4347/10000)
[Test]  Epoch: 36	Loss: 0.035162	Acc: 43.5% (4355/10000)
[Test]  Epoch: 37	Loss: 0.033029	Acc: 47.0% (4699/10000)
[Test]  Epoch: 38	Loss: 0.033439	Acc: 45.0% (4498/10000)
[Test]  Epoch: 39	Loss: 0.035160	Acc: 43.6% (4357/10000)
[Test]  Epoch: 40	Loss: 0.034902	Acc: 43.7% (4373/10000)
[Test]  Epoch: 41	Loss: 0.033449	Acc: 46.4% (4635/10000)
[Test]  Epoch: 42	Loss: 0.036684	Acc: 44.2% (4417/10000)
[Test]  Epoch: 43	Loss: 0.035475	Acc: 45.0% (4505/10000)
[Test]  Epoch: 44	Loss: 0.033083	Acc: 47.9% (4791/10000)
[Test]  Epoch: 45	Loss: 0.034344	Acc: 45.3% (4529/10000)
[Test]  Epoch: 46	Loss: 0.035433	Acc: 44.8% (4480/10000)
[Test]  Epoch: 47	Loss: 0.033896	Acc: 45.6% (4560/10000)
[Test]  Epoch: 48	Loss: 0.034530	Acc: 46.2% (4624/10000)
[Test]  Epoch: 49	Loss: 0.035037	Acc: 47.0% (4703/10000)
[Test]  Epoch: 50	Loss: 0.035797	Acc: 45.7% (4569/10000)
[Test]  Epoch: 51	Loss: 0.034045	Acc: 46.6% (4663/10000)
[Test]  Epoch: 52	Loss: 0.032951	Acc: 48.3% (4828/10000)
[Test]  Epoch: 53	Loss: 0.033240	Acc: 47.4% (4742/10000)
[Test]  Epoch: 54	Loss: 0.034238	Acc: 46.8% (4678/10000)
[Test]  Epoch: 55	Loss: 0.034372	Acc: 48.1% (4806/10000)
[Test]  Epoch: 56	Loss: 0.035967	Acc: 46.1% (4611/10000)
[Test]  Epoch: 57	Loss: 0.034260	Acc: 47.5% (4752/10000)
[Test]  Epoch: 58	Loss: 0.034556	Acc: 47.0% (4699/10000)
[Test]  Epoch: 59	Loss: 0.035052	Acc: 45.8% (4582/10000)
[Test]  Epoch: 60	Loss: 0.034913	Acc: 47.7% (4774/10000)
[Test]  Epoch: 61	Loss: 0.033565	Acc: 47.8% (4783/10000)
[Test]  Epoch: 62	Loss: 0.033144	Acc: 48.6% (4862/10000)
[Test]  Epoch: 63	Loss: 0.033078	Acc: 48.5% (4846/10000)
[Test]  Epoch: 64	Loss: 0.032974	Acc: 48.8% (4878/10000)
[Test]  Epoch: 65	Loss: 0.033062	Acc: 48.3% (4832/10000)
[Test]  Epoch: 66	Loss: 0.032776	Acc: 48.3% (4828/10000)
[Test]  Epoch: 67	Loss: 0.032965	Acc: 48.8% (4879/10000)
[Test]  Epoch: 68	Loss: 0.032870	Acc: 48.3% (4832/10000)
[Test]  Epoch: 69	Loss: 0.032996	Acc: 49.0% (4902/10000)
[Test]  Epoch: 70	Loss: 0.032551	Acc: 48.2% (4819/10000)
[Test]  Epoch: 71	Loss: 0.032824	Acc: 48.3% (4828/10000)
[Test]  Epoch: 72	Loss: 0.032669	Acc: 48.8% (4876/10000)
[Test]  Epoch: 73	Loss: 0.032731	Acc: 48.4% (4838/10000)
[Test]  Epoch: 74	Loss: 0.033020	Acc: 48.7% (4871/10000)
[Test]  Epoch: 75	Loss: 0.032694	Acc: 48.6% (4859/10000)
[Test]  Epoch: 76	Loss: 0.032965	Acc: 48.6% (4864/10000)
[Test]  Epoch: 77	Loss: 0.032769	Acc: 48.6% (4864/10000)
[Test]  Epoch: 78	Loss: 0.032640	Acc: 48.6% (4859/10000)
[Test]  Epoch: 79	Loss: 0.032691	Acc: 49.1% (4906/10000)
[Test]  Epoch: 80	Loss: 0.032951	Acc: 48.4% (4843/10000)
[Test]  Epoch: 81	Loss: 0.032797	Acc: 48.7% (4866/10000)
[Test]  Epoch: 82	Loss: 0.032873	Acc: 48.4% (4837/10000)
[Test]  Epoch: 83	Loss: 0.032606	Acc: 49.2% (4923/10000)
[Test]  Epoch: 84	Loss: 0.032772	Acc: 48.8% (4882/10000)
[Test]  Epoch: 85	Loss: 0.032635	Acc: 48.7% (4872/10000)
[Test]  Epoch: 86	Loss: 0.032306	Acc: 49.6% (4963/10000)
[Test]  Epoch: 87	Loss: 0.032433	Acc: 48.7% (4866/10000)
[Test]  Epoch: 88	Loss: 0.032396	Acc: 48.7% (4868/10000)
[Test]  Epoch: 89	Loss: 0.032490	Acc: 49.0% (4900/10000)
[Test]  Epoch: 90	Loss: 0.032421	Acc: 49.1% (4907/10000)
[Test]  Epoch: 91	Loss: 0.032315	Acc: 49.4% (4942/10000)
[Test]  Epoch: 92	Loss: 0.032511	Acc: 48.7% (4874/10000)
[Test]  Epoch: 93	Loss: 0.032075	Acc: 49.0% (4905/10000)
[Test]  Epoch: 94	Loss: 0.032360	Acc: 49.3% (4927/10000)
[Test]  Epoch: 95	Loss: 0.032556	Acc: 48.9% (4890/10000)
[Test]  Epoch: 96	Loss: 0.032673	Acc: 49.0% (4901/10000)
[Test]  Epoch: 97	Loss: 0.032155	Acc: 49.4% (4935/10000)
[Test]  Epoch: 98	Loss: 0.032585	Acc: 48.5% (4850/10000)
[Test]  Epoch: 99	Loss: 0.032953	Acc: 49.0% (4905/10000)
[Test]  Epoch: 100	Loss: 0.032461	Acc: 49.1% (4915/10000)
===========finish==========
['2024-08-19', '06:05:48.781933', '100', 'test', '0.03246131005287171', '49.15', '49.63']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.134495	Acc: 13.8% (1377/10000)
[Test]  Epoch: 2	Loss: 0.035716	Acc: 33.3% (3327/10000)
[Test]  Epoch: 3	Loss: 0.030318	Acc: 34.1% (3406/10000)
[Test]  Epoch: 4	Loss: 0.029317	Acc: 35.8% (3576/10000)
[Test]  Epoch: 5	Loss: 0.028673	Acc: 35.4% (3544/10000)
[Test]  Epoch: 6	Loss: 0.033386	Acc: 34.5% (3447/10000)
[Test]  Epoch: 7	Loss: 0.028025	Acc: 37.9% (3788/10000)
[Test]  Epoch: 8	Loss: 0.030617	Acc: 35.5% (3548/10000)
[Test]  Epoch: 9	Loss: 0.026632	Acc: 40.1% (4008/10000)
[Test]  Epoch: 10	Loss: 0.033888	Acc: 35.6% (3565/10000)
[Test]  Epoch: 11	Loss: 0.029914	Acc: 37.7% (3770/10000)
[Test]  Epoch: 12	Loss: 0.030097	Acc: 40.1% (4006/10000)
[Test]  Epoch: 13	Loss: 0.031269	Acc: 38.6% (3864/10000)
[Test]  Epoch: 14	Loss: 0.035083	Acc: 34.4% (3442/10000)
[Test]  Epoch: 15	Loss: 0.032692	Acc: 38.9% (3894/10000)
[Test]  Epoch: 16	Loss: 0.032031	Acc: 39.2% (3922/10000)
[Test]  Epoch: 17	Loss: 0.032056	Acc: 40.4% (4041/10000)
[Test]  Epoch: 18	Loss: 0.031143	Acc: 40.9% (4094/10000)
[Test]  Epoch: 19	Loss: 0.031132	Acc: 41.4% (4139/10000)
[Test]  Epoch: 20	Loss: 0.031060	Acc: 41.7% (4167/10000)
[Test]  Epoch: 21	Loss: 0.031043	Acc: 41.5% (4146/10000)
[Test]  Epoch: 22	Loss: 0.034068	Acc: 41.6% (4156/10000)
[Test]  Epoch: 23	Loss: 0.031136	Acc: 43.8% (4379/10000)
[Test]  Epoch: 24	Loss: 0.033844	Acc: 40.4% (4038/10000)
[Test]  Epoch: 25	Loss: 0.035560	Acc: 40.5% (4046/10000)
[Test]  Epoch: 26	Loss: 0.032935	Acc: 42.0% (4199/10000)
[Test]  Epoch: 27	Loss: 0.034625	Acc: 42.4% (4240/10000)
[Test]  Epoch: 28	Loss: 0.034231	Acc: 42.5% (4253/10000)
[Test]  Epoch: 29	Loss: 0.032677	Acc: 42.1% (4214/10000)
[Test]  Epoch: 30	Loss: 0.031489	Acc: 44.6% (4458/10000)
[Test]  Epoch: 31	Loss: 0.033375	Acc: 42.8% (4279/10000)
[Test]  Epoch: 32	Loss: 0.033889	Acc: 43.8% (4375/10000)
[Test]  Epoch: 33	Loss: 0.033560	Acc: 44.0% (4396/10000)
[Test]  Epoch: 34	Loss: 0.033750	Acc: 43.5% (4352/10000)
[Test]  Epoch: 35	Loss: 0.034868	Acc: 44.7% (4474/10000)
[Test]  Epoch: 36	Loss: 0.033615	Acc: 44.6% (4459/10000)
[Test]  Epoch: 37	Loss: 0.037657	Acc: 42.0% (4200/10000)
[Test]  Epoch: 38	Loss: 0.035584	Acc: 44.1% (4407/10000)
[Test]  Epoch: 39	Loss: 0.033765	Acc: 46.0% (4595/10000)
[Test]  Epoch: 40	Loss: 0.034315	Acc: 44.3% (4428/10000)
[Test]  Epoch: 41	Loss: 0.034583	Acc: 44.8% (4482/10000)
[Test]  Epoch: 42	Loss: 0.035574	Acc: 44.1% (4415/10000)
[Test]  Epoch: 43	Loss: 0.036337	Acc: 44.5% (4452/10000)
[Test]  Epoch: 44	Loss: 0.034037	Acc: 46.1% (4615/10000)
[Test]  Epoch: 45	Loss: 0.033089	Acc: 47.2% (4721/10000)
[Test]  Epoch: 46	Loss: 0.037520	Acc: 44.5% (4449/10000)
[Test]  Epoch: 47	Loss: 0.032871	Acc: 47.0% (4702/10000)
[Test]  Epoch: 48	Loss: 0.033457	Acc: 47.7% (4766/10000)
[Test]  Epoch: 49	Loss: 0.037855	Acc: 43.9% (4391/10000)
[Test]  Epoch: 50	Loss: 0.036283	Acc: 45.5% (4548/10000)
[Test]  Epoch: 51	Loss: 0.034425	Acc: 46.3% (4634/10000)
[Test]  Epoch: 52	Loss: 0.032625	Acc: 48.4% (4843/10000)
[Test]  Epoch: 53	Loss: 0.034856	Acc: 45.9% (4594/10000)
[Test]  Epoch: 54	Loss: 0.034648	Acc: 45.3% (4534/10000)
[Test]  Epoch: 55	Loss: 0.034620	Acc: 47.4% (4738/10000)
[Test]  Epoch: 56	Loss: 0.035088	Acc: 47.5% (4746/10000)
[Test]  Epoch: 57	Loss: 0.036001	Acc: 45.5% (4546/10000)
[Test]  Epoch: 58	Loss: 0.035921	Acc: 46.4% (4635/10000)
[Test]  Epoch: 59	Loss: 0.034453	Acc: 47.1% (4709/10000)
[Test]  Epoch: 60	Loss: 0.034711	Acc: 46.9% (4688/10000)
[Test]  Epoch: 61	Loss: 0.033488	Acc: 48.3% (4827/10000)
[Test]  Epoch: 62	Loss: 0.032897	Acc: 48.2% (4825/10000)
[Test]  Epoch: 63	Loss: 0.033308	Acc: 48.5% (4851/10000)
[Test]  Epoch: 64	Loss: 0.032778	Acc: 48.5% (4849/10000)
[Test]  Epoch: 65	Loss: 0.032485	Acc: 48.6% (4863/10000)
[Test]  Epoch: 66	Loss: 0.032614	Acc: 48.9% (4888/10000)
[Test]  Epoch: 67	Loss: 0.032718	Acc: 48.6% (4856/10000)
[Test]  Epoch: 68	Loss: 0.032558	Acc: 49.1% (4908/10000)
[Test]  Epoch: 69	Loss: 0.032476	Acc: 49.2% (4919/10000)
[Test]  Epoch: 70	Loss: 0.032803	Acc: 48.9% (4889/10000)
[Test]  Epoch: 71	Loss: 0.032430	Acc: 48.6% (4863/10000)
[Test]  Epoch: 72	Loss: 0.032660	Acc: 48.3% (4830/10000)
[Test]  Epoch: 73	Loss: 0.032438	Acc: 49.0% (4899/10000)
[Test]  Epoch: 74	Loss: 0.032787	Acc: 48.8% (4880/10000)
[Test]  Epoch: 75	Loss: 0.032428	Acc: 48.8% (4882/10000)
[Test]  Epoch: 76	Loss: 0.032450	Acc: 49.5% (4948/10000)
[Test]  Epoch: 77	Loss: 0.032498	Acc: 48.8% (4883/10000)
[Test]  Epoch: 78	Loss: 0.032295	Acc: 48.9% (4893/10000)
[Test]  Epoch: 79	Loss: 0.032833	Acc: 48.7% (4866/10000)
[Test]  Epoch: 80	Loss: 0.032061	Acc: 49.3% (4930/10000)
[Test]  Epoch: 81	Loss: 0.032545	Acc: 49.2% (4917/10000)
[Test]  Epoch: 82	Loss: 0.032761	Acc: 48.4% (4835/10000)
[Test]  Epoch: 83	Loss: 0.032565	Acc: 49.2% (4924/10000)
[Test]  Epoch: 84	Loss: 0.032660	Acc: 48.9% (4887/10000)
[Test]  Epoch: 85	Loss: 0.032692	Acc: 48.6% (4865/10000)
[Test]  Epoch: 86	Loss: 0.032296	Acc: 49.2% (4917/10000)
[Test]  Epoch: 87	Loss: 0.032463	Acc: 49.1% (4913/10000)
[Test]  Epoch: 88	Loss: 0.032455	Acc: 48.6% (4864/10000)
[Test]  Epoch: 89	Loss: 0.032622	Acc: 48.7% (4871/10000)
[Test]  Epoch: 90	Loss: 0.032492	Acc: 49.0% (4895/10000)
[Test]  Epoch: 91	Loss: 0.032270	Acc: 49.4% (4939/10000)
[Test]  Epoch: 92	Loss: 0.032302	Acc: 49.0% (4898/10000)
[Test]  Epoch: 93	Loss: 0.032877	Acc: 49.1% (4910/10000)
[Test]  Epoch: 94	Loss: 0.032566	Acc: 49.2% (4919/10000)
[Test]  Epoch: 95	Loss: 0.032452	Acc: 49.0% (4903/10000)
[Test]  Epoch: 96	Loss: 0.032719	Acc: 48.9% (4886/10000)
[Test]  Epoch: 97	Loss: 0.032677	Acc: 48.5% (4846/10000)
[Test]  Epoch: 98	Loss: 0.032841	Acc: 49.0% (4902/10000)
[Test]  Epoch: 99	Loss: 0.032565	Acc: 49.4% (4936/10000)
[Test]  Epoch: 100	Loss: 0.032434	Acc: 49.2% (4917/10000)
===========finish==========
['2024-08-19', '06:08:51.280744', '100', 'test', '0.03243380439281464', '49.17', '49.48']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.122823	Acc: 10.4% (1037/10000)
[Test]  Epoch: 2	Loss: 0.032760	Acc: 27.1% (2707/10000)
[Test]  Epoch: 3	Loss: 0.028634	Acc: 33.1% (3310/10000)
[Test]  Epoch: 4	Loss: 0.029139	Acc: 33.0% (3299/10000)
[Test]  Epoch: 5	Loss: 0.028521	Acc: 35.5% (3554/10000)
[Test]  Epoch: 6	Loss: 0.029642	Acc: 35.3% (3526/10000)
[Test]  Epoch: 7	Loss: 0.027855	Acc: 36.5% (3651/10000)
[Test]  Epoch: 8	Loss: 0.027897	Acc: 37.0% (3702/10000)
[Test]  Epoch: 9	Loss: 0.026974	Acc: 38.0% (3805/10000)
[Test]  Epoch: 10	Loss: 0.029930	Acc: 36.0% (3599/10000)
[Test]  Epoch: 11	Loss: 0.032378	Acc: 35.2% (3522/10000)
[Test]  Epoch: 12	Loss: 0.027547	Acc: 40.0% (4004/10000)
[Test]  Epoch: 13	Loss: 0.031053	Acc: 38.1% (3812/10000)
[Test]  Epoch: 14	Loss: 0.030414	Acc: 37.0% (3704/10000)
[Test]  Epoch: 15	Loss: 0.030584	Acc: 39.6% (3957/10000)
[Test]  Epoch: 16	Loss: 0.031715	Acc: 37.4% (3735/10000)
[Test]  Epoch: 17	Loss: 0.029930	Acc: 39.2% (3916/10000)
[Test]  Epoch: 18	Loss: 0.035069	Acc: 36.8% (3676/10000)
[Test]  Epoch: 19	Loss: 0.030244	Acc: 41.0% (4095/10000)
[Test]  Epoch: 20	Loss: 0.032745	Acc: 39.5% (3951/10000)
[Test]  Epoch: 21	Loss: 0.033285	Acc: 38.0% (3801/10000)
[Test]  Epoch: 22	Loss: 0.032258	Acc: 40.4% (4036/10000)
[Test]  Epoch: 23	Loss: 0.030001	Acc: 42.8% (4282/10000)
[Test]  Epoch: 24	Loss: 0.032876	Acc: 38.9% (3891/10000)
[Test]  Epoch: 25	Loss: 0.033877	Acc: 40.7% (4071/10000)
[Test]  Epoch: 26	Loss: 0.031380	Acc: 41.9% (4188/10000)
[Test]  Epoch: 27	Loss: 0.033788	Acc: 39.7% (3968/10000)
[Test]  Epoch: 28	Loss: 0.035113	Acc: 41.1% (4115/10000)
[Test]  Epoch: 29	Loss: 0.035719	Acc: 38.7% (3867/10000)
[Test]  Epoch: 30	Loss: 0.035345	Acc: 41.9% (4188/10000)
[Test]  Epoch: 31	Loss: 0.034274	Acc: 41.9% (4185/10000)
[Test]  Epoch: 32	Loss: 0.034208	Acc: 42.2% (4222/10000)
[Test]  Epoch: 33	Loss: 0.036310	Acc: 41.9% (4191/10000)
[Test]  Epoch: 34	Loss: 0.031709	Acc: 43.5% (4346/10000)
[Test]  Epoch: 35	Loss: 0.034582	Acc: 42.8% (4283/10000)
[Test]  Epoch: 36	Loss: 0.039944	Acc: 37.4% (3740/10000)
[Test]  Epoch: 37	Loss: 0.034922	Acc: 45.3% (4532/10000)
[Test]  Epoch: 38	Loss: 0.035374	Acc: 43.2% (4323/10000)
[Test]  Epoch: 39	Loss: 0.034913	Acc: 43.2% (4322/10000)
[Test]  Epoch: 40	Loss: 0.033546	Acc: 44.2% (4423/10000)
[Test]  Epoch: 41	Loss: 0.034654	Acc: 43.5% (4351/10000)
[Test]  Epoch: 42	Loss: 0.035429	Acc: 43.9% (4391/10000)
[Test]  Epoch: 43	Loss: 0.035620	Acc: 43.8% (4376/10000)
[Test]  Epoch: 44	Loss: 0.033356	Acc: 46.1% (4613/10000)
[Test]  Epoch: 45	Loss: 0.036271	Acc: 42.6% (4258/10000)
[Test]  Epoch: 46	Loss: 0.035488	Acc: 43.5% (4346/10000)
[Test]  Epoch: 47	Loss: 0.033884	Acc: 46.1% (4609/10000)
[Test]  Epoch: 48	Loss: 0.035494	Acc: 45.1% (4512/10000)
[Test]  Epoch: 49	Loss: 0.034430	Acc: 46.4% (4636/10000)
[Test]  Epoch: 50	Loss: 0.036845	Acc: 44.3% (4431/10000)
[Test]  Epoch: 51	Loss: 0.034941	Acc: 45.6% (4565/10000)
[Test]  Epoch: 52	Loss: 0.033344	Acc: 46.8% (4675/10000)
[Test]  Epoch: 53	Loss: 0.035508	Acc: 44.4% (4440/10000)
[Test]  Epoch: 54	Loss: 0.035704	Acc: 45.5% (4550/10000)
[Test]  Epoch: 55	Loss: 0.035389	Acc: 45.2% (4519/10000)
[Test]  Epoch: 56	Loss: 0.036485	Acc: 44.8% (4484/10000)
[Test]  Epoch: 57	Loss: 0.035101	Acc: 45.4% (4544/10000)
[Test]  Epoch: 58	Loss: 0.036933	Acc: 45.3% (4534/10000)
[Test]  Epoch: 59	Loss: 0.035034	Acc: 45.2% (4523/10000)
[Test]  Epoch: 60	Loss: 0.035255	Acc: 45.2% (4522/10000)
[Test]  Epoch: 61	Loss: 0.034036	Acc: 47.5% (4752/10000)
[Test]  Epoch: 62	Loss: 0.033692	Acc: 47.0% (4695/10000)
[Test]  Epoch: 63	Loss: 0.033915	Acc: 47.1% (4715/10000)
[Test]  Epoch: 64	Loss: 0.033755	Acc: 47.0% (4699/10000)
[Test]  Epoch: 65	Loss: 0.033265	Acc: 47.7% (4769/10000)
[Test]  Epoch: 66	Loss: 0.033808	Acc: 47.0% (4704/10000)
[Test]  Epoch: 67	Loss: 0.033562	Acc: 47.6% (4764/10000)
[Test]  Epoch: 68	Loss: 0.033688	Acc: 47.8% (4775/10000)
[Test]  Epoch: 69	Loss: 0.033378	Acc: 47.4% (4741/10000)
[Test]  Epoch: 70	Loss: 0.033640	Acc: 46.9% (4689/10000)
[Test]  Epoch: 71	Loss: 0.032990	Acc: 47.9% (4790/10000)
[Test]  Epoch: 72	Loss: 0.033353	Acc: 47.6% (4764/10000)
[Test]  Epoch: 73	Loss: 0.033545	Acc: 47.1% (4711/10000)
[Test]  Epoch: 74	Loss: 0.033698	Acc: 47.1% (4709/10000)
[Test]  Epoch: 75	Loss: 0.033402	Acc: 47.6% (4758/10000)
[Test]  Epoch: 76	Loss: 0.033754	Acc: 47.4% (4741/10000)
[Test]  Epoch: 77	Loss: 0.033475	Acc: 47.1% (4706/10000)
[Test]  Epoch: 78	Loss: 0.033444	Acc: 47.6% (4758/10000)
[Test]  Epoch: 79	Loss: 0.033391	Acc: 47.8% (4776/10000)
[Test]  Epoch: 80	Loss: 0.032987	Acc: 48.3% (4831/10000)
[Test]  Epoch: 81	Loss: 0.033558	Acc: 47.9% (4789/10000)
[Test]  Epoch: 82	Loss: 0.033657	Acc: 47.4% (4741/10000)
[Test]  Epoch: 83	Loss: 0.033508	Acc: 48.4% (4835/10000)
[Test]  Epoch: 84	Loss: 0.033612	Acc: 47.7% (4769/10000)
[Test]  Epoch: 85	Loss: 0.033470	Acc: 47.4% (4742/10000)
[Test]  Epoch: 86	Loss: 0.033185	Acc: 47.4% (4741/10000)
[Test]  Epoch: 87	Loss: 0.032969	Acc: 47.9% (4787/10000)
[Test]  Epoch: 88	Loss: 0.033171	Acc: 48.4% (4841/10000)
[Test]  Epoch: 89	Loss: 0.033199	Acc: 48.0% (4799/10000)
[Test]  Epoch: 90	Loss: 0.033023	Acc: 47.2% (4725/10000)
[Test]  Epoch: 91	Loss: 0.033035	Acc: 48.4% (4841/10000)
[Test]  Epoch: 92	Loss: 0.033232	Acc: 48.0% (4805/10000)
[Test]  Epoch: 93	Loss: 0.033174	Acc: 48.5% (4851/10000)
[Test]  Epoch: 94	Loss: 0.033475	Acc: 48.0% (4796/10000)
[Test]  Epoch: 95	Loss: 0.032997	Acc: 48.1% (4813/10000)
[Test]  Epoch: 96	Loss: 0.033295	Acc: 47.7% (4770/10000)
[Test]  Epoch: 97	Loss: 0.033015	Acc: 48.0% (4799/10000)
[Test]  Epoch: 98	Loss: 0.033418	Acc: 47.9% (4794/10000)
[Test]  Epoch: 99	Loss: 0.033437	Acc: 48.1% (4810/10000)
[Test]  Epoch: 100	Loss: 0.033108	Acc: 48.2% (4820/10000)
===========finish==========
['2024-08-19', '06:11:56.727211', '100', 'test', '0.03310781759023666', '48.2', '48.51']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.071572	Acc: 14.8% (1482/10000)
[Test]  Epoch: 2	Loss: 0.032800	Acc: 27.4% (2743/10000)
[Test]  Epoch: 3	Loss: 0.029091	Acc: 32.6% (3260/10000)
[Test]  Epoch: 4	Loss: 0.029800	Acc: 30.9% (3089/10000)
[Test]  Epoch: 5	Loss: 0.027564	Acc: 35.7% (3572/10000)
[Test]  Epoch: 6	Loss: 0.027538	Acc: 36.2% (3621/10000)
[Test]  Epoch: 7	Loss: 0.027946	Acc: 35.8% (3581/10000)
[Test]  Epoch: 8	Loss: 0.030428	Acc: 33.1% (3309/10000)
[Test]  Epoch: 9	Loss: 0.027303	Acc: 37.5% (3747/10000)
[Test]  Epoch: 10	Loss: 0.030632	Acc: 34.0% (3395/10000)
[Test]  Epoch: 11	Loss: 0.032109	Acc: 35.7% (3567/10000)
[Test]  Epoch: 12	Loss: 0.028666	Acc: 38.3% (3827/10000)
[Test]  Epoch: 13	Loss: 0.027557	Acc: 38.9% (3889/10000)
[Test]  Epoch: 14	Loss: 0.028134	Acc: 37.8% (3782/10000)
[Test]  Epoch: 15	Loss: 0.029038	Acc: 40.4% (4035/10000)
[Test]  Epoch: 16	Loss: 0.027665	Acc: 40.2% (4020/10000)
[Test]  Epoch: 17	Loss: 0.028993	Acc: 39.8% (3978/10000)
[Test]  Epoch: 18	Loss: 0.029726	Acc: 39.3% (3930/10000)
[Test]  Epoch: 19	Loss: 0.032859	Acc: 37.1% (3715/10000)
[Test]  Epoch: 20	Loss: 0.028535	Acc: 41.8% (4178/10000)
[Test]  Epoch: 21	Loss: 0.028606	Acc: 43.2% (4318/10000)
[Test]  Epoch: 22	Loss: 0.030923	Acc: 40.2% (4021/10000)
[Test]  Epoch: 23	Loss: 0.039252	Acc: 36.4% (3641/10000)
[Test]  Epoch: 24	Loss: 0.030587	Acc: 39.7% (3972/10000)
[Test]  Epoch: 25	Loss: 0.032909	Acc: 40.5% (4048/10000)
[Test]  Epoch: 26	Loss: 0.031144	Acc: 40.5% (4051/10000)
[Test]  Epoch: 27	Loss: 0.032908	Acc: 40.3% (4028/10000)
[Test]  Epoch: 28	Loss: 0.033273	Acc: 40.5% (4045/10000)
[Test]  Epoch: 29	Loss: 0.032705	Acc: 39.9% (3989/10000)
[Test]  Epoch: 30	Loss: 0.033287	Acc: 42.0% (4199/10000)
[Test]  Epoch: 31	Loss: 0.031360	Acc: 44.0% (4399/10000)
[Test]  Epoch: 32	Loss: 0.031889	Acc: 43.7% (4367/10000)
[Test]  Epoch: 33	Loss: 0.034101	Acc: 41.3% (4132/10000)
[Test]  Epoch: 34	Loss: 0.032892	Acc: 41.5% (4145/10000)
[Test]  Epoch: 35	Loss: 0.036276	Acc: 41.2% (4122/10000)
[Test]  Epoch: 36	Loss: 0.032744	Acc: 42.4% (4239/10000)
[Test]  Epoch: 37	Loss: 0.037605	Acc: 38.4% (3839/10000)
[Test]  Epoch: 38	Loss: 0.032622	Acc: 43.8% (4379/10000)
[Test]  Epoch: 39	Loss: 0.034492	Acc: 43.5% (4347/10000)
[Test]  Epoch: 40	Loss: 0.032179	Acc: 45.0% (4504/10000)
[Test]  Epoch: 41	Loss: 0.034591	Acc: 43.5% (4355/10000)
[Test]  Epoch: 42	Loss: 0.034476	Acc: 43.8% (4379/10000)
[Test]  Epoch: 43	Loss: 0.036585	Acc: 42.1% (4207/10000)
[Test]  Epoch: 44	Loss: 0.034255	Acc: 43.7% (4367/10000)
[Test]  Epoch: 45	Loss: 0.032791	Acc: 45.2% (4518/10000)
[Test]  Epoch: 46	Loss: 0.032456	Acc: 45.9% (4585/10000)
[Test]  Epoch: 47	Loss: 0.033323	Acc: 44.5% (4455/10000)
[Test]  Epoch: 48	Loss: 0.034446	Acc: 45.3% (4526/10000)
[Test]  Epoch: 49	Loss: 0.034346	Acc: 44.3% (4431/10000)
[Test]  Epoch: 50	Loss: 0.035406	Acc: 44.0% (4398/10000)
[Test]  Epoch: 51	Loss: 0.035439	Acc: 42.9% (4294/10000)
[Test]  Epoch: 52	Loss: 0.032382	Acc: 46.9% (4694/10000)
[Test]  Epoch: 53	Loss: 0.035116	Acc: 43.5% (4349/10000)
[Test]  Epoch: 54	Loss: 0.033716	Acc: 45.4% (4540/10000)
[Test]  Epoch: 55	Loss: 0.033343	Acc: 46.4% (4635/10000)
[Test]  Epoch: 56	Loss: 0.033570	Acc: 46.0% (4595/10000)
[Test]  Epoch: 57	Loss: 0.035340	Acc: 44.9% (4487/10000)
[Test]  Epoch: 58	Loss: 0.035023	Acc: 45.7% (4571/10000)
[Test]  Epoch: 59	Loss: 0.034000	Acc: 46.5% (4651/10000)
[Test]  Epoch: 60	Loss: 0.033854	Acc: 46.7% (4670/10000)
[Test]  Epoch: 61	Loss: 0.033121	Acc: 46.6% (4662/10000)
[Test]  Epoch: 62	Loss: 0.032789	Acc: 46.5% (4645/10000)
[Test]  Epoch: 63	Loss: 0.032964	Acc: 46.8% (4683/10000)
[Test]  Epoch: 64	Loss: 0.032849	Acc: 46.9% (4686/10000)
[Test]  Epoch: 65	Loss: 0.032540	Acc: 47.7% (4771/10000)
[Test]  Epoch: 66	Loss: 0.032604	Acc: 47.5% (4751/10000)
[Test]  Epoch: 67	Loss: 0.032697	Acc: 47.2% (4722/10000)
[Test]  Epoch: 68	Loss: 0.032781	Acc: 47.9% (4794/10000)
[Test]  Epoch: 69	Loss: 0.032418	Acc: 47.3% (4729/10000)
[Test]  Epoch: 70	Loss: 0.032557	Acc: 47.5% (4747/10000)
[Test]  Epoch: 71	Loss: 0.032264	Acc: 47.4% (4736/10000)
[Test]  Epoch: 72	Loss: 0.032580	Acc: 47.4% (4737/10000)
[Test]  Epoch: 73	Loss: 0.032737	Acc: 47.9% (4786/10000)
[Test]  Epoch: 74	Loss: 0.032618	Acc: 47.7% (4769/10000)
[Test]  Epoch: 75	Loss: 0.032497	Acc: 47.1% (4710/10000)
[Test]  Epoch: 76	Loss: 0.032662	Acc: 47.3% (4726/10000)
[Test]  Epoch: 77	Loss: 0.032297	Acc: 47.9% (4790/10000)
[Test]  Epoch: 78	Loss: 0.032524	Acc: 47.3% (4726/10000)
[Test]  Epoch: 79	Loss: 0.032647	Acc: 47.9% (4791/10000)
[Test]  Epoch: 80	Loss: 0.032300	Acc: 47.7% (4766/10000)
[Test]  Epoch: 81	Loss: 0.032652	Acc: 47.5% (4755/10000)
[Test]  Epoch: 82	Loss: 0.032194	Acc: 48.2% (4823/10000)
[Test]  Epoch: 83	Loss: 0.032309	Acc: 47.9% (4793/10000)
[Test]  Epoch: 84	Loss: 0.032425	Acc: 47.6% (4761/10000)
[Test]  Epoch: 85	Loss: 0.032588	Acc: 47.5% (4754/10000)
[Test]  Epoch: 86	Loss: 0.032563	Acc: 47.6% (4762/10000)
[Test]  Epoch: 87	Loss: 0.032177	Acc: 48.0% (4801/10000)
[Test]  Epoch: 88	Loss: 0.032534	Acc: 47.7% (4768/10000)
[Test]  Epoch: 89	Loss: 0.032179	Acc: 47.4% (4743/10000)
[Test]  Epoch: 90	Loss: 0.032403	Acc: 47.4% (4740/10000)
[Test]  Epoch: 91	Loss: 0.032216	Acc: 48.1% (4806/10000)
[Test]  Epoch: 92	Loss: 0.032222	Acc: 48.2% (4817/10000)
[Test]  Epoch: 93	Loss: 0.032333	Acc: 47.6% (4761/10000)
[Test]  Epoch: 94	Loss: 0.032610	Acc: 48.0% (4804/10000)
[Test]  Epoch: 95	Loss: 0.032495	Acc: 47.6% (4757/10000)
[Test]  Epoch: 96	Loss: 0.032080	Acc: 48.5% (4854/10000)
[Test]  Epoch: 97	Loss: 0.032591	Acc: 47.6% (4757/10000)
[Test]  Epoch: 98	Loss: 0.032648	Acc: 47.8% (4775/10000)
[Test]  Epoch: 99	Loss: 0.032339	Acc: 47.9% (4792/10000)
[Test]  Epoch: 100	Loss: 0.032540	Acc: 48.0% (4802/10000)
===========finish==========
['2024-08-19', '06:15:09.808000', '100', 'test', '0.03253975404500961', '48.02', '48.54']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight', 'features.40.weight', 'features.41.weight', 'classifier.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.053150	Acc: 10.9% (1094/10000)
[Test]  Epoch: 2	Loss: 0.032566	Acc: 20.1% (2006/10000)
[Test]  Epoch: 3	Loss: 0.031137	Acc: 26.6% (2656/10000)
[Test]  Epoch: 4	Loss: 0.030804	Acc: 26.0% (2604/10000)
[Test]  Epoch: 5	Loss: 0.029038	Acc: 30.1% (3008/10000)
[Test]  Epoch: 6	Loss: 0.030659	Acc: 28.1% (2815/10000)
[Test]  Epoch: 7	Loss: 0.029724	Acc: 29.6% (2955/10000)
[Test]  Epoch: 8	Loss: 0.030471	Acc: 28.1% (2814/10000)
[Test]  Epoch: 9	Loss: 0.028725	Acc: 32.7% (3273/10000)
[Test]  Epoch: 10	Loss: 0.028973	Acc: 32.7% (3270/10000)
[Test]  Epoch: 11	Loss: 0.028533	Acc: 35.2% (3522/10000)
[Test]  Epoch: 12	Loss: 0.028256	Acc: 35.5% (3553/10000)
[Test]  Epoch: 13	Loss: 0.027323	Acc: 36.7% (3667/10000)
[Test]  Epoch: 14	Loss: 0.030572	Acc: 34.5% (3452/10000)
[Test]  Epoch: 15	Loss: 0.034518	Acc: 34.5% (3452/10000)
[Test]  Epoch: 16	Loss: 0.031434	Acc: 33.4% (3342/10000)
[Test]  Epoch: 17	Loss: 0.027461	Acc: 40.3% (4027/10000)
[Test]  Epoch: 18	Loss: 0.029986	Acc: 36.8% (3676/10000)
[Test]  Epoch: 19	Loss: 0.029661	Acc: 37.9% (3785/10000)
[Test]  Epoch: 20	Loss: 0.028032	Acc: 38.3% (3830/10000)
[Test]  Epoch: 21	Loss: 0.029497	Acc: 40.5% (4052/10000)
[Test]  Epoch: 22	Loss: 0.030657	Acc: 38.4% (3839/10000)
[Test]  Epoch: 23	Loss: 0.029256	Acc: 39.5% (3949/10000)
[Test]  Epoch: 24	Loss: 0.028287	Acc: 41.5% (4152/10000)
[Test]  Epoch: 25	Loss: 0.028781	Acc: 42.6% (4258/10000)
[Test]  Epoch: 26	Loss: 0.029112	Acc: 41.7% (4173/10000)
[Test]  Epoch: 27	Loss: 0.030156	Acc: 40.0% (3997/10000)
[Test]  Epoch: 28	Loss: 0.031046	Acc: 41.6% (4161/10000)
[Test]  Epoch: 29	Loss: 0.029169	Acc: 42.3% (4227/10000)
[Test]  Epoch: 30	Loss: 0.030483	Acc: 42.9% (4294/10000)
[Test]  Epoch: 31	Loss: 0.032344	Acc: 41.6% (4156/10000)
[Test]  Epoch: 32	Loss: 0.035094	Acc: 40.4% (4039/10000)
[Test]  Epoch: 33	Loss: 0.030752	Acc: 43.9% (4393/10000)
[Test]  Epoch: 34	Loss: 0.034613	Acc: 40.2% (4025/10000)
[Test]  Epoch: 35	Loss: 0.033064	Acc: 42.7% (4270/10000)
[Test]  Epoch: 36	Loss: 0.036397	Acc: 41.4% (4138/10000)
[Test]  Epoch: 37	Loss: 0.033020	Acc: 43.4% (4343/10000)
[Test]  Epoch: 38	Loss: 0.034630	Acc: 42.5% (4248/10000)
[Test]  Epoch: 39	Loss: 0.032978	Acc: 44.2% (4423/10000)
[Test]  Epoch: 40	Loss: 0.040176	Acc: 40.7% (4073/10000)
[Test]  Epoch: 41	Loss: 0.037998	Acc: 42.3% (4229/10000)
[Test]  Epoch: 42	Loss: 0.033138	Acc: 46.3% (4632/10000)
[Test]  Epoch: 43	Loss: 0.039419	Acc: 43.0% (4305/10000)
[Test]  Epoch: 44	Loss: 0.037846	Acc: 43.6% (4362/10000)
[Test]  Epoch: 45	Loss: 0.033778	Acc: 45.5% (4547/10000)
[Test]  Epoch: 46	Loss: 0.036559	Acc: 43.0% (4299/10000)
[Test]  Epoch: 47	Loss: 0.038144	Acc: 43.7% (4374/10000)
[Test]  Epoch: 48	Loss: 0.038280	Acc: 43.3% (4330/10000)
[Test]  Epoch: 49	Loss: 0.038052	Acc: 44.6% (4456/10000)
[Test]  Epoch: 50	Loss: 0.039680	Acc: 42.1% (4212/10000)
[Test]  Epoch: 51	Loss: 0.035787	Acc: 45.4% (4537/10000)
[Test]  Epoch: 52	Loss: 0.038280	Acc: 45.3% (4532/10000)
[Test]  Epoch: 53	Loss: 0.038897	Acc: 43.6% (4364/10000)
[Test]  Epoch: 54	Loss: 0.036721	Acc: 45.1% (4514/10000)
[Test]  Epoch: 55	Loss: 0.039685	Acc: 43.3% (4329/10000)
[Test]  Epoch: 56	Loss: 0.039593	Acc: 45.3% (4528/10000)
[Test]  Epoch: 57	Loss: 0.043607	Acc: 41.3% (4126/10000)
[Test]  Epoch: 58	Loss: 0.037823	Acc: 47.1% (4710/10000)
[Test]  Epoch: 59	Loss: 0.039640	Acc: 45.8% (4575/10000)
[Test]  Epoch: 60	Loss: 0.039484	Acc: 45.7% (4571/10000)
[Test]  Epoch: 61	Loss: 0.035876	Acc: 47.9% (4786/10000)
[Test]  Epoch: 62	Loss: 0.035250	Acc: 47.5% (4754/10000)
[Test]  Epoch: 63	Loss: 0.035419	Acc: 47.3% (4729/10000)
[Test]  Epoch: 64	Loss: 0.035523	Acc: 47.6% (4762/10000)
[Test]  Epoch: 65	Loss: 0.034638	Acc: 48.2% (4820/10000)
[Test]  Epoch: 66	Loss: 0.035241	Acc: 48.2% (4824/10000)
[Test]  Epoch: 67	Loss: 0.034811	Acc: 48.0% (4800/10000)
[Test]  Epoch: 68	Loss: 0.034916	Acc: 48.4% (4838/10000)
[Test]  Epoch: 69	Loss: 0.035083	Acc: 48.6% (4860/10000)
[Test]  Epoch: 70	Loss: 0.034757	Acc: 48.4% (4840/10000)
[Test]  Epoch: 71	Loss: 0.034860	Acc: 48.6% (4857/10000)
[Test]  Epoch: 72	Loss: 0.035321	Acc: 48.1% (4810/10000)
[Test]  Epoch: 73	Loss: 0.035254	Acc: 48.1% (4812/10000)
[Test]  Epoch: 74	Loss: 0.035536	Acc: 48.1% (4811/10000)
[Test]  Epoch: 75	Loss: 0.034960	Acc: 48.4% (4839/10000)
[Test]  Epoch: 76	Loss: 0.035188	Acc: 48.6% (4857/10000)
[Test]  Epoch: 77	Loss: 0.035100	Acc: 48.7% (4871/10000)
[Test]  Epoch: 78	Loss: 0.035234	Acc: 48.5% (4854/10000)
[Test]  Epoch: 79	Loss: 0.035000	Acc: 48.9% (4888/10000)
[Test]  Epoch: 80	Loss: 0.034896	Acc: 48.1% (4813/10000)
[Test]  Epoch: 81	Loss: 0.035366	Acc: 48.9% (4886/10000)
[Test]  Epoch: 82	Loss: 0.035419	Acc: 48.5% (4846/10000)
[Test]  Epoch: 83	Loss: 0.035400	Acc: 48.5% (4850/10000)
[Test]  Epoch: 84	Loss: 0.035209	Acc: 48.5% (4849/10000)
[Test]  Epoch: 85	Loss: 0.035214	Acc: 48.9% (4889/10000)
[Test]  Epoch: 86	Loss: 0.035266	Acc: 48.8% (4880/10000)
[Test]  Epoch: 87	Loss: 0.035179	Acc: 48.6% (4861/10000)
[Test]  Epoch: 88	Loss: 0.035209	Acc: 48.5% (4850/10000)
[Test]  Epoch: 89	Loss: 0.035000	Acc: 49.1% (4907/10000)
[Test]  Epoch: 90	Loss: 0.035529	Acc: 49.1% (4913/10000)
[Test]  Epoch: 91	Loss: 0.035036	Acc: 49.6% (4956/10000)
[Test]  Epoch: 92	Loss: 0.035020	Acc: 48.9% (4891/10000)
[Test]  Epoch: 93	Loss: 0.035381	Acc: 49.0% (4904/10000)
[Test]  Epoch: 94	Loss: 0.035286	Acc: 48.6% (4860/10000)
[Test]  Epoch: 95	Loss: 0.035013	Acc: 48.9% (4894/10000)
[Test]  Epoch: 96	Loss: 0.035205	Acc: 49.0% (4897/10000)
[Test]  Epoch: 97	Loss: 0.035541	Acc: 48.6% (4859/10000)
[Test]  Epoch: 98	Loss: 0.035546	Acc: 48.5% (4849/10000)
[Test]  Epoch: 99	Loss: 0.035449	Acc: 48.8% (4880/10000)
[Test]  Epoch: 100	Loss: 0.035406	Acc: 48.7% (4873/10000)
===========finish==========
['2024-08-19', '06:18:14.861396', '100', 'test', '0.035406349217891694', '48.73', '49.56']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-resnet18/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('conv1.weight', 0.0), ('bn1.weight', 0.0), ('bn1.bias', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.bn1.weight', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.bn2.weight', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.bn1.weight', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.bn2.weight', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.bn1.weight', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.bn2.weight', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.downsample.0.weight', 0.0), ('layer2.0.downsample.1.weight', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.bn1.weight', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.bn2.weight', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.bn1.weight', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.bn2.weight', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.downsample.0.weight', 0.0), ('layer3.0.downsample.1.weight', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.bn1.weight', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.bn2.weight', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.bn1.weight', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.bn2.weight', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.downsample.0.weight', 0.0), ('layer4.0.downsample.1.weight', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.bn1.weight', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.bn2.weight', 0.0), ('layer4.1.bn2.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('conv1.weight', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.conv2.weight', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=41  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039889	Acc: 43.8% (4382/10000)
[Test]  Epoch: 2	Loss: 0.039521	Acc: 44.2% (4424/10000)
[Test]  Epoch: 3	Loss: 0.039681	Acc: 43.9% (4393/10000)
[Test]  Epoch: 4	Loss: 0.039571	Acc: 44.1% (4406/10000)
[Test]  Epoch: 5	Loss: 0.039677	Acc: 43.8% (4382/10000)
[Test]  Epoch: 6	Loss: 0.039750	Acc: 44.0% (4400/10000)
[Test]  Epoch: 7	Loss: 0.039859	Acc: 43.8% (4381/10000)
[Test]  Epoch: 8	Loss: 0.039849	Acc: 44.0% (4395/10000)
[Test]  Epoch: 9	Loss: 0.039921	Acc: 43.8% (4381/10000)
[Test]  Epoch: 10	Loss: 0.039994	Acc: 43.7% (4371/10000)
[Test]  Epoch: 11	Loss: 0.040026	Acc: 43.7% (4366/10000)
[Test]  Epoch: 12	Loss: 0.039993	Acc: 43.7% (4374/10000)
[Test]  Epoch: 13	Loss: 0.040033	Acc: 43.5% (4353/10000)
[Test]  Epoch: 14	Loss: 0.039996	Acc: 43.7% (4366/10000)
[Test]  Epoch: 15	Loss: 0.040074	Acc: 43.7% (4368/10000)
[Test]  Epoch: 16	Loss: 0.040097	Acc: 43.8% (4380/10000)
[Test]  Epoch: 17	Loss: 0.040071	Acc: 43.7% (4367/10000)
[Test]  Epoch: 18	Loss: 0.040098	Acc: 43.7% (4372/10000)
[Test]  Epoch: 19	Loss: 0.040129	Acc: 43.8% (4377/10000)
[Test]  Epoch: 20	Loss: 0.040221	Acc: 43.5% (4355/10000)
[Test]  Epoch: 21	Loss: 0.040256	Acc: 43.3% (4331/10000)
[Test]  Epoch: 22	Loss: 0.040213	Acc: 43.7% (4368/10000)
[Test]  Epoch: 23	Loss: 0.040216	Acc: 43.4% (4344/10000)
[Test]  Epoch: 24	Loss: 0.040235	Acc: 43.6% (4361/10000)
[Test]  Epoch: 25	Loss: 0.040237	Acc: 43.5% (4347/10000)
[Test]  Epoch: 26	Loss: 0.040390	Acc: 43.1% (4313/10000)
[Test]  Epoch: 27	Loss: 0.040249	Acc: 43.6% (4365/10000)
[Test]  Epoch: 28	Loss: 0.040381	Acc: 43.6% (4356/10000)
[Test]  Epoch: 29	Loss: 0.040378	Acc: 43.3% (4333/10000)
[Test]  Epoch: 30	Loss: 0.040331	Acc: 43.6% (4363/10000)
[Test]  Epoch: 31	Loss: 0.040465	Acc: 43.4% (4344/10000)
[Test]  Epoch: 32	Loss: 0.040446	Acc: 43.4% (4341/10000)
[Test]  Epoch: 33	Loss: 0.040337	Acc: 43.6% (4365/10000)
[Test]  Epoch: 34	Loss: 0.040418	Acc: 43.4% (4341/10000)
[Test]  Epoch: 35	Loss: 0.040420	Acc: 43.5% (4352/10000)
[Test]  Epoch: 36	Loss: 0.040362	Acc: 43.6% (4356/10000)
[Test]  Epoch: 37	Loss: 0.040399	Acc: 43.5% (4355/10000)
[Test]  Epoch: 38	Loss: 0.040445	Acc: 43.6% (4356/10000)
[Test]  Epoch: 39	Loss: 0.040498	Acc: 43.2% (4325/10000)
[Test]  Epoch: 40	Loss: 0.040443	Acc: 43.4% (4344/10000)
[Test]  Epoch: 41	Loss: 0.040545	Acc: 43.4% (4341/10000)
[Test]  Epoch: 42	Loss: 0.040508	Acc: 43.5% (4354/10000)
[Test]  Epoch: 43	Loss: 0.040489	Acc: 43.5% (4347/10000)
[Test]  Epoch: 44	Loss: 0.040484	Acc: 43.5% (4349/10000)
[Test]  Epoch: 45	Loss: 0.040491	Acc: 43.6% (4358/10000)
[Test]  Epoch: 46	Loss: 0.040595	Acc: 43.3% (4329/10000)
[Test]  Epoch: 47	Loss: 0.040512	Acc: 43.5% (4350/10000)
[Test]  Epoch: 48	Loss: 0.040481	Acc: 43.5% (4345/10000)
[Test]  Epoch: 49	Loss: 0.040526	Acc: 43.4% (4344/10000)
[Test]  Epoch: 50	Loss: 0.040518	Acc: 43.5% (4351/10000)
[Test]  Epoch: 51	Loss: 0.040536	Acc: 43.6% (4364/10000)
[Test]  Epoch: 52	Loss: 0.040558	Acc: 43.5% (4347/10000)
[Test]  Epoch: 53	Loss: 0.040572	Acc: 43.5% (4346/10000)
[Test]  Epoch: 54	Loss: 0.040577	Acc: 43.6% (4362/10000)
[Test]  Epoch: 55	Loss: 0.040627	Acc: 43.6% (4361/10000)
[Test]  Epoch: 56	Loss: 0.040624	Acc: 43.5% (4348/10000)
[Test]  Epoch: 57	Loss: 0.040648	Acc: 43.6% (4357/10000)
[Test]  Epoch: 58	Loss: 0.040574	Acc: 43.4% (4344/10000)
[Test]  Epoch: 59	Loss: 0.040635	Acc: 43.5% (4347/10000)
[Test]  Epoch: 60	Loss: 0.040723	Acc: 43.5% (4347/10000)
[Test]  Epoch: 61	Loss: 0.040747	Acc: 43.3% (4328/10000)
[Test]  Epoch: 62	Loss: 0.040685	Acc: 43.5% (4346/10000)
[Test]  Epoch: 63	Loss: 0.040641	Acc: 43.4% (4344/10000)
[Test]  Epoch: 64	Loss: 0.040605	Acc: 43.5% (4349/10000)
[Test]  Epoch: 65	Loss: 0.040657	Acc: 43.4% (4336/10000)
[Test]  Epoch: 66	Loss: 0.040668	Acc: 43.5% (4349/10000)
[Test]  Epoch: 67	Loss: 0.040689	Acc: 43.7% (4370/10000)
[Test]  Epoch: 68	Loss: 0.040717	Acc: 43.4% (4337/10000)
[Test]  Epoch: 69	Loss: 0.040690	Acc: 43.5% (4345/10000)
[Test]  Epoch: 70	Loss: 0.040647	Acc: 43.4% (4342/10000)
[Test]  Epoch: 71	Loss: 0.040661	Acc: 43.5% (4346/10000)
[Test]  Epoch: 72	Loss: 0.040676	Acc: 43.3% (4331/10000)
[Test]  Epoch: 73	Loss: 0.040650	Acc: 43.5% (4346/10000)
[Test]  Epoch: 74	Loss: 0.040644	Acc: 43.6% (4358/10000)
[Test]  Epoch: 75	Loss: 0.040705	Acc: 43.5% (4353/10000)
[Test]  Epoch: 76	Loss: 0.040637	Acc: 43.6% (4359/10000)
[Test]  Epoch: 77	Loss: 0.040635	Acc: 43.6% (4358/10000)
[Test]  Epoch: 78	Loss: 0.040666	Acc: 43.5% (4352/10000)
[Test]  Epoch: 79	Loss: 0.040683	Acc: 43.6% (4357/10000)
[Test]  Epoch: 80	Loss: 0.040622	Acc: 43.7% (4366/10000)
[Test]  Epoch: 81	Loss: 0.040632	Acc: 43.7% (4367/10000)
[Test]  Epoch: 82	Loss: 0.040682	Acc: 43.7% (4367/10000)
[Test]  Epoch: 83	Loss: 0.040722	Acc: 43.5% (4352/10000)
[Test]  Epoch: 84	Loss: 0.040732	Acc: 43.4% (4341/10000)
[Test]  Epoch: 85	Loss: 0.040703	Acc: 43.5% (4353/10000)
[Test]  Epoch: 86	Loss: 0.040663	Acc: 43.6% (4357/10000)
[Test]  Epoch: 87	Loss: 0.040633	Acc: 43.4% (4343/10000)
[Test]  Epoch: 88	Loss: 0.040732	Acc: 43.4% (4344/10000)
[Test]  Epoch: 89	Loss: 0.040670	Acc: 43.5% (4355/10000)
[Test]  Epoch: 90	Loss: 0.040672	Acc: 43.5% (4353/10000)
[Test]  Epoch: 91	Loss: 0.040689	Acc: 43.7% (4368/10000)
[Test]  Epoch: 92	Loss: 0.040635	Acc: 43.6% (4361/10000)
[Test]  Epoch: 93	Loss: 0.040689	Acc: 43.5% (4351/10000)
[Test]  Epoch: 94	Loss: 0.040665	Acc: 43.7% (4367/10000)
[Test]  Epoch: 95	Loss: 0.040652	Acc: 43.5% (4355/10000)
[Test]  Epoch: 96	Loss: 0.040667	Acc: 43.6% (4356/10000)
[Test]  Epoch: 97	Loss: 0.040675	Acc: 43.5% (4352/10000)
[Test]  Epoch: 98	Loss: 0.040652	Acc: 43.5% (4346/10000)
[Test]  Epoch: 99	Loss: 0.040672	Acc: 43.6% (4357/10000)
[Test]  Epoch: 100	Loss: 0.040713	Acc: 43.4% (4337/10000)
===========finish==========
['2024-08-19', '06:22:38.970631', '100', 'test', '0.040712788099050524', '43.37', '44.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.046197	Acc: 34.9% (3487/10000)
[Test]  Epoch: 2	Loss: 0.043762	Acc: 38.4% (3837/10000)
[Test]  Epoch: 3	Loss: 0.043401	Acc: 38.7% (3872/10000)
[Test]  Epoch: 4	Loss: 0.043042	Acc: 39.2% (3918/10000)
[Test]  Epoch: 5	Loss: 0.042884	Acc: 39.5% (3949/10000)
[Test]  Epoch: 6	Loss: 0.042916	Acc: 39.5% (3950/10000)
[Test]  Epoch: 7	Loss: 0.042960	Acc: 39.7% (3973/10000)
[Test]  Epoch: 8	Loss: 0.042801	Acc: 39.9% (3986/10000)
[Test]  Epoch: 9	Loss: 0.042831	Acc: 40.2% (4017/10000)
[Test]  Epoch: 10	Loss: 0.042870	Acc: 39.8% (3977/10000)
[Test]  Epoch: 11	Loss: 0.042825	Acc: 40.0% (4005/10000)
[Test]  Epoch: 12	Loss: 0.042781	Acc: 40.0% (4002/10000)
[Test]  Epoch: 13	Loss: 0.042874	Acc: 40.0% (3995/10000)
[Test]  Epoch: 14	Loss: 0.042783	Acc: 40.2% (4017/10000)
[Test]  Epoch: 15	Loss: 0.042877	Acc: 40.0% (4004/10000)
[Test]  Epoch: 16	Loss: 0.042765	Acc: 40.2% (4020/10000)
[Test]  Epoch: 17	Loss: 0.042728	Acc: 40.3% (4031/10000)
[Test]  Epoch: 18	Loss: 0.042714	Acc: 40.3% (4034/10000)
[Test]  Epoch: 19	Loss: 0.042763	Acc: 40.2% (4018/10000)
[Test]  Epoch: 20	Loss: 0.042897	Acc: 40.3% (4031/10000)
[Test]  Epoch: 21	Loss: 0.042809	Acc: 40.5% (4052/10000)
[Test]  Epoch: 22	Loss: 0.042797	Acc: 40.4% (4038/10000)
[Test]  Epoch: 23	Loss: 0.042777	Acc: 40.4% (4036/10000)
[Test]  Epoch: 24	Loss: 0.042826	Acc: 40.6% (4064/10000)
[Test]  Epoch: 25	Loss: 0.042741	Acc: 40.5% (4050/10000)
[Test]  Epoch: 26	Loss: 0.042880	Acc: 40.3% (4027/10000)
[Test]  Epoch: 27	Loss: 0.042765	Acc: 40.5% (4046/10000)
[Test]  Epoch: 28	Loss: 0.042893	Acc: 40.5% (4051/10000)
[Test]  Epoch: 29	Loss: 0.042830	Acc: 40.4% (4036/10000)
[Test]  Epoch: 30	Loss: 0.042804	Acc: 40.2% (4025/10000)
[Test]  Epoch: 31	Loss: 0.042993	Acc: 40.3% (4034/10000)
[Test]  Epoch: 32	Loss: 0.042955	Acc: 40.3% (4030/10000)
[Test]  Epoch: 33	Loss: 0.042803	Acc: 40.4% (4041/10000)
[Test]  Epoch: 34	Loss: 0.042883	Acc: 40.8% (4079/10000)
[Test]  Epoch: 35	Loss: 0.042877	Acc: 40.5% (4054/10000)
[Test]  Epoch: 36	Loss: 0.042790	Acc: 40.7% (4069/10000)
[Test]  Epoch: 37	Loss: 0.042826	Acc: 40.7% (4069/10000)
[Test]  Epoch: 38	Loss: 0.042912	Acc: 40.6% (4062/10000)
[Test]  Epoch: 39	Loss: 0.042922	Acc: 40.5% (4050/10000)
[Test]  Epoch: 40	Loss: 0.042867	Acc: 40.7% (4066/10000)
[Test]  Epoch: 41	Loss: 0.042921	Acc: 40.4% (4041/10000)
[Test]  Epoch: 42	Loss: 0.042923	Acc: 40.6% (4057/10000)
[Test]  Epoch: 43	Loss: 0.042903	Acc: 40.6% (4064/10000)
[Test]  Epoch: 44	Loss: 0.042885	Acc: 40.6% (4057/10000)
[Test]  Epoch: 45	Loss: 0.042887	Acc: 40.7% (4072/10000)
[Test]  Epoch: 46	Loss: 0.042968	Acc: 40.5% (4047/10000)
[Test]  Epoch: 47	Loss: 0.042935	Acc: 40.6% (4056/10000)
[Test]  Epoch: 48	Loss: 0.042875	Acc: 40.7% (4070/10000)
[Test]  Epoch: 49	Loss: 0.042894	Acc: 40.8% (4076/10000)
[Test]  Epoch: 50	Loss: 0.042936	Acc: 40.9% (4085/10000)
[Test]  Epoch: 51	Loss: 0.042916	Acc: 40.6% (4065/10000)
[Test]  Epoch: 52	Loss: 0.042933	Acc: 40.5% (4049/10000)
[Test]  Epoch: 53	Loss: 0.042959	Acc: 40.6% (4058/10000)
[Test]  Epoch: 54	Loss: 0.042937	Acc: 40.7% (4067/10000)
[Test]  Epoch: 55	Loss: 0.042971	Acc: 40.6% (4059/10000)
[Test]  Epoch: 56	Loss: 0.042985	Acc: 40.6% (4061/10000)
[Test]  Epoch: 57	Loss: 0.043043	Acc: 40.6% (4057/10000)
[Test]  Epoch: 58	Loss: 0.042931	Acc: 40.7% (4073/10000)
[Test]  Epoch: 59	Loss: 0.042990	Acc: 40.8% (4075/10000)
[Test]  Epoch: 60	Loss: 0.043088	Acc: 40.4% (4041/10000)
[Test]  Epoch: 61	Loss: 0.043101	Acc: 40.3% (4026/10000)
[Test]  Epoch: 62	Loss: 0.043063	Acc: 40.3% (4029/10000)
[Test]  Epoch: 63	Loss: 0.042993	Acc: 40.4% (4043/10000)
[Test]  Epoch: 64	Loss: 0.042972	Acc: 40.6% (4063/10000)
[Test]  Epoch: 65	Loss: 0.043024	Acc: 40.5% (4051/10000)
[Test]  Epoch: 66	Loss: 0.043035	Acc: 40.4% (4037/10000)
[Test]  Epoch: 67	Loss: 0.043063	Acc: 40.5% (4054/10000)
[Test]  Epoch: 68	Loss: 0.043078	Acc: 40.4% (4039/10000)
[Test]  Epoch: 69	Loss: 0.043046	Acc: 40.4% (4042/10000)
[Test]  Epoch: 70	Loss: 0.043013	Acc: 40.5% (4050/10000)
[Test]  Epoch: 71	Loss: 0.043050	Acc: 40.4% (4041/10000)
[Test]  Epoch: 72	Loss: 0.043065	Acc: 40.4% (4042/10000)
[Test]  Epoch: 73	Loss: 0.043015	Acc: 40.4% (4042/10000)
[Test]  Epoch: 74	Loss: 0.043006	Acc: 40.6% (4058/10000)
[Test]  Epoch: 75	Loss: 0.043060	Acc: 40.4% (4042/10000)
[Test]  Epoch: 76	Loss: 0.043000	Acc: 40.5% (4048/10000)
[Test]  Epoch: 77	Loss: 0.043000	Acc: 40.6% (4063/10000)
[Test]  Epoch: 78	Loss: 0.043023	Acc: 40.4% (4041/10000)
[Test]  Epoch: 79	Loss: 0.043048	Acc: 40.4% (4042/10000)
[Test]  Epoch: 80	Loss: 0.042980	Acc: 40.6% (4057/10000)
[Test]  Epoch: 81	Loss: 0.042975	Acc: 40.5% (4054/10000)
[Test]  Epoch: 82	Loss: 0.043021	Acc: 40.7% (4067/10000)
[Test]  Epoch: 83	Loss: 0.043079	Acc: 40.5% (4047/10000)
[Test]  Epoch: 84	Loss: 0.043078	Acc: 40.5% (4045/10000)
[Test]  Epoch: 85	Loss: 0.043050	Acc: 40.5% (4051/10000)
[Test]  Epoch: 86	Loss: 0.043005	Acc: 40.6% (4056/10000)
[Test]  Epoch: 87	Loss: 0.042993	Acc: 40.5% (4053/10000)
[Test]  Epoch: 88	Loss: 0.043091	Acc: 40.4% (4041/10000)
[Test]  Epoch: 89	Loss: 0.043025	Acc: 40.6% (4061/10000)
[Test]  Epoch: 90	Loss: 0.043028	Acc: 40.6% (4061/10000)
[Test]  Epoch: 91	Loss: 0.043037	Acc: 40.5% (4053/10000)
[Test]  Epoch: 92	Loss: 0.042985	Acc: 40.6% (4064/10000)
[Test]  Epoch: 93	Loss: 0.043036	Acc: 40.5% (4046/10000)
[Test]  Epoch: 94	Loss: 0.043011	Acc: 40.6% (4064/10000)
[Test]  Epoch: 95	Loss: 0.043026	Acc: 40.5% (4055/10000)
[Test]  Epoch: 96	Loss: 0.043009	Acc: 40.7% (4067/10000)
[Test]  Epoch: 97	Loss: 0.043037	Acc: 40.5% (4054/10000)
[Test]  Epoch: 98	Loss: 0.043008	Acc: 40.6% (4059/10000)
[Test]  Epoch: 99	Loss: 0.043039	Acc: 40.6% (4056/10000)
[Test]  Epoch: 100	Loss: 0.043070	Acc: 40.6% (4056/10000)
===========finish==========
['2024-08-19', '06:25:27.321269', '100', 'test', '0.04307002249956131', '40.56', '40.85']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.049637	Acc: 29.1% (2915/10000)
[Test]  Epoch: 2	Loss: 0.045963	Acc: 35.1% (3515/10000)
[Test]  Epoch: 3	Loss: 0.045099	Acc: 36.3% (3629/10000)
[Test]  Epoch: 4	Loss: 0.044675	Acc: 37.0% (3701/10000)
[Test]  Epoch: 5	Loss: 0.044401	Acc: 37.5% (3747/10000)
[Test]  Epoch: 6	Loss: 0.044343	Acc: 37.3% (3734/10000)
[Test]  Epoch: 7	Loss: 0.044354	Acc: 37.6% (3763/10000)
[Test]  Epoch: 8	Loss: 0.044210	Acc: 37.9% (3788/10000)
[Test]  Epoch: 9	Loss: 0.044118	Acc: 38.1% (3810/10000)
[Test]  Epoch: 10	Loss: 0.044148	Acc: 38.1% (3809/10000)
[Test]  Epoch: 11	Loss: 0.044104	Acc: 38.2% (3822/10000)
[Test]  Epoch: 12	Loss: 0.043963	Acc: 38.1% (3811/10000)
[Test]  Epoch: 13	Loss: 0.044073	Acc: 38.1% (3807/10000)
[Test]  Epoch: 14	Loss: 0.043968	Acc: 38.1% (3806/10000)
[Test]  Epoch: 15	Loss: 0.044085	Acc: 38.2% (3822/10000)
[Test]  Epoch: 16	Loss: 0.044020	Acc: 38.0% (3803/10000)
[Test]  Epoch: 17	Loss: 0.043897	Acc: 38.5% (3853/10000)
[Test]  Epoch: 18	Loss: 0.043958	Acc: 38.1% (3810/10000)
[Test]  Epoch: 19	Loss: 0.043996	Acc: 38.0% (3803/10000)
[Test]  Epoch: 20	Loss: 0.044011	Acc: 38.1% (3811/10000)
[Test]  Epoch: 21	Loss: 0.043961	Acc: 38.2% (3816/10000)
[Test]  Epoch: 22	Loss: 0.043972	Acc: 38.1% (3810/10000)
[Test]  Epoch: 23	Loss: 0.043899	Acc: 38.4% (3841/10000)
[Test]  Epoch: 24	Loss: 0.043960	Acc: 38.3% (3831/10000)
[Test]  Epoch: 25	Loss: 0.043882	Acc: 38.4% (3840/10000)
[Test]  Epoch: 26	Loss: 0.044063	Acc: 38.3% (3828/10000)
[Test]  Epoch: 27	Loss: 0.043908	Acc: 38.3% (3827/10000)
[Test]  Epoch: 28	Loss: 0.044035	Acc: 38.3% (3828/10000)
[Test]  Epoch: 29	Loss: 0.043952	Acc: 38.3% (3828/10000)
[Test]  Epoch: 30	Loss: 0.043980	Acc: 38.4% (3835/10000)
[Test]  Epoch: 31	Loss: 0.044100	Acc: 38.4% (3837/10000)
[Test]  Epoch: 32	Loss: 0.044076	Acc: 38.3% (3829/10000)
[Test]  Epoch: 33	Loss: 0.043977	Acc: 38.3% (3833/10000)
[Test]  Epoch: 34	Loss: 0.044012	Acc: 38.4% (3835/10000)
[Test]  Epoch: 35	Loss: 0.043982	Acc: 38.5% (3845/10000)
[Test]  Epoch: 36	Loss: 0.043876	Acc: 38.6% (3857/10000)
[Test]  Epoch: 37	Loss: 0.043969	Acc: 38.5% (3851/10000)
[Test]  Epoch: 38	Loss: 0.043975	Acc: 38.5% (3852/10000)
[Test]  Epoch: 39	Loss: 0.044044	Acc: 38.4% (3836/10000)
[Test]  Epoch: 40	Loss: 0.043995	Acc: 38.6% (3857/10000)
[Test]  Epoch: 41	Loss: 0.044037	Acc: 38.4% (3842/10000)
[Test]  Epoch: 42	Loss: 0.043985	Acc: 38.5% (3847/10000)
[Test]  Epoch: 43	Loss: 0.043999	Acc: 38.6% (3859/10000)
[Test]  Epoch: 44	Loss: 0.044000	Acc: 38.5% (3854/10000)
[Test]  Epoch: 45	Loss: 0.044054	Acc: 38.5% (3847/10000)
[Test]  Epoch: 46	Loss: 0.044101	Acc: 38.5% (3853/10000)
[Test]  Epoch: 47	Loss: 0.044004	Acc: 38.7% (3872/10000)
[Test]  Epoch: 48	Loss: 0.044029	Acc: 38.8% (3878/10000)
[Test]  Epoch: 49	Loss: 0.043974	Acc: 38.8% (3876/10000)
[Test]  Epoch: 50	Loss: 0.044021	Acc: 38.8% (3878/10000)
[Test]  Epoch: 51	Loss: 0.043980	Acc: 39.0% (3897/10000)
[Test]  Epoch: 52	Loss: 0.044004	Acc: 38.7% (3868/10000)
[Test]  Epoch: 53	Loss: 0.044059	Acc: 38.7% (3869/10000)
[Test]  Epoch: 54	Loss: 0.044068	Acc: 38.8% (3880/10000)
[Test]  Epoch: 55	Loss: 0.044052	Acc: 38.7% (3869/10000)
[Test]  Epoch: 56	Loss: 0.044099	Acc: 38.7% (3868/10000)
[Test]  Epoch: 57	Loss: 0.044119	Acc: 38.8% (3881/10000)
[Test]  Epoch: 58	Loss: 0.043990	Acc: 39.1% (3910/10000)
[Test]  Epoch: 59	Loss: 0.044045	Acc: 39.0% (3902/10000)
[Test]  Epoch: 60	Loss: 0.044120	Acc: 38.7% (3868/10000)
[Test]  Epoch: 61	Loss: 0.044156	Acc: 38.6% (3865/10000)
[Test]  Epoch: 62	Loss: 0.044121	Acc: 38.8% (3879/10000)
[Test]  Epoch: 63	Loss: 0.044049	Acc: 38.7% (3874/10000)
[Test]  Epoch: 64	Loss: 0.044025	Acc: 38.9% (3890/10000)
[Test]  Epoch: 65	Loss: 0.044090	Acc: 38.7% (3874/10000)
[Test]  Epoch: 66	Loss: 0.044109	Acc: 38.8% (3881/10000)
[Test]  Epoch: 67	Loss: 0.044118	Acc: 38.8% (3880/10000)
[Test]  Epoch: 68	Loss: 0.044156	Acc: 38.6% (3862/10000)
[Test]  Epoch: 69	Loss: 0.044114	Acc: 38.8% (3876/10000)
[Test]  Epoch: 70	Loss: 0.044087	Acc: 39.0% (3899/10000)
[Test]  Epoch: 71	Loss: 0.044104	Acc: 38.9% (3890/10000)
[Test]  Epoch: 72	Loss: 0.044115	Acc: 38.8% (3883/10000)
[Test]  Epoch: 73	Loss: 0.044070	Acc: 38.7% (3872/10000)
[Test]  Epoch: 74	Loss: 0.044072	Acc: 38.8% (3880/10000)
[Test]  Epoch: 75	Loss: 0.044132	Acc: 38.8% (3880/10000)
[Test]  Epoch: 76	Loss: 0.044068	Acc: 38.8% (3880/10000)
[Test]  Epoch: 77	Loss: 0.044091	Acc: 38.9% (3885/10000)
[Test]  Epoch: 78	Loss: 0.044105	Acc: 38.7% (3873/10000)
[Test]  Epoch: 79	Loss: 0.044111	Acc: 38.9% (3888/10000)
[Test]  Epoch: 80	Loss: 0.044052	Acc: 38.9% (3889/10000)
[Test]  Epoch: 81	Loss: 0.044036	Acc: 38.9% (3886/10000)
[Test]  Epoch: 82	Loss: 0.044111	Acc: 38.8% (3882/10000)
[Test]  Epoch: 83	Loss: 0.044140	Acc: 38.6% (3863/10000)
[Test]  Epoch: 84	Loss: 0.044149	Acc: 38.8% (3883/10000)
[Test]  Epoch: 85	Loss: 0.044127	Acc: 38.7% (3873/10000)
[Test]  Epoch: 86	Loss: 0.044115	Acc: 38.9% (3888/10000)
[Test]  Epoch: 87	Loss: 0.044069	Acc: 38.9% (3887/10000)
[Test]  Epoch: 88	Loss: 0.044160	Acc: 38.6% (3860/10000)
[Test]  Epoch: 89	Loss: 0.044082	Acc: 38.8% (3882/10000)
[Test]  Epoch: 90	Loss: 0.044097	Acc: 38.9% (3891/10000)
[Test]  Epoch: 91	Loss: 0.044098	Acc: 38.8% (3882/10000)
[Test]  Epoch: 92	Loss: 0.044037	Acc: 39.0% (3898/10000)
[Test]  Epoch: 93	Loss: 0.044133	Acc: 38.8% (3880/10000)
[Test]  Epoch: 94	Loss: 0.044067	Acc: 39.0% (3895/10000)
[Test]  Epoch: 95	Loss: 0.044075	Acc: 38.9% (3886/10000)
[Test]  Epoch: 96	Loss: 0.044078	Acc: 38.8% (3884/10000)
[Test]  Epoch: 97	Loss: 0.044108	Acc: 38.7% (3869/10000)
[Test]  Epoch: 98	Loss: 0.044077	Acc: 38.7% (3866/10000)
[Test]  Epoch: 99	Loss: 0.044092	Acc: 38.7% (3874/10000)
[Test]  Epoch: 100	Loss: 0.044103	Acc: 38.8% (3882/10000)
===========finish==========
['2024-08-19', '06:28:30.457085', '100', 'test', '0.044103094518184664', '38.82', '39.1']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.056718	Acc: 21.4% (2144/10000)
[Test]  Epoch: 2	Loss: 0.049831	Acc: 29.8% (2981/10000)
[Test]  Epoch: 3	Loss: 0.048546	Acc: 31.4% (3136/10000)
[Test]  Epoch: 4	Loss: 0.047884	Acc: 32.2% (3223/10000)
[Test]  Epoch: 5	Loss: 0.047560	Acc: 32.9% (3291/10000)
[Test]  Epoch: 6	Loss: 0.047380	Acc: 33.1% (3308/10000)
[Test]  Epoch: 7	Loss: 0.047300	Acc: 33.6% (3362/10000)
[Test]  Epoch: 8	Loss: 0.047191	Acc: 33.7% (3369/10000)
[Test]  Epoch: 9	Loss: 0.046992	Acc: 34.0% (3400/10000)
[Test]  Epoch: 10	Loss: 0.046951	Acc: 33.7% (3368/10000)
[Test]  Epoch: 11	Loss: 0.046906	Acc: 34.0% (3400/10000)
[Test]  Epoch: 12	Loss: 0.046804	Acc: 34.3% (3427/10000)
[Test]  Epoch: 13	Loss: 0.046838	Acc: 34.2% (3420/10000)
[Test]  Epoch: 14	Loss: 0.046703	Acc: 34.5% (3445/10000)
[Test]  Epoch: 15	Loss: 0.046856	Acc: 34.2% (3423/10000)
[Test]  Epoch: 16	Loss: 0.046690	Acc: 34.5% (3447/10000)
[Test]  Epoch: 17	Loss: 0.046611	Acc: 34.8% (3475/10000)
[Test]  Epoch: 18	Loss: 0.046620	Acc: 34.5% (3455/10000)
[Test]  Epoch: 19	Loss: 0.046593	Acc: 34.6% (3457/10000)
[Test]  Epoch: 20	Loss: 0.046705	Acc: 34.6% (3459/10000)
[Test]  Epoch: 21	Loss: 0.046634	Acc: 34.7% (3466/10000)
[Test]  Epoch: 22	Loss: 0.046658	Acc: 34.6% (3463/10000)
[Test]  Epoch: 23	Loss: 0.046604	Acc: 34.7% (3470/10000)
[Test]  Epoch: 24	Loss: 0.046608	Acc: 34.7% (3472/10000)
[Test]  Epoch: 25	Loss: 0.046609	Acc: 34.8% (3476/10000)
[Test]  Epoch: 26	Loss: 0.046648	Acc: 34.6% (3457/10000)
[Test]  Epoch: 27	Loss: 0.046539	Acc: 34.7% (3470/10000)
[Test]  Epoch: 28	Loss: 0.046582	Acc: 34.7% (3472/10000)
[Test]  Epoch: 29	Loss: 0.046594	Acc: 34.6% (3457/10000)
[Test]  Epoch: 30	Loss: 0.046578	Acc: 34.8% (3483/10000)
[Test]  Epoch: 31	Loss: 0.046639	Acc: 34.8% (3480/10000)
[Test]  Epoch: 32	Loss: 0.046654	Acc: 34.8% (3479/10000)
[Test]  Epoch: 33	Loss: 0.046539	Acc: 34.9% (3485/10000)
[Test]  Epoch: 34	Loss: 0.046538	Acc: 35.1% (3508/10000)
[Test]  Epoch: 35	Loss: 0.046528	Acc: 35.1% (3506/10000)
[Test]  Epoch: 36	Loss: 0.046392	Acc: 35.1% (3515/10000)
[Test]  Epoch: 37	Loss: 0.046478	Acc: 35.1% (3512/10000)
[Test]  Epoch: 38	Loss: 0.046539	Acc: 35.0% (3505/10000)
[Test]  Epoch: 39	Loss: 0.046552	Acc: 34.9% (3494/10000)
[Test]  Epoch: 40	Loss: 0.046496	Acc: 34.9% (3492/10000)
[Test]  Epoch: 41	Loss: 0.046564	Acc: 35.0% (3497/10000)
[Test]  Epoch: 42	Loss: 0.046483	Acc: 35.1% (3509/10000)
[Test]  Epoch: 43	Loss: 0.046504	Acc: 35.0% (3504/10000)
[Test]  Epoch: 44	Loss: 0.046515	Acc: 35.2% (3520/10000)
[Test]  Epoch: 45	Loss: 0.046541	Acc: 35.1% (3506/10000)
[Test]  Epoch: 46	Loss: 0.046613	Acc: 35.0% (3503/10000)
[Test]  Epoch: 47	Loss: 0.046547	Acc: 35.1% (3508/10000)
[Test]  Epoch: 48	Loss: 0.046448	Acc: 35.3% (3533/10000)
[Test]  Epoch: 49	Loss: 0.046508	Acc: 35.3% (3530/10000)
[Test]  Epoch: 50	Loss: 0.046549	Acc: 35.2% (3519/10000)
[Test]  Epoch: 51	Loss: 0.046502	Acc: 35.1% (3515/10000)
[Test]  Epoch: 52	Loss: 0.046489	Acc: 35.1% (3511/10000)
[Test]  Epoch: 53	Loss: 0.046502	Acc: 35.2% (3518/10000)
[Test]  Epoch: 54	Loss: 0.046530	Acc: 35.2% (3518/10000)
[Test]  Epoch: 55	Loss: 0.046537	Acc: 35.2% (3519/10000)
[Test]  Epoch: 56	Loss: 0.046580	Acc: 35.4% (3541/10000)
[Test]  Epoch: 57	Loss: 0.046566	Acc: 35.3% (3526/10000)
[Test]  Epoch: 58	Loss: 0.046456	Acc: 35.4% (3536/10000)
[Test]  Epoch: 59	Loss: 0.046503	Acc: 35.1% (3509/10000)
[Test]  Epoch: 60	Loss: 0.046555	Acc: 35.1% (3513/10000)
[Test]  Epoch: 61	Loss: 0.046584	Acc: 35.1% (3512/10000)
[Test]  Epoch: 62	Loss: 0.046572	Acc: 35.1% (3515/10000)
[Test]  Epoch: 63	Loss: 0.046475	Acc: 35.1% (3509/10000)
[Test]  Epoch: 64	Loss: 0.046481	Acc: 35.2% (3521/10000)
[Test]  Epoch: 65	Loss: 0.046551	Acc: 35.2% (3524/10000)
[Test]  Epoch: 66	Loss: 0.046561	Acc: 35.2% (3516/10000)
[Test]  Epoch: 67	Loss: 0.046564	Acc: 35.2% (3519/10000)
[Test]  Epoch: 68	Loss: 0.046614	Acc: 35.0% (3503/10000)
[Test]  Epoch: 69	Loss: 0.046565	Acc: 35.1% (3509/10000)
[Test]  Epoch: 70	Loss: 0.046551	Acc: 35.2% (3516/10000)
[Test]  Epoch: 71	Loss: 0.046547	Acc: 35.2% (3519/10000)
[Test]  Epoch: 72	Loss: 0.046569	Acc: 35.0% (3502/10000)
[Test]  Epoch: 73	Loss: 0.046525	Acc: 35.2% (3520/10000)
[Test]  Epoch: 74	Loss: 0.046535	Acc: 35.2% (3523/10000)
[Test]  Epoch: 75	Loss: 0.046549	Acc: 35.2% (3516/10000)
[Test]  Epoch: 76	Loss: 0.046510	Acc: 35.2% (3519/10000)
[Test]  Epoch: 77	Loss: 0.046529	Acc: 35.2% (3518/10000)
[Test]  Epoch: 78	Loss: 0.046562	Acc: 35.1% (3506/10000)
[Test]  Epoch: 79	Loss: 0.046580	Acc: 35.2% (3525/10000)
[Test]  Epoch: 80	Loss: 0.046520	Acc: 35.3% (3528/10000)
[Test]  Epoch: 81	Loss: 0.046487	Acc: 35.2% (3519/10000)
[Test]  Epoch: 82	Loss: 0.046546	Acc: 35.4% (3543/10000)
[Test]  Epoch: 83	Loss: 0.046600	Acc: 35.2% (3523/10000)
[Test]  Epoch: 84	Loss: 0.046587	Acc: 35.2% (3516/10000)
[Test]  Epoch: 85	Loss: 0.046560	Acc: 35.1% (3513/10000)
[Test]  Epoch: 86	Loss: 0.046552	Acc: 35.0% (3503/10000)
[Test]  Epoch: 87	Loss: 0.046512	Acc: 35.2% (3519/10000)
[Test]  Epoch: 88	Loss: 0.046613	Acc: 35.1% (3508/10000)
[Test]  Epoch: 89	Loss: 0.046540	Acc: 35.3% (3530/10000)
[Test]  Epoch: 90	Loss: 0.046562	Acc: 35.3% (3530/10000)
[Test]  Epoch: 91	Loss: 0.046559	Acc: 35.3% (3532/10000)
[Test]  Epoch: 92	Loss: 0.046501	Acc: 35.3% (3529/10000)
[Test]  Epoch: 93	Loss: 0.046576	Acc: 35.1% (3515/10000)
[Test]  Epoch: 94	Loss: 0.046522	Acc: 35.2% (3520/10000)
[Test]  Epoch: 95	Loss: 0.046530	Acc: 35.3% (3534/10000)
[Test]  Epoch: 96	Loss: 0.046525	Acc: 35.1% (3514/10000)
[Test]  Epoch: 97	Loss: 0.046540	Acc: 35.2% (3522/10000)
[Test]  Epoch: 98	Loss: 0.046522	Acc: 35.2% (3525/10000)
[Test]  Epoch: 99	Loss: 0.046525	Acc: 35.1% (3513/10000)
[Test]  Epoch: 100	Loss: 0.046543	Acc: 35.2% (3516/10000)
===========finish==========
['2024-08-19', '06:31:19.067183', '100', 'test', '0.046543370258808135', '35.16', '35.43']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.061593	Acc: 15.2% (1524/10000)
[Test]  Epoch: 2	Loss: 0.052530	Acc: 26.2% (2617/10000)
[Test]  Epoch: 3	Loss: 0.050888	Acc: 28.5% (2846/10000)
[Test]  Epoch: 4	Loss: 0.050284	Acc: 29.5% (2951/10000)
[Test]  Epoch: 5	Loss: 0.049847	Acc: 29.7% (2969/10000)
[Test]  Epoch: 6	Loss: 0.049667	Acc: 29.9% (2988/10000)
[Test]  Epoch: 7	Loss: 0.049546	Acc: 30.4% (3036/10000)
[Test]  Epoch: 8	Loss: 0.049301	Acc: 30.7% (3073/10000)
[Test]  Epoch: 9	Loss: 0.049326	Acc: 30.7% (3069/10000)
[Test]  Epoch: 10	Loss: 0.049183	Acc: 30.7% (3074/10000)
[Test]  Epoch: 11	Loss: 0.049012	Acc: 31.1% (3107/10000)
[Test]  Epoch: 12	Loss: 0.048900	Acc: 31.2% (3123/10000)
[Test]  Epoch: 13	Loss: 0.048875	Acc: 31.3% (3132/10000)
[Test]  Epoch: 14	Loss: 0.048909	Acc: 31.3% (3134/10000)
[Test]  Epoch: 15	Loss: 0.048952	Acc: 31.2% (3123/10000)
[Test]  Epoch: 16	Loss: 0.048813	Acc: 31.4% (3141/10000)
[Test]  Epoch: 17	Loss: 0.048723	Acc: 31.7% (3170/10000)
[Test]  Epoch: 18	Loss: 0.048702	Acc: 31.8% (3175/10000)
[Test]  Epoch: 19	Loss: 0.048579	Acc: 31.9% (3187/10000)
[Test]  Epoch: 20	Loss: 0.048737	Acc: 31.7% (3170/10000)
[Test]  Epoch: 21	Loss: 0.048689	Acc: 31.9% (3185/10000)
[Test]  Epoch: 22	Loss: 0.048694	Acc: 31.8% (3184/10000)
[Test]  Epoch: 23	Loss: 0.048602	Acc: 32.2% (3221/10000)
[Test]  Epoch: 24	Loss: 0.048669	Acc: 31.9% (3192/10000)
[Test]  Epoch: 25	Loss: 0.048623	Acc: 32.1% (3209/10000)
[Test]  Epoch: 26	Loss: 0.048698	Acc: 31.8% (3175/10000)
[Test]  Epoch: 27	Loss: 0.048593	Acc: 31.8% (3184/10000)
[Test]  Epoch: 28	Loss: 0.048662	Acc: 32.2% (3217/10000)
[Test]  Epoch: 29	Loss: 0.048592	Acc: 32.0% (3204/10000)
[Test]  Epoch: 30	Loss: 0.048493	Acc: 32.3% (3232/10000)
[Test]  Epoch: 31	Loss: 0.048667	Acc: 32.0% (3202/10000)
[Test]  Epoch: 32	Loss: 0.048631	Acc: 32.0% (3198/10000)
[Test]  Epoch: 33	Loss: 0.048550	Acc: 32.1% (3207/10000)
[Test]  Epoch: 34	Loss: 0.048571	Acc: 32.5% (3249/10000)
[Test]  Epoch: 35	Loss: 0.048554	Acc: 32.3% (3232/10000)
[Test]  Epoch: 36	Loss: 0.048395	Acc: 32.7% (3269/10000)
[Test]  Epoch: 37	Loss: 0.048464	Acc: 32.4% (3241/10000)
[Test]  Epoch: 38	Loss: 0.048530	Acc: 32.3% (3234/10000)
[Test]  Epoch: 39	Loss: 0.048545	Acc: 32.5% (3253/10000)
[Test]  Epoch: 40	Loss: 0.048490	Acc: 32.5% (3249/10000)
[Test]  Epoch: 41	Loss: 0.048527	Acc: 32.5% (3248/10000)
[Test]  Epoch: 42	Loss: 0.048448	Acc: 32.5% (3245/10000)
[Test]  Epoch: 43	Loss: 0.048477	Acc: 32.5% (3254/10000)
[Test]  Epoch: 44	Loss: 0.048468	Acc: 32.7% (3272/10000)
[Test]  Epoch: 45	Loss: 0.048475	Acc: 32.7% (3267/10000)
[Test]  Epoch: 46	Loss: 0.048603	Acc: 32.4% (3244/10000)
[Test]  Epoch: 47	Loss: 0.048556	Acc: 32.5% (3251/10000)
[Test]  Epoch: 48	Loss: 0.048487	Acc: 32.7% (3273/10000)
[Test]  Epoch: 49	Loss: 0.048415	Acc: 32.7% (3270/10000)
[Test]  Epoch: 50	Loss: 0.048471	Acc: 32.6% (3265/10000)
[Test]  Epoch: 51	Loss: 0.048469	Acc: 32.6% (3259/10000)
[Test]  Epoch: 52	Loss: 0.048439	Acc: 32.6% (3263/10000)
[Test]  Epoch: 53	Loss: 0.048456	Acc: 32.8% (3279/10000)
[Test]  Epoch: 54	Loss: 0.048491	Acc: 32.8% (3278/10000)
[Test]  Epoch: 55	Loss: 0.048458	Acc: 32.8% (3281/10000)
[Test]  Epoch: 56	Loss: 0.048433	Acc: 32.8% (3278/10000)
[Test]  Epoch: 57	Loss: 0.048523	Acc: 32.6% (3260/10000)
[Test]  Epoch: 58	Loss: 0.048433	Acc: 32.8% (3280/10000)
[Test]  Epoch: 59	Loss: 0.048458	Acc: 32.9% (3289/10000)
[Test]  Epoch: 60	Loss: 0.048467	Acc: 33.0% (3300/10000)
[Test]  Epoch: 61	Loss: 0.048517	Acc: 32.9% (3292/10000)
[Test]  Epoch: 62	Loss: 0.048529	Acc: 32.8% (3275/10000)
[Test]  Epoch: 63	Loss: 0.048449	Acc: 32.9% (3288/10000)
[Test]  Epoch: 64	Loss: 0.048419	Acc: 32.9% (3290/10000)
[Test]  Epoch: 65	Loss: 0.048492	Acc: 32.8% (3284/10000)
[Test]  Epoch: 66	Loss: 0.048508	Acc: 32.8% (3279/10000)
[Test]  Epoch: 67	Loss: 0.048511	Acc: 32.9% (3291/10000)
[Test]  Epoch: 68	Loss: 0.048561	Acc: 32.9% (3285/10000)
[Test]  Epoch: 69	Loss: 0.048504	Acc: 32.8% (3275/10000)
[Test]  Epoch: 70	Loss: 0.048480	Acc: 32.9% (3291/10000)
[Test]  Epoch: 71	Loss: 0.048504	Acc: 32.9% (3290/10000)
[Test]  Epoch: 72	Loss: 0.048523	Acc: 32.9% (3290/10000)
[Test]  Epoch: 73	Loss: 0.048482	Acc: 32.9% (3287/10000)
[Test]  Epoch: 74	Loss: 0.048464	Acc: 32.9% (3286/10000)
[Test]  Epoch: 75	Loss: 0.048500	Acc: 32.8% (3281/10000)
[Test]  Epoch: 76	Loss: 0.048477	Acc: 33.1% (3311/10000)
[Test]  Epoch: 77	Loss: 0.048455	Acc: 33.0% (3305/10000)
[Test]  Epoch: 78	Loss: 0.048489	Acc: 32.8% (3276/10000)
[Test]  Epoch: 79	Loss: 0.048513	Acc: 32.8% (3280/10000)
[Test]  Epoch: 80	Loss: 0.048438	Acc: 33.0% (3303/10000)
[Test]  Epoch: 81	Loss: 0.048428	Acc: 32.9% (3288/10000)
[Test]  Epoch: 82	Loss: 0.048498	Acc: 32.6% (3261/10000)
[Test]  Epoch: 83	Loss: 0.048527	Acc: 32.6% (3263/10000)
[Test]  Epoch: 84	Loss: 0.048511	Acc: 32.9% (3294/10000)
[Test]  Epoch: 85	Loss: 0.048497	Acc: 32.8% (3281/10000)
[Test]  Epoch: 86	Loss: 0.048472	Acc: 32.7% (3273/10000)
[Test]  Epoch: 87	Loss: 0.048465	Acc: 32.9% (3286/10000)
[Test]  Epoch: 88	Loss: 0.048534	Acc: 32.9% (3290/10000)
[Test]  Epoch: 89	Loss: 0.048471	Acc: 32.9% (3293/10000)
[Test]  Epoch: 90	Loss: 0.048476	Acc: 33.0% (3305/10000)
[Test]  Epoch: 91	Loss: 0.048487	Acc: 33.0% (3299/10000)
[Test]  Epoch: 92	Loss: 0.048425	Acc: 32.8% (3282/10000)
[Test]  Epoch: 93	Loss: 0.048501	Acc: 32.9% (3287/10000)
[Test]  Epoch: 94	Loss: 0.048460	Acc: 32.8% (3283/10000)
[Test]  Epoch: 95	Loss: 0.048467	Acc: 32.9% (3292/10000)
[Test]  Epoch: 96	Loss: 0.048447	Acc: 33.0% (3299/10000)
[Test]  Epoch: 97	Loss: 0.048480	Acc: 32.9% (3285/10000)
[Test]  Epoch: 98	Loss: 0.048452	Acc: 32.8% (3278/10000)
[Test]  Epoch: 99	Loss: 0.048467	Acc: 32.9% (3287/10000)
[Test]  Epoch: 100	Loss: 0.048483	Acc: 32.8% (3276/10000)
===========finish==========
['2024-08-19', '06:34:12.156221', '100', 'test', '0.048483271002769474', '32.76', '33.11']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.067378	Acc: 10.6% (1056/10000)
[Test]  Epoch: 2	Loss: 0.054739	Acc: 22.8% (2281/10000)
[Test]  Epoch: 3	Loss: 0.052960	Acc: 25.1% (2512/10000)
[Test]  Epoch: 4	Loss: 0.052173	Acc: 26.4% (2638/10000)
[Test]  Epoch: 5	Loss: 0.051573	Acc: 27.2% (2722/10000)
[Test]  Epoch: 6	Loss: 0.051329	Acc: 27.5% (2747/10000)
[Test]  Epoch: 7	Loss: 0.051154	Acc: 27.8% (2775/10000)
[Test]  Epoch: 8	Loss: 0.050922	Acc: 28.0% (2803/10000)
[Test]  Epoch: 9	Loss: 0.050846	Acc: 28.4% (2836/10000)
[Test]  Epoch: 10	Loss: 0.050765	Acc: 28.3% (2830/10000)
[Test]  Epoch: 11	Loss: 0.050577	Acc: 28.7% (2871/10000)
[Test]  Epoch: 12	Loss: 0.050461	Acc: 28.7% (2868/10000)
[Test]  Epoch: 13	Loss: 0.050382	Acc: 29.0% (2897/10000)
[Test]  Epoch: 14	Loss: 0.050345	Acc: 29.0% (2901/10000)
[Test]  Epoch: 15	Loss: 0.050370	Acc: 29.1% (2910/10000)
[Test]  Epoch: 16	Loss: 0.050294	Acc: 29.1% (2905/10000)
[Test]  Epoch: 17	Loss: 0.050154	Acc: 29.2% (2925/10000)
[Test]  Epoch: 18	Loss: 0.050199	Acc: 29.2% (2923/10000)
[Test]  Epoch: 19	Loss: 0.050043	Acc: 29.6% (2961/10000)
[Test]  Epoch: 20	Loss: 0.050159	Acc: 29.4% (2945/10000)
[Test]  Epoch: 21	Loss: 0.050150	Acc: 29.4% (2938/10000)
[Test]  Epoch: 22	Loss: 0.050144	Acc: 29.6% (2955/10000)
[Test]  Epoch: 23	Loss: 0.050046	Acc: 29.7% (2966/10000)
[Test]  Epoch: 24	Loss: 0.050082	Acc: 29.7% (2974/10000)
[Test]  Epoch: 25	Loss: 0.050096	Acc: 29.7% (2973/10000)
[Test]  Epoch: 26	Loss: 0.050114	Acc: 29.6% (2962/10000)
[Test]  Epoch: 27	Loss: 0.049996	Acc: 29.6% (2960/10000)
[Test]  Epoch: 28	Loss: 0.050080	Acc: 30.0% (2996/10000)
[Test]  Epoch: 29	Loss: 0.049987	Acc: 29.8% (2982/10000)
[Test]  Epoch: 30	Loss: 0.049950	Acc: 30.0% (3000/10000)
[Test]  Epoch: 31	Loss: 0.050100	Acc: 29.8% (2981/10000)
[Test]  Epoch: 32	Loss: 0.050031	Acc: 29.8% (2976/10000)
[Test]  Epoch: 33	Loss: 0.050010	Acc: 29.9% (2992/10000)
[Test]  Epoch: 34	Loss: 0.049935	Acc: 29.9% (2987/10000)
[Test]  Epoch: 35	Loss: 0.049932	Acc: 29.9% (2995/10000)
[Test]  Epoch: 36	Loss: 0.049785	Acc: 30.4% (3037/10000)
[Test]  Epoch: 37	Loss: 0.049867	Acc: 30.2% (3017/10000)
[Test]  Epoch: 38	Loss: 0.049941	Acc: 30.1% (3009/10000)
[Test]  Epoch: 39	Loss: 0.049894	Acc: 30.1% (3006/10000)
[Test]  Epoch: 40	Loss: 0.049895	Acc: 30.2% (3020/10000)
[Test]  Epoch: 41	Loss: 0.050013	Acc: 30.1% (3006/10000)
[Test]  Epoch: 42	Loss: 0.049899	Acc: 30.2% (3017/10000)
[Test]  Epoch: 43	Loss: 0.049879	Acc: 30.3% (3028/10000)
[Test]  Epoch: 44	Loss: 0.049873	Acc: 30.4% (3036/10000)
[Test]  Epoch: 45	Loss: 0.049867	Acc: 30.3% (3026/10000)
[Test]  Epoch: 46	Loss: 0.049938	Acc: 30.1% (3012/10000)
[Test]  Epoch: 47	Loss: 0.049967	Acc: 30.4% (3041/10000)
[Test]  Epoch: 48	Loss: 0.049859	Acc: 30.4% (3040/10000)
[Test]  Epoch: 49	Loss: 0.049813	Acc: 30.5% (3047/10000)
[Test]  Epoch: 50	Loss: 0.049910	Acc: 30.2% (3018/10000)
[Test]  Epoch: 51	Loss: 0.049883	Acc: 30.3% (3031/10000)
[Test]  Epoch: 52	Loss: 0.049826	Acc: 30.4% (3039/10000)
[Test]  Epoch: 53	Loss: 0.049817	Acc: 30.4% (3043/10000)
[Test]  Epoch: 54	Loss: 0.049908	Acc: 30.5% (3048/10000)
[Test]  Epoch: 55	Loss: 0.049881	Acc: 30.6% (3064/10000)
[Test]  Epoch: 56	Loss: 0.049903	Acc: 30.4% (3045/10000)
[Test]  Epoch: 57	Loss: 0.049949	Acc: 30.4% (3037/10000)
[Test]  Epoch: 58	Loss: 0.049877	Acc: 30.4% (3039/10000)
[Test]  Epoch: 59	Loss: 0.049903	Acc: 30.5% (3047/10000)
[Test]  Epoch: 60	Loss: 0.049842	Acc: 30.5% (3049/10000)
[Test]  Epoch: 61	Loss: 0.049899	Acc: 30.5% (3048/10000)
[Test]  Epoch: 62	Loss: 0.049907	Acc: 30.5% (3050/10000)
[Test]  Epoch: 63	Loss: 0.049838	Acc: 30.5% (3052/10000)
[Test]  Epoch: 64	Loss: 0.049840	Acc: 30.6% (3056/10000)
[Test]  Epoch: 65	Loss: 0.049897	Acc: 30.4% (3038/10000)
[Test]  Epoch: 66	Loss: 0.049891	Acc: 30.6% (3059/10000)
[Test]  Epoch: 67	Loss: 0.049904	Acc: 30.6% (3062/10000)
[Test]  Epoch: 68	Loss: 0.049955	Acc: 30.3% (3030/10000)
[Test]  Epoch: 69	Loss: 0.049905	Acc: 30.5% (3054/10000)
[Test]  Epoch: 70	Loss: 0.049887	Acc: 30.5% (3049/10000)
[Test]  Epoch: 71	Loss: 0.049893	Acc: 30.5% (3046/10000)
[Test]  Epoch: 72	Loss: 0.049900	Acc: 30.3% (3033/10000)
[Test]  Epoch: 73	Loss: 0.049854	Acc: 30.6% (3061/10000)
[Test]  Epoch: 74	Loss: 0.049845	Acc: 30.5% (3054/10000)
[Test]  Epoch: 75	Loss: 0.049885	Acc: 30.4% (3045/10000)
[Test]  Epoch: 76	Loss: 0.049847	Acc: 30.6% (3058/10000)
[Test]  Epoch: 77	Loss: 0.049863	Acc: 30.6% (3061/10000)
[Test]  Epoch: 78	Loss: 0.049896	Acc: 30.4% (3038/10000)
[Test]  Epoch: 79	Loss: 0.049889	Acc: 30.6% (3061/10000)
[Test]  Epoch: 80	Loss: 0.049809	Acc: 30.6% (3055/10000)
[Test]  Epoch: 81	Loss: 0.049807	Acc: 30.6% (3059/10000)
[Test]  Epoch: 82	Loss: 0.049865	Acc: 30.5% (3051/10000)
[Test]  Epoch: 83	Loss: 0.049935	Acc: 30.3% (3028/10000)
[Test]  Epoch: 84	Loss: 0.049915	Acc: 30.6% (3060/10000)
[Test]  Epoch: 85	Loss: 0.049883	Acc: 30.5% (3047/10000)
[Test]  Epoch: 86	Loss: 0.049885	Acc: 30.4% (3035/10000)
[Test]  Epoch: 87	Loss: 0.049858	Acc: 30.5% (3053/10000)
[Test]  Epoch: 88	Loss: 0.049924	Acc: 30.5% (3054/10000)
[Test]  Epoch: 89	Loss: 0.049873	Acc: 30.5% (3048/10000)
[Test]  Epoch: 90	Loss: 0.049876	Acc: 30.5% (3052/10000)
[Test]  Epoch: 91	Loss: 0.049876	Acc: 30.5% (3054/10000)
[Test]  Epoch: 92	Loss: 0.049795	Acc: 30.7% (3070/10000)
[Test]  Epoch: 93	Loss: 0.049869	Acc: 30.5% (3051/10000)
[Test]  Epoch: 94	Loss: 0.049833	Acc: 30.7% (3066/10000)
[Test]  Epoch: 95	Loss: 0.049856	Acc: 30.6% (3059/10000)
[Test]  Epoch: 96	Loss: 0.049830	Acc: 30.7% (3066/10000)
[Test]  Epoch: 97	Loss: 0.049882	Acc: 30.6% (3056/10000)
[Test]  Epoch: 98	Loss: 0.049845	Acc: 30.7% (3072/10000)
[Test]  Epoch: 99	Loss: 0.049846	Acc: 30.8% (3082/10000)
[Test]  Epoch: 100	Loss: 0.049844	Acc: 30.6% (3055/10000)
===========finish==========
['2024-08-19', '06:37:07.810615', '100', 'test', '0.04984430859088898', '30.55', '30.82']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.069650	Acc: 8.5% (851/10000)
[Test]  Epoch: 2	Loss: 0.056950	Acc: 19.1% (1909/10000)
[Test]  Epoch: 3	Loss: 0.055528	Acc: 21.0% (2102/10000)
[Test]  Epoch: 4	Loss: 0.054882	Acc: 22.1% (2209/10000)
[Test]  Epoch: 5	Loss: 0.054346	Acc: 22.9% (2286/10000)
[Test]  Epoch: 6	Loss: 0.054035	Acc: 23.4% (2340/10000)
[Test]  Epoch: 7	Loss: 0.053830	Acc: 23.6% (2356/10000)
[Test]  Epoch: 8	Loss: 0.053529	Acc: 24.0% (2398/10000)
[Test]  Epoch: 9	Loss: 0.053650	Acc: 23.9% (2390/10000)
[Test]  Epoch: 10	Loss: 0.053299	Acc: 24.2% (2417/10000)
[Test]  Epoch: 11	Loss: 0.053361	Acc: 24.5% (2446/10000)
[Test]  Epoch: 12	Loss: 0.053061	Acc: 24.7% (2470/10000)
[Test]  Epoch: 13	Loss: 0.053076	Acc: 24.9% (2487/10000)
[Test]  Epoch: 14	Loss: 0.052932	Acc: 25.2% (2520/10000)
[Test]  Epoch: 15	Loss: 0.053112	Acc: 24.8% (2484/10000)
[Test]  Epoch: 16	Loss: 0.052873	Acc: 25.3% (2534/10000)
[Test]  Epoch: 17	Loss: 0.052676	Acc: 25.3% (2532/10000)
[Test]  Epoch: 18	Loss: 0.052865	Acc: 25.2% (2521/10000)
[Test]  Epoch: 19	Loss: 0.052779	Acc: 25.2% (2519/10000)
[Test]  Epoch: 20	Loss: 0.052725	Acc: 25.6% (2559/10000)
[Test]  Epoch: 21	Loss: 0.052767	Acc: 25.2% (2525/10000)
[Test]  Epoch: 22	Loss: 0.052835	Acc: 25.5% (2554/10000)
[Test]  Epoch: 23	Loss: 0.052592	Acc: 25.8% (2583/10000)
[Test]  Epoch: 24	Loss: 0.052629	Acc: 25.8% (2577/10000)
[Test]  Epoch: 25	Loss: 0.052735	Acc: 25.8% (2576/10000)
[Test]  Epoch: 26	Loss: 0.052547	Acc: 25.8% (2582/10000)
[Test]  Epoch: 27	Loss: 0.052501	Acc: 25.9% (2588/10000)
[Test]  Epoch: 28	Loss: 0.052533	Acc: 25.9% (2594/10000)
[Test]  Epoch: 29	Loss: 0.052643	Acc: 25.8% (2577/10000)
[Test]  Epoch: 30	Loss: 0.052441	Acc: 26.2% (2617/10000)
[Test]  Epoch: 31	Loss: 0.052582	Acc: 25.9% (2593/10000)
[Test]  Epoch: 32	Loss: 0.052566	Acc: 26.1% (2611/10000)
[Test]  Epoch: 33	Loss: 0.052594	Acc: 26.0% (2600/10000)
[Test]  Epoch: 34	Loss: 0.052413	Acc: 26.4% (2635/10000)
[Test]  Epoch: 35	Loss: 0.052509	Acc: 26.3% (2627/10000)
[Test]  Epoch: 36	Loss: 0.052462	Acc: 26.5% (2647/10000)
[Test]  Epoch: 37	Loss: 0.052405	Acc: 26.6% (2658/10000)
[Test]  Epoch: 38	Loss: 0.052459	Acc: 26.4% (2636/10000)
[Test]  Epoch: 39	Loss: 0.052384	Acc: 26.5% (2649/10000)
[Test]  Epoch: 40	Loss: 0.052540	Acc: 26.2% (2621/10000)
[Test]  Epoch: 41	Loss: 0.052532	Acc: 26.2% (2621/10000)
[Test]  Epoch: 42	Loss: 0.052336	Acc: 26.5% (2654/10000)
[Test]  Epoch: 43	Loss: 0.052402	Acc: 26.4% (2643/10000)
[Test]  Epoch: 44	Loss: 0.052491	Acc: 26.4% (2641/10000)
[Test]  Epoch: 45	Loss: 0.052297	Acc: 26.8% (2684/10000)
[Test]  Epoch: 46	Loss: 0.052397	Acc: 26.4% (2640/10000)
[Test]  Epoch: 47	Loss: 0.052550	Acc: 26.6% (2665/10000)
[Test]  Epoch: 48	Loss: 0.052335	Acc: 26.7% (2667/10000)
[Test]  Epoch: 49	Loss: 0.052362	Acc: 26.8% (2676/10000)
[Test]  Epoch: 50	Loss: 0.052344	Acc: 26.8% (2680/10000)
[Test]  Epoch: 51	Loss: 0.052404	Acc: 26.7% (2671/10000)
[Test]  Epoch: 52	Loss: 0.052317	Acc: 27.0% (2701/10000)
[Test]  Epoch: 53	Loss: 0.052391	Acc: 26.9% (2685/10000)
[Test]  Epoch: 54	Loss: 0.052400	Acc: 26.9% (2687/10000)
[Test]  Epoch: 55	Loss: 0.052398	Acc: 26.9% (2690/10000)
[Test]  Epoch: 56	Loss: 0.052236	Acc: 27.0% (2701/10000)
[Test]  Epoch: 57	Loss: 0.052511	Acc: 26.8% (2675/10000)
[Test]  Epoch: 58	Loss: 0.052252	Acc: 27.0% (2700/10000)
[Test]  Epoch: 59	Loss: 0.052398	Acc: 26.8% (2683/10000)
[Test]  Epoch: 60	Loss: 0.052384	Acc: 26.8% (2675/10000)
[Test]  Epoch: 61	Loss: 0.052416	Acc: 26.8% (2677/10000)
[Test]  Epoch: 62	Loss: 0.052390	Acc: 26.8% (2682/10000)
[Test]  Epoch: 63	Loss: 0.052268	Acc: 26.9% (2687/10000)
[Test]  Epoch: 64	Loss: 0.052313	Acc: 26.9% (2689/10000)
[Test]  Epoch: 65	Loss: 0.052372	Acc: 26.9% (2687/10000)
[Test]  Epoch: 66	Loss: 0.052354	Acc: 26.7% (2669/10000)
[Test]  Epoch: 67	Loss: 0.052417	Acc: 26.8% (2675/10000)
[Test]  Epoch: 68	Loss: 0.052480	Acc: 26.7% (2671/10000)
[Test]  Epoch: 69	Loss: 0.052362	Acc: 26.8% (2681/10000)
[Test]  Epoch: 70	Loss: 0.052359	Acc: 26.9% (2694/10000)
[Test]  Epoch: 71	Loss: 0.052386	Acc: 26.8% (2677/10000)
[Test]  Epoch: 72	Loss: 0.052415	Acc: 26.8% (2675/10000)
[Test]  Epoch: 73	Loss: 0.052338	Acc: 26.9% (2694/10000)
[Test]  Epoch: 74	Loss: 0.052318	Acc: 27.0% (2699/10000)
[Test]  Epoch: 75	Loss: 0.052364	Acc: 26.8% (2675/10000)
[Test]  Epoch: 76	Loss: 0.052344	Acc: 27.0% (2698/10000)
[Test]  Epoch: 77	Loss: 0.052362	Acc: 27.0% (2700/10000)
[Test]  Epoch: 78	Loss: 0.052406	Acc: 26.9% (2687/10000)
[Test]  Epoch: 79	Loss: 0.052408	Acc: 26.8% (2682/10000)
[Test]  Epoch: 80	Loss: 0.052342	Acc: 26.9% (2692/10000)
[Test]  Epoch: 81	Loss: 0.052320	Acc: 26.9% (2692/10000)
[Test]  Epoch: 82	Loss: 0.052349	Acc: 26.6% (2662/10000)
[Test]  Epoch: 83	Loss: 0.052421	Acc: 26.8% (2679/10000)
[Test]  Epoch: 84	Loss: 0.052402	Acc: 26.8% (2684/10000)
[Test]  Epoch: 85	Loss: 0.052393	Acc: 26.7% (2671/10000)
[Test]  Epoch: 86	Loss: 0.052400	Acc: 26.8% (2675/10000)
[Test]  Epoch: 87	Loss: 0.052359	Acc: 26.8% (2683/10000)
[Test]  Epoch: 88	Loss: 0.052376	Acc: 26.7% (2674/10000)
[Test]  Epoch: 89	Loss: 0.052376	Acc: 26.7% (2669/10000)
[Test]  Epoch: 90	Loss: 0.052374	Acc: 26.8% (2684/10000)
[Test]  Epoch: 91	Loss: 0.052375	Acc: 26.9% (2691/10000)
[Test]  Epoch: 92	Loss: 0.052282	Acc: 27.0% (2696/10000)
[Test]  Epoch: 93	Loss: 0.052363	Acc: 26.7% (2674/10000)
[Test]  Epoch: 94	Loss: 0.052355	Acc: 27.0% (2702/10000)
[Test]  Epoch: 95	Loss: 0.052333	Acc: 26.9% (2694/10000)
[Test]  Epoch: 96	Loss: 0.052340	Acc: 26.8% (2681/10000)
[Test]  Epoch: 97	Loss: 0.052363	Acc: 26.8% (2675/10000)
[Test]  Epoch: 98	Loss: 0.052362	Acc: 27.0% (2700/10000)
[Test]  Epoch: 99	Loss: 0.052334	Acc: 26.9% (2685/10000)
[Test]  Epoch: 100	Loss: 0.052338	Acc: 26.9% (2693/10000)
===========finish==========
['2024-08-19', '06:40:03.880924', '100', 'test', '0.05233769428730011', '26.93', '27.02']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.070007	Acc: 8.7% (866/10000)
[Test]  Epoch: 2	Loss: 0.056936	Acc: 19.0% (1904/10000)
[Test]  Epoch: 3	Loss: 0.055181	Acc: 21.4% (2144/10000)
[Test]  Epoch: 4	Loss: 0.054527	Acc: 22.5% (2254/10000)
[Test]  Epoch: 5	Loss: 0.054042	Acc: 23.2% (2322/10000)
[Test]  Epoch: 6	Loss: 0.053610	Acc: 23.9% (2391/10000)
[Test]  Epoch: 7	Loss: 0.053434	Acc: 23.9% (2387/10000)
[Test]  Epoch: 8	Loss: 0.053063	Acc: 24.3% (2430/10000)
[Test]  Epoch: 9	Loss: 0.053194	Acc: 24.5% (2450/10000)
[Test]  Epoch: 10	Loss: 0.053155	Acc: 24.5% (2448/10000)
[Test]  Epoch: 11	Loss: 0.052847	Acc: 25.0% (2500/10000)
[Test]  Epoch: 12	Loss: 0.052776	Acc: 25.1% (2506/10000)
[Test]  Epoch: 13	Loss: 0.052796	Acc: 24.9% (2495/10000)
[Test]  Epoch: 14	Loss: 0.052510	Acc: 25.2% (2516/10000)
[Test]  Epoch: 15	Loss: 0.052748	Acc: 25.1% (2512/10000)
[Test]  Epoch: 16	Loss: 0.052410	Acc: 25.6% (2555/10000)
[Test]  Epoch: 17	Loss: 0.052330	Acc: 25.6% (2561/10000)
[Test]  Epoch: 18	Loss: 0.052722	Acc: 25.1% (2509/10000)
[Test]  Epoch: 19	Loss: 0.052299	Acc: 26.1% (2612/10000)
[Test]  Epoch: 20	Loss: 0.052615	Acc: 25.7% (2566/10000)
[Test]  Epoch: 21	Loss: 0.052475	Acc: 25.7% (2568/10000)
[Test]  Epoch: 22	Loss: 0.052525	Acc: 25.7% (2568/10000)
[Test]  Epoch: 23	Loss: 0.052590	Acc: 25.4% (2541/10000)
[Test]  Epoch: 24	Loss: 0.052298	Acc: 26.0% (2604/10000)
[Test]  Epoch: 25	Loss: 0.052334	Acc: 26.2% (2620/10000)
[Test]  Epoch: 26	Loss: 0.052442	Acc: 25.9% (2591/10000)
[Test]  Epoch: 27	Loss: 0.052126	Acc: 26.5% (2648/10000)
[Test]  Epoch: 28	Loss: 0.052430	Acc: 26.0% (2596/10000)
[Test]  Epoch: 29	Loss: 0.052427	Acc: 26.2% (2617/10000)
[Test]  Epoch: 30	Loss: 0.052351	Acc: 25.9% (2588/10000)
[Test]  Epoch: 31	Loss: 0.052658	Acc: 25.6% (2555/10000)
[Test]  Epoch: 32	Loss: 0.052402	Acc: 25.9% (2586/10000)
[Test]  Epoch: 33	Loss: 0.052390	Acc: 26.0% (2602/10000)
[Test]  Epoch: 34	Loss: 0.052487	Acc: 26.1% (2611/10000)
[Test]  Epoch: 35	Loss: 0.052447	Acc: 26.3% (2633/10000)
[Test]  Epoch: 36	Loss: 0.052433	Acc: 26.2% (2620/10000)
[Test]  Epoch: 37	Loss: 0.052332	Acc: 26.3% (2630/10000)
[Test]  Epoch: 38	Loss: 0.052194	Acc: 26.4% (2643/10000)
[Test]  Epoch: 39	Loss: 0.052450	Acc: 26.3% (2632/10000)
[Test]  Epoch: 40	Loss: 0.052472	Acc: 26.3% (2627/10000)
[Test]  Epoch: 41	Loss: 0.052326	Acc: 26.8% (2679/10000)
[Test]  Epoch: 42	Loss: 0.052333	Acc: 26.4% (2645/10000)
[Test]  Epoch: 43	Loss: 0.052429	Acc: 26.6% (2661/10000)
[Test]  Epoch: 44	Loss: 0.052379	Acc: 26.6% (2663/10000)
[Test]  Epoch: 45	Loss: 0.052315	Acc: 26.4% (2643/10000)
[Test]  Epoch: 46	Loss: 0.052355	Acc: 26.4% (2636/10000)
[Test]  Epoch: 47	Loss: 0.052593	Acc: 26.4% (2635/10000)
[Test]  Epoch: 48	Loss: 0.052746	Acc: 26.1% (2614/10000)
[Test]  Epoch: 49	Loss: 0.052420	Acc: 26.2% (2624/10000)
[Test]  Epoch: 50	Loss: 0.052154	Acc: 26.9% (2690/10000)
[Test]  Epoch: 51	Loss: 0.052549	Acc: 26.4% (2635/10000)
[Test]  Epoch: 52	Loss: 0.052634	Acc: 26.2% (2621/10000)
[Test]  Epoch: 53	Loss: 0.052574	Acc: 26.1% (2614/10000)
[Test]  Epoch: 54	Loss: 0.052463	Acc: 26.4% (2636/10000)
[Test]  Epoch: 55	Loss: 0.052545	Acc: 26.6% (2658/10000)
[Test]  Epoch: 56	Loss: 0.052383	Acc: 26.6% (2656/10000)
[Test]  Epoch: 57	Loss: 0.052423	Acc: 26.5% (2650/10000)
[Test]  Epoch: 58	Loss: 0.052359	Acc: 26.5% (2648/10000)
[Test]  Epoch: 59	Loss: 0.052415	Acc: 26.6% (2656/10000)
[Test]  Epoch: 60	Loss: 0.052555	Acc: 26.6% (2656/10000)
[Test]  Epoch: 61	Loss: 0.052494	Acc: 26.5% (2650/10000)
[Test]  Epoch: 62	Loss: 0.052441	Acc: 26.7% (2669/10000)
[Test]  Epoch: 63	Loss: 0.052310	Acc: 26.8% (2678/10000)
[Test]  Epoch: 64	Loss: 0.052368	Acc: 26.8% (2678/10000)
[Test]  Epoch: 65	Loss: 0.052436	Acc: 26.7% (2672/10000)
[Test]  Epoch: 66	Loss: 0.052422	Acc: 26.7% (2669/10000)
[Test]  Epoch: 67	Loss: 0.052483	Acc: 26.8% (2675/10000)
[Test]  Epoch: 68	Loss: 0.052551	Acc: 26.4% (2643/10000)
[Test]  Epoch: 69	Loss: 0.052444	Acc: 26.6% (2659/10000)
[Test]  Epoch: 70	Loss: 0.052439	Acc: 26.5% (2649/10000)
[Test]  Epoch: 71	Loss: 0.052460	Acc: 26.7% (2670/10000)
[Test]  Epoch: 72	Loss: 0.052514	Acc: 26.6% (2662/10000)
[Test]  Epoch: 73	Loss: 0.052410	Acc: 26.7% (2674/10000)
[Test]  Epoch: 74	Loss: 0.052383	Acc: 26.8% (2680/10000)
[Test]  Epoch: 75	Loss: 0.052441	Acc: 26.7% (2671/10000)
[Test]  Epoch: 76	Loss: 0.052430	Acc: 26.8% (2682/10000)
[Test]  Epoch: 77	Loss: 0.052445	Acc: 26.6% (2661/10000)
[Test]  Epoch: 78	Loss: 0.052495	Acc: 26.6% (2662/10000)
[Test]  Epoch: 79	Loss: 0.052489	Acc: 26.7% (2666/10000)
[Test]  Epoch: 80	Loss: 0.052451	Acc: 26.7% (2674/10000)
[Test]  Epoch: 81	Loss: 0.052407	Acc: 26.6% (2664/10000)
[Test]  Epoch: 82	Loss: 0.052431	Acc: 26.6% (2665/10000)
[Test]  Epoch: 83	Loss: 0.052478	Acc: 26.7% (2670/10000)
[Test]  Epoch: 84	Loss: 0.052493	Acc: 26.7% (2669/10000)
[Test]  Epoch: 85	Loss: 0.052498	Acc: 26.6% (2661/10000)
[Test]  Epoch: 86	Loss: 0.052514	Acc: 26.6% (2659/10000)
[Test]  Epoch: 87	Loss: 0.052523	Acc: 26.6% (2665/10000)
[Test]  Epoch: 88	Loss: 0.052495	Acc: 26.5% (2654/10000)
[Test]  Epoch: 89	Loss: 0.052483	Acc: 26.7% (2671/10000)
[Test]  Epoch: 90	Loss: 0.052512	Acc: 26.7% (2666/10000)
[Test]  Epoch: 91	Loss: 0.052479	Acc: 26.7% (2672/10000)
[Test]  Epoch: 92	Loss: 0.052382	Acc: 26.9% (2690/10000)
[Test]  Epoch: 93	Loss: 0.052475	Acc: 26.8% (2677/10000)
[Test]  Epoch: 94	Loss: 0.052459	Acc: 26.7% (2671/10000)
[Test]  Epoch: 95	Loss: 0.052437	Acc: 26.8% (2681/10000)
[Test]  Epoch: 96	Loss: 0.052433	Acc: 26.9% (2686/10000)
[Test]  Epoch: 97	Loss: 0.052474	Acc: 26.7% (2671/10000)
[Test]  Epoch: 98	Loss: 0.052492	Acc: 26.6% (2665/10000)
[Test]  Epoch: 99	Loss: 0.052453	Acc: 26.8% (2679/10000)
[Test]  Epoch: 100	Loss: 0.052449	Acc: 26.6% (2663/10000)
===========finish==========
['2024-08-19', '06:42:57.293492', '100', 'test', '0.05244900982379913', '26.63', '26.9']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.069953	Acc: 6.9% (689/10000)
[Test]  Epoch: 2	Loss: 0.059897	Acc: 14.8% (1481/10000)
[Test]  Epoch: 3	Loss: 0.058043	Acc: 17.4% (1738/10000)
[Test]  Epoch: 4	Loss: 0.057187	Acc: 18.5% (1852/10000)
[Test]  Epoch: 5	Loss: 0.056504	Acc: 19.7% (1966/10000)
[Test]  Epoch: 6	Loss: 0.056095	Acc: 20.4% (2042/10000)
[Test]  Epoch: 7	Loss: 0.055904	Acc: 20.9% (2086/10000)
[Test]  Epoch: 8	Loss: 0.055536	Acc: 21.2% (2119/10000)
[Test]  Epoch: 9	Loss: 0.055332	Acc: 21.7% (2173/10000)
[Test]  Epoch: 10	Loss: 0.055176	Acc: 22.0% (2201/10000)
[Test]  Epoch: 11	Loss: 0.055064	Acc: 22.3% (2228/10000)
[Test]  Epoch: 12	Loss: 0.054861	Acc: 22.6% (2257/10000)
[Test]  Epoch: 13	Loss: 0.054773	Acc: 22.7% (2266/10000)
[Test]  Epoch: 14	Loss: 0.054692	Acc: 22.8% (2279/10000)
[Test]  Epoch: 15	Loss: 0.054713	Acc: 22.6% (2259/10000)
[Test]  Epoch: 16	Loss: 0.054593	Acc: 22.8% (2283/10000)
[Test]  Epoch: 17	Loss: 0.054460	Acc: 23.1% (2314/10000)
[Test]  Epoch: 18	Loss: 0.054516	Acc: 23.2% (2319/10000)
[Test]  Epoch: 19	Loss: 0.054411	Acc: 23.4% (2341/10000)
[Test]  Epoch: 20	Loss: 0.054411	Acc: 23.4% (2338/10000)
[Test]  Epoch: 21	Loss: 0.054361	Acc: 23.5% (2346/10000)
[Test]  Epoch: 22	Loss: 0.054331	Acc: 23.5% (2350/10000)
[Test]  Epoch: 23	Loss: 0.054230	Acc: 23.6% (2359/10000)
[Test]  Epoch: 24	Loss: 0.054274	Acc: 23.5% (2349/10000)
[Test]  Epoch: 25	Loss: 0.054262	Acc: 23.6% (2360/10000)
[Test]  Epoch: 26	Loss: 0.054209	Acc: 23.6% (2365/10000)
[Test]  Epoch: 27	Loss: 0.054073	Acc: 23.9% (2385/10000)
[Test]  Epoch: 28	Loss: 0.054122	Acc: 23.9% (2392/10000)
[Test]  Epoch: 29	Loss: 0.054105	Acc: 23.8% (2376/10000)
[Test]  Epoch: 30	Loss: 0.054068	Acc: 24.0% (2397/10000)
[Test]  Epoch: 31	Loss: 0.054087	Acc: 23.8% (2377/10000)
[Test]  Epoch: 32	Loss: 0.054148	Acc: 23.8% (2377/10000)
[Test]  Epoch: 33	Loss: 0.053999	Acc: 24.1% (2406/10000)
[Test]  Epoch: 34	Loss: 0.053955	Acc: 24.1% (2415/10000)
[Test]  Epoch: 35	Loss: 0.053973	Acc: 24.1% (2406/10000)
[Test]  Epoch: 36	Loss: 0.053907	Acc: 24.2% (2417/10000)
[Test]  Epoch: 37	Loss: 0.053924	Acc: 24.3% (2432/10000)
[Test]  Epoch: 38	Loss: 0.053900	Acc: 24.3% (2426/10000)
[Test]  Epoch: 39	Loss: 0.053973	Acc: 24.1% (2415/10000)
[Test]  Epoch: 40	Loss: 0.053944	Acc: 24.3% (2430/10000)
[Test]  Epoch: 41	Loss: 0.053922	Acc: 24.3% (2433/10000)
[Test]  Epoch: 42	Loss: 0.053887	Acc: 24.3% (2428/10000)
[Test]  Epoch: 43	Loss: 0.053861	Acc: 24.4% (2439/10000)
[Test]  Epoch: 44	Loss: 0.053859	Acc: 24.6% (2460/10000)
[Test]  Epoch: 45	Loss: 0.053846	Acc: 24.3% (2432/10000)
[Test]  Epoch: 46	Loss: 0.053918	Acc: 24.4% (2442/10000)
[Test]  Epoch: 47	Loss: 0.053848	Acc: 24.3% (2431/10000)
[Test]  Epoch: 48	Loss: 0.053829	Acc: 24.4% (2445/10000)
[Test]  Epoch: 49	Loss: 0.053782	Acc: 24.5% (2452/10000)
[Test]  Epoch: 50	Loss: 0.053798	Acc: 24.6% (2458/10000)
[Test]  Epoch: 51	Loss: 0.053844	Acc: 24.3% (2433/10000)
[Test]  Epoch: 52	Loss: 0.053817	Acc: 24.4% (2440/10000)
[Test]  Epoch: 53	Loss: 0.053853	Acc: 24.4% (2440/10000)
[Test]  Epoch: 54	Loss: 0.053823	Acc: 24.5% (2449/10000)
[Test]  Epoch: 55	Loss: 0.053846	Acc: 24.5% (2448/10000)
[Test]  Epoch: 56	Loss: 0.053827	Acc: 24.5% (2449/10000)
[Test]  Epoch: 57	Loss: 0.053849	Acc: 24.6% (2457/10000)
[Test]  Epoch: 58	Loss: 0.053654	Acc: 24.9% (2495/10000)
[Test]  Epoch: 59	Loss: 0.053739	Acc: 24.5% (2451/10000)
[Test]  Epoch: 60	Loss: 0.053776	Acc: 24.6% (2459/10000)
[Test]  Epoch: 61	Loss: 0.053804	Acc: 24.5% (2449/10000)
[Test]  Epoch: 62	Loss: 0.053785	Acc: 24.6% (2461/10000)
[Test]  Epoch: 63	Loss: 0.053706	Acc: 24.6% (2462/10000)
[Test]  Epoch: 64	Loss: 0.053730	Acc: 24.8% (2477/10000)
[Test]  Epoch: 65	Loss: 0.053792	Acc: 24.6% (2461/10000)
[Test]  Epoch: 66	Loss: 0.053754	Acc: 24.7% (2466/10000)
[Test]  Epoch: 67	Loss: 0.053814	Acc: 24.5% (2450/10000)
[Test]  Epoch: 68	Loss: 0.053876	Acc: 24.4% (2443/10000)
[Test]  Epoch: 69	Loss: 0.053765	Acc: 24.5% (2453/10000)
[Test]  Epoch: 70	Loss: 0.053787	Acc: 24.7% (2474/10000)
[Test]  Epoch: 71	Loss: 0.053805	Acc: 24.6% (2461/10000)
[Test]  Epoch: 72	Loss: 0.053828	Acc: 24.4% (2443/10000)
[Test]  Epoch: 73	Loss: 0.053754	Acc: 24.8% (2484/10000)
[Test]  Epoch: 74	Loss: 0.053727	Acc: 24.7% (2472/10000)
[Test]  Epoch: 75	Loss: 0.053763	Acc: 24.6% (2462/10000)
[Test]  Epoch: 76	Loss: 0.053745	Acc: 24.9% (2491/10000)
[Test]  Epoch: 77	Loss: 0.053748	Acc: 24.7% (2472/10000)
[Test]  Epoch: 78	Loss: 0.053783	Acc: 24.6% (2455/10000)
[Test]  Epoch: 79	Loss: 0.053779	Acc: 24.7% (2468/10000)
[Test]  Epoch: 80	Loss: 0.053701	Acc: 24.8% (2480/10000)
[Test]  Epoch: 81	Loss: 0.053725	Acc: 24.6% (2460/10000)
[Test]  Epoch: 82	Loss: 0.053774	Acc: 24.7% (2468/10000)
[Test]  Epoch: 83	Loss: 0.053811	Acc: 24.5% (2453/10000)
[Test]  Epoch: 84	Loss: 0.053791	Acc: 24.6% (2456/10000)
[Test]  Epoch: 85	Loss: 0.053793	Acc: 24.6% (2457/10000)
[Test]  Epoch: 86	Loss: 0.053784	Acc: 24.6% (2456/10000)
[Test]  Epoch: 87	Loss: 0.053779	Acc: 24.6% (2460/10000)
[Test]  Epoch: 88	Loss: 0.053809	Acc: 24.5% (2447/10000)
[Test]  Epoch: 89	Loss: 0.053794	Acc: 24.6% (2457/10000)
[Test]  Epoch: 90	Loss: 0.053789	Acc: 24.6% (2462/10000)
[Test]  Epoch: 91	Loss: 0.053795	Acc: 24.6% (2464/10000)
[Test]  Epoch: 92	Loss: 0.053679	Acc: 24.8% (2479/10000)
[Test]  Epoch: 93	Loss: 0.053749	Acc: 24.8% (2479/10000)
[Test]  Epoch: 94	Loss: 0.053740	Acc: 24.6% (2464/10000)
[Test]  Epoch: 95	Loss: 0.053784	Acc: 24.6% (2457/10000)
[Test]  Epoch: 96	Loss: 0.053747	Acc: 24.6% (2462/10000)
[Test]  Epoch: 97	Loss: 0.053770	Acc: 24.8% (2477/10000)
[Test]  Epoch: 98	Loss: 0.053785	Acc: 24.6% (2465/10000)
[Test]  Epoch: 99	Loss: 0.053765	Acc: 24.7% (2473/10000)
[Test]  Epoch: 100	Loss: 0.053731	Acc: 24.7% (2471/10000)
===========finish==========
['2024-08-19', '06:45:45.130902', '100', 'test', '0.0537305076122284', '24.71', '24.95']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.074105	Acc: 4.2% (423/10000)
[Test]  Epoch: 2	Loss: 0.064866	Acc: 12.3% (1233/10000)
[Test]  Epoch: 3	Loss: 0.062532	Acc: 15.1% (1511/10000)
[Test]  Epoch: 4	Loss: 0.061331	Acc: 16.9% (1686/10000)
[Test]  Epoch: 5	Loss: 0.060405	Acc: 18.2% (1822/10000)
[Test]  Epoch: 6	Loss: 0.059786	Acc: 18.9% (1886/10000)
[Test]  Epoch: 7	Loss: 0.059404	Acc: 19.6% (1962/10000)
[Test]  Epoch: 8	Loss: 0.059012	Acc: 19.8% (1976/10000)
[Test]  Epoch: 9	Loss: 0.058667	Acc: 20.3% (2034/10000)
[Test]  Epoch: 10	Loss: 0.058320	Acc: 20.7% (2071/10000)
[Test]  Epoch: 11	Loss: 0.058134	Acc: 20.8% (2080/10000)
[Test]  Epoch: 12	Loss: 0.057890	Acc: 21.1% (2110/10000)
[Test]  Epoch: 13	Loss: 0.057690	Acc: 21.3% (2127/10000)
[Test]  Epoch: 14	Loss: 0.057527	Acc: 21.4% (2140/10000)
[Test]  Epoch: 15	Loss: 0.057488	Acc: 21.6% (2159/10000)
[Test]  Epoch: 16	Loss: 0.057385	Acc: 21.5% (2147/10000)
[Test]  Epoch: 17	Loss: 0.057171	Acc: 22.0% (2197/10000)
[Test]  Epoch: 18	Loss: 0.057166	Acc: 21.8% (2184/10000)
[Test]  Epoch: 19	Loss: 0.057052	Acc: 22.1% (2210/10000)
[Test]  Epoch: 20	Loss: 0.056967	Acc: 22.2% (2219/10000)
[Test]  Epoch: 21	Loss: 0.056900	Acc: 22.1% (2210/10000)
[Test]  Epoch: 22	Loss: 0.056874	Acc: 22.1% (2210/10000)
[Test]  Epoch: 23	Loss: 0.056709	Acc: 22.3% (2231/10000)
[Test]  Epoch: 24	Loss: 0.056689	Acc: 22.2% (2222/10000)
[Test]  Epoch: 25	Loss: 0.056648	Acc: 22.4% (2238/10000)
[Test]  Epoch: 26	Loss: 0.056642	Acc: 22.5% (2248/10000)
[Test]  Epoch: 27	Loss: 0.056503	Acc: 22.4% (2240/10000)
[Test]  Epoch: 28	Loss: 0.056499	Acc: 22.4% (2236/10000)
[Test]  Epoch: 29	Loss: 0.056364	Acc: 22.4% (2236/10000)
[Test]  Epoch: 30	Loss: 0.056372	Acc: 22.6% (2258/10000)
[Test]  Epoch: 31	Loss: 0.056410	Acc: 22.7% (2269/10000)
[Test]  Epoch: 32	Loss: 0.056395	Acc: 22.5% (2246/10000)
[Test]  Epoch: 33	Loss: 0.056353	Acc: 22.5% (2251/10000)
[Test]  Epoch: 34	Loss: 0.056284	Acc: 22.4% (2244/10000)
[Test]  Epoch: 35	Loss: 0.056276	Acc: 22.5% (2254/10000)
[Test]  Epoch: 36	Loss: 0.056087	Acc: 22.8% (2279/10000)
[Test]  Epoch: 37	Loss: 0.056172	Acc: 22.9% (2287/10000)
[Test]  Epoch: 38	Loss: 0.056130	Acc: 22.7% (2272/10000)
[Test]  Epoch: 39	Loss: 0.056145	Acc: 22.8% (2281/10000)
[Test]  Epoch: 40	Loss: 0.056083	Acc: 22.8% (2275/10000)
[Test]  Epoch: 41	Loss: 0.056098	Acc: 22.8% (2279/10000)
[Test]  Epoch: 42	Loss: 0.056113	Acc: 22.8% (2278/10000)
[Test]  Epoch: 43	Loss: 0.056008	Acc: 22.9% (2289/10000)
[Test]  Epoch: 44	Loss: 0.055975	Acc: 22.9% (2286/10000)
[Test]  Epoch: 45	Loss: 0.055871	Acc: 23.1% (2308/10000)
[Test]  Epoch: 46	Loss: 0.056019	Acc: 22.9% (2286/10000)
[Test]  Epoch: 47	Loss: 0.055997	Acc: 23.0% (2304/10000)
[Test]  Epoch: 48	Loss: 0.055877	Acc: 23.3% (2330/10000)
[Test]  Epoch: 49	Loss: 0.055807	Acc: 23.3% (2326/10000)
[Test]  Epoch: 50	Loss: 0.055992	Acc: 23.1% (2305/10000)
[Test]  Epoch: 51	Loss: 0.055831	Acc: 23.2% (2319/10000)
[Test]  Epoch: 52	Loss: 0.055867	Acc: 23.0% (2304/10000)
[Test]  Epoch: 53	Loss: 0.055819	Acc: 23.1% (2306/10000)
[Test]  Epoch: 54	Loss: 0.055866	Acc: 23.4% (2342/10000)
[Test]  Epoch: 55	Loss: 0.055875	Acc: 23.3% (2330/10000)
[Test]  Epoch: 56	Loss: 0.055779	Acc: 23.3% (2328/10000)
[Test]  Epoch: 57	Loss: 0.055823	Acc: 23.1% (2315/10000)
[Test]  Epoch: 58	Loss: 0.055651	Acc: 23.4% (2336/10000)
[Test]  Epoch: 59	Loss: 0.055689	Acc: 23.5% (2348/10000)
[Test]  Epoch: 60	Loss: 0.055820	Acc: 23.3% (2334/10000)
[Test]  Epoch: 61	Loss: 0.055831	Acc: 23.1% (2310/10000)
[Test]  Epoch: 62	Loss: 0.055810	Acc: 23.3% (2334/10000)
[Test]  Epoch: 63	Loss: 0.055715	Acc: 23.4% (2344/10000)
[Test]  Epoch: 64	Loss: 0.055701	Acc: 23.4% (2344/10000)
[Test]  Epoch: 65	Loss: 0.055764	Acc: 23.2% (2321/10000)
[Test]  Epoch: 66	Loss: 0.055761	Acc: 23.4% (2342/10000)
[Test]  Epoch: 67	Loss: 0.055805	Acc: 23.3% (2332/10000)
[Test]  Epoch: 68	Loss: 0.055835	Acc: 23.3% (2331/10000)
[Test]  Epoch: 69	Loss: 0.055713	Acc: 23.4% (2341/10000)
[Test]  Epoch: 70	Loss: 0.055760	Acc: 23.4% (2337/10000)
[Test]  Epoch: 71	Loss: 0.055745	Acc: 23.2% (2321/10000)
[Test]  Epoch: 72	Loss: 0.055802	Acc: 23.1% (2313/10000)
[Test]  Epoch: 73	Loss: 0.055738	Acc: 23.4% (2335/10000)
[Test]  Epoch: 74	Loss: 0.055668	Acc: 23.4% (2338/10000)
[Test]  Epoch: 75	Loss: 0.055739	Acc: 23.3% (2333/10000)
[Test]  Epoch: 76	Loss: 0.055741	Acc: 23.3% (2334/10000)
[Test]  Epoch: 77	Loss: 0.055723	Acc: 23.4% (2343/10000)
[Test]  Epoch: 78	Loss: 0.055719	Acc: 23.2% (2323/10000)
[Test]  Epoch: 79	Loss: 0.055718	Acc: 23.5% (2351/10000)
[Test]  Epoch: 80	Loss: 0.055706	Acc: 23.4% (2337/10000)
[Test]  Epoch: 81	Loss: 0.055674	Acc: 23.3% (2332/10000)
[Test]  Epoch: 82	Loss: 0.055735	Acc: 23.3% (2329/10000)
[Test]  Epoch: 83	Loss: 0.055772	Acc: 23.3% (2329/10000)
[Test]  Epoch: 84	Loss: 0.055755	Acc: 23.6% (2356/10000)
[Test]  Epoch: 85	Loss: 0.055765	Acc: 23.2% (2324/10000)
[Test]  Epoch: 86	Loss: 0.055706	Acc: 23.3% (2326/10000)
[Test]  Epoch: 87	Loss: 0.055751	Acc: 23.4% (2338/10000)
[Test]  Epoch: 88	Loss: 0.055771	Acc: 23.5% (2346/10000)
[Test]  Epoch: 89	Loss: 0.055711	Acc: 23.4% (2336/10000)
[Test]  Epoch: 90	Loss: 0.055749	Acc: 23.5% (2352/10000)
[Test]  Epoch: 91	Loss: 0.055745	Acc: 23.4% (2344/10000)
[Test]  Epoch: 92	Loss: 0.055659	Acc: 23.6% (2361/10000)
[Test]  Epoch: 93	Loss: 0.055724	Acc: 23.3% (2330/10000)
[Test]  Epoch: 94	Loss: 0.055716	Acc: 23.4% (2337/10000)
[Test]  Epoch: 95	Loss: 0.055715	Acc: 23.3% (2328/10000)
[Test]  Epoch: 96	Loss: 0.055682	Acc: 23.5% (2349/10000)
[Test]  Epoch: 97	Loss: 0.055750	Acc: 23.2% (2316/10000)
[Test]  Epoch: 98	Loss: 0.055726	Acc: 23.3% (2333/10000)
[Test]  Epoch: 99	Loss: 0.055688	Acc: 23.2% (2321/10000)
[Test]  Epoch: 100	Loss: 0.055720	Acc: 23.3% (2330/10000)
===========finish==========
['2024-08-19', '06:48:35.902096', '100', 'test', '0.055720029520988465', '23.3', '23.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'last_linear.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'last_linear.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.087212	Acc: 0.8% (76/10000)
[Test]  Epoch: 2	Loss: 0.082013	Acc: 2.4% (244/10000)
[Test]  Epoch: 3	Loss: 0.079231	Acc: 4.1% (407/10000)
[Test]  Epoch: 4	Loss: 0.077094	Acc: 5.2% (517/10000)
[Test]  Epoch: 5	Loss: 0.075468	Acc: 6.3% (630/10000)
[Test]  Epoch: 6	Loss: 0.074172	Acc: 6.8% (675/10000)
[Test]  Epoch: 7	Loss: 0.073094	Acc: 7.3% (727/10000)
[Test]  Epoch: 8	Loss: 0.071937	Acc: 7.9% (790/10000)
[Test]  Epoch: 9	Loss: 0.071101	Acc: 8.1% (812/10000)
[Test]  Epoch: 10	Loss: 0.070211	Acc: 8.6% (856/10000)
[Test]  Epoch: 11	Loss: 0.069460	Acc: 9.0% (899/10000)
[Test]  Epoch: 12	Loss: 0.068820	Acc: 9.3% (931/10000)
[Test]  Epoch: 13	Loss: 0.068237	Acc: 9.7% (974/10000)
[Test]  Epoch: 14	Loss: 0.067761	Acc: 9.8% (983/10000)
[Test]  Epoch: 15	Loss: 0.067355	Acc: 10.2% (1024/10000)
[Test]  Epoch: 16	Loss: 0.066990	Acc: 10.5% (1050/10000)
[Test]  Epoch: 17	Loss: 0.066611	Acc: 10.7% (1066/10000)
[Test]  Epoch: 18	Loss: 0.066400	Acc: 10.8% (1077/10000)
[Test]  Epoch: 19	Loss: 0.066107	Acc: 10.9% (1091/10000)
[Test]  Epoch: 20	Loss: 0.065675	Acc: 11.2% (1119/10000)
[Test]  Epoch: 21	Loss: 0.065449	Acc: 11.3% (1131/10000)
[Test]  Epoch: 22	Loss: 0.065264	Acc: 11.3% (1127/10000)
[Test]  Epoch: 23	Loss: 0.064956	Acc: 11.6% (1156/10000)
[Test]  Epoch: 24	Loss: 0.064760	Acc: 11.6% (1155/10000)
[Test]  Epoch: 25	Loss: 0.064676	Acc: 11.8% (1179/10000)
[Test]  Epoch: 26	Loss: 0.064437	Acc: 11.9% (1189/10000)
[Test]  Epoch: 27	Loss: 0.064234	Acc: 12.1% (1214/10000)
[Test]  Epoch: 28	Loss: 0.064112	Acc: 12.2% (1216/10000)
[Test]  Epoch: 29	Loss: 0.063839	Acc: 12.4% (1239/10000)
[Test]  Epoch: 30	Loss: 0.063834	Acc: 12.2% (1224/10000)
[Test]  Epoch: 31	Loss: 0.063779	Acc: 12.4% (1240/10000)
[Test]  Epoch: 32	Loss: 0.063612	Acc: 12.5% (1248/10000)
[Test]  Epoch: 33	Loss: 0.063457	Acc: 12.5% (1248/10000)
[Test]  Epoch: 34	Loss: 0.063349	Acc: 12.5% (1252/10000)
[Test]  Epoch: 35	Loss: 0.063169	Acc: 12.7% (1270/10000)
[Test]  Epoch: 36	Loss: 0.063121	Acc: 12.9% (1287/10000)
[Test]  Epoch: 37	Loss: 0.062997	Acc: 13.0% (1296/10000)
[Test]  Epoch: 38	Loss: 0.062860	Acc: 12.9% (1290/10000)
[Test]  Epoch: 39	Loss: 0.062827	Acc: 12.9% (1291/10000)
[Test]  Epoch: 40	Loss: 0.062706	Acc: 13.2% (1321/10000)
[Test]  Epoch: 41	Loss: 0.062782	Acc: 13.2% (1322/10000)
[Test]  Epoch: 42	Loss: 0.062574	Acc: 12.9% (1290/10000)
[Test]  Epoch: 43	Loss: 0.062526	Acc: 13.2% (1319/10000)
[Test]  Epoch: 44	Loss: 0.062427	Acc: 13.2% (1324/10000)
[Test]  Epoch: 45	Loss: 0.062402	Acc: 13.3% (1327/10000)
[Test]  Epoch: 46	Loss: 0.062359	Acc: 13.2% (1322/10000)
[Test]  Epoch: 47	Loss: 0.062262	Acc: 13.3% (1333/10000)
[Test]  Epoch: 48	Loss: 0.062091	Acc: 13.6% (1356/10000)
[Test]  Epoch: 49	Loss: 0.062129	Acc: 13.6% (1360/10000)
[Test]  Epoch: 50	Loss: 0.062101	Acc: 13.6% (1359/10000)
[Test]  Epoch: 51	Loss: 0.061985	Acc: 13.6% (1357/10000)
[Test]  Epoch: 52	Loss: 0.061899	Acc: 13.6% (1362/10000)
[Test]  Epoch: 53	Loss: 0.061928	Acc: 13.8% (1377/10000)
[Test]  Epoch: 54	Loss: 0.061915	Acc: 13.9% (1387/10000)
[Test]  Epoch: 55	Loss: 0.061902	Acc: 13.7% (1368/10000)
[Test]  Epoch: 56	Loss: 0.061811	Acc: 13.7% (1366/10000)
[Test]  Epoch: 57	Loss: 0.061899	Acc: 13.6% (1362/10000)
[Test]  Epoch: 58	Loss: 0.061618	Acc: 14.1% (1410/10000)
[Test]  Epoch: 59	Loss: 0.061563	Acc: 14.1% (1406/10000)
[Test]  Epoch: 60	Loss: 0.061614	Acc: 14.0% (1402/10000)
[Test]  Epoch: 61	Loss: 0.061652	Acc: 14.0% (1398/10000)
[Test]  Epoch: 62	Loss: 0.061616	Acc: 14.1% (1412/10000)
[Test]  Epoch: 63	Loss: 0.061539	Acc: 14.0% (1404/10000)
[Test]  Epoch: 64	Loss: 0.061538	Acc: 14.2% (1416/10000)
[Test]  Epoch: 65	Loss: 0.061588	Acc: 14.1% (1408/10000)
[Test]  Epoch: 66	Loss: 0.061571	Acc: 13.9% (1395/10000)
[Test]  Epoch: 67	Loss: 0.061615	Acc: 14.1% (1405/10000)
[Test]  Epoch: 68	Loss: 0.061621	Acc: 13.9% (1394/10000)
[Test]  Epoch: 69	Loss: 0.061536	Acc: 14.2% (1415/10000)
[Test]  Epoch: 70	Loss: 0.061536	Acc: 14.2% (1420/10000)
[Test]  Epoch: 71	Loss: 0.061539	Acc: 14.1% (1412/10000)
[Test]  Epoch: 72	Loss: 0.061602	Acc: 14.0% (1399/10000)
[Test]  Epoch: 73	Loss: 0.061543	Acc: 14.0% (1399/10000)
[Test]  Epoch: 74	Loss: 0.061469	Acc: 14.1% (1405/10000)
[Test]  Epoch: 75	Loss: 0.061494	Acc: 14.1% (1407/10000)
[Test]  Epoch: 76	Loss: 0.061518	Acc: 14.2% (1416/10000)
[Test]  Epoch: 77	Loss: 0.061519	Acc: 14.1% (1408/10000)
[Test]  Epoch: 78	Loss: 0.061524	Acc: 14.0% (1399/10000)
[Test]  Epoch: 79	Loss: 0.061502	Acc: 14.2% (1419/10000)
[Test]  Epoch: 80	Loss: 0.061450	Acc: 14.1% (1409/10000)
[Test]  Epoch: 81	Loss: 0.061429	Acc: 14.0% (1401/10000)
[Test]  Epoch: 82	Loss: 0.061492	Acc: 14.3% (1428/10000)
[Test]  Epoch: 83	Loss: 0.061462	Acc: 14.2% (1424/10000)
[Test]  Epoch: 84	Loss: 0.061472	Acc: 14.2% (1420/10000)
[Test]  Epoch: 85	Loss: 0.061499	Acc: 14.2% (1419/10000)
[Test]  Epoch: 86	Loss: 0.061447	Acc: 14.2% (1416/10000)
[Test]  Epoch: 87	Loss: 0.061428	Acc: 14.2% (1422/10000)
[Test]  Epoch: 88	Loss: 0.061497	Acc: 14.2% (1423/10000)
[Test]  Epoch: 89	Loss: 0.061458	Acc: 14.1% (1413/10000)
[Test]  Epoch: 90	Loss: 0.061457	Acc: 14.2% (1416/10000)
[Test]  Epoch: 91	Loss: 0.061440	Acc: 14.3% (1433/10000)
[Test]  Epoch: 92	Loss: 0.061378	Acc: 14.2% (1425/10000)
[Test]  Epoch: 93	Loss: 0.061404	Acc: 14.3% (1427/10000)
[Test]  Epoch: 94	Loss: 0.061384	Acc: 14.2% (1421/10000)
[Test]  Epoch: 95	Loss: 0.061490	Acc: 14.1% (1414/10000)
[Test]  Epoch: 96	Loss: 0.061428	Acc: 14.2% (1420/10000)
[Test]  Epoch: 97	Loss: 0.061439	Acc: 14.3% (1426/10000)
[Test]  Epoch: 98	Loss: 0.061482	Acc: 14.1% (1413/10000)
[Test]  Epoch: 99	Loss: 0.061397	Acc: 14.1% (1406/10000)
[Test]  Epoch: 100	Loss: 0.061383	Acc: 14.2% (1425/10000)
===========finish==========
['2024-08-19', '06:51:35.824858', '100', 'test', '0.06138319804668427', '14.25', '14.33']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-mobilenetv2/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('_features.0.0.weight', 0.0), ('_features.0.1.weight', 0.0), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.0.weight', 0.0), ('_features.1.conv.0.1.weight', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.1.weight', 0.0), ('_features.1.conv.2.weight', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.0.weight', 0.0), ('_features.2.conv.0.1.weight', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.0.weight', 0.0), ('_features.2.conv.1.1.weight', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.2.weight', 0.0), ('_features.2.conv.3.weight', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.0.weight', 0.0), ('_features.3.conv.0.1.weight', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.0.weight', 0.0), ('_features.3.conv.1.1.weight', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.2.weight', 0.0), ('_features.3.conv.3.weight', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.0.weight', 0.0), ('_features.4.conv.0.1.weight', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.0.weight', 0.0), ('_features.4.conv.1.1.weight', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.2.weight', 0.0), ('_features.4.conv.3.weight', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.0.weight', 0.0), ('_features.5.conv.0.1.weight', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.0.weight', 0.0), ('_features.5.conv.1.1.weight', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.2.weight', 0.0), ('_features.5.conv.3.weight', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.0.weight', 0.0), ('_features.6.conv.0.1.weight', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.0.weight', 0.0), ('_features.6.conv.1.1.weight', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.2.weight', 0.0), ('_features.6.conv.3.weight', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.0.weight', 0.0), ('_features.7.conv.0.1.weight', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.0.weight', 0.0), ('_features.7.conv.1.1.weight', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.2.weight', 0.0), ('_features.7.conv.3.weight', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.0.weight', 0.0), ('_features.8.conv.0.1.weight', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.0.weight', 0.0), ('_features.8.conv.1.1.weight', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.2.weight', 0.0), ('_features.8.conv.3.weight', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.0.weight', 0.0), ('_features.9.conv.0.1.weight', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.0.weight', 0.0), ('_features.9.conv.1.1.weight', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.2.weight', 0.0), ('_features.9.conv.3.weight', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.0.weight', 0.0), ('_features.10.conv.0.1.weight', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.0.weight', 0.0), ('_features.10.conv.1.1.weight', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.2.weight', 0.0), ('_features.10.conv.3.weight', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.0.weight', 0.0), ('_features.11.conv.0.1.weight', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.0.weight', 0.0), ('_features.11.conv.1.1.weight', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.2.weight', 0.0), ('_features.11.conv.3.weight', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.0.weight', 0.0), ('_features.12.conv.0.1.weight', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.0.weight', 0.0), ('_features.12.conv.1.1.weight', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.2.weight', 0.0), ('_features.12.conv.3.weight', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.0.weight', 0.0), ('_features.13.conv.0.1.weight', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.0.weight', 0.0), ('_features.13.conv.1.1.weight', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.2.weight', 0.0), ('_features.13.conv.3.weight', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.0.weight', 0.0), ('_features.14.conv.0.1.weight', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.0.weight', 0.0), ('_features.14.conv.1.1.weight', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.2.weight', 0.0), ('_features.14.conv.3.weight', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.0.weight', 0.0), ('_features.15.conv.0.1.weight', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.0.weight', 0.0), ('_features.15.conv.1.1.weight', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.2.weight', 0.0), ('_features.15.conv.3.weight', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.0.weight', 0.0), ('_features.16.conv.0.1.weight', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.0.weight', 0.0), ('_features.16.conv.1.1.weight', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.2.weight', 0.0), ('_features.16.conv.3.weight', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.0.weight', 0.0), ('_features.17.conv.0.1.weight', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.0.weight', 0.0), ('_features.17.conv.1.1.weight', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.2.weight', 0.0), ('_features.17.conv.3.weight', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.0.weight', 0.0), ('_features.18.1.weight', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('_features.0.0.weight', 0.0), ('_features.0.1.weight', 0.0), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.0.weight', 0.0), ('_features.1.conv.0.1.weight', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.1.weight', 0.0), ('_features.1.conv.2.weight', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.0.weight', 0.0), ('_features.2.conv.0.1.weight', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.0.weight', 0.0), ('_features.2.conv.1.1.weight', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.2.weight', 0.0), ('_features.2.conv.3.weight', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.0.weight', 0.0), ('_features.3.conv.0.1.weight', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.0.weight', 0.0), ('_features.3.conv.1.1.weight', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.2.weight', 0.0), ('_features.3.conv.3.weight', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.0.weight', 0.0), ('_features.4.conv.0.1.weight', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.0.weight', 0.0), ('_features.4.conv.1.1.weight', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.2.weight', 0.0), ('_features.4.conv.3.weight', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.0.weight', 0.0), ('_features.5.conv.0.1.weight', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.0.weight', 0.0), ('_features.5.conv.1.1.weight', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.2.weight', 0.0), ('_features.5.conv.3.weight', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.0.weight', 0.0), ('_features.6.conv.0.1.weight', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.0.weight', 0.0), ('_features.6.conv.1.1.weight', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.2.weight', 0.0), ('_features.6.conv.3.weight', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.0.weight', 0.0), ('_features.7.conv.0.1.weight', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.0.weight', 0.0), ('_features.7.conv.1.1.weight', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.2.weight', 0.0), ('_features.7.conv.3.weight', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.0.weight', 0.0), ('_features.8.conv.0.1.weight', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.0.weight', 0.0), ('_features.8.conv.1.1.weight', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.2.weight', 0.0), ('_features.8.conv.3.weight', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.0.weight', 0.0), ('_features.9.conv.0.1.weight', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.0.weight', 0.0), ('_features.9.conv.1.1.weight', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.2.weight', 0.0), ('_features.9.conv.3.weight', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.0.weight', 0.0), ('_features.10.conv.0.1.weight', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.0.weight', 0.0), ('_features.10.conv.1.1.weight', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.2.weight', 0.0), ('_features.10.conv.3.weight', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.0.weight', 0.0), ('_features.11.conv.0.1.weight', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.0.weight', 0.0), ('_features.11.conv.1.1.weight', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.2.weight', 0.0), ('_features.11.conv.3.weight', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.0.weight', 0.0), ('_features.12.conv.0.1.weight', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.0.weight', 0.0), ('_features.12.conv.1.1.weight', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.2.weight', 0.0), ('_features.12.conv.3.weight', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.0.weight', 0.0), ('_features.13.conv.0.1.weight', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.0.weight', 0.0), ('_features.13.conv.1.1.weight', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.2.weight', 0.0), ('_features.13.conv.3.weight', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.0.weight', 0.0), ('_features.14.conv.0.1.weight', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.0.weight', 0.0), ('_features.14.conv.1.1.weight', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.2.weight', 0.0), ('_features.14.conv.3.weight', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.0.weight', 0.0), ('_features.15.conv.0.1.weight', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.0.weight', 0.0), ('_features.15.conv.1.1.weight', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.2.weight', 0.0), ('_features.15.conv.3.weight', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.0.weight', 0.0), ('_features.16.conv.0.1.weight', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.0.weight', 0.0), ('_features.16.conv.1.1.weight', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.2.weight', 0.0), ('_features.16.conv.3.weight', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.0.weight', 0.0), ('_features.17.conv.0.1.weight', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.0.weight', 0.0), ('_features.17.conv.1.1.weight', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.2.weight', 0.0), ('_features.17.conv.3.weight', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.0.weight', 0.0), ('_features.18.1.weight', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=105  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039145	Acc: 45.2% (4524/10000)
[Test]  Epoch: 2	Loss: 0.038964	Acc: 45.8% (4583/10000)
[Test]  Epoch: 3	Loss: 0.039227	Acc: 45.6% (4559/10000)
[Test]  Epoch: 4	Loss: 0.039427	Acc: 45.3% (4530/10000)
[Test]  Epoch: 5	Loss: 0.039470	Acc: 45.5% (4548/10000)
[Test]  Epoch: 6	Loss: 0.039663	Acc: 45.5% (4548/10000)
[Test]  Epoch: 7	Loss: 0.039650	Acc: 45.4% (4536/10000)
[Test]  Epoch: 8	Loss: 0.039848	Acc: 45.0% (4495/10000)
[Test]  Epoch: 9	Loss: 0.039951	Acc: 45.0% (4502/10000)
[Test]  Epoch: 10	Loss: 0.040069	Acc: 45.0% (4504/10000)
[Test]  Epoch: 11	Loss: 0.040158	Acc: 44.9% (4492/10000)
[Test]  Epoch: 12	Loss: 0.040056	Acc: 45.2% (4516/10000)
[Test]  Epoch: 13	Loss: 0.040200	Acc: 45.2% (4517/10000)
[Test]  Epoch: 14	Loss: 0.040238	Acc: 45.1% (4508/10000)
[Test]  Epoch: 15	Loss: 0.040419	Acc: 45.0% (4500/10000)
[Test]  Epoch: 16	Loss: 0.040170	Acc: 45.2% (4525/10000)
[Test]  Epoch: 17	Loss: 0.040335	Acc: 45.0% (4498/10000)
[Test]  Epoch: 18	Loss: 0.040376	Acc: 45.0% (4497/10000)
[Test]  Epoch: 19	Loss: 0.040375	Acc: 44.9% (4486/10000)
[Test]  Epoch: 20	Loss: 0.040495	Acc: 45.1% (4508/10000)
[Test]  Epoch: 21	Loss: 0.040519	Acc: 45.3% (4527/10000)
[Test]  Epoch: 22	Loss: 0.040613	Acc: 44.8% (4480/10000)
[Test]  Epoch: 23	Loss: 0.040609	Acc: 44.8% (4484/10000)
[Test]  Epoch: 24	Loss: 0.040673	Acc: 44.9% (4489/10000)
[Test]  Epoch: 25	Loss: 0.040605	Acc: 45.1% (4513/10000)
[Test]  Epoch: 26	Loss: 0.040756	Acc: 44.8% (4475/10000)
[Test]  Epoch: 27	Loss: 0.040599	Acc: 45.1% (4507/10000)
[Test]  Epoch: 28	Loss: 0.040843	Acc: 44.7% (4471/10000)
[Test]  Epoch: 29	Loss: 0.040749	Acc: 44.7% (4474/10000)
[Test]  Epoch: 30	Loss: 0.040871	Acc: 44.7% (4468/10000)
[Test]  Epoch: 31	Loss: 0.041031	Acc: 44.8% (4483/10000)
[Test]  Epoch: 32	Loss: 0.040929	Acc: 44.8% (4476/10000)
[Test]  Epoch: 33	Loss: 0.040786	Acc: 44.9% (4485/10000)
[Test]  Epoch: 34	Loss: 0.040865	Acc: 44.8% (4476/10000)
[Test]  Epoch: 35	Loss: 0.040859	Acc: 44.9% (4492/10000)
[Test]  Epoch: 36	Loss: 0.040803	Acc: 44.9% (4488/10000)
[Test]  Epoch: 37	Loss: 0.040916	Acc: 45.0% (4497/10000)
[Test]  Epoch: 38	Loss: 0.041044	Acc: 44.6% (4465/10000)
[Test]  Epoch: 39	Loss: 0.041057	Acc: 44.7% (4469/10000)
[Test]  Epoch: 40	Loss: 0.040995	Acc: 44.7% (4467/10000)
[Test]  Epoch: 41	Loss: 0.041061	Acc: 44.8% (4481/10000)
[Test]  Epoch: 42	Loss: 0.040997	Acc: 44.9% (4489/10000)
[Test]  Epoch: 43	Loss: 0.040998	Acc: 44.8% (4481/10000)
[Test]  Epoch: 44	Loss: 0.041046	Acc: 44.8% (4484/10000)
[Test]  Epoch: 45	Loss: 0.041095	Acc: 44.9% (4486/10000)
[Test]  Epoch: 46	Loss: 0.041139	Acc: 44.7% (4466/10000)
[Test]  Epoch: 47	Loss: 0.041219	Acc: 44.9% (4485/10000)
[Test]  Epoch: 48	Loss: 0.041164	Acc: 44.8% (4482/10000)
[Test]  Epoch: 49	Loss: 0.041196	Acc: 44.8% (4475/10000)
[Test]  Epoch: 50	Loss: 0.041252	Acc: 44.6% (4459/10000)
[Test]  Epoch: 51	Loss: 0.041138	Acc: 44.8% (4475/10000)
[Test]  Epoch: 52	Loss: 0.041053	Acc: 44.8% (4483/10000)
[Test]  Epoch: 53	Loss: 0.041179	Acc: 44.8% (4479/10000)
[Test]  Epoch: 54	Loss: 0.041330	Acc: 44.7% (4466/10000)
[Test]  Epoch: 55	Loss: 0.041292	Acc: 44.7% (4470/10000)
[Test]  Epoch: 56	Loss: 0.041272	Acc: 44.6% (4463/10000)
[Test]  Epoch: 57	Loss: 0.041370	Acc: 44.4% (4441/10000)
[Test]  Epoch: 58	Loss: 0.041143	Acc: 44.7% (4472/10000)
[Test]  Epoch: 59	Loss: 0.041274	Acc: 44.6% (4460/10000)
[Test]  Epoch: 60	Loss: 0.041241	Acc: 44.7% (4469/10000)
[Test]  Epoch: 61	Loss: 0.041351	Acc: 44.7% (4470/10000)
[Test]  Epoch: 62	Loss: 0.041356	Acc: 44.6% (4457/10000)
[Test]  Epoch: 63	Loss: 0.041146	Acc: 45.0% (4495/10000)
[Test]  Epoch: 64	Loss: 0.041178	Acc: 44.8% (4484/10000)
[Test]  Epoch: 65	Loss: 0.041287	Acc: 44.5% (4454/10000)
[Test]  Epoch: 66	Loss: 0.041248	Acc: 44.8% (4475/10000)
[Test]  Epoch: 67	Loss: 0.041311	Acc: 44.6% (4465/10000)
[Test]  Epoch: 68	Loss: 0.041421	Acc: 44.4% (4439/10000)
[Test]  Epoch: 69	Loss: 0.041353	Acc: 44.5% (4449/10000)
[Test]  Epoch: 70	Loss: 0.041336	Acc: 44.6% (4459/10000)
[Test]  Epoch: 71	Loss: 0.041291	Acc: 44.7% (4468/10000)
[Test]  Epoch: 72	Loss: 0.041364	Acc: 44.4% (4438/10000)
[Test]  Epoch: 73	Loss: 0.041213	Acc: 44.6% (4461/10000)
[Test]  Epoch: 74	Loss: 0.041309	Acc: 44.5% (4454/10000)
[Test]  Epoch: 75	Loss: 0.041385	Acc: 44.5% (4455/10000)
[Test]  Epoch: 76	Loss: 0.041229	Acc: 44.7% (4468/10000)
[Test]  Epoch: 77	Loss: 0.041346	Acc: 44.6% (4464/10000)
[Test]  Epoch: 78	Loss: 0.041401	Acc: 44.4% (4436/10000)
[Test]  Epoch: 79	Loss: 0.041461	Acc: 44.5% (4448/10000)
[Test]  Epoch: 80	Loss: 0.041279	Acc: 44.9% (4488/10000)
[Test]  Epoch: 81	Loss: 0.041381	Acc: 44.7% (4467/10000)
[Test]  Epoch: 82	Loss: 0.041390	Acc: 44.6% (4457/10000)
[Test]  Epoch: 83	Loss: 0.041413	Acc: 44.4% (4443/10000)
[Test]  Epoch: 84	Loss: 0.041512	Acc: 44.5% (4452/10000)
[Test]  Epoch: 85	Loss: 0.041406	Acc: 44.5% (4445/10000)
[Test]  Epoch: 86	Loss: 0.041467	Acc: 44.4% (4443/10000)
[Test]  Epoch: 87	Loss: 0.041368	Acc: 44.4% (4441/10000)
[Test]  Epoch: 88	Loss: 0.041372	Acc: 44.5% (4446/10000)
[Test]  Epoch: 89	Loss: 0.041341	Acc: 44.5% (4445/10000)
[Test]  Epoch: 90	Loss: 0.041425	Acc: 44.4% (4437/10000)
[Test]  Epoch: 91	Loss: 0.041443	Acc: 44.6% (4456/10000)
[Test]  Epoch: 92	Loss: 0.041309	Acc: 44.6% (4464/10000)
[Test]  Epoch: 93	Loss: 0.041393	Acc: 44.4% (4443/10000)
[Test]  Epoch: 94	Loss: 0.041426	Acc: 44.5% (4453/10000)
[Test]  Epoch: 95	Loss: 0.041378	Acc: 44.5% (4448/10000)
[Test]  Epoch: 96	Loss: 0.041339	Acc: 44.8% (4478/10000)
[Test]  Epoch: 97	Loss: 0.041425	Acc: 44.6% (4460/10000)
[Test]  Epoch: 98	Loss: 0.041389	Acc: 44.5% (4450/10000)
[Test]  Epoch: 99	Loss: 0.041422	Acc: 44.4% (4442/10000)
[Test]  Epoch: 100	Loss: 0.041360	Acc: 44.4% (4440/10000)
===========finish==========
['2024-08-19', '06:56:24.422761', '100', 'test', '0.041360099852085115', '44.4', '45.83']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.045176	Acc: 36.3% (3627/10000)
[Test]  Epoch: 2	Loss: 0.040356	Acc: 43.9% (4387/10000)
[Test]  Epoch: 3	Loss: 0.040332	Acc: 44.2% (4425/10000)
[Test]  Epoch: 4	Loss: 0.040471	Acc: 44.1% (4413/10000)
[Test]  Epoch: 5	Loss: 0.040441	Acc: 44.4% (4439/10000)
[Test]  Epoch: 6	Loss: 0.040694	Acc: 44.1% (4407/10000)
[Test]  Epoch: 7	Loss: 0.040516	Acc: 44.3% (4426/10000)
[Test]  Epoch: 8	Loss: 0.040787	Acc: 43.9% (4391/10000)
[Test]  Epoch: 9	Loss: 0.040915	Acc: 43.8% (4379/10000)
[Test]  Epoch: 10	Loss: 0.040997	Acc: 43.6% (4364/10000)
[Test]  Epoch: 11	Loss: 0.040927	Acc: 43.8% (4381/10000)
[Test]  Epoch: 12	Loss: 0.040891	Acc: 43.8% (4381/10000)
[Test]  Epoch: 13	Loss: 0.040945	Acc: 43.8% (4379/10000)
[Test]  Epoch: 14	Loss: 0.041063	Acc: 43.8% (4377/10000)
[Test]  Epoch: 15	Loss: 0.041237	Acc: 43.9% (4386/10000)
[Test]  Epoch: 16	Loss: 0.040990	Acc: 44.0% (4395/10000)
[Test]  Epoch: 17	Loss: 0.041210	Acc: 43.8% (4378/10000)
[Test]  Epoch: 18	Loss: 0.041237	Acc: 43.6% (4365/10000)
[Test]  Epoch: 19	Loss: 0.041187	Acc: 43.7% (4368/10000)
[Test]  Epoch: 20	Loss: 0.041254	Acc: 43.9% (4385/10000)
[Test]  Epoch: 21	Loss: 0.041300	Acc: 43.6% (4365/10000)
[Test]  Epoch: 22	Loss: 0.041383	Acc: 43.8% (4378/10000)
[Test]  Epoch: 23	Loss: 0.041333	Acc: 43.7% (4371/10000)
[Test]  Epoch: 24	Loss: 0.041447	Acc: 43.9% (4392/10000)
[Test]  Epoch: 25	Loss: 0.041414	Acc: 43.8% (4376/10000)
[Test]  Epoch: 26	Loss: 0.041567	Acc: 43.6% (4356/10000)
[Test]  Epoch: 27	Loss: 0.041377	Acc: 43.9% (4389/10000)
[Test]  Epoch: 28	Loss: 0.041577	Acc: 43.6% (4361/10000)
[Test]  Epoch: 29	Loss: 0.041565	Acc: 43.4% (4343/10000)
[Test]  Epoch: 30	Loss: 0.041579	Acc: 43.4% (4342/10000)
[Test]  Epoch: 31	Loss: 0.041887	Acc: 43.3% (4327/10000)
[Test]  Epoch: 32	Loss: 0.041694	Acc: 43.6% (4358/10000)
[Test]  Epoch: 33	Loss: 0.041560	Acc: 43.9% (4390/10000)
[Test]  Epoch: 34	Loss: 0.041628	Acc: 43.6% (4356/10000)
[Test]  Epoch: 35	Loss: 0.041603	Acc: 43.6% (4359/10000)
[Test]  Epoch: 36	Loss: 0.041605	Acc: 43.6% (4358/10000)
[Test]  Epoch: 37	Loss: 0.041674	Acc: 43.8% (4375/10000)
[Test]  Epoch: 38	Loss: 0.041830	Acc: 43.5% (4345/10000)
[Test]  Epoch: 39	Loss: 0.041816	Acc: 43.5% (4348/10000)
[Test]  Epoch: 40	Loss: 0.041742	Acc: 43.6% (4363/10000)
[Test]  Epoch: 41	Loss: 0.041815	Acc: 43.5% (4355/10000)
[Test]  Epoch: 42	Loss: 0.041784	Acc: 43.5% (4350/10000)
[Test]  Epoch: 43	Loss: 0.041772	Acc: 43.6% (4365/10000)
[Test]  Epoch: 44	Loss: 0.041720	Acc: 43.8% (4375/10000)
[Test]  Epoch: 45	Loss: 0.041838	Acc: 43.5% (4346/10000)
[Test]  Epoch: 46	Loss: 0.041854	Acc: 43.5% (4353/10000)
[Test]  Epoch: 47	Loss: 0.041993	Acc: 43.2% (4324/10000)
[Test]  Epoch: 48	Loss: 0.041947	Acc: 43.5% (4354/10000)
[Test]  Epoch: 49	Loss: 0.041985	Acc: 43.6% (4359/10000)
[Test]  Epoch: 50	Loss: 0.042053	Acc: 43.4% (4339/10000)
[Test]  Epoch: 51	Loss: 0.041997	Acc: 43.2% (4325/10000)
[Test]  Epoch: 52	Loss: 0.041842	Acc: 43.7% (4370/10000)
[Test]  Epoch: 53	Loss: 0.041974	Acc: 43.5% (4348/10000)
[Test]  Epoch: 54	Loss: 0.042051	Acc: 43.5% (4355/10000)
[Test]  Epoch: 55	Loss: 0.042035	Acc: 43.6% (4360/10000)
[Test]  Epoch: 56	Loss: 0.041977	Acc: 43.6% (4360/10000)
[Test]  Epoch: 57	Loss: 0.042125	Acc: 43.3% (4328/10000)
[Test]  Epoch: 58	Loss: 0.041866	Acc: 43.5% (4353/10000)
[Test]  Epoch: 59	Loss: 0.042038	Acc: 43.5% (4347/10000)
[Test]  Epoch: 60	Loss: 0.042040	Acc: 43.3% (4327/10000)
[Test]  Epoch: 61	Loss: 0.042125	Acc: 43.6% (4358/10000)
[Test]  Epoch: 62	Loss: 0.042137	Acc: 43.3% (4333/10000)
[Test]  Epoch: 63	Loss: 0.041933	Acc: 43.6% (4363/10000)
[Test]  Epoch: 64	Loss: 0.041950	Acc: 43.8% (4376/10000)
[Test]  Epoch: 65	Loss: 0.042048	Acc: 43.5% (4353/10000)
[Test]  Epoch: 66	Loss: 0.042023	Acc: 43.5% (4352/10000)
[Test]  Epoch: 67	Loss: 0.042076	Acc: 43.6% (4358/10000)
[Test]  Epoch: 68	Loss: 0.042193	Acc: 43.4% (4337/10000)
[Test]  Epoch: 69	Loss: 0.042125	Acc: 43.4% (4337/10000)
[Test]  Epoch: 70	Loss: 0.042097	Acc: 43.4% (4344/10000)
[Test]  Epoch: 71	Loss: 0.042050	Acc: 43.6% (4359/10000)
[Test]  Epoch: 72	Loss: 0.042145	Acc: 43.4% (4339/10000)
[Test]  Epoch: 73	Loss: 0.042000	Acc: 43.5% (4355/10000)
[Test]  Epoch: 74	Loss: 0.042068	Acc: 43.3% (4334/10000)
[Test]  Epoch: 75	Loss: 0.042145	Acc: 43.3% (4331/10000)
[Test]  Epoch: 76	Loss: 0.041978	Acc: 43.7% (4367/10000)
[Test]  Epoch: 77	Loss: 0.042097	Acc: 43.5% (4345/10000)
[Test]  Epoch: 78	Loss: 0.042143	Acc: 43.2% (4325/10000)
[Test]  Epoch: 79	Loss: 0.042195	Acc: 43.2% (4322/10000)
[Test]  Epoch: 80	Loss: 0.042038	Acc: 43.6% (4356/10000)
[Test]  Epoch: 81	Loss: 0.042100	Acc: 43.5% (4346/10000)
[Test]  Epoch: 82	Loss: 0.042130	Acc: 43.6% (4356/10000)
[Test]  Epoch: 83	Loss: 0.042179	Acc: 43.3% (4334/10000)
[Test]  Epoch: 84	Loss: 0.042250	Acc: 43.4% (4344/10000)
[Test]  Epoch: 85	Loss: 0.042158	Acc: 43.5% (4348/10000)
[Test]  Epoch: 86	Loss: 0.042201	Acc: 43.4% (4342/10000)
[Test]  Epoch: 87	Loss: 0.042127	Acc: 43.4% (4342/10000)
[Test]  Epoch: 88	Loss: 0.042150	Acc: 43.5% (4349/10000)
[Test]  Epoch: 89	Loss: 0.042108	Acc: 43.3% (4332/10000)
[Test]  Epoch: 90	Loss: 0.042195	Acc: 43.2% (4325/10000)
[Test]  Epoch: 91	Loss: 0.042191	Acc: 43.4% (4336/10000)
[Test]  Epoch: 92	Loss: 0.042086	Acc: 43.5% (4346/10000)
[Test]  Epoch: 93	Loss: 0.042151	Acc: 43.6% (4356/10000)
[Test]  Epoch: 94	Loss: 0.042207	Acc: 43.4% (4342/10000)
[Test]  Epoch: 95	Loss: 0.042135	Acc: 43.5% (4353/10000)
[Test]  Epoch: 96	Loss: 0.042075	Acc: 43.6% (4360/10000)
[Test]  Epoch: 97	Loss: 0.042177	Acc: 43.6% (4358/10000)
[Test]  Epoch: 98	Loss: 0.042133	Acc: 43.5% (4348/10000)
[Test]  Epoch: 99	Loss: 0.042195	Acc: 43.5% (4345/10000)
[Test]  Epoch: 100	Loss: 0.042127	Acc: 43.5% (4353/10000)
===========finish==========
['2024-08-19', '06:59:42.273616', '100', 'test', '0.042127455186843875', '43.53', '44.39']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.057060	Acc: 21.5% (2152/10000)
[Test]  Epoch: 2	Loss: 0.042080	Acc: 40.9% (4091/10000)
[Test]  Epoch: 3	Loss: 0.041291	Acc: 42.4% (4236/10000)
[Test]  Epoch: 4	Loss: 0.041225	Acc: 42.4% (4235/10000)
[Test]  Epoch: 5	Loss: 0.041113	Acc: 43.2% (4318/10000)
[Test]  Epoch: 6	Loss: 0.041364	Acc: 42.7% (4271/10000)
[Test]  Epoch: 7	Loss: 0.041242	Acc: 43.1% (4312/10000)
[Test]  Epoch: 8	Loss: 0.041429	Acc: 42.7% (4267/10000)
[Test]  Epoch: 9	Loss: 0.041557	Acc: 42.8% (4278/10000)
[Test]  Epoch: 10	Loss: 0.041652	Acc: 42.5% (4247/10000)
[Test]  Epoch: 11	Loss: 0.041668	Acc: 42.6% (4264/10000)
[Test]  Epoch: 12	Loss: 0.041592	Acc: 42.8% (4277/10000)
[Test]  Epoch: 13	Loss: 0.041641	Acc: 42.7% (4270/10000)
[Test]  Epoch: 14	Loss: 0.041727	Acc: 42.9% (4294/10000)
[Test]  Epoch: 15	Loss: 0.041843	Acc: 42.6% (4257/10000)
[Test]  Epoch: 16	Loss: 0.041693	Acc: 42.9% (4288/10000)
[Test]  Epoch: 17	Loss: 0.041787	Acc: 42.8% (4277/10000)
[Test]  Epoch: 18	Loss: 0.041857	Acc: 42.8% (4282/10000)
[Test]  Epoch: 19	Loss: 0.041829	Acc: 42.8% (4277/10000)
[Test]  Epoch: 20	Loss: 0.041909	Acc: 42.8% (4284/10000)
[Test]  Epoch: 21	Loss: 0.041951	Acc: 42.9% (4287/10000)
[Test]  Epoch: 22	Loss: 0.042037	Acc: 42.7% (4266/10000)
[Test]  Epoch: 23	Loss: 0.041941	Acc: 42.8% (4275/10000)
[Test]  Epoch: 24	Loss: 0.042080	Acc: 42.8% (4280/10000)
[Test]  Epoch: 25	Loss: 0.042046	Acc: 42.7% (4274/10000)
[Test]  Epoch: 26	Loss: 0.042151	Acc: 42.4% (4236/10000)
[Test]  Epoch: 27	Loss: 0.041909	Acc: 42.8% (4281/10000)
[Test]  Epoch: 28	Loss: 0.042189	Acc: 42.5% (4253/10000)
[Test]  Epoch: 29	Loss: 0.042192	Acc: 42.6% (4256/10000)
[Test]  Epoch: 30	Loss: 0.042208	Acc: 42.6% (4262/10000)
[Test]  Epoch: 31	Loss: 0.042493	Acc: 42.2% (4220/10000)
[Test]  Epoch: 32	Loss: 0.042330	Acc: 42.4% (4240/10000)
[Test]  Epoch: 33	Loss: 0.042116	Acc: 42.2% (4221/10000)
[Test]  Epoch: 34	Loss: 0.042192	Acc: 42.5% (4252/10000)
[Test]  Epoch: 35	Loss: 0.042231	Acc: 42.6% (4258/10000)
[Test]  Epoch: 36	Loss: 0.042157	Acc: 42.7% (4274/10000)
[Test]  Epoch: 37	Loss: 0.042253	Acc: 42.8% (4282/10000)
[Test]  Epoch: 38	Loss: 0.042473	Acc: 42.5% (4254/10000)
[Test]  Epoch: 39	Loss: 0.042512	Acc: 42.4% (4241/10000)
[Test]  Epoch: 40	Loss: 0.042351	Acc: 42.5% (4246/10000)
[Test]  Epoch: 41	Loss: 0.042433	Acc: 42.4% (4244/10000)
[Test]  Epoch: 42	Loss: 0.042397	Acc: 42.6% (4265/10000)
[Test]  Epoch: 43	Loss: 0.042357	Acc: 42.8% (4283/10000)
[Test]  Epoch: 44	Loss: 0.042383	Acc: 42.6% (4263/10000)
[Test]  Epoch: 45	Loss: 0.042458	Acc: 42.6% (4264/10000)
[Test]  Epoch: 46	Loss: 0.042531	Acc: 42.4% (4238/10000)
[Test]  Epoch: 47	Loss: 0.042608	Acc: 42.4% (4235/10000)
[Test]  Epoch: 48	Loss: 0.042551	Acc: 42.4% (4243/10000)
[Test]  Epoch: 49	Loss: 0.042439	Acc: 42.5% (4254/10000)
[Test]  Epoch: 50	Loss: 0.042625	Acc: 42.4% (4237/10000)
[Test]  Epoch: 51	Loss: 0.042548	Acc: 42.4% (4242/10000)
[Test]  Epoch: 52	Loss: 0.042362	Acc: 42.8% (4279/10000)
[Test]  Epoch: 53	Loss: 0.042553	Acc: 42.6% (4256/10000)
[Test]  Epoch: 54	Loss: 0.042615	Acc: 42.4% (4241/10000)
[Test]  Epoch: 55	Loss: 0.042565	Acc: 42.6% (4262/10000)
[Test]  Epoch: 56	Loss: 0.042616	Acc: 42.4% (4241/10000)
[Test]  Epoch: 57	Loss: 0.042747	Acc: 42.2% (4218/10000)
[Test]  Epoch: 58	Loss: 0.042445	Acc: 42.6% (4256/10000)
[Test]  Epoch: 59	Loss: 0.042631	Acc: 42.5% (4253/10000)
[Test]  Epoch: 60	Loss: 0.042606	Acc: 42.4% (4237/10000)
[Test]  Epoch: 61	Loss: 0.042690	Acc: 42.4% (4239/10000)
[Test]  Epoch: 62	Loss: 0.042719	Acc: 42.3% (4228/10000)
[Test]  Epoch: 63	Loss: 0.042507	Acc: 42.7% (4267/10000)
[Test]  Epoch: 64	Loss: 0.042515	Acc: 42.7% (4269/10000)
[Test]  Epoch: 65	Loss: 0.042612	Acc: 42.5% (4251/10000)
[Test]  Epoch: 66	Loss: 0.042584	Acc: 42.6% (4259/10000)
[Test]  Epoch: 67	Loss: 0.042641	Acc: 42.3% (4233/10000)
[Test]  Epoch: 68	Loss: 0.042750	Acc: 42.3% (4228/10000)
[Test]  Epoch: 69	Loss: 0.042689	Acc: 42.3% (4230/10000)
[Test]  Epoch: 70	Loss: 0.042661	Acc: 42.3% (4232/10000)
[Test]  Epoch: 71	Loss: 0.042629	Acc: 42.5% (4248/10000)
[Test]  Epoch: 72	Loss: 0.042712	Acc: 42.4% (4236/10000)
[Test]  Epoch: 73	Loss: 0.042574	Acc: 42.3% (4234/10000)
[Test]  Epoch: 74	Loss: 0.042620	Acc: 42.4% (4240/10000)
[Test]  Epoch: 75	Loss: 0.042709	Acc: 42.2% (4217/10000)
[Test]  Epoch: 76	Loss: 0.042561	Acc: 42.5% (4255/10000)
[Test]  Epoch: 77	Loss: 0.042654	Acc: 42.3% (4231/10000)
[Test]  Epoch: 78	Loss: 0.042710	Acc: 42.1% (4215/10000)
[Test]  Epoch: 79	Loss: 0.042766	Acc: 42.3% (4226/10000)
[Test]  Epoch: 80	Loss: 0.042608	Acc: 42.5% (4248/10000)
[Test]  Epoch: 81	Loss: 0.042672	Acc: 42.4% (4242/10000)
[Test]  Epoch: 82	Loss: 0.042713	Acc: 42.4% (4235/10000)
[Test]  Epoch: 83	Loss: 0.042726	Acc: 42.3% (4229/10000)
[Test]  Epoch: 84	Loss: 0.042785	Acc: 42.3% (4230/10000)
[Test]  Epoch: 85	Loss: 0.042723	Acc: 42.3% (4233/10000)
[Test]  Epoch: 86	Loss: 0.042760	Acc: 42.2% (4219/10000)
[Test]  Epoch: 87	Loss: 0.042692	Acc: 42.4% (4235/10000)
[Test]  Epoch: 88	Loss: 0.042715	Acc: 42.5% (4246/10000)
[Test]  Epoch: 89	Loss: 0.042689	Acc: 42.4% (4236/10000)
[Test]  Epoch: 90	Loss: 0.042780	Acc: 42.4% (4238/10000)
[Test]  Epoch: 91	Loss: 0.042788	Acc: 42.2% (4222/10000)
[Test]  Epoch: 92	Loss: 0.042640	Acc: 42.5% (4253/10000)
[Test]  Epoch: 93	Loss: 0.042732	Acc: 42.4% (4236/10000)
[Test]  Epoch: 94	Loss: 0.042758	Acc: 42.3% (4226/10000)
[Test]  Epoch: 95	Loss: 0.042742	Acc: 42.4% (4239/10000)
[Test]  Epoch: 96	Loss: 0.042667	Acc: 42.5% (4252/10000)
[Test]  Epoch: 97	Loss: 0.042746	Acc: 42.3% (4231/10000)
[Test]  Epoch: 98	Loss: 0.042700	Acc: 42.3% (4228/10000)
[Test]  Epoch: 99	Loss: 0.042776	Acc: 42.2% (4225/10000)
[Test]  Epoch: 100	Loss: 0.042701	Acc: 42.3% (4231/10000)
===========finish==========
['2024-08-19', '07:03:07.315074', '100', 'test', '0.042700590407848356', '42.31', '43.18']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.061195	Acc: 16.7% (1672/10000)
[Test]  Epoch: 2	Loss: 0.043052	Acc: 39.6% (3957/10000)
[Test]  Epoch: 3	Loss: 0.042265	Acc: 41.4% (4139/10000)
[Test]  Epoch: 4	Loss: 0.042254	Acc: 41.7% (4174/10000)
[Test]  Epoch: 5	Loss: 0.042036	Acc: 42.0% (4203/10000)
[Test]  Epoch: 6	Loss: 0.042323	Acc: 41.9% (4189/10000)
[Test]  Epoch: 7	Loss: 0.042219	Acc: 42.2% (4217/10000)
[Test]  Epoch: 8	Loss: 0.042300	Acc: 42.2% (4224/10000)
[Test]  Epoch: 9	Loss: 0.042332	Acc: 42.2% (4224/10000)
[Test]  Epoch: 10	Loss: 0.042486	Acc: 42.0% (4204/10000)
[Test]  Epoch: 11	Loss: 0.042443	Acc: 42.3% (4228/10000)
[Test]  Epoch: 12	Loss: 0.042358	Acc: 42.3% (4229/10000)
[Test]  Epoch: 13	Loss: 0.042484	Acc: 41.9% (4194/10000)
[Test]  Epoch: 14	Loss: 0.042605	Acc: 42.1% (4209/10000)
[Test]  Epoch: 15	Loss: 0.042659	Acc: 42.1% (4208/10000)
[Test]  Epoch: 16	Loss: 0.042540	Acc: 42.1% (4206/10000)
[Test]  Epoch: 17	Loss: 0.042679	Acc: 42.0% (4202/10000)
[Test]  Epoch: 18	Loss: 0.042690	Acc: 41.8% (4180/10000)
[Test]  Epoch: 19	Loss: 0.042642	Acc: 42.2% (4221/10000)
[Test]  Epoch: 20	Loss: 0.042734	Acc: 42.2% (4219/10000)
[Test]  Epoch: 21	Loss: 0.042684	Acc: 42.2% (4221/10000)
[Test]  Epoch: 22	Loss: 0.042819	Acc: 42.0% (4205/10000)
[Test]  Epoch: 23	Loss: 0.042756	Acc: 42.2% (4220/10000)
[Test]  Epoch: 24	Loss: 0.042845	Acc: 42.2% (4222/10000)
[Test]  Epoch: 25	Loss: 0.042813	Acc: 42.4% (4238/10000)
[Test]  Epoch: 26	Loss: 0.042914	Acc: 42.0% (4198/10000)
[Test]  Epoch: 27	Loss: 0.042762	Acc: 42.3% (4228/10000)
[Test]  Epoch: 28	Loss: 0.042993	Acc: 42.1% (4215/10000)
[Test]  Epoch: 29	Loss: 0.042986	Acc: 41.9% (4192/10000)
[Test]  Epoch: 30	Loss: 0.042969	Acc: 42.1% (4213/10000)
[Test]  Epoch: 31	Loss: 0.043223	Acc: 42.0% (4196/10000)
[Test]  Epoch: 32	Loss: 0.042972	Acc: 42.1% (4213/10000)
[Test]  Epoch: 33	Loss: 0.042950	Acc: 42.1% (4214/10000)
[Test]  Epoch: 34	Loss: 0.042978	Acc: 42.1% (4210/10000)
[Test]  Epoch: 35	Loss: 0.042934	Acc: 42.3% (4233/10000)
[Test]  Epoch: 36	Loss: 0.042974	Acc: 42.3% (4227/10000)
[Test]  Epoch: 37	Loss: 0.042977	Acc: 42.5% (4245/10000)
[Test]  Epoch: 38	Loss: 0.043223	Acc: 42.0% (4196/10000)
[Test]  Epoch: 39	Loss: 0.043195	Acc: 42.0% (4198/10000)
[Test]  Epoch: 40	Loss: 0.043129	Acc: 41.9% (4187/10000)
[Test]  Epoch: 41	Loss: 0.043200	Acc: 42.0% (4197/10000)
[Test]  Epoch: 42	Loss: 0.043066	Acc: 42.3% (4229/10000)
[Test]  Epoch: 43	Loss: 0.043087	Acc: 42.3% (4231/10000)
[Test]  Epoch: 44	Loss: 0.043092	Acc: 42.2% (4218/10000)
[Test]  Epoch: 45	Loss: 0.043181	Acc: 42.0% (4203/10000)
[Test]  Epoch: 46	Loss: 0.043272	Acc: 42.0% (4195/10000)
[Test]  Epoch: 47	Loss: 0.043371	Acc: 41.9% (4186/10000)
[Test]  Epoch: 48	Loss: 0.043313	Acc: 42.2% (4221/10000)
[Test]  Epoch: 49	Loss: 0.043253	Acc: 42.0% (4201/10000)
[Test]  Epoch: 50	Loss: 0.043399	Acc: 41.9% (4189/10000)
[Test]  Epoch: 51	Loss: 0.043273	Acc: 42.0% (4200/10000)
[Test]  Epoch: 52	Loss: 0.043135	Acc: 42.3% (4226/10000)
[Test]  Epoch: 53	Loss: 0.043246	Acc: 42.1% (4215/10000)
[Test]  Epoch: 54	Loss: 0.043393	Acc: 41.9% (4188/10000)
[Test]  Epoch: 55	Loss: 0.043329	Acc: 42.1% (4213/10000)
[Test]  Epoch: 56	Loss: 0.043392	Acc: 41.7% (4168/10000)
[Test]  Epoch: 57	Loss: 0.043485	Acc: 41.7% (4173/10000)
[Test]  Epoch: 58	Loss: 0.043160	Acc: 42.1% (4211/10000)
[Test]  Epoch: 59	Loss: 0.043355	Acc: 42.0% (4201/10000)
[Test]  Epoch: 60	Loss: 0.043358	Acc: 42.1% (4207/10000)
[Test]  Epoch: 61	Loss: 0.043455	Acc: 41.9% (4185/10000)
[Test]  Epoch: 62	Loss: 0.043451	Acc: 41.9% (4185/10000)
[Test]  Epoch: 63	Loss: 0.043240	Acc: 42.1% (4207/10000)
[Test]  Epoch: 64	Loss: 0.043245	Acc: 42.0% (4199/10000)
[Test]  Epoch: 65	Loss: 0.043364	Acc: 41.8% (4183/10000)
[Test]  Epoch: 66	Loss: 0.043318	Acc: 42.0% (4199/10000)
[Test]  Epoch: 67	Loss: 0.043389	Acc: 42.1% (4207/10000)
[Test]  Epoch: 68	Loss: 0.043512	Acc: 41.9% (4193/10000)
[Test]  Epoch: 69	Loss: 0.043437	Acc: 41.9% (4194/10000)
[Test]  Epoch: 70	Loss: 0.043395	Acc: 41.8% (4184/10000)
[Test]  Epoch: 71	Loss: 0.043367	Acc: 41.9% (4192/10000)
[Test]  Epoch: 72	Loss: 0.043474	Acc: 41.9% (4187/10000)
[Test]  Epoch: 73	Loss: 0.043323	Acc: 41.9% (4191/10000)
[Test]  Epoch: 74	Loss: 0.043380	Acc: 41.9% (4189/10000)
[Test]  Epoch: 75	Loss: 0.043440	Acc: 41.9% (4186/10000)
[Test]  Epoch: 76	Loss: 0.043313	Acc: 42.0% (4201/10000)
[Test]  Epoch: 77	Loss: 0.043389	Acc: 42.0% (4201/10000)
[Test]  Epoch: 78	Loss: 0.043459	Acc: 41.7% (4169/10000)
[Test]  Epoch: 79	Loss: 0.043519	Acc: 41.7% (4170/10000)
[Test]  Epoch: 80	Loss: 0.043382	Acc: 42.0% (4195/10000)
[Test]  Epoch: 81	Loss: 0.043446	Acc: 41.8% (4184/10000)
[Test]  Epoch: 82	Loss: 0.043439	Acc: 41.9% (4185/10000)
[Test]  Epoch: 83	Loss: 0.043479	Acc: 41.8% (4183/10000)
[Test]  Epoch: 84	Loss: 0.043510	Acc: 42.0% (4201/10000)
[Test]  Epoch: 85	Loss: 0.043462	Acc: 41.9% (4189/10000)
[Test]  Epoch: 86	Loss: 0.043487	Acc: 41.6% (4165/10000)
[Test]  Epoch: 87	Loss: 0.043425	Acc: 42.0% (4195/10000)
[Test]  Epoch: 88	Loss: 0.043433	Acc: 42.1% (4207/10000)
[Test]  Epoch: 89	Loss: 0.043401	Acc: 42.0% (4196/10000)
[Test]  Epoch: 90	Loss: 0.043498	Acc: 41.9% (4187/10000)
[Test]  Epoch: 91	Loss: 0.043502	Acc: 41.9% (4188/10000)
[Test]  Epoch: 92	Loss: 0.043386	Acc: 42.0% (4199/10000)
[Test]  Epoch: 93	Loss: 0.043431	Acc: 42.0% (4200/10000)
[Test]  Epoch: 94	Loss: 0.043501	Acc: 41.7% (4172/10000)
[Test]  Epoch: 95	Loss: 0.043464	Acc: 41.9% (4191/10000)
[Test]  Epoch: 96	Loss: 0.043400	Acc: 42.0% (4203/10000)
[Test]  Epoch: 97	Loss: 0.043467	Acc: 41.8% (4183/10000)
[Test]  Epoch: 98	Loss: 0.043419	Acc: 41.8% (4181/10000)
[Test]  Epoch: 99	Loss: 0.043476	Acc: 41.9% (4185/10000)
[Test]  Epoch: 100	Loss: 0.043439	Acc: 41.9% (4185/10000)
===========finish==========
['2024-08-19', '07:06:37.586395', '100', 'test', '0.04343909522294998', '41.85', '42.45']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.068703	Acc: 10.9% (1093/10000)
[Test]  Epoch: 2	Loss: 0.044315	Acc: 38.0% (3795/10000)
[Test]  Epoch: 3	Loss: 0.043366	Acc: 39.8% (3984/10000)
[Test]  Epoch: 4	Loss: 0.043311	Acc: 40.0% (3997/10000)
[Test]  Epoch: 5	Loss: 0.043149	Acc: 40.0% (3998/10000)
[Test]  Epoch: 6	Loss: 0.043320	Acc: 40.0% (4003/10000)
[Test]  Epoch: 7	Loss: 0.043269	Acc: 40.2% (4025/10000)
[Test]  Epoch: 8	Loss: 0.043285	Acc: 40.5% (4047/10000)
[Test]  Epoch: 9	Loss: 0.043278	Acc: 40.5% (4052/10000)
[Test]  Epoch: 10	Loss: 0.043405	Acc: 40.3% (4027/10000)
[Test]  Epoch: 11	Loss: 0.043343	Acc: 40.3% (4034/10000)
[Test]  Epoch: 12	Loss: 0.043292	Acc: 40.5% (4046/10000)
[Test]  Epoch: 13	Loss: 0.043420	Acc: 40.2% (4017/10000)
[Test]  Epoch: 14	Loss: 0.043576	Acc: 40.2% (4025/10000)
[Test]  Epoch: 15	Loss: 0.043586	Acc: 40.4% (4041/10000)
[Test]  Epoch: 16	Loss: 0.043508	Acc: 40.2% (4020/10000)
[Test]  Epoch: 17	Loss: 0.043574	Acc: 40.4% (4039/10000)
[Test]  Epoch: 18	Loss: 0.043670	Acc: 40.4% (4039/10000)
[Test]  Epoch: 19	Loss: 0.043573	Acc: 40.3% (4030/10000)
[Test]  Epoch: 20	Loss: 0.043660	Acc: 40.4% (4042/10000)
[Test]  Epoch: 21	Loss: 0.043597	Acc: 40.4% (4039/10000)
[Test]  Epoch: 22	Loss: 0.043686	Acc: 40.5% (4052/10000)
[Test]  Epoch: 23	Loss: 0.043625	Acc: 40.4% (4044/10000)
[Test]  Epoch: 24	Loss: 0.043819	Acc: 40.3% (4034/10000)
[Test]  Epoch: 25	Loss: 0.043738	Acc: 40.4% (4036/10000)
[Test]  Epoch: 26	Loss: 0.043779	Acc: 40.5% (4053/10000)
[Test]  Epoch: 27	Loss: 0.043687	Acc: 40.5% (4050/10000)
[Test]  Epoch: 28	Loss: 0.043850	Acc: 40.4% (4044/10000)
[Test]  Epoch: 29	Loss: 0.043805	Acc: 40.4% (4042/10000)
[Test]  Epoch: 30	Loss: 0.043895	Acc: 40.4% (4040/10000)
[Test]  Epoch: 31	Loss: 0.044088	Acc: 40.3% (4027/10000)
[Test]  Epoch: 32	Loss: 0.043934	Acc: 40.4% (4035/10000)
[Test]  Epoch: 33	Loss: 0.043883	Acc: 40.4% (4040/10000)
[Test]  Epoch: 34	Loss: 0.043858	Acc: 40.7% (4073/10000)
[Test]  Epoch: 35	Loss: 0.043861	Acc: 40.6% (4056/10000)
[Test]  Epoch: 36	Loss: 0.043836	Acc: 40.7% (4068/10000)
[Test]  Epoch: 37	Loss: 0.043884	Acc: 40.8% (4080/10000)
[Test]  Epoch: 38	Loss: 0.044042	Acc: 40.6% (4063/10000)
[Test]  Epoch: 39	Loss: 0.044041	Acc: 40.5% (4052/10000)
[Test]  Epoch: 40	Loss: 0.044014	Acc: 40.7% (4067/10000)
[Test]  Epoch: 41	Loss: 0.044114	Acc: 40.5% (4051/10000)
[Test]  Epoch: 42	Loss: 0.043953	Acc: 40.7% (4071/10000)
[Test]  Epoch: 43	Loss: 0.043961	Acc: 40.8% (4080/10000)
[Test]  Epoch: 44	Loss: 0.043958	Acc: 40.7% (4074/10000)
[Test]  Epoch: 45	Loss: 0.044027	Acc: 40.7% (4070/10000)
[Test]  Epoch: 46	Loss: 0.044125	Acc: 40.6% (4058/10000)
[Test]  Epoch: 47	Loss: 0.044155	Acc: 40.8% (4078/10000)
[Test]  Epoch: 48	Loss: 0.044160	Acc: 40.7% (4074/10000)
[Test]  Epoch: 49	Loss: 0.044087	Acc: 40.8% (4078/10000)
[Test]  Epoch: 50	Loss: 0.044292	Acc: 40.5% (4045/10000)
[Test]  Epoch: 51	Loss: 0.044202	Acc: 40.5% (4053/10000)
[Test]  Epoch: 52	Loss: 0.044006	Acc: 40.8% (4076/10000)
[Test]  Epoch: 53	Loss: 0.044185	Acc: 40.4% (4041/10000)
[Test]  Epoch: 54	Loss: 0.044271	Acc: 40.5% (4055/10000)
[Test]  Epoch: 55	Loss: 0.044161	Acc: 40.7% (4069/10000)
[Test]  Epoch: 56	Loss: 0.044269	Acc: 40.5% (4054/10000)
[Test]  Epoch: 57	Loss: 0.044338	Acc: 40.5% (4048/10000)
[Test]  Epoch: 58	Loss: 0.044045	Acc: 40.9% (4088/10000)
[Test]  Epoch: 59	Loss: 0.044255	Acc: 40.6% (4056/10000)
[Test]  Epoch: 60	Loss: 0.044191	Acc: 40.7% (4074/10000)
[Test]  Epoch: 61	Loss: 0.044302	Acc: 40.7% (4066/10000)
[Test]  Epoch: 62	Loss: 0.044320	Acc: 40.8% (4084/10000)
[Test]  Epoch: 63	Loss: 0.044132	Acc: 40.8% (4082/10000)
[Test]  Epoch: 64	Loss: 0.044129	Acc: 41.0% (4099/10000)
[Test]  Epoch: 65	Loss: 0.044243	Acc: 40.8% (4080/10000)
[Test]  Epoch: 66	Loss: 0.044199	Acc: 40.8% (4075/10000)
[Test]  Epoch: 67	Loss: 0.044273	Acc: 40.7% (4071/10000)
[Test]  Epoch: 68	Loss: 0.044384	Acc: 40.6% (4061/10000)
[Test]  Epoch: 69	Loss: 0.044320	Acc: 40.6% (4063/10000)
[Test]  Epoch: 70	Loss: 0.044271	Acc: 40.7% (4074/10000)
[Test]  Epoch: 71	Loss: 0.044261	Acc: 40.8% (4084/10000)
[Test]  Epoch: 72	Loss: 0.044349	Acc: 40.5% (4049/10000)
[Test]  Epoch: 73	Loss: 0.044174	Acc: 40.7% (4073/10000)
[Test]  Epoch: 74	Loss: 0.044248	Acc: 40.8% (4080/10000)
[Test]  Epoch: 75	Loss: 0.044308	Acc: 40.7% (4067/10000)
[Test]  Epoch: 76	Loss: 0.044190	Acc: 40.9% (4092/10000)
[Test]  Epoch: 77	Loss: 0.044262	Acc: 40.8% (4077/10000)
[Test]  Epoch: 78	Loss: 0.044339	Acc: 40.4% (4043/10000)
[Test]  Epoch: 79	Loss: 0.044394	Acc: 40.5% (4053/10000)
[Test]  Epoch: 80	Loss: 0.044256	Acc: 40.8% (4077/10000)
[Test]  Epoch: 81	Loss: 0.044296	Acc: 40.6% (4065/10000)
[Test]  Epoch: 82	Loss: 0.044299	Acc: 40.6% (4058/10000)
[Test]  Epoch: 83	Loss: 0.044318	Acc: 40.5% (4055/10000)
[Test]  Epoch: 84	Loss: 0.044373	Acc: 40.6% (4057/10000)
[Test]  Epoch: 85	Loss: 0.044346	Acc: 40.6% (4058/10000)
[Test]  Epoch: 86	Loss: 0.044369	Acc: 40.3% (4034/10000)
[Test]  Epoch: 87	Loss: 0.044301	Acc: 40.6% (4061/10000)
[Test]  Epoch: 88	Loss: 0.044308	Acc: 40.5% (4052/10000)
[Test]  Epoch: 89	Loss: 0.044299	Acc: 40.6% (4059/10000)
[Test]  Epoch: 90	Loss: 0.044394	Acc: 40.5% (4055/10000)
[Test]  Epoch: 91	Loss: 0.044388	Acc: 40.6% (4064/10000)
[Test]  Epoch: 92	Loss: 0.044242	Acc: 40.9% (4092/10000)
[Test]  Epoch: 93	Loss: 0.044323	Acc: 40.6% (4064/10000)
[Test]  Epoch: 94	Loss: 0.044363	Acc: 40.6% (4063/10000)
[Test]  Epoch: 95	Loss: 0.044367	Acc: 40.6% (4059/10000)
[Test]  Epoch: 96	Loss: 0.044307	Acc: 40.6% (4060/10000)
[Test]  Epoch: 97	Loss: 0.044337	Acc: 40.5% (4050/10000)
[Test]  Epoch: 98	Loss: 0.044325	Acc: 40.7% (4071/10000)
[Test]  Epoch: 99	Loss: 0.044401	Acc: 40.4% (4042/10000)
[Test]  Epoch: 100	Loss: 0.044327	Acc: 40.5% (4052/10000)
===========finish==========
['2024-08-19', '07:10:02.660233', '100', 'test', '0.04432703629732132', '40.52', '40.99']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.074258	Acc: 7.7% (771/10000)
[Test]  Epoch: 2	Loss: 0.044600	Acc: 37.6% (3757/10000)
[Test]  Epoch: 3	Loss: 0.043589	Acc: 39.5% (3954/10000)
[Test]  Epoch: 4	Loss: 0.043509	Acc: 39.6% (3963/10000)
[Test]  Epoch: 5	Loss: 0.043441	Acc: 39.6% (3959/10000)
[Test]  Epoch: 6	Loss: 0.043460	Acc: 39.9% (3985/10000)
[Test]  Epoch: 7	Loss: 0.043378	Acc: 40.0% (4003/10000)
[Test]  Epoch: 8	Loss: 0.043479	Acc: 39.9% (3994/10000)
[Test]  Epoch: 9	Loss: 0.043490	Acc: 39.9% (3994/10000)
[Test]  Epoch: 10	Loss: 0.043557	Acc: 40.1% (4008/10000)
[Test]  Epoch: 11	Loss: 0.043514	Acc: 40.2% (4019/10000)
[Test]  Epoch: 12	Loss: 0.043518	Acc: 40.2% (4020/10000)
[Test]  Epoch: 13	Loss: 0.043672	Acc: 40.0% (3998/10000)
[Test]  Epoch: 14	Loss: 0.043719	Acc: 40.1% (4012/10000)
[Test]  Epoch: 15	Loss: 0.043755	Acc: 40.2% (4016/10000)
[Test]  Epoch: 16	Loss: 0.043647	Acc: 40.0% (4002/10000)
[Test]  Epoch: 17	Loss: 0.043773	Acc: 40.0% (4001/10000)
[Test]  Epoch: 18	Loss: 0.043807	Acc: 40.2% (4025/10000)
[Test]  Epoch: 19	Loss: 0.043714	Acc: 40.4% (4038/10000)
[Test]  Epoch: 20	Loss: 0.043895	Acc: 40.2% (4018/10000)
[Test]  Epoch: 21	Loss: 0.043809	Acc: 40.1% (4006/10000)
[Test]  Epoch: 22	Loss: 0.043895	Acc: 40.0% (4004/10000)
[Test]  Epoch: 23	Loss: 0.043812	Acc: 40.2% (4021/10000)
[Test]  Epoch: 24	Loss: 0.043974	Acc: 40.0% (4005/10000)
[Test]  Epoch: 25	Loss: 0.043943	Acc: 40.2% (4024/10000)
[Test]  Epoch: 26	Loss: 0.044006	Acc: 40.1% (4012/10000)
[Test]  Epoch: 27	Loss: 0.043833	Acc: 40.4% (4036/10000)
[Test]  Epoch: 28	Loss: 0.044020	Acc: 40.2% (4023/10000)
[Test]  Epoch: 29	Loss: 0.043953	Acc: 40.0% (4003/10000)
[Test]  Epoch: 30	Loss: 0.044032	Acc: 40.2% (4016/10000)
[Test]  Epoch: 31	Loss: 0.044305	Acc: 40.1% (4007/10000)
[Test]  Epoch: 32	Loss: 0.044085	Acc: 40.2% (4025/10000)
[Test]  Epoch: 33	Loss: 0.044059	Acc: 40.1% (4011/10000)
[Test]  Epoch: 34	Loss: 0.044055	Acc: 40.2% (4025/10000)
[Test]  Epoch: 35	Loss: 0.044088	Acc: 40.3% (4026/10000)
[Test]  Epoch: 36	Loss: 0.044018	Acc: 40.4% (4044/10000)
[Test]  Epoch: 37	Loss: 0.044109	Acc: 40.2% (4024/10000)
[Test]  Epoch: 38	Loss: 0.044241	Acc: 40.1% (4011/10000)
[Test]  Epoch: 39	Loss: 0.044233	Acc: 40.1% (4013/10000)
[Test]  Epoch: 40	Loss: 0.044179	Acc: 40.2% (4016/10000)
[Test]  Epoch: 41	Loss: 0.044291	Acc: 39.9% (3994/10000)
[Test]  Epoch: 42	Loss: 0.044146	Acc: 40.3% (4027/10000)
[Test]  Epoch: 43	Loss: 0.044197	Acc: 40.3% (4029/10000)
[Test]  Epoch: 44	Loss: 0.044160	Acc: 40.3% (4026/10000)
[Test]  Epoch: 45	Loss: 0.044201	Acc: 40.3% (4031/10000)
[Test]  Epoch: 46	Loss: 0.044333	Acc: 40.1% (4007/10000)
[Test]  Epoch: 47	Loss: 0.044408	Acc: 40.1% (4013/10000)
[Test]  Epoch: 48	Loss: 0.044309	Acc: 40.3% (4031/10000)
[Test]  Epoch: 49	Loss: 0.044296	Acc: 40.2% (4025/10000)
[Test]  Epoch: 50	Loss: 0.044430	Acc: 39.9% (3986/10000)
[Test]  Epoch: 51	Loss: 0.044364	Acc: 40.3% (4027/10000)
[Test]  Epoch: 52	Loss: 0.044187	Acc: 40.5% (4050/10000)
[Test]  Epoch: 53	Loss: 0.044321	Acc: 40.0% (4005/10000)
[Test]  Epoch: 54	Loss: 0.044411	Acc: 39.9% (3991/10000)
[Test]  Epoch: 55	Loss: 0.044377	Acc: 40.1% (4012/10000)
[Test]  Epoch: 56	Loss: 0.044436	Acc: 40.1% (4007/10000)
[Test]  Epoch: 57	Loss: 0.044507	Acc: 40.1% (4009/10000)
[Test]  Epoch: 58	Loss: 0.044228	Acc: 40.3% (4034/10000)
[Test]  Epoch: 59	Loss: 0.044489	Acc: 40.2% (4022/10000)
[Test]  Epoch: 60	Loss: 0.044398	Acc: 40.3% (4029/10000)
[Test]  Epoch: 61	Loss: 0.044498	Acc: 40.3% (4029/10000)
[Test]  Epoch: 62	Loss: 0.044525	Acc: 40.2% (4018/10000)
[Test]  Epoch: 63	Loss: 0.044326	Acc: 40.3% (4026/10000)
[Test]  Epoch: 64	Loss: 0.044344	Acc: 40.4% (4035/10000)
[Test]  Epoch: 65	Loss: 0.044434	Acc: 40.2% (4021/10000)
[Test]  Epoch: 66	Loss: 0.044418	Acc: 40.0% (4004/10000)
[Test]  Epoch: 67	Loss: 0.044476	Acc: 40.2% (4020/10000)
[Test]  Epoch: 68	Loss: 0.044575	Acc: 40.1% (4012/10000)
[Test]  Epoch: 69	Loss: 0.044513	Acc: 40.0% (4002/10000)
[Test]  Epoch: 70	Loss: 0.044462	Acc: 40.1% (4014/10000)
[Test]  Epoch: 71	Loss: 0.044446	Acc: 40.2% (4019/10000)
[Test]  Epoch: 72	Loss: 0.044540	Acc: 40.0% (4003/10000)
[Test]  Epoch: 73	Loss: 0.044385	Acc: 40.3% (4031/10000)
[Test]  Epoch: 74	Loss: 0.044456	Acc: 40.2% (4018/10000)
[Test]  Epoch: 75	Loss: 0.044490	Acc: 40.1% (4009/10000)
[Test]  Epoch: 76	Loss: 0.044395	Acc: 40.4% (4035/10000)
[Test]  Epoch: 77	Loss: 0.044446	Acc: 40.1% (4013/10000)
[Test]  Epoch: 78	Loss: 0.044500	Acc: 40.0% (3996/10000)
[Test]  Epoch: 79	Loss: 0.044594	Acc: 40.0% (3999/10000)
[Test]  Epoch: 80	Loss: 0.044457	Acc: 40.1% (4006/10000)
[Test]  Epoch: 81	Loss: 0.044450	Acc: 40.2% (4016/10000)
[Test]  Epoch: 82	Loss: 0.044477	Acc: 40.1% (4009/10000)
[Test]  Epoch: 83	Loss: 0.044499	Acc: 40.1% (4014/10000)
[Test]  Epoch: 84	Loss: 0.044569	Acc: 40.1% (4015/10000)
[Test]  Epoch: 85	Loss: 0.044523	Acc: 40.1% (4010/10000)
[Test]  Epoch: 86	Loss: 0.044551	Acc: 40.0% (4003/10000)
[Test]  Epoch: 87	Loss: 0.044490	Acc: 40.2% (4016/10000)
[Test]  Epoch: 88	Loss: 0.044506	Acc: 40.1% (4015/10000)
[Test]  Epoch: 89	Loss: 0.044478	Acc: 40.2% (4016/10000)
[Test]  Epoch: 90	Loss: 0.044562	Acc: 40.1% (4008/10000)
[Test]  Epoch: 91	Loss: 0.044580	Acc: 40.1% (4010/10000)
[Test]  Epoch: 92	Loss: 0.044429	Acc: 40.3% (4029/10000)
[Test]  Epoch: 93	Loss: 0.044501	Acc: 40.2% (4025/10000)
[Test]  Epoch: 94	Loss: 0.044547	Acc: 40.0% (4004/10000)
[Test]  Epoch: 95	Loss: 0.044542	Acc: 40.1% (4013/10000)
[Test]  Epoch: 96	Loss: 0.044503	Acc: 40.1% (4011/10000)
[Test]  Epoch: 97	Loss: 0.044516	Acc: 40.1% (4011/10000)
[Test]  Epoch: 98	Loss: 0.044494	Acc: 40.1% (4006/10000)
[Test]  Epoch: 99	Loss: 0.044566	Acc: 39.8% (3983/10000)
[Test]  Epoch: 100	Loss: 0.044515	Acc: 40.1% (4013/10000)
===========finish==========
['2024-08-19', '07:13:33.681390', '100', 'test', '0.044514712655544283', '40.13', '40.5']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.082865	Acc: 5.0% (502/10000)
[Test]  Epoch: 2	Loss: 0.044799	Acc: 37.0% (3696/10000)
[Test]  Epoch: 3	Loss: 0.043901	Acc: 38.9% (3891/10000)
[Test]  Epoch: 4	Loss: 0.043857	Acc: 39.3% (3926/10000)
[Test]  Epoch: 5	Loss: 0.043800	Acc: 39.3% (3930/10000)
[Test]  Epoch: 6	Loss: 0.043931	Acc: 39.0% (3904/10000)
[Test]  Epoch: 7	Loss: 0.043900	Acc: 39.0% (3897/10000)
[Test]  Epoch: 8	Loss: 0.043959	Acc: 38.9% (3891/10000)
[Test]  Epoch: 9	Loss: 0.043951	Acc: 39.3% (3933/10000)
[Test]  Epoch: 10	Loss: 0.044092	Acc: 39.2% (3921/10000)
[Test]  Epoch: 11	Loss: 0.044090	Acc: 39.0% (3903/10000)
[Test]  Epoch: 12	Loss: 0.044185	Acc: 39.0% (3901/10000)
[Test]  Epoch: 13	Loss: 0.044261	Acc: 39.1% (3908/10000)
[Test]  Epoch: 14	Loss: 0.044235	Acc: 39.1% (3907/10000)
[Test]  Epoch: 15	Loss: 0.044291	Acc: 39.0% (3901/10000)
[Test]  Epoch: 16	Loss: 0.044225	Acc: 39.2% (3918/10000)
[Test]  Epoch: 17	Loss: 0.044310	Acc: 39.0% (3895/10000)
[Test]  Epoch: 18	Loss: 0.044380	Acc: 39.1% (3907/10000)
[Test]  Epoch: 19	Loss: 0.044306	Acc: 39.1% (3912/10000)
[Test]  Epoch: 20	Loss: 0.044413	Acc: 39.3% (3930/10000)
[Test]  Epoch: 21	Loss: 0.044332	Acc: 39.4% (3936/10000)
[Test]  Epoch: 22	Loss: 0.044387	Acc: 39.2% (3918/10000)
[Test]  Epoch: 23	Loss: 0.044411	Acc: 39.2% (3925/10000)
[Test]  Epoch: 24	Loss: 0.044621	Acc: 39.2% (3925/10000)
[Test]  Epoch: 25	Loss: 0.044548	Acc: 39.4% (3936/10000)
[Test]  Epoch: 26	Loss: 0.044613	Acc: 39.3% (3929/10000)
[Test]  Epoch: 27	Loss: 0.044494	Acc: 39.4% (3939/10000)
[Test]  Epoch: 28	Loss: 0.044660	Acc: 39.3% (3927/10000)
[Test]  Epoch: 29	Loss: 0.044571	Acc: 39.1% (3908/10000)
[Test]  Epoch: 30	Loss: 0.044603	Acc: 39.2% (3919/10000)
[Test]  Epoch: 31	Loss: 0.044810	Acc: 39.1% (3909/10000)
[Test]  Epoch: 32	Loss: 0.044729	Acc: 39.1% (3907/10000)
[Test]  Epoch: 33	Loss: 0.044732	Acc: 39.1% (3913/10000)
[Test]  Epoch: 34	Loss: 0.044715	Acc: 39.1% (3914/10000)
[Test]  Epoch: 35	Loss: 0.044775	Acc: 39.1% (3911/10000)
[Test]  Epoch: 36	Loss: 0.044685	Acc: 39.3% (3928/10000)
[Test]  Epoch: 37	Loss: 0.044703	Acc: 39.5% (3948/10000)
[Test]  Epoch: 38	Loss: 0.044847	Acc: 39.3% (3928/10000)
[Test]  Epoch: 39	Loss: 0.044831	Acc: 39.2% (3916/10000)
[Test]  Epoch: 40	Loss: 0.044853	Acc: 39.4% (3940/10000)
[Test]  Epoch: 41	Loss: 0.044884	Acc: 39.3% (3934/10000)
[Test]  Epoch: 42	Loss: 0.044863	Acc: 39.3% (3934/10000)
[Test]  Epoch: 43	Loss: 0.044833	Acc: 39.4% (3940/10000)
[Test]  Epoch: 44	Loss: 0.044869	Acc: 39.3% (3930/10000)
[Test]  Epoch: 45	Loss: 0.044876	Acc: 39.4% (3942/10000)
[Test]  Epoch: 46	Loss: 0.045048	Acc: 39.1% (3911/10000)
[Test]  Epoch: 47	Loss: 0.045076	Acc: 39.2% (3924/10000)
[Test]  Epoch: 48	Loss: 0.044981	Acc: 39.2% (3921/10000)
[Test]  Epoch: 49	Loss: 0.044864	Acc: 39.3% (3930/10000)
[Test]  Epoch: 50	Loss: 0.045096	Acc: 39.1% (3912/10000)
[Test]  Epoch: 51	Loss: 0.045025	Acc: 39.3% (3926/10000)
[Test]  Epoch: 52	Loss: 0.044844	Acc: 39.4% (3942/10000)
[Test]  Epoch: 53	Loss: 0.045020	Acc: 39.3% (3934/10000)
[Test]  Epoch: 54	Loss: 0.045150	Acc: 39.2% (3916/10000)
[Test]  Epoch: 55	Loss: 0.045016	Acc: 39.5% (3947/10000)
[Test]  Epoch: 56	Loss: 0.045109	Acc: 39.2% (3921/10000)
[Test]  Epoch: 57	Loss: 0.045127	Acc: 39.2% (3924/10000)
[Test]  Epoch: 58	Loss: 0.044847	Acc: 39.5% (3953/10000)
[Test]  Epoch: 59	Loss: 0.045127	Acc: 39.3% (3930/10000)
[Test]  Epoch: 60	Loss: 0.045083	Acc: 39.1% (3914/10000)
[Test]  Epoch: 61	Loss: 0.045195	Acc: 39.2% (3918/10000)
[Test]  Epoch: 62	Loss: 0.045200	Acc: 39.3% (3929/10000)
[Test]  Epoch: 63	Loss: 0.044990	Acc: 39.4% (3938/10000)
[Test]  Epoch: 64	Loss: 0.045021	Acc: 39.4% (3939/10000)
[Test]  Epoch: 65	Loss: 0.045095	Acc: 39.2% (3918/10000)
[Test]  Epoch: 66	Loss: 0.045071	Acc: 39.5% (3950/10000)
[Test]  Epoch: 67	Loss: 0.045145	Acc: 39.4% (3938/10000)
[Test]  Epoch: 68	Loss: 0.045256	Acc: 39.1% (3910/10000)
[Test]  Epoch: 69	Loss: 0.045151	Acc: 39.1% (3908/10000)
[Test]  Epoch: 70	Loss: 0.045148	Acc: 39.3% (3927/10000)
[Test]  Epoch: 71	Loss: 0.045114	Acc: 39.2% (3921/10000)
[Test]  Epoch: 72	Loss: 0.045217	Acc: 39.1% (3915/10000)
[Test]  Epoch: 73	Loss: 0.045065	Acc: 39.3% (3930/10000)
[Test]  Epoch: 74	Loss: 0.045117	Acc: 39.4% (3936/10000)
[Test]  Epoch: 75	Loss: 0.045164	Acc: 39.1% (3914/10000)
[Test]  Epoch: 76	Loss: 0.045061	Acc: 39.5% (3948/10000)
[Test]  Epoch: 77	Loss: 0.045138	Acc: 39.4% (3935/10000)
[Test]  Epoch: 78	Loss: 0.045168	Acc: 39.0% (3896/10000)
[Test]  Epoch: 79	Loss: 0.045235	Acc: 39.1% (3906/10000)
[Test]  Epoch: 80	Loss: 0.045129	Acc: 39.2% (3922/10000)
[Test]  Epoch: 81	Loss: 0.045133	Acc: 39.3% (3931/10000)
[Test]  Epoch: 82	Loss: 0.045176	Acc: 39.1% (3907/10000)
[Test]  Epoch: 83	Loss: 0.045169	Acc: 39.2% (3919/10000)
[Test]  Epoch: 84	Loss: 0.045268	Acc: 39.2% (3921/10000)
[Test]  Epoch: 85	Loss: 0.045189	Acc: 39.1% (3913/10000)
[Test]  Epoch: 86	Loss: 0.045233	Acc: 39.0% (3900/10000)
[Test]  Epoch: 87	Loss: 0.045161	Acc: 39.3% (3928/10000)
[Test]  Epoch: 88	Loss: 0.045160	Acc: 39.2% (3923/10000)
[Test]  Epoch: 89	Loss: 0.045133	Acc: 39.1% (3906/10000)
[Test]  Epoch: 90	Loss: 0.045237	Acc: 39.0% (3896/10000)
[Test]  Epoch: 91	Loss: 0.045236	Acc: 39.1% (3906/10000)
[Test]  Epoch: 92	Loss: 0.045084	Acc: 39.3% (3929/10000)
[Test]  Epoch: 93	Loss: 0.045196	Acc: 39.3% (3929/10000)
[Test]  Epoch: 94	Loss: 0.045202	Acc: 39.2% (3920/10000)
[Test]  Epoch: 95	Loss: 0.045202	Acc: 39.2% (3920/10000)
[Test]  Epoch: 96	Loss: 0.045159	Acc: 39.3% (3928/10000)
[Test]  Epoch: 97	Loss: 0.045186	Acc: 39.1% (3908/10000)
[Test]  Epoch: 98	Loss: 0.045181	Acc: 39.3% (3929/10000)
[Test]  Epoch: 99	Loss: 0.045232	Acc: 39.2% (3922/10000)
[Test]  Epoch: 100	Loss: 0.045185	Acc: 39.2% (3920/10000)
===========finish==========
['2024-08-19', '07:16:57.154856', '100', 'test', '0.045185109388828276', '39.2', '39.53']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.101192	Acc: 2.8% (282/10000)
[Test]  Epoch: 2	Loss: 0.046248	Acc: 34.1% (3411/10000)
[Test]  Epoch: 3	Loss: 0.045320	Acc: 36.3% (3629/10000)
[Test]  Epoch: 4	Loss: 0.045331	Acc: 36.4% (3641/10000)
[Test]  Epoch: 5	Loss: 0.045188	Acc: 36.9% (3691/10000)
[Test]  Epoch: 6	Loss: 0.045325	Acc: 36.8% (3679/10000)
[Test]  Epoch: 7	Loss: 0.045333	Acc: 36.7% (3669/10000)
[Test]  Epoch: 8	Loss: 0.045374	Acc: 37.0% (3705/10000)
[Test]  Epoch: 9	Loss: 0.045395	Acc: 37.1% (3707/10000)
[Test]  Epoch: 10	Loss: 0.045467	Acc: 37.1% (3708/10000)
[Test]  Epoch: 11	Loss: 0.045496	Acc: 37.2% (3724/10000)
[Test]  Epoch: 12	Loss: 0.045442	Acc: 37.2% (3718/10000)
[Test]  Epoch: 13	Loss: 0.045538	Acc: 37.2% (3720/10000)
[Test]  Epoch: 14	Loss: 0.045600	Acc: 36.9% (3691/10000)
[Test]  Epoch: 15	Loss: 0.045703	Acc: 36.9% (3693/10000)
[Test]  Epoch: 16	Loss: 0.045606	Acc: 37.1% (3707/10000)
[Test]  Epoch: 17	Loss: 0.045574	Acc: 37.1% (3712/10000)
[Test]  Epoch: 18	Loss: 0.045647	Acc: 37.1% (3707/10000)
[Test]  Epoch: 19	Loss: 0.045681	Acc: 37.1% (3706/10000)
[Test]  Epoch: 20	Loss: 0.045704	Acc: 37.2% (3723/10000)
[Test]  Epoch: 21	Loss: 0.045658	Acc: 37.3% (3728/10000)
[Test]  Epoch: 22	Loss: 0.045724	Acc: 37.0% (3698/10000)
[Test]  Epoch: 23	Loss: 0.045699	Acc: 37.0% (3696/10000)
[Test]  Epoch: 24	Loss: 0.045914	Acc: 37.1% (3709/10000)
[Test]  Epoch: 25	Loss: 0.045887	Acc: 37.2% (3717/10000)
[Test]  Epoch: 26	Loss: 0.045874	Acc: 37.0% (3697/10000)
[Test]  Epoch: 27	Loss: 0.045783	Acc: 37.2% (3716/10000)
[Test]  Epoch: 28	Loss: 0.045911	Acc: 37.1% (3714/10000)
[Test]  Epoch: 29	Loss: 0.045824	Acc: 36.9% (3691/10000)
[Test]  Epoch: 30	Loss: 0.045909	Acc: 37.1% (3707/10000)
[Test]  Epoch: 31	Loss: 0.046114	Acc: 36.9% (3686/10000)
[Test]  Epoch: 32	Loss: 0.045988	Acc: 37.0% (3700/10000)
[Test]  Epoch: 33	Loss: 0.046022	Acc: 37.0% (3695/10000)
[Test]  Epoch: 34	Loss: 0.045949	Acc: 37.4% (3736/10000)
[Test]  Epoch: 35	Loss: 0.045963	Acc: 37.3% (3727/10000)
[Test]  Epoch: 36	Loss: 0.046002	Acc: 37.0% (3705/10000)
[Test]  Epoch: 37	Loss: 0.046029	Acc: 37.1% (3714/10000)
[Test]  Epoch: 38	Loss: 0.046039	Acc: 37.2% (3725/10000)
[Test]  Epoch: 39	Loss: 0.046138	Acc: 37.1% (3709/10000)
[Test]  Epoch: 40	Loss: 0.046144	Acc: 37.0% (3700/10000)
[Test]  Epoch: 41	Loss: 0.046205	Acc: 36.9% (3694/10000)
[Test]  Epoch: 42	Loss: 0.046107	Acc: 37.1% (3713/10000)
[Test]  Epoch: 43	Loss: 0.046151	Acc: 37.2% (3724/10000)
[Test]  Epoch: 44	Loss: 0.046080	Acc: 37.2% (3722/10000)
[Test]  Epoch: 45	Loss: 0.046143	Acc: 37.3% (3726/10000)
[Test]  Epoch: 46	Loss: 0.046254	Acc: 37.1% (3712/10000)
[Test]  Epoch: 47	Loss: 0.046234	Acc: 37.1% (3711/10000)
[Test]  Epoch: 48	Loss: 0.046244	Acc: 37.2% (3717/10000)
[Test]  Epoch: 49	Loss: 0.046160	Acc: 37.2% (3725/10000)
[Test]  Epoch: 50	Loss: 0.046273	Acc: 37.0% (3703/10000)
[Test]  Epoch: 51	Loss: 0.046228	Acc: 37.3% (3728/10000)
[Test]  Epoch: 52	Loss: 0.046125	Acc: 37.4% (3740/10000)
[Test]  Epoch: 53	Loss: 0.046264	Acc: 37.2% (3716/10000)
[Test]  Epoch: 54	Loss: 0.046364	Acc: 37.0% (3696/10000)
[Test]  Epoch: 55	Loss: 0.046239	Acc: 37.3% (3726/10000)
[Test]  Epoch: 56	Loss: 0.046451	Acc: 37.0% (3699/10000)
[Test]  Epoch: 57	Loss: 0.046358	Acc: 37.1% (3714/10000)
[Test]  Epoch: 58	Loss: 0.046082	Acc: 37.5% (3752/10000)
[Test]  Epoch: 59	Loss: 0.046394	Acc: 37.1% (3708/10000)
[Test]  Epoch: 60	Loss: 0.046340	Acc: 37.0% (3702/10000)
[Test]  Epoch: 61	Loss: 0.046400	Acc: 37.0% (3698/10000)
[Test]  Epoch: 62	Loss: 0.046381	Acc: 37.0% (3701/10000)
[Test]  Epoch: 63	Loss: 0.046233	Acc: 37.2% (3719/10000)
[Test]  Epoch: 64	Loss: 0.046270	Acc: 37.2% (3722/10000)
[Test]  Epoch: 65	Loss: 0.046317	Acc: 37.1% (3713/10000)
[Test]  Epoch: 66	Loss: 0.046268	Acc: 37.2% (3721/10000)
[Test]  Epoch: 67	Loss: 0.046363	Acc: 37.2% (3723/10000)
[Test]  Epoch: 68	Loss: 0.046486	Acc: 37.0% (3697/10000)
[Test]  Epoch: 69	Loss: 0.046367	Acc: 37.1% (3712/10000)
[Test]  Epoch: 70	Loss: 0.046366	Acc: 37.1% (3706/10000)
[Test]  Epoch: 71	Loss: 0.046357	Acc: 37.0% (3700/10000)
[Test]  Epoch: 72	Loss: 0.046422	Acc: 37.0% (3699/10000)
[Test]  Epoch: 73	Loss: 0.046291	Acc: 37.2% (3722/10000)
[Test]  Epoch: 74	Loss: 0.046323	Acc: 37.4% (3736/10000)
[Test]  Epoch: 75	Loss: 0.046374	Acc: 37.1% (3710/10000)
[Test]  Epoch: 76	Loss: 0.046301	Acc: 37.4% (3735/10000)
[Test]  Epoch: 77	Loss: 0.046388	Acc: 37.1% (3707/10000)
[Test]  Epoch: 78	Loss: 0.046424	Acc: 36.9% (3694/10000)
[Test]  Epoch: 79	Loss: 0.046433	Acc: 37.1% (3709/10000)
[Test]  Epoch: 80	Loss: 0.046349	Acc: 37.2% (3723/10000)
[Test]  Epoch: 81	Loss: 0.046345	Acc: 37.1% (3715/10000)
[Test]  Epoch: 82	Loss: 0.046396	Acc: 37.2% (3717/10000)
[Test]  Epoch: 83	Loss: 0.046407	Acc: 37.1% (3712/10000)
[Test]  Epoch: 84	Loss: 0.046496	Acc: 37.1% (3712/10000)
[Test]  Epoch: 85	Loss: 0.046442	Acc: 37.1% (3710/10000)
[Test]  Epoch: 86	Loss: 0.046476	Acc: 37.0% (3705/10000)
[Test]  Epoch: 87	Loss: 0.046391	Acc: 37.0% (3696/10000)
[Test]  Epoch: 88	Loss: 0.046386	Acc: 37.0% (3699/10000)
[Test]  Epoch: 89	Loss: 0.046355	Acc: 37.0% (3695/10000)
[Test]  Epoch: 90	Loss: 0.046437	Acc: 37.1% (3706/10000)
[Test]  Epoch: 91	Loss: 0.046475	Acc: 37.1% (3708/10000)
[Test]  Epoch: 92	Loss: 0.046311	Acc: 37.2% (3723/10000)
[Test]  Epoch: 93	Loss: 0.046426	Acc: 37.2% (3719/10000)
[Test]  Epoch: 94	Loss: 0.046454	Acc: 37.0% (3702/10000)
[Test]  Epoch: 95	Loss: 0.046441	Acc: 37.0% (3701/10000)
[Test]  Epoch: 96	Loss: 0.046374	Acc: 37.3% (3727/10000)
[Test]  Epoch: 97	Loss: 0.046400	Acc: 37.2% (3718/10000)
[Test]  Epoch: 98	Loss: 0.046411	Acc: 37.1% (3709/10000)
[Test]  Epoch: 99	Loss: 0.046460	Acc: 37.1% (3714/10000)
[Test]  Epoch: 100	Loss: 0.046408	Acc: 37.1% (3709/10000)
===========finish==========
['2024-08-19', '07:20:24.259942', '100', 'test', '0.046407605504989624', '37.09', '37.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.113873	Acc: 1.7% (170/10000)
[Test]  Epoch: 2	Loss: 0.050565	Acc: 28.1% (2807/10000)
[Test]  Epoch: 3	Loss: 0.049052	Acc: 31.0% (3103/10000)
[Test]  Epoch: 4	Loss: 0.049102	Acc: 31.2% (3125/10000)
[Test]  Epoch: 5	Loss: 0.048664	Acc: 32.0% (3196/10000)
[Test]  Epoch: 6	Loss: 0.048569	Acc: 32.0% (3198/10000)
[Test]  Epoch: 7	Loss: 0.048742	Acc: 31.7% (3172/10000)
[Test]  Epoch: 8	Loss: 0.048695	Acc: 31.9% (3188/10000)
[Test]  Epoch: 9	Loss: 0.048680	Acc: 32.3% (3231/10000)
[Test]  Epoch: 10	Loss: 0.048686	Acc: 32.0% (3203/10000)
[Test]  Epoch: 11	Loss: 0.048670	Acc: 32.1% (3214/10000)
[Test]  Epoch: 12	Loss: 0.048527	Acc: 32.2% (3224/10000)
[Test]  Epoch: 13	Loss: 0.048537	Acc: 32.1% (3211/10000)
[Test]  Epoch: 14	Loss: 0.048656	Acc: 32.2% (3221/10000)
[Test]  Epoch: 15	Loss: 0.048784	Acc: 32.1% (3214/10000)
[Test]  Epoch: 16	Loss: 0.048589	Acc: 32.2% (3216/10000)
[Test]  Epoch: 17	Loss: 0.048813	Acc: 32.3% (3228/10000)
[Test]  Epoch: 18	Loss: 0.048654	Acc: 32.5% (3250/10000)
[Test]  Epoch: 19	Loss: 0.048676	Acc: 32.2% (3219/10000)
[Test]  Epoch: 20	Loss: 0.048563	Acc: 32.4% (3239/10000)
[Test]  Epoch: 21	Loss: 0.048634	Acc: 32.3% (3231/10000)
[Test]  Epoch: 22	Loss: 0.048602	Acc: 32.5% (3247/10000)
[Test]  Epoch: 23	Loss: 0.048704	Acc: 32.3% (3227/10000)
[Test]  Epoch: 24	Loss: 0.049081	Acc: 32.1% (3209/10000)
[Test]  Epoch: 25	Loss: 0.048703	Acc: 32.5% (3245/10000)
[Test]  Epoch: 26	Loss: 0.048782	Acc: 32.4% (3244/10000)
[Test]  Epoch: 27	Loss: 0.048763	Acc: 32.4% (3244/10000)
[Test]  Epoch: 28	Loss: 0.048716	Acc: 32.8% (3279/10000)
[Test]  Epoch: 29	Loss: 0.048698	Acc: 32.4% (3235/10000)
[Test]  Epoch: 30	Loss: 0.048748	Acc: 32.3% (3229/10000)
[Test]  Epoch: 31	Loss: 0.048952	Acc: 32.4% (3241/10000)
[Test]  Epoch: 32	Loss: 0.048884	Acc: 32.6% (3263/10000)
[Test]  Epoch: 33	Loss: 0.048887	Acc: 32.6% (3258/10000)
[Test]  Epoch: 34	Loss: 0.048818	Acc: 32.8% (3276/10000)
[Test]  Epoch: 35	Loss: 0.048801	Acc: 32.8% (3278/10000)
[Test]  Epoch: 36	Loss: 0.048865	Acc: 32.7% (3270/10000)
[Test]  Epoch: 37	Loss: 0.048950	Acc: 32.7% (3269/10000)
[Test]  Epoch: 38	Loss: 0.048901	Acc: 32.6% (3256/10000)
[Test]  Epoch: 39	Loss: 0.049031	Acc: 32.7% (3267/10000)
[Test]  Epoch: 40	Loss: 0.048998	Acc: 32.9% (3289/10000)
[Test]  Epoch: 41	Loss: 0.048981	Acc: 32.9% (3285/10000)
[Test]  Epoch: 42	Loss: 0.049082	Acc: 32.7% (3271/10000)
[Test]  Epoch: 43	Loss: 0.048933	Acc: 32.8% (3279/10000)
[Test]  Epoch: 44	Loss: 0.048913	Acc: 32.8% (3276/10000)
[Test]  Epoch: 45	Loss: 0.049077	Acc: 32.9% (3294/10000)
[Test]  Epoch: 46	Loss: 0.049079	Acc: 32.8% (3280/10000)
[Test]  Epoch: 47	Loss: 0.049022	Acc: 33.1% (3312/10000)
[Test]  Epoch: 48	Loss: 0.049062	Acc: 32.9% (3291/10000)
[Test]  Epoch: 49	Loss: 0.049040	Acc: 33.1% (3306/10000)
[Test]  Epoch: 50	Loss: 0.049077	Acc: 33.0% (3304/10000)
[Test]  Epoch: 51	Loss: 0.049083	Acc: 32.9% (3294/10000)
[Test]  Epoch: 52	Loss: 0.048990	Acc: 33.2% (3316/10000)
[Test]  Epoch: 53	Loss: 0.049051	Acc: 33.0% (3303/10000)
[Test]  Epoch: 54	Loss: 0.049069	Acc: 33.0% (3298/10000)
[Test]  Epoch: 55	Loss: 0.049087	Acc: 33.0% (3301/10000)
[Test]  Epoch: 56	Loss: 0.049206	Acc: 33.0% (3301/10000)
[Test]  Epoch: 57	Loss: 0.049196	Acc: 32.9% (3292/10000)
[Test]  Epoch: 58	Loss: 0.048904	Acc: 33.3% (3334/10000)
[Test]  Epoch: 59	Loss: 0.049176	Acc: 33.0% (3300/10000)
[Test]  Epoch: 60	Loss: 0.049167	Acc: 33.3% (3330/10000)
[Test]  Epoch: 61	Loss: 0.049218	Acc: 33.2% (3320/10000)
[Test]  Epoch: 62	Loss: 0.049171	Acc: 33.2% (3316/10000)
[Test]  Epoch: 63	Loss: 0.049074	Acc: 33.3% (3332/10000)
[Test]  Epoch: 64	Loss: 0.049075	Acc: 33.2% (3325/10000)
[Test]  Epoch: 65	Loss: 0.049106	Acc: 33.2% (3325/10000)
[Test]  Epoch: 66	Loss: 0.049083	Acc: 33.2% (3325/10000)
[Test]  Epoch: 67	Loss: 0.049239	Acc: 33.2% (3324/10000)
[Test]  Epoch: 68	Loss: 0.049341	Acc: 33.0% (3296/10000)
[Test]  Epoch: 69	Loss: 0.049183	Acc: 33.1% (3312/10000)
[Test]  Epoch: 70	Loss: 0.049205	Acc: 33.1% (3308/10000)
[Test]  Epoch: 71	Loss: 0.049175	Acc: 33.3% (3331/10000)
[Test]  Epoch: 72	Loss: 0.049247	Acc: 33.0% (3305/10000)
[Test]  Epoch: 73	Loss: 0.049135	Acc: 33.3% (3328/10000)
[Test]  Epoch: 74	Loss: 0.049090	Acc: 33.2% (3321/10000)
[Test]  Epoch: 75	Loss: 0.049198	Acc: 33.0% (3304/10000)
[Test]  Epoch: 76	Loss: 0.049155	Acc: 33.4% (3335/10000)
[Test]  Epoch: 77	Loss: 0.049178	Acc: 33.2% (3325/10000)
[Test]  Epoch: 78	Loss: 0.049193	Acc: 32.9% (3294/10000)
[Test]  Epoch: 79	Loss: 0.049248	Acc: 33.0% (3297/10000)
[Test]  Epoch: 80	Loss: 0.049178	Acc: 33.3% (3330/10000)
[Test]  Epoch: 81	Loss: 0.049134	Acc: 33.2% (3316/10000)
[Test]  Epoch: 82	Loss: 0.049166	Acc: 33.0% (3299/10000)
[Test]  Epoch: 83	Loss: 0.049209	Acc: 33.0% (3300/10000)
[Test]  Epoch: 84	Loss: 0.049264	Acc: 33.1% (3312/10000)
[Test]  Epoch: 85	Loss: 0.049182	Acc: 33.1% (3314/10000)
[Test]  Epoch: 86	Loss: 0.049232	Acc: 32.8% (3283/10000)
[Test]  Epoch: 87	Loss: 0.049204	Acc: 33.1% (3312/10000)
[Test]  Epoch: 88	Loss: 0.049222	Acc: 33.0% (3305/10000)
[Test]  Epoch: 89	Loss: 0.049167	Acc: 33.1% (3312/10000)
[Test]  Epoch: 90	Loss: 0.049206	Acc: 33.3% (3329/10000)
[Test]  Epoch: 91	Loss: 0.049235	Acc: 33.0% (3295/10000)
[Test]  Epoch: 92	Loss: 0.049110	Acc: 33.2% (3321/10000)
[Test]  Epoch: 93	Loss: 0.049209	Acc: 33.0% (3305/10000)
[Test]  Epoch: 94	Loss: 0.049267	Acc: 33.0% (3296/10000)
[Test]  Epoch: 95	Loss: 0.049254	Acc: 33.0% (3295/10000)
[Test]  Epoch: 96	Loss: 0.049222	Acc: 32.9% (3290/10000)
[Test]  Epoch: 97	Loss: 0.049225	Acc: 33.2% (3318/10000)
[Test]  Epoch: 98	Loss: 0.049241	Acc: 33.1% (3308/10000)
[Test]  Epoch: 99	Loss: 0.049270	Acc: 33.0% (3297/10000)
[Test]  Epoch: 100	Loss: 0.049209	Acc: 32.9% (3288/10000)
===========finish==========
['2024-08-19', '07:23:49.032751', '100', 'test', '0.04920887341499328', '32.88', '33.35']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.15.conv.2.weight', '_features.15.conv.3.weight', '_features.16.conv.0.0.weight', '_features.16.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.117572	Acc: 1.7% (169/10000)
[Test]  Epoch: 2	Loss: 0.051425	Acc: 26.7% (2670/10000)
[Test]  Epoch: 3	Loss: 0.050325	Acc: 28.8% (2879/10000)
[Test]  Epoch: 4	Loss: 0.050209	Acc: 29.6% (2958/10000)
[Test]  Epoch: 5	Loss: 0.050079	Acc: 29.7% (2973/10000)
[Test]  Epoch: 6	Loss: 0.050175	Acc: 29.7% (2972/10000)
[Test]  Epoch: 7	Loss: 0.049929	Acc: 29.9% (2989/10000)
[Test]  Epoch: 8	Loss: 0.049950	Acc: 30.2% (3020/10000)
[Test]  Epoch: 9	Loss: 0.049868	Acc: 30.4% (3045/10000)
[Test]  Epoch: 10	Loss: 0.049862	Acc: 30.4% (3037/10000)
[Test]  Epoch: 11	Loss: 0.050006	Acc: 30.1% (3007/10000)
[Test]  Epoch: 12	Loss: 0.049835	Acc: 30.4% (3041/10000)
[Test]  Epoch: 13	Loss: 0.049901	Acc: 30.5% (3046/10000)
[Test]  Epoch: 14	Loss: 0.049968	Acc: 30.3% (3034/10000)
[Test]  Epoch: 15	Loss: 0.049921	Acc: 30.6% (3060/10000)
[Test]  Epoch: 16	Loss: 0.049836	Acc: 30.6% (3058/10000)
[Test]  Epoch: 17	Loss: 0.049992	Acc: 30.8% (3076/10000)
[Test]  Epoch: 18	Loss: 0.049830	Acc: 30.7% (3066/10000)
[Test]  Epoch: 19	Loss: 0.049741	Acc: 30.6% (3058/10000)
[Test]  Epoch: 20	Loss: 0.049834	Acc: 30.8% (3083/10000)
[Test]  Epoch: 21	Loss: 0.049978	Acc: 30.7% (3068/10000)
[Test]  Epoch: 22	Loss: 0.049834	Acc: 30.8% (3076/10000)
[Test]  Epoch: 23	Loss: 0.049754	Acc: 30.9% (3085/10000)
[Test]  Epoch: 24	Loss: 0.050143	Acc: 30.6% (3059/10000)
[Test]  Epoch: 25	Loss: 0.049832	Acc: 30.8% (3079/10000)
[Test]  Epoch: 26	Loss: 0.049899	Acc: 30.7% (3071/10000)
[Test]  Epoch: 27	Loss: 0.049872	Acc: 30.9% (3093/10000)
[Test]  Epoch: 28	Loss: 0.049865	Acc: 30.9% (3089/10000)
[Test]  Epoch: 29	Loss: 0.049801	Acc: 30.9% (3090/10000)
[Test]  Epoch: 30	Loss: 0.049861	Acc: 30.9% (3085/10000)
[Test]  Epoch: 31	Loss: 0.050076	Acc: 30.8% (3078/10000)
[Test]  Epoch: 32	Loss: 0.049933	Acc: 30.8% (3083/10000)
[Test]  Epoch: 33	Loss: 0.050084	Acc: 30.6% (3062/10000)
[Test]  Epoch: 34	Loss: 0.049877	Acc: 31.0% (3101/10000)
[Test]  Epoch: 35	Loss: 0.049910	Acc: 30.9% (3088/10000)
[Test]  Epoch: 36	Loss: 0.049952	Acc: 30.9% (3089/10000)
[Test]  Epoch: 37	Loss: 0.049932	Acc: 31.1% (3115/10000)
[Test]  Epoch: 38	Loss: 0.049925	Acc: 31.0% (3099/10000)
[Test]  Epoch: 39	Loss: 0.050108	Acc: 30.8% (3084/10000)
[Test]  Epoch: 40	Loss: 0.049931	Acc: 31.0% (3102/10000)
[Test]  Epoch: 41	Loss: 0.050069	Acc: 30.9% (3086/10000)
[Test]  Epoch: 42	Loss: 0.050025	Acc: 31.2% (3117/10000)
[Test]  Epoch: 43	Loss: 0.049951	Acc: 31.2% (3125/10000)
[Test]  Epoch: 44	Loss: 0.049923	Acc: 31.2% (3117/10000)
[Test]  Epoch: 45	Loss: 0.049990	Acc: 31.1% (3111/10000)
[Test]  Epoch: 46	Loss: 0.050003	Acc: 31.1% (3113/10000)
[Test]  Epoch: 47	Loss: 0.050010	Acc: 31.2% (3116/10000)
[Test]  Epoch: 48	Loss: 0.049999	Acc: 31.1% (3109/10000)
[Test]  Epoch: 49	Loss: 0.049990	Acc: 31.2% (3124/10000)
[Test]  Epoch: 50	Loss: 0.050052	Acc: 31.1% (3111/10000)
[Test]  Epoch: 51	Loss: 0.050103	Acc: 31.1% (3107/10000)
[Test]  Epoch: 52	Loss: 0.049978	Acc: 31.2% (3120/10000)
[Test]  Epoch: 53	Loss: 0.050017	Acc: 31.2% (3118/10000)
[Test]  Epoch: 54	Loss: 0.050140	Acc: 31.1% (3113/10000)
[Test]  Epoch: 55	Loss: 0.050071	Acc: 31.4% (3136/10000)
[Test]  Epoch: 56	Loss: 0.050181	Acc: 31.4% (3135/10000)
[Test]  Epoch: 57	Loss: 0.050076	Acc: 31.1% (3115/10000)
[Test]  Epoch: 58	Loss: 0.049841	Acc: 31.5% (3147/10000)
[Test]  Epoch: 59	Loss: 0.050047	Acc: 31.5% (3147/10000)
[Test]  Epoch: 60	Loss: 0.050047	Acc: 31.2% (3121/10000)
[Test]  Epoch: 61	Loss: 0.050088	Acc: 31.4% (3137/10000)
[Test]  Epoch: 62	Loss: 0.050076	Acc: 31.1% (3114/10000)
[Test]  Epoch: 63	Loss: 0.049965	Acc: 31.4% (3145/10000)
[Test]  Epoch: 64	Loss: 0.049997	Acc: 31.3% (3129/10000)
[Test]  Epoch: 65	Loss: 0.050053	Acc: 31.2% (3123/10000)
[Test]  Epoch: 66	Loss: 0.049981	Acc: 31.3% (3134/10000)
[Test]  Epoch: 67	Loss: 0.050181	Acc: 31.4% (3136/10000)
[Test]  Epoch: 68	Loss: 0.050185	Acc: 31.1% (3112/10000)
[Test]  Epoch: 69	Loss: 0.050049	Acc: 31.4% (3136/10000)
[Test]  Epoch: 70	Loss: 0.050069	Acc: 31.2% (3125/10000)
[Test]  Epoch: 71	Loss: 0.050032	Acc: 31.4% (3135/10000)
[Test]  Epoch: 72	Loss: 0.050150	Acc: 31.0% (3104/10000)
[Test]  Epoch: 73	Loss: 0.050049	Acc: 31.2% (3123/10000)
[Test]  Epoch: 74	Loss: 0.050014	Acc: 31.2% (3120/10000)
[Test]  Epoch: 75	Loss: 0.050065	Acc: 31.3% (3127/10000)
[Test]  Epoch: 76	Loss: 0.050057	Acc: 31.4% (3142/10000)
[Test]  Epoch: 77	Loss: 0.050082	Acc: 31.3% (3128/10000)
[Test]  Epoch: 78	Loss: 0.050116	Acc: 31.2% (3117/10000)
[Test]  Epoch: 79	Loss: 0.050127	Acc: 31.2% (3119/10000)
[Test]  Epoch: 80	Loss: 0.050055	Acc: 31.3% (3127/10000)
[Test]  Epoch: 81	Loss: 0.050052	Acc: 31.4% (3137/10000)
[Test]  Epoch: 82	Loss: 0.050120	Acc: 31.0% (3104/10000)
[Test]  Epoch: 83	Loss: 0.050070	Acc: 31.1% (3110/10000)
[Test]  Epoch: 84	Loss: 0.050189	Acc: 31.2% (3119/10000)
[Test]  Epoch: 85	Loss: 0.050088	Acc: 31.3% (3127/10000)
[Test]  Epoch: 86	Loss: 0.050127	Acc: 31.1% (3107/10000)
[Test]  Epoch: 87	Loss: 0.050078	Acc: 31.1% (3111/10000)
[Test]  Epoch: 88	Loss: 0.050122	Acc: 31.1% (3114/10000)
[Test]  Epoch: 89	Loss: 0.050089	Acc: 31.1% (3108/10000)
[Test]  Epoch: 90	Loss: 0.050057	Acc: 31.2% (3116/10000)
[Test]  Epoch: 91	Loss: 0.050102	Acc: 31.2% (3120/10000)
[Test]  Epoch: 92	Loss: 0.049974	Acc: 31.4% (3136/10000)
[Test]  Epoch: 93	Loss: 0.050044	Acc: 31.3% (3134/10000)
[Test]  Epoch: 94	Loss: 0.050136	Acc: 31.2% (3118/10000)
[Test]  Epoch: 95	Loss: 0.050154	Acc: 31.2% (3118/10000)
[Test]  Epoch: 96	Loss: 0.050091	Acc: 31.4% (3135/10000)
[Test]  Epoch: 97	Loss: 0.050112	Acc: 31.1% (3114/10000)
[Test]  Epoch: 98	Loss: 0.050136	Acc: 31.1% (3113/10000)
[Test]  Epoch: 99	Loss: 0.050128	Acc: 31.1% (3106/10000)
[Test]  Epoch: 100	Loss: 0.050103	Acc: 31.1% (3114/10000)
===========finish==========
['2024-08-19', '07:27:12.836644', '100', 'test', '0.05010283875465393', '31.14', '31.47']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.0.0.weight', '_features.0.1.weight', '_features.1.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.1.conv.1.weight', '_features.1.conv.2.weight', '_features.2.conv.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.2.conv.2.weight', '_features.2.conv.3.weight', '_features.3.conv.0.0.weight', '_features.3.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.3.conv.1.1.weight', '_features.3.conv.2.weight', '_features.3.conv.3.weight', '_features.4.conv.0.0.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.4.conv.2.weight', '_features.4.conv.3.weight', '_features.5.conv.0.0.weight', '_features.5.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.5.conv.2.weight', '_features.5.conv.3.weight', '_features.6.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.6.conv.2.weight', '_features.6.conv.3.weight', '_features.7.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.7.conv.2.weight', '_features.7.conv.3.weight', '_features.8.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.8.conv.1.1.weight', '_features.8.conv.2.weight', '_features.8.conv.3.weight', '_features.9.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.2.weight', '_features.9.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.0.weight', '_features.10.conv.1.1.weight', '_features.10.conv.2.weight', '_features.10.conv.3.weight', '_features.11.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.11.conv.1.0.weight', '_features.11.conv.1.1.weight', '_features.11.conv.2.weight', '_features.11.conv.3.weight', '_features.12.conv.0.0.weight', '_features.12.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.12.conv.2.weight', '_features.12.conv.3.weight', '_features.13.conv.0.0.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.13.conv.1.1.weight', '_features.13.conv.2.weight', '_features.13.conv.3.weight', '_features.14.conv.0.0.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.0.1.weight', '_features.15.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.15.conv.2.weight', '_features.15.conv.3.weight', '_features.16.conv.0.0.weight', '_features.16.conv.0.1.weight', '_features.16.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.2.weight', '_features.16.conv.3.weight', '_features.17.conv.0.0.weight', '_features.17.conv.0.1.weight', '_features.17.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.17.conv.2.weight', '_features.17.conv.3.weight', '_features.18.0.weight', '_features.18.1.weight', 'last_linear.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.105416	Acc: 0.6% (61/10000)
[Test]  Epoch: 2	Loss: 0.082397	Acc: 2.2% (221/10000)
[Test]  Epoch: 3	Loss: 0.079775	Acc: 3.9% (388/10000)
[Test]  Epoch: 4	Loss: 0.077784	Acc: 5.0% (498/10000)
[Test]  Epoch: 5	Loss: 0.075876	Acc: 6.3% (635/10000)
[Test]  Epoch: 6	Loss: 0.074363	Acc: 7.0% (697/10000)
[Test]  Epoch: 7	Loss: 0.073103	Acc: 7.6% (764/10000)
[Test]  Epoch: 8	Loss: 0.071612	Acc: 8.3% (832/10000)
[Test]  Epoch: 9	Loss: 0.070567	Acc: 8.9% (893/10000)
[Test]  Epoch: 10	Loss: 0.069521	Acc: 9.7% (965/10000)
[Test]  Epoch: 11	Loss: 0.068733	Acc: 10.1% (1007/10000)
[Test]  Epoch: 12	Loss: 0.068056	Acc: 10.5% (1054/10000)
[Test]  Epoch: 13	Loss: 0.067416	Acc: 10.8% (1076/10000)
[Test]  Epoch: 14	Loss: 0.066780	Acc: 11.4% (1145/10000)
[Test]  Epoch: 15	Loss: 0.066162	Acc: 11.9% (1195/10000)
[Test]  Epoch: 16	Loss: 0.065682	Acc: 11.9% (1188/10000)
[Test]  Epoch: 17	Loss: 0.065229	Acc: 12.4% (1242/10000)
[Test]  Epoch: 18	Loss: 0.064862	Acc: 12.5% (1254/10000)
[Test]  Epoch: 19	Loss: 0.064508	Acc: 12.9% (1287/10000)
[Test]  Epoch: 20	Loss: 0.064150	Acc: 13.3% (1327/10000)
[Test]  Epoch: 21	Loss: 0.063859	Acc: 13.4% (1343/10000)
[Test]  Epoch: 22	Loss: 0.063540	Acc: 13.5% (1353/10000)
[Test]  Epoch: 23	Loss: 0.063334	Acc: 14.0% (1403/10000)
[Test]  Epoch: 24	Loss: 0.063140	Acc: 14.0% (1399/10000)
[Test]  Epoch: 25	Loss: 0.062956	Acc: 14.2% (1420/10000)
[Test]  Epoch: 26	Loss: 0.062817	Acc: 14.6% (1461/10000)
[Test]  Epoch: 27	Loss: 0.062557	Acc: 14.8% (1475/10000)
[Test]  Epoch: 28	Loss: 0.062449	Acc: 14.7% (1474/10000)
[Test]  Epoch: 29	Loss: 0.062170	Acc: 15.0% (1497/10000)
[Test]  Epoch: 30	Loss: 0.062048	Acc: 15.0% (1503/10000)
[Test]  Epoch: 31	Loss: 0.061968	Acc: 15.4% (1539/10000)
[Test]  Epoch: 32	Loss: 0.061773	Acc: 15.2% (1520/10000)
[Test]  Epoch: 33	Loss: 0.061672	Acc: 15.4% (1543/10000)
[Test]  Epoch: 34	Loss: 0.061499	Acc: 15.7% (1570/10000)
[Test]  Epoch: 35	Loss: 0.061419	Acc: 15.6% (1562/10000)
[Test]  Epoch: 36	Loss: 0.061298	Acc: 15.8% (1579/10000)
[Test]  Epoch: 37	Loss: 0.061166	Acc: 16.0% (1598/10000)
[Test]  Epoch: 38	Loss: 0.061185	Acc: 16.1% (1611/10000)
[Test]  Epoch: 39	Loss: 0.061099	Acc: 16.0% (1596/10000)
[Test]  Epoch: 40	Loss: 0.060983	Acc: 15.9% (1586/10000)
[Test]  Epoch: 41	Loss: 0.060941	Acc: 16.3% (1630/10000)
[Test]  Epoch: 42	Loss: 0.060800	Acc: 16.3% (1632/10000)
[Test]  Epoch: 43	Loss: 0.060618	Acc: 16.4% (1637/10000)
[Test]  Epoch: 44	Loss: 0.060520	Acc: 16.5% (1651/10000)
[Test]  Epoch: 45	Loss: 0.060574	Acc: 16.4% (1635/10000)
[Test]  Epoch: 46	Loss: 0.060426	Acc: 16.4% (1644/10000)
[Test]  Epoch: 47	Loss: 0.060475	Acc: 16.3% (1628/10000)
[Test]  Epoch: 48	Loss: 0.060321	Acc: 16.5% (1653/10000)
[Test]  Epoch: 49	Loss: 0.060410	Acc: 16.4% (1643/10000)
[Test]  Epoch: 50	Loss: 0.060369	Acc: 16.6% (1659/10000)
[Test]  Epoch: 51	Loss: 0.060204	Acc: 16.9% (1690/10000)
[Test]  Epoch: 52	Loss: 0.060035	Acc: 17.0% (1701/10000)
[Test]  Epoch: 53	Loss: 0.060012	Acc: 17.1% (1707/10000)
[Test]  Epoch: 54	Loss: 0.060122	Acc: 16.6% (1663/10000)
[Test]  Epoch: 55	Loss: 0.060095	Acc: 16.8% (1676/10000)
[Test]  Epoch: 56	Loss: 0.060073	Acc: 17.2% (1721/10000)
[Test]  Epoch: 57	Loss: 0.059998	Acc: 17.2% (1722/10000)
[Test]  Epoch: 58	Loss: 0.059775	Acc: 17.4% (1735/10000)
[Test]  Epoch: 59	Loss: 0.059870	Acc: 17.2% (1716/10000)
[Test]  Epoch: 60	Loss: 0.059910	Acc: 17.2% (1720/10000)
[Test]  Epoch: 61	Loss: 0.059920	Acc: 17.3% (1734/10000)
[Test]  Epoch: 62	Loss: 0.059914	Acc: 17.3% (1734/10000)
[Test]  Epoch: 63	Loss: 0.059750	Acc: 17.5% (1751/10000)
[Test]  Epoch: 64	Loss: 0.059723	Acc: 17.6% (1755/10000)
[Test]  Epoch: 65	Loss: 0.059760	Acc: 17.4% (1738/10000)
[Test]  Epoch: 66	Loss: 0.059755	Acc: 17.3% (1731/10000)
[Test]  Epoch: 67	Loss: 0.059866	Acc: 17.3% (1733/10000)
[Test]  Epoch: 68	Loss: 0.059891	Acc: 17.4% (1742/10000)
[Test]  Epoch: 69	Loss: 0.059717	Acc: 17.7% (1774/10000)
[Test]  Epoch: 70	Loss: 0.059755	Acc: 17.6% (1764/10000)
[Test]  Epoch: 71	Loss: 0.059723	Acc: 17.4% (1743/10000)
[Test]  Epoch: 72	Loss: 0.059835	Acc: 17.3% (1730/10000)
[Test]  Epoch: 73	Loss: 0.059711	Acc: 17.3% (1733/10000)
[Test]  Epoch: 74	Loss: 0.059681	Acc: 17.4% (1735/10000)
[Test]  Epoch: 75	Loss: 0.059707	Acc: 17.5% (1747/10000)
[Test]  Epoch: 76	Loss: 0.059691	Acc: 17.5% (1751/10000)
[Test]  Epoch: 77	Loss: 0.059726	Acc: 17.5% (1748/10000)
[Test]  Epoch: 78	Loss: 0.059680	Acc: 17.6% (1755/10000)
[Test]  Epoch: 79	Loss: 0.059766	Acc: 17.5% (1747/10000)
[Test]  Epoch: 80	Loss: 0.059681	Acc: 17.7% (1769/10000)
[Test]  Epoch: 81	Loss: 0.059659	Acc: 17.5% (1748/10000)
[Test]  Epoch: 82	Loss: 0.059701	Acc: 17.6% (1758/10000)
[Test]  Epoch: 83	Loss: 0.059697	Acc: 17.4% (1735/10000)
[Test]  Epoch: 84	Loss: 0.059767	Acc: 17.6% (1759/10000)
[Test]  Epoch: 85	Loss: 0.059702	Acc: 17.6% (1763/10000)
[Test]  Epoch: 86	Loss: 0.059717	Acc: 17.4% (1740/10000)
[Test]  Epoch: 87	Loss: 0.059699	Acc: 17.4% (1743/10000)
[Test]  Epoch: 88	Loss: 0.059686	Acc: 17.5% (1747/10000)
[Test]  Epoch: 89	Loss: 0.059572	Acc: 17.5% (1754/10000)
[Test]  Epoch: 90	Loss: 0.059622	Acc: 17.6% (1763/10000)
[Test]  Epoch: 91	Loss: 0.059716	Acc: 17.4% (1741/10000)
[Test]  Epoch: 92	Loss: 0.059619	Acc: 17.7% (1773/10000)
[Test]  Epoch: 93	Loss: 0.059662	Acc: 17.6% (1760/10000)
[Test]  Epoch: 94	Loss: 0.059721	Acc: 17.6% (1763/10000)
[Test]  Epoch: 95	Loss: 0.059692	Acc: 17.5% (1752/10000)
[Test]  Epoch: 96	Loss: 0.059565	Acc: 17.6% (1755/10000)
[Test]  Epoch: 97	Loss: 0.059658	Acc: 17.6% (1760/10000)
[Test]  Epoch: 98	Loss: 0.059719	Acc: 17.7% (1770/10000)
[Test]  Epoch: 99	Loss: 0.059634	Acc: 17.6% (1755/10000)
[Test]  Epoch: 100	Loss: 0.059631	Acc: 17.6% (1764/10000)
===========finish==========
['2024-08-19', '07:30:41.739345', '100', 'test', '0.059630589246749875', '17.64', '17.74']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-vgg16_bn/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('features.0.weight', 0.0), ('features.0.bias', 0.0), ('features.1.weight', 0.0), ('features.1.bias', 0.0), ('features.3.weight', 0.0), ('features.3.bias', 0.0), ('features.4.weight', 0.0), ('features.4.bias', 0.0), ('features.7.weight', 0.0), ('features.7.bias', 0.0), ('features.8.weight', 0.0), ('features.8.bias', 0.0), ('features.10.weight', 0.0), ('features.10.bias', 0.0), ('features.11.weight', 0.0), ('features.11.bias', 0.0), ('features.14.weight', 0.0), ('features.14.bias', 0.0), ('features.15.weight', 0.0), ('features.15.bias', 0.0), ('features.17.weight', 0.0), ('features.17.bias', 0.0), ('features.18.weight', 0.0), ('features.18.bias', 0.0), ('features.20.weight', 0.0), ('features.20.bias', 0.0), ('features.21.weight', 0.0), ('features.21.bias', 0.0), ('features.24.weight', 0.0), ('features.24.bias', 0.0), ('features.25.weight', 0.0), ('features.25.bias', 0.0), ('features.27.weight', 0.0), ('features.27.bias', 0.0), ('features.28.weight', 0.0), ('features.28.bias', 0.0), ('features.30.weight', 0.0), ('features.30.bias', 0.0), ('features.31.weight', 0.0), ('features.31.bias', 0.0), ('features.34.weight', 0.0), ('features.34.bias', 0.0), ('features.35.weight', 0.0), ('features.35.bias', 0.0), ('features.37.weight', 0.0), ('features.37.bias', 0.0), ('features.38.weight', 0.0), ('features.38.bias', 0.0), ('features.40.weight', 0.0), ('features.40.bias', 0.0), ('features.41.weight', 0.0), ('features.41.bias', 0.0), ('classifier.weight', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('features.0.weight', 0.0), ('features.0.bias', 0.0), ('features.1.weight', 0.0), ('features.1.bias', 0.0), ('features.3.weight', 0.0), ('features.3.bias', 0.0), ('features.4.weight', 0.0), ('features.4.bias', 0.0), ('features.7.weight', 0.0), ('features.7.bias', 0.0), ('features.8.weight', 0.0), ('features.8.bias', 0.0), ('features.10.weight', 0.0), ('features.10.bias', 0.0), ('features.11.weight', 0.0), ('features.11.bias', 0.0), ('features.14.weight', 0.0), ('features.14.bias', 0.0), ('features.15.weight', 0.0), ('features.15.bias', 0.0), ('features.17.weight', 0.0), ('features.17.bias', 0.0), ('features.18.weight', 0.0), ('features.18.bias', 0.0), ('features.20.weight', 0.0), ('features.20.bias', 0.0), ('features.21.weight', 0.0), ('features.21.bias', 0.0), ('features.24.weight', 0.0), ('features.24.bias', 0.0), ('features.25.weight', 0.0), ('features.25.bias', 0.0), ('features.27.weight', 0.0), ('features.27.bias', 0.0), ('features.28.weight', 0.0), ('features.28.bias', 0.0), ('features.30.weight', 0.0), ('features.30.bias', 0.0), ('features.31.weight', 0.0), ('features.31.bias', 0.0), ('features.34.weight', 0.0), ('features.34.bias', 0.0), ('features.35.weight', 0.0), ('features.35.bias', 0.0), ('features.37.weight', 0.0), ('features.37.bias', 0.0), ('features.38.weight', 0.0), ('features.38.bias', 0.0), ('features.40.weight', 0.0), ('features.40.bias', 0.0), ('features.41.weight', 0.0), ('features.41.bias', 0.0), ('classifier.weight', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
get_sample_layers n=27  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042412	Acc: 38.2% (3820/10000)
[Test]  Epoch: 2	Loss: 0.043121	Acc: 36.9% (3691/10000)
[Test]  Epoch: 3	Loss: 0.043289	Acc: 36.4% (3644/10000)
[Test]  Epoch: 4	Loss: 0.043356	Acc: 36.5% (3646/10000)
[Test]  Epoch: 5	Loss: 0.043857	Acc: 36.3% (3626/10000)
[Test]  Epoch: 6	Loss: 0.044438	Acc: 35.2% (3523/10000)
[Test]  Epoch: 7	Loss: 0.044488	Acc: 35.5% (3546/10000)
[Test]  Epoch: 8	Loss: 0.044584	Acc: 35.3% (3533/10000)
[Test]  Epoch: 9	Loss: 0.044424	Acc: 35.5% (3547/10000)
[Test]  Epoch: 10	Loss: 0.044476	Acc: 35.4% (3543/10000)
[Test]  Epoch: 11	Loss: 0.044709	Acc: 34.9% (3493/10000)
[Test]  Epoch: 12	Loss: 0.044504	Acc: 35.5% (3546/10000)
[Test]  Epoch: 13	Loss: 0.045189	Acc: 34.9% (3490/10000)
[Test]  Epoch: 14	Loss: 0.044902	Acc: 34.8% (3477/10000)
[Test]  Epoch: 15	Loss: 0.044939	Acc: 35.0% (3495/10000)
[Test]  Epoch: 16	Loss: 0.045337	Acc: 33.9% (3386/10000)
[Test]  Epoch: 17	Loss: 0.045216	Acc: 34.7% (3470/10000)
[Test]  Epoch: 18	Loss: 0.045273	Acc: 34.6% (3456/10000)
[Test]  Epoch: 19	Loss: 0.045306	Acc: 33.9% (3391/10000)
[Test]  Epoch: 20	Loss: 0.045411	Acc: 34.3% (3431/10000)
[Test]  Epoch: 21	Loss: 0.045281	Acc: 34.9% (3492/10000)
[Test]  Epoch: 22	Loss: 0.045364	Acc: 34.2% (3417/10000)
[Test]  Epoch: 23	Loss: 0.045605	Acc: 34.3% (3429/10000)
[Test]  Epoch: 24	Loss: 0.045416	Acc: 34.5% (3451/10000)
[Test]  Epoch: 25	Loss: 0.045138	Acc: 34.1% (3415/10000)
[Test]  Epoch: 26	Loss: 0.045646	Acc: 34.0% (3401/10000)
[Test]  Epoch: 27	Loss: 0.045316	Acc: 33.9% (3387/10000)
[Test]  Epoch: 28	Loss: 0.045459	Acc: 34.6% (3460/10000)
[Test]  Epoch: 29	Loss: 0.045628	Acc: 34.2% (3423/10000)
[Test]  Epoch: 30	Loss: 0.046120	Acc: 33.8% (3381/10000)
[Test]  Epoch: 31	Loss: 0.046239	Acc: 32.8% (3283/10000)
[Test]  Epoch: 32	Loss: 0.045746	Acc: 34.4% (3436/10000)
[Test]  Epoch: 33	Loss: 0.046014	Acc: 33.6% (3363/10000)
[Test]  Epoch: 34	Loss: 0.045821	Acc: 33.5% (3353/10000)
[Test]  Epoch: 35	Loss: 0.046159	Acc: 33.7% (3371/10000)
[Test]  Epoch: 36	Loss: 0.046398	Acc: 32.9% (3293/10000)
[Test]  Epoch: 37	Loss: 0.046374	Acc: 33.0% (3303/10000)
[Test]  Epoch: 38	Loss: 0.046288	Acc: 33.3% (3326/10000)
[Test]  Epoch: 39	Loss: 0.046253	Acc: 33.0% (3304/10000)
[Test]  Epoch: 40	Loss: 0.046559	Acc: 32.8% (3279/10000)
[Test]  Epoch: 41	Loss: 0.046546	Acc: 32.8% (3280/10000)
[Test]  Epoch: 42	Loss: 0.046159	Acc: 32.9% (3293/10000)
[Test]  Epoch: 43	Loss: 0.046393	Acc: 32.5% (3255/10000)
[Test]  Epoch: 44	Loss: 0.046145	Acc: 33.2% (3321/10000)
[Test]  Epoch: 45	Loss: 0.046176	Acc: 33.2% (3321/10000)
[Test]  Epoch: 46	Loss: 0.046674	Acc: 33.4% (3340/10000)
[Test]  Epoch: 47	Loss: 0.046720	Acc: 32.2% (3219/10000)
[Test]  Epoch: 48	Loss: 0.046831	Acc: 32.6% (3263/10000)
[Test]  Epoch: 49	Loss: 0.046768	Acc: 32.8% (3278/10000)
[Test]  Epoch: 50	Loss: 0.046301	Acc: 32.9% (3285/10000)
[Test]  Epoch: 51	Loss: 0.046680	Acc: 32.6% (3262/10000)
[Test]  Epoch: 52	Loss: 0.046921	Acc: 32.4% (3236/10000)
[Test]  Epoch: 53	Loss: 0.046877	Acc: 32.2% (3220/10000)
[Test]  Epoch: 54	Loss: 0.046630	Acc: 32.5% (3251/10000)
[Test]  Epoch: 55	Loss: 0.046626	Acc: 33.1% (3314/10000)
[Test]  Epoch: 56	Loss: 0.047371	Acc: 31.9% (3191/10000)
[Test]  Epoch: 57	Loss: 0.046871	Acc: 32.1% (3213/10000)
[Test]  Epoch: 58	Loss: 0.047060	Acc: 32.0% (3199/10000)
[Test]  Epoch: 59	Loss: 0.047070	Acc: 31.6% (3155/10000)
[Test]  Epoch: 60	Loss: 0.047352	Acc: 32.6% (3265/10000)
[Test]  Epoch: 61	Loss: 0.047238	Acc: 32.2% (3218/10000)
[Test]  Epoch: 62	Loss: 0.046983	Acc: 32.1% (3211/10000)
[Test]  Epoch: 63	Loss: 0.046980	Acc: 32.0% (3197/10000)
[Test]  Epoch: 64	Loss: 0.046862	Acc: 32.3% (3230/10000)
[Test]  Epoch: 65	Loss: 0.046758	Acc: 33.2% (3316/10000)
[Test]  Epoch: 66	Loss: 0.047072	Acc: 32.6% (3264/10000)
[Test]  Epoch: 67	Loss: 0.047067	Acc: 32.0% (3196/10000)
[Test]  Epoch: 68	Loss: 0.047017	Acc: 32.0% (3199/10000)
[Test]  Epoch: 69	Loss: 0.047279	Acc: 32.4% (3235/10000)
[Test]  Epoch: 70	Loss: 0.046641	Acc: 32.3% (3229/10000)
[Test]  Epoch: 71	Loss: 0.046953	Acc: 32.1% (3207/10000)
[Test]  Epoch: 72	Loss: 0.046563	Acc: 33.1% (3314/10000)
[Test]  Epoch: 73	Loss: 0.046827	Acc: 33.0% (3298/10000)
[Test]  Epoch: 74	Loss: 0.047078	Acc: 32.6% (3259/10000)
[Test]  Epoch: 75	Loss: 0.046878	Acc: 32.7% (3270/10000)
[Test]  Epoch: 76	Loss: 0.047142	Acc: 32.8% (3282/10000)
[Test]  Epoch: 77	Loss: 0.046847	Acc: 32.8% (3283/10000)
[Test]  Epoch: 78	Loss: 0.046971	Acc: 32.4% (3243/10000)
[Test]  Epoch: 79	Loss: 0.046969	Acc: 32.1% (3210/10000)
[Test]  Epoch: 80	Loss: 0.046422	Acc: 33.0% (3302/10000)
[Test]  Epoch: 81	Loss: 0.047093	Acc: 32.2% (3225/10000)
[Test]  Epoch: 82	Loss: 0.046884	Acc: 32.7% (3270/10000)
[Test]  Epoch: 83	Loss: 0.046842	Acc: 32.6% (3265/10000)
[Test]  Epoch: 84	Loss: 0.046793	Acc: 32.5% (3253/10000)
[Test]  Epoch: 85	Loss: 0.046818	Acc: 32.4% (3240/10000)
[Test]  Epoch: 86	Loss: 0.046953	Acc: 31.9% (3188/10000)
[Test]  Epoch: 87	Loss: 0.046794	Acc: 32.7% (3272/10000)
[Test]  Epoch: 88	Loss: 0.046758	Acc: 32.4% (3240/10000)
[Test]  Epoch: 89	Loss: 0.046791	Acc: 32.7% (3269/10000)
[Test]  Epoch: 90	Loss: 0.046771	Acc: 32.7% (3268/10000)
[Test]  Epoch: 91	Loss: 0.046945	Acc: 32.6% (3263/10000)
[Test]  Epoch: 92	Loss: 0.046717	Acc: 32.6% (3260/10000)
[Test]  Epoch: 93	Loss: 0.046992	Acc: 32.2% (3218/10000)
[Test]  Epoch: 94	Loss: 0.046607	Acc: 33.1% (3306/10000)
[Test]  Epoch: 95	Loss: 0.046768	Acc: 32.4% (3239/10000)
[Test]  Epoch: 96	Loss: 0.046852	Acc: 32.4% (3235/10000)
[Test]  Epoch: 97	Loss: 0.046405	Acc: 33.2% (3318/10000)
[Test]  Epoch: 98	Loss: 0.047158	Acc: 31.7% (3173/10000)
[Test]  Epoch: 99	Loss: 0.046899	Acc: 32.8% (3278/10000)
[Test]  Epoch: 100	Loss: 0.046866	Acc: 31.8% (3182/10000)
===========finish==========
['2024-08-19', '07:37:19.272284', '100', 'test', '0.04686619166135788', '31.82', '38.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.0.weight', 'features.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.080577	Acc: 8.6% (856/10000)
[Test]  Epoch: 2	Loss: 0.062228	Acc: 18.9% (1885/10000)
[Test]  Epoch: 3	Loss: 0.054881	Acc: 24.0% (2397/10000)
[Test]  Epoch: 4	Loss: 0.053234	Acc: 25.7% (2568/10000)
[Test]  Epoch: 5	Loss: 0.051635	Acc: 27.3% (2728/10000)
[Test]  Epoch: 6	Loss: 0.051053	Acc: 28.3% (2831/10000)
[Test]  Epoch: 7	Loss: 0.050307	Acc: 28.6% (2863/10000)
[Test]  Epoch: 8	Loss: 0.050566	Acc: 28.3% (2828/10000)
[Test]  Epoch: 9	Loss: 0.049874	Acc: 29.2% (2922/10000)
[Test]  Epoch: 10	Loss: 0.049822	Acc: 28.8% (2879/10000)
[Test]  Epoch: 11	Loss: 0.049070	Acc: 29.6% (2959/10000)
[Test]  Epoch: 12	Loss: 0.049544	Acc: 29.5% (2953/10000)
[Test]  Epoch: 13	Loss: 0.049723	Acc: 29.2% (2917/10000)
[Test]  Epoch: 14	Loss: 0.049385	Acc: 29.4% (2935/10000)
[Test]  Epoch: 15	Loss: 0.049279	Acc: 29.6% (2956/10000)
[Test]  Epoch: 16	Loss: 0.049120	Acc: 29.9% (2987/10000)
[Test]  Epoch: 17	Loss: 0.049260	Acc: 29.6% (2959/10000)
[Test]  Epoch: 18	Loss: 0.048877	Acc: 30.1% (3014/10000)
[Test]  Epoch: 19	Loss: 0.049051	Acc: 29.4% (2940/10000)
[Test]  Epoch: 20	Loss: 0.048942	Acc: 30.3% (3027/10000)
[Test]  Epoch: 21	Loss: 0.048878	Acc: 29.8% (2981/10000)
[Test]  Epoch: 22	Loss: 0.048745	Acc: 30.5% (3050/10000)
[Test]  Epoch: 23	Loss: 0.048958	Acc: 30.0% (3003/10000)
[Test]  Epoch: 24	Loss: 0.049308	Acc: 29.3% (2933/10000)
[Test]  Epoch: 25	Loss: 0.049000	Acc: 29.6% (2964/10000)
[Test]  Epoch: 26	Loss: 0.048663	Acc: 30.4% (3041/10000)
[Test]  Epoch: 27	Loss: 0.048593	Acc: 30.0% (2997/10000)
[Test]  Epoch: 28	Loss: 0.048913	Acc: 30.3% (3030/10000)
[Test]  Epoch: 29	Loss: 0.048699	Acc: 30.6% (3064/10000)
[Test]  Epoch: 30	Loss: 0.049160	Acc: 29.9% (2987/10000)
[Test]  Epoch: 31	Loss: 0.049357	Acc: 28.8% (2883/10000)
[Test]  Epoch: 32	Loss: 0.049289	Acc: 29.9% (2991/10000)
[Test]  Epoch: 33	Loss: 0.049200	Acc: 29.8% (2975/10000)
[Test]  Epoch: 34	Loss: 0.049314	Acc: 29.2% (2922/10000)
[Test]  Epoch: 35	Loss: 0.048918	Acc: 30.2% (3020/10000)
[Test]  Epoch: 36	Loss: 0.049975	Acc: 28.7% (2866/10000)
[Test]  Epoch: 37	Loss: 0.049338	Acc: 29.6% (2959/10000)
[Test]  Epoch: 38	Loss: 0.049100	Acc: 29.9% (2991/10000)
[Test]  Epoch: 39	Loss: 0.049539	Acc: 29.2% (2924/10000)
[Test]  Epoch: 40	Loss: 0.049429	Acc: 29.3% (2933/10000)
[Test]  Epoch: 41	Loss: 0.048986	Acc: 29.6% (2965/10000)
[Test]  Epoch: 42	Loss: 0.048727	Acc: 30.2% (3022/10000)
[Test]  Epoch: 43	Loss: 0.049212	Acc: 29.8% (2977/10000)
[Test]  Epoch: 44	Loss: 0.048761	Acc: 30.2% (3025/10000)
[Test]  Epoch: 45	Loss: 0.048876	Acc: 30.0% (2997/10000)
[Test]  Epoch: 46	Loss: 0.050027	Acc: 28.5% (2853/10000)
[Test]  Epoch: 47	Loss: 0.049367	Acc: 29.4% (2937/10000)
[Test]  Epoch: 48	Loss: 0.049638	Acc: 29.0% (2898/10000)
[Test]  Epoch: 49	Loss: 0.049746	Acc: 29.3% (2926/10000)
[Test]  Epoch: 50	Loss: 0.048749	Acc: 30.3% (3033/10000)
[Test]  Epoch: 51	Loss: 0.049411	Acc: 29.4% (2945/10000)
[Test]  Epoch: 52	Loss: 0.049526	Acc: 29.1% (2906/10000)
[Test]  Epoch: 53	Loss: 0.049273	Acc: 29.3% (2931/10000)
[Test]  Epoch: 54	Loss: 0.049152	Acc: 29.4% (2943/10000)
[Test]  Epoch: 55	Loss: 0.049536	Acc: 30.1% (3015/10000)
[Test]  Epoch: 56	Loss: 0.050133	Acc: 28.5% (2850/10000)
[Test]  Epoch: 57	Loss: 0.049486	Acc: 29.4% (2936/10000)
[Test]  Epoch: 58	Loss: 0.049737	Acc: 29.6% (2956/10000)
[Test]  Epoch: 59	Loss: 0.049716	Acc: 28.9% (2894/10000)
[Test]  Epoch: 60	Loss: 0.050221	Acc: 28.6% (2857/10000)
[Test]  Epoch: 61	Loss: 0.049633	Acc: 29.3% (2929/10000)
[Test]  Epoch: 62	Loss: 0.049383	Acc: 29.4% (2937/10000)
[Test]  Epoch: 63	Loss: 0.049419	Acc: 29.7% (2973/10000)
[Test]  Epoch: 64	Loss: 0.049089	Acc: 29.4% (2943/10000)
[Test]  Epoch: 65	Loss: 0.049132	Acc: 30.0% (3002/10000)
[Test]  Epoch: 66	Loss: 0.049268	Acc: 29.5% (2946/10000)
[Test]  Epoch: 67	Loss: 0.049341	Acc: 29.3% (2927/10000)
[Test]  Epoch: 68	Loss: 0.049353	Acc: 29.2% (2920/10000)
[Test]  Epoch: 69	Loss: 0.049543	Acc: 29.4% (2938/10000)
[Test]  Epoch: 70	Loss: 0.049131	Acc: 29.6% (2956/10000)
[Test]  Epoch: 71	Loss: 0.049236	Acc: 29.9% (2995/10000)
[Test]  Epoch: 72	Loss: 0.048956	Acc: 30.7% (3073/10000)
[Test]  Epoch: 73	Loss: 0.049191	Acc: 29.7% (2969/10000)
[Test]  Epoch: 74	Loss: 0.049179	Acc: 30.2% (3017/10000)
[Test]  Epoch: 75	Loss: 0.049139	Acc: 29.9% (2994/10000)
[Test]  Epoch: 76	Loss: 0.049286	Acc: 30.1% (3014/10000)
[Test]  Epoch: 77	Loss: 0.049020	Acc: 30.1% (3010/10000)
[Test]  Epoch: 78	Loss: 0.049333	Acc: 29.6% (2955/10000)
[Test]  Epoch: 79	Loss: 0.049281	Acc: 29.5% (2950/10000)
[Test]  Epoch: 80	Loss: 0.048769	Acc: 30.1% (3011/10000)
[Test]  Epoch: 81	Loss: 0.049398	Acc: 29.4% (2945/10000)
[Test]  Epoch: 82	Loss: 0.049053	Acc: 29.9% (2990/10000)
[Test]  Epoch: 83	Loss: 0.049081	Acc: 30.1% (3008/10000)
[Test]  Epoch: 84	Loss: 0.049165	Acc: 29.8% (2975/10000)
[Test]  Epoch: 85	Loss: 0.049120	Acc: 29.6% (2957/10000)
[Test]  Epoch: 86	Loss: 0.049189	Acc: 28.9% (2894/10000)
[Test]  Epoch: 87	Loss: 0.048887	Acc: 29.9% (2990/10000)
[Test]  Epoch: 88	Loss: 0.048863	Acc: 30.1% (3013/10000)
[Test]  Epoch: 89	Loss: 0.048999	Acc: 30.2% (3025/10000)
[Test]  Epoch: 90	Loss: 0.048916	Acc: 30.0% (3004/10000)
[Test]  Epoch: 91	Loss: 0.049031	Acc: 30.4% (3039/10000)
[Test]  Epoch: 92	Loss: 0.049024	Acc: 29.8% (2980/10000)
[Test]  Epoch: 93	Loss: 0.049368	Acc: 29.8% (2979/10000)
[Test]  Epoch: 94	Loss: 0.048894	Acc: 30.2% (3019/10000)
[Test]  Epoch: 95	Loss: 0.048992	Acc: 30.5% (3048/10000)
[Test]  Epoch: 96	Loss: 0.049154	Acc: 29.6% (2956/10000)
[Test]  Epoch: 97	Loss: 0.048735	Acc: 30.1% (3008/10000)
[Test]  Epoch: 98	Loss: 0.049402	Acc: 29.5% (2952/10000)
[Test]  Epoch: 99	Loss: 0.049268	Acc: 29.7% (2967/10000)
[Test]  Epoch: 100	Loss: 0.049195	Acc: 28.9% (2895/10000)
===========finish==========
['2024-08-19', '07:40:31.085787', '100', 'test', '0.04919460307359695', '28.95', '30.73']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.115857	Acc: 1.2% (118/10000)
[Test]  Epoch: 2	Loss: 0.084720	Acc: 3.4% (335/10000)
[Test]  Epoch: 3	Loss: 0.077360	Acc: 5.2% (524/10000)
[Test]  Epoch: 4	Loss: 0.077526	Acc: 5.5% (548/10000)
[Test]  Epoch: 5	Loss: 0.075337	Acc: 5.9% (586/10000)
[Test]  Epoch: 6	Loss: 0.073770	Acc: 7.1% (706/10000)
[Test]  Epoch: 7	Loss: 0.072310	Acc: 7.2% (723/10000)
[Test]  Epoch: 8	Loss: 0.073237	Acc: 7.1% (711/10000)
[Test]  Epoch: 9	Loss: 0.071162	Acc: 9.0% (896/10000)
[Test]  Epoch: 10	Loss: 0.070992	Acc: 8.8% (875/10000)
[Test]  Epoch: 11	Loss: 0.070585	Acc: 9.2% (918/10000)
[Test]  Epoch: 12	Loss: 0.069659	Acc: 9.7% (966/10000)
[Test]  Epoch: 13	Loss: 0.069376	Acc: 9.6% (962/10000)
[Test]  Epoch: 14	Loss: 0.068267	Acc: 10.3% (1028/10000)
[Test]  Epoch: 15	Loss: 0.067584	Acc: 11.2% (1120/10000)
[Test]  Epoch: 16	Loss: 0.068261	Acc: 10.9% (1089/10000)
[Test]  Epoch: 17	Loss: 0.065755	Acc: 12.9% (1288/10000)
[Test]  Epoch: 18	Loss: 0.066096	Acc: 12.7% (1270/10000)
[Test]  Epoch: 19	Loss: 0.066092	Acc: 12.9% (1291/10000)
[Test]  Epoch: 20	Loss: 0.064650	Acc: 14.5% (1449/10000)
[Test]  Epoch: 21	Loss: 0.065575	Acc: 13.1% (1314/10000)
[Test]  Epoch: 22	Loss: 0.064714	Acc: 13.9% (1393/10000)
[Test]  Epoch: 23	Loss: 0.064960	Acc: 13.9% (1392/10000)
[Test]  Epoch: 24	Loss: 0.064322	Acc: 13.8% (1382/10000)
[Test]  Epoch: 25	Loss: 0.062800	Acc: 15.1% (1506/10000)
[Test]  Epoch: 26	Loss: 0.062849	Acc: 15.6% (1562/10000)
[Test]  Epoch: 27	Loss: 0.064716	Acc: 13.9% (1391/10000)
[Test]  Epoch: 28	Loss: 0.063932	Acc: 14.8% (1476/10000)
[Test]  Epoch: 29	Loss: 0.062916	Acc: 15.6% (1560/10000)
[Test]  Epoch: 30	Loss: 0.062701	Acc: 15.8% (1578/10000)
[Test]  Epoch: 31	Loss: 0.063426	Acc: 15.2% (1521/10000)
[Test]  Epoch: 32	Loss: 0.062005	Acc: 16.6% (1662/10000)
[Test]  Epoch: 33	Loss: 0.062658	Acc: 15.9% (1590/10000)
[Test]  Epoch: 34	Loss: 0.062686	Acc: 16.2% (1624/10000)
[Test]  Epoch: 35	Loss: 0.062136	Acc: 16.2% (1624/10000)
[Test]  Epoch: 36	Loss: 0.061794	Acc: 16.6% (1662/10000)
[Test]  Epoch: 37	Loss: 0.061775	Acc: 16.7% (1673/10000)
[Test]  Epoch: 38	Loss: 0.061612	Acc: 16.7% (1671/10000)
[Test]  Epoch: 39	Loss: 0.061456	Acc: 16.8% (1683/10000)
[Test]  Epoch: 40	Loss: 0.061223	Acc: 16.8% (1678/10000)
[Test]  Epoch: 41	Loss: 0.061589	Acc: 16.5% (1649/10000)
[Test]  Epoch: 42	Loss: 0.061165	Acc: 17.1% (1710/10000)
[Test]  Epoch: 43	Loss: 0.061550	Acc: 17.0% (1697/10000)
[Test]  Epoch: 44	Loss: 0.060454	Acc: 17.7% (1767/10000)
[Test]  Epoch: 45	Loss: 0.060047	Acc: 18.1% (1811/10000)
[Test]  Epoch: 46	Loss: 0.061605	Acc: 16.8% (1683/10000)
[Test]  Epoch: 47	Loss: 0.060132	Acc: 18.2% (1819/10000)
[Test]  Epoch: 48	Loss: 0.060700	Acc: 17.6% (1764/10000)
[Test]  Epoch: 49	Loss: 0.060571	Acc: 17.9% (1790/10000)
[Test]  Epoch: 50	Loss: 0.059830	Acc: 18.7% (1866/10000)
[Test]  Epoch: 51	Loss: 0.060361	Acc: 17.9% (1793/10000)
[Test]  Epoch: 52	Loss: 0.060434	Acc: 17.7% (1773/10000)
[Test]  Epoch: 53	Loss: 0.060444	Acc: 17.9% (1789/10000)
[Test]  Epoch: 54	Loss: 0.059535	Acc: 18.3% (1830/10000)
[Test]  Epoch: 55	Loss: 0.060050	Acc: 17.8% (1779/10000)
[Test]  Epoch: 56	Loss: 0.061155	Acc: 17.4% (1744/10000)
[Test]  Epoch: 57	Loss: 0.059547	Acc: 18.4% (1837/10000)
[Test]  Epoch: 58	Loss: 0.060080	Acc: 18.5% (1847/10000)
[Test]  Epoch: 59	Loss: 0.059958	Acc: 18.2% (1821/10000)
[Test]  Epoch: 60	Loss: 0.060222	Acc: 18.4% (1842/10000)
[Test]  Epoch: 61	Loss: 0.059389	Acc: 18.5% (1854/10000)
[Test]  Epoch: 62	Loss: 0.059050	Acc: 18.9% (1887/10000)
[Test]  Epoch: 63	Loss: 0.059194	Acc: 18.7% (1872/10000)
[Test]  Epoch: 64	Loss: 0.059028	Acc: 19.2% (1922/10000)
[Test]  Epoch: 65	Loss: 0.058949	Acc: 19.0% (1897/10000)
[Test]  Epoch: 66	Loss: 0.058948	Acc: 19.0% (1901/10000)
[Test]  Epoch: 67	Loss: 0.058968	Acc: 18.8% (1883/10000)
[Test]  Epoch: 68	Loss: 0.058938	Acc: 18.5% (1851/10000)
[Test]  Epoch: 69	Loss: 0.058926	Acc: 19.2% (1918/10000)
[Test]  Epoch: 70	Loss: 0.058989	Acc: 18.7% (1874/10000)
[Test]  Epoch: 71	Loss: 0.058695	Acc: 19.1% (1914/10000)
[Test]  Epoch: 72	Loss: 0.058430	Acc: 20.1% (2006/10000)
[Test]  Epoch: 73	Loss: 0.058779	Acc: 19.3% (1930/10000)
[Test]  Epoch: 74	Loss: 0.059002	Acc: 19.6% (1955/10000)
[Test]  Epoch: 75	Loss: 0.058471	Acc: 19.4% (1937/10000)
[Test]  Epoch: 76	Loss: 0.058788	Acc: 19.1% (1910/10000)
[Test]  Epoch: 77	Loss: 0.058620	Acc: 19.3% (1927/10000)
[Test]  Epoch: 78	Loss: 0.058764	Acc: 19.1% (1906/10000)
[Test]  Epoch: 79	Loss: 0.058862	Acc: 19.3% (1927/10000)
[Test]  Epoch: 80	Loss: 0.058115	Acc: 19.6% (1955/10000)
[Test]  Epoch: 81	Loss: 0.058769	Acc: 19.1% (1912/10000)
[Test]  Epoch: 82	Loss: 0.058604	Acc: 19.1% (1909/10000)
[Test]  Epoch: 83	Loss: 0.058648	Acc: 19.2% (1922/10000)
[Test]  Epoch: 84	Loss: 0.058812	Acc: 19.3% (1928/10000)
[Test]  Epoch: 85	Loss: 0.058839	Acc: 19.1% (1914/10000)
[Test]  Epoch: 86	Loss: 0.058660	Acc: 19.0% (1902/10000)
[Test]  Epoch: 87	Loss: 0.058455	Acc: 19.5% (1949/10000)
[Test]  Epoch: 88	Loss: 0.058321	Acc: 19.6% (1961/10000)
[Test]  Epoch: 89	Loss: 0.058323	Acc: 19.9% (1989/10000)
[Test]  Epoch: 90	Loss: 0.058378	Acc: 19.8% (1976/10000)
[Test]  Epoch: 91	Loss: 0.058541	Acc: 19.7% (1973/10000)
[Test]  Epoch: 92	Loss: 0.058554	Acc: 19.7% (1972/10000)
[Test]  Epoch: 93	Loss: 0.058615	Acc: 19.7% (1968/10000)
[Test]  Epoch: 94	Loss: 0.058257	Acc: 19.8% (1982/10000)
[Test]  Epoch: 95	Loss: 0.058656	Acc: 19.3% (1927/10000)
[Test]  Epoch: 96	Loss: 0.058685	Acc: 19.2% (1924/10000)
[Test]  Epoch: 97	Loss: 0.058260	Acc: 19.4% (1941/10000)
[Test]  Epoch: 98	Loss: 0.058558	Acc: 19.5% (1950/10000)
[Test]  Epoch: 99	Loss: 0.058585	Acc: 19.4% (1940/10000)
[Test]  Epoch: 100	Loss: 0.058424	Acc: 20.0% (1998/10000)
===========finish==========
['2024-08-19', '07:43:49.010555', '100', 'test', '0.058424238324165344', '19.98', '20.06']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.138978	Acc: 1.4% (136/10000)
[Test]  Epoch: 2	Loss: 0.084509	Acc: 3.6% (363/10000)
[Test]  Epoch: 3	Loss: 0.076672	Acc: 5.0% (496/10000)
[Test]  Epoch: 4	Loss: 0.076967	Acc: 5.2% (519/10000)
[Test]  Epoch: 5	Loss: 0.074992	Acc: 6.4% (644/10000)
[Test]  Epoch: 6	Loss: 0.073841	Acc: 7.1% (711/10000)
[Test]  Epoch: 7	Loss: 0.073990	Acc: 6.6% (664/10000)
[Test]  Epoch: 8	Loss: 0.073533	Acc: 6.8% (684/10000)
[Test]  Epoch: 9	Loss: 0.072457	Acc: 8.1% (806/10000)
[Test]  Epoch: 10	Loss: 0.073059	Acc: 7.5% (749/10000)
[Test]  Epoch: 11	Loss: 0.072874	Acc: 7.7% (769/10000)
[Test]  Epoch: 12	Loss: 0.071515	Acc: 8.5% (850/10000)
[Test]  Epoch: 13	Loss: 0.072895	Acc: 7.4% (742/10000)
[Test]  Epoch: 14	Loss: 0.071453	Acc: 8.4% (837/10000)
[Test]  Epoch: 15	Loss: 0.071658	Acc: 8.8% (877/10000)
[Test]  Epoch: 16	Loss: 0.069921	Acc: 9.3% (933/10000)
[Test]  Epoch: 17	Loss: 0.069755	Acc: 9.8% (977/10000)
[Test]  Epoch: 18	Loss: 0.069672	Acc: 9.9% (990/10000)
[Test]  Epoch: 19	Loss: 0.070052	Acc: 9.6% (963/10000)
[Test]  Epoch: 20	Loss: 0.069743	Acc: 10.3% (1035/10000)
[Test]  Epoch: 21	Loss: 0.068378	Acc: 11.1% (1111/10000)
[Test]  Epoch: 22	Loss: 0.068620	Acc: 11.0% (1097/10000)
[Test]  Epoch: 23	Loss: 0.068201	Acc: 10.8% (1082/10000)
[Test]  Epoch: 24	Loss: 0.067873	Acc: 11.0% (1101/10000)
[Test]  Epoch: 25	Loss: 0.067968	Acc: 11.3% (1133/10000)
[Test]  Epoch: 26	Loss: 0.069257	Acc: 10.2% (1025/10000)
[Test]  Epoch: 27	Loss: 0.068580	Acc: 10.9% (1095/10000)
[Test]  Epoch: 28	Loss: 0.067459	Acc: 11.5% (1150/10000)
[Test]  Epoch: 29	Loss: 0.066961	Acc: 12.0% (1196/10000)
[Test]  Epoch: 30	Loss: 0.066763	Acc: 12.3% (1234/10000)
[Test]  Epoch: 31	Loss: 0.067829	Acc: 11.6% (1161/10000)
[Test]  Epoch: 32	Loss: 0.067113	Acc: 12.2% (1225/10000)
[Test]  Epoch: 33	Loss: 0.068086	Acc: 11.1% (1105/10000)
[Test]  Epoch: 34	Loss: 0.066893	Acc: 11.7% (1171/10000)
[Test]  Epoch: 35	Loss: 0.068231	Acc: 12.0% (1204/10000)
[Test]  Epoch: 36	Loss: 0.066403	Acc: 12.8% (1279/10000)
[Test]  Epoch: 37	Loss: 0.066455	Acc: 12.4% (1239/10000)
[Test]  Epoch: 38	Loss: 0.066292	Acc: 12.4% (1244/10000)
[Test]  Epoch: 39	Loss: 0.066193	Acc: 12.5% (1250/10000)
[Test]  Epoch: 40	Loss: 0.066807	Acc: 12.2% (1218/10000)
[Test]  Epoch: 41	Loss: 0.066953	Acc: 12.1% (1212/10000)
[Test]  Epoch: 42	Loss: 0.065869	Acc: 12.8% (1279/10000)
[Test]  Epoch: 43	Loss: 0.065529	Acc: 13.6% (1359/10000)
[Test]  Epoch: 44	Loss: 0.065098	Acc: 13.8% (1384/10000)
[Test]  Epoch: 45	Loss: 0.066207	Acc: 13.2% (1317/10000)
[Test]  Epoch: 46	Loss: 0.066736	Acc: 12.2% (1221/10000)
[Test]  Epoch: 47	Loss: 0.066534	Acc: 12.3% (1232/10000)
[Test]  Epoch: 48	Loss: 0.064932	Acc: 13.7% (1367/10000)
[Test]  Epoch: 49	Loss: 0.065714	Acc: 13.2% (1320/10000)
[Test]  Epoch: 50	Loss: 0.064933	Acc: 13.5% (1349/10000)
[Test]  Epoch: 51	Loss: 0.065831	Acc: 13.5% (1347/10000)
[Test]  Epoch: 52	Loss: 0.064990	Acc: 13.4% (1343/10000)
[Test]  Epoch: 53	Loss: 0.065693	Acc: 13.1% (1313/10000)
[Test]  Epoch: 54	Loss: 0.065473	Acc: 13.1% (1312/10000)
[Test]  Epoch: 55	Loss: 0.064888	Acc: 13.9% (1387/10000)
[Test]  Epoch: 56	Loss: 0.065381	Acc: 13.3% (1331/10000)
[Test]  Epoch: 57	Loss: 0.065017	Acc: 13.5% (1354/10000)
[Test]  Epoch: 58	Loss: 0.064624	Acc: 14.2% (1421/10000)
[Test]  Epoch: 59	Loss: 0.064991	Acc: 13.2% (1320/10000)
[Test]  Epoch: 60	Loss: 0.065711	Acc: 13.7% (1371/10000)
[Test]  Epoch: 61	Loss: 0.064058	Acc: 14.0% (1400/10000)
[Test]  Epoch: 62	Loss: 0.064063	Acc: 14.1% (1412/10000)
[Test]  Epoch: 63	Loss: 0.064085	Acc: 14.3% (1428/10000)
[Test]  Epoch: 64	Loss: 0.063535	Acc: 14.9% (1493/10000)
[Test]  Epoch: 65	Loss: 0.063871	Acc: 14.3% (1431/10000)
[Test]  Epoch: 66	Loss: 0.063594	Acc: 14.4% (1437/10000)
[Test]  Epoch: 67	Loss: 0.063839	Acc: 14.3% (1430/10000)
[Test]  Epoch: 68	Loss: 0.063617	Acc: 14.2% (1420/10000)
[Test]  Epoch: 69	Loss: 0.063600	Acc: 14.9% (1492/10000)
[Test]  Epoch: 70	Loss: 0.063700	Acc: 14.6% (1462/10000)
[Test]  Epoch: 71	Loss: 0.063571	Acc: 14.4% (1441/10000)
[Test]  Epoch: 72	Loss: 0.063189	Acc: 15.4% (1541/10000)
[Test]  Epoch: 73	Loss: 0.063542	Acc: 14.6% (1461/10000)
[Test]  Epoch: 74	Loss: 0.063645	Acc: 14.8% (1480/10000)
[Test]  Epoch: 75	Loss: 0.063429	Acc: 14.8% (1478/10000)
[Test]  Epoch: 76	Loss: 0.063215	Acc: 14.6% (1456/10000)
[Test]  Epoch: 77	Loss: 0.063413	Acc: 15.0% (1497/10000)
[Test]  Epoch: 78	Loss: 0.063497	Acc: 14.7% (1473/10000)
[Test]  Epoch: 79	Loss: 0.063671	Acc: 14.8% (1484/10000)
[Test]  Epoch: 80	Loss: 0.062941	Acc: 15.3% (1532/10000)
[Test]  Epoch: 81	Loss: 0.063454	Acc: 14.8% (1478/10000)
[Test]  Epoch: 82	Loss: 0.063402	Acc: 15.0% (1496/10000)
[Test]  Epoch: 83	Loss: 0.063472	Acc: 14.8% (1479/10000)
[Test]  Epoch: 84	Loss: 0.063430	Acc: 14.7% (1465/10000)
[Test]  Epoch: 85	Loss: 0.063727	Acc: 14.4% (1442/10000)
[Test]  Epoch: 86	Loss: 0.063388	Acc: 14.9% (1486/10000)
[Test]  Epoch: 87	Loss: 0.063413	Acc: 15.1% (1508/10000)
[Test]  Epoch: 88	Loss: 0.063179	Acc: 14.8% (1483/10000)
[Test]  Epoch: 89	Loss: 0.063188	Acc: 15.1% (1508/10000)
[Test]  Epoch: 90	Loss: 0.063214	Acc: 14.8% (1476/10000)
[Test]  Epoch: 91	Loss: 0.063378	Acc: 14.8% (1479/10000)
[Test]  Epoch: 92	Loss: 0.063098	Acc: 15.7% (1565/10000)
[Test]  Epoch: 93	Loss: 0.063297	Acc: 15.2% (1516/10000)
[Test]  Epoch: 94	Loss: 0.063150	Acc: 14.9% (1493/10000)
[Test]  Epoch: 95	Loss: 0.063479	Acc: 14.5% (1451/10000)
[Test]  Epoch: 96	Loss: 0.063411	Acc: 14.7% (1474/10000)
[Test]  Epoch: 97	Loss: 0.063111	Acc: 15.1% (1514/10000)
[Test]  Epoch: 98	Loss: 0.063131	Acc: 14.7% (1470/10000)
[Test]  Epoch: 99	Loss: 0.063267	Acc: 15.2% (1525/10000)
[Test]  Epoch: 100	Loss: 0.063029	Acc: 15.1% (1511/10000)
===========finish==========
['2024-08-19', '07:47:01.938466', '100', 'test', '0.06302879087924958', '15.11', '15.65']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.125899	Acc: 1.5% (146/10000)
[Test]  Epoch: 2	Loss: 0.081876	Acc: 2.7% (271/10000)
[Test]  Epoch: 3	Loss: 0.079409	Acc: 2.9% (292/10000)
[Test]  Epoch: 4	Loss: 0.078944	Acc: 3.1% (315/10000)
[Test]  Epoch: 5	Loss: 0.077935	Acc: 3.7% (368/10000)
[Test]  Epoch: 6	Loss: 0.078753	Acc: 4.0% (405/10000)
[Test]  Epoch: 7	Loss: 0.076396	Acc: 4.5% (455/10000)
[Test]  Epoch: 8	Loss: 0.077757	Acc: 4.2% (420/10000)
[Test]  Epoch: 9	Loss: 0.076253	Acc: 4.5% (454/10000)
[Test]  Epoch: 10	Loss: 0.075786	Acc: 5.5% (549/10000)
[Test]  Epoch: 11	Loss: 0.076451	Acc: 5.1% (512/10000)
[Test]  Epoch: 12	Loss: 0.074982	Acc: 5.7% (568/10000)
[Test]  Epoch: 13	Loss: 0.075485	Acc: 5.7% (566/10000)
[Test]  Epoch: 14	Loss: 0.075347	Acc: 5.5% (550/10000)
[Test]  Epoch: 15	Loss: 0.074831	Acc: 6.2% (621/10000)
[Test]  Epoch: 16	Loss: 0.074382	Acc: 5.9% (590/10000)
[Test]  Epoch: 17	Loss: 0.075634	Acc: 5.6% (561/10000)
[Test]  Epoch: 18	Loss: 0.075226	Acc: 6.0% (599/10000)
[Test]  Epoch: 19	Loss: 0.074598	Acc: 6.2% (622/10000)
[Test]  Epoch: 20	Loss: 0.073492	Acc: 7.0% (699/10000)
[Test]  Epoch: 21	Loss: 0.073644	Acc: 7.0% (702/10000)
[Test]  Epoch: 22	Loss: 0.072970	Acc: 7.3% (733/10000)
[Test]  Epoch: 23	Loss: 0.074109	Acc: 7.0% (701/10000)
[Test]  Epoch: 24	Loss: 0.072577	Acc: 7.3% (732/10000)
[Test]  Epoch: 25	Loss: 0.072457	Acc: 7.6% (758/10000)
[Test]  Epoch: 26	Loss: 0.073318	Acc: 7.1% (712/10000)
[Test]  Epoch: 27	Loss: 0.073345	Acc: 7.8% (780/10000)
[Test]  Epoch: 28	Loss: 0.072513	Acc: 7.3% (735/10000)
[Test]  Epoch: 29	Loss: 0.073797	Acc: 7.2% (719/10000)
[Test]  Epoch: 30	Loss: 0.072165	Acc: 7.7% (774/10000)
[Test]  Epoch: 31	Loss: 0.072099	Acc: 8.2% (815/10000)
[Test]  Epoch: 32	Loss: 0.072665	Acc: 8.1% (806/10000)
[Test]  Epoch: 33	Loss: 0.074347	Acc: 7.4% (739/10000)
[Test]  Epoch: 34	Loss: 0.072155	Acc: 8.6% (858/10000)
[Test]  Epoch: 35	Loss: 0.073101	Acc: 7.5% (751/10000)
[Test]  Epoch: 36	Loss: 0.072486	Acc: 8.3% (834/10000)
[Test]  Epoch: 37	Loss: 0.072478	Acc: 8.0% (795/10000)
[Test]  Epoch: 38	Loss: 0.070970	Acc: 9.0% (900/10000)
[Test]  Epoch: 39	Loss: 0.071425	Acc: 8.5% (849/10000)
[Test]  Epoch: 40	Loss: 0.071266	Acc: 8.7% (867/10000)
[Test]  Epoch: 41	Loss: 0.071278	Acc: 8.8% (878/10000)
[Test]  Epoch: 42	Loss: 0.071126	Acc: 9.1% (908/10000)
[Test]  Epoch: 43	Loss: 0.071258	Acc: 8.8% (883/10000)
[Test]  Epoch: 44	Loss: 0.071362	Acc: 8.9% (889/10000)
[Test]  Epoch: 45	Loss: 0.071754	Acc: 8.3% (835/10000)
[Test]  Epoch: 46	Loss: 0.071644	Acc: 8.7% (871/10000)
[Test]  Epoch: 47	Loss: 0.070998	Acc: 9.1% (910/10000)
[Test]  Epoch: 48	Loss: 0.071175	Acc: 9.2% (917/10000)
[Test]  Epoch: 49	Loss: 0.072672	Acc: 8.6% (860/10000)
[Test]  Epoch: 50	Loss: 0.070692	Acc: 9.9% (991/10000)
[Test]  Epoch: 51	Loss: 0.070415	Acc: 9.8% (976/10000)
[Test]  Epoch: 52	Loss: 0.070756	Acc: 8.9% (895/10000)
[Test]  Epoch: 53	Loss: 0.070359	Acc: 9.6% (963/10000)
[Test]  Epoch: 54	Loss: 0.070440	Acc: 9.1% (905/10000)
[Test]  Epoch: 55	Loss: 0.072687	Acc: 8.5% (850/10000)
[Test]  Epoch: 56	Loss: 0.071371	Acc: 9.0% (901/10000)
[Test]  Epoch: 57	Loss: 0.070586	Acc: 9.4% (943/10000)
[Test]  Epoch: 58	Loss: 0.070931	Acc: 9.6% (960/10000)
[Test]  Epoch: 59	Loss: 0.071418	Acc: 8.6% (857/10000)
[Test]  Epoch: 60	Loss: 0.070070	Acc: 9.8% (976/10000)
[Test]  Epoch: 61	Loss: 0.069117	Acc: 10.3% (1029/10000)
[Test]  Epoch: 62	Loss: 0.068701	Acc: 10.3% (1035/10000)
[Test]  Epoch: 63	Loss: 0.068813	Acc: 10.4% (1041/10000)
[Test]  Epoch: 64	Loss: 0.068723	Acc: 10.4% (1039/10000)
[Test]  Epoch: 65	Loss: 0.068760	Acc: 10.4% (1038/10000)
[Test]  Epoch: 66	Loss: 0.068529	Acc: 10.6% (1056/10000)
[Test]  Epoch: 67	Loss: 0.068726	Acc: 10.6% (1056/10000)
[Test]  Epoch: 68	Loss: 0.068756	Acc: 10.1% (1007/10000)
[Test]  Epoch: 69	Loss: 0.068531	Acc: 10.4% (1039/10000)
[Test]  Epoch: 70	Loss: 0.068494	Acc: 10.7% (1070/10000)
[Test]  Epoch: 71	Loss: 0.068480	Acc: 10.5% (1051/10000)
[Test]  Epoch: 72	Loss: 0.068383	Acc: 10.8% (1084/10000)
[Test]  Epoch: 73	Loss: 0.068677	Acc: 11.1% (1106/10000)
[Test]  Epoch: 74	Loss: 0.068866	Acc: 10.8% (1082/10000)
[Test]  Epoch: 75	Loss: 0.068361	Acc: 10.5% (1047/10000)
[Test]  Epoch: 76	Loss: 0.068442	Acc: 10.6% (1057/10000)
[Test]  Epoch: 77	Loss: 0.068548	Acc: 10.7% (1072/10000)
[Test]  Epoch: 78	Loss: 0.068555	Acc: 10.8% (1077/10000)
[Test]  Epoch: 79	Loss: 0.068390	Acc: 10.8% (1075/10000)
[Test]  Epoch: 80	Loss: 0.067999	Acc: 11.0% (1099/10000)
[Test]  Epoch: 81	Loss: 0.068531	Acc: 10.3% (1028/10000)
[Test]  Epoch: 82	Loss: 0.068201	Acc: 10.7% (1073/10000)
[Test]  Epoch: 83	Loss: 0.068574	Acc: 10.8% (1075/10000)
[Test]  Epoch: 84	Loss: 0.068428	Acc: 10.3% (1034/10000)
[Test]  Epoch: 85	Loss: 0.068397	Acc: 10.7% (1067/10000)
[Test]  Epoch: 86	Loss: 0.068576	Acc: 11.0% (1099/10000)
[Test]  Epoch: 87	Loss: 0.068189	Acc: 11.2% (1121/10000)
[Test]  Epoch: 88	Loss: 0.068147	Acc: 11.0% (1100/10000)
[Test]  Epoch: 89	Loss: 0.067950	Acc: 10.8% (1079/10000)
[Test]  Epoch: 90	Loss: 0.068435	Acc: 10.9% (1090/10000)
[Test]  Epoch: 91	Loss: 0.068223	Acc: 11.0% (1101/10000)
[Test]  Epoch: 92	Loss: 0.068083	Acc: 11.4% (1137/10000)
[Test]  Epoch: 93	Loss: 0.068315	Acc: 10.6% (1062/10000)
[Test]  Epoch: 94	Loss: 0.068378	Acc: 10.8% (1083/10000)
[Test]  Epoch: 95	Loss: 0.068564	Acc: 10.9% (1089/10000)
[Test]  Epoch: 96	Loss: 0.068442	Acc: 10.2% (1023/10000)
[Test]  Epoch: 97	Loss: 0.067989	Acc: 11.0% (1102/10000)
[Test]  Epoch: 98	Loss: 0.068230	Acc: 11.2% (1120/10000)
[Test]  Epoch: 99	Loss: 0.068294	Acc: 10.7% (1072/10000)
[Test]  Epoch: 100	Loss: 0.068141	Acc: 11.1% (1108/10000)
===========finish==========
['2024-08-19', '07:50:15.944863', '100', 'test', '0.06814114010334014', '11.08', '11.37']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.299905	Acc: 0.8% (78/10000)
[Test]  Epoch: 2	Loss: 0.083301	Acc: 2.2% (217/10000)
[Test]  Epoch: 3	Loss: 0.080948	Acc: 2.4% (240/10000)
[Test]  Epoch: 4	Loss: 0.079836	Acc: 3.5% (347/10000)
[Test]  Epoch: 5	Loss: 0.078501	Acc: 3.3% (326/10000)
[Test]  Epoch: 6	Loss: 0.078389	Acc: 3.6% (365/10000)
[Test]  Epoch: 7	Loss: 0.078185	Acc: 3.4% (336/10000)
[Test]  Epoch: 8	Loss: 0.077705	Acc: 3.8% (378/10000)
[Test]  Epoch: 9	Loss: 0.077841	Acc: 4.1% (407/10000)
[Test]  Epoch: 10	Loss: 0.077228	Acc: 4.0% (403/10000)
[Test]  Epoch: 11	Loss: 0.076731	Acc: 4.4% (439/10000)
[Test]  Epoch: 12	Loss: 0.075957	Acc: 4.8% (484/10000)
[Test]  Epoch: 13	Loss: 0.076733	Acc: 4.5% (453/10000)
[Test]  Epoch: 14	Loss: 0.076300	Acc: 4.5% (451/10000)
[Test]  Epoch: 15	Loss: 0.077158	Acc: 4.4% (437/10000)
[Test]  Epoch: 16	Loss: 0.077100	Acc: 4.9% (489/10000)
[Test]  Epoch: 17	Loss: 0.075583	Acc: 5.6% (556/10000)
[Test]  Epoch: 18	Loss: 0.076324	Acc: 5.0% (501/10000)
[Test]  Epoch: 19	Loss: 0.075996	Acc: 5.4% (538/10000)
[Test]  Epoch: 20	Loss: 0.074920	Acc: 6.0% (597/10000)
[Test]  Epoch: 21	Loss: 0.074260	Acc: 6.4% (637/10000)
[Test]  Epoch: 22	Loss: 0.075461	Acc: 5.8% (579/10000)
[Test]  Epoch: 23	Loss: 0.075232	Acc: 5.4% (544/10000)
[Test]  Epoch: 24	Loss: 0.074796	Acc: 5.6% (563/10000)
[Test]  Epoch: 25	Loss: 0.075302	Acc: 6.4% (639/10000)
[Test]  Epoch: 26	Loss: 0.074112	Acc: 6.2% (625/10000)
[Test]  Epoch: 27	Loss: 0.074153	Acc: 6.5% (648/10000)
[Test]  Epoch: 28	Loss: 0.073426	Acc: 6.6% (658/10000)
[Test]  Epoch: 29	Loss: 0.073683	Acc: 6.4% (637/10000)
[Test]  Epoch: 30	Loss: 0.073848	Acc: 6.6% (659/10000)
[Test]  Epoch: 31	Loss: 0.074422	Acc: 6.6% (660/10000)
[Test]  Epoch: 32	Loss: 0.073716	Acc: 6.9% (693/10000)
[Test]  Epoch: 33	Loss: 0.074157	Acc: 6.8% (677/10000)
[Test]  Epoch: 34	Loss: 0.074086	Acc: 7.2% (719/10000)
[Test]  Epoch: 35	Loss: 0.073440	Acc: 7.0% (699/10000)
[Test]  Epoch: 36	Loss: 0.074262	Acc: 6.8% (678/10000)
[Test]  Epoch: 37	Loss: 0.074057	Acc: 6.8% (684/10000)
[Test]  Epoch: 38	Loss: 0.074218	Acc: 7.3% (727/10000)
[Test]  Epoch: 39	Loss: 0.074008	Acc: 7.0% (703/10000)
[Test]  Epoch: 40	Loss: 0.073821	Acc: 6.7% (665/10000)
[Test]  Epoch: 41	Loss: 0.073394	Acc: 7.1% (710/10000)
[Test]  Epoch: 42	Loss: 0.074253	Acc: 6.8% (678/10000)
[Test]  Epoch: 43	Loss: 0.072434	Acc: 7.6% (758/10000)
[Test]  Epoch: 44	Loss: 0.073366	Acc: 7.7% (767/10000)
[Test]  Epoch: 45	Loss: 0.073345	Acc: 7.0% (699/10000)
[Test]  Epoch: 46	Loss: 0.074330	Acc: 7.2% (718/10000)
[Test]  Epoch: 47	Loss: 0.073114	Acc: 7.5% (750/10000)
[Test]  Epoch: 48	Loss: 0.074044	Acc: 7.0% (702/10000)
[Test]  Epoch: 49	Loss: 0.073129	Acc: 7.7% (773/10000)
[Test]  Epoch: 50	Loss: 0.073431	Acc: 7.6% (756/10000)
[Test]  Epoch: 51	Loss: 0.073371	Acc: 8.1% (805/10000)
[Test]  Epoch: 52	Loss: 0.072067	Acc: 7.9% (786/10000)
[Test]  Epoch: 53	Loss: 0.071973	Acc: 8.2% (821/10000)
[Test]  Epoch: 54	Loss: 0.072850	Acc: 8.0% (800/10000)
[Test]  Epoch: 55	Loss: 0.074553	Acc: 6.8% (682/10000)
[Test]  Epoch: 56	Loss: 0.073438	Acc: 7.5% (751/10000)
[Test]  Epoch: 57	Loss: 0.072458	Acc: 8.3% (828/10000)
[Test]  Epoch: 58	Loss: 0.072601	Acc: 7.9% (794/10000)
[Test]  Epoch: 59	Loss: 0.072847	Acc: 7.6% (756/10000)
[Test]  Epoch: 60	Loss: 0.072299	Acc: 7.8% (781/10000)
[Test]  Epoch: 61	Loss: 0.071257	Acc: 8.5% (853/10000)
[Test]  Epoch: 62	Loss: 0.071074	Acc: 8.5% (846/10000)
[Test]  Epoch: 63	Loss: 0.070898	Acc: 9.1% (909/10000)
[Test]  Epoch: 64	Loss: 0.070846	Acc: 8.8% (882/10000)
[Test]  Epoch: 65	Loss: 0.070939	Acc: 8.7% (867/10000)
[Test]  Epoch: 66	Loss: 0.070522	Acc: 9.4% (942/10000)
[Test]  Epoch: 67	Loss: 0.070563	Acc: 9.3% (931/10000)
[Test]  Epoch: 68	Loss: 0.070803	Acc: 8.9% (886/10000)
[Test]  Epoch: 69	Loss: 0.070888	Acc: 8.9% (892/10000)
[Test]  Epoch: 70	Loss: 0.070552	Acc: 9.3% (932/10000)
[Test]  Epoch: 71	Loss: 0.070765	Acc: 8.8% (885/10000)
[Test]  Epoch: 72	Loss: 0.070512	Acc: 9.1% (908/10000)
[Test]  Epoch: 73	Loss: 0.070525	Acc: 9.4% (945/10000)
[Test]  Epoch: 74	Loss: 0.070546	Acc: 9.1% (913/10000)
[Test]  Epoch: 75	Loss: 0.070713	Acc: 8.4% (842/10000)
[Test]  Epoch: 76	Loss: 0.070511	Acc: 9.3% (928/10000)
[Test]  Epoch: 77	Loss: 0.070533	Acc: 9.3% (931/10000)
[Test]  Epoch: 78	Loss: 0.070590	Acc: 9.1% (912/10000)
[Test]  Epoch: 79	Loss: 0.070830	Acc: 9.2% (921/10000)
[Test]  Epoch: 80	Loss: 0.070333	Acc: 9.2% (923/10000)
[Test]  Epoch: 81	Loss: 0.070620	Acc: 9.4% (936/10000)
[Test]  Epoch: 82	Loss: 0.070433	Acc: 9.1% (908/10000)
[Test]  Epoch: 83	Loss: 0.070539	Acc: 9.2% (921/10000)
[Test]  Epoch: 84	Loss: 0.070554	Acc: 9.3% (928/10000)
[Test]  Epoch: 85	Loss: 0.070367	Acc: 9.5% (946/10000)
[Test]  Epoch: 86	Loss: 0.070652	Acc: 9.5% (952/10000)
[Test]  Epoch: 87	Loss: 0.070531	Acc: 9.1% (906/10000)
[Test]  Epoch: 88	Loss: 0.070358	Acc: 9.5% (947/10000)
[Test]  Epoch: 89	Loss: 0.070371	Acc: 9.4% (938/10000)
[Test]  Epoch: 90	Loss: 0.070319	Acc: 8.8% (881/10000)
[Test]  Epoch: 91	Loss: 0.070563	Acc: 9.0% (900/10000)
[Test]  Epoch: 92	Loss: 0.070131	Acc: 9.4% (945/10000)
[Test]  Epoch: 93	Loss: 0.070478	Acc: 9.3% (930/10000)
[Test]  Epoch: 94	Loss: 0.070626	Acc: 8.9% (894/10000)
[Test]  Epoch: 95	Loss: 0.070693	Acc: 9.3% (928/10000)
[Test]  Epoch: 96	Loss: 0.070716	Acc: 8.8% (880/10000)
[Test]  Epoch: 97	Loss: 0.070142	Acc: 9.4% (938/10000)
[Test]  Epoch: 98	Loss: 0.070741	Acc: 9.1% (912/10000)
[Test]  Epoch: 99	Loss: 0.070376	Acc: 9.1% (911/10000)
[Test]  Epoch: 100	Loss: 0.070610	Acc: 9.3% (930/10000)
===========finish==========
['2024-08-19', '07:53:34.239289', '100', 'test', '0.07061044034957886', '9.3', '9.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.229562	Acc: 0.5% (48/10000)
[Test]  Epoch: 2	Loss: 0.081259	Acc: 2.5% (253/10000)
[Test]  Epoch: 3	Loss: 0.079845	Acc: 2.9% (291/10000)
[Test]  Epoch: 4	Loss: 0.080302	Acc: 2.8% (281/10000)
[Test]  Epoch: 5	Loss: 0.078469	Acc: 3.5% (351/10000)
[Test]  Epoch: 6	Loss: 0.078038	Acc: 3.3% (334/10000)
[Test]  Epoch: 7	Loss: 0.078661	Acc: 3.6% (361/10000)
[Test]  Epoch: 8	Loss: 0.078090	Acc: 3.7% (368/10000)
[Test]  Epoch: 9	Loss: 0.077673	Acc: 3.9% (391/10000)
[Test]  Epoch: 10	Loss: 0.077444	Acc: 3.6% (364/10000)
[Test]  Epoch: 11	Loss: 0.076968	Acc: 3.6% (362/10000)
[Test]  Epoch: 12	Loss: 0.076493	Acc: 4.3% (426/10000)
[Test]  Epoch: 13	Loss: 0.077908	Acc: 4.1% (413/10000)
[Test]  Epoch: 14	Loss: 0.077653	Acc: 4.2% (421/10000)
[Test]  Epoch: 15	Loss: 0.076566	Acc: 4.6% (462/10000)
[Test]  Epoch: 16	Loss: 0.077557	Acc: 4.6% (458/10000)
[Test]  Epoch: 17	Loss: 0.076946	Acc: 4.5% (452/10000)
[Test]  Epoch: 18	Loss: 0.078536	Acc: 4.0% (402/10000)
[Test]  Epoch: 19	Loss: 0.076797	Acc: 5.1% (509/10000)
[Test]  Epoch: 20	Loss: 0.076190	Acc: 5.1% (507/10000)
[Test]  Epoch: 21	Loss: 0.076062	Acc: 5.3% (530/10000)
[Test]  Epoch: 22	Loss: 0.075465	Acc: 5.7% (570/10000)
[Test]  Epoch: 23	Loss: 0.075626	Acc: 5.5% (545/10000)
[Test]  Epoch: 24	Loss: 0.075351	Acc: 5.5% (545/10000)
[Test]  Epoch: 25	Loss: 0.077998	Acc: 4.6% (461/10000)
[Test]  Epoch: 26	Loss: 0.076537	Acc: 5.3% (530/10000)
[Test]  Epoch: 27	Loss: 0.075668	Acc: 5.4% (542/10000)
[Test]  Epoch: 28	Loss: 0.075091	Acc: 5.8% (580/10000)
[Test]  Epoch: 29	Loss: 0.075038	Acc: 5.9% (590/10000)
[Test]  Epoch: 30	Loss: 0.074981	Acc: 6.1% (610/10000)
[Test]  Epoch: 31	Loss: 0.075177	Acc: 5.5% (549/10000)
[Test]  Epoch: 32	Loss: 0.074933	Acc: 6.0% (595/10000)
[Test]  Epoch: 33	Loss: 0.076213	Acc: 5.5% (547/10000)
[Test]  Epoch: 34	Loss: 0.074969	Acc: 6.2% (622/10000)
[Test]  Epoch: 35	Loss: 0.076018	Acc: 5.7% (571/10000)
[Test]  Epoch: 36	Loss: 0.075607	Acc: 6.0% (603/10000)
[Test]  Epoch: 37	Loss: 0.076501	Acc: 5.5% (554/10000)
[Test]  Epoch: 38	Loss: 0.074733	Acc: 6.5% (652/10000)
[Test]  Epoch: 39	Loss: 0.074613	Acc: 6.1% (611/10000)
[Test]  Epoch: 40	Loss: 0.074713	Acc: 6.0% (596/10000)
[Test]  Epoch: 41	Loss: 0.075420	Acc: 6.1% (610/10000)
[Test]  Epoch: 42	Loss: 0.074628	Acc: 6.0% (597/10000)
[Test]  Epoch: 43	Loss: 0.075107	Acc: 6.1% (610/10000)
[Test]  Epoch: 44	Loss: 0.073880	Acc: 6.7% (670/10000)
[Test]  Epoch: 45	Loss: 0.074866	Acc: 6.2% (617/10000)
[Test]  Epoch: 46	Loss: 0.075730	Acc: 5.8% (583/10000)
[Test]  Epoch: 47	Loss: 0.074841	Acc: 6.2% (620/10000)
[Test]  Epoch: 48	Loss: 0.074518	Acc: 6.7% (673/10000)
[Test]  Epoch: 49	Loss: 0.075799	Acc: 6.2% (625/10000)
[Test]  Epoch: 50	Loss: 0.075276	Acc: 6.3% (629/10000)
[Test]  Epoch: 51	Loss: 0.074577	Acc: 6.6% (658/10000)
[Test]  Epoch: 52	Loss: 0.073977	Acc: 6.6% (656/10000)
[Test]  Epoch: 53	Loss: 0.074706	Acc: 6.3% (632/10000)
[Test]  Epoch: 54	Loss: 0.073753	Acc: 7.0% (702/10000)
[Test]  Epoch: 55	Loss: 0.075149	Acc: 6.7% (668/10000)
[Test]  Epoch: 56	Loss: 0.074641	Acc: 6.7% (669/10000)
[Test]  Epoch: 57	Loss: 0.074347	Acc: 6.7% (673/10000)
[Test]  Epoch: 58	Loss: 0.073490	Acc: 6.9% (693/10000)
[Test]  Epoch: 59	Loss: 0.074416	Acc: 6.7% (670/10000)
[Test]  Epoch: 60	Loss: 0.074639	Acc: 6.6% (658/10000)
[Test]  Epoch: 61	Loss: 0.072644	Acc: 7.5% (752/10000)
[Test]  Epoch: 62	Loss: 0.072451	Acc: 7.7% (767/10000)
[Test]  Epoch: 63	Loss: 0.072536	Acc: 7.6% (762/10000)
[Test]  Epoch: 64	Loss: 0.072268	Acc: 8.0% (798/10000)
[Test]  Epoch: 65	Loss: 0.072251	Acc: 7.8% (781/10000)
[Test]  Epoch: 66	Loss: 0.072190	Acc: 8.0% (804/10000)
[Test]  Epoch: 67	Loss: 0.072295	Acc: 7.9% (788/10000)
[Test]  Epoch: 68	Loss: 0.072243	Acc: 7.6% (758/10000)
[Test]  Epoch: 69	Loss: 0.072142	Acc: 7.5% (753/10000)
[Test]  Epoch: 70	Loss: 0.071979	Acc: 8.0% (804/10000)
[Test]  Epoch: 71	Loss: 0.072051	Acc: 7.5% (755/10000)
[Test]  Epoch: 72	Loss: 0.071993	Acc: 8.2% (816/10000)
[Test]  Epoch: 73	Loss: 0.072004	Acc: 7.8% (776/10000)
[Test]  Epoch: 74	Loss: 0.072301	Acc: 7.7% (768/10000)
[Test]  Epoch: 75	Loss: 0.072120	Acc: 8.1% (806/10000)
[Test]  Epoch: 76	Loss: 0.072257	Acc: 7.7% (770/10000)
[Test]  Epoch: 77	Loss: 0.072215	Acc: 7.6% (757/10000)
[Test]  Epoch: 78	Loss: 0.072084	Acc: 7.8% (783/10000)
[Test]  Epoch: 79	Loss: 0.072124	Acc: 8.0% (802/10000)
[Test]  Epoch: 80	Loss: 0.071697	Acc: 8.3% (832/10000)
[Test]  Epoch: 81	Loss: 0.072274	Acc: 7.7% (770/10000)
[Test]  Epoch: 82	Loss: 0.071881	Acc: 8.0% (795/10000)
[Test]  Epoch: 83	Loss: 0.072276	Acc: 8.0% (795/10000)
[Test]  Epoch: 84	Loss: 0.072230	Acc: 7.6% (759/10000)
[Test]  Epoch: 85	Loss: 0.072034	Acc: 7.9% (791/10000)
[Test]  Epoch: 86	Loss: 0.072118	Acc: 7.8% (781/10000)
[Test]  Epoch: 87	Loss: 0.072156	Acc: 7.8% (779/10000)
[Test]  Epoch: 88	Loss: 0.071866	Acc: 8.1% (807/10000)
[Test]  Epoch: 89	Loss: 0.071876	Acc: 8.0% (799/10000)
[Test]  Epoch: 90	Loss: 0.071898	Acc: 8.2% (822/10000)
[Test]  Epoch: 91	Loss: 0.072288	Acc: 8.1% (807/10000)
[Test]  Epoch: 92	Loss: 0.071667	Acc: 7.7% (765/10000)
[Test]  Epoch: 93	Loss: 0.072201	Acc: 7.8% (785/10000)
[Test]  Epoch: 94	Loss: 0.071893	Acc: 7.9% (793/10000)
[Test]  Epoch: 95	Loss: 0.072250	Acc: 7.9% (791/10000)
[Test]  Epoch: 96	Loss: 0.072321	Acc: 8.0% (799/10000)
[Test]  Epoch: 97	Loss: 0.071770	Acc: 8.2% (819/10000)
[Test]  Epoch: 98	Loss: 0.072402	Acc: 8.2% (821/10000)
[Test]  Epoch: 99	Loss: 0.071921	Acc: 8.0% (802/10000)
[Test]  Epoch: 100	Loss: 0.072134	Acc: 8.2% (821/10000)
===========finish==========
['2024-08-19', '07:56:45.737467', '100', 'test', '0.07213394157886505', '8.21', '8.32']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.234891	Acc: 0.5% (52/10000)
[Test]  Epoch: 2	Loss: 0.080795	Acc: 2.5% (254/10000)
[Test]  Epoch: 3	Loss: 0.079888	Acc: 2.8% (275/10000)
[Test]  Epoch: 4	Loss: 0.079182	Acc: 2.7% (274/10000)
[Test]  Epoch: 5	Loss: 0.078031	Acc: 3.3% (331/10000)
[Test]  Epoch: 6	Loss: 0.078178	Acc: 3.4% (344/10000)
[Test]  Epoch: 7	Loss: 0.077765	Acc: 3.6% (363/10000)
[Test]  Epoch: 8	Loss: 0.077627	Acc: 3.5% (349/10000)
[Test]  Epoch: 9	Loss: 0.077242	Acc: 3.7% (369/10000)
[Test]  Epoch: 10	Loss: 0.077805	Acc: 3.8% (375/10000)
[Test]  Epoch: 11	Loss: 0.076839	Acc: 4.0% (402/10000)
[Test]  Epoch: 12	Loss: 0.077108	Acc: 3.9% (387/10000)
[Test]  Epoch: 13	Loss: 0.077190	Acc: 4.1% (410/10000)
[Test]  Epoch: 14	Loss: 0.076440	Acc: 4.3% (435/10000)
[Test]  Epoch: 15	Loss: 0.077282	Acc: 4.4% (440/10000)
[Test]  Epoch: 16	Loss: 0.076624	Acc: 4.7% (471/10000)
[Test]  Epoch: 17	Loss: 0.077142	Acc: 4.4% (440/10000)
[Test]  Epoch: 18	Loss: 0.076534	Acc: 4.6% (459/10000)
[Test]  Epoch: 19	Loss: 0.075280	Acc: 5.0% (502/10000)
[Test]  Epoch: 20	Loss: 0.076038	Acc: 5.4% (542/10000)
[Test]  Epoch: 21	Loss: 0.076670	Acc: 4.6% (458/10000)
[Test]  Epoch: 22	Loss: 0.076149	Acc: 5.0% (496/10000)
[Test]  Epoch: 23	Loss: 0.077872	Acc: 4.4% (444/10000)
[Test]  Epoch: 24	Loss: 0.076053	Acc: 5.2% (522/10000)
[Test]  Epoch: 25	Loss: 0.074966	Acc: 5.5% (549/10000)
[Test]  Epoch: 26	Loss: 0.076200	Acc: 5.4% (538/10000)
[Test]  Epoch: 27	Loss: 0.075953	Acc: 5.2% (525/10000)
[Test]  Epoch: 28	Loss: 0.075580	Acc: 5.1% (514/10000)
[Test]  Epoch: 29	Loss: 0.075271	Acc: 5.0% (504/10000)
[Test]  Epoch: 30	Loss: 0.075816	Acc: 5.5% (546/10000)
[Test]  Epoch: 31	Loss: 0.075072	Acc: 5.5% (546/10000)
[Test]  Epoch: 32	Loss: 0.074298	Acc: 6.1% (607/10000)
[Test]  Epoch: 33	Loss: 0.075659	Acc: 5.7% (566/10000)
[Test]  Epoch: 34	Loss: 0.075415	Acc: 6.1% (611/10000)
[Test]  Epoch: 35	Loss: 0.075495	Acc: 5.8% (577/10000)
[Test]  Epoch: 36	Loss: 0.075694	Acc: 6.3% (626/10000)
[Test]  Epoch: 37	Loss: 0.075680	Acc: 5.5% (553/10000)
[Test]  Epoch: 38	Loss: 0.075047	Acc: 6.0% (600/10000)
[Test]  Epoch: 39	Loss: 0.074835	Acc: 6.0% (601/10000)
[Test]  Epoch: 40	Loss: 0.074535	Acc: 6.0% (600/10000)
[Test]  Epoch: 41	Loss: 0.074358	Acc: 6.2% (623/10000)
[Test]  Epoch: 42	Loss: 0.074450	Acc: 6.0% (602/10000)
[Test]  Epoch: 43	Loss: 0.074528	Acc: 6.2% (621/10000)
[Test]  Epoch: 44	Loss: 0.075038	Acc: 6.7% (667/10000)
[Test]  Epoch: 45	Loss: 0.075030	Acc: 6.1% (610/10000)
[Test]  Epoch: 46	Loss: 0.074681	Acc: 6.1% (611/10000)
[Test]  Epoch: 47	Loss: 0.074534	Acc: 6.3% (630/10000)
[Test]  Epoch: 48	Loss: 0.074803	Acc: 6.4% (643/10000)
[Test]  Epoch: 49	Loss: 0.074990	Acc: 6.5% (653/10000)
[Test]  Epoch: 50	Loss: 0.075692	Acc: 5.9% (590/10000)
[Test]  Epoch: 51	Loss: 0.075228	Acc: 6.1% (609/10000)
[Test]  Epoch: 52	Loss: 0.075144	Acc: 6.4% (643/10000)
[Test]  Epoch: 53	Loss: 0.074250	Acc: 6.8% (680/10000)
[Test]  Epoch: 54	Loss: 0.074345	Acc: 6.7% (669/10000)
[Test]  Epoch: 55	Loss: 0.075146	Acc: 6.2% (620/10000)
[Test]  Epoch: 56	Loss: 0.073976	Acc: 6.8% (682/10000)
[Test]  Epoch: 57	Loss: 0.075064	Acc: 6.1% (611/10000)
[Test]  Epoch: 58	Loss: 0.075319	Acc: 6.2% (622/10000)
[Test]  Epoch: 59	Loss: 0.074632	Acc: 6.6% (657/10000)
[Test]  Epoch: 60	Loss: 0.074612	Acc: 6.6% (661/10000)
[Test]  Epoch: 61	Loss: 0.073012	Acc: 7.2% (716/10000)
[Test]  Epoch: 62	Loss: 0.073010	Acc: 7.4% (741/10000)
[Test]  Epoch: 63	Loss: 0.072991	Acc: 7.4% (739/10000)
[Test]  Epoch: 64	Loss: 0.072730	Acc: 7.3% (731/10000)
[Test]  Epoch: 65	Loss: 0.072672	Acc: 7.5% (745/10000)
[Test]  Epoch: 66	Loss: 0.072709	Acc: 7.8% (782/10000)
[Test]  Epoch: 67	Loss: 0.072989	Acc: 7.8% (783/10000)
[Test]  Epoch: 68	Loss: 0.072653	Acc: 7.6% (761/10000)
[Test]  Epoch: 69	Loss: 0.072798	Acc: 7.4% (736/10000)
[Test]  Epoch: 70	Loss: 0.072472	Acc: 7.9% (790/10000)
[Test]  Epoch: 71	Loss: 0.072525	Acc: 7.4% (741/10000)
[Test]  Epoch: 72	Loss: 0.072388	Acc: 7.7% (769/10000)
[Test]  Epoch: 73	Loss: 0.072585	Acc: 7.8% (779/10000)
[Test]  Epoch: 74	Loss: 0.072782	Acc: 7.8% (778/10000)
[Test]  Epoch: 75	Loss: 0.072587	Acc: 7.8% (780/10000)
[Test]  Epoch: 76	Loss: 0.072535	Acc: 7.6% (756/10000)
[Test]  Epoch: 77	Loss: 0.072689	Acc: 7.5% (746/10000)
[Test]  Epoch: 78	Loss: 0.072610	Acc: 7.7% (772/10000)
[Test]  Epoch: 79	Loss: 0.072612	Acc: 7.7% (768/10000)
[Test]  Epoch: 80	Loss: 0.072139	Acc: 7.9% (788/10000)
[Test]  Epoch: 81	Loss: 0.072511	Acc: 7.9% (792/10000)
[Test]  Epoch: 82	Loss: 0.072496	Acc: 7.6% (760/10000)
[Test]  Epoch: 83	Loss: 0.072695	Acc: 7.7% (771/10000)
[Test]  Epoch: 84	Loss: 0.072674	Acc: 7.6% (762/10000)
[Test]  Epoch: 85	Loss: 0.072577	Acc: 7.5% (755/10000)
[Test]  Epoch: 86	Loss: 0.072802	Acc: 7.7% (769/10000)
[Test]  Epoch: 87	Loss: 0.072615	Acc: 7.6% (764/10000)
[Test]  Epoch: 88	Loss: 0.072375	Acc: 7.9% (789/10000)
[Test]  Epoch: 89	Loss: 0.072500	Acc: 7.6% (757/10000)
[Test]  Epoch: 90	Loss: 0.072289	Acc: 7.6% (760/10000)
[Test]  Epoch: 91	Loss: 0.072656	Acc: 7.8% (779/10000)
[Test]  Epoch: 92	Loss: 0.072262	Acc: 7.8% (785/10000)
[Test]  Epoch: 93	Loss: 0.072699	Acc: 7.7% (770/10000)
[Test]  Epoch: 94	Loss: 0.072601	Acc: 7.6% (757/10000)
[Test]  Epoch: 95	Loss: 0.072879	Acc: 7.7% (765/10000)
[Test]  Epoch: 96	Loss: 0.072979	Acc: 7.6% (756/10000)
[Test]  Epoch: 97	Loss: 0.072274	Acc: 8.0% (797/10000)
[Test]  Epoch: 98	Loss: 0.072706	Acc: 7.8% (778/10000)
[Test]  Epoch: 99	Loss: 0.072591	Acc: 7.6% (764/10000)
[Test]  Epoch: 100	Loss: 0.072678	Acc: 7.9% (788/10000)
===========finish==========
['2024-08-19', '08:00:06.403338', '100', 'test', '0.07267801821231842', '7.88', '7.97']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.195876	Acc: 0.6% (60/10000)
[Test]  Epoch: 2	Loss: 0.082869	Acc: 2.0% (201/10000)
[Test]  Epoch: 3	Loss: 0.080888	Acc: 2.3% (230/10000)
[Test]  Epoch: 4	Loss: 0.083750	Acc: 2.3% (230/10000)
[Test]  Epoch: 5	Loss: 0.079780	Acc: 2.6% (262/10000)
[Test]  Epoch: 6	Loss: 0.079873	Acc: 2.9% (291/10000)
[Test]  Epoch: 7	Loss: 0.078778	Acc: 3.3% (327/10000)
[Test]  Epoch: 8	Loss: 0.078441	Acc: 3.0% (300/10000)
[Test]  Epoch: 9	Loss: 0.077509	Acc: 3.2% (322/10000)
[Test]  Epoch: 10	Loss: 0.077735	Acc: 3.6% (360/10000)
[Test]  Epoch: 11	Loss: 0.078164	Acc: 3.4% (340/10000)
[Test]  Epoch: 12	Loss: 0.077066	Acc: 3.6% (357/10000)
[Test]  Epoch: 13	Loss: 0.077298	Acc: 3.8% (380/10000)
[Test]  Epoch: 14	Loss: 0.077498	Acc: 3.8% (377/10000)
[Test]  Epoch: 15	Loss: 0.077513	Acc: 3.9% (393/10000)
[Test]  Epoch: 16	Loss: 0.078233	Acc: 3.7% (372/10000)
[Test]  Epoch: 17	Loss: 0.077527	Acc: 4.1% (412/10000)
[Test]  Epoch: 18	Loss: 0.076880	Acc: 4.0% (404/10000)
[Test]  Epoch: 19	Loss: 0.076768	Acc: 4.3% (433/10000)
[Test]  Epoch: 20	Loss: 0.077043	Acc: 4.4% (441/10000)
[Test]  Epoch: 21	Loss: 0.076305	Acc: 4.8% (481/10000)
[Test]  Epoch: 22	Loss: 0.076304	Acc: 4.5% (445/10000)
[Test]  Epoch: 23	Loss: 0.077595	Acc: 4.3% (431/10000)
[Test]  Epoch: 24	Loss: 0.075712	Acc: 4.8% (476/10000)
[Test]  Epoch: 25	Loss: 0.077253	Acc: 4.4% (443/10000)
[Test]  Epoch: 26	Loss: 0.078069	Acc: 4.5% (449/10000)
[Test]  Epoch: 27	Loss: 0.076551	Acc: 4.8% (481/10000)
[Test]  Epoch: 28	Loss: 0.075289	Acc: 5.4% (541/10000)
[Test]  Epoch: 29	Loss: 0.076268	Acc: 5.0% (503/10000)
[Test]  Epoch: 30	Loss: 0.074943	Acc: 5.3% (533/10000)
[Test]  Epoch: 31	Loss: 0.076104	Acc: 4.9% (492/10000)
[Test]  Epoch: 32	Loss: 0.075422	Acc: 5.1% (513/10000)
[Test]  Epoch: 33	Loss: 0.076229	Acc: 4.9% (491/10000)
[Test]  Epoch: 34	Loss: 0.075959	Acc: 5.0% (505/10000)
[Test]  Epoch: 35	Loss: 0.076883	Acc: 4.8% (479/10000)
[Test]  Epoch: 36	Loss: 0.076061	Acc: 5.4% (543/10000)
[Test]  Epoch: 37	Loss: 0.076342	Acc: 5.0% (496/10000)
[Test]  Epoch: 38	Loss: 0.075601	Acc: 5.2% (521/10000)
[Test]  Epoch: 39	Loss: 0.075751	Acc: 5.5% (551/10000)
[Test]  Epoch: 40	Loss: 0.075606	Acc: 5.5% (555/10000)
[Test]  Epoch: 41	Loss: 0.075536	Acc: 5.4% (540/10000)
[Test]  Epoch: 42	Loss: 0.075840	Acc: 5.1% (511/10000)
[Test]  Epoch: 43	Loss: 0.075920	Acc: 5.4% (539/10000)
[Test]  Epoch: 44	Loss: 0.074604	Acc: 5.6% (556/10000)
[Test]  Epoch: 45	Loss: 0.075970	Acc: 5.4% (536/10000)
[Test]  Epoch: 46	Loss: 0.076303	Acc: 5.4% (536/10000)
[Test]  Epoch: 47	Loss: 0.075424	Acc: 5.7% (565/10000)
[Test]  Epoch: 48	Loss: 0.075148	Acc: 5.5% (553/10000)
[Test]  Epoch: 49	Loss: 0.076369	Acc: 5.7% (572/10000)
[Test]  Epoch: 50	Loss: 0.076137	Acc: 5.5% (547/10000)
[Test]  Epoch: 51	Loss: 0.076121	Acc: 5.7% (569/10000)
[Test]  Epoch: 52	Loss: 0.075378	Acc: 5.7% (565/10000)
[Test]  Epoch: 53	Loss: 0.075650	Acc: 5.7% (572/10000)
[Test]  Epoch: 54	Loss: 0.075236	Acc: 6.0% (595/10000)
[Test]  Epoch: 55	Loss: 0.076225	Acc: 5.9% (589/10000)
[Test]  Epoch: 56	Loss: 0.075716	Acc: 5.5% (554/10000)
[Test]  Epoch: 57	Loss: 0.077107	Acc: 5.6% (561/10000)
[Test]  Epoch: 58	Loss: 0.075377	Acc: 5.9% (593/10000)
[Test]  Epoch: 59	Loss: 0.075901	Acc: 5.4% (539/10000)
[Test]  Epoch: 60	Loss: 0.075419	Acc: 5.6% (561/10000)
[Test]  Epoch: 61	Loss: 0.074193	Acc: 6.5% (650/10000)
[Test]  Epoch: 62	Loss: 0.073863	Acc: 6.4% (638/10000)
[Test]  Epoch: 63	Loss: 0.074045	Acc: 6.7% (668/10000)
[Test]  Epoch: 64	Loss: 0.073851	Acc: 6.5% (650/10000)
[Test]  Epoch: 65	Loss: 0.073586	Acc: 6.6% (664/10000)
[Test]  Epoch: 66	Loss: 0.073562	Acc: 6.9% (692/10000)
[Test]  Epoch: 67	Loss: 0.073958	Acc: 6.4% (643/10000)
[Test]  Epoch: 68	Loss: 0.073664	Acc: 6.7% (671/10000)
[Test]  Epoch: 69	Loss: 0.073702	Acc: 6.8% (676/10000)
[Test]  Epoch: 70	Loss: 0.073401	Acc: 7.0% (701/10000)
[Test]  Epoch: 71	Loss: 0.073580	Acc: 6.5% (651/10000)
[Test]  Epoch: 72	Loss: 0.073858	Acc: 6.9% (687/10000)
[Test]  Epoch: 73	Loss: 0.073455	Acc: 7.2% (715/10000)
[Test]  Epoch: 74	Loss: 0.073952	Acc: 6.6% (658/10000)
[Test]  Epoch: 75	Loss: 0.073739	Acc: 6.5% (651/10000)
[Test]  Epoch: 76	Loss: 0.073569	Acc: 6.9% (689/10000)
[Test]  Epoch: 77	Loss: 0.073850	Acc: 6.8% (684/10000)
[Test]  Epoch: 78	Loss: 0.073968	Acc: 7.0% (700/10000)
[Test]  Epoch: 79	Loss: 0.073738	Acc: 7.2% (721/10000)
[Test]  Epoch: 80	Loss: 0.073330	Acc: 6.9% (694/10000)
[Test]  Epoch: 81	Loss: 0.073852	Acc: 6.7% (673/10000)
[Test]  Epoch: 82	Loss: 0.073689	Acc: 6.6% (664/10000)
[Test]  Epoch: 83	Loss: 0.073805	Acc: 6.8% (676/10000)
[Test]  Epoch: 84	Loss: 0.073724	Acc: 7.0% (699/10000)
[Test]  Epoch: 85	Loss: 0.073501	Acc: 6.8% (680/10000)
[Test]  Epoch: 86	Loss: 0.073765	Acc: 7.0% (700/10000)
[Test]  Epoch: 87	Loss: 0.073810	Acc: 7.0% (700/10000)
[Test]  Epoch: 88	Loss: 0.073441	Acc: 7.3% (732/10000)
[Test]  Epoch: 89	Loss: 0.073791	Acc: 6.7% (674/10000)
[Test]  Epoch: 90	Loss: 0.073593	Acc: 7.0% (704/10000)
[Test]  Epoch: 91	Loss: 0.073797	Acc: 7.1% (711/10000)
[Test]  Epoch: 92	Loss: 0.073612	Acc: 6.8% (684/10000)
[Test]  Epoch: 93	Loss: 0.073561	Acc: 7.0% (705/10000)
[Test]  Epoch: 94	Loss: 0.074093	Acc: 6.7% (670/10000)
[Test]  Epoch: 95	Loss: 0.073758	Acc: 6.8% (679/10000)
[Test]  Epoch: 96	Loss: 0.074037	Acc: 6.6% (663/10000)
[Test]  Epoch: 97	Loss: 0.073615	Acc: 6.8% (685/10000)
[Test]  Epoch: 98	Loss: 0.073879	Acc: 6.9% (694/10000)
[Test]  Epoch: 99	Loss: 0.073769	Acc: 6.8% (677/10000)
[Test]  Epoch: 100	Loss: 0.074135	Acc: 6.6% (663/10000)
===========finish==========
['2024-08-19', '08:03:19.367416', '100', 'test', '0.07413519968986511', '6.63', '7.32']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.276498	Acc: 0.5% (47/10000)
[Test]  Epoch: 2	Loss: 0.083478	Acc: 1.7% (174/10000)
[Test]  Epoch: 3	Loss: 0.082226	Acc: 2.1% (211/10000)
[Test]  Epoch: 4	Loss: 0.080360	Acc: 2.2% (221/10000)
[Test]  Epoch: 5	Loss: 0.079299	Acc: 2.6% (263/10000)
[Test]  Epoch: 6	Loss: 0.079929	Acc: 2.6% (260/10000)
[Test]  Epoch: 7	Loss: 0.079438	Acc: 2.9% (285/10000)
[Test]  Epoch: 8	Loss: 0.078795	Acc: 3.0% (303/10000)
[Test]  Epoch: 9	Loss: 0.078061	Acc: 3.4% (341/10000)
[Test]  Epoch: 10	Loss: 0.077863	Acc: 3.2% (319/10000)
[Test]  Epoch: 11	Loss: 0.077951	Acc: 3.3% (326/10000)
[Test]  Epoch: 12	Loss: 0.077462	Acc: 3.7% (366/10000)
[Test]  Epoch: 13	Loss: 0.078359	Acc: 3.5% (345/10000)
[Test]  Epoch: 14	Loss: 0.077931	Acc: 3.5% (355/10000)
[Test]  Epoch: 15	Loss: 0.077285	Acc: 3.7% (372/10000)
[Test]  Epoch: 16	Loss: 0.077807	Acc: 3.5% (353/10000)
[Test]  Epoch: 17	Loss: 0.077854	Acc: 4.1% (410/10000)
[Test]  Epoch: 18	Loss: 0.076773	Acc: 4.2% (425/10000)
[Test]  Epoch: 19	Loss: 0.076771	Acc: 4.2% (418/10000)
[Test]  Epoch: 20	Loss: 0.076274	Acc: 4.3% (426/10000)
[Test]  Epoch: 21	Loss: 0.076009	Acc: 4.5% (453/10000)
[Test]  Epoch: 22	Loss: 0.076397	Acc: 4.2% (420/10000)
[Test]  Epoch: 23	Loss: 0.076491	Acc: 4.3% (432/10000)
[Test]  Epoch: 24	Loss: 0.076751	Acc: 4.2% (417/10000)
[Test]  Epoch: 25	Loss: 0.076404	Acc: 4.3% (430/10000)
[Test]  Epoch: 26	Loss: 0.077835	Acc: 4.6% (460/10000)
[Test]  Epoch: 27	Loss: 0.076210	Acc: 4.7% (473/10000)
[Test]  Epoch: 28	Loss: 0.075709	Acc: 4.6% (464/10000)
[Test]  Epoch: 29	Loss: 0.076495	Acc: 4.4% (437/10000)
[Test]  Epoch: 30	Loss: 0.075911	Acc: 4.9% (490/10000)
[Test]  Epoch: 31	Loss: 0.076574	Acc: 4.5% (449/10000)
[Test]  Epoch: 32	Loss: 0.075193	Acc: 5.3% (534/10000)
[Test]  Epoch: 33	Loss: 0.075723	Acc: 4.8% (484/10000)
[Test]  Epoch: 34	Loss: 0.076763	Acc: 4.6% (462/10000)
[Test]  Epoch: 35	Loss: 0.077296	Acc: 4.3% (429/10000)
[Test]  Epoch: 36	Loss: 0.075470	Acc: 5.2% (516/10000)
[Test]  Epoch: 37	Loss: 0.075860	Acc: 5.2% (524/10000)
[Test]  Epoch: 38	Loss: 0.076253	Acc: 4.8% (475/10000)
[Test]  Epoch: 39	Loss: 0.075540	Acc: 5.4% (540/10000)
[Test]  Epoch: 40	Loss: 0.075538	Acc: 5.0% (497/10000)
[Test]  Epoch: 41	Loss: 0.074758	Acc: 5.4% (543/10000)
[Test]  Epoch: 42	Loss: 0.075385	Acc: 5.0% (500/10000)
[Test]  Epoch: 43	Loss: 0.075519	Acc: 5.3% (527/10000)
[Test]  Epoch: 44	Loss: 0.075732	Acc: 5.0% (502/10000)
[Test]  Epoch: 45	Loss: 0.076308	Acc: 4.8% (476/10000)
[Test]  Epoch: 46	Loss: 0.076189	Acc: 5.0% (499/10000)
[Test]  Epoch: 47	Loss: 0.075490	Acc: 5.5% (549/10000)
[Test]  Epoch: 48	Loss: 0.075560	Acc: 5.8% (585/10000)
[Test]  Epoch: 49	Loss: 0.076610	Acc: 5.3% (531/10000)
[Test]  Epoch: 50	Loss: 0.075471	Acc: 5.7% (565/10000)
[Test]  Epoch: 51	Loss: 0.075582	Acc: 5.7% (571/10000)
[Test]  Epoch: 52	Loss: 0.075622	Acc: 5.6% (564/10000)
[Test]  Epoch: 53	Loss: 0.075829	Acc: 5.3% (532/10000)
[Test]  Epoch: 54	Loss: 0.074665	Acc: 5.7% (574/10000)
[Test]  Epoch: 55	Loss: 0.075956	Acc: 5.8% (580/10000)
[Test]  Epoch: 56	Loss: 0.075629	Acc: 5.6% (562/10000)
[Test]  Epoch: 57	Loss: 0.077280	Acc: 5.2% (519/10000)
[Test]  Epoch: 58	Loss: 0.076105	Acc: 5.2% (521/10000)
[Test]  Epoch: 59	Loss: 0.075759	Acc: 5.6% (559/10000)
[Test]  Epoch: 60	Loss: 0.076075	Acc: 5.4% (544/10000)
[Test]  Epoch: 61	Loss: 0.074609	Acc: 6.0% (603/10000)
[Test]  Epoch: 62	Loss: 0.074093	Acc: 6.2% (621/10000)
[Test]  Epoch: 63	Loss: 0.074232	Acc: 6.2% (616/10000)
[Test]  Epoch: 64	Loss: 0.073999	Acc: 6.2% (623/10000)
[Test]  Epoch: 65	Loss: 0.074038	Acc: 6.3% (626/10000)
[Test]  Epoch: 66	Loss: 0.073860	Acc: 6.3% (635/10000)
[Test]  Epoch: 67	Loss: 0.074000	Acc: 6.3% (635/10000)
[Test]  Epoch: 68	Loss: 0.073929	Acc: 6.4% (636/10000)
[Test]  Epoch: 69	Loss: 0.073930	Acc: 6.5% (648/10000)
[Test]  Epoch: 70	Loss: 0.073656	Acc: 6.6% (664/10000)
[Test]  Epoch: 71	Loss: 0.073862	Acc: 6.3% (628/10000)
[Test]  Epoch: 72	Loss: 0.073872	Acc: 6.5% (649/10000)
[Test]  Epoch: 73	Loss: 0.073763	Acc: 6.8% (683/10000)
[Test]  Epoch: 74	Loss: 0.074083	Acc: 6.5% (655/10000)
[Test]  Epoch: 75	Loss: 0.073850	Acc: 6.4% (644/10000)
[Test]  Epoch: 76	Loss: 0.074096	Acc: 6.2% (616/10000)
[Test]  Epoch: 77	Loss: 0.074100	Acc: 6.2% (617/10000)
[Test]  Epoch: 78	Loss: 0.074068	Acc: 6.5% (654/10000)
[Test]  Epoch: 79	Loss: 0.074080	Acc: 6.5% (650/10000)
[Test]  Epoch: 80	Loss: 0.073525	Acc: 6.6% (664/10000)
[Test]  Epoch: 81	Loss: 0.073902	Acc: 6.5% (652/10000)
[Test]  Epoch: 82	Loss: 0.073795	Acc: 6.3% (626/10000)
[Test]  Epoch: 83	Loss: 0.074128	Acc: 6.3% (628/10000)
[Test]  Epoch: 84	Loss: 0.073905	Acc: 6.4% (639/10000)
[Test]  Epoch: 85	Loss: 0.073616	Acc: 7.0% (700/10000)
[Test]  Epoch: 86	Loss: 0.074010	Acc: 6.4% (638/10000)
[Test]  Epoch: 87	Loss: 0.074016	Acc: 6.3% (630/10000)
[Test]  Epoch: 88	Loss: 0.073693	Acc: 6.5% (655/10000)
[Test]  Epoch: 89	Loss: 0.073784	Acc: 6.3% (633/10000)
[Test]  Epoch: 90	Loss: 0.073743	Acc: 6.5% (647/10000)
[Test]  Epoch: 91	Loss: 0.074104	Acc: 6.6% (657/10000)
[Test]  Epoch: 92	Loss: 0.073862	Acc: 6.6% (661/10000)
[Test]  Epoch: 93	Loss: 0.074041	Acc: 6.7% (671/10000)
[Test]  Epoch: 94	Loss: 0.073975	Acc: 6.7% (673/10000)
[Test]  Epoch: 95	Loss: 0.074211	Acc: 6.7% (669/10000)
[Test]  Epoch: 96	Loss: 0.074259	Acc: 6.5% (645/10000)
[Test]  Epoch: 97	Loss: 0.073721	Acc: 6.8% (680/10000)
[Test]  Epoch: 98	Loss: 0.074352	Acc: 6.3% (626/10000)
[Test]  Epoch: 99	Loss: 0.074140	Acc: 6.5% (655/10000)
[Test]  Epoch: 100	Loss: 0.074183	Acc: 6.6% (656/10000)
===========finish==========
['2024-08-19', '08:06:38.943082', '100', 'test', '0.07418253765106202', '6.56', '7.0']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight', 'features.40.weight', 'features.41.weight', 'classifier.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.109279	Acc: 0.5% (53/10000)
[Test]  Epoch: 2	Loss: 0.083713	Acc: 0.7% (67/10000)
[Test]  Epoch: 3	Loss: 0.083438	Acc: 0.8% (82/10000)
[Test]  Epoch: 4	Loss: 0.083263	Acc: 1.0% (96/10000)
[Test]  Epoch: 5	Loss: 0.083137	Acc: 1.0% (104/10000)
[Test]  Epoch: 6	Loss: 0.083007	Acc: 1.1% (114/10000)
[Test]  Epoch: 7	Loss: 0.082641	Acc: 1.6% (159/10000)
[Test]  Epoch: 8	Loss: 0.082561	Acc: 1.6% (157/10000)
[Test]  Epoch: 9	Loss: 0.082092	Acc: 1.8% (175/10000)
[Test]  Epoch: 10	Loss: 0.081600	Acc: 2.2% (217/10000)
[Test]  Epoch: 11	Loss: 0.081428	Acc: 2.2% (218/10000)
[Test]  Epoch: 12	Loss: 0.081274	Acc: 2.0% (197/10000)
[Test]  Epoch: 13	Loss: 0.080874	Acc: 2.4% (242/10000)
[Test]  Epoch: 14	Loss: 0.080565	Acc: 2.5% (247/10000)
[Test]  Epoch: 15	Loss: 0.080331	Acc: 2.5% (250/10000)
[Test]  Epoch: 16	Loss: 0.080137	Acc: 2.4% (235/10000)
[Test]  Epoch: 17	Loss: 0.079888	Acc: 2.9% (292/10000)
[Test]  Epoch: 18	Loss: 0.079676	Acc: 2.6% (265/10000)
[Test]  Epoch: 19	Loss: 0.079300	Acc: 3.0% (303/10000)
[Test]  Epoch: 20	Loss: 0.079011	Acc: 2.9% (286/10000)
[Test]  Epoch: 21	Loss: 0.078933	Acc: 3.0% (299/10000)
[Test]  Epoch: 22	Loss: 0.078890	Acc: 3.2% (318/10000)
[Test]  Epoch: 23	Loss: 0.078721	Acc: 2.9% (288/10000)
[Test]  Epoch: 24	Loss: 0.078452	Acc: 3.0% (300/10000)
[Test]  Epoch: 25	Loss: 0.078153	Acc: 3.2% (320/10000)
[Test]  Epoch: 26	Loss: 0.078046	Acc: 3.4% (335/10000)
[Test]  Epoch: 27	Loss: 0.077922	Acc: 3.2% (322/10000)
[Test]  Epoch: 28	Loss: 0.077705	Acc: 3.4% (343/10000)
[Test]  Epoch: 29	Loss: 0.077903	Acc: 3.3% (334/10000)
[Test]  Epoch: 30	Loss: 0.077623	Acc: 3.5% (354/10000)
[Test]  Epoch: 31	Loss: 0.077542	Acc: 3.6% (362/10000)
[Test]  Epoch: 32	Loss: 0.077432	Acc: 3.8% (375/10000)
[Test]  Epoch: 33	Loss: 0.077444	Acc: 3.6% (361/10000)
[Test]  Epoch: 34	Loss: 0.077885	Acc: 3.4% (341/10000)
[Test]  Epoch: 35	Loss: 0.077420	Acc: 3.5% (353/10000)
[Test]  Epoch: 36	Loss: 0.076968	Acc: 3.8% (383/10000)
[Test]  Epoch: 37	Loss: 0.076946	Acc: 3.9% (386/10000)
[Test]  Epoch: 38	Loss: 0.077414	Acc: 3.8% (378/10000)
[Test]  Epoch: 39	Loss: 0.076862	Acc: 3.8% (381/10000)
[Test]  Epoch: 40	Loss: 0.076528	Acc: 4.2% (421/10000)
[Test]  Epoch: 41	Loss: 0.076722	Acc: 3.7% (372/10000)
[Test]  Epoch: 42	Loss: 0.077667	Acc: 3.6% (356/10000)
[Test]  Epoch: 43	Loss: 0.076512	Acc: 3.7% (371/10000)
[Test]  Epoch: 44	Loss: 0.076289	Acc: 4.1% (406/10000)
[Test]  Epoch: 45	Loss: 0.076928	Acc: 3.6% (356/10000)
[Test]  Epoch: 46	Loss: 0.076638	Acc: 3.9% (389/10000)
[Test]  Epoch: 47	Loss: 0.076429	Acc: 4.0% (402/10000)
[Test]  Epoch: 48	Loss: 0.076212	Acc: 4.0% (404/10000)
[Test]  Epoch: 49	Loss: 0.076482	Acc: 4.4% (436/10000)
[Test]  Epoch: 50	Loss: 0.075837	Acc: 4.3% (433/10000)
[Test]  Epoch: 51	Loss: 0.076870	Acc: 4.0% (400/10000)
[Test]  Epoch: 52	Loss: 0.076212	Acc: 4.7% (466/10000)
[Test]  Epoch: 53	Loss: 0.076008	Acc: 4.0% (400/10000)
[Test]  Epoch: 54	Loss: 0.076055	Acc: 4.2% (420/10000)
[Test]  Epoch: 55	Loss: 0.075508	Acc: 4.4% (440/10000)
[Test]  Epoch: 56	Loss: 0.075574	Acc: 4.7% (465/10000)
[Test]  Epoch: 57	Loss: 0.076118	Acc: 4.2% (425/10000)
[Test]  Epoch: 58	Loss: 0.075679	Acc: 4.3% (431/10000)
[Test]  Epoch: 59	Loss: 0.075914	Acc: 4.5% (446/10000)
[Test]  Epoch: 60	Loss: 0.077587	Acc: 4.0% (401/10000)
[Test]  Epoch: 61	Loss: 0.075111	Acc: 4.6% (459/10000)
[Test]  Epoch: 62	Loss: 0.074907	Acc: 4.9% (493/10000)
[Test]  Epoch: 63	Loss: 0.075088	Acc: 4.7% (473/10000)
[Test]  Epoch: 64	Loss: 0.074858	Acc: 5.1% (508/10000)
[Test]  Epoch: 65	Loss: 0.074871	Acc: 5.2% (519/10000)
[Test]  Epoch: 66	Loss: 0.074969	Acc: 4.9% (488/10000)
[Test]  Epoch: 67	Loss: 0.075070	Acc: 4.7% (465/10000)
[Test]  Epoch: 68	Loss: 0.074806	Acc: 4.8% (479/10000)
[Test]  Epoch: 69	Loss: 0.075016	Acc: 4.7% (470/10000)
[Test]  Epoch: 70	Loss: 0.074725	Acc: 5.1% (514/10000)
[Test]  Epoch: 71	Loss: 0.074808	Acc: 4.8% (483/10000)
[Test]  Epoch: 72	Loss: 0.074768	Acc: 5.1% (507/10000)
[Test]  Epoch: 73	Loss: 0.074963	Acc: 4.9% (490/10000)
[Test]  Epoch: 74	Loss: 0.074793	Acc: 4.8% (481/10000)
[Test]  Epoch: 75	Loss: 0.074825	Acc: 4.8% (475/10000)
[Test]  Epoch: 76	Loss: 0.074701	Acc: 4.8% (480/10000)
[Test]  Epoch: 77	Loss: 0.074763	Acc: 5.1% (512/10000)
[Test]  Epoch: 78	Loss: 0.074822	Acc: 5.2% (521/10000)
[Test]  Epoch: 79	Loss: 0.074685	Acc: 5.2% (516/10000)
[Test]  Epoch: 80	Loss: 0.074537	Acc: 5.2% (519/10000)
[Test]  Epoch: 81	Loss: 0.074779	Acc: 5.0% (499/10000)
[Test]  Epoch: 82	Loss: 0.074709	Acc: 5.2% (517/10000)
[Test]  Epoch: 83	Loss: 0.074830	Acc: 4.8% (478/10000)
[Test]  Epoch: 84	Loss: 0.074810	Acc: 5.0% (504/10000)
[Test]  Epoch: 85	Loss: 0.074576	Acc: 5.3% (527/10000)
[Test]  Epoch: 86	Loss: 0.074635	Acc: 5.0% (502/10000)
[Test]  Epoch: 87	Loss: 0.074782	Acc: 4.8% (485/10000)
[Test]  Epoch: 88	Loss: 0.074515	Acc: 5.0% (501/10000)
[Test]  Epoch: 89	Loss: 0.074549	Acc: 4.8% (484/10000)
[Test]  Epoch: 90	Loss: 0.074555	Acc: 5.2% (519/10000)
[Test]  Epoch: 91	Loss: 0.074481	Acc: 5.1% (511/10000)
[Test]  Epoch: 92	Loss: 0.074592	Acc: 5.0% (504/10000)
[Test]  Epoch: 93	Loss: 0.074612	Acc: 5.1% (510/10000)
[Test]  Epoch: 94	Loss: 0.074606	Acc: 5.0% (504/10000)
[Test]  Epoch: 95	Loss: 0.074541	Acc: 5.2% (520/10000)
[Test]  Epoch: 96	Loss: 0.074738	Acc: 5.0% (502/10000)
[Test]  Epoch: 97	Loss: 0.074334	Acc: 5.3% (526/10000)
[Test]  Epoch: 98	Loss: 0.074695	Acc: 5.1% (511/10000)
[Test]  Epoch: 99	Loss: 0.074538	Acc: 5.3% (529/10000)
[Test]  Epoch: 100	Loss: 0.074702	Acc: 5.2% (515/10000)
===========finish==========
['2024-08-19', '08:09:52.477024', '100', 'test', '0.07470203490257263', '5.15', '5.29']
result path:  /home/gpu2/jbw/other_XAI_grad/knockoffnets/ms_elastictrainer_result_resnet50.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/cifar100-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('conv1.weight', 0.0), ('bn1.weight', 0.0), ('bn1.bias', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.bn1.weight', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.bn2.weight', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.conv3.weight', 0.0), ('layer1.0.bn3.weight', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.0.weight', 0.0), ('layer1.0.downsample.1.weight', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.bn1.weight', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.bn2.weight', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.conv3.weight', 0.0), ('layer1.1.bn3.weight', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.conv1.weight', 0.0), ('layer1.2.bn1.weight', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.conv2.weight', 0.0), ('layer1.2.bn2.weight', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.conv3.weight', 0.0), ('layer1.2.bn3.weight', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.bn1.weight', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.bn2.weight', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.conv3.weight', 0.0), ('layer2.0.bn3.weight', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.0.weight', 0.0), ('layer2.0.downsample.1.weight', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.bn1.weight', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.bn2.weight', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.conv3.weight', 0.0), ('layer2.1.bn3.weight', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.conv1.weight', 0.0), ('layer2.2.bn1.weight', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.conv2.weight', 0.0), ('layer2.2.bn2.weight', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.conv3.weight', 0.0), ('layer2.2.bn3.weight', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.conv1.weight', 0.0), ('layer2.3.bn1.weight', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.conv2.weight', 0.0), ('layer2.3.bn2.weight', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.conv3.weight', 0.0), ('layer2.3.bn3.weight', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.bn1.weight', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.bn2.weight', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.conv3.weight', 0.0), ('layer3.0.bn3.weight', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.0.weight', 0.0), ('layer3.0.downsample.1.weight', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.bn1.weight', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.bn2.weight', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.conv3.weight', 0.0), ('layer3.1.bn3.weight', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.conv1.weight', 0.0), ('layer3.2.bn1.weight', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.conv2.weight', 0.0), ('layer3.2.bn2.weight', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.conv3.weight', 0.0), ('layer3.2.bn3.weight', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.conv1.weight', 0.0), ('layer3.3.bn1.weight', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.conv2.weight', 0.0), ('layer3.3.bn2.weight', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.conv3.weight', 0.0), ('layer3.3.bn3.weight', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.conv1.weight', 0.0), ('layer3.4.bn1.weight', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.conv2.weight', 0.0), ('layer3.4.bn2.weight', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.conv3.weight', 0.0), ('layer3.4.bn3.weight', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.conv1.weight', 0.0), ('layer3.5.bn1.weight', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.conv2.weight', 0.0), ('layer3.5.bn2.weight', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.conv3.weight', 0.0), ('layer3.5.bn3.weight', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.bn1.weight', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.bn2.weight', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.conv3.weight', 0.0), ('layer4.0.bn3.weight', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.0.weight', 0.0), ('layer4.0.downsample.1.weight', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.bn1.weight', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.bn2.weight', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.conv3.weight', 0.0), ('layer4.1.bn3.weight', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.conv1.weight', 0.0), ('layer4.2.bn1.weight', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.conv2.weight', 0.0), ('layer4.2.bn2.weight', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.conv3.weight', 0.0), ('layer4.2.bn3.weight', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('conv1.weight', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.conv3.weight', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.conv3.weight', 0.0), ('layer1.2.conv1.weight', 0.0), ('layer1.2.conv2.weight', 0.0), ('layer1.2.conv3.weight', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.conv3.weight', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.conv3.weight', 0.0), ('layer2.2.conv1.weight', 0.0), ('layer2.2.conv2.weight', 0.0), ('layer2.2.conv3.weight', 0.0), ('layer2.3.conv1.weight', 0.0), ('layer2.3.conv2.weight', 0.0), ('layer2.3.conv3.weight', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.conv3.weight', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.conv3.weight', 0.0), ('layer3.2.conv1.weight', 0.0), ('layer3.2.conv2.weight', 0.0), ('layer3.2.conv3.weight', 0.0), ('layer3.3.conv1.weight', 0.0), ('layer3.3.conv2.weight', 0.0), ('layer3.3.conv3.weight', 0.0), ('layer3.4.conv1.weight', 0.0), ('layer3.4.conv2.weight', 0.0), ('layer3.4.conv3.weight', 0.0), ('layer3.5.conv1.weight', 0.0), ('layer3.5.conv2.weight', 0.0), ('layer3.5.conv3.weight', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.conv3.weight', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.conv3.weight', 0.0), ('layer4.2.conv1.weight', 0.0), ('layer4.2.conv2.weight', 0.0), ('layer4.2.conv3.weight', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.026460	Acc: 63.2% (6320/10000)
[Test]  Epoch: 2	Loss: 0.026499	Acc: 63.0% (6301/10000)
[Test]  Epoch: 3	Loss: 0.026499	Acc: 62.8% (6281/10000)
[Test]  Epoch: 4	Loss: 0.026596	Acc: 62.8% (6279/10000)
[Test]  Epoch: 5	Loss: 0.026559	Acc: 62.9% (6285/10000)
[Test]  Epoch: 6	Loss: 0.026499	Acc: 62.7% (6274/10000)
[Test]  Epoch: 7	Loss: 0.026498	Acc: 62.6% (6260/10000)
[Test]  Epoch: 8	Loss: 0.026460	Acc: 62.9% (6290/10000)
[Test]  Epoch: 9	Loss: 0.026413	Acc: 62.8% (6282/10000)
[Test]  Epoch: 10	Loss: 0.026344	Acc: 62.9% (6293/10000)
[Test]  Epoch: 11	Loss: 0.026257	Acc: 62.9% (6290/10000)
[Test]  Epoch: 12	Loss: 0.026265	Acc: 63.0% (6298/10000)
[Test]  Epoch: 13	Loss: 0.026241	Acc: 63.1% (6307/10000)
[Test]  Epoch: 14	Loss: 0.026251	Acc: 63.0% (6298/10000)
[Test]  Epoch: 15	Loss: 0.026195	Acc: 63.2% (6317/10000)
[Test]  Epoch: 16	Loss: 0.026248	Acc: 63.2% (6317/10000)
[Test]  Epoch: 17	Loss: 0.026106	Acc: 63.2% (6318/10000)
[Test]  Epoch: 18	Loss: 0.026178	Acc: 63.1% (6306/10000)
[Test]  Epoch: 19	Loss: 0.026086	Acc: 63.1% (6314/10000)
[Test]  Epoch: 20	Loss: 0.025991	Acc: 63.4% (6338/10000)
[Test]  Epoch: 21	Loss: 0.026055	Acc: 63.3% (6326/10000)
[Test]  Epoch: 22	Loss: 0.025977	Acc: 63.3% (6332/10000)
[Test]  Epoch: 23	Loss: 0.026057	Acc: 63.1% (6312/10000)
[Test]  Epoch: 24	Loss: 0.026035	Acc: 63.1% (6311/10000)
[Test]  Epoch: 25	Loss: 0.026033	Acc: 63.4% (6343/10000)
[Test]  Epoch: 26	Loss: 0.026056	Acc: 63.1% (6315/10000)
[Test]  Epoch: 27	Loss: 0.026142	Acc: 63.1% (6307/10000)
[Test]  Epoch: 28	Loss: 0.026002	Acc: 63.1% (6315/10000)
[Test]  Epoch: 29	Loss: 0.026004	Acc: 63.3% (6330/10000)
[Test]  Epoch: 30	Loss: 0.026001	Acc: 63.3% (6332/10000)
[Test]  Epoch: 31	Loss: 0.025959	Acc: 63.3% (6334/10000)
[Test]  Epoch: 32	Loss: 0.025959	Acc: 63.4% (6340/10000)
[Test]  Epoch: 33	Loss: 0.025982	Acc: 63.3% (6329/10000)
[Test]  Epoch: 34	Loss: 0.026017	Acc: 63.1% (6314/10000)
[Test]  Epoch: 35	Loss: 0.025911	Acc: 63.4% (6335/10000)
[Test]  Epoch: 36	Loss: 0.025895	Acc: 63.3% (6333/10000)
[Test]  Epoch: 37	Loss: 0.025902	Acc: 63.2% (6320/10000)
[Test]  Epoch: 38	Loss: 0.025982	Acc: 63.2% (6321/10000)
[Test]  Epoch: 39	Loss: 0.025996	Acc: 63.3% (6326/10000)
[Test]  Epoch: 40	Loss: 0.025909	Acc: 63.2% (6320/10000)
[Test]  Epoch: 41	Loss: 0.025878	Acc: 63.3% (6326/10000)
[Test]  Epoch: 42	Loss: 0.025838	Acc: 63.3% (6327/10000)
[Test]  Epoch: 43	Loss: 0.025933	Acc: 63.7% (6372/10000)
[Test]  Epoch: 44	Loss: 0.025926	Acc: 63.5% (6346/10000)
[Test]  Epoch: 45	Loss: 0.025816	Acc: 63.5% (6351/10000)
[Test]  Epoch: 46	Loss: 0.025778	Acc: 63.4% (6337/10000)
[Test]  Epoch: 47	Loss: 0.025894	Acc: 63.4% (6336/10000)
[Test]  Epoch: 48	Loss: 0.025843	Acc: 63.5% (6345/10000)
[Test]  Epoch: 49	Loss: 0.025822	Acc: 63.4% (6338/10000)
[Test]  Epoch: 50	Loss: 0.025815	Acc: 63.3% (6334/10000)
[Test]  Epoch: 51	Loss: 0.025923	Acc: 63.2% (6320/10000)
[Test]  Epoch: 52	Loss: 0.025805	Acc: 63.4% (6342/10000)
[Test]  Epoch: 53	Loss: 0.025768	Acc: 63.5% (6355/10000)
[Test]  Epoch: 54	Loss: 0.025829	Acc: 63.4% (6341/10000)
[Test]  Epoch: 55	Loss: 0.025740	Acc: 63.5% (6345/10000)
[Test]  Epoch: 56	Loss: 0.025837	Acc: 63.2% (6320/10000)
[Test]  Epoch: 57	Loss: 0.025828	Acc: 63.4% (6338/10000)
[Test]  Epoch: 58	Loss: 0.025737	Acc: 63.6% (6360/10000)
[Test]  Epoch: 59	Loss: 0.025809	Acc: 63.2% (6325/10000)
[Test]  Epoch: 60	Loss: 0.025817	Acc: 63.3% (6330/10000)
[Test]  Epoch: 61	Loss: 0.025827	Acc: 63.4% (6341/10000)
[Test]  Epoch: 62	Loss: 0.025837	Acc: 63.3% (6332/10000)
[Test]  Epoch: 63	Loss: 0.025775	Acc: 63.5% (6349/10000)
[Test]  Epoch: 64	Loss: 0.025776	Acc: 63.4% (6338/10000)
[Test]  Epoch: 65	Loss: 0.025704	Acc: 63.3% (6334/10000)
[Test]  Epoch: 66	Loss: 0.025776	Acc: 63.5% (6345/10000)
[Test]  Epoch: 67	Loss: 0.025820	Acc: 63.2% (6325/10000)
[Test]  Epoch: 68	Loss: 0.025748	Acc: 63.4% (6340/10000)
[Test]  Epoch: 69	Loss: 0.025778	Acc: 63.5% (6346/10000)
[Test]  Epoch: 70	Loss: 0.025739	Acc: 63.4% (6342/10000)
[Test]  Epoch: 71	Loss: 0.025752	Acc: 63.6% (6361/10000)
[Test]  Epoch: 72	Loss: 0.025714	Acc: 63.4% (6338/10000)
[Test]  Epoch: 73	Loss: 0.025754	Acc: 63.5% (6354/10000)
[Test]  Epoch: 74	Loss: 0.025746	Acc: 63.4% (6343/10000)
[Test]  Epoch: 75	Loss: 0.025687	Acc: 63.5% (6354/10000)
[Test]  Epoch: 76	Loss: 0.025807	Acc: 63.2% (6324/10000)
[Test]  Epoch: 77	Loss: 0.025840	Acc: 63.2% (6317/10000)
[Test]  Epoch: 78	Loss: 0.025762	Acc: 63.4% (6340/10000)
[Test]  Epoch: 79	Loss: 0.025833	Acc: 63.4% (6340/10000)
[Test]  Epoch: 80	Loss: 0.025775	Acc: 63.5% (6348/10000)
[Test]  Epoch: 81	Loss: 0.025784	Acc: 63.3% (6329/10000)
[Test]  Epoch: 82	Loss: 0.025781	Acc: 63.2% (6325/10000)
[Test]  Epoch: 83	Loss: 0.025753	Acc: 63.2% (6318/10000)
[Test]  Epoch: 84	Loss: 0.025701	Acc: 63.4% (6340/10000)
[Test]  Epoch: 85	Loss: 0.025785	Acc: 63.5% (6350/10000)
[Test]  Epoch: 86	Loss: 0.025795	Acc: 63.5% (6351/10000)
[Test]  Epoch: 87	Loss: 0.025744	Acc: 63.6% (6356/10000)
[Test]  Epoch: 88	Loss: 0.025812	Acc: 63.4% (6344/10000)
[Test]  Epoch: 89	Loss: 0.025766	Acc: 63.4% (6341/10000)
[Test]  Epoch: 90	Loss: 0.025691	Acc: 63.6% (6361/10000)
[Test]  Epoch: 91	Loss: 0.025700	Acc: 63.4% (6339/10000)
[Test]  Epoch: 92	Loss: 0.025814	Acc: 63.3% (6329/10000)
[Test]  Epoch: 93	Loss: 0.025716	Acc: 63.6% (6357/10000)
[Test]  Epoch: 94	Loss: 0.025696	Acc: 63.5% (6349/10000)
[Test]  Epoch: 95	Loss: 0.025794	Acc: 63.3% (6331/10000)
[Test]  Epoch: 96	Loss: 0.025751	Acc: 63.5% (6348/10000)
[Test]  Epoch: 97	Loss: 0.025740	Acc: 63.5% (6346/10000)
[Test]  Epoch: 98	Loss: 0.025722	Acc: 63.6% (6358/10000)
[Test]  Epoch: 99	Loss: 0.025802	Acc: 63.4% (6342/10000)
[Test]  Epoch: 100	Loss: 0.025788	Acc: 63.4% (6337/10000)
===========finish==========
['2024-08-19', '16:20:48.923450', '100', 'test', '0.025787652403116226', '63.37', '63.72']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.032650	Acc: 56.3% (5632/10000)
[Test]  Epoch: 2	Loss: 0.029375	Acc: 58.9% (5890/10000)
[Test]  Epoch: 3	Loss: 0.028820	Acc: 59.0% (5901/10000)
[Test]  Epoch: 4	Loss: 0.028693	Acc: 59.4% (5936/10000)
[Test]  Epoch: 5	Loss: 0.028671	Acc: 59.5% (5954/10000)
[Test]  Epoch: 6	Loss: 0.028610	Acc: 59.3% (5932/10000)
[Test]  Epoch: 7	Loss: 0.028574	Acc: 59.5% (5954/10000)
[Test]  Epoch: 8	Loss: 0.028530	Acc: 59.5% (5955/10000)
[Test]  Epoch: 9	Loss: 0.028455	Acc: 59.5% (5947/10000)
[Test]  Epoch: 10	Loss: 0.028347	Acc: 59.5% (5955/10000)
[Test]  Epoch: 11	Loss: 0.028290	Acc: 59.6% (5962/10000)
[Test]  Epoch: 12	Loss: 0.028346	Acc: 59.8% (5977/10000)
[Test]  Epoch: 13	Loss: 0.028310	Acc: 59.8% (5976/10000)
[Test]  Epoch: 14	Loss: 0.028313	Acc: 59.8% (5984/10000)
[Test]  Epoch: 15	Loss: 0.028261	Acc: 60.1% (6007/10000)
[Test]  Epoch: 16	Loss: 0.028376	Acc: 59.9% (5990/10000)
[Test]  Epoch: 17	Loss: 0.028225	Acc: 59.9% (5992/10000)
[Test]  Epoch: 18	Loss: 0.028272	Acc: 59.8% (5981/10000)
[Test]  Epoch: 19	Loss: 0.028188	Acc: 59.9% (5991/10000)
[Test]  Epoch: 20	Loss: 0.028098	Acc: 60.1% (6013/10000)
[Test]  Epoch: 21	Loss: 0.028092	Acc: 59.9% (5994/10000)
[Test]  Epoch: 22	Loss: 0.028057	Acc: 60.0% (5997/10000)
[Test]  Epoch: 23	Loss: 0.028105	Acc: 60.0% (6003/10000)
[Test]  Epoch: 24	Loss: 0.028110	Acc: 59.9% (5991/10000)
[Test]  Epoch: 25	Loss: 0.028078	Acc: 60.1% (6014/10000)
[Test]  Epoch: 26	Loss: 0.028036	Acc: 60.2% (6024/10000)
[Test]  Epoch: 27	Loss: 0.028031	Acc: 60.2% (6018/10000)
[Test]  Epoch: 28	Loss: 0.027993	Acc: 60.0% (6002/10000)
[Test]  Epoch: 29	Loss: 0.027999	Acc: 60.2% (6020/10000)
[Test]  Epoch: 30	Loss: 0.028006	Acc: 60.0% (6005/10000)
[Test]  Epoch: 31	Loss: 0.027969	Acc: 60.0% (5996/10000)
[Test]  Epoch: 32	Loss: 0.027998	Acc: 60.1% (6011/10000)
[Test]  Epoch: 33	Loss: 0.028012	Acc: 59.8% (5984/10000)
[Test]  Epoch: 34	Loss: 0.028032	Acc: 60.0% (6002/10000)
[Test]  Epoch: 35	Loss: 0.028004	Acc: 60.2% (6017/10000)
[Test]  Epoch: 36	Loss: 0.027943	Acc: 60.0% (6005/10000)
[Test]  Epoch: 37	Loss: 0.027931	Acc: 60.2% (6019/10000)
[Test]  Epoch: 38	Loss: 0.027889	Acc: 60.3% (6033/10000)
[Test]  Epoch: 39	Loss: 0.028004	Acc: 60.2% (6024/10000)
[Test]  Epoch: 40	Loss: 0.027949	Acc: 60.2% (6018/10000)
[Test]  Epoch: 41	Loss: 0.027895	Acc: 60.2% (6021/10000)
[Test]  Epoch: 42	Loss: 0.027825	Acc: 60.3% (6032/10000)
[Test]  Epoch: 43	Loss: 0.027974	Acc: 60.3% (6030/10000)
[Test]  Epoch: 44	Loss: 0.027956	Acc: 60.0% (6003/10000)
[Test]  Epoch: 45	Loss: 0.027817	Acc: 60.3% (6031/10000)
[Test]  Epoch: 46	Loss: 0.027829	Acc: 60.2% (6021/10000)
[Test]  Epoch: 47	Loss: 0.027873	Acc: 60.3% (6030/10000)
[Test]  Epoch: 48	Loss: 0.027862	Acc: 60.2% (6022/10000)
[Test]  Epoch: 49	Loss: 0.027893	Acc: 60.1% (6007/10000)
[Test]  Epoch: 50	Loss: 0.027810	Acc: 60.4% (6041/10000)
[Test]  Epoch: 51	Loss: 0.027921	Acc: 60.2% (6020/10000)
[Test]  Epoch: 52	Loss: 0.027748	Acc: 60.2% (6019/10000)
[Test]  Epoch: 53	Loss: 0.027772	Acc: 60.3% (6026/10000)
[Test]  Epoch: 54	Loss: 0.027818	Acc: 60.2% (6023/10000)
[Test]  Epoch: 55	Loss: 0.027684	Acc: 60.4% (6038/10000)
[Test]  Epoch: 56	Loss: 0.027733	Acc: 60.3% (6030/10000)
[Test]  Epoch: 57	Loss: 0.027788	Acc: 60.2% (6023/10000)
[Test]  Epoch: 58	Loss: 0.027731	Acc: 60.3% (6034/10000)
[Test]  Epoch: 59	Loss: 0.027711	Acc: 60.4% (6035/10000)
[Test]  Epoch: 60	Loss: 0.027755	Acc: 60.2% (6023/10000)
[Test]  Epoch: 61	Loss: 0.027807	Acc: 60.3% (6030/10000)
[Test]  Epoch: 62	Loss: 0.027805	Acc: 60.4% (6036/10000)
[Test]  Epoch: 63	Loss: 0.027739	Acc: 60.4% (6039/10000)
[Test]  Epoch: 64	Loss: 0.027748	Acc: 60.4% (6037/10000)
[Test]  Epoch: 65	Loss: 0.027712	Acc: 60.3% (6029/10000)
[Test]  Epoch: 66	Loss: 0.027703	Acc: 60.4% (6040/10000)
[Test]  Epoch: 67	Loss: 0.027796	Acc: 60.4% (6041/10000)
[Test]  Epoch: 68	Loss: 0.027720	Acc: 60.4% (6037/10000)
[Test]  Epoch: 69	Loss: 0.027755	Acc: 60.3% (6034/10000)
[Test]  Epoch: 70	Loss: 0.027671	Acc: 60.4% (6036/10000)
[Test]  Epoch: 71	Loss: 0.027696	Acc: 60.5% (6049/10000)
[Test]  Epoch: 72	Loss: 0.027716	Acc: 60.4% (6044/10000)
[Test]  Epoch: 73	Loss: 0.027663	Acc: 60.4% (6042/10000)
[Test]  Epoch: 74	Loss: 0.027712	Acc: 60.3% (6027/10000)
[Test]  Epoch: 75	Loss: 0.027715	Acc: 60.4% (6041/10000)
[Test]  Epoch: 76	Loss: 0.027705	Acc: 60.2% (6023/10000)
[Test]  Epoch: 77	Loss: 0.027772	Acc: 60.3% (6027/10000)
[Test]  Epoch: 78	Loss: 0.027690	Acc: 60.4% (6044/10000)
[Test]  Epoch: 79	Loss: 0.027773	Acc: 60.4% (6038/10000)
[Test]  Epoch: 80	Loss: 0.027751	Acc: 60.5% (6046/10000)
[Test]  Epoch: 81	Loss: 0.027756	Acc: 60.1% (6014/10000)
[Test]  Epoch: 82	Loss: 0.027722	Acc: 60.3% (6028/10000)
[Test]  Epoch: 83	Loss: 0.027766	Acc: 60.3% (6026/10000)
[Test]  Epoch: 84	Loss: 0.027670	Acc: 60.2% (6025/10000)
[Test]  Epoch: 85	Loss: 0.027708	Acc: 60.5% (6054/10000)
[Test]  Epoch: 86	Loss: 0.027754	Acc: 60.4% (6039/10000)
[Test]  Epoch: 87	Loss: 0.027692	Acc: 60.3% (6032/10000)
[Test]  Epoch: 88	Loss: 0.027779	Acc: 60.3% (6026/10000)
[Test]  Epoch: 89	Loss: 0.027767	Acc: 60.2% (6021/10000)
[Test]  Epoch: 90	Loss: 0.027707	Acc: 60.2% (6021/10000)
[Test]  Epoch: 91	Loss: 0.027671	Acc: 60.5% (6045/10000)
[Test]  Epoch: 92	Loss: 0.027770	Acc: 60.2% (6025/10000)
[Test]  Epoch: 93	Loss: 0.027697	Acc: 60.5% (6048/10000)
[Test]  Epoch: 94	Loss: 0.027724	Acc: 60.3% (6032/10000)
[Test]  Epoch: 95	Loss: 0.027740	Acc: 60.4% (6039/10000)
[Test]  Epoch: 96	Loss: 0.027753	Acc: 60.4% (6038/10000)
[Test]  Epoch: 97	Loss: 0.027742	Acc: 60.3% (6026/10000)
[Test]  Epoch: 98	Loss: 0.027660	Acc: 60.6% (6057/10000)
[Test]  Epoch: 99	Loss: 0.027731	Acc: 60.4% (6043/10000)
[Test]  Epoch: 100	Loss: 0.027733	Acc: 60.2% (6024/10000)
===========finish==========
['2024-08-19', '16:25:21.725674', '100', 'test', '0.027732886207103728', '60.24', '60.57']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.033692	Acc: 54.6% (5457/10000)
[Test]  Epoch: 2	Loss: 0.029827	Acc: 58.4% (5835/10000)
[Test]  Epoch: 3	Loss: 0.029209	Acc: 59.0% (5901/10000)
[Test]  Epoch: 4	Loss: 0.029184	Acc: 58.9% (5891/10000)
[Test]  Epoch: 5	Loss: 0.029287	Acc: 58.8% (5880/10000)
[Test]  Epoch: 6	Loss: 0.029229	Acc: 59.1% (5906/10000)
[Test]  Epoch: 7	Loss: 0.029071	Acc: 59.3% (5932/10000)
[Test]  Epoch: 8	Loss: 0.029023	Acc: 59.2% (5916/10000)
[Test]  Epoch: 9	Loss: 0.029034	Acc: 59.3% (5931/10000)
[Test]  Epoch: 10	Loss: 0.028904	Acc: 59.3% (5931/10000)
[Test]  Epoch: 11	Loss: 0.028886	Acc: 59.2% (5922/10000)
[Test]  Epoch: 12	Loss: 0.028823	Acc: 59.3% (5927/10000)
[Test]  Epoch: 13	Loss: 0.028835	Acc: 59.2% (5916/10000)
[Test]  Epoch: 14	Loss: 0.028776	Acc: 59.2% (5923/10000)
[Test]  Epoch: 15	Loss: 0.028672	Acc: 59.4% (5940/10000)
[Test]  Epoch: 16	Loss: 0.028822	Acc: 59.5% (5945/10000)
[Test]  Epoch: 17	Loss: 0.028680	Acc: 59.4% (5939/10000)
[Test]  Epoch: 18	Loss: 0.028768	Acc: 59.6% (5962/10000)
[Test]  Epoch: 19	Loss: 0.028675	Acc: 59.6% (5962/10000)
[Test]  Epoch: 20	Loss: 0.028559	Acc: 59.5% (5949/10000)
[Test]  Epoch: 21	Loss: 0.028522	Acc: 59.5% (5945/10000)
[Test]  Epoch: 22	Loss: 0.028538	Acc: 59.4% (5938/10000)
[Test]  Epoch: 23	Loss: 0.028515	Acc: 59.4% (5944/10000)
[Test]  Epoch: 24	Loss: 0.028524	Acc: 59.6% (5956/10000)
[Test]  Epoch: 25	Loss: 0.028479	Acc: 59.3% (5932/10000)
[Test]  Epoch: 26	Loss: 0.028466	Acc: 59.4% (5937/10000)
[Test]  Epoch: 27	Loss: 0.028431	Acc: 59.5% (5946/10000)
[Test]  Epoch: 28	Loss: 0.028418	Acc: 59.6% (5959/10000)
[Test]  Epoch: 29	Loss: 0.028516	Acc: 59.6% (5956/10000)
[Test]  Epoch: 30	Loss: 0.028474	Acc: 59.5% (5949/10000)
[Test]  Epoch: 31	Loss: 0.028441	Acc: 59.5% (5947/10000)
[Test]  Epoch: 32	Loss: 0.028437	Acc: 59.5% (5954/10000)
[Test]  Epoch: 33	Loss: 0.028435	Acc: 59.5% (5949/10000)
[Test]  Epoch: 34	Loss: 0.028457	Acc: 59.4% (5936/10000)
[Test]  Epoch: 35	Loss: 0.028442	Acc: 59.5% (5952/10000)
[Test]  Epoch: 36	Loss: 0.028370	Acc: 59.5% (5953/10000)
[Test]  Epoch: 37	Loss: 0.028352	Acc: 59.5% (5948/10000)
[Test]  Epoch: 38	Loss: 0.028349	Acc: 59.4% (5940/10000)
[Test]  Epoch: 39	Loss: 0.028435	Acc: 59.4% (5944/10000)
[Test]  Epoch: 40	Loss: 0.028366	Acc: 59.4% (5936/10000)
[Test]  Epoch: 41	Loss: 0.028324	Acc: 59.6% (5961/10000)
[Test]  Epoch: 42	Loss: 0.028232	Acc: 59.5% (5951/10000)
[Test]  Epoch: 43	Loss: 0.028382	Acc: 59.5% (5953/10000)
[Test]  Epoch: 44	Loss: 0.028344	Acc: 59.6% (5963/10000)
[Test]  Epoch: 45	Loss: 0.028222	Acc: 59.7% (5971/10000)
[Test]  Epoch: 46	Loss: 0.028220	Acc: 59.6% (5961/10000)
[Test]  Epoch: 47	Loss: 0.028278	Acc: 59.4% (5942/10000)
[Test]  Epoch: 48	Loss: 0.028225	Acc: 59.7% (5967/10000)
[Test]  Epoch: 49	Loss: 0.028225	Acc: 59.5% (5954/10000)
[Test]  Epoch: 50	Loss: 0.028190	Acc: 59.7% (5971/10000)
[Test]  Epoch: 51	Loss: 0.028261	Acc: 59.6% (5965/10000)
[Test]  Epoch: 52	Loss: 0.028157	Acc: 59.6% (5963/10000)
[Test]  Epoch: 53	Loss: 0.028154	Acc: 59.7% (5973/10000)
[Test]  Epoch: 54	Loss: 0.028194	Acc: 59.7% (5970/10000)
[Test]  Epoch: 55	Loss: 0.028106	Acc: 59.8% (5976/10000)
[Test]  Epoch: 56	Loss: 0.028125	Acc: 59.8% (5979/10000)
[Test]  Epoch: 57	Loss: 0.028146	Acc: 59.6% (5965/10000)
[Test]  Epoch: 58	Loss: 0.028164	Acc: 59.8% (5977/10000)
[Test]  Epoch: 59	Loss: 0.028141	Acc: 59.6% (5964/10000)
[Test]  Epoch: 60	Loss: 0.028136	Acc: 59.8% (5981/10000)
[Test]  Epoch: 61	Loss: 0.028122	Acc: 59.7% (5974/10000)
[Test]  Epoch: 62	Loss: 0.028124	Acc: 59.8% (5980/10000)
[Test]  Epoch: 63	Loss: 0.028087	Acc: 59.9% (5993/10000)
[Test]  Epoch: 64	Loss: 0.028085	Acc: 59.9% (5987/10000)
[Test]  Epoch: 65	Loss: 0.028033	Acc: 59.7% (5974/10000)
[Test]  Epoch: 66	Loss: 0.028045	Acc: 59.7% (5969/10000)
[Test]  Epoch: 67	Loss: 0.028131	Acc: 59.7% (5974/10000)
[Test]  Epoch: 68	Loss: 0.028051	Acc: 59.8% (5979/10000)
[Test]  Epoch: 69	Loss: 0.028087	Acc: 59.9% (5986/10000)
[Test]  Epoch: 70	Loss: 0.028035	Acc: 59.8% (5983/10000)
[Test]  Epoch: 71	Loss: 0.028025	Acc: 59.9% (5994/10000)
[Test]  Epoch: 72	Loss: 0.028034	Acc: 59.8% (5984/10000)
[Test]  Epoch: 73	Loss: 0.027989	Acc: 60.0% (6003/10000)
[Test]  Epoch: 74	Loss: 0.028038	Acc: 59.9% (5993/10000)
[Test]  Epoch: 75	Loss: 0.028024	Acc: 59.9% (5987/10000)
[Test]  Epoch: 76	Loss: 0.028076	Acc: 60.0% (5997/10000)
[Test]  Epoch: 77	Loss: 0.028100	Acc: 59.9% (5988/10000)
[Test]  Epoch: 78	Loss: 0.028033	Acc: 60.0% (5997/10000)
[Test]  Epoch: 79	Loss: 0.028088	Acc: 59.9% (5990/10000)
[Test]  Epoch: 80	Loss: 0.028064	Acc: 59.7% (5974/10000)
[Test]  Epoch: 81	Loss: 0.028097	Acc: 59.5% (5955/10000)
[Test]  Epoch: 82	Loss: 0.028047	Acc: 59.9% (5987/10000)
[Test]  Epoch: 83	Loss: 0.028108	Acc: 59.8% (5980/10000)
[Test]  Epoch: 84	Loss: 0.028031	Acc: 59.9% (5989/10000)
[Test]  Epoch: 85	Loss: 0.028065	Acc: 59.8% (5983/10000)
[Test]  Epoch: 86	Loss: 0.028094	Acc: 59.9% (5993/10000)
[Test]  Epoch: 87	Loss: 0.028023	Acc: 59.8% (5982/10000)
[Test]  Epoch: 88	Loss: 0.028106	Acc: 59.9% (5992/10000)
[Test]  Epoch: 89	Loss: 0.028074	Acc: 59.9% (5989/10000)
[Test]  Epoch: 90	Loss: 0.028030	Acc: 59.8% (5981/10000)
[Test]  Epoch: 91	Loss: 0.028028	Acc: 59.9% (5986/10000)
[Test]  Epoch: 92	Loss: 0.028106	Acc: 59.8% (5983/10000)
[Test]  Epoch: 93	Loss: 0.028045	Acc: 60.0% (6000/10000)
[Test]  Epoch: 94	Loss: 0.028032	Acc: 60.0% (6002/10000)
[Test]  Epoch: 95	Loss: 0.028082	Acc: 60.0% (5998/10000)
[Test]  Epoch: 96	Loss: 0.028117	Acc: 59.9% (5993/10000)
[Test]  Epoch: 97	Loss: 0.028071	Acc: 59.7% (5966/10000)
[Test]  Epoch: 98	Loss: 0.028004	Acc: 59.9% (5989/10000)
[Test]  Epoch: 99	Loss: 0.028069	Acc: 59.6% (5965/10000)
[Test]  Epoch: 100	Loss: 0.028045	Acc: 59.7% (5970/10000)
===========finish==========
['2024-08-19', '16:29:47.766296', '100', 'test', '0.028044821739196778', '59.7', '60.03']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039294	Acc: 48.7% (4866/10000)
[Test]  Epoch: 2	Loss: 0.032584	Acc: 54.6% (5459/10000)
[Test]  Epoch: 3	Loss: 0.031701	Acc: 55.1% (5514/10000)
[Test]  Epoch: 4	Loss: 0.031642	Acc: 55.3% (5531/10000)
[Test]  Epoch: 5	Loss: 0.031485	Acc: 55.4% (5535/10000)
[Test]  Epoch: 6	Loss: 0.031448	Acc: 55.5% (5550/10000)
[Test]  Epoch: 7	Loss: 0.031373	Acc: 55.6% (5564/10000)
[Test]  Epoch: 8	Loss: 0.031319	Acc: 55.6% (5556/10000)
[Test]  Epoch: 9	Loss: 0.031336	Acc: 55.6% (5558/10000)
[Test]  Epoch: 10	Loss: 0.031236	Acc: 55.7% (5568/10000)
[Test]  Epoch: 11	Loss: 0.031099	Acc: 55.5% (5552/10000)
[Test]  Epoch: 12	Loss: 0.031108	Acc: 55.8% (5578/10000)
[Test]  Epoch: 13	Loss: 0.031140	Acc: 55.9% (5593/10000)
[Test]  Epoch: 14	Loss: 0.031112	Acc: 55.7% (5568/10000)
[Test]  Epoch: 15	Loss: 0.030952	Acc: 55.9% (5594/10000)
[Test]  Epoch: 16	Loss: 0.031095	Acc: 56.0% (5596/10000)
[Test]  Epoch: 17	Loss: 0.030966	Acc: 56.0% (5602/10000)
[Test]  Epoch: 18	Loss: 0.031075	Acc: 56.0% (5600/10000)
[Test]  Epoch: 19	Loss: 0.031020	Acc: 55.9% (5593/10000)
[Test]  Epoch: 20	Loss: 0.030938	Acc: 56.0% (5596/10000)
[Test]  Epoch: 21	Loss: 0.030877	Acc: 56.1% (5613/10000)
[Test]  Epoch: 22	Loss: 0.030870	Acc: 56.1% (5614/10000)
[Test]  Epoch: 23	Loss: 0.030830	Acc: 56.1% (5613/10000)
[Test]  Epoch: 24	Loss: 0.030792	Acc: 56.0% (5598/10000)
[Test]  Epoch: 25	Loss: 0.030750	Acc: 55.9% (5593/10000)
[Test]  Epoch: 26	Loss: 0.030704	Acc: 56.1% (5613/10000)
[Test]  Epoch: 27	Loss: 0.030674	Acc: 56.2% (5618/10000)
[Test]  Epoch: 28	Loss: 0.030648	Acc: 56.3% (5626/10000)
[Test]  Epoch: 29	Loss: 0.030785	Acc: 56.4% (5636/10000)
[Test]  Epoch: 30	Loss: 0.030731	Acc: 56.2% (5623/10000)
[Test]  Epoch: 31	Loss: 0.030652	Acc: 56.4% (5635/10000)
[Test]  Epoch: 32	Loss: 0.030623	Acc: 56.3% (5627/10000)
[Test]  Epoch: 33	Loss: 0.030666	Acc: 56.3% (5630/10000)
[Test]  Epoch: 34	Loss: 0.030666	Acc: 56.1% (5608/10000)
[Test]  Epoch: 35	Loss: 0.030621	Acc: 56.1% (5608/10000)
[Test]  Epoch: 36	Loss: 0.030654	Acc: 56.2% (5622/10000)
[Test]  Epoch: 37	Loss: 0.030632	Acc: 56.1% (5613/10000)
[Test]  Epoch: 38	Loss: 0.030626	Acc: 56.4% (5637/10000)
[Test]  Epoch: 39	Loss: 0.030726	Acc: 56.3% (5628/10000)
[Test]  Epoch: 40	Loss: 0.030574	Acc: 56.3% (5628/10000)
[Test]  Epoch: 41	Loss: 0.030535	Acc: 56.3% (5629/10000)
[Test]  Epoch: 42	Loss: 0.030458	Acc: 56.2% (5624/10000)
[Test]  Epoch: 43	Loss: 0.030606	Acc: 56.1% (5608/10000)
[Test]  Epoch: 44	Loss: 0.030586	Acc: 56.1% (5613/10000)
[Test]  Epoch: 45	Loss: 0.030498	Acc: 56.3% (5628/10000)
[Test]  Epoch: 46	Loss: 0.030558	Acc: 56.3% (5629/10000)
[Test]  Epoch: 47	Loss: 0.030586	Acc: 56.1% (5614/10000)
[Test]  Epoch: 48	Loss: 0.030506	Acc: 56.1% (5614/10000)
[Test]  Epoch: 49	Loss: 0.030562	Acc: 56.2% (5621/10000)
[Test]  Epoch: 50	Loss: 0.030501	Acc: 56.2% (5621/10000)
[Test]  Epoch: 51	Loss: 0.030556	Acc: 56.3% (5630/10000)
[Test]  Epoch: 52	Loss: 0.030431	Acc: 56.1% (5615/10000)
[Test]  Epoch: 53	Loss: 0.030459	Acc: 56.1% (5615/10000)
[Test]  Epoch: 54	Loss: 0.030412	Acc: 56.3% (5628/10000)
[Test]  Epoch: 55	Loss: 0.030388	Acc: 56.4% (5639/10000)
[Test]  Epoch: 56	Loss: 0.030382	Acc: 56.4% (5643/10000)
[Test]  Epoch: 57	Loss: 0.030365	Acc: 56.4% (5635/10000)
[Test]  Epoch: 58	Loss: 0.030492	Acc: 56.2% (5622/10000)
[Test]  Epoch: 59	Loss: 0.030388	Acc: 56.3% (5628/10000)
[Test]  Epoch: 60	Loss: 0.030374	Acc: 56.4% (5643/10000)
[Test]  Epoch: 61	Loss: 0.030341	Acc: 56.4% (5635/10000)
[Test]  Epoch: 62	Loss: 0.030362	Acc: 56.4% (5640/10000)
[Test]  Epoch: 63	Loss: 0.030374	Acc: 56.3% (5632/10000)
[Test]  Epoch: 64	Loss: 0.030384	Acc: 56.5% (5649/10000)
[Test]  Epoch: 65	Loss: 0.030370	Acc: 56.2% (5616/10000)
[Test]  Epoch: 66	Loss: 0.030376	Acc: 56.3% (5634/10000)
[Test]  Epoch: 67	Loss: 0.030399	Acc: 56.4% (5644/10000)
[Test]  Epoch: 68	Loss: 0.030276	Acc: 56.4% (5635/10000)
[Test]  Epoch: 69	Loss: 0.030343	Acc: 56.3% (5632/10000)
[Test]  Epoch: 70	Loss: 0.030328	Acc: 56.3% (5627/10000)
[Test]  Epoch: 71	Loss: 0.030335	Acc: 56.3% (5633/10000)
[Test]  Epoch: 72	Loss: 0.030320	Acc: 56.3% (5634/10000)
[Test]  Epoch: 73	Loss: 0.030281	Acc: 56.4% (5642/10000)
[Test]  Epoch: 74	Loss: 0.030319	Acc: 56.3% (5631/10000)
[Test]  Epoch: 75	Loss: 0.030321	Acc: 56.5% (5647/10000)
[Test]  Epoch: 76	Loss: 0.030327	Acc: 56.5% (5646/10000)
[Test]  Epoch: 77	Loss: 0.030320	Acc: 56.5% (5649/10000)
[Test]  Epoch: 78	Loss: 0.030270	Acc: 56.4% (5637/10000)
[Test]  Epoch: 79	Loss: 0.030348	Acc: 56.3% (5631/10000)
[Test]  Epoch: 80	Loss: 0.030349	Acc: 56.3% (5632/10000)
[Test]  Epoch: 81	Loss: 0.030329	Acc: 56.4% (5638/10000)
[Test]  Epoch: 82	Loss: 0.030380	Acc: 56.3% (5631/10000)
[Test]  Epoch: 83	Loss: 0.030395	Acc: 56.4% (5637/10000)
[Test]  Epoch: 84	Loss: 0.030302	Acc: 56.4% (5637/10000)
[Test]  Epoch: 85	Loss: 0.030359	Acc: 56.4% (5635/10000)
[Test]  Epoch: 86	Loss: 0.030334	Acc: 56.4% (5642/10000)
[Test]  Epoch: 87	Loss: 0.030300	Acc: 56.4% (5640/10000)
[Test]  Epoch: 88	Loss: 0.030345	Acc: 56.3% (5632/10000)
[Test]  Epoch: 89	Loss: 0.030349	Acc: 56.4% (5638/10000)
[Test]  Epoch: 90	Loss: 0.030295	Acc: 56.4% (5636/10000)
[Test]  Epoch: 91	Loss: 0.030301	Acc: 56.3% (5634/10000)
[Test]  Epoch: 92	Loss: 0.030442	Acc: 56.3% (5632/10000)
[Test]  Epoch: 93	Loss: 0.030282	Acc: 56.4% (5635/10000)
[Test]  Epoch: 94	Loss: 0.030290	Acc: 56.4% (5644/10000)
[Test]  Epoch: 95	Loss: 0.030372	Acc: 56.4% (5636/10000)
[Test]  Epoch: 96	Loss: 0.030372	Acc: 56.4% (5639/10000)
[Test]  Epoch: 97	Loss: 0.030354	Acc: 56.5% (5647/10000)
[Test]  Epoch: 98	Loss: 0.030281	Acc: 56.3% (5631/10000)
[Test]  Epoch: 99	Loss: 0.030335	Acc: 56.2% (5625/10000)
[Test]  Epoch: 100	Loss: 0.030307	Acc: 56.3% (5633/10000)
===========finish==========
['2024-08-19', '16:34:16.993347', '100', 'test', '0.03030674995183945', '56.33', '56.49']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.041686	Acc: 46.3% (4630/10000)
[Test]  Epoch: 2	Loss: 0.033517	Acc: 53.0% (5304/10000)
[Test]  Epoch: 3	Loss: 0.032561	Acc: 53.7% (5369/10000)
[Test]  Epoch: 4	Loss: 0.032347	Acc: 53.8% (5383/10000)
[Test]  Epoch: 5	Loss: 0.032171	Acc: 54.0% (5396/10000)
[Test]  Epoch: 6	Loss: 0.032124	Acc: 54.0% (5395/10000)
[Test]  Epoch: 7	Loss: 0.032180	Acc: 54.1% (5406/10000)
[Test]  Epoch: 8	Loss: 0.032122	Acc: 54.1% (5407/10000)
[Test]  Epoch: 9	Loss: 0.032089	Acc: 54.4% (5435/10000)
[Test]  Epoch: 10	Loss: 0.031884	Acc: 54.2% (5425/10000)
[Test]  Epoch: 11	Loss: 0.031859	Acc: 54.4% (5437/10000)
[Test]  Epoch: 12	Loss: 0.031870	Acc: 54.2% (5420/10000)
[Test]  Epoch: 13	Loss: 0.031842	Acc: 54.4% (5438/10000)
[Test]  Epoch: 14	Loss: 0.031836	Acc: 54.2% (5424/10000)
[Test]  Epoch: 15	Loss: 0.031778	Acc: 54.2% (5422/10000)
[Test]  Epoch: 16	Loss: 0.031801	Acc: 54.3% (5431/10000)
[Test]  Epoch: 17	Loss: 0.031718	Acc: 54.5% (5452/10000)
[Test]  Epoch: 18	Loss: 0.031753	Acc: 54.6% (5460/10000)
[Test]  Epoch: 19	Loss: 0.031794	Acc: 54.4% (5444/10000)
[Test]  Epoch: 20	Loss: 0.031639	Acc: 54.4% (5440/10000)
[Test]  Epoch: 21	Loss: 0.031640	Acc: 54.5% (5455/10000)
[Test]  Epoch: 22	Loss: 0.031601	Acc: 54.5% (5454/10000)
[Test]  Epoch: 23	Loss: 0.031480	Acc: 54.6% (5462/10000)
[Test]  Epoch: 24	Loss: 0.031556	Acc: 54.6% (5459/10000)
[Test]  Epoch: 25	Loss: 0.031453	Acc: 54.6% (5456/10000)
[Test]  Epoch: 26	Loss: 0.031412	Acc: 54.8% (5476/10000)
[Test]  Epoch: 27	Loss: 0.031453	Acc: 54.6% (5465/10000)
[Test]  Epoch: 28	Loss: 0.031387	Acc: 54.8% (5477/10000)
[Test]  Epoch: 29	Loss: 0.031519	Acc: 54.7% (5471/10000)
[Test]  Epoch: 30	Loss: 0.031411	Acc: 54.6% (5462/10000)
[Test]  Epoch: 31	Loss: 0.031274	Acc: 54.9% (5486/10000)
[Test]  Epoch: 32	Loss: 0.031343	Acc: 54.7% (5472/10000)
[Test]  Epoch: 33	Loss: 0.031388	Acc: 54.9% (5485/10000)
[Test]  Epoch: 34	Loss: 0.031361	Acc: 54.7% (5469/10000)
[Test]  Epoch: 35	Loss: 0.031315	Acc: 54.9% (5491/10000)
[Test]  Epoch: 36	Loss: 0.031291	Acc: 55.0% (5495/10000)
[Test]  Epoch: 37	Loss: 0.031302	Acc: 54.8% (5479/10000)
[Test]  Epoch: 38	Loss: 0.031310	Acc: 54.8% (5483/10000)
[Test]  Epoch: 39	Loss: 0.031345	Acc: 55.0% (5497/10000)
[Test]  Epoch: 40	Loss: 0.031265	Acc: 54.8% (5480/10000)
[Test]  Epoch: 41	Loss: 0.031206	Acc: 54.9% (5494/10000)
[Test]  Epoch: 42	Loss: 0.031086	Acc: 55.0% (5502/10000)
[Test]  Epoch: 43	Loss: 0.031271	Acc: 55.0% (5495/10000)
[Test]  Epoch: 44	Loss: 0.031206	Acc: 54.9% (5494/10000)
[Test]  Epoch: 45	Loss: 0.031157	Acc: 55.0% (5497/10000)
[Test]  Epoch: 46	Loss: 0.031138	Acc: 54.9% (5489/10000)
[Test]  Epoch: 47	Loss: 0.031204	Acc: 55.0% (5502/10000)
[Test]  Epoch: 48	Loss: 0.031138	Acc: 54.8% (5476/10000)
[Test]  Epoch: 49	Loss: 0.031140	Acc: 54.9% (5489/10000)
[Test]  Epoch: 50	Loss: 0.031141	Acc: 55.0% (5499/10000)
[Test]  Epoch: 51	Loss: 0.031182	Acc: 54.9% (5491/10000)
[Test]  Epoch: 52	Loss: 0.031076	Acc: 54.8% (5484/10000)
[Test]  Epoch: 53	Loss: 0.031086	Acc: 55.0% (5502/10000)
[Test]  Epoch: 54	Loss: 0.031117	Acc: 55.1% (5506/10000)
[Test]  Epoch: 55	Loss: 0.031020	Acc: 55.1% (5512/10000)
[Test]  Epoch: 56	Loss: 0.030995	Acc: 55.0% (5495/10000)
[Test]  Epoch: 57	Loss: 0.031009	Acc: 55.2% (5522/10000)
[Test]  Epoch: 58	Loss: 0.031125	Acc: 54.9% (5488/10000)
[Test]  Epoch: 59	Loss: 0.031090	Acc: 54.9% (5494/10000)
[Test]  Epoch: 60	Loss: 0.031036	Acc: 55.1% (5508/10000)
[Test]  Epoch: 61	Loss: 0.031038	Acc: 55.0% (5495/10000)
[Test]  Epoch: 62	Loss: 0.030996	Acc: 55.1% (5515/10000)
[Test]  Epoch: 63	Loss: 0.031010	Acc: 55.0% (5503/10000)
[Test]  Epoch: 64	Loss: 0.030963	Acc: 55.0% (5505/10000)
[Test]  Epoch: 65	Loss: 0.030973	Acc: 54.9% (5490/10000)
[Test]  Epoch: 66	Loss: 0.030997	Acc: 55.1% (5506/10000)
[Test]  Epoch: 67	Loss: 0.031077	Acc: 55.1% (5510/10000)
[Test]  Epoch: 68	Loss: 0.030967	Acc: 55.0% (5501/10000)
[Test]  Epoch: 69	Loss: 0.030962	Acc: 55.1% (5511/10000)
[Test]  Epoch: 70	Loss: 0.030961	Acc: 54.9% (5492/10000)
[Test]  Epoch: 71	Loss: 0.030991	Acc: 55.1% (5515/10000)
[Test]  Epoch: 72	Loss: 0.030990	Acc: 55.0% (5497/10000)
[Test]  Epoch: 73	Loss: 0.030928	Acc: 55.1% (5511/10000)
[Test]  Epoch: 74	Loss: 0.030972	Acc: 55.0% (5501/10000)
[Test]  Epoch: 75	Loss: 0.030995	Acc: 55.0% (5501/10000)
[Test]  Epoch: 76	Loss: 0.031023	Acc: 55.0% (5504/10000)
[Test]  Epoch: 77	Loss: 0.030955	Acc: 55.2% (5516/10000)
[Test]  Epoch: 78	Loss: 0.030937	Acc: 55.0% (5503/10000)
[Test]  Epoch: 79	Loss: 0.031002	Acc: 55.1% (5506/10000)
[Test]  Epoch: 80	Loss: 0.030965	Acc: 55.2% (5525/10000)
[Test]  Epoch: 81	Loss: 0.031012	Acc: 55.0% (5504/10000)
[Test]  Epoch: 82	Loss: 0.031060	Acc: 55.1% (5511/10000)
[Test]  Epoch: 83	Loss: 0.031031	Acc: 54.9% (5487/10000)
[Test]  Epoch: 84	Loss: 0.031012	Acc: 55.1% (5510/10000)
[Test]  Epoch: 85	Loss: 0.031002	Acc: 55.1% (5514/10000)
[Test]  Epoch: 86	Loss: 0.031017	Acc: 55.1% (5508/10000)
[Test]  Epoch: 87	Loss: 0.030952	Acc: 55.2% (5519/10000)
[Test]  Epoch: 88	Loss: 0.031006	Acc: 55.0% (5499/10000)
[Test]  Epoch: 89	Loss: 0.030995	Acc: 55.1% (5514/10000)
[Test]  Epoch: 90	Loss: 0.030937	Acc: 55.0% (5498/10000)
[Test]  Epoch: 91	Loss: 0.030999	Acc: 54.9% (5493/10000)
[Test]  Epoch: 92	Loss: 0.031063	Acc: 54.9% (5491/10000)
[Test]  Epoch: 93	Loss: 0.030941	Acc: 55.1% (5512/10000)
[Test]  Epoch: 94	Loss: 0.030957	Acc: 55.2% (5522/10000)
[Test]  Epoch: 95	Loss: 0.030982	Acc: 55.0% (5502/10000)
[Test]  Epoch: 96	Loss: 0.030978	Acc: 55.0% (5501/10000)
[Test]  Epoch: 97	Loss: 0.031007	Acc: 55.0% (5496/10000)
[Test]  Epoch: 98	Loss: 0.030950	Acc: 55.2% (5516/10000)
[Test]  Epoch: 99	Loss: 0.031012	Acc: 55.1% (5509/10000)
[Test]  Epoch: 100	Loss: 0.031002	Acc: 55.0% (5498/10000)
===========finish==========
['2024-08-19', '16:38:53.156877', '100', 'test', '0.031001823747158052', '54.98', '55.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.054844	Acc: 30.5% (3048/10000)
[Test]  Epoch: 2	Loss: 0.043423	Acc: 39.3% (3928/10000)
[Test]  Epoch: 3	Loss: 0.041551	Acc: 40.8% (4082/10000)
[Test]  Epoch: 4	Loss: 0.041161	Acc: 40.8% (4083/10000)
[Test]  Epoch: 5	Loss: 0.040945	Acc: 41.3% (4128/10000)
[Test]  Epoch: 6	Loss: 0.040946	Acc: 41.1% (4113/10000)
[Test]  Epoch: 7	Loss: 0.040823	Acc: 41.0% (4101/10000)
[Test]  Epoch: 8	Loss: 0.040755	Acc: 41.2% (4121/10000)
[Test]  Epoch: 9	Loss: 0.040678	Acc: 41.4% (4141/10000)
[Test]  Epoch: 10	Loss: 0.040563	Acc: 41.6% (4160/10000)
[Test]  Epoch: 11	Loss: 0.040532	Acc: 41.6% (4159/10000)
[Test]  Epoch: 12	Loss: 0.040432	Acc: 42.0% (4203/10000)
[Test]  Epoch: 13	Loss: 0.040535	Acc: 41.4% (4143/10000)
[Test]  Epoch: 14	Loss: 0.040449	Acc: 41.6% (4165/10000)
[Test]  Epoch: 15	Loss: 0.040390	Acc: 41.8% (4177/10000)
[Test]  Epoch: 16	Loss: 0.040417	Acc: 41.8% (4181/10000)
[Test]  Epoch: 17	Loss: 0.040360	Acc: 41.6% (4162/10000)
[Test]  Epoch: 18	Loss: 0.040490	Acc: 41.5% (4146/10000)
[Test]  Epoch: 19	Loss: 0.040422	Acc: 41.7% (4167/10000)
[Test]  Epoch: 20	Loss: 0.040272	Acc: 41.8% (4178/10000)
[Test]  Epoch: 21	Loss: 0.040167	Acc: 41.8% (4180/10000)
[Test]  Epoch: 22	Loss: 0.040176	Acc: 42.0% (4203/10000)
[Test]  Epoch: 23	Loss: 0.040007	Acc: 42.0% (4200/10000)
[Test]  Epoch: 24	Loss: 0.040204	Acc: 41.6% (4156/10000)
[Test]  Epoch: 25	Loss: 0.040123	Acc: 42.0% (4202/10000)
[Test]  Epoch: 26	Loss: 0.040037	Acc: 41.9% (4191/10000)
[Test]  Epoch: 27	Loss: 0.040080	Acc: 42.0% (4195/10000)
[Test]  Epoch: 28	Loss: 0.040051	Acc: 41.8% (4177/10000)
[Test]  Epoch: 29	Loss: 0.040194	Acc: 41.8% (4178/10000)
[Test]  Epoch: 30	Loss: 0.040090	Acc: 41.8% (4179/10000)
[Test]  Epoch: 31	Loss: 0.039992	Acc: 42.1% (4206/10000)
[Test]  Epoch: 32	Loss: 0.039980	Acc: 42.1% (4209/10000)
[Test]  Epoch: 33	Loss: 0.039945	Acc: 42.0% (4199/10000)
[Test]  Epoch: 34	Loss: 0.040010	Acc: 42.0% (4195/10000)
[Test]  Epoch: 35	Loss: 0.039888	Acc: 42.0% (4201/10000)
[Test]  Epoch: 36	Loss: 0.039850	Acc: 42.2% (4222/10000)
[Test]  Epoch: 37	Loss: 0.039885	Acc: 42.0% (4204/10000)
[Test]  Epoch: 38	Loss: 0.039800	Acc: 42.3% (4229/10000)
[Test]  Epoch: 39	Loss: 0.039897	Acc: 42.1% (4206/10000)
[Test]  Epoch: 40	Loss: 0.039884	Acc: 42.2% (4218/10000)
[Test]  Epoch: 41	Loss: 0.039768	Acc: 42.4% (4236/10000)
[Test]  Epoch: 42	Loss: 0.039608	Acc: 42.3% (4234/10000)
[Test]  Epoch: 43	Loss: 0.039680	Acc: 42.2% (4219/10000)
[Test]  Epoch: 44	Loss: 0.039654	Acc: 42.3% (4229/10000)
[Test]  Epoch: 45	Loss: 0.039634	Acc: 42.4% (4244/10000)
[Test]  Epoch: 46	Loss: 0.039728	Acc: 42.3% (4234/10000)
[Test]  Epoch: 47	Loss: 0.039720	Acc: 42.3% (4234/10000)
[Test]  Epoch: 48	Loss: 0.039557	Acc: 42.4% (4237/10000)
[Test]  Epoch: 49	Loss: 0.039686	Acc: 42.5% (4247/10000)
[Test]  Epoch: 50	Loss: 0.039678	Acc: 42.4% (4240/10000)
[Test]  Epoch: 51	Loss: 0.039661	Acc: 42.2% (4224/10000)
[Test]  Epoch: 52	Loss: 0.039559	Acc: 42.4% (4240/10000)
[Test]  Epoch: 53	Loss: 0.039665	Acc: 42.4% (4239/10000)
[Test]  Epoch: 54	Loss: 0.039661	Acc: 42.5% (4255/10000)
[Test]  Epoch: 55	Loss: 0.039530	Acc: 42.4% (4235/10000)
[Test]  Epoch: 56	Loss: 0.039512	Acc: 42.2% (4224/10000)
[Test]  Epoch: 57	Loss: 0.039530	Acc: 42.5% (4248/10000)
[Test]  Epoch: 58	Loss: 0.039559	Acc: 42.6% (4258/10000)
[Test]  Epoch: 59	Loss: 0.039614	Acc: 42.2% (4224/10000)
[Test]  Epoch: 60	Loss: 0.039606	Acc: 42.4% (4242/10000)
[Test]  Epoch: 61	Loss: 0.039574	Acc: 42.4% (4243/10000)
[Test]  Epoch: 62	Loss: 0.039566	Acc: 42.5% (4245/10000)
[Test]  Epoch: 63	Loss: 0.039574	Acc: 42.3% (4230/10000)
[Test]  Epoch: 64	Loss: 0.039483	Acc: 42.4% (4238/10000)
[Test]  Epoch: 65	Loss: 0.039577	Acc: 42.5% (4245/10000)
[Test]  Epoch: 66	Loss: 0.039532	Acc: 42.5% (4251/10000)
[Test]  Epoch: 67	Loss: 0.039502	Acc: 42.4% (4244/10000)
[Test]  Epoch: 68	Loss: 0.039570	Acc: 42.4% (4244/10000)
[Test]  Epoch: 69	Loss: 0.039484	Acc: 42.5% (4245/10000)
[Test]  Epoch: 70	Loss: 0.039523	Acc: 42.5% (4253/10000)
[Test]  Epoch: 71	Loss: 0.039510	Acc: 42.4% (4243/10000)
[Test]  Epoch: 72	Loss: 0.039496	Acc: 42.4% (4244/10000)
[Test]  Epoch: 73	Loss: 0.039387	Acc: 42.3% (4232/10000)
[Test]  Epoch: 74	Loss: 0.039426	Acc: 42.4% (4240/10000)
[Test]  Epoch: 75	Loss: 0.039494	Acc: 42.5% (4248/10000)
[Test]  Epoch: 76	Loss: 0.039570	Acc: 42.3% (4233/10000)
[Test]  Epoch: 77	Loss: 0.039401	Acc: 42.4% (4236/10000)
[Test]  Epoch: 78	Loss: 0.039445	Acc: 42.4% (4242/10000)
[Test]  Epoch: 79	Loss: 0.039486	Acc: 42.5% (4254/10000)
[Test]  Epoch: 80	Loss: 0.039498	Acc: 42.5% (4251/10000)
[Test]  Epoch: 81	Loss: 0.039581	Acc: 42.6% (4259/10000)
[Test]  Epoch: 82	Loss: 0.039533	Acc: 42.6% (4263/10000)
[Test]  Epoch: 83	Loss: 0.039538	Acc: 42.4% (4235/10000)
[Test]  Epoch: 84	Loss: 0.039533	Acc: 42.5% (4255/10000)
[Test]  Epoch: 85	Loss: 0.039455	Acc: 42.5% (4255/10000)
[Test]  Epoch: 86	Loss: 0.039426	Acc: 42.6% (4265/10000)
[Test]  Epoch: 87	Loss: 0.039403	Acc: 42.2% (4224/10000)
[Test]  Epoch: 88	Loss: 0.039460	Acc: 42.4% (4236/10000)
[Test]  Epoch: 89	Loss: 0.039527	Acc: 42.4% (4242/10000)
[Test]  Epoch: 90	Loss: 0.039499	Acc: 42.4% (4240/10000)
[Test]  Epoch: 91	Loss: 0.039500	Acc: 42.5% (4249/10000)
[Test]  Epoch: 92	Loss: 0.039580	Acc: 42.4% (4244/10000)
[Test]  Epoch: 93	Loss: 0.039490	Acc: 42.5% (4251/10000)
[Test]  Epoch: 94	Loss: 0.039394	Acc: 42.6% (4258/10000)
[Test]  Epoch: 95	Loss: 0.039546	Acc: 42.1% (4212/10000)
[Test]  Epoch: 96	Loss: 0.039541	Acc: 42.3% (4231/10000)
[Test]  Epoch: 97	Loss: 0.039539	Acc: 42.4% (4242/10000)
[Test]  Epoch: 98	Loss: 0.039381	Acc: 42.6% (4259/10000)
[Test]  Epoch: 99	Loss: 0.039458	Acc: 42.6% (4263/10000)
[Test]  Epoch: 100	Loss: 0.039511	Acc: 42.5% (4250/10000)
===========finish==========
['2024-08-19', '16:43:16.648626', '100', 'test', '0.03951086938381195', '42.5', '42.65']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.066633	Acc: 20.1% (2006/10000)
[Test]  Epoch: 2	Loss: 0.046388	Acc: 34.2% (3417/10000)
[Test]  Epoch: 3	Loss: 0.044237	Acc: 35.7% (3574/10000)
[Test]  Epoch: 4	Loss: 0.043705	Acc: 36.0% (3597/10000)
[Test]  Epoch: 5	Loss: 0.043620	Acc: 36.0% (3601/10000)
[Test]  Epoch: 6	Loss: 0.043461	Acc: 36.4% (3637/10000)
[Test]  Epoch: 7	Loss: 0.043311	Acc: 36.4% (3643/10000)
[Test]  Epoch: 8	Loss: 0.043292	Acc: 36.5% (3646/10000)
[Test]  Epoch: 9	Loss: 0.043308	Acc: 36.5% (3646/10000)
[Test]  Epoch: 10	Loss: 0.043017	Acc: 36.6% (3660/10000)
[Test]  Epoch: 11	Loss: 0.043025	Acc: 36.7% (3670/10000)
[Test]  Epoch: 12	Loss: 0.042999	Acc: 36.9% (3685/10000)
[Test]  Epoch: 13	Loss: 0.043134	Acc: 36.6% (3657/10000)
[Test]  Epoch: 14	Loss: 0.042912	Acc: 36.7% (3668/10000)
[Test]  Epoch: 15	Loss: 0.043006	Acc: 36.9% (3686/10000)
[Test]  Epoch: 16	Loss: 0.042949	Acc: 36.7% (3670/10000)
[Test]  Epoch: 17	Loss: 0.042888	Acc: 36.7% (3669/10000)
[Test]  Epoch: 18	Loss: 0.043058	Acc: 36.7% (3673/10000)
[Test]  Epoch: 19	Loss: 0.043046	Acc: 36.5% (3655/10000)
[Test]  Epoch: 20	Loss: 0.042919	Acc: 36.9% (3687/10000)
[Test]  Epoch: 21	Loss: 0.042887	Acc: 36.9% (3690/10000)
[Test]  Epoch: 22	Loss: 0.042853	Acc: 36.9% (3690/10000)
[Test]  Epoch: 23	Loss: 0.042828	Acc: 37.0% (3703/10000)
[Test]  Epoch: 24	Loss: 0.042847	Acc: 36.9% (3689/10000)
[Test]  Epoch: 25	Loss: 0.042797	Acc: 36.9% (3687/10000)
[Test]  Epoch: 26	Loss: 0.042689	Acc: 37.0% (3705/10000)
[Test]  Epoch: 27	Loss: 0.042690	Acc: 36.9% (3694/10000)
[Test]  Epoch: 28	Loss: 0.042663	Acc: 37.0% (3702/10000)
[Test]  Epoch: 29	Loss: 0.042727	Acc: 36.9% (3692/10000)
[Test]  Epoch: 30	Loss: 0.042693	Acc: 37.1% (3707/10000)
[Test]  Epoch: 31	Loss: 0.042667	Acc: 37.1% (3713/10000)
[Test]  Epoch: 32	Loss: 0.042686	Acc: 36.8% (3681/10000)
[Test]  Epoch: 33	Loss: 0.042559	Acc: 37.1% (3714/10000)
[Test]  Epoch: 34	Loss: 0.042651	Acc: 37.0% (3699/10000)
[Test]  Epoch: 35	Loss: 0.042593	Acc: 37.2% (3717/10000)
[Test]  Epoch: 36	Loss: 0.042417	Acc: 37.2% (3719/10000)
[Test]  Epoch: 37	Loss: 0.042521	Acc: 37.0% (3703/10000)
[Test]  Epoch: 38	Loss: 0.042507	Acc: 37.2% (3722/10000)
[Test]  Epoch: 39	Loss: 0.042573	Acc: 37.0% (3703/10000)
[Test]  Epoch: 40	Loss: 0.042530	Acc: 37.3% (3733/10000)
[Test]  Epoch: 41	Loss: 0.042520	Acc: 37.2% (3720/10000)
[Test]  Epoch: 42	Loss: 0.042441	Acc: 37.2% (3723/10000)
[Test]  Epoch: 43	Loss: 0.042523	Acc: 37.3% (3733/10000)
[Test]  Epoch: 44	Loss: 0.042504	Acc: 37.1% (3706/10000)
[Test]  Epoch: 45	Loss: 0.042512	Acc: 37.0% (3701/10000)
[Test]  Epoch: 46	Loss: 0.042505	Acc: 37.1% (3714/10000)
[Test]  Epoch: 47	Loss: 0.042504	Acc: 37.0% (3705/10000)
[Test]  Epoch: 48	Loss: 0.042364	Acc: 37.2% (3722/10000)
[Test]  Epoch: 49	Loss: 0.042371	Acc: 37.1% (3711/10000)
[Test]  Epoch: 50	Loss: 0.042469	Acc: 37.1% (3714/10000)
[Test]  Epoch: 51	Loss: 0.042427	Acc: 37.2% (3719/10000)
[Test]  Epoch: 52	Loss: 0.042447	Acc: 37.1% (3713/10000)
[Test]  Epoch: 53	Loss: 0.042426	Acc: 37.1% (3713/10000)
[Test]  Epoch: 54	Loss: 0.042418	Acc: 37.2% (3718/10000)
[Test]  Epoch: 55	Loss: 0.042408	Acc: 37.0% (3698/10000)
[Test]  Epoch: 56	Loss: 0.042362	Acc: 37.3% (3730/10000)
[Test]  Epoch: 57	Loss: 0.042212	Acc: 37.3% (3734/10000)
[Test]  Epoch: 58	Loss: 0.042299	Acc: 37.2% (3717/10000)
[Test]  Epoch: 59	Loss: 0.042348	Acc: 37.2% (3718/10000)
[Test]  Epoch: 60	Loss: 0.042312	Acc: 37.2% (3720/10000)
[Test]  Epoch: 61	Loss: 0.042277	Acc: 37.0% (3704/10000)
[Test]  Epoch: 62	Loss: 0.042247	Acc: 37.2% (3721/10000)
[Test]  Epoch: 63	Loss: 0.042264	Acc: 37.3% (3730/10000)
[Test]  Epoch: 64	Loss: 0.042227	Acc: 37.4% (3742/10000)
[Test]  Epoch: 65	Loss: 0.042276	Acc: 37.2% (3716/10000)
[Test]  Epoch: 66	Loss: 0.042172	Acc: 37.2% (3723/10000)
[Test]  Epoch: 67	Loss: 0.042282	Acc: 37.3% (3726/10000)
[Test]  Epoch: 68	Loss: 0.042292	Acc: 36.9% (3694/10000)
[Test]  Epoch: 69	Loss: 0.042245	Acc: 37.3% (3730/10000)
[Test]  Epoch: 70	Loss: 0.042252	Acc: 37.2% (3723/10000)
[Test]  Epoch: 71	Loss: 0.042232	Acc: 37.2% (3724/10000)
[Test]  Epoch: 72	Loss: 0.042211	Acc: 37.2% (3721/10000)
[Test]  Epoch: 73	Loss: 0.042206	Acc: 37.3% (3734/10000)
[Test]  Epoch: 74	Loss: 0.042200	Acc: 37.3% (3731/10000)
[Test]  Epoch: 75	Loss: 0.042257	Acc: 37.3% (3732/10000)
[Test]  Epoch: 76	Loss: 0.042236	Acc: 37.3% (3728/10000)
[Test]  Epoch: 77	Loss: 0.042231	Acc: 37.1% (3715/10000)
[Test]  Epoch: 78	Loss: 0.042219	Acc: 37.3% (3730/10000)
[Test]  Epoch: 79	Loss: 0.042220	Acc: 37.3% (3729/10000)
[Test]  Epoch: 80	Loss: 0.042279	Acc: 37.2% (3721/10000)
[Test]  Epoch: 81	Loss: 0.042362	Acc: 37.0% (3703/10000)
[Test]  Epoch: 82	Loss: 0.042290	Acc: 37.0% (3698/10000)
[Test]  Epoch: 83	Loss: 0.042339	Acc: 37.0% (3697/10000)
[Test]  Epoch: 84	Loss: 0.042329	Acc: 37.2% (3718/10000)
[Test]  Epoch: 85	Loss: 0.042197	Acc: 37.2% (3722/10000)
[Test]  Epoch: 86	Loss: 0.042209	Acc: 37.2% (3725/10000)
[Test]  Epoch: 87	Loss: 0.042191	Acc: 37.4% (3736/10000)
[Test]  Epoch: 88	Loss: 0.042244	Acc: 37.4% (3743/10000)
[Test]  Epoch: 89	Loss: 0.042293	Acc: 37.2% (3721/10000)
[Test]  Epoch: 90	Loss: 0.042236	Acc: 37.2% (3719/10000)
[Test]  Epoch: 91	Loss: 0.042222	Acc: 37.2% (3724/10000)
[Test]  Epoch: 92	Loss: 0.042233	Acc: 37.1% (3714/10000)
[Test]  Epoch: 93	Loss: 0.042218	Acc: 37.4% (3735/10000)
[Test]  Epoch: 94	Loss: 0.042196	Acc: 37.4% (3741/10000)
[Test]  Epoch: 95	Loss: 0.042290	Acc: 37.4% (3740/10000)
[Test]  Epoch: 96	Loss: 0.042361	Acc: 37.3% (3734/10000)
[Test]  Epoch: 97	Loss: 0.042298	Acc: 37.3% (3731/10000)
[Test]  Epoch: 98	Loss: 0.042199	Acc: 37.2% (3725/10000)
[Test]  Epoch: 99	Loss: 0.042238	Acc: 37.4% (3735/10000)
[Test]  Epoch: 100	Loss: 0.042298	Acc: 37.1% (3714/10000)
===========finish==========
['2024-08-19', '16:47:47.548818', '100', 'test', '0.04229816645383835', '37.14', '37.43']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.070769	Acc: 16.0% (1598/10000)
[Test]  Epoch: 2	Loss: 0.047310	Acc: 32.2% (3223/10000)
[Test]  Epoch: 3	Loss: 0.046406	Acc: 32.9% (3285/10000)
[Test]  Epoch: 4	Loss: 0.045922	Acc: 33.6% (3360/10000)
[Test]  Epoch: 5	Loss: 0.045659	Acc: 33.8% (3376/10000)
[Test]  Epoch: 6	Loss: 0.045690	Acc: 33.6% (3358/10000)
[Test]  Epoch: 7	Loss: 0.045430	Acc: 34.0% (3402/10000)
[Test]  Epoch: 8	Loss: 0.045592	Acc: 33.7% (3370/10000)
[Test]  Epoch: 9	Loss: 0.045525	Acc: 33.6% (3358/10000)
[Test]  Epoch: 10	Loss: 0.045317	Acc: 34.0% (3396/10000)
[Test]  Epoch: 11	Loss: 0.045250	Acc: 33.9% (3391/10000)
[Test]  Epoch: 12	Loss: 0.045157	Acc: 33.9% (3391/10000)
[Test]  Epoch: 13	Loss: 0.045358	Acc: 33.9% (3394/10000)
[Test]  Epoch: 14	Loss: 0.045141	Acc: 34.1% (3413/10000)
[Test]  Epoch: 15	Loss: 0.045168	Acc: 34.1% (3414/10000)
[Test]  Epoch: 16	Loss: 0.045286	Acc: 33.9% (3394/10000)
[Test]  Epoch: 17	Loss: 0.045029	Acc: 34.2% (3421/10000)
[Test]  Epoch: 18	Loss: 0.045208	Acc: 34.1% (3409/10000)
[Test]  Epoch: 19	Loss: 0.045131	Acc: 34.1% (3409/10000)
[Test]  Epoch: 20	Loss: 0.044836	Acc: 34.4% (3435/10000)
[Test]  Epoch: 21	Loss: 0.044664	Acc: 34.5% (3448/10000)
[Test]  Epoch: 22	Loss: 0.044729	Acc: 34.4% (3438/10000)
[Test]  Epoch: 23	Loss: 0.044675	Acc: 34.4% (3443/10000)
[Test]  Epoch: 24	Loss: 0.044649	Acc: 34.3% (3432/10000)
[Test]  Epoch: 25	Loss: 0.044688	Acc: 34.4% (3440/10000)
[Test]  Epoch: 26	Loss: 0.044669	Acc: 34.4% (3441/10000)
[Test]  Epoch: 27	Loss: 0.044686	Acc: 34.5% (3445/10000)
[Test]  Epoch: 28	Loss: 0.044591	Acc: 34.7% (3468/10000)
[Test]  Epoch: 29	Loss: 0.044703	Acc: 34.4% (3440/10000)
[Test]  Epoch: 30	Loss: 0.044642	Acc: 34.5% (3454/10000)
[Test]  Epoch: 31	Loss: 0.044625	Acc: 34.4% (3444/10000)
[Test]  Epoch: 32	Loss: 0.044625	Acc: 34.5% (3455/10000)
[Test]  Epoch: 33	Loss: 0.044525	Acc: 34.4% (3439/10000)
[Test]  Epoch: 34	Loss: 0.044661	Acc: 34.4% (3436/10000)
[Test]  Epoch: 35	Loss: 0.044517	Acc: 34.4% (3442/10000)
[Test]  Epoch: 36	Loss: 0.044551	Acc: 34.4% (3435/10000)
[Test]  Epoch: 37	Loss: 0.044643	Acc: 34.4% (3436/10000)
[Test]  Epoch: 38	Loss: 0.044555	Acc: 34.5% (3447/10000)
[Test]  Epoch: 39	Loss: 0.044583	Acc: 34.3% (3428/10000)
[Test]  Epoch: 40	Loss: 0.044537	Acc: 34.6% (3461/10000)
[Test]  Epoch: 41	Loss: 0.044537	Acc: 34.4% (3440/10000)
[Test]  Epoch: 42	Loss: 0.044388	Acc: 34.6% (3456/10000)
[Test]  Epoch: 43	Loss: 0.044492	Acc: 34.4% (3442/10000)
[Test]  Epoch: 44	Loss: 0.044472	Acc: 34.6% (3460/10000)
[Test]  Epoch: 45	Loss: 0.044476	Acc: 34.6% (3461/10000)
[Test]  Epoch: 46	Loss: 0.044504	Acc: 34.5% (3454/10000)
[Test]  Epoch: 47	Loss: 0.044428	Acc: 34.5% (3455/10000)
[Test]  Epoch: 48	Loss: 0.044343	Acc: 34.5% (3450/10000)
[Test]  Epoch: 49	Loss: 0.044358	Acc: 34.7% (3466/10000)
[Test]  Epoch: 50	Loss: 0.044380	Acc: 34.6% (3465/10000)
[Test]  Epoch: 51	Loss: 0.044351	Acc: 34.9% (3486/10000)
[Test]  Epoch: 52	Loss: 0.044276	Acc: 34.6% (3461/10000)
[Test]  Epoch: 53	Loss: 0.044377	Acc: 34.7% (3468/10000)
[Test]  Epoch: 54	Loss: 0.044423	Acc: 34.6% (3462/10000)
[Test]  Epoch: 55	Loss: 0.044300	Acc: 34.6% (3462/10000)
[Test]  Epoch: 56	Loss: 0.044304	Acc: 34.7% (3468/10000)
[Test]  Epoch: 57	Loss: 0.044184	Acc: 34.5% (3451/10000)
[Test]  Epoch: 58	Loss: 0.044278	Acc: 34.6% (3460/10000)
[Test]  Epoch: 59	Loss: 0.044247	Acc: 34.7% (3469/10000)
[Test]  Epoch: 60	Loss: 0.044283	Acc: 34.7% (3473/10000)
[Test]  Epoch: 61	Loss: 0.044280	Acc: 34.6% (3465/10000)
[Test]  Epoch: 62	Loss: 0.044333	Acc: 34.6% (3459/10000)
[Test]  Epoch: 63	Loss: 0.044312	Acc: 34.6% (3464/10000)
[Test]  Epoch: 64	Loss: 0.044267	Acc: 34.5% (3450/10000)
[Test]  Epoch: 65	Loss: 0.044288	Acc: 34.6% (3465/10000)
[Test]  Epoch: 66	Loss: 0.044123	Acc: 34.6% (3460/10000)
[Test]  Epoch: 67	Loss: 0.044286	Acc: 34.7% (3468/10000)
[Test]  Epoch: 68	Loss: 0.044300	Acc: 34.8% (3483/10000)
[Test]  Epoch: 69	Loss: 0.044240	Acc: 34.7% (3468/10000)
[Test]  Epoch: 70	Loss: 0.044275	Acc: 34.8% (3476/10000)
[Test]  Epoch: 71	Loss: 0.044157	Acc: 34.8% (3484/10000)
[Test]  Epoch: 72	Loss: 0.044248	Acc: 34.7% (3472/10000)
[Test]  Epoch: 73	Loss: 0.044168	Acc: 34.8% (3479/10000)
[Test]  Epoch: 74	Loss: 0.044207	Acc: 34.8% (3478/10000)
[Test]  Epoch: 75	Loss: 0.044165	Acc: 34.7% (3471/10000)
[Test]  Epoch: 76	Loss: 0.044281	Acc: 34.7% (3473/10000)
[Test]  Epoch: 77	Loss: 0.044202	Acc: 34.6% (3465/10000)
[Test]  Epoch: 78	Loss: 0.044239	Acc: 34.7% (3472/10000)
[Test]  Epoch: 79	Loss: 0.044224	Acc: 34.7% (3474/10000)
[Test]  Epoch: 80	Loss: 0.044245	Acc: 34.8% (3476/10000)
[Test]  Epoch: 81	Loss: 0.044333	Acc: 34.5% (3450/10000)
[Test]  Epoch: 82	Loss: 0.044261	Acc: 34.6% (3464/10000)
[Test]  Epoch: 83	Loss: 0.044324	Acc: 34.7% (3474/10000)
[Test]  Epoch: 84	Loss: 0.044348	Acc: 34.7% (3471/10000)
[Test]  Epoch: 85	Loss: 0.044217	Acc: 34.6% (3459/10000)
[Test]  Epoch: 86	Loss: 0.044212	Acc: 34.7% (3472/10000)
[Test]  Epoch: 87	Loss: 0.044148	Acc: 34.8% (3477/10000)
[Test]  Epoch: 88	Loss: 0.044174	Acc: 34.8% (3479/10000)
[Test]  Epoch: 89	Loss: 0.044238	Acc: 34.5% (3450/10000)
[Test]  Epoch: 90	Loss: 0.044228	Acc: 34.5% (3449/10000)
[Test]  Epoch: 91	Loss: 0.044295	Acc: 34.7% (3466/10000)
[Test]  Epoch: 92	Loss: 0.044197	Acc: 34.8% (3478/10000)
[Test]  Epoch: 93	Loss: 0.044161	Acc: 34.9% (3485/10000)
[Test]  Epoch: 94	Loss: 0.044095	Acc: 34.7% (3472/10000)
[Test]  Epoch: 95	Loss: 0.044253	Acc: 34.7% (3471/10000)
[Test]  Epoch: 96	Loss: 0.044326	Acc: 34.6% (3465/10000)
[Test]  Epoch: 97	Loss: 0.044347	Acc: 34.5% (3455/10000)
[Test]  Epoch: 98	Loss: 0.044126	Acc: 34.8% (3479/10000)
[Test]  Epoch: 99	Loss: 0.044329	Acc: 34.6% (3465/10000)
[Test]  Epoch: 100	Loss: 0.044254	Acc: 34.6% (3462/10000)
===========finish==========
['2024-08-19', '16:52:30.834623', '100', 'test', '0.04425404036045075', '34.62', '34.86']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.067245	Acc: 18.5% (1853/10000)
[Test]  Epoch: 2	Loss: 0.048581	Acc: 29.2% (2920/10000)
[Test]  Epoch: 3	Loss: 0.046596	Acc: 30.8% (3084/10000)
[Test]  Epoch: 4	Loss: 0.046076	Acc: 31.3% (3131/10000)
[Test]  Epoch: 5	Loss: 0.045947	Acc: 31.4% (3141/10000)
[Test]  Epoch: 6	Loss: 0.046102	Acc: 31.6% (3156/10000)
[Test]  Epoch: 7	Loss: 0.045882	Acc: 31.5% (3146/10000)
[Test]  Epoch: 8	Loss: 0.046056	Acc: 31.4% (3139/10000)
[Test]  Epoch: 9	Loss: 0.046152	Acc: 31.6% (3156/10000)
[Test]  Epoch: 10	Loss: 0.045890	Acc: 31.7% (3174/10000)
[Test]  Epoch: 11	Loss: 0.045705	Acc: 32.0% (3204/10000)
[Test]  Epoch: 12	Loss: 0.045610	Acc: 32.2% (3219/10000)
[Test]  Epoch: 13	Loss: 0.045772	Acc: 32.1% (3212/10000)
[Test]  Epoch: 14	Loss: 0.045610	Acc: 32.0% (3198/10000)
[Test]  Epoch: 15	Loss: 0.045636	Acc: 32.0% (3200/10000)
[Test]  Epoch: 16	Loss: 0.045637	Acc: 32.0% (3196/10000)
[Test]  Epoch: 17	Loss: 0.045495	Acc: 32.0% (3204/10000)
[Test]  Epoch: 18	Loss: 0.045743	Acc: 31.9% (3185/10000)
[Test]  Epoch: 19	Loss: 0.045545	Acc: 32.0% (3205/10000)
[Test]  Epoch: 20	Loss: 0.045491	Acc: 32.1% (3206/10000)
[Test]  Epoch: 21	Loss: 0.045396	Acc: 32.3% (3227/10000)
[Test]  Epoch: 22	Loss: 0.045378	Acc: 32.4% (3243/10000)
[Test]  Epoch: 23	Loss: 0.045405	Acc: 32.2% (3216/10000)
[Test]  Epoch: 24	Loss: 0.045458	Acc: 32.3% (3227/10000)
[Test]  Epoch: 25	Loss: 0.045444	Acc: 32.2% (3223/10000)
[Test]  Epoch: 26	Loss: 0.045270	Acc: 32.5% (3249/10000)
[Test]  Epoch: 27	Loss: 0.045222	Acc: 32.5% (3246/10000)
[Test]  Epoch: 28	Loss: 0.045215	Acc: 32.6% (3256/10000)
[Test]  Epoch: 29	Loss: 0.045310	Acc: 32.5% (3252/10000)
[Test]  Epoch: 30	Loss: 0.045337	Acc: 32.4% (3237/10000)
[Test]  Epoch: 31	Loss: 0.045272	Acc: 32.4% (3238/10000)
[Test]  Epoch: 32	Loss: 0.045259	Acc: 32.5% (3255/10000)
[Test]  Epoch: 33	Loss: 0.045183	Acc: 32.6% (3261/10000)
[Test]  Epoch: 34	Loss: 0.045242	Acc: 32.4% (3243/10000)
[Test]  Epoch: 35	Loss: 0.045096	Acc: 32.5% (3247/10000)
[Test]  Epoch: 36	Loss: 0.045055	Acc: 32.4% (3244/10000)
[Test]  Epoch: 37	Loss: 0.045143	Acc: 32.4% (3236/10000)
[Test]  Epoch: 38	Loss: 0.045045	Acc: 32.6% (3259/10000)
[Test]  Epoch: 39	Loss: 0.045077	Acc: 32.3% (3232/10000)
[Test]  Epoch: 40	Loss: 0.045044	Acc: 32.6% (3257/10000)
[Test]  Epoch: 41	Loss: 0.045035	Acc: 32.6% (3258/10000)
[Test]  Epoch: 42	Loss: 0.044916	Acc: 32.5% (3254/10000)
[Test]  Epoch: 43	Loss: 0.045059	Acc: 32.5% (3250/10000)
[Test]  Epoch: 44	Loss: 0.044998	Acc: 32.5% (3245/10000)
[Test]  Epoch: 45	Loss: 0.044956	Acc: 32.8% (3279/10000)
[Test]  Epoch: 46	Loss: 0.044980	Acc: 32.7% (3274/10000)
[Test]  Epoch: 47	Loss: 0.044912	Acc: 32.7% (3273/10000)
[Test]  Epoch: 48	Loss: 0.044940	Acc: 32.8% (3278/10000)
[Test]  Epoch: 49	Loss: 0.044933	Acc: 32.9% (3285/10000)
[Test]  Epoch: 50	Loss: 0.044834	Acc: 33.0% (3300/10000)
[Test]  Epoch: 51	Loss: 0.044819	Acc: 32.6% (3265/10000)
[Test]  Epoch: 52	Loss: 0.044860	Acc: 32.7% (3268/10000)
[Test]  Epoch: 53	Loss: 0.044927	Acc: 32.9% (3294/10000)
[Test]  Epoch: 54	Loss: 0.044917	Acc: 33.0% (3301/10000)
[Test]  Epoch: 55	Loss: 0.044780	Acc: 32.9% (3293/10000)
[Test]  Epoch: 56	Loss: 0.044831	Acc: 33.1% (3306/10000)
[Test]  Epoch: 57	Loss: 0.044671	Acc: 33.1% (3312/10000)
[Test]  Epoch: 58	Loss: 0.044712	Acc: 33.1% (3313/10000)
[Test]  Epoch: 59	Loss: 0.044824	Acc: 33.0% (3296/10000)
[Test]  Epoch: 60	Loss: 0.044839	Acc: 32.7% (3267/10000)
[Test]  Epoch: 61	Loss: 0.044769	Acc: 32.8% (3278/10000)
[Test]  Epoch: 62	Loss: 0.044820	Acc: 33.0% (3297/10000)
[Test]  Epoch: 63	Loss: 0.044794	Acc: 32.8% (3276/10000)
[Test]  Epoch: 64	Loss: 0.044769	Acc: 32.7% (3271/10000)
[Test]  Epoch: 65	Loss: 0.044855	Acc: 33.0% (3302/10000)
[Test]  Epoch: 66	Loss: 0.044753	Acc: 32.9% (3289/10000)
[Test]  Epoch: 67	Loss: 0.044796	Acc: 32.9% (3289/10000)
[Test]  Epoch: 68	Loss: 0.044824	Acc: 33.0% (3300/10000)
[Test]  Epoch: 69	Loss: 0.044797	Acc: 32.9% (3285/10000)
[Test]  Epoch: 70	Loss: 0.044836	Acc: 32.8% (3276/10000)
[Test]  Epoch: 71	Loss: 0.044722	Acc: 33.0% (3299/10000)
[Test]  Epoch: 72	Loss: 0.044802	Acc: 32.7% (3274/10000)
[Test]  Epoch: 73	Loss: 0.044740	Acc: 33.0% (3301/10000)
[Test]  Epoch: 74	Loss: 0.044713	Acc: 33.0% (3304/10000)
[Test]  Epoch: 75	Loss: 0.044680	Acc: 33.0% (3296/10000)
[Test]  Epoch: 76	Loss: 0.044774	Acc: 32.9% (3289/10000)
[Test]  Epoch: 77	Loss: 0.044705	Acc: 32.9% (3285/10000)
[Test]  Epoch: 78	Loss: 0.044750	Acc: 32.9% (3293/10000)
[Test]  Epoch: 79	Loss: 0.044751	Acc: 32.7% (3267/10000)
[Test]  Epoch: 80	Loss: 0.044816	Acc: 32.8% (3278/10000)
[Test]  Epoch: 81	Loss: 0.044800	Acc: 33.1% (3315/10000)
[Test]  Epoch: 82	Loss: 0.044880	Acc: 33.0% (3297/10000)
[Test]  Epoch: 83	Loss: 0.044886	Acc: 32.5% (3250/10000)
[Test]  Epoch: 84	Loss: 0.044832	Acc: 32.9% (3294/10000)
[Test]  Epoch: 85	Loss: 0.044714	Acc: 33.1% (3309/10000)
[Test]  Epoch: 86	Loss: 0.044698	Acc: 33.1% (3307/10000)
[Test]  Epoch: 87	Loss: 0.044628	Acc: 33.0% (3301/10000)
[Test]  Epoch: 88	Loss: 0.044767	Acc: 32.9% (3292/10000)
[Test]  Epoch: 89	Loss: 0.044808	Acc: 32.9% (3292/10000)
[Test]  Epoch: 90	Loss: 0.044778	Acc: 32.9% (3285/10000)
[Test]  Epoch: 91	Loss: 0.044747	Acc: 33.1% (3307/10000)
[Test]  Epoch: 92	Loss: 0.044811	Acc: 33.0% (3300/10000)
[Test]  Epoch: 93	Loss: 0.044648	Acc: 33.0% (3304/10000)
[Test]  Epoch: 94	Loss: 0.044710	Acc: 33.0% (3300/10000)
[Test]  Epoch: 95	Loss: 0.044773	Acc: 33.0% (3304/10000)
[Test]  Epoch: 96	Loss: 0.044800	Acc: 33.2% (3321/10000)
[Test]  Epoch: 97	Loss: 0.044937	Acc: 33.1% (3306/10000)
[Test]  Epoch: 98	Loss: 0.044706	Acc: 33.1% (3311/10000)
[Test]  Epoch: 99	Loss: 0.044842	Acc: 33.0% (3298/10000)
[Test]  Epoch: 100	Loss: 0.044774	Acc: 32.8% (3282/10000)
===========finish==========
['2024-08-19', '16:56:54.428993', '100', 'test', '0.04477425401210785', '32.82', '33.21']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.062474	Acc: 11.8% (1183/10000)
[Test]  Epoch: 2	Loss: 0.049473	Acc: 21.5% (2154/10000)
[Test]  Epoch: 3	Loss: 0.047842	Acc: 23.9% (2388/10000)
[Test]  Epoch: 4	Loss: 0.047313	Acc: 24.4% (2443/10000)
[Test]  Epoch: 5	Loss: 0.046888	Acc: 24.8% (2475/10000)
[Test]  Epoch: 6	Loss: 0.046683	Acc: 24.9% (2490/10000)
[Test]  Epoch: 7	Loss: 0.046720	Acc: 25.2% (2522/10000)
[Test]  Epoch: 8	Loss: 0.046570	Acc: 25.3% (2530/10000)
[Test]  Epoch: 9	Loss: 0.046657	Acc: 25.4% (2536/10000)
[Test]  Epoch: 10	Loss: 0.046535	Acc: 25.6% (2555/10000)
[Test]  Epoch: 11	Loss: 0.046480	Acc: 25.5% (2548/10000)
[Test]  Epoch: 12	Loss: 0.046423	Acc: 25.7% (2568/10000)
[Test]  Epoch: 13	Loss: 0.046498	Acc: 25.8% (2575/10000)
[Test]  Epoch: 14	Loss: 0.046424	Acc: 25.7% (2567/10000)
[Test]  Epoch: 15	Loss: 0.046422	Acc: 25.9% (2592/10000)
[Test]  Epoch: 16	Loss: 0.046467	Acc: 25.9% (2588/10000)
[Test]  Epoch: 17	Loss: 0.046348	Acc: 26.1% (2610/10000)
[Test]  Epoch: 18	Loss: 0.046525	Acc: 25.8% (2581/10000)
[Test]  Epoch: 19	Loss: 0.046485	Acc: 25.9% (2586/10000)
[Test]  Epoch: 20	Loss: 0.046369	Acc: 26.1% (2610/10000)
[Test]  Epoch: 21	Loss: 0.046315	Acc: 25.8% (2580/10000)
[Test]  Epoch: 22	Loss: 0.046352	Acc: 26.1% (2605/10000)
[Test]  Epoch: 23	Loss: 0.046338	Acc: 26.1% (2609/10000)
[Test]  Epoch: 24	Loss: 0.046316	Acc: 25.9% (2595/10000)
[Test]  Epoch: 25	Loss: 0.046294	Acc: 26.2% (2619/10000)
[Test]  Epoch: 26	Loss: 0.046267	Acc: 26.2% (2621/10000)
[Test]  Epoch: 27	Loss: 0.046204	Acc: 26.2% (2622/10000)
[Test]  Epoch: 28	Loss: 0.046258	Acc: 26.4% (2636/10000)
[Test]  Epoch: 29	Loss: 0.046378	Acc: 26.3% (2633/10000)
[Test]  Epoch: 30	Loss: 0.046298	Acc: 26.4% (2645/10000)
[Test]  Epoch: 31	Loss: 0.046325	Acc: 26.4% (2641/10000)
[Test]  Epoch: 32	Loss: 0.046253	Acc: 26.6% (2665/10000)
[Test]  Epoch: 33	Loss: 0.046274	Acc: 26.6% (2657/10000)
[Test]  Epoch: 34	Loss: 0.046310	Acc: 26.4% (2636/10000)
[Test]  Epoch: 35	Loss: 0.046231	Acc: 26.4% (2642/10000)
[Test]  Epoch: 36	Loss: 0.046180	Acc: 26.5% (2650/10000)
[Test]  Epoch: 37	Loss: 0.046299	Acc: 26.2% (2624/10000)
[Test]  Epoch: 38	Loss: 0.046250	Acc: 26.3% (2632/10000)
[Test]  Epoch: 39	Loss: 0.046207	Acc: 26.6% (2656/10000)
[Test]  Epoch: 40	Loss: 0.046173	Acc: 26.6% (2659/10000)
[Test]  Epoch: 41	Loss: 0.046174	Acc: 26.6% (2660/10000)
[Test]  Epoch: 42	Loss: 0.046169	Acc: 26.8% (2676/10000)
[Test]  Epoch: 43	Loss: 0.046194	Acc: 26.6% (2661/10000)
[Test]  Epoch: 44	Loss: 0.046158	Acc: 26.7% (2666/10000)
[Test]  Epoch: 45	Loss: 0.046223	Acc: 26.6% (2663/10000)
[Test]  Epoch: 46	Loss: 0.046240	Acc: 26.6% (2660/10000)
[Test]  Epoch: 47	Loss: 0.046196	Acc: 26.4% (2636/10000)
[Test]  Epoch: 48	Loss: 0.046199	Acc: 26.5% (2648/10000)
[Test]  Epoch: 49	Loss: 0.046238	Acc: 26.5% (2649/10000)
[Test]  Epoch: 50	Loss: 0.046142	Acc: 26.9% (2690/10000)
[Test]  Epoch: 51	Loss: 0.046222	Acc: 26.8% (2678/10000)
[Test]  Epoch: 52	Loss: 0.046196	Acc: 26.8% (2682/10000)
[Test]  Epoch: 53	Loss: 0.046265	Acc: 26.7% (2671/10000)
[Test]  Epoch: 54	Loss: 0.046223	Acc: 26.8% (2675/10000)
[Test]  Epoch: 55	Loss: 0.046175	Acc: 26.8% (2680/10000)
[Test]  Epoch: 56	Loss: 0.046120	Acc: 26.9% (2692/10000)
[Test]  Epoch: 57	Loss: 0.046119	Acc: 26.9% (2691/10000)
[Test]  Epoch: 58	Loss: 0.046171	Acc: 26.9% (2688/10000)
[Test]  Epoch: 59	Loss: 0.046146	Acc: 26.8% (2677/10000)
[Test]  Epoch: 60	Loss: 0.046178	Acc: 26.8% (2684/10000)
[Test]  Epoch: 61	Loss: 0.046191	Acc: 27.0% (2699/10000)
[Test]  Epoch: 62	Loss: 0.046185	Acc: 26.6% (2662/10000)
[Test]  Epoch: 63	Loss: 0.046175	Acc: 26.9% (2695/10000)
[Test]  Epoch: 64	Loss: 0.046111	Acc: 26.8% (2682/10000)
[Test]  Epoch: 65	Loss: 0.046210	Acc: 26.7% (2674/10000)
[Test]  Epoch: 66	Loss: 0.046105	Acc: 26.9% (2690/10000)
[Test]  Epoch: 67	Loss: 0.046146	Acc: 26.8% (2679/10000)
[Test]  Epoch: 68	Loss: 0.046152	Acc: 26.9% (2695/10000)
[Test]  Epoch: 69	Loss: 0.046163	Acc: 26.9% (2686/10000)
[Test]  Epoch: 70	Loss: 0.046170	Acc: 26.9% (2693/10000)
[Test]  Epoch: 71	Loss: 0.046151	Acc: 26.9% (2690/10000)
[Test]  Epoch: 72	Loss: 0.046168	Acc: 27.0% (2696/10000)
[Test]  Epoch: 73	Loss: 0.046072	Acc: 27.0% (2702/10000)
[Test]  Epoch: 74	Loss: 0.046136	Acc: 26.9% (2690/10000)
[Test]  Epoch: 75	Loss: 0.046180	Acc: 26.9% (2690/10000)
[Test]  Epoch: 76	Loss: 0.046074	Acc: 27.0% (2699/10000)
[Test]  Epoch: 77	Loss: 0.046082	Acc: 26.9% (2685/10000)
[Test]  Epoch: 78	Loss: 0.046139	Acc: 26.7% (2672/10000)
[Test]  Epoch: 79	Loss: 0.046132	Acc: 26.9% (2688/10000)
[Test]  Epoch: 80	Loss: 0.046144	Acc: 26.8% (2675/10000)
[Test]  Epoch: 81	Loss: 0.046097	Acc: 27.0% (2698/10000)
[Test]  Epoch: 82	Loss: 0.046202	Acc: 27.0% (2696/10000)
[Test]  Epoch: 83	Loss: 0.046131	Acc: 26.8% (2684/10000)
[Test]  Epoch: 84	Loss: 0.046106	Acc: 27.1% (2710/10000)
[Test]  Epoch: 85	Loss: 0.046066	Acc: 26.9% (2690/10000)
[Test]  Epoch: 86	Loss: 0.046108	Acc: 26.8% (2683/10000)
[Test]  Epoch: 87	Loss: 0.045974	Acc: 27.1% (2707/10000)
[Test]  Epoch: 88	Loss: 0.046061	Acc: 26.8% (2684/10000)
[Test]  Epoch: 89	Loss: 0.046170	Acc: 26.9% (2692/10000)
[Test]  Epoch: 90	Loss: 0.046228	Acc: 26.9% (2692/10000)
[Test]  Epoch: 91	Loss: 0.046113	Acc: 27.0% (2701/10000)
[Test]  Epoch: 92	Loss: 0.046174	Acc: 26.7% (2666/10000)
[Test]  Epoch: 93	Loss: 0.046103	Acc: 26.8% (2678/10000)
[Test]  Epoch: 94	Loss: 0.046114	Acc: 27.1% (2705/10000)
[Test]  Epoch: 95	Loss: 0.046057	Acc: 26.9% (2686/10000)
[Test]  Epoch: 96	Loss: 0.046115	Acc: 26.9% (2687/10000)
[Test]  Epoch: 97	Loss: 0.046172	Acc: 26.9% (2685/10000)
[Test]  Epoch: 98	Loss: 0.046079	Acc: 26.9% (2695/10000)
[Test]  Epoch: 99	Loss: 0.046195	Acc: 26.9% (2692/10000)
[Test]  Epoch: 100	Loss: 0.046191	Acc: 26.8% (2678/10000)
===========finish==========
['2024-08-19', '17:01:26.581593', '100', 'test', '0.04619108364582062', '26.78', '27.1']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.074144	Acc: 4.3% (432/10000)
[Test]  Epoch: 2	Loss: 0.064667	Acc: 11.1% (1109/10000)
[Test]  Epoch: 3	Loss: 0.060909	Acc: 14.5% (1449/10000)
[Test]  Epoch: 4	Loss: 0.058957	Acc: 16.2% (1621/10000)
[Test]  Epoch: 5	Loss: 0.057260	Acc: 18.0% (1802/10000)
[Test]  Epoch: 6	Loss: 0.056277	Acc: 18.8% (1879/10000)
[Test]  Epoch: 7	Loss: 0.056145	Acc: 18.6% (1865/10000)
[Test]  Epoch: 8	Loss: 0.055698	Acc: 19.6% (1957/10000)
[Test]  Epoch: 9	Loss: 0.055787	Acc: 19.4% (1937/10000)
[Test]  Epoch: 10	Loss: 0.055514	Acc: 19.8% (1981/10000)
[Test]  Epoch: 11	Loss: 0.055526	Acc: 20.0% (2000/10000)
[Test]  Epoch: 12	Loss: 0.055379	Acc: 20.1% (2007/10000)
[Test]  Epoch: 13	Loss: 0.055398	Acc: 20.2% (2025/10000)
[Test]  Epoch: 14	Loss: 0.055432	Acc: 20.1% (2013/10000)
[Test]  Epoch: 15	Loss: 0.055304	Acc: 20.3% (2034/10000)
[Test]  Epoch: 16	Loss: 0.055394	Acc: 20.3% (2030/10000)
[Test]  Epoch: 17	Loss: 0.055306	Acc: 20.4% (2038/10000)
[Test]  Epoch: 18	Loss: 0.055429	Acc: 20.3% (2026/10000)
[Test]  Epoch: 19	Loss: 0.055435	Acc: 20.3% (2030/10000)
[Test]  Epoch: 20	Loss: 0.055277	Acc: 20.6% (2064/10000)
[Test]  Epoch: 21	Loss: 0.055367	Acc: 20.4% (2042/10000)
[Test]  Epoch: 22	Loss: 0.055330	Acc: 20.6% (2057/10000)
[Test]  Epoch: 23	Loss: 0.055281	Acc: 20.6% (2055/10000)
[Test]  Epoch: 24	Loss: 0.055318	Acc: 20.4% (2039/10000)
[Test]  Epoch: 25	Loss: 0.055269	Acc: 20.6% (2057/10000)
[Test]  Epoch: 26	Loss: 0.055365	Acc: 20.6% (2060/10000)
[Test]  Epoch: 27	Loss: 0.055231	Acc: 20.7% (2073/10000)
[Test]  Epoch: 28	Loss: 0.055226	Acc: 20.7% (2068/10000)
[Test]  Epoch: 29	Loss: 0.055294	Acc: 20.8% (2075/10000)
[Test]  Epoch: 30	Loss: 0.055208	Acc: 20.8% (2082/10000)
[Test]  Epoch: 31	Loss: 0.055295	Acc: 20.8% (2082/10000)
[Test]  Epoch: 32	Loss: 0.055142	Acc: 21.0% (2103/10000)
[Test]  Epoch: 33	Loss: 0.055297	Acc: 20.9% (2090/10000)
[Test]  Epoch: 34	Loss: 0.055209	Acc: 21.1% (2105/10000)
[Test]  Epoch: 35	Loss: 0.055146	Acc: 21.0% (2096/10000)
[Test]  Epoch: 36	Loss: 0.055174	Acc: 21.1% (2113/10000)
[Test]  Epoch: 37	Loss: 0.055084	Acc: 20.9% (2086/10000)
[Test]  Epoch: 38	Loss: 0.055096	Acc: 21.0% (2103/10000)
[Test]  Epoch: 39	Loss: 0.055126	Acc: 21.2% (2120/10000)
[Test]  Epoch: 40	Loss: 0.055088	Acc: 20.9% (2090/10000)
[Test]  Epoch: 41	Loss: 0.055098	Acc: 21.1% (2107/10000)
[Test]  Epoch: 42	Loss: 0.055015	Acc: 21.2% (2121/10000)
[Test]  Epoch: 43	Loss: 0.055004	Acc: 21.4% (2139/10000)
[Test]  Epoch: 44	Loss: 0.054972	Acc: 21.2% (2125/10000)
[Test]  Epoch: 45	Loss: 0.055098	Acc: 21.0% (2104/10000)
[Test]  Epoch: 46	Loss: 0.055104	Acc: 21.1% (2110/10000)
[Test]  Epoch: 47	Loss: 0.055118	Acc: 21.3% (2128/10000)
[Test]  Epoch: 48	Loss: 0.055053	Acc: 21.3% (2126/10000)
[Test]  Epoch: 49	Loss: 0.055026	Acc: 21.1% (2111/10000)
[Test]  Epoch: 50	Loss: 0.054957	Acc: 21.2% (2121/10000)
[Test]  Epoch: 51	Loss: 0.055104	Acc: 21.3% (2127/10000)
[Test]  Epoch: 52	Loss: 0.055040	Acc: 21.2% (2124/10000)
[Test]  Epoch: 53	Loss: 0.055057	Acc: 21.3% (2130/10000)
[Test]  Epoch: 54	Loss: 0.055014	Acc: 21.4% (2145/10000)
[Test]  Epoch: 55	Loss: 0.054866	Acc: 21.6% (2158/10000)
[Test]  Epoch: 56	Loss: 0.054902	Acc: 21.4% (2141/10000)
[Test]  Epoch: 57	Loss: 0.054913	Acc: 21.3% (2128/10000)
[Test]  Epoch: 58	Loss: 0.055010	Acc: 21.4% (2137/10000)
[Test]  Epoch: 59	Loss: 0.054971	Acc: 21.3% (2132/10000)
[Test]  Epoch: 60	Loss: 0.054981	Acc: 21.2% (2124/10000)
[Test]  Epoch: 61	Loss: 0.055019	Acc: 21.1% (2115/10000)
[Test]  Epoch: 62	Loss: 0.054895	Acc: 21.5% (2150/10000)
[Test]  Epoch: 63	Loss: 0.054969	Acc: 21.1% (2114/10000)
[Test]  Epoch: 64	Loss: 0.054920	Acc: 21.4% (2141/10000)
[Test]  Epoch: 65	Loss: 0.054945	Acc: 21.4% (2138/10000)
[Test]  Epoch: 66	Loss: 0.054898	Acc: 21.5% (2147/10000)
[Test]  Epoch: 67	Loss: 0.054873	Acc: 21.4% (2138/10000)
[Test]  Epoch: 68	Loss: 0.054873	Acc: 21.4% (2138/10000)
[Test]  Epoch: 69	Loss: 0.054918	Acc: 21.5% (2149/10000)
[Test]  Epoch: 70	Loss: 0.054863	Acc: 21.3% (2132/10000)
[Test]  Epoch: 71	Loss: 0.054847	Acc: 21.3% (2132/10000)
[Test]  Epoch: 72	Loss: 0.054908	Acc: 21.3% (2133/10000)
[Test]  Epoch: 73	Loss: 0.054825	Acc: 21.5% (2151/10000)
[Test]  Epoch: 74	Loss: 0.054860	Acc: 21.4% (2136/10000)
[Test]  Epoch: 75	Loss: 0.054924	Acc: 21.2% (2120/10000)
[Test]  Epoch: 76	Loss: 0.054905	Acc: 21.5% (2149/10000)
[Test]  Epoch: 77	Loss: 0.054814	Acc: 21.6% (2164/10000)
[Test]  Epoch: 78	Loss: 0.054845	Acc: 21.4% (2136/10000)
[Test]  Epoch: 79	Loss: 0.054894	Acc: 21.3% (2133/10000)
[Test]  Epoch: 80	Loss: 0.054986	Acc: 21.3% (2126/10000)
[Test]  Epoch: 81	Loss: 0.054895	Acc: 21.3% (2134/10000)
[Test]  Epoch: 82	Loss: 0.054954	Acc: 21.2% (2123/10000)
[Test]  Epoch: 83	Loss: 0.054867	Acc: 21.4% (2138/10000)
[Test]  Epoch: 84	Loss: 0.054814	Acc: 21.4% (2136/10000)
[Test]  Epoch: 85	Loss: 0.054860	Acc: 21.3% (2131/10000)
[Test]  Epoch: 86	Loss: 0.054899	Acc: 21.1% (2112/10000)
[Test]  Epoch: 87	Loss: 0.054770	Acc: 21.3% (2129/10000)
[Test]  Epoch: 88	Loss: 0.054793	Acc: 21.4% (2141/10000)
[Test]  Epoch: 89	Loss: 0.054934	Acc: 21.4% (2136/10000)
[Test]  Epoch: 90	Loss: 0.054930	Acc: 21.1% (2111/10000)
[Test]  Epoch: 91	Loss: 0.054876	Acc: 21.3% (2128/10000)
[Test]  Epoch: 92	Loss: 0.054818	Acc: 21.4% (2143/10000)
[Test]  Epoch: 93	Loss: 0.054786	Acc: 21.5% (2148/10000)
[Test]  Epoch: 94	Loss: 0.054833	Acc: 21.5% (2151/10000)
[Test]  Epoch: 95	Loss: 0.054789	Acc: 21.4% (2135/10000)
[Test]  Epoch: 96	Loss: 0.054909	Acc: 21.3% (2130/10000)
[Test]  Epoch: 97	Loss: 0.054929	Acc: 21.4% (2143/10000)
[Test]  Epoch: 98	Loss: 0.054808	Acc: 21.5% (2149/10000)
[Test]  Epoch: 99	Loss: 0.054882	Acc: 21.4% (2137/10000)
[Test]  Epoch: 100	Loss: 0.054944	Acc: 21.3% (2129/10000)
===========finish==========
['2024-08-19', '17:06:00.901530', '100', 'test', '0.05494412579536438', '21.29', '21.64']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('conv1.weight', 0.0), ('bn1.weight', 0.0), ('bn1.bias', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.bn1.weight', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.bn2.weight', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.conv3.weight', 0.0), ('layer1.0.bn3.weight', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.0.weight', 0.0), ('layer1.0.downsample.1.weight', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.bn1.weight', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.bn2.weight', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.conv3.weight', 0.0), ('layer1.1.bn3.weight', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.conv1.weight', 0.0), ('layer1.2.bn1.weight', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.conv2.weight', 0.0), ('layer1.2.bn2.weight', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.conv3.weight', 0.0), ('layer1.2.bn3.weight', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.bn1.weight', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.bn2.weight', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.conv3.weight', 0.0), ('layer2.0.bn3.weight', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.0.weight', 0.0), ('layer2.0.downsample.1.weight', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.bn1.weight', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.bn2.weight', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.conv3.weight', 0.0), ('layer2.1.bn3.weight', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.conv1.weight', 0.0), ('layer2.2.bn1.weight', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.conv2.weight', 0.0), ('layer2.2.bn2.weight', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.conv3.weight', 0.0), ('layer2.2.bn3.weight', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.conv1.weight', 0.0), ('layer2.3.bn1.weight', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.conv2.weight', 0.0), ('layer2.3.bn2.weight', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.conv3.weight', 0.0), ('layer2.3.bn3.weight', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.bn1.weight', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.bn2.weight', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.conv3.weight', 0.0), ('layer3.0.bn3.weight', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.0.weight', 0.0), ('layer3.0.downsample.1.weight', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.bn1.weight', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.bn2.weight', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.conv3.weight', 0.0), ('layer3.1.bn3.weight', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.conv1.weight', 0.0), ('layer3.2.bn1.weight', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.conv2.weight', 0.0), ('layer3.2.bn2.weight', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.conv3.weight', 0.0), ('layer3.2.bn3.weight', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.conv1.weight', 0.0), ('layer3.3.bn1.weight', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.conv2.weight', 0.0), ('layer3.3.bn2.weight', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.conv3.weight', 0.0), ('layer3.3.bn3.weight', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.conv1.weight', 0.0), ('layer3.4.bn1.weight', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.conv2.weight', 0.0), ('layer3.4.bn2.weight', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.conv3.weight', 0.0), ('layer3.4.bn3.weight', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.conv1.weight', 0.0), ('layer3.5.bn1.weight', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.conv2.weight', 0.0), ('layer3.5.bn2.weight', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.conv3.weight', 0.0), ('layer3.5.bn3.weight', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.bn1.weight', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.bn2.weight', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.conv3.weight', 0.0), ('layer4.0.bn3.weight', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.0.weight', 0.0), ('layer4.0.downsample.1.weight', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.bn1.weight', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.bn2.weight', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.conv3.weight', 0.0), ('layer4.1.bn3.weight', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.conv1.weight', 0.0), ('layer4.2.bn1.weight', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.conv2.weight', 0.0), ('layer4.2.bn2.weight', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.conv3.weight', 0.0), ('layer4.2.bn3.weight', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('conv1.weight', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.conv3.weight', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.conv3.weight', 0.0), ('layer1.2.conv1.weight', 0.0), ('layer1.2.conv2.weight', 0.0), ('layer1.2.conv3.weight', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.conv3.weight', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.conv3.weight', 0.0), ('layer2.2.conv1.weight', 0.0), ('layer2.2.conv2.weight', 0.0), ('layer2.2.conv3.weight', 0.0), ('layer2.3.conv1.weight', 0.0), ('layer2.3.conv2.weight', 0.0), ('layer2.3.conv3.weight', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.conv3.weight', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.conv3.weight', 0.0), ('layer3.2.conv1.weight', 0.0), ('layer3.2.conv2.weight', 0.0), ('layer3.2.conv3.weight', 0.0), ('layer3.3.conv1.weight', 0.0), ('layer3.3.conv2.weight', 0.0), ('layer3.3.conv3.weight', 0.0), ('layer3.4.conv1.weight', 0.0), ('layer3.4.conv2.weight', 0.0), ('layer3.4.conv3.weight', 0.0), ('layer3.5.conv1.weight', 0.0), ('layer3.5.conv2.weight', 0.0), ('layer3.5.conv3.weight', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.conv3.weight', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.conv3.weight', 0.0), ('layer4.2.conv1.weight', 0.0), ('layer4.2.conv2.weight', 0.0), ('layer4.2.conv3.weight', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007164	Acc: 85.8% (8584/10000)
[Test]  Epoch: 2	Loss: 0.006687	Acc: 86.5% (8647/10000)
[Test]  Epoch: 3	Loss: 0.006649	Acc: 86.4% (8643/10000)
[Test]  Epoch: 4	Loss: 0.006606	Acc: 86.7% (8665/10000)
[Test]  Epoch: 5	Loss: 0.006633	Acc: 86.7% (8669/10000)
[Test]  Epoch: 6	Loss: 0.006566	Acc: 86.7% (8667/10000)
[Test]  Epoch: 7	Loss: 0.006472	Acc: 86.8% (8678/10000)
[Test]  Epoch: 8	Loss: 0.006549	Acc: 86.5% (8645/10000)
[Test]  Epoch: 9	Loss: 0.006454	Acc: 86.7% (8669/10000)
[Test]  Epoch: 10	Loss: 0.006414	Acc: 86.8% (8683/10000)
[Test]  Epoch: 11	Loss: 0.006422	Acc: 86.7% (8669/10000)
[Test]  Epoch: 12	Loss: 0.006384	Acc: 86.9% (8687/10000)
[Test]  Epoch: 13	Loss: 0.006378	Acc: 86.8% (8675/10000)
[Test]  Epoch: 14	Loss: 0.006323	Acc: 87.1% (8707/10000)
[Test]  Epoch: 15	Loss: 0.006381	Acc: 86.9% (8688/10000)
[Test]  Epoch: 16	Loss: 0.006339	Acc: 86.9% (8694/10000)
[Test]  Epoch: 17	Loss: 0.006370	Acc: 87.0% (8698/10000)
[Test]  Epoch: 18	Loss: 0.006300	Acc: 86.9% (8691/10000)
[Test]  Epoch: 19	Loss: 0.006277	Acc: 86.8% (8678/10000)
[Test]  Epoch: 20	Loss: 0.006275	Acc: 87.0% (8697/10000)
[Test]  Epoch: 21	Loss: 0.006313	Acc: 86.9% (8691/10000)
[Test]  Epoch: 22	Loss: 0.006269	Acc: 87.0% (8705/10000)
[Test]  Epoch: 23	Loss: 0.006278	Acc: 87.0% (8695/10000)
[Test]  Epoch: 24	Loss: 0.006244	Acc: 87.0% (8699/10000)
[Test]  Epoch: 25	Loss: 0.006281	Acc: 87.0% (8700/10000)
[Test]  Epoch: 26	Loss: 0.006261	Acc: 87.0% (8696/10000)
[Test]  Epoch: 27	Loss: 0.006239	Acc: 87.0% (8699/10000)
[Test]  Epoch: 28	Loss: 0.006227	Acc: 87.0% (8701/10000)
[Test]  Epoch: 29	Loss: 0.006244	Acc: 87.0% (8703/10000)
[Test]  Epoch: 30	Loss: 0.006227	Acc: 87.0% (8695/10000)
[Test]  Epoch: 31	Loss: 0.006213	Acc: 87.0% (8698/10000)
[Test]  Epoch: 32	Loss: 0.006253	Acc: 87.1% (8708/10000)
[Test]  Epoch: 33	Loss: 0.006230	Acc: 87.1% (8711/10000)
[Test]  Epoch: 34	Loss: 0.006220	Acc: 87.0% (8703/10000)
[Test]  Epoch: 35	Loss: 0.006235	Acc: 87.1% (8708/10000)
[Test]  Epoch: 36	Loss: 0.006251	Acc: 86.9% (8694/10000)
[Test]  Epoch: 37	Loss: 0.006231	Acc: 87.0% (8698/10000)
[Test]  Epoch: 38	Loss: 0.006229	Acc: 87.0% (8697/10000)
[Test]  Epoch: 39	Loss: 0.006208	Acc: 87.1% (8714/10000)
[Test]  Epoch: 40	Loss: 0.006219	Acc: 87.0% (8705/10000)
[Test]  Epoch: 41	Loss: 0.006211	Acc: 87.0% (8702/10000)
[Test]  Epoch: 42	Loss: 0.006183	Acc: 87.1% (8712/10000)
[Test]  Epoch: 43	Loss: 0.006216	Acc: 87.0% (8703/10000)
[Test]  Epoch: 44	Loss: 0.006196	Acc: 87.1% (8706/10000)
[Test]  Epoch: 45	Loss: 0.006185	Acc: 87.0% (8704/10000)
[Test]  Epoch: 46	Loss: 0.006186	Acc: 87.0% (8701/10000)
[Test]  Epoch: 47	Loss: 0.006183	Acc: 87.0% (8698/10000)
[Test]  Epoch: 48	Loss: 0.006174	Acc: 87.1% (8711/10000)
[Test]  Epoch: 49	Loss: 0.006180	Acc: 87.1% (8709/10000)
[Test]  Epoch: 50	Loss: 0.006182	Acc: 87.1% (8706/10000)
[Test]  Epoch: 51	Loss: 0.006146	Acc: 87.1% (8713/10000)
[Test]  Epoch: 52	Loss: 0.006157	Acc: 87.1% (8710/10000)
[Test]  Epoch: 53	Loss: 0.006162	Acc: 87.1% (8711/10000)
[Test]  Epoch: 54	Loss: 0.006163	Acc: 87.3% (8726/10000)
[Test]  Epoch: 55	Loss: 0.006181	Acc: 87.0% (8705/10000)
[Test]  Epoch: 56	Loss: 0.006145	Acc: 87.1% (8710/10000)
[Test]  Epoch: 57	Loss: 0.006156	Acc: 86.9% (8694/10000)
[Test]  Epoch: 58	Loss: 0.006168	Acc: 87.0% (8701/10000)
[Test]  Epoch: 59	Loss: 0.006177	Acc: 87.1% (8706/10000)
[Test]  Epoch: 60	Loss: 0.006176	Acc: 87.2% (8718/10000)
[Test]  Epoch: 61	Loss: 0.006162	Acc: 87.0% (8702/10000)
[Test]  Epoch: 62	Loss: 0.006180	Acc: 87.1% (8708/10000)
[Test]  Epoch: 63	Loss: 0.006189	Acc: 87.1% (8707/10000)
[Test]  Epoch: 64	Loss: 0.006152	Acc: 87.2% (8718/10000)
[Test]  Epoch: 65	Loss: 0.006153	Acc: 87.2% (8718/10000)
[Test]  Epoch: 66	Loss: 0.006155	Acc: 87.1% (8707/10000)
[Test]  Epoch: 67	Loss: 0.006154	Acc: 87.2% (8716/10000)
[Test]  Epoch: 68	Loss: 0.006162	Acc: 87.2% (8717/10000)
[Test]  Epoch: 69	Loss: 0.006185	Acc: 87.0% (8698/10000)
[Test]  Epoch: 70	Loss: 0.006162	Acc: 87.1% (8712/10000)
[Test]  Epoch: 71	Loss: 0.006138	Acc: 87.1% (8709/10000)
[Test]  Epoch: 72	Loss: 0.006134	Acc: 87.2% (8718/10000)
[Test]  Epoch: 73	Loss: 0.006155	Acc: 87.1% (8712/10000)
[Test]  Epoch: 74	Loss: 0.006143	Acc: 87.1% (8710/10000)
[Test]  Epoch: 75	Loss: 0.006153	Acc: 87.0% (8700/10000)
[Test]  Epoch: 76	Loss: 0.006136	Acc: 87.3% (8727/10000)
[Test]  Epoch: 77	Loss: 0.006162	Acc: 87.0% (8703/10000)
[Test]  Epoch: 78	Loss: 0.006145	Acc: 87.2% (8716/10000)
[Test]  Epoch: 79	Loss: 0.006149	Acc: 87.1% (8707/10000)
[Test]  Epoch: 80	Loss: 0.006151	Acc: 87.0% (8704/10000)
[Test]  Epoch: 81	Loss: 0.006165	Acc: 87.0% (8704/10000)
[Test]  Epoch: 82	Loss: 0.006165	Acc: 87.0% (8698/10000)
[Test]  Epoch: 83	Loss: 0.006131	Acc: 87.1% (8711/10000)
[Test]  Epoch: 84	Loss: 0.006141	Acc: 87.1% (8711/10000)
[Test]  Epoch: 85	Loss: 0.006149	Acc: 87.1% (8708/10000)
[Test]  Epoch: 86	Loss: 0.006131	Acc: 87.1% (8714/10000)
[Test]  Epoch: 87	Loss: 0.006145	Acc: 87.2% (8717/10000)
[Test]  Epoch: 88	Loss: 0.006143	Acc: 87.1% (8707/10000)
[Test]  Epoch: 89	Loss: 0.006150	Acc: 87.1% (8712/10000)
[Test]  Epoch: 90	Loss: 0.006152	Acc: 87.1% (8713/10000)
[Test]  Epoch: 91	Loss: 0.006167	Acc: 87.0% (8700/10000)
[Test]  Epoch: 92	Loss: 0.006142	Acc: 87.1% (8711/10000)
[Test]  Epoch: 93	Loss: 0.006135	Acc: 87.1% (8712/10000)
[Test]  Epoch: 94	Loss: 0.006155	Acc: 87.0% (8703/10000)
[Test]  Epoch: 95	Loss: 0.006134	Acc: 87.2% (8715/10000)
[Test]  Epoch: 96	Loss: 0.006134	Acc: 87.2% (8717/10000)
[Test]  Epoch: 97	Loss: 0.006131	Acc: 87.2% (8718/10000)
[Test]  Epoch: 98	Loss: 0.006158	Acc: 87.1% (8709/10000)
[Test]  Epoch: 99	Loss: 0.006149	Acc: 87.1% (8710/10000)
[Test]  Epoch: 100	Loss: 0.006174	Acc: 87.0% (8695/10000)
===========finish==========
['2024-08-19', '17:13:16.504991', '100', 'test', '0.006173622617125511', '86.95', '87.27']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.025110	Acc: 53.7% (5371/10000)
[Test]  Epoch: 2	Loss: 0.017118	Acc: 66.0% (6597/10000)
[Test]  Epoch: 3	Loss: 0.016015	Acc: 68.3% (6829/10000)
[Test]  Epoch: 4	Loss: 0.015931	Acc: 68.8% (6879/10000)
[Test]  Epoch: 5	Loss: 0.015776	Acc: 69.0% (6902/10000)
[Test]  Epoch: 6	Loss: 0.015663	Acc: 69.3% (6928/10000)
[Test]  Epoch: 7	Loss: 0.015527	Acc: 69.6% (6956/10000)
[Test]  Epoch: 8	Loss: 0.015602	Acc: 69.3% (6933/10000)
[Test]  Epoch: 9	Loss: 0.015645	Acc: 69.4% (6943/10000)
[Test]  Epoch: 10	Loss: 0.015458	Acc: 69.5% (6947/10000)
[Test]  Epoch: 11	Loss: 0.015568	Acc: 69.4% (6943/10000)
[Test]  Epoch: 12	Loss: 0.015446	Acc: 69.9% (6991/10000)
[Test]  Epoch: 13	Loss: 0.015397	Acc: 69.5% (6955/10000)
[Test]  Epoch: 14	Loss: 0.015329	Acc: 69.9% (6986/10000)
[Test]  Epoch: 15	Loss: 0.015231	Acc: 70.0% (7002/10000)
[Test]  Epoch: 16	Loss: 0.015306	Acc: 69.9% (6992/10000)
[Test]  Epoch: 17	Loss: 0.015219	Acc: 70.0% (7005/10000)
[Test]  Epoch: 18	Loss: 0.015145	Acc: 70.0% (6997/10000)
[Test]  Epoch: 19	Loss: 0.015119	Acc: 70.3% (7027/10000)
[Test]  Epoch: 20	Loss: 0.015113	Acc: 70.2% (7019/10000)
[Test]  Epoch: 21	Loss: 0.015107	Acc: 70.1% (7009/10000)
[Test]  Epoch: 22	Loss: 0.015005	Acc: 70.2% (7025/10000)
[Test]  Epoch: 23	Loss: 0.015099	Acc: 70.1% (7008/10000)
[Test]  Epoch: 24	Loss: 0.015009	Acc: 70.4% (7043/10000)
[Test]  Epoch: 25	Loss: 0.015033	Acc: 70.1% (7011/10000)
[Test]  Epoch: 26	Loss: 0.014994	Acc: 70.5% (7047/10000)
[Test]  Epoch: 27	Loss: 0.014928	Acc: 70.2% (7018/10000)
[Test]  Epoch: 28	Loss: 0.014897	Acc: 70.3% (7031/10000)
[Test]  Epoch: 29	Loss: 0.014935	Acc: 70.3% (7029/10000)
[Test]  Epoch: 30	Loss: 0.014901	Acc: 70.4% (7044/10000)
[Test]  Epoch: 31	Loss: 0.014789	Acc: 70.5% (7046/10000)
[Test]  Epoch: 32	Loss: 0.014965	Acc: 70.1% (7013/10000)
[Test]  Epoch: 33	Loss: 0.014847	Acc: 70.4% (7041/10000)
[Test]  Epoch: 34	Loss: 0.014799	Acc: 70.5% (7051/10000)
[Test]  Epoch: 35	Loss: 0.014776	Acc: 70.5% (7049/10000)
[Test]  Epoch: 36	Loss: 0.014887	Acc: 70.5% (7049/10000)
[Test]  Epoch: 37	Loss: 0.014803	Acc: 70.6% (7060/10000)
[Test]  Epoch: 38	Loss: 0.014877	Acc: 70.3% (7028/10000)
[Test]  Epoch: 39	Loss: 0.014831	Acc: 70.5% (7045/10000)
[Test]  Epoch: 40	Loss: 0.014761	Acc: 70.5% (7046/10000)
[Test]  Epoch: 41	Loss: 0.014777	Acc: 70.5% (7053/10000)
[Test]  Epoch: 42	Loss: 0.014619	Acc: 70.6% (7059/10000)
[Test]  Epoch: 43	Loss: 0.014774	Acc: 70.4% (7039/10000)
[Test]  Epoch: 44	Loss: 0.014617	Acc: 70.7% (7065/10000)
[Test]  Epoch: 45	Loss: 0.014616	Acc: 70.7% (7073/10000)
[Test]  Epoch: 46	Loss: 0.014567	Acc: 70.7% (7072/10000)
[Test]  Epoch: 47	Loss: 0.014526	Acc: 70.7% (7071/10000)
[Test]  Epoch: 48	Loss: 0.014632	Acc: 70.7% (7068/10000)
[Test]  Epoch: 49	Loss: 0.014648	Acc: 70.6% (7060/10000)
[Test]  Epoch: 50	Loss: 0.014632	Acc: 70.8% (7081/10000)
[Test]  Epoch: 51	Loss: 0.014570	Acc: 70.8% (7081/10000)
[Test]  Epoch: 52	Loss: 0.014598	Acc: 70.8% (7079/10000)
[Test]  Epoch: 53	Loss: 0.014648	Acc: 70.7% (7070/10000)
[Test]  Epoch: 54	Loss: 0.014443	Acc: 71.0% (7105/10000)
[Test]  Epoch: 55	Loss: 0.014602	Acc: 70.8% (7083/10000)
[Test]  Epoch: 56	Loss: 0.014495	Acc: 70.9% (7090/10000)
[Test]  Epoch: 57	Loss: 0.014542	Acc: 70.7% (7067/10000)
[Test]  Epoch: 58	Loss: 0.014405	Acc: 71.0% (7095/10000)
[Test]  Epoch: 59	Loss: 0.014514	Acc: 71.1% (7107/10000)
[Test]  Epoch: 60	Loss: 0.014531	Acc: 70.9% (7091/10000)
[Test]  Epoch: 61	Loss: 0.014486	Acc: 70.9% (7093/10000)
[Test]  Epoch: 62	Loss: 0.014494	Acc: 70.8% (7083/10000)
[Test]  Epoch: 63	Loss: 0.014452	Acc: 70.9% (7086/10000)
[Test]  Epoch: 64	Loss: 0.014422	Acc: 70.9% (7094/10000)
[Test]  Epoch: 65	Loss: 0.014452	Acc: 70.9% (7088/10000)
[Test]  Epoch: 66	Loss: 0.014455	Acc: 70.9% (7088/10000)
[Test]  Epoch: 67	Loss: 0.014413	Acc: 71.0% (7097/10000)
[Test]  Epoch: 68	Loss: 0.014464	Acc: 70.9% (7086/10000)
[Test]  Epoch: 69	Loss: 0.014495	Acc: 70.9% (7093/10000)
[Test]  Epoch: 70	Loss: 0.014422	Acc: 71.0% (7104/10000)
[Test]  Epoch: 71	Loss: 0.014439	Acc: 71.0% (7096/10000)
[Test]  Epoch: 72	Loss: 0.014398	Acc: 70.9% (7086/10000)
[Test]  Epoch: 73	Loss: 0.014414	Acc: 71.1% (7111/10000)
[Test]  Epoch: 74	Loss: 0.014446	Acc: 70.8% (7083/10000)
[Test]  Epoch: 75	Loss: 0.014409	Acc: 71.0% (7095/10000)
[Test]  Epoch: 76	Loss: 0.014425	Acc: 71.0% (7099/10000)
[Test]  Epoch: 77	Loss: 0.014467	Acc: 71.0% (7101/10000)
[Test]  Epoch: 78	Loss: 0.014441	Acc: 70.9% (7092/10000)
[Test]  Epoch: 79	Loss: 0.014473	Acc: 70.9% (7089/10000)
[Test]  Epoch: 80	Loss: 0.014452	Acc: 70.9% (7090/10000)
[Test]  Epoch: 81	Loss: 0.014434	Acc: 70.9% (7093/10000)
[Test]  Epoch: 82	Loss: 0.014443	Acc: 71.0% (7099/10000)
[Test]  Epoch: 83	Loss: 0.014415	Acc: 71.0% (7096/10000)
[Test]  Epoch: 84	Loss: 0.014419	Acc: 71.0% (7102/10000)
[Test]  Epoch: 85	Loss: 0.014414	Acc: 70.8% (7085/10000)
[Test]  Epoch: 86	Loss: 0.014401	Acc: 71.0% (7099/10000)
[Test]  Epoch: 87	Loss: 0.014391	Acc: 71.0% (7104/10000)
[Test]  Epoch: 88	Loss: 0.014443	Acc: 71.0% (7099/10000)
[Test]  Epoch: 89	Loss: 0.014440	Acc: 71.0% (7100/10000)
[Test]  Epoch: 90	Loss: 0.014437	Acc: 70.9% (7093/10000)
[Test]  Epoch: 91	Loss: 0.014424	Acc: 71.0% (7104/10000)
[Test]  Epoch: 92	Loss: 0.014469	Acc: 70.8% (7083/10000)
[Test]  Epoch: 93	Loss: 0.014398	Acc: 71.1% (7107/10000)
[Test]  Epoch: 94	Loss: 0.014422	Acc: 71.0% (7105/10000)
[Test]  Epoch: 95	Loss: 0.014405	Acc: 71.0% (7095/10000)
[Test]  Epoch: 96	Loss: 0.014391	Acc: 71.1% (7106/10000)
[Test]  Epoch: 97	Loss: 0.014428	Acc: 70.8% (7083/10000)
[Test]  Epoch: 98	Loss: 0.014406	Acc: 71.0% (7096/10000)
[Test]  Epoch: 99	Loss: 0.014401	Acc: 71.0% (7096/10000)
[Test]  Epoch: 100	Loss: 0.014381	Acc: 71.0% (7103/10000)
===========finish==========
['2024-08-19', '17:17:45.287029', '100', 'test', '0.014381152480840683', '71.03', '71.11']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039617	Acc: 37.8% (3779/10000)
[Test]  Epoch: 2	Loss: 0.019173	Acc: 60.8% (6084/10000)
[Test]  Epoch: 3	Loss: 0.018085	Acc: 62.9% (6289/10000)
[Test]  Epoch: 4	Loss: 0.017638	Acc: 64.0% (6399/10000)
[Test]  Epoch: 5	Loss: 0.017635	Acc: 64.3% (6432/10000)
[Test]  Epoch: 6	Loss: 0.017616	Acc: 64.2% (6420/10000)
[Test]  Epoch: 7	Loss: 0.017470	Acc: 64.6% (6456/10000)
[Test]  Epoch: 8	Loss: 0.017557	Acc: 64.4% (6442/10000)
[Test]  Epoch: 9	Loss: 0.017553	Acc: 64.6% (6463/10000)
[Test]  Epoch: 10	Loss: 0.017438	Acc: 65.0% (6501/10000)
[Test]  Epoch: 11	Loss: 0.017380	Acc: 64.8% (6484/10000)
[Test]  Epoch: 12	Loss: 0.017477	Acc: 65.1% (6511/10000)
[Test]  Epoch: 13	Loss: 0.017391	Acc: 64.9% (6487/10000)
[Test]  Epoch: 14	Loss: 0.017210	Acc: 65.4% (6543/10000)
[Test]  Epoch: 15	Loss: 0.017287	Acc: 65.1% (6510/10000)
[Test]  Epoch: 16	Loss: 0.017198	Acc: 65.3% (6534/10000)
[Test]  Epoch: 17	Loss: 0.017212	Acc: 65.4% (6543/10000)
[Test]  Epoch: 18	Loss: 0.017223	Acc: 65.3% (6534/10000)
[Test]  Epoch: 19	Loss: 0.017092	Acc: 65.3% (6532/10000)
[Test]  Epoch: 20	Loss: 0.017076	Acc: 65.6% (6563/10000)
[Test]  Epoch: 21	Loss: 0.017166	Acc: 65.5% (6547/10000)
[Test]  Epoch: 22	Loss: 0.017036	Acc: 65.6% (6557/10000)
[Test]  Epoch: 23	Loss: 0.016999	Acc: 65.5% (6555/10000)
[Test]  Epoch: 24	Loss: 0.016990	Acc: 65.6% (6557/10000)
[Test]  Epoch: 25	Loss: 0.017131	Acc: 65.8% (6576/10000)
[Test]  Epoch: 26	Loss: 0.016970	Acc: 65.9% (6589/10000)
[Test]  Epoch: 27	Loss: 0.016984	Acc: 65.7% (6572/10000)
[Test]  Epoch: 28	Loss: 0.016962	Acc: 65.7% (6569/10000)
[Test]  Epoch: 29	Loss: 0.016940	Acc: 65.6% (6558/10000)
[Test]  Epoch: 30	Loss: 0.016880	Acc: 66.1% (6610/10000)
[Test]  Epoch: 31	Loss: 0.016861	Acc: 66.0% (6602/10000)
[Test]  Epoch: 32	Loss: 0.017016	Acc: 65.9% (6589/10000)
[Test]  Epoch: 33	Loss: 0.016848	Acc: 65.9% (6586/10000)
[Test]  Epoch: 34	Loss: 0.016819	Acc: 66.0% (6601/10000)
[Test]  Epoch: 35	Loss: 0.016788	Acc: 66.0% (6599/10000)
[Test]  Epoch: 36	Loss: 0.016807	Acc: 66.1% (6606/10000)
[Test]  Epoch: 37	Loss: 0.016770	Acc: 66.1% (6614/10000)
[Test]  Epoch: 38	Loss: 0.016864	Acc: 66.3% (6627/10000)
[Test]  Epoch: 39	Loss: 0.016801	Acc: 66.3% (6626/10000)
[Test]  Epoch: 40	Loss: 0.016755	Acc: 66.5% (6645/10000)
[Test]  Epoch: 41	Loss: 0.016704	Acc: 66.4% (6641/10000)
[Test]  Epoch: 42	Loss: 0.016748	Acc: 66.2% (6620/10000)
[Test]  Epoch: 43	Loss: 0.016741	Acc: 66.1% (6613/10000)
[Test]  Epoch: 44	Loss: 0.016559	Acc: 66.5% (6651/10000)
[Test]  Epoch: 45	Loss: 0.016617	Acc: 66.2% (6621/10000)
[Test]  Epoch: 46	Loss: 0.016683	Acc: 66.2% (6623/10000)
[Test]  Epoch: 47	Loss: 0.016591	Acc: 66.4% (6638/10000)
[Test]  Epoch: 48	Loss: 0.016644	Acc: 66.5% (6645/10000)
[Test]  Epoch: 49	Loss: 0.016652	Acc: 66.5% (6645/10000)
[Test]  Epoch: 50	Loss: 0.016623	Acc: 66.4% (6639/10000)
[Test]  Epoch: 51	Loss: 0.016534	Acc: 66.5% (6645/10000)
[Test]  Epoch: 52	Loss: 0.016576	Acc: 66.6% (6661/10000)
[Test]  Epoch: 53	Loss: 0.016589	Acc: 66.4% (6641/10000)
[Test]  Epoch: 54	Loss: 0.016528	Acc: 66.6% (6659/10000)
[Test]  Epoch: 55	Loss: 0.016500	Acc: 66.7% (6665/10000)
[Test]  Epoch: 56	Loss: 0.016527	Acc: 66.5% (6651/10000)
[Test]  Epoch: 57	Loss: 0.016518	Acc: 66.3% (6626/10000)
[Test]  Epoch: 58	Loss: 0.016441	Acc: 66.5% (6649/10000)
[Test]  Epoch: 59	Loss: 0.016559	Acc: 66.5% (6654/10000)
[Test]  Epoch: 60	Loss: 0.016687	Acc: 66.3% (6629/10000)
[Test]  Epoch: 61	Loss: 0.016608	Acc: 66.5% (6648/10000)
[Test]  Epoch: 62	Loss: 0.016552	Acc: 66.6% (6657/10000)
[Test]  Epoch: 63	Loss: 0.016521	Acc: 66.6% (6657/10000)
[Test]  Epoch: 64	Loss: 0.016506	Acc: 66.6% (6657/10000)
[Test]  Epoch: 65	Loss: 0.016477	Acc: 66.5% (6652/10000)
[Test]  Epoch: 66	Loss: 0.016474	Acc: 66.5% (6648/10000)
[Test]  Epoch: 67	Loss: 0.016463	Acc: 66.6% (6663/10000)
[Test]  Epoch: 68	Loss: 0.016503	Acc: 66.7% (6669/10000)
[Test]  Epoch: 69	Loss: 0.016532	Acc: 66.5% (6655/10000)
[Test]  Epoch: 70	Loss: 0.016433	Acc: 66.6% (6664/10000)
[Test]  Epoch: 71	Loss: 0.016462	Acc: 66.6% (6657/10000)
[Test]  Epoch: 72	Loss: 0.016413	Acc: 66.8% (6676/10000)
[Test]  Epoch: 73	Loss: 0.016454	Acc: 66.8% (6675/10000)
[Test]  Epoch: 74	Loss: 0.016436	Acc: 66.7% (6666/10000)
[Test]  Epoch: 75	Loss: 0.016472	Acc: 66.6% (6661/10000)
[Test]  Epoch: 76	Loss: 0.016465	Acc: 66.7% (6673/10000)
[Test]  Epoch: 77	Loss: 0.016525	Acc: 66.6% (6661/10000)
[Test]  Epoch: 78	Loss: 0.016471	Acc: 66.7% (6672/10000)
[Test]  Epoch: 79	Loss: 0.016460	Acc: 66.8% (6676/10000)
[Test]  Epoch: 80	Loss: 0.016467	Acc: 66.8% (6675/10000)
[Test]  Epoch: 81	Loss: 0.016485	Acc: 66.5% (6653/10000)
[Test]  Epoch: 82	Loss: 0.016470	Acc: 66.6% (6662/10000)
[Test]  Epoch: 83	Loss: 0.016460	Acc: 66.7% (6674/10000)
[Test]  Epoch: 84	Loss: 0.016462	Acc: 66.7% (6670/10000)
[Test]  Epoch: 85	Loss: 0.016464	Acc: 66.6% (6663/10000)
[Test]  Epoch: 86	Loss: 0.016438	Acc: 66.8% (6675/10000)
[Test]  Epoch: 87	Loss: 0.016436	Acc: 66.6% (6656/10000)
[Test]  Epoch: 88	Loss: 0.016455	Acc: 66.7% (6668/10000)
[Test]  Epoch: 89	Loss: 0.016460	Acc: 66.7% (6666/10000)
[Test]  Epoch: 90	Loss: 0.016486	Acc: 66.6% (6656/10000)
[Test]  Epoch: 91	Loss: 0.016443	Acc: 66.6% (6664/10000)
[Test]  Epoch: 92	Loss: 0.016484	Acc: 66.6% (6663/10000)
[Test]  Epoch: 93	Loss: 0.016426	Acc: 66.9% (6690/10000)
[Test]  Epoch: 94	Loss: 0.016474	Acc: 66.7% (6668/10000)
[Test]  Epoch: 95	Loss: 0.016429	Acc: 66.7% (6674/10000)
[Test]  Epoch: 96	Loss: 0.016418	Acc: 66.8% (6681/10000)
[Test]  Epoch: 97	Loss: 0.016462	Acc: 66.8% (6680/10000)
[Test]  Epoch: 98	Loss: 0.016446	Acc: 66.8% (6679/10000)
[Test]  Epoch: 99	Loss: 0.016450	Acc: 66.7% (6673/10000)
[Test]  Epoch: 100	Loss: 0.016451	Acc: 66.8% (6675/10000)
===========finish==========
['2024-08-19', '17:22:10.558341', '100', 'test', '0.01645095685720444', '66.75', '66.9']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.034748	Acc: 39.5% (3948/10000)
[Test]  Epoch: 2	Loss: 0.023771	Acc: 51.5% (5152/10000)
[Test]  Epoch: 3	Loss: 0.022222	Acc: 54.3% (5432/10000)
[Test]  Epoch: 4	Loss: 0.022150	Acc: 55.4% (5536/10000)
[Test]  Epoch: 5	Loss: 0.022190	Acc: 55.1% (5511/10000)
[Test]  Epoch: 6	Loss: 0.022072	Acc: 55.3% (5530/10000)
[Test]  Epoch: 7	Loss: 0.022031	Acc: 55.6% (5557/10000)
[Test]  Epoch: 8	Loss: 0.021841	Acc: 56.2% (5617/10000)
[Test]  Epoch: 9	Loss: 0.021842	Acc: 56.0% (5605/10000)
[Test]  Epoch: 10	Loss: 0.022030	Acc: 56.0% (5597/10000)
[Test]  Epoch: 11	Loss: 0.021793	Acc: 56.2% (5618/10000)
[Test]  Epoch: 12	Loss: 0.021826	Acc: 56.3% (5628/10000)
[Test]  Epoch: 13	Loss: 0.021617	Acc: 56.6% (5658/10000)
[Test]  Epoch: 14	Loss: 0.021778	Acc: 56.5% (5654/10000)
[Test]  Epoch: 15	Loss: 0.021774	Acc: 56.5% (5654/10000)
[Test]  Epoch: 16	Loss: 0.021746	Acc: 56.5% (5653/10000)
[Test]  Epoch: 17	Loss: 0.021651	Acc: 56.7% (5669/10000)
[Test]  Epoch: 18	Loss: 0.021726	Acc: 56.5% (5653/10000)
[Test]  Epoch: 19	Loss: 0.021642	Acc: 56.6% (5661/10000)
[Test]  Epoch: 20	Loss: 0.021625	Acc: 56.6% (5657/10000)
[Test]  Epoch: 21	Loss: 0.021651	Acc: 56.5% (5650/10000)
[Test]  Epoch: 22	Loss: 0.021660	Acc: 56.7% (5669/10000)
[Test]  Epoch: 23	Loss: 0.021606	Acc: 56.8% (5680/10000)
[Test]  Epoch: 24	Loss: 0.021522	Acc: 56.9% (5686/10000)
[Test]  Epoch: 25	Loss: 0.021417	Acc: 57.1% (5708/10000)
[Test]  Epoch: 26	Loss: 0.021345	Acc: 57.4% (5735/10000)
[Test]  Epoch: 27	Loss: 0.021493	Acc: 57.1% (5708/10000)
[Test]  Epoch: 28	Loss: 0.021571	Acc: 56.9% (5687/10000)
[Test]  Epoch: 29	Loss: 0.021443	Acc: 57.2% (5721/10000)
[Test]  Epoch: 30	Loss: 0.021450	Acc: 56.9% (5691/10000)
[Test]  Epoch: 31	Loss: 0.021365	Acc: 56.9% (5690/10000)
[Test]  Epoch: 32	Loss: 0.021351	Acc: 57.5% (5747/10000)
[Test]  Epoch: 33	Loss: 0.021419	Acc: 57.1% (5715/10000)
[Test]  Epoch: 34	Loss: 0.021201	Acc: 57.5% (5753/10000)
[Test]  Epoch: 35	Loss: 0.021319	Acc: 57.1% (5706/10000)
[Test]  Epoch: 36	Loss: 0.021254	Acc: 57.2% (5724/10000)
[Test]  Epoch: 37	Loss: 0.021273	Acc: 57.3% (5727/10000)
[Test]  Epoch: 38	Loss: 0.021354	Acc: 57.0% (5698/10000)
[Test]  Epoch: 39	Loss: 0.021249	Acc: 57.4% (5737/10000)
[Test]  Epoch: 40	Loss: 0.021150	Acc: 57.4% (5742/10000)
[Test]  Epoch: 41	Loss: 0.021235	Acc: 57.4% (5742/10000)
[Test]  Epoch: 42	Loss: 0.021176	Acc: 57.5% (5751/10000)
[Test]  Epoch: 43	Loss: 0.021306	Acc: 57.4% (5737/10000)
[Test]  Epoch: 44	Loss: 0.021103	Acc: 57.8% (5777/10000)
[Test]  Epoch: 45	Loss: 0.021217	Acc: 57.5% (5753/10000)
[Test]  Epoch: 46	Loss: 0.021125	Acc: 57.5% (5749/10000)
[Test]  Epoch: 47	Loss: 0.021012	Acc: 57.6% (5765/10000)
[Test]  Epoch: 48	Loss: 0.021119	Acc: 57.7% (5768/10000)
[Test]  Epoch: 49	Loss: 0.021187	Acc: 57.6% (5761/10000)
[Test]  Epoch: 50	Loss: 0.021107	Acc: 57.7% (5766/10000)
[Test]  Epoch: 51	Loss: 0.021154	Acc: 57.8% (5781/10000)
[Test]  Epoch: 52	Loss: 0.021103	Acc: 57.6% (5760/10000)
[Test]  Epoch: 53	Loss: 0.021076	Acc: 57.5% (5754/10000)
[Test]  Epoch: 54	Loss: 0.021111	Acc: 57.6% (5762/10000)
[Test]  Epoch: 55	Loss: 0.021036	Acc: 57.9% (5792/10000)
[Test]  Epoch: 56	Loss: 0.021067	Acc: 57.9% (5791/10000)
[Test]  Epoch: 57	Loss: 0.021053	Acc: 57.6% (5763/10000)
[Test]  Epoch: 58	Loss: 0.020939	Acc: 57.8% (5782/10000)
[Test]  Epoch: 59	Loss: 0.021053	Acc: 57.8% (5776/10000)
[Test]  Epoch: 60	Loss: 0.021119	Acc: 57.6% (5765/10000)
[Test]  Epoch: 61	Loss: 0.021102	Acc: 57.7% (5774/10000)
[Test]  Epoch: 62	Loss: 0.021084	Acc: 57.8% (5780/10000)
[Test]  Epoch: 63	Loss: 0.021023	Acc: 57.8% (5777/10000)
[Test]  Epoch: 64	Loss: 0.020989	Acc: 57.8% (5784/10000)
[Test]  Epoch: 65	Loss: 0.020954	Acc: 57.8% (5775/10000)
[Test]  Epoch: 66	Loss: 0.021011	Acc: 57.8% (5777/10000)
[Test]  Epoch: 67	Loss: 0.020958	Acc: 57.9% (5786/10000)
[Test]  Epoch: 68	Loss: 0.021059	Acc: 57.6% (5764/10000)
[Test]  Epoch: 69	Loss: 0.021058	Acc: 57.7% (5768/10000)
[Test]  Epoch: 70	Loss: 0.021017	Acc: 57.8% (5775/10000)
[Test]  Epoch: 71	Loss: 0.020962	Acc: 58.0% (5801/10000)
[Test]  Epoch: 72	Loss: 0.020952	Acc: 57.8% (5784/10000)
[Test]  Epoch: 73	Loss: 0.021020	Acc: 57.8% (5775/10000)
[Test]  Epoch: 74	Loss: 0.020955	Acc: 57.8% (5783/10000)
[Test]  Epoch: 75	Loss: 0.020967	Acc: 57.9% (5791/10000)
[Test]  Epoch: 76	Loss: 0.020978	Acc: 57.8% (5777/10000)
[Test]  Epoch: 77	Loss: 0.021024	Acc: 57.8% (5784/10000)
[Test]  Epoch: 78	Loss: 0.021070	Acc: 57.8% (5780/10000)
[Test]  Epoch: 79	Loss: 0.021002	Acc: 57.9% (5785/10000)
[Test]  Epoch: 80	Loss: 0.020961	Acc: 57.8% (5775/10000)
[Test]  Epoch: 81	Loss: 0.020981	Acc: 57.8% (5781/10000)
[Test]  Epoch: 82	Loss: 0.021001	Acc: 57.8% (5778/10000)
[Test]  Epoch: 83	Loss: 0.020950	Acc: 57.9% (5792/10000)
[Test]  Epoch: 84	Loss: 0.021002	Acc: 57.9% (5789/10000)
[Test]  Epoch: 85	Loss: 0.020950	Acc: 58.0% (5797/10000)
[Test]  Epoch: 86	Loss: 0.020960	Acc: 57.9% (5791/10000)
[Test]  Epoch: 87	Loss: 0.021006	Acc: 57.9% (5789/10000)
[Test]  Epoch: 88	Loss: 0.020983	Acc: 58.0% (5801/10000)
[Test]  Epoch: 89	Loss: 0.021067	Acc: 57.9% (5786/10000)
[Test]  Epoch: 90	Loss: 0.021017	Acc: 57.9% (5788/10000)
[Test]  Epoch: 91	Loss: 0.021033	Acc: 58.0% (5795/10000)
[Test]  Epoch: 92	Loss: 0.021008	Acc: 57.9% (5787/10000)
[Test]  Epoch: 93	Loss: 0.020955	Acc: 57.8% (5775/10000)
[Test]  Epoch: 94	Loss: 0.021051	Acc: 57.9% (5791/10000)
[Test]  Epoch: 95	Loss: 0.020975	Acc: 57.9% (5793/10000)
[Test]  Epoch: 96	Loss: 0.020940	Acc: 57.9% (5786/10000)
[Test]  Epoch: 97	Loss: 0.020948	Acc: 57.8% (5784/10000)
[Test]  Epoch: 98	Loss: 0.021035	Acc: 57.8% (5783/10000)
[Test]  Epoch: 99	Loss: 0.020965	Acc: 58.0% (5798/10000)
[Test]  Epoch: 100	Loss: 0.020978	Acc: 57.9% (5794/10000)
===========finish==========
['2024-08-19', '17:26:37.411194', '100', 'test', '0.020977604347467423', '57.94', '58.01']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.043931	Acc: 37.5% (3749/10000)
[Test]  Epoch: 2	Loss: 0.023120	Acc: 53.2% (5320/10000)
[Test]  Epoch: 3	Loss: 0.022360	Acc: 54.3% (5429/10000)
[Test]  Epoch: 4	Loss: 0.021990	Acc: 55.5% (5553/10000)
[Test]  Epoch: 5	Loss: 0.021980	Acc: 55.6% (5563/10000)
[Test]  Epoch: 6	Loss: 0.022154	Acc: 55.4% (5540/10000)
[Test]  Epoch: 7	Loss: 0.021946	Acc: 55.8% (5582/10000)
[Test]  Epoch: 8	Loss: 0.021877	Acc: 56.1% (5608/10000)
[Test]  Epoch: 9	Loss: 0.021716	Acc: 56.4% (5644/10000)
[Test]  Epoch: 10	Loss: 0.021783	Acc: 56.3% (5633/10000)
[Test]  Epoch: 11	Loss: 0.021742	Acc: 56.6% (5661/10000)
[Test]  Epoch: 12	Loss: 0.021793	Acc: 56.3% (5631/10000)
[Test]  Epoch: 13	Loss: 0.021485	Acc: 56.6% (5663/10000)
[Test]  Epoch: 14	Loss: 0.021738	Acc: 56.5% (5655/10000)
[Test]  Epoch: 15	Loss: 0.021674	Acc: 56.6% (5656/10000)
[Test]  Epoch: 16	Loss: 0.021671	Acc: 56.7% (5674/10000)
[Test]  Epoch: 17	Loss: 0.021537	Acc: 57.0% (5703/10000)
[Test]  Epoch: 18	Loss: 0.021405	Acc: 57.4% (5735/10000)
[Test]  Epoch: 19	Loss: 0.021495	Acc: 57.0% (5705/10000)
[Test]  Epoch: 20	Loss: 0.021382	Acc: 57.2% (5719/10000)
[Test]  Epoch: 21	Loss: 0.021451	Acc: 57.2% (5724/10000)
[Test]  Epoch: 22	Loss: 0.021316	Acc: 57.4% (5743/10000)
[Test]  Epoch: 23	Loss: 0.021292	Acc: 57.5% (5751/10000)
[Test]  Epoch: 24	Loss: 0.021273	Acc: 57.1% (5711/10000)
[Test]  Epoch: 25	Loss: 0.021206	Acc: 57.7% (5769/10000)
[Test]  Epoch: 26	Loss: 0.021132	Acc: 57.5% (5750/10000)
[Test]  Epoch: 27	Loss: 0.021223	Acc: 57.4% (5739/10000)
[Test]  Epoch: 28	Loss: 0.021215	Acc: 57.5% (5745/10000)
[Test]  Epoch: 29	Loss: 0.021263	Acc: 57.6% (5765/10000)
[Test]  Epoch: 30	Loss: 0.021158	Acc: 57.4% (5744/10000)
[Test]  Epoch: 31	Loss: 0.021079	Acc: 58.0% (5796/10000)
[Test]  Epoch: 32	Loss: 0.021017	Acc: 57.8% (5783/10000)
[Test]  Epoch: 33	Loss: 0.021016	Acc: 57.8% (5780/10000)
[Test]  Epoch: 34	Loss: 0.020986	Acc: 58.0% (5805/10000)
[Test]  Epoch: 35	Loss: 0.021080	Acc: 57.7% (5769/10000)
[Test]  Epoch: 36	Loss: 0.021040	Acc: 57.5% (5754/10000)
[Test]  Epoch: 37	Loss: 0.020993	Acc: 57.7% (5774/10000)
[Test]  Epoch: 38	Loss: 0.020956	Acc: 57.9% (5790/10000)
[Test]  Epoch: 39	Loss: 0.020939	Acc: 57.9% (5793/10000)
[Test]  Epoch: 40	Loss: 0.020961	Acc: 58.1% (5806/10000)
[Test]  Epoch: 41	Loss: 0.020880	Acc: 58.0% (5802/10000)
[Test]  Epoch: 42	Loss: 0.020913	Acc: 58.0% (5802/10000)
[Test]  Epoch: 43	Loss: 0.020970	Acc: 57.8% (5780/10000)
[Test]  Epoch: 44	Loss: 0.020755	Acc: 58.3% (5827/10000)
[Test]  Epoch: 45	Loss: 0.020886	Acc: 58.2% (5817/10000)
[Test]  Epoch: 46	Loss: 0.020902	Acc: 57.9% (5789/10000)
[Test]  Epoch: 47	Loss: 0.020755	Acc: 58.2% (5819/10000)
[Test]  Epoch: 48	Loss: 0.020842	Acc: 58.1% (5809/10000)
[Test]  Epoch: 49	Loss: 0.020886	Acc: 58.3% (5832/10000)
[Test]  Epoch: 50	Loss: 0.020821	Acc: 58.0% (5801/10000)
[Test]  Epoch: 51	Loss: 0.020767	Acc: 58.4% (5837/10000)
[Test]  Epoch: 52	Loss: 0.020791	Acc: 58.1% (5809/10000)
[Test]  Epoch: 53	Loss: 0.020739	Acc: 58.4% (5839/10000)
[Test]  Epoch: 54	Loss: 0.020761	Acc: 58.3% (5829/10000)
[Test]  Epoch: 55	Loss: 0.020700	Acc: 58.3% (5832/10000)
[Test]  Epoch: 56	Loss: 0.020767	Acc: 58.3% (5829/10000)
[Test]  Epoch: 57	Loss: 0.020630	Acc: 58.3% (5834/10000)
[Test]  Epoch: 58	Loss: 0.020689	Acc: 58.1% (5815/10000)
[Test]  Epoch: 59	Loss: 0.020684	Acc: 58.0% (5800/10000)
[Test]  Epoch: 60	Loss: 0.020798	Acc: 58.2% (5817/10000)
[Test]  Epoch: 61	Loss: 0.020742	Acc: 58.2% (5818/10000)
[Test]  Epoch: 62	Loss: 0.020716	Acc: 58.1% (5812/10000)
[Test]  Epoch: 63	Loss: 0.020710	Acc: 58.4% (5841/10000)
[Test]  Epoch: 64	Loss: 0.020671	Acc: 58.2% (5822/10000)
[Test]  Epoch: 65	Loss: 0.020632	Acc: 58.3% (5829/10000)
[Test]  Epoch: 66	Loss: 0.020636	Acc: 58.1% (5814/10000)
[Test]  Epoch: 67	Loss: 0.020675	Acc: 58.2% (5816/10000)
[Test]  Epoch: 68	Loss: 0.020662	Acc: 58.1% (5812/10000)
[Test]  Epoch: 69	Loss: 0.020654	Acc: 58.2% (5823/10000)
[Test]  Epoch: 70	Loss: 0.020588	Acc: 58.3% (5832/10000)
[Test]  Epoch: 71	Loss: 0.020619	Acc: 58.3% (5832/10000)
[Test]  Epoch: 72	Loss: 0.020642	Acc: 58.3% (5833/10000)
[Test]  Epoch: 73	Loss: 0.020697	Acc: 58.2% (5819/10000)
[Test]  Epoch: 74	Loss: 0.020625	Acc: 58.3% (5831/10000)
[Test]  Epoch: 75	Loss: 0.020661	Acc: 58.2% (5824/10000)
[Test]  Epoch: 76	Loss: 0.020606	Acc: 58.2% (5818/10000)
[Test]  Epoch: 77	Loss: 0.020650	Acc: 58.3% (5830/10000)
[Test]  Epoch: 78	Loss: 0.020667	Acc: 58.3% (5829/10000)
[Test]  Epoch: 79	Loss: 0.020622	Acc: 58.2% (5822/10000)
[Test]  Epoch: 80	Loss: 0.020603	Acc: 58.3% (5834/10000)
[Test]  Epoch: 81	Loss: 0.020607	Acc: 58.4% (5842/10000)
[Test]  Epoch: 82	Loss: 0.020632	Acc: 58.3% (5832/10000)
[Test]  Epoch: 83	Loss: 0.020576	Acc: 58.3% (5828/10000)
[Test]  Epoch: 84	Loss: 0.020636	Acc: 58.3% (5826/10000)
[Test]  Epoch: 85	Loss: 0.020611	Acc: 58.3% (5831/10000)
[Test]  Epoch: 86	Loss: 0.020582	Acc: 58.3% (5831/10000)
[Test]  Epoch: 87	Loss: 0.020631	Acc: 58.2% (5820/10000)
[Test]  Epoch: 88	Loss: 0.020616	Acc: 58.4% (5842/10000)
[Test]  Epoch: 89	Loss: 0.020624	Acc: 58.3% (5834/10000)
[Test]  Epoch: 90	Loss: 0.020619	Acc: 58.3% (5833/10000)
[Test]  Epoch: 91	Loss: 0.020668	Acc: 58.5% (5845/10000)
[Test]  Epoch: 92	Loss: 0.020617	Acc: 58.2% (5820/10000)
[Test]  Epoch: 93	Loss: 0.020597	Acc: 58.3% (5831/10000)
[Test]  Epoch: 94	Loss: 0.020640	Acc: 58.3% (5834/10000)
[Test]  Epoch: 95	Loss: 0.020574	Acc: 58.4% (5842/10000)
[Test]  Epoch: 96	Loss: 0.020569	Acc: 58.3% (5831/10000)
[Test]  Epoch: 97	Loss: 0.020621	Acc: 58.3% (5831/10000)
[Test]  Epoch: 98	Loss: 0.020675	Acc: 58.3% (5832/10000)
[Test]  Epoch: 99	Loss: 0.020610	Acc: 58.3% (5832/10000)
[Test]  Epoch: 100	Loss: 0.020645	Acc: 58.3% (5829/10000)
===========finish==========
['2024-08-19', '17:31:08.760587', '100', 'test', '0.020645138543844224', '58.29', '58.45']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.034754	Acc: 39.4% (3942/10000)
[Test]  Epoch: 2	Loss: 0.020626	Acc: 56.0% (5604/10000)
[Test]  Epoch: 3	Loss: 0.020191	Acc: 57.5% (5750/10000)
[Test]  Epoch: 4	Loss: 0.020036	Acc: 58.0% (5804/10000)
[Test]  Epoch: 5	Loss: 0.020096	Acc: 58.2% (5824/10000)
[Test]  Epoch: 6	Loss: 0.020117	Acc: 58.5% (5845/10000)
[Test]  Epoch: 7	Loss: 0.020112	Acc: 58.4% (5839/10000)
[Test]  Epoch: 8	Loss: 0.020032	Acc: 58.6% (5861/10000)
[Test]  Epoch: 9	Loss: 0.019990	Acc: 59.0% (5899/10000)
[Test]  Epoch: 10	Loss: 0.019982	Acc: 59.2% (5917/10000)
[Test]  Epoch: 11	Loss: 0.019848	Acc: 59.1% (5911/10000)
[Test]  Epoch: 12	Loss: 0.019891	Acc: 59.2% (5925/10000)
[Test]  Epoch: 13	Loss: 0.019866	Acc: 59.5% (5949/10000)
[Test]  Epoch: 14	Loss: 0.019869	Acc: 59.3% (5929/10000)
[Test]  Epoch: 15	Loss: 0.019933	Acc: 59.3% (5926/10000)
[Test]  Epoch: 16	Loss: 0.019827	Acc: 59.5% (5946/10000)
[Test]  Epoch: 17	Loss: 0.019775	Acc: 59.6% (5964/10000)
[Test]  Epoch: 18	Loss: 0.019824	Acc: 59.5% (5952/10000)
[Test]  Epoch: 19	Loss: 0.019863	Acc: 59.6% (5956/10000)
[Test]  Epoch: 20	Loss: 0.019801	Acc: 59.7% (5969/10000)
[Test]  Epoch: 21	Loss: 0.019870	Acc: 59.4% (5935/10000)
[Test]  Epoch: 22	Loss: 0.019747	Acc: 59.6% (5963/10000)
[Test]  Epoch: 23	Loss: 0.019714	Acc: 59.8% (5978/10000)
[Test]  Epoch: 24	Loss: 0.019791	Acc: 59.7% (5973/10000)
[Test]  Epoch: 25	Loss: 0.019722	Acc: 59.9% (5992/10000)
[Test]  Epoch: 26	Loss: 0.019722	Acc: 59.9% (5988/10000)
[Test]  Epoch: 27	Loss: 0.019627	Acc: 59.7% (5970/10000)
[Test]  Epoch: 28	Loss: 0.019752	Acc: 59.7% (5969/10000)
[Test]  Epoch: 29	Loss: 0.019645	Acc: 59.6% (5962/10000)
[Test]  Epoch: 30	Loss: 0.019627	Acc: 59.9% (5991/10000)
[Test]  Epoch: 31	Loss: 0.019603	Acc: 60.0% (6000/10000)
[Test]  Epoch: 32	Loss: 0.019636	Acc: 59.8% (5979/10000)
[Test]  Epoch: 33	Loss: 0.019535	Acc: 60.0% (5996/10000)
[Test]  Epoch: 34	Loss: 0.019553	Acc: 59.9% (5985/10000)
[Test]  Epoch: 35	Loss: 0.019518	Acc: 60.2% (6016/10000)
[Test]  Epoch: 36	Loss: 0.019559	Acc: 60.1% (6008/10000)
[Test]  Epoch: 37	Loss: 0.019497	Acc: 60.1% (6008/10000)
[Test]  Epoch: 38	Loss: 0.019463	Acc: 60.3% (6027/10000)
[Test]  Epoch: 39	Loss: 0.019484	Acc: 60.2% (6019/10000)
[Test]  Epoch: 40	Loss: 0.019431	Acc: 60.1% (6015/10000)
[Test]  Epoch: 41	Loss: 0.019362	Acc: 60.4% (6042/10000)
[Test]  Epoch: 42	Loss: 0.019411	Acc: 60.2% (6022/10000)
[Test]  Epoch: 43	Loss: 0.019428	Acc: 60.1% (6010/10000)
[Test]  Epoch: 44	Loss: 0.019355	Acc: 60.4% (6040/10000)
[Test]  Epoch: 45	Loss: 0.019391	Acc: 60.2% (6024/10000)
[Test]  Epoch: 46	Loss: 0.019386	Acc: 60.3% (6030/10000)
[Test]  Epoch: 47	Loss: 0.019397	Acc: 60.3% (6028/10000)
[Test]  Epoch: 48	Loss: 0.019409	Acc: 60.4% (6041/10000)
[Test]  Epoch: 49	Loss: 0.019388	Acc: 60.3% (6031/10000)
[Test]  Epoch: 50	Loss: 0.019331	Acc: 60.5% (6050/10000)
[Test]  Epoch: 51	Loss: 0.019322	Acc: 60.3% (6033/10000)
[Test]  Epoch: 52	Loss: 0.019328	Acc: 60.4% (6037/10000)
[Test]  Epoch: 53	Loss: 0.019356	Acc: 60.4% (6044/10000)
[Test]  Epoch: 54	Loss: 0.019300	Acc: 60.5% (6048/10000)
[Test]  Epoch: 55	Loss: 0.019257	Acc: 60.5% (6047/10000)
[Test]  Epoch: 56	Loss: 0.019212	Acc: 60.6% (6059/10000)
[Test]  Epoch: 57	Loss: 0.019246	Acc: 60.4% (6038/10000)
[Test]  Epoch: 58	Loss: 0.019264	Acc: 60.5% (6045/10000)
[Test]  Epoch: 59	Loss: 0.019257	Acc: 60.6% (6058/10000)
[Test]  Epoch: 60	Loss: 0.019307	Acc: 60.5% (6053/10000)
[Test]  Epoch: 61	Loss: 0.019263	Acc: 60.8% (6084/10000)
[Test]  Epoch: 62	Loss: 0.019268	Acc: 60.8% (6077/10000)
[Test]  Epoch: 63	Loss: 0.019211	Acc: 60.7% (6067/10000)
[Test]  Epoch: 64	Loss: 0.019225	Acc: 60.8% (6078/10000)
[Test]  Epoch: 65	Loss: 0.019225	Acc: 60.7% (6068/10000)
[Test]  Epoch: 66	Loss: 0.019208	Acc: 60.6% (6058/10000)
[Test]  Epoch: 67	Loss: 0.019201	Acc: 60.6% (6065/10000)
[Test]  Epoch: 68	Loss: 0.019223	Acc: 60.8% (6077/10000)
[Test]  Epoch: 69	Loss: 0.019232	Acc: 60.8% (6078/10000)
[Test]  Epoch: 70	Loss: 0.019200	Acc: 60.8% (6077/10000)
[Test]  Epoch: 71	Loss: 0.019222	Acc: 60.6% (6062/10000)
[Test]  Epoch: 72	Loss: 0.019177	Acc: 60.8% (6077/10000)
[Test]  Epoch: 73	Loss: 0.019231	Acc: 60.6% (6063/10000)
[Test]  Epoch: 74	Loss: 0.019239	Acc: 60.7% (6073/10000)
[Test]  Epoch: 75	Loss: 0.019232	Acc: 60.7% (6067/10000)
[Test]  Epoch: 76	Loss: 0.019193	Acc: 60.8% (6077/10000)
[Test]  Epoch: 77	Loss: 0.019223	Acc: 60.6% (6063/10000)
[Test]  Epoch: 78	Loss: 0.019196	Acc: 60.8% (6080/10000)
[Test]  Epoch: 79	Loss: 0.019160	Acc: 60.7% (6069/10000)
[Test]  Epoch: 80	Loss: 0.019196	Acc: 60.7% (6069/10000)
[Test]  Epoch: 81	Loss: 0.019199	Acc: 60.6% (6061/10000)
[Test]  Epoch: 82	Loss: 0.019236	Acc: 60.6% (6060/10000)
[Test]  Epoch: 83	Loss: 0.019216	Acc: 60.6% (6059/10000)
[Test]  Epoch: 84	Loss: 0.019197	Acc: 60.7% (6067/10000)
[Test]  Epoch: 85	Loss: 0.019204	Acc: 60.6% (6059/10000)
[Test]  Epoch: 86	Loss: 0.019165	Acc: 60.7% (6068/10000)
[Test]  Epoch: 87	Loss: 0.019203	Acc: 60.6% (6056/10000)
[Test]  Epoch: 88	Loss: 0.019217	Acc: 60.6% (6064/10000)
[Test]  Epoch: 89	Loss: 0.019240	Acc: 60.6% (6060/10000)
[Test]  Epoch: 90	Loss: 0.019219	Acc: 60.7% (6070/10000)
[Test]  Epoch: 91	Loss: 0.019238	Acc: 60.7% (6068/10000)
[Test]  Epoch: 92	Loss: 0.019214	Acc: 60.7% (6071/10000)
[Test]  Epoch: 93	Loss: 0.019186	Acc: 60.6% (6062/10000)
[Test]  Epoch: 94	Loss: 0.019219	Acc: 60.7% (6067/10000)
[Test]  Epoch: 95	Loss: 0.019203	Acc: 60.7% (6072/10000)
[Test]  Epoch: 96	Loss: 0.019186	Acc: 60.7% (6066/10000)
[Test]  Epoch: 97	Loss: 0.019231	Acc: 60.7% (6066/10000)
[Test]  Epoch: 98	Loss: 0.019243	Acc: 60.6% (6062/10000)
[Test]  Epoch: 99	Loss: 0.019246	Acc: 60.5% (6050/10000)
[Test]  Epoch: 100	Loss: 0.019252	Acc: 60.6% (6058/10000)
===========finish==========
['2024-08-19', '17:35:36.617553', '100', 'test', '0.019252022337913514', '60.58', '60.84']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035994	Acc: 37.9% (3787/10000)
[Test]  Epoch: 2	Loss: 0.019793	Acc: 57.7% (5771/10000)
[Test]  Epoch: 3	Loss: 0.019070	Acc: 59.5% (5946/10000)
[Test]  Epoch: 4	Loss: 0.018944	Acc: 60.1% (6015/10000)
[Test]  Epoch: 5	Loss: 0.018995	Acc: 59.9% (5990/10000)
[Test]  Epoch: 6	Loss: 0.018889	Acc: 60.3% (6028/10000)
[Test]  Epoch: 7	Loss: 0.018853	Acc: 60.3% (6029/10000)
[Test]  Epoch: 8	Loss: 0.018871	Acc: 60.5% (6050/10000)
[Test]  Epoch: 9	Loss: 0.018845	Acc: 60.6% (6064/10000)
[Test]  Epoch: 10	Loss: 0.018875	Acc: 60.6% (6063/10000)
[Test]  Epoch: 11	Loss: 0.018735	Acc: 60.8% (6076/10000)
[Test]  Epoch: 12	Loss: 0.018850	Acc: 60.8% (6082/10000)
[Test]  Epoch: 13	Loss: 0.018754	Acc: 60.6% (6061/10000)
[Test]  Epoch: 14	Loss: 0.018776	Acc: 60.7% (6071/10000)
[Test]  Epoch: 15	Loss: 0.018746	Acc: 60.8% (6079/10000)
[Test]  Epoch: 16	Loss: 0.018755	Acc: 60.7% (6068/10000)
[Test]  Epoch: 17	Loss: 0.018737	Acc: 61.0% (6103/10000)
[Test]  Epoch: 18	Loss: 0.018607	Acc: 61.0% (6098/10000)
[Test]  Epoch: 19	Loss: 0.018654	Acc: 61.0% (6097/10000)
[Test]  Epoch: 20	Loss: 0.018582	Acc: 61.2% (6119/10000)
[Test]  Epoch: 21	Loss: 0.018631	Acc: 60.9% (6091/10000)
[Test]  Epoch: 22	Loss: 0.018464	Acc: 61.1% (6106/10000)
[Test]  Epoch: 23	Loss: 0.018524	Acc: 60.8% (6082/10000)
[Test]  Epoch: 24	Loss: 0.018581	Acc: 61.1% (6115/10000)
[Test]  Epoch: 25	Loss: 0.018474	Acc: 61.4% (6137/10000)
[Test]  Epoch: 26	Loss: 0.018482	Acc: 61.3% (6129/10000)
[Test]  Epoch: 27	Loss: 0.018395	Acc: 61.1% (6109/10000)
[Test]  Epoch: 28	Loss: 0.018421	Acc: 61.1% (6112/10000)
[Test]  Epoch: 29	Loss: 0.018450	Acc: 61.2% (6121/10000)
[Test]  Epoch: 30	Loss: 0.018454	Acc: 61.1% (6109/10000)
[Test]  Epoch: 31	Loss: 0.018398	Acc: 61.5% (6155/10000)
[Test]  Epoch: 32	Loss: 0.018410	Acc: 61.4% (6137/10000)
[Test]  Epoch: 33	Loss: 0.018315	Acc: 61.4% (6135/10000)
[Test]  Epoch: 34	Loss: 0.018283	Acc: 61.7% (6167/10000)
[Test]  Epoch: 35	Loss: 0.018310	Acc: 61.8% (6176/10000)
[Test]  Epoch: 36	Loss: 0.018296	Acc: 61.4% (6142/10000)
[Test]  Epoch: 37	Loss: 0.018173	Acc: 61.8% (6179/10000)
[Test]  Epoch: 38	Loss: 0.018291	Acc: 61.6% (6158/10000)
[Test]  Epoch: 39	Loss: 0.018283	Acc: 61.7% (6174/10000)
[Test]  Epoch: 40	Loss: 0.018255	Acc: 61.7% (6173/10000)
[Test]  Epoch: 41	Loss: 0.018243	Acc: 61.5% (6150/10000)
[Test]  Epoch: 42	Loss: 0.018194	Acc: 61.8% (6180/10000)
[Test]  Epoch: 43	Loss: 0.018261	Acc: 61.6% (6165/10000)
[Test]  Epoch: 44	Loss: 0.018120	Acc: 61.8% (6184/10000)
[Test]  Epoch: 45	Loss: 0.018137	Acc: 61.9% (6192/10000)
[Test]  Epoch: 46	Loss: 0.018175	Acc: 61.8% (6179/10000)
[Test]  Epoch: 47	Loss: 0.018115	Acc: 61.7% (6172/10000)
[Test]  Epoch: 48	Loss: 0.018188	Acc: 61.9% (6192/10000)
[Test]  Epoch: 49	Loss: 0.018115	Acc: 61.8% (6179/10000)
[Test]  Epoch: 50	Loss: 0.018050	Acc: 62.0% (6200/10000)
[Test]  Epoch: 51	Loss: 0.018043	Acc: 61.9% (6186/10000)
[Test]  Epoch: 52	Loss: 0.018070	Acc: 61.9% (6187/10000)
[Test]  Epoch: 53	Loss: 0.018068	Acc: 61.7% (6170/10000)
[Test]  Epoch: 54	Loss: 0.018053	Acc: 62.0% (6199/10000)
[Test]  Epoch: 55	Loss: 0.018002	Acc: 61.9% (6187/10000)
[Test]  Epoch: 56	Loss: 0.018035	Acc: 61.8% (6184/10000)
[Test]  Epoch: 57	Loss: 0.017993	Acc: 62.0% (6203/10000)
[Test]  Epoch: 58	Loss: 0.017987	Acc: 62.0% (6203/10000)
[Test]  Epoch: 59	Loss: 0.018068	Acc: 61.8% (6176/10000)
[Test]  Epoch: 60	Loss: 0.018115	Acc: 61.8% (6177/10000)
[Test]  Epoch: 61	Loss: 0.018060	Acc: 61.8% (6183/10000)
[Test]  Epoch: 62	Loss: 0.018033	Acc: 61.8% (6184/10000)
[Test]  Epoch: 63	Loss: 0.017986	Acc: 61.9% (6187/10000)
[Test]  Epoch: 64	Loss: 0.017998	Acc: 61.9% (6185/10000)
[Test]  Epoch: 65	Loss: 0.018015	Acc: 61.9% (6189/10000)
[Test]  Epoch: 66	Loss: 0.017988	Acc: 62.0% (6195/10000)
[Test]  Epoch: 67	Loss: 0.018009	Acc: 61.9% (6191/10000)
[Test]  Epoch: 68	Loss: 0.017987	Acc: 62.0% (6205/10000)
[Test]  Epoch: 69	Loss: 0.018011	Acc: 61.8% (6181/10000)
[Test]  Epoch: 70	Loss: 0.017949	Acc: 62.0% (6195/10000)
[Test]  Epoch: 71	Loss: 0.017988	Acc: 61.9% (6192/10000)
[Test]  Epoch: 72	Loss: 0.017968	Acc: 62.1% (6208/10000)
[Test]  Epoch: 73	Loss: 0.017973	Acc: 62.0% (6203/10000)
[Test]  Epoch: 74	Loss: 0.017963	Acc: 62.0% (6202/10000)
[Test]  Epoch: 75	Loss: 0.017973	Acc: 62.1% (6207/10000)
[Test]  Epoch: 76	Loss: 0.017932	Acc: 62.0% (6203/10000)
[Test]  Epoch: 77	Loss: 0.017977	Acc: 61.9% (6186/10000)
[Test]  Epoch: 78	Loss: 0.018010	Acc: 61.9% (6194/10000)
[Test]  Epoch: 79	Loss: 0.018012	Acc: 62.0% (6199/10000)
[Test]  Epoch: 80	Loss: 0.017972	Acc: 62.0% (6204/10000)
[Test]  Epoch: 81	Loss: 0.017974	Acc: 61.9% (6188/10000)
[Test]  Epoch: 82	Loss: 0.017996	Acc: 61.7% (6174/10000)
[Test]  Epoch: 83	Loss: 0.017964	Acc: 62.0% (6195/10000)
[Test]  Epoch: 84	Loss: 0.017961	Acc: 62.0% (6196/10000)
[Test]  Epoch: 85	Loss: 0.017961	Acc: 62.0% (6198/10000)
[Test]  Epoch: 86	Loss: 0.017972	Acc: 62.0% (6205/10000)
[Test]  Epoch: 87	Loss: 0.017922	Acc: 62.0% (6204/10000)
[Test]  Epoch: 88	Loss: 0.017943	Acc: 62.1% (6207/10000)
[Test]  Epoch: 89	Loss: 0.018021	Acc: 62.0% (6201/10000)
[Test]  Epoch: 90	Loss: 0.018008	Acc: 61.9% (6189/10000)
[Test]  Epoch: 91	Loss: 0.017953	Acc: 62.0% (6196/10000)
[Test]  Epoch: 92	Loss: 0.017979	Acc: 62.1% (6212/10000)
[Test]  Epoch: 93	Loss: 0.017955	Acc: 61.9% (6190/10000)
[Test]  Epoch: 94	Loss: 0.017941	Acc: 62.3% (6226/10000)
[Test]  Epoch: 95	Loss: 0.017922	Acc: 62.2% (6220/10000)
[Test]  Epoch: 96	Loss: 0.017928	Acc: 62.1% (6211/10000)
[Test]  Epoch: 97	Loss: 0.017963	Acc: 62.2% (6216/10000)
[Test]  Epoch: 98	Loss: 0.017999	Acc: 61.9% (6191/10000)
[Test]  Epoch: 99	Loss: 0.017977	Acc: 62.1% (6209/10000)
[Test]  Epoch: 100	Loss: 0.017984	Acc: 61.8% (6182/10000)
===========finish==========
['2024-08-19', '17:39:56.249564', '100', 'test', '0.01798372524380684', '61.82', '62.26']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.031958	Acc: 39.8% (3982/10000)
[Test]  Epoch: 2	Loss: 0.019808	Acc: 57.6% (5758/10000)
[Test]  Epoch: 3	Loss: 0.018957	Acc: 59.2% (5920/10000)
[Test]  Epoch: 4	Loss: 0.018952	Acc: 59.7% (5972/10000)
[Test]  Epoch: 5	Loss: 0.019013	Acc: 59.5% (5954/10000)
[Test]  Epoch: 6	Loss: 0.018899	Acc: 59.9% (5992/10000)
[Test]  Epoch: 7	Loss: 0.018851	Acc: 60.0% (5997/10000)
[Test]  Epoch: 8	Loss: 0.018852	Acc: 60.0% (6000/10000)
[Test]  Epoch: 9	Loss: 0.018937	Acc: 60.4% (6042/10000)
[Test]  Epoch: 10	Loss: 0.018867	Acc: 60.4% (6036/10000)
[Test]  Epoch: 11	Loss: 0.018718	Acc: 60.3% (6034/10000)
[Test]  Epoch: 12	Loss: 0.018902	Acc: 60.3% (6027/10000)
[Test]  Epoch: 13	Loss: 0.018804	Acc: 60.5% (6054/10000)
[Test]  Epoch: 14	Loss: 0.018878	Acc: 60.4% (6038/10000)
[Test]  Epoch: 15	Loss: 0.018830	Acc: 60.4% (6035/10000)
[Test]  Epoch: 16	Loss: 0.018812	Acc: 60.3% (6031/10000)
[Test]  Epoch: 17	Loss: 0.018793	Acc: 60.4% (6037/10000)
[Test]  Epoch: 18	Loss: 0.018645	Acc: 60.5% (6047/10000)
[Test]  Epoch: 19	Loss: 0.018752	Acc: 60.5% (6045/10000)
[Test]  Epoch: 20	Loss: 0.018631	Acc: 60.5% (6051/10000)
[Test]  Epoch: 21	Loss: 0.018681	Acc: 60.6% (6065/10000)
[Test]  Epoch: 22	Loss: 0.018533	Acc: 60.7% (6074/10000)
[Test]  Epoch: 23	Loss: 0.018577	Acc: 60.6% (6059/10000)
[Test]  Epoch: 24	Loss: 0.018671	Acc: 60.6% (6058/10000)
[Test]  Epoch: 25	Loss: 0.018629	Acc: 60.6% (6062/10000)
[Test]  Epoch: 26	Loss: 0.018551	Acc: 60.4% (6038/10000)
[Test]  Epoch: 27	Loss: 0.018452	Acc: 60.9% (6086/10000)
[Test]  Epoch: 28	Loss: 0.018591	Acc: 60.6% (6060/10000)
[Test]  Epoch: 29	Loss: 0.018588	Acc: 60.6% (6061/10000)
[Test]  Epoch: 30	Loss: 0.018528	Acc: 60.8% (6077/10000)
[Test]  Epoch: 31	Loss: 0.018479	Acc: 61.1% (6108/10000)
[Test]  Epoch: 32	Loss: 0.018501	Acc: 61.3% (6131/10000)
[Test]  Epoch: 33	Loss: 0.018417	Acc: 61.1% (6109/10000)
[Test]  Epoch: 34	Loss: 0.018409	Acc: 61.4% (6140/10000)
[Test]  Epoch: 35	Loss: 0.018390	Acc: 61.4% (6135/10000)
[Test]  Epoch: 36	Loss: 0.018403	Acc: 61.0% (6096/10000)
[Test]  Epoch: 37	Loss: 0.018315	Acc: 61.3% (6127/10000)
[Test]  Epoch: 38	Loss: 0.018395	Acc: 61.1% (6109/10000)
[Test]  Epoch: 39	Loss: 0.018382	Acc: 61.2% (6125/10000)
[Test]  Epoch: 40	Loss: 0.018317	Acc: 61.3% (6131/10000)
[Test]  Epoch: 41	Loss: 0.018318	Acc: 61.4% (6139/10000)
[Test]  Epoch: 42	Loss: 0.018378	Acc: 61.2% (6125/10000)
[Test]  Epoch: 43	Loss: 0.018335	Acc: 61.5% (6154/10000)
[Test]  Epoch: 44	Loss: 0.018252	Acc: 61.6% (6158/10000)
[Test]  Epoch: 45	Loss: 0.018326	Acc: 61.6% (6157/10000)
[Test]  Epoch: 46	Loss: 0.018281	Acc: 61.1% (6115/10000)
[Test]  Epoch: 47	Loss: 0.018278	Acc: 61.4% (6144/10000)
[Test]  Epoch: 48	Loss: 0.018340	Acc: 61.5% (6150/10000)
[Test]  Epoch: 49	Loss: 0.018242	Acc: 61.6% (6160/10000)
[Test]  Epoch: 50	Loss: 0.018217	Acc: 61.3% (6131/10000)
[Test]  Epoch: 51	Loss: 0.018234	Acc: 61.3% (6126/10000)
[Test]  Epoch: 52	Loss: 0.018256	Acc: 61.5% (6145/10000)
[Test]  Epoch: 53	Loss: 0.018195	Acc: 61.4% (6137/10000)
[Test]  Epoch: 54	Loss: 0.018184	Acc: 61.6% (6165/10000)
[Test]  Epoch: 55	Loss: 0.018150	Acc: 61.7% (6172/10000)
[Test]  Epoch: 56	Loss: 0.018227	Acc: 61.5% (6153/10000)
[Test]  Epoch: 57	Loss: 0.018174	Acc: 61.5% (6149/10000)
[Test]  Epoch: 58	Loss: 0.018207	Acc: 61.4% (6139/10000)
[Test]  Epoch: 59	Loss: 0.018218	Acc: 61.5% (6146/10000)
[Test]  Epoch: 60	Loss: 0.018264	Acc: 61.2% (6118/10000)
[Test]  Epoch: 61	Loss: 0.018190	Acc: 61.3% (6134/10000)
[Test]  Epoch: 62	Loss: 0.018147	Acc: 61.4% (6142/10000)
[Test]  Epoch: 63	Loss: 0.018089	Acc: 61.5% (6150/10000)
[Test]  Epoch: 64	Loss: 0.018124	Acc: 61.6% (6162/10000)
[Test]  Epoch: 65	Loss: 0.018158	Acc: 61.5% (6149/10000)
[Test]  Epoch: 66	Loss: 0.018134	Acc: 61.5% (6152/10000)
[Test]  Epoch: 67	Loss: 0.018160	Acc: 61.4% (6141/10000)
[Test]  Epoch: 68	Loss: 0.018139	Acc: 61.7% (6168/10000)
[Test]  Epoch: 69	Loss: 0.018136	Acc: 61.6% (6158/10000)
[Test]  Epoch: 70	Loss: 0.018073	Acc: 61.7% (6171/10000)
[Test]  Epoch: 71	Loss: 0.018129	Acc: 61.4% (6135/10000)
[Test]  Epoch: 72	Loss: 0.018124	Acc: 61.6% (6162/10000)
[Test]  Epoch: 73	Loss: 0.018108	Acc: 61.8% (6183/10000)
[Test]  Epoch: 74	Loss: 0.018127	Acc: 61.5% (6153/10000)
[Test]  Epoch: 75	Loss: 0.018105	Acc: 61.6% (6163/10000)
[Test]  Epoch: 76	Loss: 0.018095	Acc: 61.6% (6157/10000)
[Test]  Epoch: 77	Loss: 0.018099	Acc: 61.7% (6166/10000)
[Test]  Epoch: 78	Loss: 0.018137	Acc: 61.6% (6156/10000)
[Test]  Epoch: 79	Loss: 0.018147	Acc: 61.6% (6156/10000)
[Test]  Epoch: 80	Loss: 0.018109	Acc: 61.6% (6158/10000)
[Test]  Epoch: 81	Loss: 0.018136	Acc: 61.5% (6146/10000)
[Test]  Epoch: 82	Loss: 0.018149	Acc: 61.6% (6161/10000)
[Test]  Epoch: 83	Loss: 0.018159	Acc: 61.5% (6154/10000)
[Test]  Epoch: 84	Loss: 0.018133	Acc: 61.6% (6160/10000)
[Test]  Epoch: 85	Loss: 0.018138	Acc: 61.5% (6145/10000)
[Test]  Epoch: 86	Loss: 0.018130	Acc: 61.6% (6157/10000)
[Test]  Epoch: 87	Loss: 0.018113	Acc: 61.6% (6164/10000)
[Test]  Epoch: 88	Loss: 0.018113	Acc: 61.6% (6160/10000)
[Test]  Epoch: 89	Loss: 0.018201	Acc: 61.3% (6134/10000)
[Test]  Epoch: 90	Loss: 0.018162	Acc: 61.6% (6162/10000)
[Test]  Epoch: 91	Loss: 0.018093	Acc: 61.6% (6162/10000)
[Test]  Epoch: 92	Loss: 0.018136	Acc: 61.6% (6159/10000)
[Test]  Epoch: 93	Loss: 0.018084	Acc: 61.5% (6154/10000)
[Test]  Epoch: 94	Loss: 0.018091	Acc: 61.6% (6165/10000)
[Test]  Epoch: 95	Loss: 0.018093	Acc: 61.6% (6165/10000)
[Test]  Epoch: 96	Loss: 0.018093	Acc: 61.5% (6153/10000)
[Test]  Epoch: 97	Loss: 0.018108	Acc: 61.6% (6163/10000)
[Test]  Epoch: 98	Loss: 0.018124	Acc: 61.6% (6160/10000)
[Test]  Epoch: 99	Loss: 0.018120	Acc: 61.7% (6169/10000)
[Test]  Epoch: 100	Loss: 0.018132	Acc: 61.7% (6169/10000)
===========finish==========
['2024-08-19', '17:44:19.335625', '100', 'test', '0.018131614327430726', '61.69', '61.83']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.033445	Acc: 38.1% (3809/10000)
[Test]  Epoch: 2	Loss: 0.019927	Acc: 57.4% (5736/10000)
[Test]  Epoch: 3	Loss: 0.019204	Acc: 58.9% (5891/10000)
[Test]  Epoch: 4	Loss: 0.019147	Acc: 59.4% (5944/10000)
[Test]  Epoch: 5	Loss: 0.019150	Acc: 59.8% (5979/10000)
[Test]  Epoch: 6	Loss: 0.019066	Acc: 60.0% (6005/10000)
[Test]  Epoch: 7	Loss: 0.019067	Acc: 60.2% (6020/10000)
[Test]  Epoch: 8	Loss: 0.019024	Acc: 60.1% (6013/10000)
[Test]  Epoch: 9	Loss: 0.019020	Acc: 59.9% (5992/10000)
[Test]  Epoch: 10	Loss: 0.018934	Acc: 60.4% (6037/10000)
[Test]  Epoch: 11	Loss: 0.018858	Acc: 60.7% (6066/10000)
[Test]  Epoch: 12	Loss: 0.018896	Acc: 60.5% (6053/10000)
[Test]  Epoch: 13	Loss: 0.018875	Acc: 60.5% (6052/10000)
[Test]  Epoch: 14	Loss: 0.018996	Acc: 60.6% (6059/10000)
[Test]  Epoch: 15	Loss: 0.018858	Acc: 60.5% (6050/10000)
[Test]  Epoch: 16	Loss: 0.018899	Acc: 60.6% (6057/10000)
[Test]  Epoch: 17	Loss: 0.018776	Acc: 60.8% (6078/10000)
[Test]  Epoch: 18	Loss: 0.018723	Acc: 60.8% (6082/10000)
[Test]  Epoch: 19	Loss: 0.018835	Acc: 60.9% (6093/10000)
[Test]  Epoch: 20	Loss: 0.018857	Acc: 60.4% (6042/10000)
[Test]  Epoch: 21	Loss: 0.018817	Acc: 61.0% (6096/10000)
[Test]  Epoch: 22	Loss: 0.018679	Acc: 61.1% (6108/10000)
[Test]  Epoch: 23	Loss: 0.018640	Acc: 61.0% (6098/10000)
[Test]  Epoch: 24	Loss: 0.018765	Acc: 60.9% (6088/10000)
[Test]  Epoch: 25	Loss: 0.018726	Acc: 61.1% (6114/10000)
[Test]  Epoch: 26	Loss: 0.018631	Acc: 61.1% (6111/10000)
[Test]  Epoch: 27	Loss: 0.018530	Acc: 61.3% (6131/10000)
[Test]  Epoch: 28	Loss: 0.018640	Acc: 61.5% (6148/10000)
[Test]  Epoch: 29	Loss: 0.018611	Acc: 61.2% (6119/10000)
[Test]  Epoch: 30	Loss: 0.018525	Acc: 61.5% (6153/10000)
[Test]  Epoch: 31	Loss: 0.018446	Acc: 61.6% (6159/10000)
[Test]  Epoch: 32	Loss: 0.018489	Acc: 61.5% (6153/10000)
[Test]  Epoch: 33	Loss: 0.018401	Acc: 61.8% (6184/10000)
[Test]  Epoch: 34	Loss: 0.018380	Acc: 61.9% (6186/10000)
[Test]  Epoch: 35	Loss: 0.018358	Acc: 61.9% (6187/10000)
[Test]  Epoch: 36	Loss: 0.018379	Acc: 61.7% (6170/10000)
[Test]  Epoch: 37	Loss: 0.018379	Acc: 61.6% (6159/10000)
[Test]  Epoch: 38	Loss: 0.018370	Acc: 61.8% (6177/10000)
[Test]  Epoch: 39	Loss: 0.018451	Acc: 61.7% (6170/10000)
[Test]  Epoch: 40	Loss: 0.018392	Acc: 61.6% (6163/10000)
[Test]  Epoch: 41	Loss: 0.018352	Acc: 61.8% (6179/10000)
[Test]  Epoch: 42	Loss: 0.018364	Acc: 61.6% (6159/10000)
[Test]  Epoch: 43	Loss: 0.018432	Acc: 61.9% (6190/10000)
[Test]  Epoch: 44	Loss: 0.018274	Acc: 62.0% (6195/10000)
[Test]  Epoch: 45	Loss: 0.018264	Acc: 62.0% (6202/10000)
[Test]  Epoch: 46	Loss: 0.018307	Acc: 61.9% (6193/10000)
[Test]  Epoch: 47	Loss: 0.018284	Acc: 61.7% (6173/10000)
[Test]  Epoch: 48	Loss: 0.018390	Acc: 61.4% (6142/10000)
[Test]  Epoch: 49	Loss: 0.018277	Acc: 61.9% (6194/10000)
[Test]  Epoch: 50	Loss: 0.018243	Acc: 62.0% (6201/10000)
[Test]  Epoch: 51	Loss: 0.018311	Acc: 61.9% (6186/10000)
[Test]  Epoch: 52	Loss: 0.018276	Acc: 61.8% (6179/10000)
[Test]  Epoch: 53	Loss: 0.018270	Acc: 61.9% (6189/10000)
[Test]  Epoch: 54	Loss: 0.018207	Acc: 61.8% (6180/10000)
[Test]  Epoch: 55	Loss: 0.018169	Acc: 61.9% (6191/10000)
[Test]  Epoch: 56	Loss: 0.018217	Acc: 61.9% (6191/10000)
[Test]  Epoch: 57	Loss: 0.018206	Acc: 62.0% (6196/10000)
[Test]  Epoch: 58	Loss: 0.018260	Acc: 61.8% (6179/10000)
[Test]  Epoch: 59	Loss: 0.018285	Acc: 61.8% (6184/10000)
[Test]  Epoch: 60	Loss: 0.018259	Acc: 61.8% (6181/10000)
[Test]  Epoch: 61	Loss: 0.018242	Acc: 62.0% (6197/10000)
[Test]  Epoch: 62	Loss: 0.018214	Acc: 61.9% (6187/10000)
[Test]  Epoch: 63	Loss: 0.018118	Acc: 62.0% (6202/10000)
[Test]  Epoch: 64	Loss: 0.018161	Acc: 62.1% (6210/10000)
[Test]  Epoch: 65	Loss: 0.018162	Acc: 62.0% (6203/10000)
[Test]  Epoch: 66	Loss: 0.018166	Acc: 62.0% (6203/10000)
[Test]  Epoch: 67	Loss: 0.018181	Acc: 62.0% (6196/10000)
[Test]  Epoch: 68	Loss: 0.018154	Acc: 62.1% (6208/10000)
[Test]  Epoch: 69	Loss: 0.018150	Acc: 62.1% (6211/10000)
[Test]  Epoch: 70	Loss: 0.018101	Acc: 62.2% (6221/10000)
[Test]  Epoch: 71	Loss: 0.018137	Acc: 62.0% (6201/10000)
[Test]  Epoch: 72	Loss: 0.018118	Acc: 62.1% (6214/10000)
[Test]  Epoch: 73	Loss: 0.018138	Acc: 62.0% (6202/10000)
[Test]  Epoch: 74	Loss: 0.018139	Acc: 62.1% (6207/10000)
[Test]  Epoch: 75	Loss: 0.018108	Acc: 62.1% (6215/10000)
[Test]  Epoch: 76	Loss: 0.018096	Acc: 62.1% (6212/10000)
[Test]  Epoch: 77	Loss: 0.018115	Acc: 62.1% (6212/10000)
[Test]  Epoch: 78	Loss: 0.018142	Acc: 62.0% (6205/10000)
[Test]  Epoch: 79	Loss: 0.018146	Acc: 62.1% (6214/10000)
[Test]  Epoch: 80	Loss: 0.018142	Acc: 62.3% (6229/10000)
[Test]  Epoch: 81	Loss: 0.018144	Acc: 62.0% (6204/10000)
[Test]  Epoch: 82	Loss: 0.018182	Acc: 61.9% (6194/10000)
[Test]  Epoch: 83	Loss: 0.018194	Acc: 62.1% (6207/10000)
[Test]  Epoch: 84	Loss: 0.018163	Acc: 62.0% (6200/10000)
[Test]  Epoch: 85	Loss: 0.018134	Acc: 62.0% (6197/10000)
[Test]  Epoch: 86	Loss: 0.018141	Acc: 62.1% (6210/10000)
[Test]  Epoch: 87	Loss: 0.018131	Acc: 62.1% (6209/10000)
[Test]  Epoch: 88	Loss: 0.018143	Acc: 62.3% (6227/10000)
[Test]  Epoch: 89	Loss: 0.018202	Acc: 62.2% (6222/10000)
[Test]  Epoch: 90	Loss: 0.018200	Acc: 62.0% (6203/10000)
[Test]  Epoch: 91	Loss: 0.018120	Acc: 62.2% (6217/10000)
[Test]  Epoch: 92	Loss: 0.018149	Acc: 62.2% (6220/10000)
[Test]  Epoch: 93	Loss: 0.018116	Acc: 62.2% (6224/10000)
[Test]  Epoch: 94	Loss: 0.018107	Acc: 62.2% (6225/10000)
[Test]  Epoch: 95	Loss: 0.018122	Acc: 62.1% (6209/10000)
[Test]  Epoch: 96	Loss: 0.018138	Acc: 62.3% (6232/10000)
[Test]  Epoch: 97	Loss: 0.018145	Acc: 62.2% (6221/10000)
[Test]  Epoch: 98	Loss: 0.018138	Acc: 62.1% (6209/10000)
[Test]  Epoch: 99	Loss: 0.018135	Acc: 62.1% (6215/10000)
[Test]  Epoch: 100	Loss: 0.018175	Acc: 62.0% (6197/10000)
===========finish==========
['2024-08-19', '17:48:37.785349', '100', 'test', '0.018174892514944075', '61.97', '62.32']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.031584	Acc: 33.1% (3313/10000)
[Test]  Epoch: 2	Loss: 0.020981	Acc: 53.4% (5338/10000)
[Test]  Epoch: 3	Loss: 0.020178	Acc: 55.7% (5567/10000)
[Test]  Epoch: 4	Loss: 0.020086	Acc: 56.3% (5628/10000)
[Test]  Epoch: 5	Loss: 0.019966	Acc: 56.6% (5659/10000)
[Test]  Epoch: 6	Loss: 0.020111	Acc: 56.8% (5679/10000)
[Test]  Epoch: 7	Loss: 0.020187	Acc: 56.5% (5653/10000)
[Test]  Epoch: 8	Loss: 0.020077	Acc: 56.8% (5681/10000)
[Test]  Epoch: 9	Loss: 0.020032	Acc: 57.1% (5714/10000)
[Test]  Epoch: 10	Loss: 0.019977	Acc: 57.3% (5731/10000)
[Test]  Epoch: 11	Loss: 0.019941	Acc: 57.4% (5738/10000)
[Test]  Epoch: 12	Loss: 0.020132	Acc: 57.4% (5742/10000)
[Test]  Epoch: 13	Loss: 0.019937	Acc: 57.8% (5776/10000)
[Test]  Epoch: 14	Loss: 0.020084	Acc: 57.5% (5747/10000)
[Test]  Epoch: 15	Loss: 0.020051	Acc: 57.4% (5735/10000)
[Test]  Epoch: 16	Loss: 0.019905	Acc: 57.8% (5776/10000)
[Test]  Epoch: 17	Loss: 0.020035	Acc: 57.7% (5771/10000)
[Test]  Epoch: 18	Loss: 0.019912	Acc: 57.9% (5789/10000)
[Test]  Epoch: 19	Loss: 0.019955	Acc: 58.0% (5800/10000)
[Test]  Epoch: 20	Loss: 0.019905	Acc: 58.1% (5806/10000)
[Test]  Epoch: 21	Loss: 0.019959	Acc: 58.0% (5804/10000)
[Test]  Epoch: 22	Loss: 0.019962	Acc: 57.8% (5784/10000)
[Test]  Epoch: 23	Loss: 0.019873	Acc: 58.0% (5803/10000)
[Test]  Epoch: 24	Loss: 0.019891	Acc: 57.8% (5778/10000)
[Test]  Epoch: 25	Loss: 0.019968	Acc: 57.8% (5778/10000)
[Test]  Epoch: 26	Loss: 0.019856	Acc: 58.0% (5802/10000)
[Test]  Epoch: 27	Loss: 0.019799	Acc: 58.0% (5797/10000)
[Test]  Epoch: 28	Loss: 0.019858	Acc: 57.9% (5785/10000)
[Test]  Epoch: 29	Loss: 0.019918	Acc: 57.9% (5785/10000)
[Test]  Epoch: 30	Loss: 0.019794	Acc: 58.0% (5797/10000)
[Test]  Epoch: 31	Loss: 0.019769	Acc: 58.0% (5804/10000)
[Test]  Epoch: 32	Loss: 0.019727	Acc: 58.3% (5828/10000)
[Test]  Epoch: 33	Loss: 0.019694	Acc: 58.2% (5819/10000)
[Test]  Epoch: 34	Loss: 0.019713	Acc: 58.0% (5795/10000)
[Test]  Epoch: 35	Loss: 0.019582	Acc: 58.4% (5837/10000)
[Test]  Epoch: 36	Loss: 0.019561	Acc: 58.4% (5841/10000)
[Test]  Epoch: 37	Loss: 0.019578	Acc: 58.5% (5846/10000)
[Test]  Epoch: 38	Loss: 0.019649	Acc: 58.4% (5842/10000)
[Test]  Epoch: 39	Loss: 0.019646	Acc: 58.3% (5831/10000)
[Test]  Epoch: 40	Loss: 0.019527	Acc: 58.3% (5834/10000)
[Test]  Epoch: 41	Loss: 0.019520	Acc: 58.3% (5829/10000)
[Test]  Epoch: 42	Loss: 0.019551	Acc: 58.5% (5848/10000)
[Test]  Epoch: 43	Loss: 0.019511	Acc: 58.4% (5843/10000)
[Test]  Epoch: 44	Loss: 0.019471	Acc: 58.6% (5856/10000)
[Test]  Epoch: 45	Loss: 0.019468	Acc: 58.5% (5853/10000)
[Test]  Epoch: 46	Loss: 0.019465	Acc: 58.6% (5862/10000)
[Test]  Epoch: 47	Loss: 0.019399	Acc: 58.7% (5871/10000)
[Test]  Epoch: 48	Loss: 0.019552	Acc: 58.5% (5848/10000)
[Test]  Epoch: 49	Loss: 0.019466	Acc: 58.5% (5848/10000)
[Test]  Epoch: 50	Loss: 0.019432	Acc: 58.8% (5881/10000)
[Test]  Epoch: 51	Loss: 0.019462	Acc: 58.5% (5854/10000)
[Test]  Epoch: 52	Loss: 0.019459	Acc: 58.5% (5850/10000)
[Test]  Epoch: 53	Loss: 0.019420	Acc: 58.6% (5861/10000)
[Test]  Epoch: 54	Loss: 0.019415	Acc: 58.5% (5851/10000)
[Test]  Epoch: 55	Loss: 0.019363	Acc: 58.5% (5855/10000)
[Test]  Epoch: 56	Loss: 0.019421	Acc: 58.9% (5886/10000)
[Test]  Epoch: 57	Loss: 0.019271	Acc: 59.2% (5921/10000)
[Test]  Epoch: 58	Loss: 0.019363	Acc: 58.6% (5864/10000)
[Test]  Epoch: 59	Loss: 0.019430	Acc: 58.5% (5849/10000)
[Test]  Epoch: 60	Loss: 0.019532	Acc: 58.6% (5864/10000)
[Test]  Epoch: 61	Loss: 0.019433	Acc: 59.0% (5896/10000)
[Test]  Epoch: 62	Loss: 0.019429	Acc: 58.9% (5890/10000)
[Test]  Epoch: 63	Loss: 0.019383	Acc: 58.9% (5891/10000)
[Test]  Epoch: 64	Loss: 0.019387	Acc: 59.0% (5895/10000)
[Test]  Epoch: 65	Loss: 0.019345	Acc: 58.9% (5889/10000)
[Test]  Epoch: 66	Loss: 0.019349	Acc: 59.0% (5905/10000)
[Test]  Epoch: 67	Loss: 0.019418	Acc: 58.7% (5872/10000)
[Test]  Epoch: 68	Loss: 0.019333	Acc: 59.0% (5895/10000)
[Test]  Epoch: 69	Loss: 0.019353	Acc: 59.1% (5906/10000)
[Test]  Epoch: 70	Loss: 0.019308	Acc: 59.0% (5897/10000)
[Test]  Epoch: 71	Loss: 0.019334	Acc: 58.9% (5888/10000)
[Test]  Epoch: 72	Loss: 0.019351	Acc: 58.7% (5868/10000)
[Test]  Epoch: 73	Loss: 0.019363	Acc: 58.9% (5888/10000)
[Test]  Epoch: 74	Loss: 0.019366	Acc: 58.7% (5870/10000)
[Test]  Epoch: 75	Loss: 0.019377	Acc: 58.8% (5876/10000)
[Test]  Epoch: 76	Loss: 0.019316	Acc: 58.9% (5890/10000)
[Test]  Epoch: 77	Loss: 0.019289	Acc: 59.1% (5910/10000)
[Test]  Epoch: 78	Loss: 0.019293	Acc: 59.0% (5900/10000)
[Test]  Epoch: 79	Loss: 0.019316	Acc: 58.9% (5885/10000)
[Test]  Epoch: 80	Loss: 0.019351	Acc: 59.0% (5899/10000)
[Test]  Epoch: 81	Loss: 0.019334	Acc: 58.9% (5892/10000)
[Test]  Epoch: 82	Loss: 0.019394	Acc: 58.9% (5892/10000)
[Test]  Epoch: 83	Loss: 0.019333	Acc: 59.0% (5901/10000)
[Test]  Epoch: 84	Loss: 0.019367	Acc: 58.9% (5892/10000)
[Test]  Epoch: 85	Loss: 0.019317	Acc: 59.1% (5911/10000)
[Test]  Epoch: 86	Loss: 0.019326	Acc: 59.2% (5920/10000)
[Test]  Epoch: 87	Loss: 0.019391	Acc: 59.0% (5897/10000)
[Test]  Epoch: 88	Loss: 0.019416	Acc: 58.8% (5878/10000)
[Test]  Epoch: 89	Loss: 0.019411	Acc: 59.0% (5895/10000)
[Test]  Epoch: 90	Loss: 0.019368	Acc: 59.1% (5910/10000)
[Test]  Epoch: 91	Loss: 0.019380	Acc: 58.7% (5868/10000)
[Test]  Epoch: 92	Loss: 0.019368	Acc: 58.8% (5883/10000)
[Test]  Epoch: 93	Loss: 0.019345	Acc: 58.9% (5889/10000)
[Test]  Epoch: 94	Loss: 0.019381	Acc: 58.9% (5887/10000)
[Test]  Epoch: 95	Loss: 0.019275	Acc: 59.1% (5910/10000)
[Test]  Epoch: 96	Loss: 0.019279	Acc: 59.0% (5905/10000)
[Test]  Epoch: 97	Loss: 0.019318	Acc: 59.0% (5900/10000)
[Test]  Epoch: 98	Loss: 0.019392	Acc: 58.9% (5891/10000)
[Test]  Epoch: 99	Loss: 0.019344	Acc: 59.0% (5897/10000)
[Test]  Epoch: 100	Loss: 0.019334	Acc: 58.9% (5894/10000)
===========finish==========
['2024-08-19', '17:53:07.516162', '100', 'test', '0.019334466713666915', '58.94', '59.21']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036241	Acc: 22.9% (2288/10000)
[Test]  Epoch: 2	Loss: 0.024143	Acc: 47.7% (4767/10000)
[Test]  Epoch: 3	Loss: 0.021981	Acc: 52.7% (5272/10000)
[Test]  Epoch: 4	Loss: 0.022245	Acc: 53.3% (5328/10000)
[Test]  Epoch: 5	Loss: 0.022301	Acc: 54.0% (5401/10000)
[Test]  Epoch: 6	Loss: 0.022274	Acc: 54.1% (5407/10000)
[Test]  Epoch: 7	Loss: 0.021819	Acc: 54.9% (5492/10000)
[Test]  Epoch: 8	Loss: 0.022250	Acc: 55.0% (5500/10000)
[Test]  Epoch: 9	Loss: 0.022299	Acc: 54.8% (5476/10000)
[Test]  Epoch: 10	Loss: 0.022025	Acc: 55.0% (5501/10000)
[Test]  Epoch: 11	Loss: 0.021898	Acc: 55.5% (5545/10000)
[Test]  Epoch: 12	Loss: 0.022202	Acc: 55.3% (5528/10000)
[Test]  Epoch: 13	Loss: 0.021808	Acc: 55.5% (5554/10000)
[Test]  Epoch: 14	Loss: 0.021576	Acc: 55.5% (5551/10000)
[Test]  Epoch: 15	Loss: 0.021850	Acc: 55.9% (5592/10000)
[Test]  Epoch: 16	Loss: 0.021605	Acc: 55.8% (5578/10000)
[Test]  Epoch: 17	Loss: 0.021831	Acc: 55.7% (5574/10000)
[Test]  Epoch: 18	Loss: 0.021428	Acc: 55.8% (5577/10000)
[Test]  Epoch: 19	Loss: 0.021664	Acc: 56.0% (5604/10000)
[Test]  Epoch: 20	Loss: 0.021282	Acc: 56.5% (5645/10000)
[Test]  Epoch: 21	Loss: 0.021235	Acc: 56.7% (5666/10000)
[Test]  Epoch: 22	Loss: 0.021343	Acc: 56.7% (5670/10000)
[Test]  Epoch: 23	Loss: 0.021300	Acc: 56.8% (5678/10000)
[Test]  Epoch: 24	Loss: 0.021048	Acc: 57.1% (5713/10000)
[Test]  Epoch: 25	Loss: 0.021103	Acc: 57.0% (5698/10000)
[Test]  Epoch: 26	Loss: 0.021075	Acc: 57.1% (5715/10000)
[Test]  Epoch: 27	Loss: 0.020969	Acc: 57.3% (5728/10000)
[Test]  Epoch: 28	Loss: 0.021097	Acc: 57.3% (5726/10000)
[Test]  Epoch: 29	Loss: 0.021143	Acc: 57.1% (5706/10000)
[Test]  Epoch: 30	Loss: 0.021203	Acc: 57.1% (5706/10000)
[Test]  Epoch: 31	Loss: 0.021144	Acc: 56.9% (5693/10000)
[Test]  Epoch: 32	Loss: 0.021194	Acc: 57.0% (5704/10000)
[Test]  Epoch: 33	Loss: 0.021109	Acc: 57.2% (5717/10000)
[Test]  Epoch: 34	Loss: 0.021102	Acc: 57.2% (5722/10000)
[Test]  Epoch: 35	Loss: 0.021123	Acc: 57.3% (5726/10000)
[Test]  Epoch: 36	Loss: 0.021101	Acc: 57.0% (5702/10000)
[Test]  Epoch: 37	Loss: 0.020960	Acc: 57.4% (5735/10000)
[Test]  Epoch: 38	Loss: 0.021035	Acc: 57.5% (5753/10000)
[Test]  Epoch: 39	Loss: 0.021322	Acc: 56.9% (5687/10000)
[Test]  Epoch: 40	Loss: 0.021134	Acc: 57.5% (5748/10000)
[Test]  Epoch: 41	Loss: 0.021001	Acc: 57.7% (5773/10000)
[Test]  Epoch: 42	Loss: 0.021143	Acc: 57.5% (5745/10000)
[Test]  Epoch: 43	Loss: 0.021081	Acc: 57.6% (5765/10000)
[Test]  Epoch: 44	Loss: 0.020928	Acc: 57.8% (5776/10000)
[Test]  Epoch: 45	Loss: 0.021083	Acc: 57.5% (5747/10000)
[Test]  Epoch: 46	Loss: 0.021012	Acc: 57.4% (5743/10000)
[Test]  Epoch: 47	Loss: 0.020903	Acc: 57.7% (5767/10000)
[Test]  Epoch: 48	Loss: 0.021018	Acc: 57.6% (5765/10000)
[Test]  Epoch: 49	Loss: 0.021011	Acc: 57.8% (5781/10000)
[Test]  Epoch: 50	Loss: 0.021021	Acc: 57.8% (5779/10000)
[Test]  Epoch: 51	Loss: 0.020998	Acc: 57.7% (5766/10000)
[Test]  Epoch: 52	Loss: 0.020995	Acc: 57.5% (5755/10000)
[Test]  Epoch: 53	Loss: 0.020965	Acc: 57.8% (5784/10000)
[Test]  Epoch: 54	Loss: 0.020896	Acc: 57.8% (5783/10000)
[Test]  Epoch: 55	Loss: 0.021013	Acc: 57.5% (5753/10000)
[Test]  Epoch: 56	Loss: 0.021010	Acc: 57.4% (5735/10000)
[Test]  Epoch: 57	Loss: 0.020918	Acc: 57.8% (5775/10000)
[Test]  Epoch: 58	Loss: 0.021058	Acc: 57.6% (5764/10000)
[Test]  Epoch: 59	Loss: 0.021104	Acc: 57.5% (5753/10000)
[Test]  Epoch: 60	Loss: 0.021157	Acc: 57.7% (5774/10000)
[Test]  Epoch: 61	Loss: 0.021105	Acc: 57.6% (5756/10000)
[Test]  Epoch: 62	Loss: 0.021069	Acc: 57.7% (5772/10000)
[Test]  Epoch: 63	Loss: 0.021020	Acc: 57.9% (5785/10000)
[Test]  Epoch: 64	Loss: 0.021017	Acc: 57.7% (5773/10000)
[Test]  Epoch: 65	Loss: 0.021097	Acc: 57.7% (5770/10000)
[Test]  Epoch: 66	Loss: 0.021068	Acc: 57.7% (5770/10000)
[Test]  Epoch: 67	Loss: 0.021181	Acc: 57.3% (5727/10000)
[Test]  Epoch: 68	Loss: 0.021065	Acc: 57.6% (5758/10000)
[Test]  Epoch: 69	Loss: 0.021047	Acc: 57.7% (5769/10000)
[Test]  Epoch: 70	Loss: 0.020966	Acc: 57.8% (5776/10000)
[Test]  Epoch: 71	Loss: 0.021047	Acc: 57.6% (5762/10000)
[Test]  Epoch: 72	Loss: 0.020994	Acc: 57.9% (5788/10000)
[Test]  Epoch: 73	Loss: 0.021067	Acc: 57.6% (5759/10000)
[Test]  Epoch: 74	Loss: 0.021047	Acc: 57.8% (5778/10000)
[Test]  Epoch: 75	Loss: 0.020990	Acc: 57.7% (5773/10000)
[Test]  Epoch: 76	Loss: 0.021038	Acc: 57.5% (5755/10000)
[Test]  Epoch: 77	Loss: 0.020961	Acc: 57.9% (5794/10000)
[Test]  Epoch: 78	Loss: 0.021087	Acc: 57.6% (5763/10000)
[Test]  Epoch: 79	Loss: 0.021052	Acc: 57.9% (5786/10000)
[Test]  Epoch: 80	Loss: 0.021035	Acc: 57.8% (5777/10000)
[Test]  Epoch: 81	Loss: 0.021027	Acc: 57.8% (5781/10000)
[Test]  Epoch: 82	Loss: 0.021034	Acc: 57.7% (5772/10000)
[Test]  Epoch: 83	Loss: 0.021043	Acc: 57.7% (5766/10000)
[Test]  Epoch: 84	Loss: 0.020971	Acc: 57.7% (5774/10000)
[Test]  Epoch: 85	Loss: 0.021015	Acc: 57.7% (5767/10000)
[Test]  Epoch: 86	Loss: 0.021047	Acc: 57.7% (5774/10000)
[Test]  Epoch: 87	Loss: 0.021083	Acc: 57.6% (5757/10000)
[Test]  Epoch: 88	Loss: 0.021070	Acc: 57.5% (5751/10000)
[Test]  Epoch: 89	Loss: 0.021053	Acc: 57.7% (5770/10000)
[Test]  Epoch: 90	Loss: 0.021043	Acc: 57.9% (5786/10000)
[Test]  Epoch: 91	Loss: 0.020976	Acc: 57.9% (5791/10000)
[Test]  Epoch: 92	Loss: 0.021039	Acc: 57.8% (5776/10000)
[Test]  Epoch: 93	Loss: 0.021017	Acc: 57.8% (5776/10000)
[Test]  Epoch: 94	Loss: 0.020983	Acc: 58.0% (5796/10000)
[Test]  Epoch: 95	Loss: 0.020966	Acc: 57.9% (5786/10000)
[Test]  Epoch: 96	Loss: 0.021009	Acc: 57.9% (5792/10000)
[Test]  Epoch: 97	Loss: 0.020987	Acc: 57.9% (5789/10000)
[Test]  Epoch: 98	Loss: 0.021002	Acc: 57.7% (5772/10000)
[Test]  Epoch: 99	Loss: 0.021041	Acc: 57.7% (5773/10000)
[Test]  Epoch: 100	Loss: 0.020921	Acc: 57.8% (5782/10000)
===========finish==========
['2024-08-19', '17:57:33.154736', '100', 'test', '0.020921491813659668', '57.82', '57.96']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('conv1.weight', 0.0), ('bn1.weight', 0.0), ('bn1.bias', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.bn1.weight', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.bn2.weight', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.conv3.weight', 0.0), ('layer1.0.bn3.weight', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.0.weight', 0.0), ('layer1.0.downsample.1.weight', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.bn1.weight', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.bn2.weight', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.conv3.weight', 0.0), ('layer1.1.bn3.weight', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.conv1.weight', 0.0), ('layer1.2.bn1.weight', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.conv2.weight', 0.0), ('layer1.2.bn2.weight', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.conv3.weight', 0.0), ('layer1.2.bn3.weight', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.bn1.weight', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.bn2.weight', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.conv3.weight', 0.0), ('layer2.0.bn3.weight', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.0.weight', 0.0), ('layer2.0.downsample.1.weight', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.bn1.weight', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.bn2.weight', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.conv3.weight', 0.0), ('layer2.1.bn3.weight', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.conv1.weight', 0.0), ('layer2.2.bn1.weight', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.conv2.weight', 0.0), ('layer2.2.bn2.weight', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.conv3.weight', 0.0), ('layer2.2.bn3.weight', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.conv1.weight', 0.0), ('layer2.3.bn1.weight', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.conv2.weight', 0.0), ('layer2.3.bn2.weight', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.conv3.weight', 0.0), ('layer2.3.bn3.weight', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.bn1.weight', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.bn2.weight', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.conv3.weight', 0.0), ('layer3.0.bn3.weight', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.0.weight', 0.0), ('layer3.0.downsample.1.weight', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.bn1.weight', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.bn2.weight', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.conv3.weight', 0.0), ('layer3.1.bn3.weight', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.conv1.weight', 0.0), ('layer3.2.bn1.weight', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.conv2.weight', 0.0), ('layer3.2.bn2.weight', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.conv3.weight', 0.0), ('layer3.2.bn3.weight', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.conv1.weight', 0.0), ('layer3.3.bn1.weight', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.conv2.weight', 0.0), ('layer3.3.bn2.weight', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.conv3.weight', 0.0), ('layer3.3.bn3.weight', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.conv1.weight', 0.0), ('layer3.4.bn1.weight', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.conv2.weight', 0.0), ('layer3.4.bn2.weight', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.conv3.weight', 0.0), ('layer3.4.bn3.weight', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.conv1.weight', 0.0), ('layer3.5.bn1.weight', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.conv2.weight', 0.0), ('layer3.5.bn2.weight', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.conv3.weight', 0.0), ('layer3.5.bn3.weight', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.bn1.weight', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.bn2.weight', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.conv3.weight', 0.0), ('layer4.0.bn3.weight', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.0.weight', 0.0), ('layer4.0.downsample.1.weight', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.bn1.weight', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.bn2.weight', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.conv3.weight', 0.0), ('layer4.1.bn3.weight', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.conv1.weight', 0.0), ('layer4.2.bn1.weight', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.conv2.weight', 0.0), ('layer4.2.bn2.weight', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.conv3.weight', 0.0), ('layer4.2.bn3.weight', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('conv1.weight', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.conv3.weight', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.conv3.weight', 0.0), ('layer1.2.conv1.weight', 0.0), ('layer1.2.conv2.weight', 0.0), ('layer1.2.conv3.weight', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.conv3.weight', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.conv3.weight', 0.0), ('layer2.2.conv1.weight', 0.0), ('layer2.2.conv2.weight', 0.0), ('layer2.2.conv3.weight', 0.0), ('layer2.3.conv1.weight', 0.0), ('layer2.3.conv2.weight', 0.0), ('layer2.3.conv3.weight', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.conv3.weight', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.conv3.weight', 0.0), ('layer3.2.conv1.weight', 0.0), ('layer3.2.conv2.weight', 0.0), ('layer3.2.conv3.weight', 0.0), ('layer3.3.conv1.weight', 0.0), ('layer3.3.conv2.weight', 0.0), ('layer3.3.conv3.weight', 0.0), ('layer3.4.conv1.weight', 0.0), ('layer3.4.conv2.weight', 0.0), ('layer3.4.conv3.weight', 0.0), ('layer3.5.conv1.weight', 0.0), ('layer3.5.conv2.weight', 0.0), ('layer3.5.conv3.weight', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.conv3.weight', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.conv3.weight', 0.0), ('layer4.2.conv1.weight', 0.0), ('layer4.2.conv2.weight', 0.0), ('layer4.2.conv3.weight', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.032274	Acc: 54.8% (5483/10000)
[Test]  Epoch: 2	Loss: 0.032157	Acc: 54.9% (5492/10000)
[Test]  Epoch: 3	Loss: 0.032259	Acc: 55.0% (5505/10000)
[Test]  Epoch: 4	Loss: 0.032232	Acc: 55.3% (5526/10000)
[Test]  Epoch: 5	Loss: 0.032359	Acc: 55.2% (5522/10000)
[Test]  Epoch: 6	Loss: 0.032440	Acc: 55.1% (5506/10000)
[Test]  Epoch: 7	Loss: 0.032412	Acc: 55.2% (5524/10000)
[Test]  Epoch: 8	Loss: 0.032483	Acc: 55.3% (5526/10000)
[Test]  Epoch: 9	Loss: 0.032505	Acc: 55.1% (5513/10000)
[Test]  Epoch: 10	Loss: 0.032562	Acc: 55.2% (5521/10000)
[Test]  Epoch: 11	Loss: 0.032618	Acc: 55.1% (5515/10000)
[Test]  Epoch: 12	Loss: 0.032545	Acc: 55.1% (5511/10000)
[Test]  Epoch: 13	Loss: 0.032654	Acc: 55.2% (5519/10000)
[Test]  Epoch: 14	Loss: 0.032690	Acc: 55.3% (5534/10000)
[Test]  Epoch: 15	Loss: 0.032762	Acc: 55.3% (5529/10000)
[Test]  Epoch: 16	Loss: 0.032713	Acc: 55.1% (5513/10000)
[Test]  Epoch: 17	Loss: 0.032697	Acc: 55.3% (5529/10000)
[Test]  Epoch: 18	Loss: 0.032737	Acc: 55.4% (5543/10000)
[Test]  Epoch: 19	Loss: 0.032779	Acc: 55.1% (5512/10000)
[Test]  Epoch: 20	Loss: 0.032808	Acc: 55.4% (5535/10000)
[Test]  Epoch: 21	Loss: 0.032767	Acc: 55.3% (5529/10000)
[Test]  Epoch: 22	Loss: 0.032837	Acc: 55.3% (5529/10000)
[Test]  Epoch: 23	Loss: 0.032846	Acc: 55.3% (5532/10000)
[Test]  Epoch: 24	Loss: 0.032934	Acc: 55.2% (5521/10000)
[Test]  Epoch: 25	Loss: 0.032846	Acc: 55.3% (5532/10000)
[Test]  Epoch: 26	Loss: 0.032822	Acc: 55.3% (5534/10000)
[Test]  Epoch: 27	Loss: 0.032873	Acc: 55.3% (5529/10000)
[Test]  Epoch: 28	Loss: 0.032972	Acc: 55.2% (5518/10000)
[Test]  Epoch: 29	Loss: 0.032956	Acc: 55.3% (5528/10000)
[Test]  Epoch: 30	Loss: 0.032924	Acc: 55.4% (5541/10000)
[Test]  Epoch: 31	Loss: 0.033065	Acc: 55.2% (5519/10000)
[Test]  Epoch: 32	Loss: 0.033066	Acc: 55.1% (5511/10000)
[Test]  Epoch: 33	Loss: 0.033039	Acc: 55.3% (5531/10000)
[Test]  Epoch: 34	Loss: 0.032959	Acc: 55.4% (5537/10000)
[Test]  Epoch: 35	Loss: 0.033026	Acc: 55.4% (5542/10000)
[Test]  Epoch: 36	Loss: 0.033062	Acc: 55.3% (5530/10000)
[Test]  Epoch: 37	Loss: 0.033093	Acc: 55.2% (5525/10000)
[Test]  Epoch: 38	Loss: 0.033153	Acc: 55.2% (5523/10000)
[Test]  Epoch: 39	Loss: 0.033153	Acc: 55.3% (5527/10000)
[Test]  Epoch: 40	Loss: 0.033178	Acc: 55.2% (5525/10000)
[Test]  Epoch: 41	Loss: 0.033129	Acc: 55.3% (5534/10000)
[Test]  Epoch: 42	Loss: 0.033212	Acc: 55.3% (5531/10000)
[Test]  Epoch: 43	Loss: 0.033197	Acc: 55.3% (5533/10000)
[Test]  Epoch: 44	Loss: 0.033204	Acc: 55.5% (5549/10000)
[Test]  Epoch: 45	Loss: 0.033208	Acc: 55.4% (5535/10000)
[Test]  Epoch: 46	Loss: 0.033271	Acc: 55.2% (5521/10000)
[Test]  Epoch: 47	Loss: 0.033202	Acc: 55.4% (5542/10000)
[Test]  Epoch: 48	Loss: 0.033201	Acc: 55.4% (5538/10000)
[Test]  Epoch: 49	Loss: 0.033261	Acc: 55.2% (5519/10000)
[Test]  Epoch: 50	Loss: 0.033348	Acc: 55.4% (5541/10000)
[Test]  Epoch: 51	Loss: 0.033331	Acc: 55.6% (5564/10000)
[Test]  Epoch: 52	Loss: 0.033293	Acc: 55.3% (5533/10000)
[Test]  Epoch: 53	Loss: 0.033356	Acc: 55.4% (5541/10000)
[Test]  Epoch: 54	Loss: 0.033354	Acc: 55.4% (5539/10000)
[Test]  Epoch: 55	Loss: 0.033346	Acc: 55.5% (5545/10000)
[Test]  Epoch: 56	Loss: 0.033340	Acc: 55.6% (5563/10000)
[Test]  Epoch: 57	Loss: 0.033348	Acc: 55.5% (5545/10000)
[Test]  Epoch: 58	Loss: 0.033343	Acc: 55.5% (5549/10000)
[Test]  Epoch: 59	Loss: 0.033417	Acc: 55.5% (5551/10000)
[Test]  Epoch: 60	Loss: 0.033485	Acc: 55.4% (5536/10000)
[Test]  Epoch: 61	Loss: 0.033493	Acc: 55.5% (5548/10000)
[Test]  Epoch: 62	Loss: 0.033438	Acc: 55.5% (5546/10000)
[Test]  Epoch: 63	Loss: 0.033412	Acc: 55.4% (5538/10000)
[Test]  Epoch: 64	Loss: 0.033434	Acc: 55.3% (5533/10000)
[Test]  Epoch: 65	Loss: 0.033424	Acc: 55.4% (5538/10000)
[Test]  Epoch: 66	Loss: 0.033452	Acc: 55.6% (5558/10000)
[Test]  Epoch: 67	Loss: 0.033413	Acc: 55.6% (5558/10000)
[Test]  Epoch: 68	Loss: 0.033470	Acc: 55.5% (5552/10000)
[Test]  Epoch: 69	Loss: 0.033452	Acc: 55.6% (5558/10000)
[Test]  Epoch: 70	Loss: 0.033389	Acc: 55.6% (5556/10000)
[Test]  Epoch: 71	Loss: 0.033480	Acc: 55.3% (5534/10000)
[Test]  Epoch: 72	Loss: 0.033446	Acc: 55.4% (5542/10000)
[Test]  Epoch: 73	Loss: 0.033375	Acc: 55.4% (5540/10000)
[Test]  Epoch: 74	Loss: 0.033356	Acc: 55.5% (5548/10000)
[Test]  Epoch: 75	Loss: 0.033386	Acc: 55.5% (5553/10000)
[Test]  Epoch: 76	Loss: 0.033343	Acc: 55.6% (5556/10000)
[Test]  Epoch: 77	Loss: 0.033389	Acc: 55.4% (5540/10000)
[Test]  Epoch: 78	Loss: 0.033438	Acc: 55.3% (5532/10000)
[Test]  Epoch: 79	Loss: 0.033428	Acc: 55.5% (5548/10000)
[Test]  Epoch: 80	Loss: 0.033442	Acc: 55.4% (5540/10000)
[Test]  Epoch: 81	Loss: 0.033479	Acc: 55.4% (5540/10000)
[Test]  Epoch: 82	Loss: 0.033497	Acc: 55.3% (5534/10000)
[Test]  Epoch: 83	Loss: 0.033442	Acc: 55.4% (5539/10000)
[Test]  Epoch: 84	Loss: 0.033481	Acc: 55.5% (5547/10000)
[Test]  Epoch: 85	Loss: 0.033444	Acc: 55.5% (5546/10000)
[Test]  Epoch: 86	Loss: 0.033500	Acc: 55.2% (5520/10000)
[Test]  Epoch: 87	Loss: 0.033445	Acc: 55.4% (5544/10000)
[Test]  Epoch: 88	Loss: 0.033443	Acc: 55.4% (5540/10000)
[Test]  Epoch: 89	Loss: 0.033474	Acc: 55.5% (5551/10000)
[Test]  Epoch: 90	Loss: 0.033480	Acc: 55.5% (5546/10000)
[Test]  Epoch: 91	Loss: 0.033469	Acc: 55.6% (5556/10000)
[Test]  Epoch: 92	Loss: 0.033379	Acc: 55.4% (5540/10000)
[Test]  Epoch: 93	Loss: 0.033460	Acc: 55.5% (5551/10000)
[Test]  Epoch: 94	Loss: 0.033442	Acc: 55.5% (5547/10000)
[Test]  Epoch: 95	Loss: 0.033442	Acc: 55.5% (5545/10000)
[Test]  Epoch: 96	Loss: 0.033426	Acc: 55.5% (5555/10000)
[Test]  Epoch: 97	Loss: 0.033463	Acc: 55.4% (5536/10000)
[Test]  Epoch: 98	Loss: 0.033452	Acc: 55.5% (5552/10000)
[Test]  Epoch: 99	Loss: 0.033439	Acc: 55.4% (5544/10000)
[Test]  Epoch: 100	Loss: 0.033431	Acc: 55.5% (5549/10000)
===========finish==========
['2024-08-19', '18:08:34.047478', '100', 'test', '0.03343064622879028', '55.49', '55.64']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036526	Acc: 48.3% (4829/10000)
[Test]  Epoch: 2	Loss: 0.033797	Acc: 52.4% (5240/10000)
[Test]  Epoch: 3	Loss: 0.033798	Acc: 52.6% (5262/10000)
[Test]  Epoch: 4	Loss: 0.033631	Acc: 53.0% (5305/10000)
[Test]  Epoch: 5	Loss: 0.033721	Acc: 52.7% (5270/10000)
[Test]  Epoch: 6	Loss: 0.033731	Acc: 53.1% (5307/10000)
[Test]  Epoch: 7	Loss: 0.033741	Acc: 53.1% (5309/10000)
[Test]  Epoch: 8	Loss: 0.033746	Acc: 53.1% (5306/10000)
[Test]  Epoch: 9	Loss: 0.033773	Acc: 53.1% (5308/10000)
[Test]  Epoch: 10	Loss: 0.033727	Acc: 53.4% (5339/10000)
[Test]  Epoch: 11	Loss: 0.033808	Acc: 53.2% (5320/10000)
[Test]  Epoch: 12	Loss: 0.033701	Acc: 53.3% (5334/10000)
[Test]  Epoch: 13	Loss: 0.033832	Acc: 53.4% (5344/10000)
[Test]  Epoch: 14	Loss: 0.033859	Acc: 53.3% (5327/10000)
[Test]  Epoch: 15	Loss: 0.033952	Acc: 53.4% (5335/10000)
[Test]  Epoch: 16	Loss: 0.033861	Acc: 53.3% (5334/10000)
[Test]  Epoch: 17	Loss: 0.033872	Acc: 53.3% (5334/10000)
[Test]  Epoch: 18	Loss: 0.033926	Acc: 53.3% (5333/10000)
[Test]  Epoch: 19	Loss: 0.033901	Acc: 53.5% (5354/10000)
[Test]  Epoch: 20	Loss: 0.033969	Acc: 53.4% (5341/10000)
[Test]  Epoch: 21	Loss: 0.033911	Acc: 53.2% (5324/10000)
[Test]  Epoch: 22	Loss: 0.033998	Acc: 53.3% (5332/10000)
[Test]  Epoch: 23	Loss: 0.033970	Acc: 53.4% (5342/10000)
[Test]  Epoch: 24	Loss: 0.034077	Acc: 53.4% (5341/10000)
[Test]  Epoch: 25	Loss: 0.034000	Acc: 53.4% (5340/10000)
[Test]  Epoch: 26	Loss: 0.033959	Acc: 53.6% (5364/10000)
[Test]  Epoch: 27	Loss: 0.034002	Acc: 53.5% (5347/10000)
[Test]  Epoch: 28	Loss: 0.034101	Acc: 53.5% (5350/10000)
[Test]  Epoch: 29	Loss: 0.034074	Acc: 53.7% (5371/10000)
[Test]  Epoch: 30	Loss: 0.034012	Acc: 53.6% (5363/10000)
[Test]  Epoch: 31	Loss: 0.034176	Acc: 53.4% (5341/10000)
[Test]  Epoch: 32	Loss: 0.034192	Acc: 53.5% (5347/10000)
[Test]  Epoch: 33	Loss: 0.034133	Acc: 53.5% (5354/10000)
[Test]  Epoch: 34	Loss: 0.034060	Acc: 53.8% (5375/10000)
[Test]  Epoch: 35	Loss: 0.034117	Acc: 53.7% (5366/10000)
[Test]  Epoch: 36	Loss: 0.034204	Acc: 53.5% (5348/10000)
[Test]  Epoch: 37	Loss: 0.034204	Acc: 53.7% (5370/10000)
[Test]  Epoch: 38	Loss: 0.034234	Acc: 53.8% (5377/10000)
[Test]  Epoch: 39	Loss: 0.034265	Acc: 53.7% (5367/10000)
[Test]  Epoch: 40	Loss: 0.034267	Acc: 53.7% (5370/10000)
[Test]  Epoch: 41	Loss: 0.034200	Acc: 53.6% (5363/10000)
[Test]  Epoch: 42	Loss: 0.034272	Acc: 53.6% (5362/10000)
[Test]  Epoch: 43	Loss: 0.034248	Acc: 53.8% (5383/10000)
[Test]  Epoch: 44	Loss: 0.034287	Acc: 53.6% (5363/10000)
[Test]  Epoch: 45	Loss: 0.034285	Acc: 53.7% (5370/10000)
[Test]  Epoch: 46	Loss: 0.034350	Acc: 53.7% (5370/10000)
[Test]  Epoch: 47	Loss: 0.034322	Acc: 53.7% (5371/10000)
[Test]  Epoch: 48	Loss: 0.034284	Acc: 53.6% (5362/10000)
[Test]  Epoch: 49	Loss: 0.034349	Acc: 53.5% (5354/10000)
[Test]  Epoch: 50	Loss: 0.034442	Acc: 53.9% (5385/10000)
[Test]  Epoch: 51	Loss: 0.034430	Acc: 53.7% (5374/10000)
[Test]  Epoch: 52	Loss: 0.034396	Acc: 53.7% (5372/10000)
[Test]  Epoch: 53	Loss: 0.034445	Acc: 53.7% (5367/10000)
[Test]  Epoch: 54	Loss: 0.034439	Acc: 53.9% (5385/10000)
[Test]  Epoch: 55	Loss: 0.034430	Acc: 53.6% (5360/10000)
[Test]  Epoch: 56	Loss: 0.034423	Acc: 53.6% (5360/10000)
[Test]  Epoch: 57	Loss: 0.034409	Acc: 53.8% (5377/10000)
[Test]  Epoch: 58	Loss: 0.034416	Acc: 53.6% (5362/10000)
[Test]  Epoch: 59	Loss: 0.034485	Acc: 53.7% (5368/10000)
[Test]  Epoch: 60	Loss: 0.034558	Acc: 53.8% (5377/10000)
[Test]  Epoch: 61	Loss: 0.034575	Acc: 53.7% (5366/10000)
[Test]  Epoch: 62	Loss: 0.034521	Acc: 53.7% (5367/10000)
[Test]  Epoch: 63	Loss: 0.034487	Acc: 53.7% (5374/10000)
[Test]  Epoch: 64	Loss: 0.034503	Acc: 53.7% (5370/10000)
[Test]  Epoch: 65	Loss: 0.034498	Acc: 53.7% (5366/10000)
[Test]  Epoch: 66	Loss: 0.034525	Acc: 53.7% (5369/10000)
[Test]  Epoch: 67	Loss: 0.034498	Acc: 53.7% (5373/10000)
[Test]  Epoch: 68	Loss: 0.034562	Acc: 53.8% (5376/10000)
[Test]  Epoch: 69	Loss: 0.034515	Acc: 53.9% (5389/10000)
[Test]  Epoch: 70	Loss: 0.034457	Acc: 53.8% (5382/10000)
[Test]  Epoch: 71	Loss: 0.034543	Acc: 53.7% (5367/10000)
[Test]  Epoch: 72	Loss: 0.034525	Acc: 53.7% (5371/10000)
[Test]  Epoch: 73	Loss: 0.034469	Acc: 53.7% (5367/10000)
[Test]  Epoch: 74	Loss: 0.034446	Acc: 53.7% (5372/10000)
[Test]  Epoch: 75	Loss: 0.034486	Acc: 53.9% (5387/10000)
[Test]  Epoch: 76	Loss: 0.034429	Acc: 53.6% (5364/10000)
[Test]  Epoch: 77	Loss: 0.034445	Acc: 54.0% (5395/10000)
[Test]  Epoch: 78	Loss: 0.034510	Acc: 53.8% (5376/10000)
[Test]  Epoch: 79	Loss: 0.034485	Acc: 53.8% (5375/10000)
[Test]  Epoch: 80	Loss: 0.034532	Acc: 53.5% (5355/10000)
[Test]  Epoch: 81	Loss: 0.034560	Acc: 53.7% (5373/10000)
[Test]  Epoch: 82	Loss: 0.034551	Acc: 53.6% (5364/10000)
[Test]  Epoch: 83	Loss: 0.034514	Acc: 53.8% (5380/10000)
[Test]  Epoch: 84	Loss: 0.034557	Acc: 53.8% (5376/10000)
[Test]  Epoch: 85	Loss: 0.034538	Acc: 53.8% (5382/10000)
[Test]  Epoch: 86	Loss: 0.034581	Acc: 53.7% (5369/10000)
[Test]  Epoch: 87	Loss: 0.034525	Acc: 53.8% (5378/10000)
[Test]  Epoch: 88	Loss: 0.034505	Acc: 53.8% (5380/10000)
[Test]  Epoch: 89	Loss: 0.034548	Acc: 53.8% (5378/10000)
[Test]  Epoch: 90	Loss: 0.034568	Acc: 53.7% (5374/10000)
[Test]  Epoch: 91	Loss: 0.034547	Acc: 53.9% (5389/10000)
[Test]  Epoch: 92	Loss: 0.034447	Acc: 53.8% (5376/10000)
[Test]  Epoch: 93	Loss: 0.034536	Acc: 53.8% (5379/10000)
[Test]  Epoch: 94	Loss: 0.034521	Acc: 53.8% (5377/10000)
[Test]  Epoch: 95	Loss: 0.034514	Acc: 53.8% (5381/10000)
[Test]  Epoch: 96	Loss: 0.034505	Acc: 53.9% (5386/10000)
[Test]  Epoch: 97	Loss: 0.034540	Acc: 53.6% (5364/10000)
[Test]  Epoch: 98	Loss: 0.034537	Acc: 53.5% (5355/10000)
[Test]  Epoch: 99	Loss: 0.034522	Acc: 53.8% (5379/10000)
[Test]  Epoch: 100	Loss: 0.034512	Acc: 53.8% (5381/10000)
===========finish==========
['2024-08-19', '18:15:50.002071', '100', 'test', '0.034512230062484744', '53.81', '53.95']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.038275	Acc: 45.9% (4587/10000)
[Test]  Epoch: 2	Loss: 0.034088	Acc: 52.0% (5204/10000)
[Test]  Epoch: 3	Loss: 0.033963	Acc: 52.4% (5239/10000)
[Test]  Epoch: 4	Loss: 0.033787	Acc: 52.7% (5272/10000)
[Test]  Epoch: 5	Loss: 0.033882	Acc: 52.7% (5267/10000)
[Test]  Epoch: 6	Loss: 0.033927	Acc: 52.6% (5262/10000)
[Test]  Epoch: 7	Loss: 0.033878	Acc: 53.0% (5302/10000)
[Test]  Epoch: 8	Loss: 0.033911	Acc: 52.8% (5277/10000)
[Test]  Epoch: 9	Loss: 0.033949	Acc: 52.8% (5277/10000)
[Test]  Epoch: 10	Loss: 0.033936	Acc: 52.9% (5292/10000)
[Test]  Epoch: 11	Loss: 0.033993	Acc: 53.1% (5315/10000)
[Test]  Epoch: 12	Loss: 0.033901	Acc: 53.0% (5301/10000)
[Test]  Epoch: 13	Loss: 0.034042	Acc: 52.9% (5292/10000)
[Test]  Epoch: 14	Loss: 0.034064	Acc: 52.8% (5280/10000)
[Test]  Epoch: 15	Loss: 0.034144	Acc: 52.9% (5294/10000)
[Test]  Epoch: 16	Loss: 0.034062	Acc: 52.9% (5287/10000)
[Test]  Epoch: 17	Loss: 0.034064	Acc: 53.0% (5295/10000)
[Test]  Epoch: 18	Loss: 0.034108	Acc: 52.9% (5291/10000)
[Test]  Epoch: 19	Loss: 0.034114	Acc: 53.1% (5308/10000)
[Test]  Epoch: 20	Loss: 0.034154	Acc: 53.0% (5305/10000)
[Test]  Epoch: 21	Loss: 0.034118	Acc: 53.1% (5313/10000)
[Test]  Epoch: 22	Loss: 0.034175	Acc: 53.0% (5298/10000)
[Test]  Epoch: 23	Loss: 0.034155	Acc: 53.3% (5333/10000)
[Test]  Epoch: 24	Loss: 0.034285	Acc: 53.1% (5310/10000)
[Test]  Epoch: 25	Loss: 0.034198	Acc: 53.3% (5328/10000)
[Test]  Epoch: 26	Loss: 0.034147	Acc: 53.4% (5338/10000)
[Test]  Epoch: 27	Loss: 0.034219	Acc: 53.3% (5327/10000)
[Test]  Epoch: 28	Loss: 0.034315	Acc: 53.2% (5325/10000)
[Test]  Epoch: 29	Loss: 0.034299	Acc: 53.3% (5331/10000)
[Test]  Epoch: 30	Loss: 0.034210	Acc: 53.2% (5321/10000)
[Test]  Epoch: 31	Loss: 0.034381	Acc: 53.2% (5318/10000)
[Test]  Epoch: 32	Loss: 0.034403	Acc: 53.2% (5324/10000)
[Test]  Epoch: 33	Loss: 0.034350	Acc: 53.1% (5311/10000)
[Test]  Epoch: 34	Loss: 0.034257	Acc: 53.3% (5328/10000)
[Test]  Epoch: 35	Loss: 0.034313	Acc: 53.5% (5348/10000)
[Test]  Epoch: 36	Loss: 0.034396	Acc: 53.1% (5313/10000)
[Test]  Epoch: 37	Loss: 0.034412	Acc: 53.2% (5319/10000)
[Test]  Epoch: 38	Loss: 0.034413	Acc: 53.4% (5344/10000)
[Test]  Epoch: 39	Loss: 0.034456	Acc: 53.4% (5336/10000)
[Test]  Epoch: 40	Loss: 0.034483	Acc: 53.2% (5324/10000)
[Test]  Epoch: 41	Loss: 0.034396	Acc: 53.4% (5343/10000)
[Test]  Epoch: 42	Loss: 0.034457	Acc: 53.4% (5336/10000)
[Test]  Epoch: 43	Loss: 0.034445	Acc: 53.4% (5336/10000)
[Test]  Epoch: 44	Loss: 0.034501	Acc: 53.3% (5326/10000)
[Test]  Epoch: 45	Loss: 0.034487	Acc: 53.5% (5354/10000)
[Test]  Epoch: 46	Loss: 0.034569	Acc: 53.4% (5336/10000)
[Test]  Epoch: 47	Loss: 0.034516	Acc: 53.5% (5355/10000)
[Test]  Epoch: 48	Loss: 0.034483	Acc: 53.5% (5348/10000)
[Test]  Epoch: 49	Loss: 0.034554	Acc: 53.5% (5346/10000)
[Test]  Epoch: 50	Loss: 0.034626	Acc: 53.4% (5337/10000)
[Test]  Epoch: 51	Loss: 0.034606	Acc: 53.4% (5340/10000)
[Test]  Epoch: 52	Loss: 0.034604	Acc: 53.2% (5323/10000)
[Test]  Epoch: 53	Loss: 0.034655	Acc: 53.2% (5324/10000)
[Test]  Epoch: 54	Loss: 0.034640	Acc: 53.5% (5347/10000)
[Test]  Epoch: 55	Loss: 0.034641	Acc: 53.3% (5334/10000)
[Test]  Epoch: 56	Loss: 0.034645	Acc: 53.4% (5335/10000)
[Test]  Epoch: 57	Loss: 0.034623	Acc: 53.6% (5361/10000)
[Test]  Epoch: 58	Loss: 0.034610	Acc: 53.3% (5329/10000)
[Test]  Epoch: 59	Loss: 0.034684	Acc: 53.3% (5329/10000)
[Test]  Epoch: 60	Loss: 0.034768	Acc: 53.5% (5351/10000)
[Test]  Epoch: 61	Loss: 0.034787	Acc: 53.5% (5345/10000)
[Test]  Epoch: 62	Loss: 0.034732	Acc: 53.5% (5348/10000)
[Test]  Epoch: 63	Loss: 0.034690	Acc: 53.6% (5359/10000)
[Test]  Epoch: 64	Loss: 0.034723	Acc: 53.4% (5340/10000)
[Test]  Epoch: 65	Loss: 0.034708	Acc: 53.5% (5346/10000)
[Test]  Epoch: 66	Loss: 0.034723	Acc: 53.4% (5335/10000)
[Test]  Epoch: 67	Loss: 0.034709	Acc: 53.5% (5354/10000)
[Test]  Epoch: 68	Loss: 0.034769	Acc: 53.4% (5340/10000)
[Test]  Epoch: 69	Loss: 0.034724	Acc: 53.5% (5348/10000)
[Test]  Epoch: 70	Loss: 0.034669	Acc: 53.4% (5343/10000)
[Test]  Epoch: 71	Loss: 0.034763	Acc: 53.2% (5318/10000)
[Test]  Epoch: 72	Loss: 0.034742	Acc: 53.5% (5346/10000)
[Test]  Epoch: 73	Loss: 0.034681	Acc: 53.3% (5330/10000)
[Test]  Epoch: 74	Loss: 0.034648	Acc: 53.4% (5342/10000)
[Test]  Epoch: 75	Loss: 0.034695	Acc: 53.5% (5355/10000)
[Test]  Epoch: 76	Loss: 0.034634	Acc: 53.6% (5356/10000)
[Test]  Epoch: 77	Loss: 0.034661	Acc: 53.4% (5344/10000)
[Test]  Epoch: 78	Loss: 0.034740	Acc: 53.5% (5346/10000)
[Test]  Epoch: 79	Loss: 0.034688	Acc: 53.5% (5350/10000)
[Test]  Epoch: 80	Loss: 0.034738	Acc: 53.2% (5320/10000)
[Test]  Epoch: 81	Loss: 0.034762	Acc: 53.4% (5337/10000)
[Test]  Epoch: 82	Loss: 0.034759	Acc: 53.1% (5311/10000)
[Test]  Epoch: 83	Loss: 0.034722	Acc: 53.3% (5331/10000)
[Test]  Epoch: 84	Loss: 0.034760	Acc: 53.5% (5350/10000)
[Test]  Epoch: 85	Loss: 0.034745	Acc: 53.4% (5340/10000)
[Test]  Epoch: 86	Loss: 0.034798	Acc: 53.3% (5331/10000)
[Test]  Epoch: 87	Loss: 0.034737	Acc: 53.4% (5339/10000)
[Test]  Epoch: 88	Loss: 0.034725	Acc: 53.5% (5348/10000)
[Test]  Epoch: 89	Loss: 0.034762	Acc: 53.3% (5331/10000)
[Test]  Epoch: 90	Loss: 0.034780	Acc: 53.4% (5340/10000)
[Test]  Epoch: 91	Loss: 0.034765	Acc: 53.5% (5346/10000)
[Test]  Epoch: 92	Loss: 0.034650	Acc: 53.4% (5344/10000)
[Test]  Epoch: 93	Loss: 0.034749	Acc: 53.4% (5340/10000)
[Test]  Epoch: 94	Loss: 0.034741	Acc: 53.5% (5350/10000)
[Test]  Epoch: 95	Loss: 0.034721	Acc: 53.5% (5345/10000)
[Test]  Epoch: 96	Loss: 0.034718	Acc: 53.4% (5343/10000)
[Test]  Epoch: 97	Loss: 0.034754	Acc: 53.4% (5335/10000)
[Test]  Epoch: 98	Loss: 0.034747	Acc: 53.3% (5330/10000)
[Test]  Epoch: 99	Loss: 0.034735	Acc: 53.5% (5348/10000)
[Test]  Epoch: 100	Loss: 0.034730	Acc: 53.4% (5344/10000)
===========finish==========
['2024-08-19', '18:22:57.657932', '100', 'test', '0.034729806214571', '53.44', '53.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.038915	Acc: 44.3% (4433/10000)
[Test]  Epoch: 2	Loss: 0.034511	Acc: 51.6% (5157/10000)
[Test]  Epoch: 3	Loss: 0.034365	Acc: 51.7% (5172/10000)
[Test]  Epoch: 4	Loss: 0.034211	Acc: 52.1% (5214/10000)
[Test]  Epoch: 5	Loss: 0.034321	Acc: 51.8% (5181/10000)
[Test]  Epoch: 6	Loss: 0.034372	Acc: 52.1% (5211/10000)
[Test]  Epoch: 7	Loss: 0.034310	Acc: 51.8% (5184/10000)
[Test]  Epoch: 8	Loss: 0.034325	Acc: 52.1% (5207/10000)
[Test]  Epoch: 9	Loss: 0.034340	Acc: 52.1% (5213/10000)
[Test]  Epoch: 10	Loss: 0.034342	Acc: 52.2% (5220/10000)
[Test]  Epoch: 11	Loss: 0.034379	Acc: 52.3% (5234/10000)
[Test]  Epoch: 12	Loss: 0.034314	Acc: 52.2% (5218/10000)
[Test]  Epoch: 13	Loss: 0.034476	Acc: 52.2% (5220/10000)
[Test]  Epoch: 14	Loss: 0.034495	Acc: 52.3% (5232/10000)
[Test]  Epoch: 15	Loss: 0.034551	Acc: 52.4% (5243/10000)
[Test]  Epoch: 16	Loss: 0.034462	Acc: 52.4% (5242/10000)
[Test]  Epoch: 17	Loss: 0.034440	Acc: 52.5% (5247/10000)
[Test]  Epoch: 18	Loss: 0.034471	Acc: 52.4% (5243/10000)
[Test]  Epoch: 19	Loss: 0.034511	Acc: 52.5% (5252/10000)
[Test]  Epoch: 20	Loss: 0.034538	Acc: 52.4% (5241/10000)
[Test]  Epoch: 21	Loss: 0.034481	Acc: 52.5% (5247/10000)
[Test]  Epoch: 22	Loss: 0.034554	Acc: 52.4% (5237/10000)
[Test]  Epoch: 23	Loss: 0.034544	Acc: 52.5% (5255/10000)
[Test]  Epoch: 24	Loss: 0.034652	Acc: 52.4% (5242/10000)
[Test]  Epoch: 25	Loss: 0.034587	Acc: 52.5% (5253/10000)
[Test]  Epoch: 26	Loss: 0.034531	Acc: 52.6% (5265/10000)
[Test]  Epoch: 27	Loss: 0.034580	Acc: 52.5% (5255/10000)
[Test]  Epoch: 28	Loss: 0.034661	Acc: 52.6% (5257/10000)
[Test]  Epoch: 29	Loss: 0.034669	Acc: 52.8% (5277/10000)
[Test]  Epoch: 30	Loss: 0.034634	Acc: 52.9% (5291/10000)
[Test]  Epoch: 31	Loss: 0.034790	Acc: 52.6% (5258/10000)
[Test]  Epoch: 32	Loss: 0.034780	Acc: 52.4% (5237/10000)
[Test]  Epoch: 33	Loss: 0.034745	Acc: 52.5% (5247/10000)
[Test]  Epoch: 34	Loss: 0.034641	Acc: 52.4% (5244/10000)
[Test]  Epoch: 35	Loss: 0.034688	Acc: 52.6% (5264/10000)
[Test]  Epoch: 36	Loss: 0.034747	Acc: 52.7% (5271/10000)
[Test]  Epoch: 37	Loss: 0.034754	Acc: 52.7% (5269/10000)
[Test]  Epoch: 38	Loss: 0.034783	Acc: 52.6% (5261/10000)
[Test]  Epoch: 39	Loss: 0.034839	Acc: 52.6% (5261/10000)
[Test]  Epoch: 40	Loss: 0.034842	Acc: 52.6% (5265/10000)
[Test]  Epoch: 41	Loss: 0.034775	Acc: 52.9% (5285/10000)
[Test]  Epoch: 42	Loss: 0.034830	Acc: 52.7% (5273/10000)
[Test]  Epoch: 43	Loss: 0.034819	Acc: 52.6% (5264/10000)
[Test]  Epoch: 44	Loss: 0.034873	Acc: 52.6% (5265/10000)
[Test]  Epoch: 45	Loss: 0.034841	Acc: 52.7% (5266/10000)
[Test]  Epoch: 46	Loss: 0.034938	Acc: 52.7% (5267/10000)
[Test]  Epoch: 47	Loss: 0.034880	Acc: 52.9% (5286/10000)
[Test]  Epoch: 48	Loss: 0.034866	Acc: 52.8% (5284/10000)
[Test]  Epoch: 49	Loss: 0.034898	Acc: 53.0% (5295/10000)
[Test]  Epoch: 50	Loss: 0.034994	Acc: 52.8% (5279/10000)
[Test]  Epoch: 51	Loss: 0.034969	Acc: 52.9% (5286/10000)
[Test]  Epoch: 52	Loss: 0.034963	Acc: 52.7% (5271/10000)
[Test]  Epoch: 53	Loss: 0.034971	Acc: 52.8% (5279/10000)
[Test]  Epoch: 54	Loss: 0.035009	Acc: 52.7% (5269/10000)
[Test]  Epoch: 55	Loss: 0.034992	Acc: 52.7% (5266/10000)
[Test]  Epoch: 56	Loss: 0.035026	Acc: 52.6% (5262/10000)
[Test]  Epoch: 57	Loss: 0.035023	Acc: 52.8% (5277/10000)
[Test]  Epoch: 58	Loss: 0.035003	Acc: 52.8% (5280/10000)
[Test]  Epoch: 59	Loss: 0.035041	Acc: 52.7% (5273/10000)
[Test]  Epoch: 60	Loss: 0.035110	Acc: 53.0% (5301/10000)
[Test]  Epoch: 61	Loss: 0.035152	Acc: 53.0% (5302/10000)
[Test]  Epoch: 62	Loss: 0.035073	Acc: 53.0% (5301/10000)
[Test]  Epoch: 63	Loss: 0.035037	Acc: 52.8% (5282/10000)
[Test]  Epoch: 64	Loss: 0.035078	Acc: 52.9% (5288/10000)
[Test]  Epoch: 65	Loss: 0.035078	Acc: 52.7% (5272/10000)
[Test]  Epoch: 66	Loss: 0.035091	Acc: 52.9% (5290/10000)
[Test]  Epoch: 67	Loss: 0.035071	Acc: 52.9% (5290/10000)
[Test]  Epoch: 68	Loss: 0.035110	Acc: 52.8% (5283/10000)
[Test]  Epoch: 69	Loss: 0.035079	Acc: 52.8% (5284/10000)
[Test]  Epoch: 70	Loss: 0.035043	Acc: 53.0% (5305/10000)
[Test]  Epoch: 71	Loss: 0.035130	Acc: 52.6% (5261/10000)
[Test]  Epoch: 72	Loss: 0.035102	Acc: 52.9% (5291/10000)
[Test]  Epoch: 73	Loss: 0.035039	Acc: 52.8% (5283/10000)
[Test]  Epoch: 74	Loss: 0.035018	Acc: 52.9% (5286/10000)
[Test]  Epoch: 75	Loss: 0.035043	Acc: 53.2% (5318/10000)
[Test]  Epoch: 76	Loss: 0.034989	Acc: 53.0% (5296/10000)
[Test]  Epoch: 77	Loss: 0.035020	Acc: 52.9% (5288/10000)
[Test]  Epoch: 78	Loss: 0.035084	Acc: 52.9% (5286/10000)
[Test]  Epoch: 79	Loss: 0.035033	Acc: 52.9% (5293/10000)
[Test]  Epoch: 80	Loss: 0.035099	Acc: 52.8% (5276/10000)
[Test]  Epoch: 81	Loss: 0.035116	Acc: 52.8% (5277/10000)
[Test]  Epoch: 82	Loss: 0.035121	Acc: 52.6% (5265/10000)
[Test]  Epoch: 83	Loss: 0.035076	Acc: 52.9% (5290/10000)
[Test]  Epoch: 84	Loss: 0.035121	Acc: 53.0% (5295/10000)
[Test]  Epoch: 85	Loss: 0.035097	Acc: 52.9% (5290/10000)
[Test]  Epoch: 86	Loss: 0.035142	Acc: 52.6% (5259/10000)
[Test]  Epoch: 87	Loss: 0.035097	Acc: 52.8% (5284/10000)
[Test]  Epoch: 88	Loss: 0.035066	Acc: 52.8% (5284/10000)
[Test]  Epoch: 89	Loss: 0.035096	Acc: 52.9% (5285/10000)
[Test]  Epoch: 90	Loss: 0.035115	Acc: 52.8% (5279/10000)
[Test]  Epoch: 91	Loss: 0.035102	Acc: 52.9% (5289/10000)
[Test]  Epoch: 92	Loss: 0.035003	Acc: 53.0% (5299/10000)
[Test]  Epoch: 93	Loss: 0.035104	Acc: 52.8% (5281/10000)
[Test]  Epoch: 94	Loss: 0.035074	Acc: 53.1% (5308/10000)
[Test]  Epoch: 95	Loss: 0.035079	Acc: 52.6% (5257/10000)
[Test]  Epoch: 96	Loss: 0.035054	Acc: 52.9% (5294/10000)
[Test]  Epoch: 97	Loss: 0.035113	Acc: 52.8% (5279/10000)
[Test]  Epoch: 98	Loss: 0.035103	Acc: 52.8% (5281/10000)
[Test]  Epoch: 99	Loss: 0.035092	Acc: 52.8% (5281/10000)
[Test]  Epoch: 100	Loss: 0.035073	Acc: 53.0% (5303/10000)
===========finish==========
['2024-08-19', '18:30:05.783028', '100', 'test', '0.0350731836438179', '53.03', '53.18']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.041224	Acc: 41.7% (4170/10000)
[Test]  Epoch: 2	Loss: 0.035080	Acc: 51.0% (5101/10000)
[Test]  Epoch: 3	Loss: 0.034914	Acc: 51.5% (5149/10000)
[Test]  Epoch: 4	Loss: 0.034734	Acc: 51.5% (5153/10000)
[Test]  Epoch: 5	Loss: 0.034818	Acc: 51.6% (5161/10000)
[Test]  Epoch: 6	Loss: 0.034852	Acc: 51.8% (5178/10000)
[Test]  Epoch: 7	Loss: 0.034782	Acc: 51.7% (5167/10000)
[Test]  Epoch: 8	Loss: 0.034840	Acc: 51.7% (5169/10000)
[Test]  Epoch: 9	Loss: 0.034824	Acc: 51.9% (5189/10000)
[Test]  Epoch: 10	Loss: 0.034815	Acc: 51.9% (5192/10000)
[Test]  Epoch: 11	Loss: 0.034874	Acc: 51.7% (5173/10000)
[Test]  Epoch: 12	Loss: 0.034808	Acc: 51.7% (5170/10000)
[Test]  Epoch: 13	Loss: 0.034929	Acc: 51.8% (5180/10000)
[Test]  Epoch: 14	Loss: 0.034965	Acc: 51.9% (5190/10000)
[Test]  Epoch: 15	Loss: 0.034999	Acc: 51.8% (5176/10000)
[Test]  Epoch: 16	Loss: 0.034919	Acc: 51.7% (5172/10000)
[Test]  Epoch: 17	Loss: 0.034906	Acc: 51.9% (5186/10000)
[Test]  Epoch: 18	Loss: 0.034931	Acc: 51.9% (5186/10000)
[Test]  Epoch: 19	Loss: 0.034960	Acc: 51.9% (5192/10000)
[Test]  Epoch: 20	Loss: 0.035000	Acc: 51.9% (5188/10000)
[Test]  Epoch: 21	Loss: 0.034955	Acc: 52.1% (5210/10000)
[Test]  Epoch: 22	Loss: 0.035001	Acc: 52.0% (5202/10000)
[Test]  Epoch: 23	Loss: 0.035013	Acc: 52.2% (5216/10000)
[Test]  Epoch: 24	Loss: 0.035125	Acc: 52.1% (5213/10000)
[Test]  Epoch: 25	Loss: 0.035016	Acc: 52.2% (5219/10000)
[Test]  Epoch: 26	Loss: 0.034961	Acc: 52.3% (5228/10000)
[Test]  Epoch: 27	Loss: 0.035049	Acc: 52.0% (5203/10000)
[Test]  Epoch: 28	Loss: 0.035112	Acc: 52.3% (5227/10000)
[Test]  Epoch: 29	Loss: 0.035147	Acc: 52.1% (5215/10000)
[Test]  Epoch: 30	Loss: 0.035102	Acc: 52.1% (5213/10000)
[Test]  Epoch: 31	Loss: 0.035248	Acc: 51.9% (5186/10000)
[Test]  Epoch: 32	Loss: 0.035228	Acc: 51.8% (5183/10000)
[Test]  Epoch: 33	Loss: 0.035185	Acc: 51.9% (5191/10000)
[Test]  Epoch: 34	Loss: 0.035098	Acc: 52.2% (5225/10000)
[Test]  Epoch: 35	Loss: 0.035113	Acc: 52.2% (5221/10000)
[Test]  Epoch: 36	Loss: 0.035206	Acc: 52.1% (5210/10000)
[Test]  Epoch: 37	Loss: 0.035204	Acc: 52.1% (5214/10000)
[Test]  Epoch: 38	Loss: 0.035220	Acc: 52.2% (5222/10000)
[Test]  Epoch: 39	Loss: 0.035267	Acc: 52.1% (5209/10000)
[Test]  Epoch: 40	Loss: 0.035279	Acc: 52.1% (5207/10000)
[Test]  Epoch: 41	Loss: 0.035218	Acc: 52.2% (5219/10000)
[Test]  Epoch: 42	Loss: 0.035274	Acc: 52.1% (5213/10000)
[Test]  Epoch: 43	Loss: 0.035245	Acc: 52.2% (5216/10000)
[Test]  Epoch: 44	Loss: 0.035335	Acc: 52.2% (5216/10000)
[Test]  Epoch: 45	Loss: 0.035270	Acc: 52.1% (5209/10000)
[Test]  Epoch: 46	Loss: 0.035360	Acc: 52.1% (5210/10000)
[Test]  Epoch: 47	Loss: 0.035294	Acc: 52.4% (5242/10000)
[Test]  Epoch: 48	Loss: 0.035296	Acc: 52.4% (5236/10000)
[Test]  Epoch: 49	Loss: 0.035336	Acc: 52.2% (5217/10000)
[Test]  Epoch: 50	Loss: 0.035467	Acc: 52.1% (5209/10000)
[Test]  Epoch: 51	Loss: 0.035386	Acc: 52.2% (5220/10000)
[Test]  Epoch: 52	Loss: 0.035378	Acc: 52.1% (5208/10000)
[Test]  Epoch: 53	Loss: 0.035416	Acc: 52.2% (5222/10000)
[Test]  Epoch: 54	Loss: 0.035460	Acc: 52.1% (5214/10000)
[Test]  Epoch: 55	Loss: 0.035420	Acc: 52.2% (5219/10000)
[Test]  Epoch: 56	Loss: 0.035450	Acc: 52.2% (5219/10000)
[Test]  Epoch: 57	Loss: 0.035465	Acc: 52.1% (5213/10000)
[Test]  Epoch: 58	Loss: 0.035446	Acc: 52.3% (5230/10000)
[Test]  Epoch: 59	Loss: 0.035476	Acc: 52.2% (5222/10000)
[Test]  Epoch: 60	Loss: 0.035540	Acc: 52.2% (5222/10000)
[Test]  Epoch: 61	Loss: 0.035569	Acc: 52.2% (5217/10000)
[Test]  Epoch: 62	Loss: 0.035503	Acc: 52.4% (5243/10000)
[Test]  Epoch: 63	Loss: 0.035466	Acc: 52.1% (5214/10000)
[Test]  Epoch: 64	Loss: 0.035504	Acc: 52.4% (5235/10000)
[Test]  Epoch: 65	Loss: 0.035519	Acc: 52.2% (5223/10000)
[Test]  Epoch: 66	Loss: 0.035519	Acc: 52.3% (5229/10000)
[Test]  Epoch: 67	Loss: 0.035504	Acc: 52.4% (5239/10000)
[Test]  Epoch: 68	Loss: 0.035554	Acc: 52.3% (5233/10000)
[Test]  Epoch: 69	Loss: 0.035527	Acc: 52.5% (5247/10000)
[Test]  Epoch: 70	Loss: 0.035476	Acc: 52.3% (5232/10000)
[Test]  Epoch: 71	Loss: 0.035556	Acc: 52.1% (5206/10000)
[Test]  Epoch: 72	Loss: 0.035549	Acc: 52.2% (5222/10000)
[Test]  Epoch: 73	Loss: 0.035463	Acc: 52.3% (5226/10000)
[Test]  Epoch: 74	Loss: 0.035442	Acc: 52.3% (5230/10000)
[Test]  Epoch: 75	Loss: 0.035475	Acc: 52.5% (5249/10000)
[Test]  Epoch: 76	Loss: 0.035414	Acc: 52.4% (5237/10000)
[Test]  Epoch: 77	Loss: 0.035437	Acc: 52.4% (5237/10000)
[Test]  Epoch: 78	Loss: 0.035526	Acc: 52.3% (5230/10000)
[Test]  Epoch: 79	Loss: 0.035474	Acc: 52.4% (5239/10000)
[Test]  Epoch: 80	Loss: 0.035535	Acc: 52.3% (5226/10000)
[Test]  Epoch: 81	Loss: 0.035543	Acc: 52.2% (5218/10000)
[Test]  Epoch: 82	Loss: 0.035552	Acc: 52.3% (5226/10000)
[Test]  Epoch: 83	Loss: 0.035524	Acc: 52.3% (5230/10000)
[Test]  Epoch: 84	Loss: 0.035539	Acc: 52.2% (5220/10000)
[Test]  Epoch: 85	Loss: 0.035514	Acc: 52.3% (5232/10000)
[Test]  Epoch: 86	Loss: 0.035572	Acc: 52.2% (5224/10000)
[Test]  Epoch: 87	Loss: 0.035530	Acc: 52.3% (5229/10000)
[Test]  Epoch: 88	Loss: 0.035488	Acc: 52.4% (5242/10000)
[Test]  Epoch: 89	Loss: 0.035525	Acc: 52.3% (5229/10000)
[Test]  Epoch: 90	Loss: 0.035544	Acc: 52.2% (5218/10000)
[Test]  Epoch: 91	Loss: 0.035527	Acc: 52.4% (5244/10000)
[Test]  Epoch: 92	Loss: 0.035431	Acc: 52.5% (5245/10000)
[Test]  Epoch: 93	Loss: 0.035530	Acc: 52.4% (5236/10000)
[Test]  Epoch: 94	Loss: 0.035510	Acc: 52.5% (5249/10000)
[Test]  Epoch: 95	Loss: 0.035494	Acc: 52.2% (5218/10000)
[Test]  Epoch: 96	Loss: 0.035473	Acc: 52.5% (5245/10000)
[Test]  Epoch: 97	Loss: 0.035539	Acc: 52.2% (5217/10000)
[Test]  Epoch: 98	Loss: 0.035536	Acc: 52.2% (5224/10000)
[Test]  Epoch: 99	Loss: 0.035513	Acc: 52.2% (5216/10000)
[Test]  Epoch: 100	Loss: 0.035496	Acc: 52.5% (5245/10000)
===========finish==========
['2024-08-19', '18:37:18.834253', '100', 'test', '0.03549635929465294', '52.45', '52.49']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.043773	Acc: 38.2% (3824/10000)
[Test]  Epoch: 2	Loss: 0.036642	Acc: 48.9% (4888/10000)
[Test]  Epoch: 3	Loss: 0.036370	Acc: 49.8% (4978/10000)
[Test]  Epoch: 4	Loss: 0.036156	Acc: 49.8% (4980/10000)
[Test]  Epoch: 5	Loss: 0.036233	Acc: 49.9% (4991/10000)
[Test]  Epoch: 6	Loss: 0.036274	Acc: 50.1% (5015/10000)
[Test]  Epoch: 7	Loss: 0.036214	Acc: 50.2% (5025/10000)
[Test]  Epoch: 8	Loss: 0.036267	Acc: 50.1% (5015/10000)
[Test]  Epoch: 9	Loss: 0.036305	Acc: 50.1% (5015/10000)
[Test]  Epoch: 10	Loss: 0.036283	Acc: 50.2% (5022/10000)
[Test]  Epoch: 11	Loss: 0.036333	Acc: 50.3% (5030/10000)
[Test]  Epoch: 12	Loss: 0.036248	Acc: 50.3% (5026/10000)
[Test]  Epoch: 13	Loss: 0.036314	Acc: 50.4% (5042/10000)
[Test]  Epoch: 14	Loss: 0.036383	Acc: 50.5% (5049/10000)
[Test]  Epoch: 15	Loss: 0.036460	Acc: 50.3% (5027/10000)
[Test]  Epoch: 16	Loss: 0.036344	Acc: 50.5% (5045/10000)
[Test]  Epoch: 17	Loss: 0.036331	Acc: 50.4% (5042/10000)
[Test]  Epoch: 18	Loss: 0.036325	Acc: 50.4% (5044/10000)
[Test]  Epoch: 19	Loss: 0.036374	Acc: 50.4% (5040/10000)
[Test]  Epoch: 20	Loss: 0.036415	Acc: 50.3% (5034/10000)
[Test]  Epoch: 21	Loss: 0.036425	Acc: 50.4% (5036/10000)
[Test]  Epoch: 22	Loss: 0.036475	Acc: 50.6% (5059/10000)
[Test]  Epoch: 23	Loss: 0.036421	Acc: 50.5% (5049/10000)
[Test]  Epoch: 24	Loss: 0.036516	Acc: 50.3% (5032/10000)
[Test]  Epoch: 25	Loss: 0.036436	Acc: 50.6% (5063/10000)
[Test]  Epoch: 26	Loss: 0.036402	Acc: 50.8% (5080/10000)
[Test]  Epoch: 27	Loss: 0.036492	Acc: 50.5% (5051/10000)
[Test]  Epoch: 28	Loss: 0.036536	Acc: 50.6% (5059/10000)
[Test]  Epoch: 29	Loss: 0.036573	Acc: 50.5% (5046/10000)
[Test]  Epoch: 30	Loss: 0.036519	Acc: 50.6% (5062/10000)
[Test]  Epoch: 31	Loss: 0.036678	Acc: 50.5% (5050/10000)
[Test]  Epoch: 32	Loss: 0.036676	Acc: 50.5% (5048/10000)
[Test]  Epoch: 33	Loss: 0.036632	Acc: 50.5% (5047/10000)
[Test]  Epoch: 34	Loss: 0.036529	Acc: 50.7% (5072/10000)
[Test]  Epoch: 35	Loss: 0.036518	Acc: 50.7% (5068/10000)
[Test]  Epoch: 36	Loss: 0.036641	Acc: 50.6% (5064/10000)
[Test]  Epoch: 37	Loss: 0.036670	Acc: 50.7% (5071/10000)
[Test]  Epoch: 38	Loss: 0.036671	Acc: 50.8% (5082/10000)
[Test]  Epoch: 39	Loss: 0.036739	Acc: 50.6% (5058/10000)
[Test]  Epoch: 40	Loss: 0.036737	Acc: 50.6% (5061/10000)
[Test]  Epoch: 41	Loss: 0.036651	Acc: 50.8% (5079/10000)
[Test]  Epoch: 42	Loss: 0.036754	Acc: 50.6% (5062/10000)
[Test]  Epoch: 43	Loss: 0.036707	Acc: 50.6% (5061/10000)
[Test]  Epoch: 44	Loss: 0.036810	Acc: 50.6% (5063/10000)
[Test]  Epoch: 45	Loss: 0.036726	Acc: 50.8% (5078/10000)
[Test]  Epoch: 46	Loss: 0.036824	Acc: 50.6% (5057/10000)
[Test]  Epoch: 47	Loss: 0.036740	Acc: 50.9% (5091/10000)
[Test]  Epoch: 48	Loss: 0.036739	Acc: 50.9% (5088/10000)
[Test]  Epoch: 49	Loss: 0.036786	Acc: 50.6% (5061/10000)
[Test]  Epoch: 50	Loss: 0.036887	Acc: 50.6% (5060/10000)
[Test]  Epoch: 51	Loss: 0.036806	Acc: 50.7% (5072/10000)
[Test]  Epoch: 52	Loss: 0.036797	Acc: 50.7% (5071/10000)
[Test]  Epoch: 53	Loss: 0.036859	Acc: 50.6% (5062/10000)
[Test]  Epoch: 54	Loss: 0.036948	Acc: 50.6% (5058/10000)
[Test]  Epoch: 55	Loss: 0.036906	Acc: 50.8% (5078/10000)
[Test]  Epoch: 56	Loss: 0.036902	Acc: 50.8% (5077/10000)
[Test]  Epoch: 57	Loss: 0.036921	Acc: 50.8% (5077/10000)
[Test]  Epoch: 58	Loss: 0.036872	Acc: 50.9% (5086/10000)
[Test]  Epoch: 59	Loss: 0.036932	Acc: 50.7% (5073/10000)
[Test]  Epoch: 60	Loss: 0.036981	Acc: 50.9% (5092/10000)
[Test]  Epoch: 61	Loss: 0.037011	Acc: 50.7% (5074/10000)
[Test]  Epoch: 62	Loss: 0.036953	Acc: 50.8% (5080/10000)
[Test]  Epoch: 63	Loss: 0.036926	Acc: 50.8% (5078/10000)
[Test]  Epoch: 64	Loss: 0.036959	Acc: 50.8% (5081/10000)
[Test]  Epoch: 65	Loss: 0.036988	Acc: 51.0% (5100/10000)
[Test]  Epoch: 66	Loss: 0.036978	Acc: 50.8% (5078/10000)
[Test]  Epoch: 67	Loss: 0.036957	Acc: 50.8% (5082/10000)
[Test]  Epoch: 68	Loss: 0.036999	Acc: 50.7% (5067/10000)
[Test]  Epoch: 69	Loss: 0.036966	Acc: 50.8% (5082/10000)
[Test]  Epoch: 70	Loss: 0.036931	Acc: 50.8% (5080/10000)
[Test]  Epoch: 71	Loss: 0.037018	Acc: 50.7% (5073/10000)
[Test]  Epoch: 72	Loss: 0.037008	Acc: 50.8% (5080/10000)
[Test]  Epoch: 73	Loss: 0.036922	Acc: 50.8% (5080/10000)
[Test]  Epoch: 74	Loss: 0.036889	Acc: 50.9% (5085/10000)
[Test]  Epoch: 75	Loss: 0.036927	Acc: 50.8% (5083/10000)
[Test]  Epoch: 76	Loss: 0.036873	Acc: 50.8% (5080/10000)
[Test]  Epoch: 77	Loss: 0.036896	Acc: 50.9% (5092/10000)
[Test]  Epoch: 78	Loss: 0.036983	Acc: 51.0% (5095/10000)
[Test]  Epoch: 79	Loss: 0.036934	Acc: 50.9% (5087/10000)
[Test]  Epoch: 80	Loss: 0.036985	Acc: 50.8% (5081/10000)
[Test]  Epoch: 81	Loss: 0.037000	Acc: 50.9% (5091/10000)
[Test]  Epoch: 82	Loss: 0.036993	Acc: 50.6% (5061/10000)
[Test]  Epoch: 83	Loss: 0.036978	Acc: 50.8% (5075/10000)
[Test]  Epoch: 84	Loss: 0.036997	Acc: 50.7% (5066/10000)
[Test]  Epoch: 85	Loss: 0.036984	Acc: 50.8% (5081/10000)
[Test]  Epoch: 86	Loss: 0.037038	Acc: 50.7% (5069/10000)
[Test]  Epoch: 87	Loss: 0.036972	Acc: 50.8% (5081/10000)
[Test]  Epoch: 88	Loss: 0.036937	Acc: 50.8% (5082/10000)
[Test]  Epoch: 89	Loss: 0.036988	Acc: 50.9% (5087/10000)
[Test]  Epoch: 90	Loss: 0.037000	Acc: 50.9% (5088/10000)
[Test]  Epoch: 91	Loss: 0.036972	Acc: 50.9% (5094/10000)
[Test]  Epoch: 92	Loss: 0.036884	Acc: 50.9% (5085/10000)
[Test]  Epoch: 93	Loss: 0.036984	Acc: 50.8% (5081/10000)
[Test]  Epoch: 94	Loss: 0.036977	Acc: 50.9% (5088/10000)
[Test]  Epoch: 95	Loss: 0.036967	Acc: 50.8% (5083/10000)
[Test]  Epoch: 96	Loss: 0.036931	Acc: 51.0% (5098/10000)
[Test]  Epoch: 97	Loss: 0.037010	Acc: 50.8% (5076/10000)
[Test]  Epoch: 98	Loss: 0.037013	Acc: 50.7% (5071/10000)
[Test]  Epoch: 99	Loss: 0.036992	Acc: 50.8% (5077/10000)
[Test]  Epoch: 100	Loss: 0.036967	Acc: 50.8% (5083/10000)
===========finish==========
['2024-08-19', '18:44:19.142131', '100', 'test', '0.036966843962669374', '50.83', '51.0']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.047927	Acc: 32.3% (3230/10000)
[Test]  Epoch: 2	Loss: 0.037352	Acc: 48.0% (4803/10000)
[Test]  Epoch: 3	Loss: 0.036895	Acc: 49.0% (4904/10000)
[Test]  Epoch: 4	Loss: 0.036612	Acc: 49.5% (4953/10000)
[Test]  Epoch: 5	Loss: 0.036809	Acc: 49.4% (4937/10000)
[Test]  Epoch: 6	Loss: 0.036810	Acc: 49.4% (4936/10000)
[Test]  Epoch: 7	Loss: 0.036779	Acc: 49.4% (4944/10000)
[Test]  Epoch: 8	Loss: 0.036819	Acc: 49.4% (4942/10000)
[Test]  Epoch: 9	Loss: 0.036833	Acc: 49.4% (4942/10000)
[Test]  Epoch: 10	Loss: 0.036738	Acc: 49.7% (4969/10000)
[Test]  Epoch: 11	Loss: 0.036821	Acc: 49.7% (4971/10000)
[Test]  Epoch: 12	Loss: 0.036731	Acc: 49.7% (4972/10000)
[Test]  Epoch: 13	Loss: 0.036825	Acc: 49.8% (4984/10000)
[Test]  Epoch: 14	Loss: 0.036859	Acc: 49.7% (4969/10000)
[Test]  Epoch: 15	Loss: 0.036913	Acc: 49.8% (4980/10000)
[Test]  Epoch: 16	Loss: 0.036864	Acc: 49.7% (4970/10000)
[Test]  Epoch: 17	Loss: 0.036864	Acc: 49.9% (4987/10000)
[Test]  Epoch: 18	Loss: 0.036843	Acc: 49.8% (4982/10000)
[Test]  Epoch: 19	Loss: 0.036859	Acc: 49.9% (4993/10000)
[Test]  Epoch: 20	Loss: 0.036900	Acc: 49.8% (4983/10000)
[Test]  Epoch: 21	Loss: 0.036920	Acc: 49.9% (4992/10000)
[Test]  Epoch: 22	Loss: 0.036955	Acc: 50.0% (5003/10000)
[Test]  Epoch: 23	Loss: 0.036925	Acc: 50.0% (5005/10000)
[Test]  Epoch: 24	Loss: 0.036976	Acc: 49.8% (4984/10000)
[Test]  Epoch: 25	Loss: 0.036920	Acc: 50.0% (4995/10000)
[Test]  Epoch: 26	Loss: 0.036859	Acc: 50.2% (5020/10000)
[Test]  Epoch: 27	Loss: 0.036980	Acc: 50.1% (5009/10000)
[Test]  Epoch: 28	Loss: 0.036979	Acc: 50.1% (5010/10000)
[Test]  Epoch: 29	Loss: 0.037045	Acc: 50.1% (5007/10000)
[Test]  Epoch: 30	Loss: 0.036966	Acc: 50.2% (5025/10000)
[Test]  Epoch: 31	Loss: 0.037109	Acc: 49.9% (4992/10000)
[Test]  Epoch: 32	Loss: 0.037113	Acc: 50.0% (5003/10000)
[Test]  Epoch: 33	Loss: 0.037084	Acc: 49.9% (4991/10000)
[Test]  Epoch: 34	Loss: 0.037009	Acc: 50.0% (5004/10000)
[Test]  Epoch: 35	Loss: 0.037001	Acc: 50.3% (5026/10000)
[Test]  Epoch: 36	Loss: 0.037112	Acc: 49.8% (4984/10000)
[Test]  Epoch: 37	Loss: 0.037163	Acc: 49.9% (4990/10000)
[Test]  Epoch: 38	Loss: 0.037117	Acc: 50.1% (5014/10000)
[Test]  Epoch: 39	Loss: 0.037166	Acc: 50.0% (4998/10000)
[Test]  Epoch: 40	Loss: 0.037209	Acc: 50.0% (5005/10000)
[Test]  Epoch: 41	Loss: 0.037131	Acc: 50.3% (5028/10000)
[Test]  Epoch: 42	Loss: 0.037193	Acc: 50.0% (5005/10000)
[Test]  Epoch: 43	Loss: 0.037189	Acc: 50.1% (5006/10000)
[Test]  Epoch: 44	Loss: 0.037295	Acc: 50.0% (5000/10000)
[Test]  Epoch: 45	Loss: 0.037234	Acc: 50.0% (5001/10000)
[Test]  Epoch: 46	Loss: 0.037344	Acc: 50.0% (4995/10000)
[Test]  Epoch: 47	Loss: 0.037230	Acc: 50.2% (5017/10000)
[Test]  Epoch: 48	Loss: 0.037234	Acc: 50.1% (5006/10000)
[Test]  Epoch: 49	Loss: 0.037200	Acc: 50.2% (5020/10000)
[Test]  Epoch: 50	Loss: 0.037342	Acc: 50.1% (5007/10000)
[Test]  Epoch: 51	Loss: 0.037321	Acc: 50.1% (5011/10000)
[Test]  Epoch: 52	Loss: 0.037334	Acc: 50.2% (5024/10000)
[Test]  Epoch: 53	Loss: 0.037379	Acc: 50.0% (5000/10000)
[Test]  Epoch: 54	Loss: 0.037393	Acc: 50.1% (5013/10000)
[Test]  Epoch: 55	Loss: 0.037376	Acc: 50.0% (5004/10000)
[Test]  Epoch: 56	Loss: 0.037388	Acc: 50.1% (5013/10000)
[Test]  Epoch: 57	Loss: 0.037388	Acc: 50.1% (5008/10000)
[Test]  Epoch: 58	Loss: 0.037321	Acc: 50.1% (5012/10000)
[Test]  Epoch: 59	Loss: 0.037393	Acc: 50.2% (5016/10000)
[Test]  Epoch: 60	Loss: 0.037447	Acc: 50.1% (5014/10000)
[Test]  Epoch: 61	Loss: 0.037486	Acc: 50.2% (5016/10000)
[Test]  Epoch: 62	Loss: 0.037424	Acc: 50.0% (4999/10000)
[Test]  Epoch: 63	Loss: 0.037395	Acc: 50.2% (5021/10000)
[Test]  Epoch: 64	Loss: 0.037425	Acc: 50.1% (5009/10000)
[Test]  Epoch: 65	Loss: 0.037436	Acc: 50.1% (5011/10000)
[Test]  Epoch: 66	Loss: 0.037441	Acc: 50.1% (5014/10000)
[Test]  Epoch: 67	Loss: 0.037461	Acc: 50.0% (4998/10000)
[Test]  Epoch: 68	Loss: 0.037501	Acc: 50.1% (5007/10000)
[Test]  Epoch: 69	Loss: 0.037421	Acc: 50.2% (5022/10000)
[Test]  Epoch: 70	Loss: 0.037415	Acc: 50.2% (5020/10000)
[Test]  Epoch: 71	Loss: 0.037489	Acc: 50.0% (5004/10000)
[Test]  Epoch: 72	Loss: 0.037520	Acc: 50.1% (5015/10000)
[Test]  Epoch: 73	Loss: 0.037411	Acc: 50.1% (5010/10000)
[Test]  Epoch: 74	Loss: 0.037382	Acc: 50.1% (5007/10000)
[Test]  Epoch: 75	Loss: 0.037393	Acc: 50.2% (5019/10000)
[Test]  Epoch: 76	Loss: 0.037368	Acc: 50.2% (5019/10000)
[Test]  Epoch: 77	Loss: 0.037398	Acc: 50.1% (5010/10000)
[Test]  Epoch: 78	Loss: 0.037473	Acc: 49.9% (4990/10000)
[Test]  Epoch: 79	Loss: 0.037423	Acc: 50.2% (5017/10000)
[Test]  Epoch: 80	Loss: 0.037459	Acc: 49.9% (4988/10000)
[Test]  Epoch: 81	Loss: 0.037466	Acc: 50.0% (5002/10000)
[Test]  Epoch: 82	Loss: 0.037479	Acc: 49.8% (4979/10000)
[Test]  Epoch: 83	Loss: 0.037459	Acc: 50.0% (4995/10000)
[Test]  Epoch: 84	Loss: 0.037495	Acc: 50.1% (5006/10000)
[Test]  Epoch: 85	Loss: 0.037462	Acc: 50.0% (4999/10000)
[Test]  Epoch: 86	Loss: 0.037514	Acc: 49.9% (4991/10000)
[Test]  Epoch: 87	Loss: 0.037454	Acc: 50.0% (4997/10000)
[Test]  Epoch: 88	Loss: 0.037428	Acc: 50.0% (5001/10000)
[Test]  Epoch: 89	Loss: 0.037476	Acc: 50.0% (5003/10000)
[Test]  Epoch: 90	Loss: 0.037472	Acc: 50.1% (5013/10000)
[Test]  Epoch: 91	Loss: 0.037446	Acc: 50.0% (5000/10000)
[Test]  Epoch: 92	Loss: 0.037400	Acc: 50.1% (5012/10000)
[Test]  Epoch: 93	Loss: 0.037493	Acc: 50.1% (5010/10000)
[Test]  Epoch: 94	Loss: 0.037475	Acc: 50.1% (5015/10000)
[Test]  Epoch: 95	Loss: 0.037467	Acc: 50.0% (5005/10000)
[Test]  Epoch: 96	Loss: 0.037425	Acc: 50.3% (5028/10000)
[Test]  Epoch: 97	Loss: 0.037480	Acc: 50.0% (5002/10000)
[Test]  Epoch: 98	Loss: 0.037508	Acc: 50.0% (4996/10000)
[Test]  Epoch: 99	Loss: 0.037471	Acc: 50.1% (5012/10000)
[Test]  Epoch: 100	Loss: 0.037451	Acc: 50.0% (4999/10000)
===========finish==========
['2024-08-19', '18:51:26.575294', '100', 'test', '0.03745125907659531', '49.99', '50.28']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.048883	Acc: 30.8% (3084/10000)
[Test]  Epoch: 2	Loss: 0.038188	Acc: 47.1% (4707/10000)
[Test]  Epoch: 3	Loss: 0.037717	Acc: 48.0% (4800/10000)
[Test]  Epoch: 4	Loss: 0.037407	Acc: 48.5% (4847/10000)
[Test]  Epoch: 5	Loss: 0.037564	Acc: 48.5% (4848/10000)
[Test]  Epoch: 6	Loss: 0.037545	Acc: 48.6% (4860/10000)
[Test]  Epoch: 7	Loss: 0.037517	Acc: 48.5% (4851/10000)
[Test]  Epoch: 8	Loss: 0.037598	Acc: 48.5% (4847/10000)
[Test]  Epoch: 9	Loss: 0.037608	Acc: 48.7% (4867/10000)
[Test]  Epoch: 10	Loss: 0.037512	Acc: 48.8% (4875/10000)
[Test]  Epoch: 11	Loss: 0.037593	Acc: 48.9% (4887/10000)
[Test]  Epoch: 12	Loss: 0.037449	Acc: 49.0% (4898/10000)
[Test]  Epoch: 13	Loss: 0.037570	Acc: 49.1% (4911/10000)
[Test]  Epoch: 14	Loss: 0.037616	Acc: 49.1% (4906/10000)
[Test]  Epoch: 15	Loss: 0.037687	Acc: 48.8% (4884/10000)
[Test]  Epoch: 16	Loss: 0.037590	Acc: 49.2% (4923/10000)
[Test]  Epoch: 17	Loss: 0.037634	Acc: 49.2% (4920/10000)
[Test]  Epoch: 18	Loss: 0.037598	Acc: 49.3% (4930/10000)
[Test]  Epoch: 19	Loss: 0.037592	Acc: 49.1% (4915/10000)
[Test]  Epoch: 20	Loss: 0.037611	Acc: 49.2% (4920/10000)
[Test]  Epoch: 21	Loss: 0.037612	Acc: 49.4% (4943/10000)
[Test]  Epoch: 22	Loss: 0.037656	Acc: 49.4% (4938/10000)
[Test]  Epoch: 23	Loss: 0.037638	Acc: 49.3% (4928/10000)
[Test]  Epoch: 24	Loss: 0.037709	Acc: 49.3% (4933/10000)
[Test]  Epoch: 25	Loss: 0.037660	Acc: 49.4% (4941/10000)
[Test]  Epoch: 26	Loss: 0.037558	Acc: 49.7% (4967/10000)
[Test]  Epoch: 27	Loss: 0.037723	Acc: 49.3% (4929/10000)
[Test]  Epoch: 28	Loss: 0.037752	Acc: 49.2% (4917/10000)
[Test]  Epoch: 29	Loss: 0.037775	Acc: 49.3% (4934/10000)
[Test]  Epoch: 30	Loss: 0.037705	Acc: 49.5% (4946/10000)
[Test]  Epoch: 31	Loss: 0.037872	Acc: 49.2% (4919/10000)
[Test]  Epoch: 32	Loss: 0.037857	Acc: 49.1% (4912/10000)
[Test]  Epoch: 33	Loss: 0.037821	Acc: 49.2% (4916/10000)
[Test]  Epoch: 34	Loss: 0.037744	Acc: 49.4% (4942/10000)
[Test]  Epoch: 35	Loss: 0.037739	Acc: 49.6% (4958/10000)
[Test]  Epoch: 36	Loss: 0.037862	Acc: 49.4% (4936/10000)
[Test]  Epoch: 37	Loss: 0.037916	Acc: 49.4% (4937/10000)
[Test]  Epoch: 38	Loss: 0.037837	Acc: 49.6% (4960/10000)
[Test]  Epoch: 39	Loss: 0.037895	Acc: 49.4% (4943/10000)
[Test]  Epoch: 40	Loss: 0.037921	Acc: 49.3% (4929/10000)
[Test]  Epoch: 41	Loss: 0.037878	Acc: 49.4% (4943/10000)
[Test]  Epoch: 42	Loss: 0.037950	Acc: 49.6% (4956/10000)
[Test]  Epoch: 43	Loss: 0.037911	Acc: 49.5% (4948/10000)
[Test]  Epoch: 44	Loss: 0.038050	Acc: 49.5% (4945/10000)
[Test]  Epoch: 45	Loss: 0.038007	Acc: 49.6% (4958/10000)
[Test]  Epoch: 46	Loss: 0.038096	Acc: 49.2% (4925/10000)
[Test]  Epoch: 47	Loss: 0.037944	Acc: 49.6% (4962/10000)
[Test]  Epoch: 48	Loss: 0.037964	Acc: 49.6% (4960/10000)
[Test]  Epoch: 49	Loss: 0.037951	Acc: 49.5% (4946/10000)
[Test]  Epoch: 50	Loss: 0.038046	Acc: 49.6% (4961/10000)
[Test]  Epoch: 51	Loss: 0.038019	Acc: 49.7% (4973/10000)
[Test]  Epoch: 52	Loss: 0.038059	Acc: 49.6% (4962/10000)
[Test]  Epoch: 53	Loss: 0.038126	Acc: 49.6% (4960/10000)
[Test]  Epoch: 54	Loss: 0.038164	Acc: 49.5% (4954/10000)
[Test]  Epoch: 55	Loss: 0.038099	Acc: 49.6% (4960/10000)
[Test]  Epoch: 56	Loss: 0.038112	Acc: 49.5% (4947/10000)
[Test]  Epoch: 57	Loss: 0.038089	Acc: 49.6% (4962/10000)
[Test]  Epoch: 58	Loss: 0.038014	Acc: 49.7% (4969/10000)
[Test]  Epoch: 59	Loss: 0.038134	Acc: 49.4% (4941/10000)
[Test]  Epoch: 60	Loss: 0.038182	Acc: 49.6% (4961/10000)
[Test]  Epoch: 61	Loss: 0.038229	Acc: 49.7% (4974/10000)
[Test]  Epoch: 62	Loss: 0.038136	Acc: 49.7% (4971/10000)
[Test]  Epoch: 63	Loss: 0.038139	Acc: 49.6% (4962/10000)
[Test]  Epoch: 64	Loss: 0.038152	Acc: 49.7% (4971/10000)
[Test]  Epoch: 65	Loss: 0.038160	Acc: 49.8% (4980/10000)
[Test]  Epoch: 66	Loss: 0.038150	Acc: 49.7% (4974/10000)
[Test]  Epoch: 67	Loss: 0.038189	Acc: 49.7% (4969/10000)
[Test]  Epoch: 68	Loss: 0.038239	Acc: 49.7% (4968/10000)
[Test]  Epoch: 69	Loss: 0.038136	Acc: 49.8% (4975/10000)
[Test]  Epoch: 70	Loss: 0.038128	Acc: 49.7% (4973/10000)
[Test]  Epoch: 71	Loss: 0.038223	Acc: 49.6% (4956/10000)
[Test]  Epoch: 72	Loss: 0.038252	Acc: 49.5% (4952/10000)
[Test]  Epoch: 73	Loss: 0.038115	Acc: 49.7% (4971/10000)
[Test]  Epoch: 74	Loss: 0.038071	Acc: 49.9% (4991/10000)
[Test]  Epoch: 75	Loss: 0.038111	Acc: 49.7% (4970/10000)
[Test]  Epoch: 76	Loss: 0.038098	Acc: 49.5% (4953/10000)
[Test]  Epoch: 77	Loss: 0.038127	Acc: 49.8% (4979/10000)
[Test]  Epoch: 78	Loss: 0.038206	Acc: 49.6% (4960/10000)
[Test]  Epoch: 79	Loss: 0.038125	Acc: 49.7% (4973/10000)
[Test]  Epoch: 80	Loss: 0.038175	Acc: 49.6% (4958/10000)
[Test]  Epoch: 81	Loss: 0.038177	Acc: 49.6% (4956/10000)
[Test]  Epoch: 82	Loss: 0.038194	Acc: 49.5% (4947/10000)
[Test]  Epoch: 83	Loss: 0.038173	Acc: 49.6% (4960/10000)
[Test]  Epoch: 84	Loss: 0.038200	Acc: 49.8% (4978/10000)
[Test]  Epoch: 85	Loss: 0.038184	Acc: 49.7% (4970/10000)
[Test]  Epoch: 86	Loss: 0.038236	Acc: 49.5% (4954/10000)
[Test]  Epoch: 87	Loss: 0.038194	Acc: 49.8% (4976/10000)
[Test]  Epoch: 88	Loss: 0.038184	Acc: 49.7% (4972/10000)
[Test]  Epoch: 89	Loss: 0.038212	Acc: 49.5% (4946/10000)
[Test]  Epoch: 90	Loss: 0.038197	Acc: 49.5% (4952/10000)
[Test]  Epoch: 91	Loss: 0.038161	Acc: 49.7% (4972/10000)
[Test]  Epoch: 92	Loss: 0.038118	Acc: 49.7% (4967/10000)
[Test]  Epoch: 93	Loss: 0.038206	Acc: 49.5% (4949/10000)
[Test]  Epoch: 94	Loss: 0.038181	Acc: 49.7% (4973/10000)
[Test]  Epoch: 95	Loss: 0.038169	Acc: 49.7% (4968/10000)
[Test]  Epoch: 96	Loss: 0.038135	Acc: 49.6% (4957/10000)
[Test]  Epoch: 97	Loss: 0.038216	Acc: 49.5% (4948/10000)
[Test]  Epoch: 98	Loss: 0.038254	Acc: 49.5% (4953/10000)
[Test]  Epoch: 99	Loss: 0.038189	Acc: 49.7% (4967/10000)
[Test]  Epoch: 100	Loss: 0.038170	Acc: 49.6% (4962/10000)
===========finish==========
['2024-08-19', '18:58:32.511970', '100', 'test', '0.038169801771640775', '49.62', '49.91']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.048763	Acc: 31.4% (3137/10000)
[Test]  Epoch: 2	Loss: 0.039227	Acc: 45.6% (4563/10000)
[Test]  Epoch: 3	Loss: 0.038782	Acc: 46.3% (4632/10000)
[Test]  Epoch: 4	Loss: 0.038508	Acc: 46.7% (4674/10000)
[Test]  Epoch: 5	Loss: 0.038591	Acc: 46.8% (4683/10000)
[Test]  Epoch: 6	Loss: 0.038540	Acc: 47.0% (4704/10000)
[Test]  Epoch: 7	Loss: 0.038523	Acc: 47.1% (4709/10000)
[Test]  Epoch: 8	Loss: 0.038616	Acc: 47.0% (4695/10000)
[Test]  Epoch: 9	Loss: 0.038571	Acc: 47.2% (4723/10000)
[Test]  Epoch: 10	Loss: 0.038493	Acc: 47.2% (4721/10000)
[Test]  Epoch: 11	Loss: 0.038597	Acc: 47.3% (4727/10000)
[Test]  Epoch: 12	Loss: 0.038443	Acc: 47.6% (4756/10000)
[Test]  Epoch: 13	Loss: 0.038533	Acc: 47.3% (4728/10000)
[Test]  Epoch: 14	Loss: 0.038612	Acc: 47.3% (4728/10000)
[Test]  Epoch: 15	Loss: 0.038613	Acc: 47.5% (4746/10000)
[Test]  Epoch: 16	Loss: 0.038556	Acc: 47.5% (4748/10000)
[Test]  Epoch: 17	Loss: 0.038571	Acc: 47.6% (4761/10000)
[Test]  Epoch: 18	Loss: 0.038589	Acc: 47.7% (4774/10000)
[Test]  Epoch: 19	Loss: 0.038543	Acc: 47.6% (4763/10000)
[Test]  Epoch: 20	Loss: 0.038565	Acc: 47.6% (4764/10000)
[Test]  Epoch: 21	Loss: 0.038556	Acc: 47.8% (4777/10000)
[Test]  Epoch: 22	Loss: 0.038567	Acc: 47.9% (4786/10000)
[Test]  Epoch: 23	Loss: 0.038529	Acc: 47.7% (4769/10000)
[Test]  Epoch: 24	Loss: 0.038634	Acc: 47.8% (4775/10000)
[Test]  Epoch: 25	Loss: 0.038637	Acc: 48.0% (4805/10000)
[Test]  Epoch: 26	Loss: 0.038512	Acc: 48.2% (4823/10000)
[Test]  Epoch: 27	Loss: 0.038658	Acc: 48.0% (4798/10000)
[Test]  Epoch: 28	Loss: 0.038658	Acc: 48.1% (4808/10000)
[Test]  Epoch: 29	Loss: 0.038703	Acc: 48.0% (4800/10000)
[Test]  Epoch: 30	Loss: 0.038648	Acc: 48.1% (4808/10000)
[Test]  Epoch: 31	Loss: 0.038793	Acc: 47.8% (4779/10000)
[Test]  Epoch: 32	Loss: 0.038782	Acc: 48.1% (4810/10000)
[Test]  Epoch: 33	Loss: 0.038751	Acc: 48.0% (4805/10000)
[Test]  Epoch: 34	Loss: 0.038671	Acc: 48.0% (4803/10000)
[Test]  Epoch: 35	Loss: 0.038693	Acc: 48.1% (4812/10000)
[Test]  Epoch: 36	Loss: 0.038776	Acc: 48.2% (4820/10000)
[Test]  Epoch: 37	Loss: 0.038824	Acc: 48.0% (4801/10000)
[Test]  Epoch: 38	Loss: 0.038770	Acc: 48.2% (4821/10000)
[Test]  Epoch: 39	Loss: 0.038816	Acc: 48.1% (4808/10000)
[Test]  Epoch: 40	Loss: 0.038821	Acc: 48.1% (4815/10000)
[Test]  Epoch: 41	Loss: 0.038770	Acc: 48.1% (4810/10000)
[Test]  Epoch: 42	Loss: 0.038864	Acc: 48.1% (4809/10000)
[Test]  Epoch: 43	Loss: 0.038834	Acc: 48.2% (4824/10000)
[Test]  Epoch: 44	Loss: 0.038934	Acc: 47.9% (4792/10000)
[Test]  Epoch: 45	Loss: 0.038890	Acc: 48.0% (4804/10000)
[Test]  Epoch: 46	Loss: 0.039015	Acc: 47.8% (4783/10000)
[Test]  Epoch: 47	Loss: 0.038864	Acc: 48.1% (4813/10000)
[Test]  Epoch: 48	Loss: 0.038874	Acc: 48.2% (4822/10000)
[Test]  Epoch: 49	Loss: 0.038862	Acc: 48.2% (4824/10000)
[Test]  Epoch: 50	Loss: 0.038918	Acc: 48.1% (4810/10000)
[Test]  Epoch: 51	Loss: 0.038955	Acc: 48.2% (4825/10000)
[Test]  Epoch: 52	Loss: 0.038958	Acc: 48.2% (4824/10000)
[Test]  Epoch: 53	Loss: 0.039014	Acc: 48.0% (4798/10000)
[Test]  Epoch: 54	Loss: 0.039087	Acc: 48.0% (4796/10000)
[Test]  Epoch: 55	Loss: 0.039053	Acc: 48.1% (4808/10000)
[Test]  Epoch: 56	Loss: 0.039012	Acc: 48.2% (4822/10000)
[Test]  Epoch: 57	Loss: 0.038988	Acc: 48.2% (4822/10000)
[Test]  Epoch: 58	Loss: 0.038920	Acc: 48.0% (4802/10000)
[Test]  Epoch: 59	Loss: 0.039042	Acc: 48.0% (4804/10000)
[Test]  Epoch: 60	Loss: 0.039077	Acc: 48.0% (4803/10000)
[Test]  Epoch: 61	Loss: 0.039130	Acc: 48.0% (4804/10000)
[Test]  Epoch: 62	Loss: 0.039026	Acc: 48.3% (4829/10000)
[Test]  Epoch: 63	Loss: 0.039030	Acc: 48.0% (4805/10000)
[Test]  Epoch: 64	Loss: 0.039059	Acc: 48.0% (4802/10000)
[Test]  Epoch: 65	Loss: 0.039044	Acc: 48.0% (4799/10000)
[Test]  Epoch: 66	Loss: 0.039034	Acc: 48.2% (4820/10000)
[Test]  Epoch: 67	Loss: 0.039057	Acc: 48.2% (4820/10000)
[Test]  Epoch: 68	Loss: 0.039128	Acc: 48.0% (4803/10000)
[Test]  Epoch: 69	Loss: 0.039037	Acc: 48.0% (4799/10000)
[Test]  Epoch: 70	Loss: 0.039026	Acc: 48.0% (4803/10000)
[Test]  Epoch: 71	Loss: 0.039148	Acc: 47.9% (4794/10000)
[Test]  Epoch: 72	Loss: 0.039156	Acc: 48.0% (4797/10000)
[Test]  Epoch: 73	Loss: 0.039027	Acc: 48.1% (4812/10000)
[Test]  Epoch: 74	Loss: 0.038980	Acc: 48.2% (4817/10000)
[Test]  Epoch: 75	Loss: 0.039000	Acc: 48.0% (4804/10000)
[Test]  Epoch: 76	Loss: 0.039006	Acc: 48.2% (4817/10000)
[Test]  Epoch: 77	Loss: 0.039014	Acc: 48.2% (4821/10000)
[Test]  Epoch: 78	Loss: 0.039074	Acc: 48.0% (4804/10000)
[Test]  Epoch: 79	Loss: 0.038980	Acc: 48.1% (4813/10000)
[Test]  Epoch: 80	Loss: 0.039056	Acc: 48.0% (4799/10000)
[Test]  Epoch: 81	Loss: 0.039062	Acc: 48.1% (4814/10000)
[Test]  Epoch: 82	Loss: 0.039072	Acc: 48.0% (4802/10000)
[Test]  Epoch: 83	Loss: 0.039019	Acc: 48.1% (4815/10000)
[Test]  Epoch: 84	Loss: 0.039068	Acc: 48.1% (4809/10000)
[Test]  Epoch: 85	Loss: 0.039043	Acc: 48.1% (4813/10000)
[Test]  Epoch: 86	Loss: 0.039123	Acc: 48.0% (4802/10000)
[Test]  Epoch: 87	Loss: 0.039091	Acc: 48.1% (4806/10000)
[Test]  Epoch: 88	Loss: 0.039078	Acc: 48.0% (4799/10000)
[Test]  Epoch: 89	Loss: 0.039113	Acc: 48.2% (4819/10000)
[Test]  Epoch: 90	Loss: 0.039080	Acc: 48.0% (4805/10000)
[Test]  Epoch: 91	Loss: 0.039063	Acc: 48.0% (4805/10000)
[Test]  Epoch: 92	Loss: 0.039028	Acc: 48.1% (4811/10000)
[Test]  Epoch: 93	Loss: 0.039088	Acc: 48.2% (4817/10000)
[Test]  Epoch: 94	Loss: 0.039049	Acc: 48.0% (4798/10000)
[Test]  Epoch: 95	Loss: 0.039044	Acc: 48.0% (4797/10000)
[Test]  Epoch: 96	Loss: 0.039020	Acc: 48.2% (4824/10000)
[Test]  Epoch: 97	Loss: 0.039119	Acc: 48.0% (4795/10000)
[Test]  Epoch: 98	Loss: 0.039141	Acc: 47.9% (4794/10000)
[Test]  Epoch: 99	Loss: 0.039060	Acc: 48.2% (4823/10000)
[Test]  Epoch: 100	Loss: 0.039020	Acc: 48.0% (4805/10000)
===========finish==========
['2024-08-19', '19:05:36.822846', '100', 'test', '0.03902009574174881', '48.05', '48.29']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.053991	Acc: 22.9% (2285/10000)
[Test]  Epoch: 2	Loss: 0.044815	Acc: 36.8% (3683/10000)
[Test]  Epoch: 3	Loss: 0.044641	Acc: 38.2% (3824/10000)
[Test]  Epoch: 4	Loss: 0.044164	Acc: 39.0% (3898/10000)
[Test]  Epoch: 5	Loss: 0.044070	Acc: 39.2% (3920/10000)
[Test]  Epoch: 6	Loss: 0.044016	Acc: 39.6% (3964/10000)
[Test]  Epoch: 7	Loss: 0.044043	Acc: 39.5% (3946/10000)
[Test]  Epoch: 8	Loss: 0.043981	Acc: 39.9% (3993/10000)
[Test]  Epoch: 9	Loss: 0.043921	Acc: 40.0% (3995/10000)
[Test]  Epoch: 10	Loss: 0.043719	Acc: 40.0% (4005/10000)
[Test]  Epoch: 11	Loss: 0.043843	Acc: 40.3% (4029/10000)
[Test]  Epoch: 12	Loss: 0.043663	Acc: 40.3% (4028/10000)
[Test]  Epoch: 13	Loss: 0.043759	Acc: 40.3% (4028/10000)
[Test]  Epoch: 14	Loss: 0.043804	Acc: 40.1% (4013/10000)
[Test]  Epoch: 15	Loss: 0.043759	Acc: 40.2% (4022/10000)
[Test]  Epoch: 16	Loss: 0.043755	Acc: 40.2% (4022/10000)
[Test]  Epoch: 17	Loss: 0.043699	Acc: 40.4% (4042/10000)
[Test]  Epoch: 18	Loss: 0.043734	Acc: 40.3% (4027/10000)
[Test]  Epoch: 19	Loss: 0.043741	Acc: 40.4% (4035/10000)
[Test]  Epoch: 20	Loss: 0.043705	Acc: 40.3% (4030/10000)
[Test]  Epoch: 21	Loss: 0.043680	Acc: 40.3% (4033/10000)
[Test]  Epoch: 22	Loss: 0.043659	Acc: 40.3% (4031/10000)
[Test]  Epoch: 23	Loss: 0.043604	Acc: 40.5% (4049/10000)
[Test]  Epoch: 24	Loss: 0.043755	Acc: 40.3% (4026/10000)
[Test]  Epoch: 25	Loss: 0.043793	Acc: 40.2% (4024/10000)
[Test]  Epoch: 26	Loss: 0.043619	Acc: 40.6% (4062/10000)
[Test]  Epoch: 27	Loss: 0.043733	Acc: 40.3% (4029/10000)
[Test]  Epoch: 28	Loss: 0.043701	Acc: 40.6% (4064/10000)
[Test]  Epoch: 29	Loss: 0.043715	Acc: 40.4% (4043/10000)
[Test]  Epoch: 30	Loss: 0.043690	Acc: 40.7% (4072/10000)
[Test]  Epoch: 31	Loss: 0.043790	Acc: 40.5% (4052/10000)
[Test]  Epoch: 32	Loss: 0.043821	Acc: 40.4% (4043/10000)
[Test]  Epoch: 33	Loss: 0.043797	Acc: 40.6% (4064/10000)
[Test]  Epoch: 34	Loss: 0.043696	Acc: 40.6% (4065/10000)
[Test]  Epoch: 35	Loss: 0.043642	Acc: 40.9% (4087/10000)
[Test]  Epoch: 36	Loss: 0.043815	Acc: 40.7% (4070/10000)
[Test]  Epoch: 37	Loss: 0.043829	Acc: 40.7% (4068/10000)
[Test]  Epoch: 38	Loss: 0.043771	Acc: 40.9% (4093/10000)
[Test]  Epoch: 39	Loss: 0.043802	Acc: 40.8% (4076/10000)
[Test]  Epoch: 40	Loss: 0.043860	Acc: 40.7% (4071/10000)
[Test]  Epoch: 41	Loss: 0.043782	Acc: 40.9% (4088/10000)
[Test]  Epoch: 42	Loss: 0.043804	Acc: 40.7% (4074/10000)
[Test]  Epoch: 43	Loss: 0.043782	Acc: 40.8% (4079/10000)
[Test]  Epoch: 44	Loss: 0.043968	Acc: 40.7% (4070/10000)
[Test]  Epoch: 45	Loss: 0.043915	Acc: 40.7% (4066/10000)
[Test]  Epoch: 46	Loss: 0.044016	Acc: 40.8% (4075/10000)
[Test]  Epoch: 47	Loss: 0.043787	Acc: 41.2% (4121/10000)
[Test]  Epoch: 48	Loss: 0.043863	Acc: 40.9% (4088/10000)
[Test]  Epoch: 49	Loss: 0.043820	Acc: 41.1% (4111/10000)
[Test]  Epoch: 50	Loss: 0.043876	Acc: 41.0% (4097/10000)
[Test]  Epoch: 51	Loss: 0.043877	Acc: 41.2% (4122/10000)
[Test]  Epoch: 52	Loss: 0.043923	Acc: 41.1% (4106/10000)
[Test]  Epoch: 53	Loss: 0.043983	Acc: 40.9% (4093/10000)
[Test]  Epoch: 54	Loss: 0.044032	Acc: 40.9% (4085/10000)
[Test]  Epoch: 55	Loss: 0.043954	Acc: 41.0% (4100/10000)
[Test]  Epoch: 56	Loss: 0.043964	Acc: 41.0% (4100/10000)
[Test]  Epoch: 57	Loss: 0.043909	Acc: 41.0% (4102/10000)
[Test]  Epoch: 58	Loss: 0.043831	Acc: 41.1% (4114/10000)
[Test]  Epoch: 59	Loss: 0.043968	Acc: 41.1% (4106/10000)
[Test]  Epoch: 60	Loss: 0.043952	Acc: 41.2% (4123/10000)
[Test]  Epoch: 61	Loss: 0.044014	Acc: 41.0% (4096/10000)
[Test]  Epoch: 62	Loss: 0.043911	Acc: 41.2% (4125/10000)
[Test]  Epoch: 63	Loss: 0.043929	Acc: 41.1% (4113/10000)
[Test]  Epoch: 64	Loss: 0.043969	Acc: 41.0% (4104/10000)
[Test]  Epoch: 65	Loss: 0.043962	Acc: 41.0% (4103/10000)
[Test]  Epoch: 66	Loss: 0.043906	Acc: 41.2% (4123/10000)
[Test]  Epoch: 67	Loss: 0.043958	Acc: 41.0% (4105/10000)
[Test]  Epoch: 68	Loss: 0.044016	Acc: 41.1% (4110/10000)
[Test]  Epoch: 69	Loss: 0.043892	Acc: 41.3% (4130/10000)
[Test]  Epoch: 70	Loss: 0.043918	Acc: 41.0% (4104/10000)
[Test]  Epoch: 71	Loss: 0.044040	Acc: 41.0% (4103/10000)
[Test]  Epoch: 72	Loss: 0.044056	Acc: 40.9% (4091/10000)
[Test]  Epoch: 73	Loss: 0.043958	Acc: 41.0% (4105/10000)
[Test]  Epoch: 74	Loss: 0.043932	Acc: 41.1% (4110/10000)
[Test]  Epoch: 75	Loss: 0.043918	Acc: 41.2% (4122/10000)
[Test]  Epoch: 76	Loss: 0.043924	Acc: 41.2% (4118/10000)
[Test]  Epoch: 77	Loss: 0.043911	Acc: 41.2% (4120/10000)
[Test]  Epoch: 78	Loss: 0.043956	Acc: 41.2% (4117/10000)
[Test]  Epoch: 79	Loss: 0.043865	Acc: 41.2% (4118/10000)
[Test]  Epoch: 80	Loss: 0.043957	Acc: 41.2% (4120/10000)
[Test]  Epoch: 81	Loss: 0.044001	Acc: 41.0% (4105/10000)
[Test]  Epoch: 82	Loss: 0.043978	Acc: 41.0% (4097/10000)
[Test]  Epoch: 83	Loss: 0.043936	Acc: 41.1% (4110/10000)
[Test]  Epoch: 84	Loss: 0.043981	Acc: 41.1% (4113/10000)
[Test]  Epoch: 85	Loss: 0.043924	Acc: 41.1% (4108/10000)
[Test]  Epoch: 86	Loss: 0.044010	Acc: 41.0% (4103/10000)
[Test]  Epoch: 87	Loss: 0.043986	Acc: 41.0% (4102/10000)
[Test]  Epoch: 88	Loss: 0.043987	Acc: 40.9% (4091/10000)
[Test]  Epoch: 89	Loss: 0.044023	Acc: 41.1% (4114/10000)
[Test]  Epoch: 90	Loss: 0.043979	Acc: 41.1% (4111/10000)
[Test]  Epoch: 91	Loss: 0.043944	Acc: 41.2% (4119/10000)
[Test]  Epoch: 92	Loss: 0.043946	Acc: 41.1% (4113/10000)
[Test]  Epoch: 93	Loss: 0.044026	Acc: 41.0% (4096/10000)
[Test]  Epoch: 94	Loss: 0.043949	Acc: 41.2% (4117/10000)
[Test]  Epoch: 95	Loss: 0.044010	Acc: 41.0% (4104/10000)
[Test]  Epoch: 96	Loss: 0.043948	Acc: 41.1% (4114/10000)
[Test]  Epoch: 97	Loss: 0.044056	Acc: 41.0% (4097/10000)
[Test]  Epoch: 98	Loss: 0.044074	Acc: 41.0% (4098/10000)
[Test]  Epoch: 99	Loss: 0.043969	Acc: 41.0% (4100/10000)
[Test]  Epoch: 100	Loss: 0.043940	Acc: 41.0% (4099/10000)
===========finish==========
['2024-08-19', '19:12:38.530486', '100', 'test', '0.04393954257965088', '40.99', '41.3']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.083444	Acc: 1.1% (115/10000)
[Test]  Epoch: 2	Loss: 0.078991	Acc: 5.0% (498/10000)
[Test]  Epoch: 3	Loss: 0.075750	Acc: 8.3% (828/10000)
[Test]  Epoch: 4	Loss: 0.072667	Acc: 11.2% (1120/10000)
[Test]  Epoch: 5	Loss: 0.070021	Acc: 13.5% (1346/10000)
[Test]  Epoch: 6	Loss: 0.068054	Acc: 15.1% (1513/10000)
[Test]  Epoch: 7	Loss: 0.066461	Acc: 16.1% (1610/10000)
[Test]  Epoch: 8	Loss: 0.064854	Acc: 17.0% (1697/10000)
[Test]  Epoch: 9	Loss: 0.063535	Acc: 17.8% (1779/10000)
[Test]  Epoch: 10	Loss: 0.062373	Acc: 18.5% (1846/10000)
[Test]  Epoch: 11	Loss: 0.061523	Acc: 18.8% (1878/10000)
[Test]  Epoch: 12	Loss: 0.060627	Acc: 19.5% (1954/10000)
[Test]  Epoch: 13	Loss: 0.060142	Acc: 19.8% (1980/10000)
[Test]  Epoch: 14	Loss: 0.059687	Acc: 19.9% (1987/10000)
[Test]  Epoch: 15	Loss: 0.059069	Acc: 20.4% (2035/10000)
[Test]  Epoch: 16	Loss: 0.058602	Acc: 20.6% (2062/10000)
[Test]  Epoch: 17	Loss: 0.058167	Acc: 21.1% (2105/10000)
[Test]  Epoch: 18	Loss: 0.057986	Acc: 20.9% (2086/10000)
[Test]  Epoch: 19	Loss: 0.057641	Acc: 21.5% (2152/10000)
[Test]  Epoch: 20	Loss: 0.057270	Acc: 21.7% (2169/10000)
[Test]  Epoch: 21	Loss: 0.057017	Acc: 22.1% (2210/10000)
[Test]  Epoch: 22	Loss: 0.056724	Acc: 22.4% (2243/10000)
[Test]  Epoch: 23	Loss: 0.056530	Acc: 22.4% (2245/10000)
[Test]  Epoch: 24	Loss: 0.056374	Acc: 22.4% (2239/10000)
[Test]  Epoch: 25	Loss: 0.056312	Acc: 22.6% (2263/10000)
[Test]  Epoch: 26	Loss: 0.056066	Acc: 22.9% (2292/10000)
[Test]  Epoch: 27	Loss: 0.055870	Acc: 23.0% (2304/10000)
[Test]  Epoch: 28	Loss: 0.055707	Acc: 23.2% (2316/10000)
[Test]  Epoch: 29	Loss: 0.055547	Acc: 23.5% (2347/10000)
[Test]  Epoch: 30	Loss: 0.055364	Acc: 23.7% (2368/10000)
[Test]  Epoch: 31	Loss: 0.055377	Acc: 23.8% (2376/10000)
[Test]  Epoch: 32	Loss: 0.055302	Acc: 23.8% (2376/10000)
[Test]  Epoch: 33	Loss: 0.055213	Acc: 23.9% (2395/10000)
[Test]  Epoch: 34	Loss: 0.054980	Acc: 24.3% (2433/10000)
[Test]  Epoch: 35	Loss: 0.054848	Acc: 24.3% (2432/10000)
[Test]  Epoch: 36	Loss: 0.054912	Acc: 24.2% (2416/10000)
[Test]  Epoch: 37	Loss: 0.054843	Acc: 24.4% (2441/10000)
[Test]  Epoch: 38	Loss: 0.054594	Acc: 24.6% (2456/10000)
[Test]  Epoch: 39	Loss: 0.054614	Acc: 24.6% (2461/10000)
[Test]  Epoch: 40	Loss: 0.054536	Acc: 24.7% (2470/10000)
[Test]  Epoch: 41	Loss: 0.054374	Acc: 25.0% (2497/10000)
[Test]  Epoch: 42	Loss: 0.054374	Acc: 25.0% (2501/10000)
[Test]  Epoch: 43	Loss: 0.054211	Acc: 25.3% (2533/10000)
[Test]  Epoch: 44	Loss: 0.054341	Acc: 25.0% (2499/10000)
[Test]  Epoch: 45	Loss: 0.054099	Acc: 25.3% (2529/10000)
[Test]  Epoch: 46	Loss: 0.054186	Acc: 25.4% (2538/10000)
[Test]  Epoch: 47	Loss: 0.054024	Acc: 25.5% (2549/10000)
[Test]  Epoch: 48	Loss: 0.053933	Acc: 25.6% (2564/10000)
[Test]  Epoch: 49	Loss: 0.053911	Acc: 25.6% (2565/10000)
[Test]  Epoch: 50	Loss: 0.053882	Acc: 25.7% (2572/10000)
[Test]  Epoch: 51	Loss: 0.053840	Acc: 25.7% (2569/10000)
[Test]  Epoch: 52	Loss: 0.053791	Acc: 25.8% (2577/10000)
[Test]  Epoch: 53	Loss: 0.053750	Acc: 25.7% (2570/10000)
[Test]  Epoch: 54	Loss: 0.053769	Acc: 26.0% (2598/10000)
[Test]  Epoch: 55	Loss: 0.053679	Acc: 26.1% (2606/10000)
[Test]  Epoch: 56	Loss: 0.053650	Acc: 26.1% (2608/10000)
[Test]  Epoch: 57	Loss: 0.053604	Acc: 26.2% (2619/10000)
[Test]  Epoch: 58	Loss: 0.053368	Acc: 26.4% (2638/10000)
[Test]  Epoch: 59	Loss: 0.053481	Acc: 26.4% (2635/10000)
[Test]  Epoch: 60	Loss: 0.053468	Acc: 26.3% (2632/10000)
[Test]  Epoch: 61	Loss: 0.053499	Acc: 26.2% (2622/10000)
[Test]  Epoch: 62	Loss: 0.053467	Acc: 26.3% (2631/10000)
[Test]  Epoch: 63	Loss: 0.053444	Acc: 26.4% (2638/10000)
[Test]  Epoch: 64	Loss: 0.053394	Acc: 26.4% (2639/10000)
[Test]  Epoch: 65	Loss: 0.053482	Acc: 26.2% (2625/10000)
[Test]  Epoch: 66	Loss: 0.053432	Acc: 26.4% (2635/10000)
[Test]  Epoch: 67	Loss: 0.053443	Acc: 26.2% (2624/10000)
[Test]  Epoch: 68	Loss: 0.053477	Acc: 26.2% (2622/10000)
[Test]  Epoch: 69	Loss: 0.053342	Acc: 26.6% (2665/10000)
[Test]  Epoch: 70	Loss: 0.053376	Acc: 26.5% (2652/10000)
[Test]  Epoch: 71	Loss: 0.053496	Acc: 26.4% (2645/10000)
[Test]  Epoch: 72	Loss: 0.053490	Acc: 26.5% (2648/10000)
[Test]  Epoch: 73	Loss: 0.053370	Acc: 26.6% (2660/10000)
[Test]  Epoch: 74	Loss: 0.053318	Acc: 26.6% (2655/10000)
[Test]  Epoch: 75	Loss: 0.053346	Acc: 26.7% (2667/10000)
[Test]  Epoch: 76	Loss: 0.053388	Acc: 26.5% (2653/10000)
[Test]  Epoch: 77	Loss: 0.053326	Acc: 26.5% (2654/10000)
[Test]  Epoch: 78	Loss: 0.053384	Acc: 26.4% (2637/10000)
[Test]  Epoch: 79	Loss: 0.053291	Acc: 26.6% (2663/10000)
[Test]  Epoch: 80	Loss: 0.053428	Acc: 26.4% (2639/10000)
[Test]  Epoch: 81	Loss: 0.053385	Acc: 26.6% (2659/10000)
[Test]  Epoch: 82	Loss: 0.053401	Acc: 26.5% (2649/10000)
[Test]  Epoch: 83	Loss: 0.053335	Acc: 26.5% (2652/10000)
[Test]  Epoch: 84	Loss: 0.053355	Acc: 26.7% (2668/10000)
[Test]  Epoch: 85	Loss: 0.053347	Acc: 26.6% (2663/10000)
[Test]  Epoch: 86	Loss: 0.053349	Acc: 26.4% (2644/10000)
[Test]  Epoch: 87	Loss: 0.053377	Acc: 26.5% (2654/10000)
[Test]  Epoch: 88	Loss: 0.053371	Acc: 26.6% (2658/10000)
[Test]  Epoch: 89	Loss: 0.053377	Acc: 26.6% (2656/10000)
[Test]  Epoch: 90	Loss: 0.053325	Acc: 26.6% (2665/10000)
[Test]  Epoch: 91	Loss: 0.053288	Acc: 26.7% (2674/10000)
[Test]  Epoch: 92	Loss: 0.053301	Acc: 26.6% (2659/10000)
[Test]  Epoch: 93	Loss: 0.053316	Acc: 26.6% (2656/10000)
[Test]  Epoch: 94	Loss: 0.053321	Acc: 26.6% (2665/10000)
[Test]  Epoch: 95	Loss: 0.053356	Acc: 26.6% (2662/10000)
[Test]  Epoch: 96	Loss: 0.053256	Acc: 26.8% (2676/10000)
[Test]  Epoch: 97	Loss: 0.053363	Acc: 26.6% (2661/10000)
[Test]  Epoch: 98	Loss: 0.053398	Acc: 26.6% (2658/10000)
[Test]  Epoch: 99	Loss: 0.053227	Acc: 26.8% (2682/10000)
[Test]  Epoch: 100	Loss: 0.053253	Acc: 26.9% (2688/10000)
===========finish==========
['2024-08-19', '19:19:44.025452', '100', 'test', '0.05325329995155335', '26.88', '26.88']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/stl10-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('conv1.weight', 0.0), ('bn1.weight', 0.0), ('bn1.bias', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.bn1.weight', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.bn2.weight', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.conv3.weight', 0.0), ('layer1.0.bn3.weight', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.0.weight', 0.0), ('layer1.0.downsample.1.weight', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.bn1.weight', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.bn2.weight', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.conv3.weight', 0.0), ('layer1.1.bn3.weight', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.conv1.weight', 0.0), ('layer1.2.bn1.weight', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.conv2.weight', 0.0), ('layer1.2.bn2.weight', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.conv3.weight', 0.0), ('layer1.2.bn3.weight', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.bn1.weight', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.bn2.weight', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.conv3.weight', 0.0), ('layer2.0.bn3.weight', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.0.weight', 0.0), ('layer2.0.downsample.1.weight', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.bn1.weight', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.bn2.weight', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.conv3.weight', 0.0), ('layer2.1.bn3.weight', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.conv1.weight', 0.0), ('layer2.2.bn1.weight', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.conv2.weight', 0.0), ('layer2.2.bn2.weight', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.conv3.weight', 0.0), ('layer2.2.bn3.weight', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.conv1.weight', 0.0), ('layer2.3.bn1.weight', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.conv2.weight', 0.0), ('layer2.3.bn2.weight', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.conv3.weight', 0.0), ('layer2.3.bn3.weight', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.bn1.weight', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.bn2.weight', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.conv3.weight', 0.0), ('layer3.0.bn3.weight', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.0.weight', 0.0), ('layer3.0.downsample.1.weight', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.bn1.weight', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.bn2.weight', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.conv3.weight', 0.0), ('layer3.1.bn3.weight', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.conv1.weight', 0.0), ('layer3.2.bn1.weight', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.conv2.weight', 0.0), ('layer3.2.bn2.weight', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.conv3.weight', 0.0), ('layer3.2.bn3.weight', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.conv1.weight', 0.0), ('layer3.3.bn1.weight', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.conv2.weight', 0.0), ('layer3.3.bn2.weight', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.conv3.weight', 0.0), ('layer3.3.bn3.weight', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.conv1.weight', 0.0), ('layer3.4.bn1.weight', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.conv2.weight', 0.0), ('layer3.4.bn2.weight', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.conv3.weight', 0.0), ('layer3.4.bn3.weight', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.conv1.weight', 0.0), ('layer3.5.bn1.weight', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.conv2.weight', 0.0), ('layer3.5.bn2.weight', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.conv3.weight', 0.0), ('layer3.5.bn3.weight', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.bn1.weight', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.bn2.weight', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.conv3.weight', 0.0), ('layer4.0.bn3.weight', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.0.weight', 0.0), ('layer4.0.downsample.1.weight', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.bn1.weight', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.bn2.weight', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.conv3.weight', 0.0), ('layer4.1.bn3.weight', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.conv1.weight', 0.0), ('layer4.2.bn1.weight', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.conv2.weight', 0.0), ('layer4.2.bn2.weight', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.conv3.weight', 0.0), ('layer4.2.bn3.weight', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('conv1.weight', 0.0), ('layer1.0.conv1.weight', 0.0), ('layer1.0.conv2.weight', 0.0), ('layer1.0.conv3.weight', 0.0), ('layer1.1.conv1.weight', 0.0), ('layer1.1.conv2.weight', 0.0), ('layer1.1.conv3.weight', 0.0), ('layer1.2.conv1.weight', 0.0), ('layer1.2.conv2.weight', 0.0), ('layer1.2.conv3.weight', 0.0), ('layer2.0.conv1.weight', 0.0), ('layer2.0.conv2.weight', 0.0), ('layer2.0.conv3.weight', 0.0), ('layer2.1.conv1.weight', 0.0), ('layer2.1.conv2.weight', 0.0), ('layer2.1.conv3.weight', 0.0), ('layer2.2.conv1.weight', 0.0), ('layer2.2.conv2.weight', 0.0), ('layer2.2.conv3.weight', 0.0), ('layer2.3.conv1.weight', 0.0), ('layer2.3.conv2.weight', 0.0), ('layer2.3.conv3.weight', 0.0), ('layer3.0.conv1.weight', 0.0), ('layer3.0.conv2.weight', 0.0), ('layer3.0.conv3.weight', 0.0), ('layer3.1.conv1.weight', 0.0), ('layer3.1.conv2.weight', 0.0), ('layer3.1.conv3.weight', 0.0), ('layer3.2.conv1.weight', 0.0), ('layer3.2.conv2.weight', 0.0), ('layer3.2.conv3.weight', 0.0), ('layer3.3.conv1.weight', 0.0), ('layer3.3.conv2.weight', 0.0), ('layer3.3.conv3.weight', 0.0), ('layer3.4.conv1.weight', 0.0), ('layer3.4.conv2.weight', 0.0), ('layer3.4.conv3.weight', 0.0), ('layer3.5.conv1.weight', 0.0), ('layer3.5.conv2.weight', 0.0), ('layer3.5.conv3.weight', 0.0), ('layer4.0.conv1.weight', 0.0), ('layer4.0.conv2.weight', 0.0), ('layer4.0.conv3.weight', 0.0), ('layer4.1.conv1.weight', 0.0), ('layer4.1.conv2.weight', 0.0), ('layer4.1.conv3.weight', 0.0), ('layer4.2.conv1.weight', 0.0), ('layer4.2.conv2.weight', 0.0), ('layer4.2.conv3.weight', 0.0), ('last_linear.weight', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.014418	Acc: 67.0% (5359/8000)
[Test]  Epoch: 2	Loss: 0.014409	Acc: 67.2% (5372/8000)
[Test]  Epoch: 3	Loss: 0.014490	Acc: 67.2% (5375/8000)
[Test]  Epoch: 4	Loss: 0.014472	Acc: 66.8% (5342/8000)
[Test]  Epoch: 5	Loss: 0.014454	Acc: 67.0% (5357/8000)
[Test]  Epoch: 6	Loss: 0.014467	Acc: 67.1% (5366/8000)
[Test]  Epoch: 7	Loss: 0.014401	Acc: 67.0% (5359/8000)
[Test]  Epoch: 8	Loss: 0.014354	Acc: 67.2% (5380/8000)
[Test]  Epoch: 9	Loss: 0.014382	Acc: 66.8% (5343/8000)
[Test]  Epoch: 10	Loss: 0.014340	Acc: 67.3% (5382/8000)
[Test]  Epoch: 11	Loss: 0.014435	Acc: 66.9% (5351/8000)
[Test]  Epoch: 12	Loss: 0.014291	Acc: 67.5% (5399/8000)
[Test]  Epoch: 13	Loss: 0.014325	Acc: 67.4% (5390/8000)
[Test]  Epoch: 14	Loss: 0.014310	Acc: 67.2% (5379/8000)
[Test]  Epoch: 15	Loss: 0.014389	Acc: 67.1% (5369/8000)
[Test]  Epoch: 16	Loss: 0.014337	Acc: 67.1% (5367/8000)
[Test]  Epoch: 17	Loss: 0.014302	Acc: 67.2% (5377/8000)
[Test]  Epoch: 18	Loss: 0.014377	Acc: 66.9% (5352/8000)
[Test]  Epoch: 19	Loss: 0.014366	Acc: 67.2% (5372/8000)
[Test]  Epoch: 20	Loss: 0.014451	Acc: 66.8% (5343/8000)
[Test]  Epoch: 21	Loss: 0.014350	Acc: 67.4% (5389/8000)
[Test]  Epoch: 22	Loss: 0.014377	Acc: 67.5% (5400/8000)
[Test]  Epoch: 23	Loss: 0.014273	Acc: 67.5% (5404/8000)
[Test]  Epoch: 24	Loss: 0.014323	Acc: 67.1% (5367/8000)
[Test]  Epoch: 25	Loss: 0.014421	Acc: 66.9% (5355/8000)
[Test]  Epoch: 26	Loss: 0.014324	Acc: 67.3% (5388/8000)
[Test]  Epoch: 27	Loss: 0.014316	Acc: 67.5% (5397/8000)
[Test]  Epoch: 28	Loss: 0.014321	Acc: 67.1% (5369/8000)
[Test]  Epoch: 29	Loss: 0.014321	Acc: 67.4% (5391/8000)
[Test]  Epoch: 30	Loss: 0.014341	Acc: 67.1% (5365/8000)
[Test]  Epoch: 31	Loss: 0.014312	Acc: 67.3% (5384/8000)
[Test]  Epoch: 32	Loss: 0.014283	Acc: 67.3% (5387/8000)
[Test]  Epoch: 33	Loss: 0.014325	Acc: 67.1% (5370/8000)
[Test]  Epoch: 34	Loss: 0.014252	Acc: 67.5% (5396/8000)
[Test]  Epoch: 35	Loss: 0.014269	Acc: 67.3% (5384/8000)
[Test]  Epoch: 36	Loss: 0.014243	Acc: 67.2% (5379/8000)
[Test]  Epoch: 37	Loss: 0.014269	Acc: 67.4% (5392/8000)
[Test]  Epoch: 38	Loss: 0.014341	Acc: 67.3% (5382/8000)
[Test]  Epoch: 39	Loss: 0.014279	Acc: 67.3% (5382/8000)
[Test]  Epoch: 40	Loss: 0.014298	Acc: 67.0% (5361/8000)
[Test]  Epoch: 41	Loss: 0.014247	Acc: 67.0% (5364/8000)
[Test]  Epoch: 42	Loss: 0.014343	Acc: 67.4% (5393/8000)
[Test]  Epoch: 43	Loss: 0.014299	Acc: 67.3% (5381/8000)
[Test]  Epoch: 44	Loss: 0.014277	Acc: 67.7% (5418/8000)
[Test]  Epoch: 45	Loss: 0.014281	Acc: 67.4% (5389/8000)
[Test]  Epoch: 46	Loss: 0.014303	Acc: 67.4% (5392/8000)
[Test]  Epoch: 47	Loss: 0.014292	Acc: 67.5% (5400/8000)
[Test]  Epoch: 48	Loss: 0.014325	Acc: 67.4% (5393/8000)
[Test]  Epoch: 49	Loss: 0.014282	Acc: 67.5% (5403/8000)
[Test]  Epoch: 50	Loss: 0.014269	Acc: 67.5% (5399/8000)
[Test]  Epoch: 51	Loss: 0.014294	Acc: 67.3% (5383/8000)
[Test]  Epoch: 52	Loss: 0.014303	Acc: 67.2% (5380/8000)
[Test]  Epoch: 53	Loss: 0.014275	Acc: 67.2% (5376/8000)
[Test]  Epoch: 54	Loss: 0.014314	Acc: 67.4% (5394/8000)
[Test]  Epoch: 55	Loss: 0.014305	Acc: 67.4% (5389/8000)
[Test]  Epoch: 56	Loss: 0.014292	Acc: 67.4% (5393/8000)
[Test]  Epoch: 57	Loss: 0.014243	Acc: 67.2% (5375/8000)
[Test]  Epoch: 58	Loss: 0.014281	Acc: 67.5% (5398/8000)
[Test]  Epoch: 59	Loss: 0.014314	Acc: 67.2% (5378/8000)
[Test]  Epoch: 60	Loss: 0.014325	Acc: 67.5% (5398/8000)
[Test]  Epoch: 61	Loss: 0.014311	Acc: 67.6% (5406/8000)
[Test]  Epoch: 62	Loss: 0.014310	Acc: 67.4% (5389/8000)
[Test]  Epoch: 63	Loss: 0.014288	Acc: 67.5% (5396/8000)
[Test]  Epoch: 64	Loss: 0.014291	Acc: 67.5% (5401/8000)
[Test]  Epoch: 65	Loss: 0.014280	Acc: 67.4% (5391/8000)
[Test]  Epoch: 66	Loss: 0.014290	Acc: 67.4% (5390/8000)
[Test]  Epoch: 67	Loss: 0.014292	Acc: 67.3% (5385/8000)
[Test]  Epoch: 68	Loss: 0.014292	Acc: 67.4% (5393/8000)
[Test]  Epoch: 69	Loss: 0.014272	Acc: 67.5% (5400/8000)
[Test]  Epoch: 70	Loss: 0.014300	Acc: 67.5% (5396/8000)
[Test]  Epoch: 71	Loss: 0.014281	Acc: 67.5% (5399/8000)
[Test]  Epoch: 72	Loss: 0.014278	Acc: 67.4% (5395/8000)
[Test]  Epoch: 73	Loss: 0.014277	Acc: 67.4% (5390/8000)
[Test]  Epoch: 74	Loss: 0.014280	Acc: 67.5% (5398/8000)
[Test]  Epoch: 75	Loss: 0.014280	Acc: 67.5% (5396/8000)
[Test]  Epoch: 76	Loss: 0.014294	Acc: 67.1% (5371/8000)
[Test]  Epoch: 77	Loss: 0.014276	Acc: 67.4% (5389/8000)
[Test]  Epoch: 78	Loss: 0.014291	Acc: 67.2% (5378/8000)
[Test]  Epoch: 79	Loss: 0.014282	Acc: 67.5% (5400/8000)
[Test]  Epoch: 80	Loss: 0.014275	Acc: 67.5% (5396/8000)
[Test]  Epoch: 81	Loss: 0.014278	Acc: 67.4% (5390/8000)
[Test]  Epoch: 82	Loss: 0.014350	Acc: 67.2% (5375/8000)
[Test]  Epoch: 83	Loss: 0.014275	Acc: 67.3% (5381/8000)
[Test]  Epoch: 84	Loss: 0.014284	Acc: 67.2% (5377/8000)
[Test]  Epoch: 85	Loss: 0.014283	Acc: 67.4% (5389/8000)
[Test]  Epoch: 86	Loss: 0.014291	Acc: 67.4% (5395/8000)
[Test]  Epoch: 87	Loss: 0.014291	Acc: 67.3% (5386/8000)
[Test]  Epoch: 88	Loss: 0.014285	Acc: 67.3% (5384/8000)
[Test]  Epoch: 89	Loss: 0.014282	Acc: 67.5% (5399/8000)
[Test]  Epoch: 90	Loss: 0.014284	Acc: 67.2% (5377/8000)
[Test]  Epoch: 91	Loss: 0.014301	Acc: 67.3% (5382/8000)
[Test]  Epoch: 92	Loss: 0.014289	Acc: 67.2% (5374/8000)
[Test]  Epoch: 93	Loss: 0.014269	Acc: 67.3% (5388/8000)
[Test]  Epoch: 94	Loss: 0.014276	Acc: 67.3% (5386/8000)
[Test]  Epoch: 95	Loss: 0.014273	Acc: 67.3% (5383/8000)
[Test]  Epoch: 96	Loss: 0.014258	Acc: 67.3% (5383/8000)
[Test]  Epoch: 97	Loss: 0.014289	Acc: 67.5% (5396/8000)
[Test]  Epoch: 98	Loss: 0.014282	Acc: 67.5% (5400/8000)
[Test]  Epoch: 99	Loss: 0.014285	Acc: 67.3% (5388/8000)
[Test]  Epoch: 100	Loss: 0.014283	Acc: 67.7% (5413/8000)
===========finish==========
['2024-08-19', '19:28:40.030212', '100', 'test', '0.01428293488919735', '67.6625', '67.725']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039128	Acc: 24.3% (1942/8000)
[Test]  Epoch: 2	Loss: 0.020604	Acc: 52.6% (4209/8000)
[Test]  Epoch: 3	Loss: 0.019429	Acc: 55.5% (4444/8000)
[Test]  Epoch: 4	Loss: 0.017516	Acc: 60.6% (4845/8000)
[Test]  Epoch: 5	Loss: 0.016896	Acc: 61.5% (4919/8000)
[Test]  Epoch: 6	Loss: 0.016797	Acc: 61.8% (4940/8000)
[Test]  Epoch: 7	Loss: 0.016595	Acc: 61.9% (4955/8000)
[Test]  Epoch: 8	Loss: 0.016645	Acc: 61.6% (4932/8000)
[Test]  Epoch: 9	Loss: 0.016559	Acc: 63.0% (5038/8000)
[Test]  Epoch: 10	Loss: 0.016809	Acc: 62.2% (4973/8000)
[Test]  Epoch: 11	Loss: 0.016506	Acc: 62.7% (5013/8000)
[Test]  Epoch: 12	Loss: 0.016346	Acc: 62.8% (5023/8000)
[Test]  Epoch: 13	Loss: 0.016348	Acc: 62.7% (5015/8000)
[Test]  Epoch: 14	Loss: 0.016248	Acc: 63.0% (5042/8000)
[Test]  Epoch: 15	Loss: 0.016239	Acc: 63.0% (5043/8000)
[Test]  Epoch: 16	Loss: 0.016310	Acc: 62.6% (5008/8000)
[Test]  Epoch: 17	Loss: 0.016053	Acc: 63.2% (5058/8000)
[Test]  Epoch: 18	Loss: 0.016151	Acc: 62.7% (5015/8000)
[Test]  Epoch: 19	Loss: 0.016064	Acc: 63.2% (5060/8000)
[Test]  Epoch: 20	Loss: 0.016103	Acc: 63.0% (5039/8000)
[Test]  Epoch: 21	Loss: 0.016101	Acc: 63.0% (5044/8000)
[Test]  Epoch: 22	Loss: 0.016033	Acc: 63.4% (5073/8000)
[Test]  Epoch: 23	Loss: 0.016077	Acc: 63.1% (5050/8000)
[Test]  Epoch: 24	Loss: 0.015978	Acc: 63.5% (5084/8000)
[Test]  Epoch: 25	Loss: 0.016171	Acc: 62.7% (5016/8000)
[Test]  Epoch: 26	Loss: 0.015948	Acc: 63.4% (5072/8000)
[Test]  Epoch: 27	Loss: 0.015939	Acc: 63.4% (5072/8000)
[Test]  Epoch: 28	Loss: 0.015870	Acc: 63.5% (5077/8000)
[Test]  Epoch: 29	Loss: 0.015945	Acc: 63.2% (5060/8000)
[Test]  Epoch: 30	Loss: 0.016041	Acc: 63.1% (5047/8000)
[Test]  Epoch: 31	Loss: 0.015888	Acc: 63.6% (5087/8000)
[Test]  Epoch: 32	Loss: 0.015875	Acc: 63.1% (5051/8000)
[Test]  Epoch: 33	Loss: 0.015906	Acc: 63.2% (5059/8000)
[Test]  Epoch: 34	Loss: 0.015839	Acc: 63.5% (5076/8000)
[Test]  Epoch: 35	Loss: 0.015942	Acc: 63.3% (5066/8000)
[Test]  Epoch: 36	Loss: 0.015815	Acc: 63.8% (5101/8000)
[Test]  Epoch: 37	Loss: 0.015802	Acc: 63.5% (5081/8000)
[Test]  Epoch: 38	Loss: 0.015789	Acc: 63.6% (5088/8000)
[Test]  Epoch: 39	Loss: 0.015754	Acc: 63.7% (5097/8000)
[Test]  Epoch: 40	Loss: 0.015864	Acc: 63.5% (5084/8000)
[Test]  Epoch: 41	Loss: 0.015812	Acc: 63.5% (5084/8000)
[Test]  Epoch: 42	Loss: 0.015738	Acc: 63.8% (5102/8000)
[Test]  Epoch: 43	Loss: 0.015696	Acc: 63.7% (5096/8000)
[Test]  Epoch: 44	Loss: 0.015733	Acc: 63.9% (5108/8000)
[Test]  Epoch: 45	Loss: 0.015773	Acc: 63.7% (5097/8000)
[Test]  Epoch: 46	Loss: 0.015881	Acc: 63.2% (5059/8000)
[Test]  Epoch: 47	Loss: 0.015762	Acc: 63.9% (5108/8000)
[Test]  Epoch: 48	Loss: 0.015746	Acc: 63.6% (5085/8000)
[Test]  Epoch: 49	Loss: 0.015728	Acc: 63.7% (5099/8000)
[Test]  Epoch: 50	Loss: 0.015733	Acc: 63.7% (5093/8000)
[Test]  Epoch: 51	Loss: 0.015770	Acc: 63.8% (5102/8000)
[Test]  Epoch: 52	Loss: 0.015783	Acc: 64.0% (5117/8000)
[Test]  Epoch: 53	Loss: 0.015812	Acc: 63.6% (5088/8000)
[Test]  Epoch: 54	Loss: 0.015661	Acc: 64.0% (5118/8000)
[Test]  Epoch: 55	Loss: 0.015821	Acc: 63.5% (5076/8000)
[Test]  Epoch: 56	Loss: 0.015698	Acc: 63.9% (5108/8000)
[Test]  Epoch: 57	Loss: 0.015681	Acc: 64.0% (5117/8000)
[Test]  Epoch: 58	Loss: 0.015739	Acc: 63.7% (5093/8000)
[Test]  Epoch: 59	Loss: 0.015705	Acc: 63.6% (5090/8000)
[Test]  Epoch: 60	Loss: 0.015742	Acc: 63.8% (5105/8000)
[Test]  Epoch: 61	Loss: 0.015687	Acc: 63.8% (5104/8000)
[Test]  Epoch: 62	Loss: 0.015667	Acc: 63.7% (5097/8000)
[Test]  Epoch: 63	Loss: 0.015660	Acc: 64.0% (5119/8000)
[Test]  Epoch: 64	Loss: 0.015674	Acc: 63.8% (5107/8000)
[Test]  Epoch: 65	Loss: 0.015650	Acc: 64.1% (5127/8000)
[Test]  Epoch: 66	Loss: 0.015641	Acc: 63.9% (5111/8000)
[Test]  Epoch: 67	Loss: 0.015656	Acc: 63.9% (5114/8000)
[Test]  Epoch: 68	Loss: 0.015668	Acc: 64.0% (5118/8000)
[Test]  Epoch: 69	Loss: 0.015647	Acc: 64.0% (5124/8000)
[Test]  Epoch: 70	Loss: 0.015651	Acc: 64.0% (5124/8000)
[Test]  Epoch: 71	Loss: 0.015630	Acc: 64.0% (5116/8000)
[Test]  Epoch: 72	Loss: 0.015640	Acc: 64.2% (5132/8000)
[Test]  Epoch: 73	Loss: 0.015644	Acc: 63.9% (5114/8000)
[Test]  Epoch: 74	Loss: 0.015657	Acc: 63.9% (5114/8000)
[Test]  Epoch: 75	Loss: 0.015660	Acc: 64.1% (5126/8000)
[Test]  Epoch: 76	Loss: 0.015681	Acc: 64.0% (5123/8000)
[Test]  Epoch: 77	Loss: 0.015659	Acc: 63.8% (5105/8000)
[Test]  Epoch: 78	Loss: 0.015672	Acc: 63.9% (5112/8000)
[Test]  Epoch: 79	Loss: 0.015646	Acc: 63.9% (5111/8000)
[Test]  Epoch: 80	Loss: 0.015650	Acc: 64.0% (5121/8000)
[Test]  Epoch: 81	Loss: 0.015637	Acc: 64.1% (5128/8000)
[Test]  Epoch: 82	Loss: 0.015685	Acc: 63.7% (5099/8000)
[Test]  Epoch: 83	Loss: 0.015650	Acc: 64.0% (5118/8000)
[Test]  Epoch: 84	Loss: 0.015654	Acc: 63.9% (5115/8000)
[Test]  Epoch: 85	Loss: 0.015657	Acc: 63.9% (5115/8000)
[Test]  Epoch: 86	Loss: 0.015662	Acc: 64.0% (5118/8000)
[Test]  Epoch: 87	Loss: 0.015655	Acc: 64.1% (5126/8000)
[Test]  Epoch: 88	Loss: 0.015656	Acc: 64.1% (5128/8000)
[Test]  Epoch: 89	Loss: 0.015650	Acc: 63.8% (5107/8000)
[Test]  Epoch: 90	Loss: 0.015659	Acc: 63.9% (5113/8000)
[Test]  Epoch: 91	Loss: 0.015658	Acc: 63.9% (5115/8000)
[Test]  Epoch: 92	Loss: 0.015641	Acc: 64.1% (5126/8000)
[Test]  Epoch: 93	Loss: 0.015602	Acc: 64.2% (5135/8000)
[Test]  Epoch: 94	Loss: 0.015634	Acc: 64.0% (5121/8000)
[Test]  Epoch: 95	Loss: 0.015638	Acc: 64.2% (5132/8000)
[Test]  Epoch: 96	Loss: 0.015625	Acc: 64.0% (5117/8000)
[Test]  Epoch: 97	Loss: 0.015648	Acc: 64.0% (5121/8000)
[Test]  Epoch: 98	Loss: 0.015636	Acc: 63.9% (5115/8000)
[Test]  Epoch: 99	Loss: 0.015646	Acc: 63.9% (5111/8000)
[Test]  Epoch: 100	Loss: 0.015644	Acc: 64.1% (5126/8000)
===========finish==========
['2024-08-19', '19:38:06.511749', '100', 'test', '0.015643917500972746', '64.075', '64.1875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042083	Acc: 19.4% (1548/8000)
[Test]  Epoch: 2	Loss: 0.026105	Acc: 42.5% (3403/8000)
[Test]  Epoch: 3	Loss: 0.018248	Acc: 57.5% (4598/8000)
[Test]  Epoch: 4	Loss: 0.016206	Acc: 63.1% (5047/8000)
[Test]  Epoch: 5	Loss: 0.015703	Acc: 63.8% (5100/8000)
[Test]  Epoch: 6	Loss: 0.015464	Acc: 64.6% (5167/8000)
[Test]  Epoch: 7	Loss: 0.015542	Acc: 64.6% (5167/8000)
[Test]  Epoch: 8	Loss: 0.015438	Acc: 64.9% (5192/8000)
[Test]  Epoch: 9	Loss: 0.015392	Acc: 64.7% (5174/8000)
[Test]  Epoch: 10	Loss: 0.015285	Acc: 65.2% (5216/8000)
[Test]  Epoch: 11	Loss: 0.015564	Acc: 64.7% (5174/8000)
[Test]  Epoch: 12	Loss: 0.015391	Acc: 65.2% (5218/8000)
[Test]  Epoch: 13	Loss: 0.015307	Acc: 65.2% (5219/8000)
[Test]  Epoch: 14	Loss: 0.015317	Acc: 65.0% (5201/8000)
[Test]  Epoch: 15	Loss: 0.015299	Acc: 64.8% (5188/8000)
[Test]  Epoch: 16	Loss: 0.015269	Acc: 65.1% (5206/8000)
[Test]  Epoch: 17	Loss: 0.015160	Acc: 65.3% (5221/8000)
[Test]  Epoch: 18	Loss: 0.015320	Acc: 64.5% (5158/8000)
[Test]  Epoch: 19	Loss: 0.015210	Acc: 65.1% (5211/8000)
[Test]  Epoch: 20	Loss: 0.015303	Acc: 64.8% (5184/8000)
[Test]  Epoch: 21	Loss: 0.015273	Acc: 65.2% (5220/8000)
[Test]  Epoch: 22	Loss: 0.015156	Acc: 65.4% (5229/8000)
[Test]  Epoch: 23	Loss: 0.015150	Acc: 65.2% (5219/8000)
[Test]  Epoch: 24	Loss: 0.015226	Acc: 65.2% (5218/8000)
[Test]  Epoch: 25	Loss: 0.015208	Acc: 65.7% (5254/8000)
[Test]  Epoch: 26	Loss: 0.015111	Acc: 65.4% (5234/8000)
[Test]  Epoch: 27	Loss: 0.015093	Acc: 65.5% (5240/8000)
[Test]  Epoch: 28	Loss: 0.015062	Acc: 65.5% (5243/8000)
[Test]  Epoch: 29	Loss: 0.015085	Acc: 65.4% (5229/8000)
[Test]  Epoch: 30	Loss: 0.015183	Acc: 65.2% (5217/8000)
[Test]  Epoch: 31	Loss: 0.015086	Acc: 65.5% (5236/8000)
[Test]  Epoch: 32	Loss: 0.015042	Acc: 65.7% (5252/8000)
[Test]  Epoch: 33	Loss: 0.015047	Acc: 65.6% (5248/8000)
[Test]  Epoch: 34	Loss: 0.015071	Acc: 65.5% (5240/8000)
[Test]  Epoch: 35	Loss: 0.015144	Acc: 65.2% (5216/8000)
[Test]  Epoch: 36	Loss: 0.015121	Acc: 65.2% (5219/8000)
[Test]  Epoch: 37	Loss: 0.015136	Acc: 65.2% (5216/8000)
[Test]  Epoch: 38	Loss: 0.015056	Acc: 65.5% (5236/8000)
[Test]  Epoch: 39	Loss: 0.015044	Acc: 65.6% (5247/8000)
[Test]  Epoch: 40	Loss: 0.015144	Acc: 65.2% (5220/8000)
[Test]  Epoch: 41	Loss: 0.014992	Acc: 65.5% (5241/8000)
[Test]  Epoch: 42	Loss: 0.015014	Acc: 65.8% (5264/8000)
[Test]  Epoch: 43	Loss: 0.014989	Acc: 65.5% (5237/8000)
[Test]  Epoch: 44	Loss: 0.015067	Acc: 65.2% (5215/8000)
[Test]  Epoch: 45	Loss: 0.015023	Acc: 65.6% (5249/8000)
[Test]  Epoch: 46	Loss: 0.015038	Acc: 65.4% (5233/8000)
[Test]  Epoch: 47	Loss: 0.015084	Acc: 65.3% (5226/8000)
[Test]  Epoch: 48	Loss: 0.015043	Acc: 65.6% (5251/8000)
[Test]  Epoch: 49	Loss: 0.014946	Acc: 65.8% (5265/8000)
[Test]  Epoch: 50	Loss: 0.014979	Acc: 65.5% (5242/8000)
[Test]  Epoch: 51	Loss: 0.014993	Acc: 65.9% (5270/8000)
[Test]  Epoch: 52	Loss: 0.014968	Acc: 65.8% (5264/8000)
[Test]  Epoch: 53	Loss: 0.014998	Acc: 65.5% (5244/8000)
[Test]  Epoch: 54	Loss: 0.014966	Acc: 65.7% (5257/8000)
[Test]  Epoch: 55	Loss: 0.015048	Acc: 65.4% (5235/8000)
[Test]  Epoch: 56	Loss: 0.015037	Acc: 65.6% (5250/8000)
[Test]  Epoch: 57	Loss: 0.014932	Acc: 65.6% (5248/8000)
[Test]  Epoch: 58	Loss: 0.014973	Acc: 65.8% (5264/8000)
[Test]  Epoch: 59	Loss: 0.015001	Acc: 65.4% (5235/8000)
[Test]  Epoch: 60	Loss: 0.014981	Acc: 65.6% (5251/8000)
[Test]  Epoch: 61	Loss: 0.014976	Acc: 65.6% (5250/8000)
[Test]  Epoch: 62	Loss: 0.014970	Acc: 65.7% (5257/8000)
[Test]  Epoch: 63	Loss: 0.014949	Acc: 65.6% (5247/8000)
[Test]  Epoch: 64	Loss: 0.014940	Acc: 65.7% (5252/8000)
[Test]  Epoch: 65	Loss: 0.014942	Acc: 65.7% (5252/8000)
[Test]  Epoch: 66	Loss: 0.014949	Acc: 65.7% (5258/8000)
[Test]  Epoch: 67	Loss: 0.014953	Acc: 65.7% (5253/8000)
[Test]  Epoch: 68	Loss: 0.014963	Acc: 65.7% (5254/8000)
[Test]  Epoch: 69	Loss: 0.014933	Acc: 65.6% (5249/8000)
[Test]  Epoch: 70	Loss: 0.014951	Acc: 65.6% (5249/8000)
[Test]  Epoch: 71	Loss: 0.014954	Acc: 65.7% (5258/8000)
[Test]  Epoch: 72	Loss: 0.014957	Acc: 65.5% (5243/8000)
[Test]  Epoch: 73	Loss: 0.014953	Acc: 65.5% (5236/8000)
[Test]  Epoch: 74	Loss: 0.014946	Acc: 65.5% (5240/8000)
[Test]  Epoch: 75	Loss: 0.014946	Acc: 65.6% (5246/8000)
[Test]  Epoch: 76	Loss: 0.014951	Acc: 65.6% (5248/8000)
[Test]  Epoch: 77	Loss: 0.014940	Acc: 65.5% (5240/8000)
[Test]  Epoch: 78	Loss: 0.014947	Acc: 65.5% (5244/8000)
[Test]  Epoch: 79	Loss: 0.014946	Acc: 65.8% (5263/8000)
[Test]  Epoch: 80	Loss: 0.014928	Acc: 65.6% (5245/8000)
[Test]  Epoch: 81	Loss: 0.014936	Acc: 65.7% (5259/8000)
[Test]  Epoch: 82	Loss: 0.014989	Acc: 65.6% (5248/8000)
[Test]  Epoch: 83	Loss: 0.014934	Acc: 65.7% (5257/8000)
[Test]  Epoch: 84	Loss: 0.014925	Acc: 65.8% (5263/8000)
[Test]  Epoch: 85	Loss: 0.014917	Acc: 65.7% (5252/8000)
[Test]  Epoch: 86	Loss: 0.014942	Acc: 65.8% (5262/8000)
[Test]  Epoch: 87	Loss: 0.014944	Acc: 65.6% (5251/8000)
[Test]  Epoch: 88	Loss: 0.014922	Acc: 65.8% (5260/8000)
[Test]  Epoch: 89	Loss: 0.014942	Acc: 65.6% (5246/8000)
[Test]  Epoch: 90	Loss: 0.014939	Acc: 65.7% (5256/8000)
[Test]  Epoch: 91	Loss: 0.014945	Acc: 65.5% (5242/8000)
[Test]  Epoch: 92	Loss: 0.014936	Acc: 65.7% (5259/8000)
[Test]  Epoch: 93	Loss: 0.014911	Acc: 65.8% (5264/8000)
[Test]  Epoch: 94	Loss: 0.014918	Acc: 65.8% (5263/8000)
[Test]  Epoch: 95	Loss: 0.014932	Acc: 65.6% (5246/8000)
[Test]  Epoch: 96	Loss: 0.014926	Acc: 65.6% (5247/8000)
[Test]  Epoch: 97	Loss: 0.014948	Acc: 65.6% (5249/8000)
[Test]  Epoch: 98	Loss: 0.014939	Acc: 65.7% (5254/8000)
[Test]  Epoch: 99	Loss: 0.014928	Acc: 65.7% (5258/8000)
[Test]  Epoch: 100	Loss: 0.014930	Acc: 65.8% (5263/8000)
===========finish==========
['2024-08-19', '19:47:40.741491', '100', 'test', '0.01492988134920597', '65.7875', '65.875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039359	Acc: 19.0% (1520/8000)
[Test]  Epoch: 2	Loss: 0.025897	Acc: 39.4% (3151/8000)
[Test]  Epoch: 3	Loss: 0.019200	Acc: 55.3% (4424/8000)
[Test]  Epoch: 4	Loss: 0.018081	Acc: 58.2% (4657/8000)
[Test]  Epoch: 5	Loss: 0.017526	Acc: 60.0% (4797/8000)
[Test]  Epoch: 6	Loss: 0.017034	Acc: 60.7% (4854/8000)
[Test]  Epoch: 7	Loss: 0.016982	Acc: 61.1% (4891/8000)
[Test]  Epoch: 8	Loss: 0.016746	Acc: 61.2% (4893/8000)
[Test]  Epoch: 9	Loss: 0.016754	Acc: 61.7% (4937/8000)
[Test]  Epoch: 10	Loss: 0.016679	Acc: 61.7% (4939/8000)
[Test]  Epoch: 11	Loss: 0.016736	Acc: 61.8% (4942/8000)
[Test]  Epoch: 12	Loss: 0.016625	Acc: 61.6% (4925/8000)
[Test]  Epoch: 13	Loss: 0.016510	Acc: 61.9% (4955/8000)
[Test]  Epoch: 14	Loss: 0.016585	Acc: 61.8% (4946/8000)
[Test]  Epoch: 15	Loss: 0.016459	Acc: 62.0% (4959/8000)
[Test]  Epoch: 16	Loss: 0.016400	Acc: 62.1% (4972/8000)
[Test]  Epoch: 17	Loss: 0.016310	Acc: 62.3% (4986/8000)
[Test]  Epoch: 18	Loss: 0.016365	Acc: 62.4% (4994/8000)
[Test]  Epoch: 19	Loss: 0.016294	Acc: 62.6% (5008/8000)
[Test]  Epoch: 20	Loss: 0.016451	Acc: 62.1% (4972/8000)
[Test]  Epoch: 21	Loss: 0.016319	Acc: 62.3% (4983/8000)
[Test]  Epoch: 22	Loss: 0.016250	Acc: 62.4% (4994/8000)
[Test]  Epoch: 23	Loss: 0.016240	Acc: 62.3% (4981/8000)
[Test]  Epoch: 24	Loss: 0.016196	Acc: 62.4% (4995/8000)
[Test]  Epoch: 25	Loss: 0.016291	Acc: 62.8% (5021/8000)
[Test]  Epoch: 26	Loss: 0.016133	Acc: 62.5% (5004/8000)
[Test]  Epoch: 27	Loss: 0.016120	Acc: 62.6% (5005/8000)
[Test]  Epoch: 28	Loss: 0.016115	Acc: 62.9% (5031/8000)
[Test]  Epoch: 29	Loss: 0.016095	Acc: 62.9% (5028/8000)
[Test]  Epoch: 30	Loss: 0.016152	Acc: 63.0% (5038/8000)
[Test]  Epoch: 31	Loss: 0.016104	Acc: 63.0% (5039/8000)
[Test]  Epoch: 32	Loss: 0.016048	Acc: 62.9% (5029/8000)
[Test]  Epoch: 33	Loss: 0.016074	Acc: 62.8% (5026/8000)
[Test]  Epoch: 34	Loss: 0.016013	Acc: 62.9% (5028/8000)
[Test]  Epoch: 35	Loss: 0.016052	Acc: 63.1% (5048/8000)
[Test]  Epoch: 36	Loss: 0.016027	Acc: 63.2% (5060/8000)
[Test]  Epoch: 37	Loss: 0.016019	Acc: 63.0% (5041/8000)
[Test]  Epoch: 38	Loss: 0.015988	Acc: 62.8% (5022/8000)
[Test]  Epoch: 39	Loss: 0.015961	Acc: 63.0% (5043/8000)
[Test]  Epoch: 40	Loss: 0.016053	Acc: 63.2% (5053/8000)
[Test]  Epoch: 41	Loss: 0.015956	Acc: 63.1% (5049/8000)
[Test]  Epoch: 42	Loss: 0.015925	Acc: 63.1% (5049/8000)
[Test]  Epoch: 43	Loss: 0.015920	Acc: 63.4% (5072/8000)
[Test]  Epoch: 44	Loss: 0.015929	Acc: 63.2% (5054/8000)
[Test]  Epoch: 45	Loss: 0.015949	Acc: 63.4% (5070/8000)
[Test]  Epoch: 46	Loss: 0.015970	Acc: 63.2% (5059/8000)
[Test]  Epoch: 47	Loss: 0.015912	Acc: 63.3% (5063/8000)
[Test]  Epoch: 48	Loss: 0.015904	Acc: 63.3% (5063/8000)
[Test]  Epoch: 49	Loss: 0.015894	Acc: 63.0% (5044/8000)
[Test]  Epoch: 50	Loss: 0.015944	Acc: 63.2% (5055/8000)
[Test]  Epoch: 51	Loss: 0.015962	Acc: 63.1% (5048/8000)
[Test]  Epoch: 52	Loss: 0.015928	Acc: 62.8% (5025/8000)
[Test]  Epoch: 53	Loss: 0.015959	Acc: 63.2% (5058/8000)
[Test]  Epoch: 54	Loss: 0.015911	Acc: 63.6% (5089/8000)
[Test]  Epoch: 55	Loss: 0.015876	Acc: 63.6% (5088/8000)
[Test]  Epoch: 56	Loss: 0.015810	Acc: 63.7% (5097/8000)
[Test]  Epoch: 57	Loss: 0.015831	Acc: 63.4% (5074/8000)
[Test]  Epoch: 58	Loss: 0.015843	Acc: 63.5% (5077/8000)
[Test]  Epoch: 59	Loss: 0.015938	Acc: 63.3% (5065/8000)
[Test]  Epoch: 60	Loss: 0.015873	Acc: 63.3% (5064/8000)
[Test]  Epoch: 61	Loss: 0.015880	Acc: 63.4% (5072/8000)
[Test]  Epoch: 62	Loss: 0.015860	Acc: 63.4% (5075/8000)
[Test]  Epoch: 63	Loss: 0.015838	Acc: 63.4% (5075/8000)
[Test]  Epoch: 64	Loss: 0.015846	Acc: 63.4% (5072/8000)
[Test]  Epoch: 65	Loss: 0.015832	Acc: 63.5% (5082/8000)
[Test]  Epoch: 66	Loss: 0.015828	Acc: 63.7% (5093/8000)
[Test]  Epoch: 67	Loss: 0.015824	Acc: 63.3% (5066/8000)
[Test]  Epoch: 68	Loss: 0.015813	Acc: 63.5% (5084/8000)
[Test]  Epoch: 69	Loss: 0.015810	Acc: 63.6% (5088/8000)
[Test]  Epoch: 70	Loss: 0.015807	Acc: 63.7% (5093/8000)
[Test]  Epoch: 71	Loss: 0.015814	Acc: 63.5% (5081/8000)
[Test]  Epoch: 72	Loss: 0.015803	Acc: 63.7% (5095/8000)
[Test]  Epoch: 73	Loss: 0.015801	Acc: 63.6% (5089/8000)
[Test]  Epoch: 74	Loss: 0.015815	Acc: 63.5% (5084/8000)
[Test]  Epoch: 75	Loss: 0.015801	Acc: 63.6% (5091/8000)
[Test]  Epoch: 76	Loss: 0.015810	Acc: 63.4% (5074/8000)
[Test]  Epoch: 77	Loss: 0.015822	Acc: 63.5% (5081/8000)
[Test]  Epoch: 78	Loss: 0.015840	Acc: 63.4% (5073/8000)
[Test]  Epoch: 79	Loss: 0.015817	Acc: 63.4% (5074/8000)
[Test]  Epoch: 80	Loss: 0.015800	Acc: 63.5% (5083/8000)
[Test]  Epoch: 81	Loss: 0.015802	Acc: 63.6% (5091/8000)
[Test]  Epoch: 82	Loss: 0.015864	Acc: 63.4% (5071/8000)
[Test]  Epoch: 83	Loss: 0.015798	Acc: 63.5% (5082/8000)
[Test]  Epoch: 84	Loss: 0.015795	Acc: 63.6% (5088/8000)
[Test]  Epoch: 85	Loss: 0.015812	Acc: 63.5% (5081/8000)
[Test]  Epoch: 86	Loss: 0.015820	Acc: 63.6% (5085/8000)
[Test]  Epoch: 87	Loss: 0.015796	Acc: 63.5% (5076/8000)
[Test]  Epoch: 88	Loss: 0.015796	Acc: 63.6% (5087/8000)
[Test]  Epoch: 89	Loss: 0.015818	Acc: 63.5% (5083/8000)
[Test]  Epoch: 90	Loss: 0.015832	Acc: 63.6% (5092/8000)
[Test]  Epoch: 91	Loss: 0.015803	Acc: 63.7% (5093/8000)
[Test]  Epoch: 92	Loss: 0.015811	Acc: 63.6% (5090/8000)
[Test]  Epoch: 93	Loss: 0.015777	Acc: 63.6% (5090/8000)
[Test]  Epoch: 94	Loss: 0.015803	Acc: 63.5% (5082/8000)
[Test]  Epoch: 95	Loss: 0.015818	Acc: 63.7% (5098/8000)
[Test]  Epoch: 96	Loss: 0.015782	Acc: 63.8% (5106/8000)
[Test]  Epoch: 97	Loss: 0.015810	Acc: 63.5% (5084/8000)
[Test]  Epoch: 98	Loss: 0.015772	Acc: 63.7% (5093/8000)
[Test]  Epoch: 99	Loss: 0.015796	Acc: 63.5% (5083/8000)
[Test]  Epoch: 100	Loss: 0.015805	Acc: 63.5% (5084/8000)
===========finish==========
['2024-08-19', '19:57:09.933822', '100', 'test', '0.015805151812732218', '63.55', '63.825']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.056174	Acc: 10.9% (875/8000)
[Test]  Epoch: 2	Loss: 0.025979	Acc: 38.0% (3041/8000)
[Test]  Epoch: 3	Loss: 0.018510	Acc: 57.0% (4560/8000)
[Test]  Epoch: 4	Loss: 0.017162	Acc: 60.6% (4846/8000)
[Test]  Epoch: 5	Loss: 0.016927	Acc: 61.1% (4890/8000)
[Test]  Epoch: 6	Loss: 0.016160	Acc: 62.7% (5013/8000)
[Test]  Epoch: 7	Loss: 0.016179	Acc: 62.6% (5007/8000)
[Test]  Epoch: 8	Loss: 0.016048	Acc: 62.8% (5021/8000)
[Test]  Epoch: 9	Loss: 0.016149	Acc: 63.1% (5048/8000)
[Test]  Epoch: 10	Loss: 0.016128	Acc: 62.9% (5034/8000)
[Test]  Epoch: 11	Loss: 0.016139	Acc: 62.6% (5009/8000)
[Test]  Epoch: 12	Loss: 0.016016	Acc: 63.0% (5036/8000)
[Test]  Epoch: 13	Loss: 0.015997	Acc: 63.4% (5072/8000)
[Test]  Epoch: 14	Loss: 0.015981	Acc: 63.1% (5050/8000)
[Test]  Epoch: 15	Loss: 0.015932	Acc: 63.2% (5059/8000)
[Test]  Epoch: 16	Loss: 0.015862	Acc: 63.3% (5067/8000)
[Test]  Epoch: 17	Loss: 0.015856	Acc: 63.2% (5060/8000)
[Test]  Epoch: 18	Loss: 0.015890	Acc: 63.3% (5065/8000)
[Test]  Epoch: 19	Loss: 0.015916	Acc: 63.3% (5066/8000)
[Test]  Epoch: 20	Loss: 0.015853	Acc: 63.2% (5055/8000)
[Test]  Epoch: 21	Loss: 0.015818	Acc: 63.5% (5077/8000)
[Test]  Epoch: 22	Loss: 0.015741	Acc: 63.5% (5081/8000)
[Test]  Epoch: 23	Loss: 0.015728	Acc: 63.7% (5097/8000)
[Test]  Epoch: 24	Loss: 0.015751	Acc: 63.6% (5086/8000)
[Test]  Epoch: 25	Loss: 0.015705	Acc: 63.7% (5096/8000)
[Test]  Epoch: 26	Loss: 0.015724	Acc: 63.8% (5104/8000)
[Test]  Epoch: 27	Loss: 0.015739	Acc: 63.4% (5071/8000)
[Test]  Epoch: 28	Loss: 0.015716	Acc: 63.3% (5063/8000)
[Test]  Epoch: 29	Loss: 0.015633	Acc: 63.8% (5102/8000)
[Test]  Epoch: 30	Loss: 0.015710	Acc: 63.7% (5098/8000)
[Test]  Epoch: 31	Loss: 0.015651	Acc: 63.7% (5095/8000)
[Test]  Epoch: 32	Loss: 0.015652	Acc: 63.9% (5115/8000)
[Test]  Epoch: 33	Loss: 0.015661	Acc: 63.8% (5104/8000)
[Test]  Epoch: 34	Loss: 0.015620	Acc: 63.8% (5105/8000)
[Test]  Epoch: 35	Loss: 0.015679	Acc: 63.9% (5110/8000)
[Test]  Epoch: 36	Loss: 0.015676	Acc: 63.8% (5103/8000)
[Test]  Epoch: 37	Loss: 0.015573	Acc: 63.8% (5101/8000)
[Test]  Epoch: 38	Loss: 0.015585	Acc: 63.8% (5101/8000)
[Test]  Epoch: 39	Loss: 0.015577	Acc: 63.8% (5105/8000)
[Test]  Epoch: 40	Loss: 0.015804	Acc: 63.4% (5071/8000)
[Test]  Epoch: 41	Loss: 0.015579	Acc: 64.1% (5131/8000)
[Test]  Epoch: 42	Loss: 0.015520	Acc: 63.8% (5104/8000)
[Test]  Epoch: 43	Loss: 0.015486	Acc: 64.1% (5126/8000)
[Test]  Epoch: 44	Loss: 0.015465	Acc: 64.4% (5150/8000)
[Test]  Epoch: 45	Loss: 0.015522	Acc: 63.9% (5113/8000)
[Test]  Epoch: 46	Loss: 0.015615	Acc: 64.0% (5118/8000)
[Test]  Epoch: 47	Loss: 0.015516	Acc: 64.1% (5131/8000)
[Test]  Epoch: 48	Loss: 0.015507	Acc: 64.2% (5137/8000)
[Test]  Epoch: 49	Loss: 0.015532	Acc: 64.0% (5117/8000)
[Test]  Epoch: 50	Loss: 0.015509	Acc: 64.1% (5128/8000)
[Test]  Epoch: 51	Loss: 0.015500	Acc: 64.1% (5129/8000)
[Test]  Epoch: 52	Loss: 0.015472	Acc: 64.2% (5133/8000)
[Test]  Epoch: 53	Loss: 0.015556	Acc: 64.1% (5129/8000)
[Test]  Epoch: 54	Loss: 0.015502	Acc: 64.3% (5148/8000)
[Test]  Epoch: 55	Loss: 0.015462	Acc: 64.3% (5144/8000)
[Test]  Epoch: 56	Loss: 0.015416	Acc: 64.3% (5146/8000)
[Test]  Epoch: 57	Loss: 0.015383	Acc: 64.5% (5160/8000)
[Test]  Epoch: 58	Loss: 0.015447	Acc: 64.3% (5145/8000)
[Test]  Epoch: 59	Loss: 0.015471	Acc: 64.4% (5152/8000)
[Test]  Epoch: 60	Loss: 0.015468	Acc: 64.2% (5139/8000)
[Test]  Epoch: 61	Loss: 0.015442	Acc: 64.1% (5131/8000)
[Test]  Epoch: 62	Loss: 0.015425	Acc: 64.4% (5150/8000)
[Test]  Epoch: 63	Loss: 0.015407	Acc: 64.3% (5145/8000)
[Test]  Epoch: 64	Loss: 0.015416	Acc: 64.3% (5146/8000)
[Test]  Epoch: 65	Loss: 0.015437	Acc: 64.2% (5136/8000)
[Test]  Epoch: 66	Loss: 0.015424	Acc: 64.4% (5152/8000)
[Test]  Epoch: 67	Loss: 0.015448	Acc: 64.3% (5145/8000)
[Test]  Epoch: 68	Loss: 0.015435	Acc: 64.3% (5145/8000)
[Test]  Epoch: 69	Loss: 0.015399	Acc: 64.4% (5150/8000)
[Test]  Epoch: 70	Loss: 0.015416	Acc: 64.4% (5155/8000)
[Test]  Epoch: 71	Loss: 0.015427	Acc: 64.4% (5151/8000)
[Test]  Epoch: 72	Loss: 0.015429	Acc: 64.4% (5150/8000)
[Test]  Epoch: 73	Loss: 0.015414	Acc: 64.5% (5159/8000)
[Test]  Epoch: 74	Loss: 0.015423	Acc: 64.2% (5136/8000)
[Test]  Epoch: 75	Loss: 0.015421	Acc: 64.4% (5152/8000)
[Test]  Epoch: 76	Loss: 0.015436	Acc: 64.4% (5153/8000)
[Test]  Epoch: 77	Loss: 0.015421	Acc: 64.3% (5143/8000)
[Test]  Epoch: 78	Loss: 0.015433	Acc: 64.3% (5147/8000)
[Test]  Epoch: 79	Loss: 0.015419	Acc: 64.3% (5147/8000)
[Test]  Epoch: 80	Loss: 0.015416	Acc: 64.4% (5150/8000)
[Test]  Epoch: 81	Loss: 0.015405	Acc: 64.3% (5144/8000)
[Test]  Epoch: 82	Loss: 0.015457	Acc: 64.2% (5134/8000)
[Test]  Epoch: 83	Loss: 0.015414	Acc: 64.5% (5156/8000)
[Test]  Epoch: 84	Loss: 0.015431	Acc: 64.4% (5154/8000)
[Test]  Epoch: 85	Loss: 0.015419	Acc: 64.5% (5158/8000)
[Test]  Epoch: 86	Loss: 0.015440	Acc: 64.2% (5138/8000)
[Test]  Epoch: 87	Loss: 0.015415	Acc: 64.3% (5144/8000)
[Test]  Epoch: 88	Loss: 0.015412	Acc: 64.3% (5148/8000)
[Test]  Epoch: 89	Loss: 0.015434	Acc: 64.5% (5157/8000)
[Test]  Epoch: 90	Loss: 0.015424	Acc: 64.1% (5131/8000)
[Test]  Epoch: 91	Loss: 0.015419	Acc: 64.5% (5158/8000)
[Test]  Epoch: 92	Loss: 0.015418	Acc: 64.3% (5143/8000)
[Test]  Epoch: 93	Loss: 0.015401	Acc: 64.2% (5140/8000)
[Test]  Epoch: 94	Loss: 0.015434	Acc: 64.2% (5135/8000)
[Test]  Epoch: 95	Loss: 0.015426	Acc: 64.4% (5151/8000)
[Test]  Epoch: 96	Loss: 0.015398	Acc: 64.4% (5155/8000)
[Test]  Epoch: 97	Loss: 0.015418	Acc: 64.4% (5152/8000)
[Test]  Epoch: 98	Loss: 0.015419	Acc: 64.4% (5154/8000)
[Test]  Epoch: 99	Loss: 0.015412	Acc: 64.3% (5143/8000)
[Test]  Epoch: 100	Loss: 0.015403	Acc: 64.3% (5148/8000)
===========finish==========
['2024-08-19', '20:06:40.861970', '100', 'test', '0.015403083540499211', '64.35', '64.5']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.050446	Acc: 17.9% (1434/8000)
[Test]  Epoch: 2	Loss: 0.029849	Acc: 31.8% (2545/8000)
[Test]  Epoch: 3	Loss: 0.018248	Acc: 56.7% (4533/8000)
[Test]  Epoch: 4	Loss: 0.016806	Acc: 60.2% (4814/8000)
[Test]  Epoch: 5	Loss: 0.016727	Acc: 61.0% (4882/8000)
[Test]  Epoch: 6	Loss: 0.016526	Acc: 61.3% (4907/8000)
[Test]  Epoch: 7	Loss: 0.016328	Acc: 61.9% (4952/8000)
[Test]  Epoch: 8	Loss: 0.016303	Acc: 62.0% (4958/8000)
[Test]  Epoch: 9	Loss: 0.016310	Acc: 61.8% (4944/8000)
[Test]  Epoch: 10	Loss: 0.016317	Acc: 62.2% (4975/8000)
[Test]  Epoch: 11	Loss: 0.016248	Acc: 61.9% (4955/8000)
[Test]  Epoch: 12	Loss: 0.016242	Acc: 62.0% (4963/8000)
[Test]  Epoch: 13	Loss: 0.016219	Acc: 62.4% (4992/8000)
[Test]  Epoch: 14	Loss: 0.016157	Acc: 62.4% (4988/8000)
[Test]  Epoch: 15	Loss: 0.016147	Acc: 62.5% (5000/8000)
[Test]  Epoch: 16	Loss: 0.016062	Acc: 62.7% (5013/8000)
[Test]  Epoch: 17	Loss: 0.016054	Acc: 62.5% (5003/8000)
[Test]  Epoch: 18	Loss: 0.016018	Acc: 63.0% (5043/8000)
[Test]  Epoch: 19	Loss: 0.016078	Acc: 62.9% (5032/8000)
[Test]  Epoch: 20	Loss: 0.016061	Acc: 62.7% (5017/8000)
[Test]  Epoch: 21	Loss: 0.016006	Acc: 62.5% (5001/8000)
[Test]  Epoch: 22	Loss: 0.015911	Acc: 63.0% (5041/8000)
[Test]  Epoch: 23	Loss: 0.015889	Acc: 63.3% (5066/8000)
[Test]  Epoch: 24	Loss: 0.015890	Acc: 63.1% (5049/8000)
[Test]  Epoch: 25	Loss: 0.015989	Acc: 63.3% (5062/8000)
[Test]  Epoch: 26	Loss: 0.015820	Acc: 63.4% (5073/8000)
[Test]  Epoch: 27	Loss: 0.015846	Acc: 63.2% (5055/8000)
[Test]  Epoch: 28	Loss: 0.015888	Acc: 63.5% (5078/8000)
[Test]  Epoch: 29	Loss: 0.015795	Acc: 63.7% (5094/8000)
[Test]  Epoch: 30	Loss: 0.015825	Acc: 63.2% (5059/8000)
[Test]  Epoch: 31	Loss: 0.015730	Acc: 63.9% (5108/8000)
[Test]  Epoch: 32	Loss: 0.015811	Acc: 63.4% (5075/8000)
[Test]  Epoch: 33	Loss: 0.015730	Acc: 63.6% (5092/8000)
[Test]  Epoch: 34	Loss: 0.015759	Acc: 63.4% (5074/8000)
[Test]  Epoch: 35	Loss: 0.015708	Acc: 63.5% (5080/8000)
[Test]  Epoch: 36	Loss: 0.015692	Acc: 63.5% (5078/8000)
[Test]  Epoch: 37	Loss: 0.015689	Acc: 63.7% (5098/8000)
[Test]  Epoch: 38	Loss: 0.015683	Acc: 63.5% (5084/8000)
[Test]  Epoch: 39	Loss: 0.015691	Acc: 63.6% (5089/8000)
[Test]  Epoch: 40	Loss: 0.015693	Acc: 63.7% (5096/8000)
[Test]  Epoch: 41	Loss: 0.015658	Acc: 63.7% (5097/8000)
[Test]  Epoch: 42	Loss: 0.015654	Acc: 63.8% (5101/8000)
[Test]  Epoch: 43	Loss: 0.015671	Acc: 63.7% (5098/8000)
[Test]  Epoch: 44	Loss: 0.015650	Acc: 63.9% (5113/8000)
[Test]  Epoch: 45	Loss: 0.015626	Acc: 63.9% (5113/8000)
[Test]  Epoch: 46	Loss: 0.015630	Acc: 63.8% (5101/8000)
[Test]  Epoch: 47	Loss: 0.015583	Acc: 64.0% (5119/8000)
[Test]  Epoch: 48	Loss: 0.015634	Acc: 63.8% (5107/8000)
[Test]  Epoch: 49	Loss: 0.015542	Acc: 64.2% (5140/8000)
[Test]  Epoch: 50	Loss: 0.015547	Acc: 64.2% (5133/8000)
[Test]  Epoch: 51	Loss: 0.015556	Acc: 64.2% (5133/8000)
[Test]  Epoch: 52	Loss: 0.015553	Acc: 64.1% (5129/8000)
[Test]  Epoch: 53	Loss: 0.015595	Acc: 63.8% (5106/8000)
[Test]  Epoch: 54	Loss: 0.015569	Acc: 64.1% (5127/8000)
[Test]  Epoch: 55	Loss: 0.015545	Acc: 64.2% (5137/8000)
[Test]  Epoch: 56	Loss: 0.015528	Acc: 64.3% (5141/8000)
[Test]  Epoch: 57	Loss: 0.015538	Acc: 64.1% (5126/8000)
[Test]  Epoch: 58	Loss: 0.015586	Acc: 64.1% (5125/8000)
[Test]  Epoch: 59	Loss: 0.015578	Acc: 64.2% (5139/8000)
[Test]  Epoch: 60	Loss: 0.015555	Acc: 64.2% (5138/8000)
[Test]  Epoch: 61	Loss: 0.015549	Acc: 64.3% (5141/8000)
[Test]  Epoch: 62	Loss: 0.015542	Acc: 64.3% (5144/8000)
[Test]  Epoch: 63	Loss: 0.015508	Acc: 64.3% (5141/8000)
[Test]  Epoch: 64	Loss: 0.015505	Acc: 64.4% (5152/8000)
[Test]  Epoch: 65	Loss: 0.015536	Acc: 64.2% (5140/8000)
[Test]  Epoch: 66	Loss: 0.015528	Acc: 64.4% (5152/8000)
[Test]  Epoch: 67	Loss: 0.015520	Acc: 64.3% (5142/8000)
[Test]  Epoch: 68	Loss: 0.015532	Acc: 64.2% (5137/8000)
[Test]  Epoch: 69	Loss: 0.015495	Acc: 64.4% (5152/8000)
[Test]  Epoch: 70	Loss: 0.015517	Acc: 64.4% (5152/8000)
[Test]  Epoch: 71	Loss: 0.015518	Acc: 64.4% (5153/8000)
[Test]  Epoch: 72	Loss: 0.015499	Acc: 64.4% (5155/8000)
[Test]  Epoch: 73	Loss: 0.015503	Acc: 64.3% (5147/8000)
[Test]  Epoch: 74	Loss: 0.015520	Acc: 64.2% (5139/8000)
[Test]  Epoch: 75	Loss: 0.015509	Acc: 64.4% (5154/8000)
[Test]  Epoch: 76	Loss: 0.015521	Acc: 64.3% (5143/8000)
[Test]  Epoch: 77	Loss: 0.015505	Acc: 64.4% (5153/8000)
[Test]  Epoch: 78	Loss: 0.015524	Acc: 64.4% (5149/8000)
[Test]  Epoch: 79	Loss: 0.015499	Acc: 64.3% (5146/8000)
[Test]  Epoch: 80	Loss: 0.015497	Acc: 64.4% (5150/8000)
[Test]  Epoch: 81	Loss: 0.015494	Acc: 64.1% (5129/8000)
[Test]  Epoch: 82	Loss: 0.015575	Acc: 64.0% (5124/8000)
[Test]  Epoch: 83	Loss: 0.015493	Acc: 64.4% (5150/8000)
[Test]  Epoch: 84	Loss: 0.015517	Acc: 64.4% (5150/8000)
[Test]  Epoch: 85	Loss: 0.015503	Acc: 64.2% (5140/8000)
[Test]  Epoch: 86	Loss: 0.015507	Acc: 64.5% (5161/8000)
[Test]  Epoch: 87	Loss: 0.015512	Acc: 64.3% (5146/8000)
[Test]  Epoch: 88	Loss: 0.015502	Acc: 64.5% (5161/8000)
[Test]  Epoch: 89	Loss: 0.015516	Acc: 64.3% (5145/8000)
[Test]  Epoch: 90	Loss: 0.015522	Acc: 64.3% (5148/8000)
[Test]  Epoch: 91	Loss: 0.015522	Acc: 64.5% (5156/8000)
[Test]  Epoch: 92	Loss: 0.015522	Acc: 64.4% (5155/8000)
[Test]  Epoch: 93	Loss: 0.015513	Acc: 64.4% (5154/8000)
[Test]  Epoch: 94	Loss: 0.015535	Acc: 64.4% (5150/8000)
[Test]  Epoch: 95	Loss: 0.015525	Acc: 64.3% (5147/8000)
[Test]  Epoch: 96	Loss: 0.015484	Acc: 64.4% (5150/8000)
[Test]  Epoch: 97	Loss: 0.015522	Acc: 64.4% (5153/8000)
[Test]  Epoch: 98	Loss: 0.015499	Acc: 64.5% (5161/8000)
[Test]  Epoch: 99	Loss: 0.015513	Acc: 64.4% (5151/8000)
[Test]  Epoch: 100	Loss: 0.015520	Acc: 64.4% (5149/8000)
===========finish==========
['2024-08-19', '20:16:01.948297', '100', 'test', '0.015519633568823337', '64.3625', '64.5125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042595	Acc: 10.2% (815/8000)
[Test]  Epoch: 2	Loss: 0.033656	Acc: 24.6% (1965/8000)
[Test]  Epoch: 3	Loss: 0.017093	Acc: 59.4% (4753/8000)
[Test]  Epoch: 4	Loss: 0.015390	Acc: 64.1% (5127/8000)
[Test]  Epoch: 5	Loss: 0.015364	Acc: 64.1% (5131/8000)
[Test]  Epoch: 6	Loss: 0.015046	Acc: 65.3% (5226/8000)
[Test]  Epoch: 7	Loss: 0.015042	Acc: 64.8% (5181/8000)
[Test]  Epoch: 8	Loss: 0.014845	Acc: 65.5% (5243/8000)
[Test]  Epoch: 9	Loss: 0.014960	Acc: 65.2% (5212/8000)
[Test]  Epoch: 10	Loss: 0.014760	Acc: 65.4% (5235/8000)
[Test]  Epoch: 11	Loss: 0.014775	Acc: 65.5% (5243/8000)
[Test]  Epoch: 12	Loss: 0.014747	Acc: 65.6% (5246/8000)
[Test]  Epoch: 13	Loss: 0.014750	Acc: 65.7% (5257/8000)
[Test]  Epoch: 14	Loss: 0.014625	Acc: 65.9% (5270/8000)
[Test]  Epoch: 15	Loss: 0.014698	Acc: 65.9% (5270/8000)
[Test]  Epoch: 16	Loss: 0.014530	Acc: 66.5% (5318/8000)
[Test]  Epoch: 17	Loss: 0.014567	Acc: 66.0% (5283/8000)
[Test]  Epoch: 18	Loss: 0.014474	Acc: 66.5% (5316/8000)
[Test]  Epoch: 19	Loss: 0.014570	Acc: 66.2% (5296/8000)
[Test]  Epoch: 20	Loss: 0.014581	Acc: 65.8% (5260/8000)
[Test]  Epoch: 21	Loss: 0.014463	Acc: 66.6% (5328/8000)
[Test]  Epoch: 22	Loss: 0.014540	Acc: 65.9% (5272/8000)
[Test]  Epoch: 23	Loss: 0.014451	Acc: 66.7% (5337/8000)
[Test]  Epoch: 24	Loss: 0.014463	Acc: 66.2% (5299/8000)
[Test]  Epoch: 25	Loss: 0.014416	Acc: 66.6% (5326/8000)
[Test]  Epoch: 26	Loss: 0.014414	Acc: 66.5% (5318/8000)
[Test]  Epoch: 27	Loss: 0.014398	Acc: 66.5% (5319/8000)
[Test]  Epoch: 28	Loss: 0.014349	Acc: 66.6% (5331/8000)
[Test]  Epoch: 29	Loss: 0.014333	Acc: 66.8% (5340/8000)
[Test]  Epoch: 30	Loss: 0.014347	Acc: 66.9% (5350/8000)
[Test]  Epoch: 31	Loss: 0.014375	Acc: 66.5% (5316/8000)
[Test]  Epoch: 32	Loss: 0.014315	Acc: 67.1% (5369/8000)
[Test]  Epoch: 33	Loss: 0.014342	Acc: 67.1% (5367/8000)
[Test]  Epoch: 34	Loss: 0.014289	Acc: 66.8% (5345/8000)
[Test]  Epoch: 35	Loss: 0.014256	Acc: 67.0% (5362/8000)
[Test]  Epoch: 36	Loss: 0.014249	Acc: 66.9% (5355/8000)
[Test]  Epoch: 37	Loss: 0.014274	Acc: 66.8% (5340/8000)
[Test]  Epoch: 38	Loss: 0.014263	Acc: 67.0% (5358/8000)
[Test]  Epoch: 39	Loss: 0.014271	Acc: 67.2% (5375/8000)
[Test]  Epoch: 40	Loss: 0.014256	Acc: 67.0% (5358/8000)
[Test]  Epoch: 41	Loss: 0.014209	Acc: 67.2% (5375/8000)
[Test]  Epoch: 42	Loss: 0.014205	Acc: 67.1% (5367/8000)
[Test]  Epoch: 43	Loss: 0.014167	Acc: 67.3% (5383/8000)
[Test]  Epoch: 44	Loss: 0.014207	Acc: 67.2% (5372/8000)
[Test]  Epoch: 45	Loss: 0.014140	Acc: 67.1% (5370/8000)
[Test]  Epoch: 46	Loss: 0.014177	Acc: 67.2% (5374/8000)
[Test]  Epoch: 47	Loss: 0.014138	Acc: 67.6% (5408/8000)
[Test]  Epoch: 48	Loss: 0.014123	Acc: 67.3% (5384/8000)
[Test]  Epoch: 49	Loss: 0.014101	Acc: 67.5% (5396/8000)
[Test]  Epoch: 50	Loss: 0.014077	Acc: 67.2% (5378/8000)
[Test]  Epoch: 51	Loss: 0.014170	Acc: 67.2% (5372/8000)
[Test]  Epoch: 52	Loss: 0.014102	Acc: 67.5% (5399/8000)
[Test]  Epoch: 53	Loss: 0.014156	Acc: 67.2% (5378/8000)
[Test]  Epoch: 54	Loss: 0.014114	Acc: 67.6% (5411/8000)
[Test]  Epoch: 55	Loss: 0.014165	Acc: 67.3% (5381/8000)
[Test]  Epoch: 56	Loss: 0.014111	Acc: 67.3% (5384/8000)
[Test]  Epoch: 57	Loss: 0.014111	Acc: 67.5% (5398/8000)
[Test]  Epoch: 58	Loss: 0.014112	Acc: 67.5% (5397/8000)
[Test]  Epoch: 59	Loss: 0.014158	Acc: 67.5% (5396/8000)
[Test]  Epoch: 60	Loss: 0.014118	Acc: 67.5% (5396/8000)
[Test]  Epoch: 61	Loss: 0.014120	Acc: 67.5% (5400/8000)
[Test]  Epoch: 62	Loss: 0.014096	Acc: 67.5% (5396/8000)
[Test]  Epoch: 63	Loss: 0.014081	Acc: 67.5% (5400/8000)
[Test]  Epoch: 64	Loss: 0.014096	Acc: 67.5% (5396/8000)
[Test]  Epoch: 65	Loss: 0.014087	Acc: 67.6% (5410/8000)
[Test]  Epoch: 66	Loss: 0.014103	Acc: 67.6% (5410/8000)
[Test]  Epoch: 67	Loss: 0.014078	Acc: 67.7% (5419/8000)
[Test]  Epoch: 68	Loss: 0.014103	Acc: 67.5% (5400/8000)
[Test]  Epoch: 69	Loss: 0.014101	Acc: 67.5% (5397/8000)
[Test]  Epoch: 70	Loss: 0.014099	Acc: 67.5% (5400/8000)
[Test]  Epoch: 71	Loss: 0.014085	Acc: 67.6% (5405/8000)
[Test]  Epoch: 72	Loss: 0.014067	Acc: 67.7% (5416/8000)
[Test]  Epoch: 73	Loss: 0.014073	Acc: 67.7% (5414/8000)
[Test]  Epoch: 74	Loss: 0.014093	Acc: 67.5% (5404/8000)
[Test]  Epoch: 75	Loss: 0.014084	Acc: 67.5% (5396/8000)
[Test]  Epoch: 76	Loss: 0.014075	Acc: 67.7% (5414/8000)
[Test]  Epoch: 77	Loss: 0.014081	Acc: 67.5% (5404/8000)
[Test]  Epoch: 78	Loss: 0.014092	Acc: 67.5% (5399/8000)
[Test]  Epoch: 79	Loss: 0.014083	Acc: 67.6% (5406/8000)
[Test]  Epoch: 80	Loss: 0.014095	Acc: 67.6% (5405/8000)
[Test]  Epoch: 81	Loss: 0.014077	Acc: 67.6% (5408/8000)
[Test]  Epoch: 82	Loss: 0.014135	Acc: 67.7% (5415/8000)
[Test]  Epoch: 83	Loss: 0.014079	Acc: 67.6% (5411/8000)
[Test]  Epoch: 84	Loss: 0.014085	Acc: 67.6% (5407/8000)
[Test]  Epoch: 85	Loss: 0.014090	Acc: 67.5% (5403/8000)
[Test]  Epoch: 86	Loss: 0.014074	Acc: 67.6% (5411/8000)
[Test]  Epoch: 87	Loss: 0.014065	Acc: 67.6% (5407/8000)
[Test]  Epoch: 88	Loss: 0.014066	Acc: 67.7% (5415/8000)
[Test]  Epoch: 89	Loss: 0.014086	Acc: 67.6% (5407/8000)
[Test]  Epoch: 90	Loss: 0.014075	Acc: 67.5% (5404/8000)
[Test]  Epoch: 91	Loss: 0.014070	Acc: 67.6% (5409/8000)
[Test]  Epoch: 92	Loss: 0.014099	Acc: 67.7% (5412/8000)
[Test]  Epoch: 93	Loss: 0.014079	Acc: 67.6% (5407/8000)
[Test]  Epoch: 94	Loss: 0.014081	Acc: 67.5% (5403/8000)
[Test]  Epoch: 95	Loss: 0.014084	Acc: 67.7% (5418/8000)
[Test]  Epoch: 96	Loss: 0.014084	Acc: 67.7% (5416/8000)
[Test]  Epoch: 97	Loss: 0.014083	Acc: 67.7% (5412/8000)
[Test]  Epoch: 98	Loss: 0.014079	Acc: 67.6% (5409/8000)
[Test]  Epoch: 99	Loss: 0.014073	Acc: 67.6% (5405/8000)
[Test]  Epoch: 100	Loss: 0.014082	Acc: 67.6% (5411/8000)
===========finish==========
['2024-08-19', '20:25:35.797594', '100', 'test', '0.014081959642469883', '67.6375', '67.7375']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.046264	Acc: 11.9% (955/8000)
[Test]  Epoch: 2	Loss: 0.028895	Acc: 34.6% (2768/8000)
[Test]  Epoch: 3	Loss: 0.015758	Acc: 63.5% (5082/8000)
[Test]  Epoch: 4	Loss: 0.014623	Acc: 66.8% (5343/8000)
[Test]  Epoch: 5	Loss: 0.014495	Acc: 66.7% (5334/8000)
[Test]  Epoch: 6	Loss: 0.014268	Acc: 67.6% (5408/8000)
[Test]  Epoch: 7	Loss: 0.014325	Acc: 67.2% (5372/8000)
[Test]  Epoch: 8	Loss: 0.014106	Acc: 67.5% (5402/8000)
[Test]  Epoch: 9	Loss: 0.014066	Acc: 67.9% (5435/8000)
[Test]  Epoch: 10	Loss: 0.014081	Acc: 67.9% (5435/8000)
[Test]  Epoch: 11	Loss: 0.014065	Acc: 67.6% (5411/8000)
[Test]  Epoch: 12	Loss: 0.014069	Acc: 67.7% (5418/8000)
[Test]  Epoch: 13	Loss: 0.014040	Acc: 67.6% (5411/8000)
[Test]  Epoch: 14	Loss: 0.013995	Acc: 68.0% (5439/8000)
[Test]  Epoch: 15	Loss: 0.014041	Acc: 67.7% (5417/8000)
[Test]  Epoch: 16	Loss: 0.013951	Acc: 68.0% (5444/8000)
[Test]  Epoch: 17	Loss: 0.013943	Acc: 68.0% (5437/8000)
[Test]  Epoch: 18	Loss: 0.013942	Acc: 68.2% (5452/8000)
[Test]  Epoch: 19	Loss: 0.013924	Acc: 68.2% (5454/8000)
[Test]  Epoch: 20	Loss: 0.013964	Acc: 68.0% (5436/8000)
[Test]  Epoch: 21	Loss: 0.013861	Acc: 68.3% (5461/8000)
[Test]  Epoch: 22	Loss: 0.013895	Acc: 68.4% (5473/8000)
[Test]  Epoch: 23	Loss: 0.013844	Acc: 68.2% (5459/8000)
[Test]  Epoch: 24	Loss: 0.013815	Acc: 68.2% (5458/8000)
[Test]  Epoch: 25	Loss: 0.013841	Acc: 68.4% (5469/8000)
[Test]  Epoch: 26	Loss: 0.013788	Acc: 68.6% (5491/8000)
[Test]  Epoch: 27	Loss: 0.013775	Acc: 68.3% (5464/8000)
[Test]  Epoch: 28	Loss: 0.013818	Acc: 68.3% (5465/8000)
[Test]  Epoch: 29	Loss: 0.013728	Acc: 68.6% (5487/8000)
[Test]  Epoch: 30	Loss: 0.013745	Acc: 68.7% (5494/8000)
[Test]  Epoch: 31	Loss: 0.013797	Acc: 68.3% (5465/8000)
[Test]  Epoch: 32	Loss: 0.013740	Acc: 68.5% (5484/8000)
[Test]  Epoch: 33	Loss: 0.013756	Acc: 68.4% (5473/8000)
[Test]  Epoch: 34	Loss: 0.013724	Acc: 68.6% (5490/8000)
[Test]  Epoch: 35	Loss: 0.013728	Acc: 68.5% (5483/8000)
[Test]  Epoch: 36	Loss: 0.013718	Acc: 68.8% (5503/8000)
[Test]  Epoch: 37	Loss: 0.013754	Acc: 68.9% (5509/8000)
[Test]  Epoch: 38	Loss: 0.013668	Acc: 68.8% (5501/8000)
[Test]  Epoch: 39	Loss: 0.013702	Acc: 68.9% (5512/8000)
[Test]  Epoch: 40	Loss: 0.013740	Acc: 68.6% (5487/8000)
[Test]  Epoch: 41	Loss: 0.013667	Acc: 68.8% (5506/8000)
[Test]  Epoch: 42	Loss: 0.013620	Acc: 68.9% (5512/8000)
[Test]  Epoch: 43	Loss: 0.013647	Acc: 69.0% (5523/8000)
[Test]  Epoch: 44	Loss: 0.013672	Acc: 69.0% (5522/8000)
[Test]  Epoch: 45	Loss: 0.013661	Acc: 69.0% (5517/8000)
[Test]  Epoch: 46	Loss: 0.013663	Acc: 68.6% (5485/8000)
[Test]  Epoch: 47	Loss: 0.013632	Acc: 68.7% (5499/8000)
[Test]  Epoch: 48	Loss: 0.013612	Acc: 68.8% (5506/8000)
[Test]  Epoch: 49	Loss: 0.013603	Acc: 69.2% (5534/8000)
[Test]  Epoch: 50	Loss: 0.013625	Acc: 69.0% (5518/8000)
[Test]  Epoch: 51	Loss: 0.013617	Acc: 69.0% (5521/8000)
[Test]  Epoch: 52	Loss: 0.013620	Acc: 69.0% (5520/8000)
[Test]  Epoch: 53	Loss: 0.013615	Acc: 68.9% (5513/8000)
[Test]  Epoch: 54	Loss: 0.013631	Acc: 68.9% (5514/8000)
[Test]  Epoch: 55	Loss: 0.013689	Acc: 68.7% (5496/8000)
[Test]  Epoch: 56	Loss: 0.013640	Acc: 69.0% (5521/8000)
[Test]  Epoch: 57	Loss: 0.013625	Acc: 69.0% (5521/8000)
[Test]  Epoch: 58	Loss: 0.013653	Acc: 68.9% (5514/8000)
[Test]  Epoch: 59	Loss: 0.013629	Acc: 69.0% (5523/8000)
[Test]  Epoch: 60	Loss: 0.013644	Acc: 69.1% (5531/8000)
[Test]  Epoch: 61	Loss: 0.013654	Acc: 69.1% (5525/8000)
[Test]  Epoch: 62	Loss: 0.013646	Acc: 69.2% (5537/8000)
[Test]  Epoch: 63	Loss: 0.013611	Acc: 69.1% (5530/8000)
[Test]  Epoch: 64	Loss: 0.013624	Acc: 69.0% (5522/8000)
[Test]  Epoch: 65	Loss: 0.013615	Acc: 69.0% (5517/8000)
[Test]  Epoch: 66	Loss: 0.013637	Acc: 68.9% (5512/8000)
[Test]  Epoch: 67	Loss: 0.013613	Acc: 69.0% (5516/8000)
[Test]  Epoch: 68	Loss: 0.013607	Acc: 69.2% (5532/8000)
[Test]  Epoch: 69	Loss: 0.013611	Acc: 69.0% (5521/8000)
[Test]  Epoch: 70	Loss: 0.013616	Acc: 69.0% (5521/8000)
[Test]  Epoch: 71	Loss: 0.013592	Acc: 69.0% (5522/8000)
[Test]  Epoch: 72	Loss: 0.013595	Acc: 69.2% (5533/8000)
[Test]  Epoch: 73	Loss: 0.013617	Acc: 68.9% (5514/8000)
[Test]  Epoch: 74	Loss: 0.013612	Acc: 69.1% (5525/8000)
[Test]  Epoch: 75	Loss: 0.013593	Acc: 69.0% (5523/8000)
[Test]  Epoch: 76	Loss: 0.013595	Acc: 69.3% (5545/8000)
[Test]  Epoch: 77	Loss: 0.013591	Acc: 69.0% (5519/8000)
[Test]  Epoch: 78	Loss: 0.013597	Acc: 69.1% (5526/8000)
[Test]  Epoch: 79	Loss: 0.013600	Acc: 69.2% (5532/8000)
[Test]  Epoch: 80	Loss: 0.013593	Acc: 69.1% (5527/8000)
[Test]  Epoch: 81	Loss: 0.013573	Acc: 69.1% (5528/8000)
[Test]  Epoch: 82	Loss: 0.013630	Acc: 69.0% (5521/8000)
[Test]  Epoch: 83	Loss: 0.013595	Acc: 69.1% (5525/8000)
[Test]  Epoch: 84	Loss: 0.013592	Acc: 69.0% (5523/8000)
[Test]  Epoch: 85	Loss: 0.013611	Acc: 69.1% (5527/8000)
[Test]  Epoch: 86	Loss: 0.013598	Acc: 69.1% (5528/8000)
[Test]  Epoch: 87	Loss: 0.013576	Acc: 69.0% (5520/8000)
[Test]  Epoch: 88	Loss: 0.013591	Acc: 69.0% (5524/8000)
[Test]  Epoch: 89	Loss: 0.013608	Acc: 69.0% (5520/8000)
[Test]  Epoch: 90	Loss: 0.013612	Acc: 69.0% (5524/8000)
[Test]  Epoch: 91	Loss: 0.013597	Acc: 69.1% (5528/8000)
[Test]  Epoch: 92	Loss: 0.013616	Acc: 69.2% (5532/8000)
[Test]  Epoch: 93	Loss: 0.013602	Acc: 69.0% (5523/8000)
[Test]  Epoch: 94	Loss: 0.013616	Acc: 69.1% (5529/8000)
[Test]  Epoch: 95	Loss: 0.013594	Acc: 69.2% (5532/8000)
[Test]  Epoch: 96	Loss: 0.013607	Acc: 69.1% (5526/8000)
[Test]  Epoch: 97	Loss: 0.013589	Acc: 69.1% (5525/8000)
[Test]  Epoch: 98	Loss: 0.013585	Acc: 69.2% (5535/8000)
[Test]  Epoch: 99	Loss: 0.013594	Acc: 69.2% (5534/8000)
[Test]  Epoch: 100	Loss: 0.013611	Acc: 69.0% (5523/8000)
===========finish==========
['2024-08-19', '20:35:02.302230', '100', 'test', '0.013610875077545643', '69.0375', '69.3125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042398	Acc: 11.9% (954/8000)
[Test]  Epoch: 2	Loss: 0.024263	Acc: 43.2% (3455/8000)
[Test]  Epoch: 3	Loss: 0.014981	Acc: 65.3% (5228/8000)
[Test]  Epoch: 4	Loss: 0.013775	Acc: 68.6% (5487/8000)
[Test]  Epoch: 5	Loss: 0.013507	Acc: 69.2% (5536/8000)
[Test]  Epoch: 6	Loss: 0.013428	Acc: 69.2% (5534/8000)
[Test]  Epoch: 7	Loss: 0.013370	Acc: 69.2% (5535/8000)
[Test]  Epoch: 8	Loss: 0.013137	Acc: 70.1% (5611/8000)
[Test]  Epoch: 9	Loss: 0.013271	Acc: 69.6% (5569/8000)
[Test]  Epoch: 10	Loss: 0.013144	Acc: 69.9% (5595/8000)
[Test]  Epoch: 11	Loss: 0.013246	Acc: 69.9% (5592/8000)
[Test]  Epoch: 12	Loss: 0.013107	Acc: 70.4% (5629/8000)
[Test]  Epoch: 13	Loss: 0.013164	Acc: 70.0% (5604/8000)
[Test]  Epoch: 14	Loss: 0.013052	Acc: 70.1% (5611/8000)
[Test]  Epoch: 15	Loss: 0.013140	Acc: 70.0% (5598/8000)
[Test]  Epoch: 16	Loss: 0.013005	Acc: 70.4% (5635/8000)
[Test]  Epoch: 17	Loss: 0.013130	Acc: 70.0% (5604/8000)
[Test]  Epoch: 18	Loss: 0.013107	Acc: 70.1% (5611/8000)
[Test]  Epoch: 19	Loss: 0.013020	Acc: 70.2% (5614/8000)
[Test]  Epoch: 20	Loss: 0.012982	Acc: 70.8% (5660/8000)
[Test]  Epoch: 21	Loss: 0.012997	Acc: 70.2% (5618/8000)
[Test]  Epoch: 22	Loss: 0.012972	Acc: 70.3% (5622/8000)
[Test]  Epoch: 23	Loss: 0.012921	Acc: 70.7% (5652/8000)
[Test]  Epoch: 24	Loss: 0.013022	Acc: 70.2% (5614/8000)
[Test]  Epoch: 25	Loss: 0.012907	Acc: 70.5% (5639/8000)
[Test]  Epoch: 26	Loss: 0.012929	Acc: 70.6% (5648/8000)
[Test]  Epoch: 27	Loss: 0.012872	Acc: 70.8% (5666/8000)
[Test]  Epoch: 28	Loss: 0.012979	Acc: 70.5% (5637/8000)
[Test]  Epoch: 29	Loss: 0.012861	Acc: 70.7% (5656/8000)
[Test]  Epoch: 30	Loss: 0.012889	Acc: 70.6% (5650/8000)
[Test]  Epoch: 31	Loss: 0.012927	Acc: 70.4% (5631/8000)
[Test]  Epoch: 32	Loss: 0.012871	Acc: 70.8% (5660/8000)
[Test]  Epoch: 33	Loss: 0.012908	Acc: 70.7% (5656/8000)
[Test]  Epoch: 34	Loss: 0.012840	Acc: 70.8% (5663/8000)
[Test]  Epoch: 35	Loss: 0.012878	Acc: 70.7% (5658/8000)
[Test]  Epoch: 36	Loss: 0.012788	Acc: 71.2% (5692/8000)
[Test]  Epoch: 37	Loss: 0.012866	Acc: 70.5% (5643/8000)
[Test]  Epoch: 38	Loss: 0.012863	Acc: 70.9% (5669/8000)
[Test]  Epoch: 39	Loss: 0.012848	Acc: 70.8% (5662/8000)
[Test]  Epoch: 40	Loss: 0.012809	Acc: 70.8% (5666/8000)
[Test]  Epoch: 41	Loss: 0.012757	Acc: 71.3% (5705/8000)
[Test]  Epoch: 42	Loss: 0.012807	Acc: 71.1% (5687/8000)
[Test]  Epoch: 43	Loss: 0.012732	Acc: 71.2% (5694/8000)
[Test]  Epoch: 44	Loss: 0.012770	Acc: 71.2% (5699/8000)
[Test]  Epoch: 45	Loss: 0.012771	Acc: 71.2% (5693/8000)
[Test]  Epoch: 46	Loss: 0.012793	Acc: 71.3% (5701/8000)
[Test]  Epoch: 47	Loss: 0.012722	Acc: 71.2% (5696/8000)
[Test]  Epoch: 48	Loss: 0.012723	Acc: 71.3% (5704/8000)
[Test]  Epoch: 49	Loss: 0.012724	Acc: 71.2% (5693/8000)
[Test]  Epoch: 50	Loss: 0.012721	Acc: 71.4% (5714/8000)
[Test]  Epoch: 51	Loss: 0.012824	Acc: 71.0% (5683/8000)
[Test]  Epoch: 52	Loss: 0.012762	Acc: 71.3% (5704/8000)
[Test]  Epoch: 53	Loss: 0.012777	Acc: 71.3% (5704/8000)
[Test]  Epoch: 54	Loss: 0.012783	Acc: 71.1% (5691/8000)
[Test]  Epoch: 55	Loss: 0.012788	Acc: 71.3% (5704/8000)
[Test]  Epoch: 56	Loss: 0.012783	Acc: 71.0% (5683/8000)
[Test]  Epoch: 57	Loss: 0.012776	Acc: 71.2% (5694/8000)
[Test]  Epoch: 58	Loss: 0.012796	Acc: 71.2% (5696/8000)
[Test]  Epoch: 59	Loss: 0.012751	Acc: 71.3% (5706/8000)
[Test]  Epoch: 60	Loss: 0.012769	Acc: 71.3% (5705/8000)
[Test]  Epoch: 61	Loss: 0.012772	Acc: 71.1% (5689/8000)
[Test]  Epoch: 62	Loss: 0.012771	Acc: 71.3% (5706/8000)
[Test]  Epoch: 63	Loss: 0.012733	Acc: 71.6% (5729/8000)
[Test]  Epoch: 64	Loss: 0.012757	Acc: 71.5% (5716/8000)
[Test]  Epoch: 65	Loss: 0.012749	Acc: 71.5% (5719/8000)
[Test]  Epoch: 66	Loss: 0.012740	Acc: 71.4% (5712/8000)
[Test]  Epoch: 67	Loss: 0.012748	Acc: 71.6% (5727/8000)
[Test]  Epoch: 68	Loss: 0.012744	Acc: 71.3% (5705/8000)
[Test]  Epoch: 69	Loss: 0.012734	Acc: 71.5% (5719/8000)
[Test]  Epoch: 70	Loss: 0.012729	Acc: 71.6% (5727/8000)
[Test]  Epoch: 71	Loss: 0.012717	Acc: 71.5% (5719/8000)
[Test]  Epoch: 72	Loss: 0.012707	Acc: 71.6% (5725/8000)
[Test]  Epoch: 73	Loss: 0.012730	Acc: 71.5% (5720/8000)
[Test]  Epoch: 74	Loss: 0.012732	Acc: 71.3% (5704/8000)
[Test]  Epoch: 75	Loss: 0.012688	Acc: 71.6% (5728/8000)
[Test]  Epoch: 76	Loss: 0.012690	Acc: 71.3% (5702/8000)
[Test]  Epoch: 77	Loss: 0.012710	Acc: 71.4% (5709/8000)
[Test]  Epoch: 78	Loss: 0.012733	Acc: 71.4% (5709/8000)
[Test]  Epoch: 79	Loss: 0.012734	Acc: 71.5% (5724/8000)
[Test]  Epoch: 80	Loss: 0.012732	Acc: 71.5% (5717/8000)
[Test]  Epoch: 81	Loss: 0.012705	Acc: 71.4% (5711/8000)
[Test]  Epoch: 82	Loss: 0.012776	Acc: 71.1% (5687/8000)
[Test]  Epoch: 83	Loss: 0.012709	Acc: 71.3% (5707/8000)
[Test]  Epoch: 84	Loss: 0.012720	Acc: 71.4% (5711/8000)
[Test]  Epoch: 85	Loss: 0.012733	Acc: 71.3% (5707/8000)
[Test]  Epoch: 86	Loss: 0.012737	Acc: 71.5% (5721/8000)
[Test]  Epoch: 87	Loss: 0.012719	Acc: 71.6% (5726/8000)
[Test]  Epoch: 88	Loss: 0.012742	Acc: 71.4% (5709/8000)
[Test]  Epoch: 89	Loss: 0.012743	Acc: 71.6% (5731/8000)
[Test]  Epoch: 90	Loss: 0.012723	Acc: 71.6% (5727/8000)
[Test]  Epoch: 91	Loss: 0.012701	Acc: 71.4% (5715/8000)
[Test]  Epoch: 92	Loss: 0.012729	Acc: 71.5% (5716/8000)
[Test]  Epoch: 93	Loss: 0.012718	Acc: 71.6% (5729/8000)
[Test]  Epoch: 94	Loss: 0.012723	Acc: 71.4% (5712/8000)
[Test]  Epoch: 95	Loss: 0.012720	Acc: 71.3% (5708/8000)
[Test]  Epoch: 96	Loss: 0.012736	Acc: 71.4% (5713/8000)
[Test]  Epoch: 97	Loss: 0.012701	Acc: 71.4% (5715/8000)
[Test]  Epoch: 98	Loss: 0.012716	Acc: 71.6% (5731/8000)
[Test]  Epoch: 99	Loss: 0.012728	Acc: 71.2% (5700/8000)
[Test]  Epoch: 100	Loss: 0.012728	Acc: 71.4% (5711/8000)
===========finish==========
['2024-08-19', '20:44:22.839715', '100', 'test', '0.01272765627503395', '71.3875', '71.6375']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.054465	Acc: 10.2% (815/8000)
[Test]  Epoch: 2	Loss: 0.023271	Acc: 47.4% (3788/8000)
[Test]  Epoch: 3	Loss: 0.013166	Acc: 70.7% (5656/8000)
[Test]  Epoch: 4	Loss: 0.012256	Acc: 73.1% (5849/8000)
[Test]  Epoch: 5	Loss: 0.011851	Acc: 73.9% (5915/8000)
[Test]  Epoch: 6	Loss: 0.011881	Acc: 73.8% (5905/8000)
[Test]  Epoch: 7	Loss: 0.011669	Acc: 74.3% (5941/8000)
[Test]  Epoch: 8	Loss: 0.011606	Acc: 74.5% (5957/8000)
[Test]  Epoch: 9	Loss: 0.011636	Acc: 74.7% (5973/8000)
[Test]  Epoch: 10	Loss: 0.011593	Acc: 74.5% (5962/8000)
[Test]  Epoch: 11	Loss: 0.011593	Acc: 74.7% (5978/8000)
[Test]  Epoch: 12	Loss: 0.011499	Acc: 74.8% (5987/8000)
[Test]  Epoch: 13	Loss: 0.011608	Acc: 74.3% (5945/8000)
[Test]  Epoch: 14	Loss: 0.011527	Acc: 75.0% (5997/8000)
[Test]  Epoch: 15	Loss: 0.011508	Acc: 75.0% (6004/8000)
[Test]  Epoch: 16	Loss: 0.011484	Acc: 75.0% (6000/8000)
[Test]  Epoch: 17	Loss: 0.011490	Acc: 75.1% (6006/8000)
[Test]  Epoch: 18	Loss: 0.011565	Acc: 74.5% (5962/8000)
[Test]  Epoch: 19	Loss: 0.011427	Acc: 75.3% (6027/8000)
[Test]  Epoch: 20	Loss: 0.011550	Acc: 75.0% (5996/8000)
[Test]  Epoch: 21	Loss: 0.011484	Acc: 75.3% (6022/8000)
[Test]  Epoch: 22	Loss: 0.011509	Acc: 74.9% (5993/8000)
[Test]  Epoch: 23	Loss: 0.011463	Acc: 75.1% (6011/8000)
[Test]  Epoch: 24	Loss: 0.011459	Acc: 75.1% (6011/8000)
[Test]  Epoch: 25	Loss: 0.011429	Acc: 75.0% (5996/8000)
[Test]  Epoch: 26	Loss: 0.011449	Acc: 75.2% (6015/8000)
[Test]  Epoch: 27	Loss: 0.011382	Acc: 75.0% (6003/8000)
[Test]  Epoch: 28	Loss: 0.011437	Acc: 75.2% (6012/8000)
[Test]  Epoch: 29	Loss: 0.011356	Acc: 75.5% (6036/8000)
[Test]  Epoch: 30	Loss: 0.011353	Acc: 75.2% (6016/8000)
[Test]  Epoch: 31	Loss: 0.011388	Acc: 75.2% (6012/8000)
[Test]  Epoch: 32	Loss: 0.011371	Acc: 75.6% (6045/8000)
[Test]  Epoch: 33	Loss: 0.011365	Acc: 75.3% (6023/8000)
[Test]  Epoch: 34	Loss: 0.011373	Acc: 75.5% (6044/8000)
[Test]  Epoch: 35	Loss: 0.011346	Acc: 75.5% (6043/8000)
[Test]  Epoch: 36	Loss: 0.011324	Acc: 75.6% (6045/8000)
[Test]  Epoch: 37	Loss: 0.011291	Acc: 75.4% (6035/8000)
[Test]  Epoch: 38	Loss: 0.011388	Acc: 75.6% (6051/8000)
[Test]  Epoch: 39	Loss: 0.011292	Acc: 75.5% (6040/8000)
[Test]  Epoch: 40	Loss: 0.011363	Acc: 75.3% (6027/8000)
[Test]  Epoch: 41	Loss: 0.011341	Acc: 75.4% (6029/8000)
[Test]  Epoch: 42	Loss: 0.011322	Acc: 75.6% (6046/8000)
[Test]  Epoch: 43	Loss: 0.011317	Acc: 75.5% (6041/8000)
[Test]  Epoch: 44	Loss: 0.011301	Acc: 75.3% (6021/8000)
[Test]  Epoch: 45	Loss: 0.011313	Acc: 75.9% (6072/8000)
[Test]  Epoch: 46	Loss: 0.011289	Acc: 75.9% (6074/8000)
[Test]  Epoch: 47	Loss: 0.011267	Acc: 75.7% (6056/8000)
[Test]  Epoch: 48	Loss: 0.011287	Acc: 75.6% (6045/8000)
[Test]  Epoch: 49	Loss: 0.011281	Acc: 75.8% (6062/8000)
[Test]  Epoch: 50	Loss: 0.011311	Acc: 75.8% (6065/8000)
[Test]  Epoch: 51	Loss: 0.011343	Acc: 75.7% (6053/8000)
[Test]  Epoch: 52	Loss: 0.011347	Acc: 75.7% (6055/8000)
[Test]  Epoch: 53	Loss: 0.011333	Acc: 75.7% (6058/8000)
[Test]  Epoch: 54	Loss: 0.011337	Acc: 75.7% (6052/8000)
[Test]  Epoch: 55	Loss: 0.011326	Acc: 75.5% (6043/8000)
[Test]  Epoch: 56	Loss: 0.011331	Acc: 75.7% (6058/8000)
[Test]  Epoch: 57	Loss: 0.011269	Acc: 76.0% (6082/8000)
[Test]  Epoch: 58	Loss: 0.011359	Acc: 75.7% (6057/8000)
[Test]  Epoch: 59	Loss: 0.011264	Acc: 76.2% (6093/8000)
[Test]  Epoch: 60	Loss: 0.011349	Acc: 76.0% (6078/8000)
[Test]  Epoch: 61	Loss: 0.011339	Acc: 75.8% (6067/8000)
[Test]  Epoch: 62	Loss: 0.011292	Acc: 76.0% (6080/8000)
[Test]  Epoch: 63	Loss: 0.011288	Acc: 76.0% (6082/8000)
[Test]  Epoch: 64	Loss: 0.011323	Acc: 75.8% (6063/8000)
[Test]  Epoch: 65	Loss: 0.011298	Acc: 75.9% (6075/8000)
[Test]  Epoch: 66	Loss: 0.011315	Acc: 76.0% (6076/8000)
[Test]  Epoch: 67	Loss: 0.011306	Acc: 75.9% (6074/8000)
[Test]  Epoch: 68	Loss: 0.011310	Acc: 76.0% (6084/8000)
[Test]  Epoch: 69	Loss: 0.011307	Acc: 75.9% (6074/8000)
[Test]  Epoch: 70	Loss: 0.011295	Acc: 75.9% (6071/8000)
[Test]  Epoch: 71	Loss: 0.011295	Acc: 76.1% (6085/8000)
[Test]  Epoch: 72	Loss: 0.011308	Acc: 75.9% (6071/8000)
[Test]  Epoch: 73	Loss: 0.011285	Acc: 75.9% (6071/8000)
[Test]  Epoch: 74	Loss: 0.011291	Acc: 76.0% (6077/8000)
[Test]  Epoch: 75	Loss: 0.011280	Acc: 76.0% (6077/8000)
[Test]  Epoch: 76	Loss: 0.011259	Acc: 76.1% (6090/8000)
[Test]  Epoch: 77	Loss: 0.011298	Acc: 75.9% (6074/8000)
[Test]  Epoch: 78	Loss: 0.011305	Acc: 76.0% (6082/8000)
[Test]  Epoch: 79	Loss: 0.011342	Acc: 75.8% (6064/8000)
[Test]  Epoch: 80	Loss: 0.011322	Acc: 75.9% (6075/8000)
[Test]  Epoch: 81	Loss: 0.011279	Acc: 76.0% (6076/8000)
[Test]  Epoch: 82	Loss: 0.011313	Acc: 75.8% (6063/8000)
[Test]  Epoch: 83	Loss: 0.011297	Acc: 75.9% (6071/8000)
[Test]  Epoch: 84	Loss: 0.011286	Acc: 76.0% (6076/8000)
[Test]  Epoch: 85	Loss: 0.011316	Acc: 75.9% (6070/8000)
[Test]  Epoch: 86	Loss: 0.011296	Acc: 75.8% (6068/8000)
[Test]  Epoch: 87	Loss: 0.011252	Acc: 75.9% (6073/8000)
[Test]  Epoch: 88	Loss: 0.011298	Acc: 76.2% (6096/8000)
[Test]  Epoch: 89	Loss: 0.011332	Acc: 75.8% (6063/8000)
[Test]  Epoch: 90	Loss: 0.011309	Acc: 75.9% (6074/8000)
[Test]  Epoch: 91	Loss: 0.011306	Acc: 76.0% (6079/8000)
[Test]  Epoch: 92	Loss: 0.011305	Acc: 75.9% (6073/8000)
[Test]  Epoch: 93	Loss: 0.011273	Acc: 76.0% (6078/8000)
[Test]  Epoch: 94	Loss: 0.011292	Acc: 76.1% (6085/8000)
[Test]  Epoch: 95	Loss: 0.011307	Acc: 76.0% (6077/8000)
[Test]  Epoch: 96	Loss: 0.011356	Acc: 75.7% (6057/8000)
[Test]  Epoch: 97	Loss: 0.011269	Acc: 76.0% (6076/8000)
[Test]  Epoch: 98	Loss: 0.011309	Acc: 75.9% (6069/8000)
[Test]  Epoch: 99	Loss: 0.011318	Acc: 75.9% (6073/8000)
[Test]  Epoch: 100	Loss: 0.011303	Acc: 75.8% (6067/8000)
===========finish==========
['2024-08-19', '20:53:39.670721', '100', 'test', '0.01130318495631218', '75.8375', '76.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']
['conv1.weight', 'bn1.weight', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'last_linear.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036695	Acc: 11.6% (927/8000)
[Test]  Epoch: 2	Loss: 0.024737	Acc: 47.9% (3831/8000)
[Test]  Epoch: 3	Loss: 0.014283	Acc: 71.4% (5715/8000)
[Test]  Epoch: 4	Loss: 0.012580	Acc: 75.6% (6045/8000)
[Test]  Epoch: 5	Loss: 0.011922	Acc: 76.6% (6127/8000)
[Test]  Epoch: 6	Loss: 0.011717	Acc: 76.7% (6132/8000)
[Test]  Epoch: 7	Loss: 0.011426	Acc: 77.7% (6219/8000)
[Test]  Epoch: 8	Loss: 0.011469	Acc: 76.6% (6131/8000)
[Test]  Epoch: 9	Loss: 0.011443	Acc: 77.0% (6161/8000)
[Test]  Epoch: 10	Loss: 0.011419	Acc: 76.9% (6152/8000)
[Test]  Epoch: 11	Loss: 0.011322	Acc: 77.5% (6203/8000)
[Test]  Epoch: 12	Loss: 0.011363	Acc: 77.0% (6160/8000)
[Test]  Epoch: 13	Loss: 0.011266	Acc: 77.3% (6183/8000)
[Test]  Epoch: 14	Loss: 0.011230	Acc: 77.2% (6173/8000)
[Test]  Epoch: 15	Loss: 0.011265	Acc: 77.1% (6166/8000)
[Test]  Epoch: 16	Loss: 0.011135	Acc: 77.3% (6187/8000)
[Test]  Epoch: 17	Loss: 0.011223	Acc: 77.6% (6207/8000)
[Test]  Epoch: 18	Loss: 0.011225	Acc: 77.0% (6159/8000)
[Test]  Epoch: 19	Loss: 0.011163	Acc: 77.5% (6203/8000)
[Test]  Epoch: 20	Loss: 0.011234	Acc: 77.4% (6193/8000)
[Test]  Epoch: 21	Loss: 0.011207	Acc: 77.2% (6178/8000)
[Test]  Epoch: 22	Loss: 0.011188	Acc: 77.4% (6193/8000)
[Test]  Epoch: 23	Loss: 0.011045	Acc: 77.7% (6214/8000)
[Test]  Epoch: 24	Loss: 0.011137	Acc: 77.6% (6208/8000)
[Test]  Epoch: 25	Loss: 0.011189	Acc: 77.0% (6161/8000)
[Test]  Epoch: 26	Loss: 0.011192	Acc: 77.2% (6178/8000)
[Test]  Epoch: 27	Loss: 0.011134	Acc: 77.2% (6176/8000)
[Test]  Epoch: 28	Loss: 0.011153	Acc: 77.3% (6184/8000)
[Test]  Epoch: 29	Loss: 0.011097	Acc: 77.3% (6187/8000)
[Test]  Epoch: 30	Loss: 0.011129	Acc: 77.0% (6161/8000)
[Test]  Epoch: 31	Loss: 0.011174	Acc: 77.2% (6173/8000)
[Test]  Epoch: 32	Loss: 0.011063	Acc: 77.6% (6207/8000)
[Test]  Epoch: 33	Loss: 0.011192	Acc: 77.2% (6177/8000)
[Test]  Epoch: 34	Loss: 0.011038	Acc: 77.9% (6234/8000)
[Test]  Epoch: 35	Loss: 0.011086	Acc: 77.3% (6185/8000)
[Test]  Epoch: 36	Loss: 0.011115	Acc: 77.8% (6221/8000)
[Test]  Epoch: 37	Loss: 0.011131	Acc: 77.4% (6194/8000)
[Test]  Epoch: 38	Loss: 0.011082	Acc: 77.4% (6193/8000)
[Test]  Epoch: 39	Loss: 0.011098	Acc: 77.4% (6191/8000)
[Test]  Epoch: 40	Loss: 0.011092	Acc: 77.4% (6193/8000)
[Test]  Epoch: 41	Loss: 0.011161	Acc: 77.2% (6176/8000)
[Test]  Epoch: 42	Loss: 0.011138	Acc: 77.2% (6175/8000)
[Test]  Epoch: 43	Loss: 0.011052	Acc: 77.2% (6179/8000)
[Test]  Epoch: 44	Loss: 0.011152	Acc: 77.1% (6167/8000)
[Test]  Epoch: 45	Loss: 0.011025	Acc: 77.4% (6194/8000)
[Test]  Epoch: 46	Loss: 0.011099	Acc: 77.6% (6208/8000)
[Test]  Epoch: 47	Loss: 0.011103	Acc: 77.1% (6168/8000)
[Test]  Epoch: 48	Loss: 0.011005	Acc: 77.5% (6204/8000)
[Test]  Epoch: 49	Loss: 0.011067	Acc: 77.5% (6204/8000)
[Test]  Epoch: 50	Loss: 0.011069	Acc: 77.5% (6202/8000)
[Test]  Epoch: 51	Loss: 0.011208	Acc: 77.1% (6167/8000)
[Test]  Epoch: 52	Loss: 0.011157	Acc: 77.4% (6189/8000)
[Test]  Epoch: 53	Loss: 0.011049	Acc: 77.5% (6199/8000)
[Test]  Epoch: 54	Loss: 0.011193	Acc: 77.0% (6156/8000)
[Test]  Epoch: 55	Loss: 0.011192	Acc: 76.7% (6134/8000)
[Test]  Epoch: 56	Loss: 0.011142	Acc: 77.2% (6179/8000)
[Test]  Epoch: 57	Loss: 0.011148	Acc: 77.3% (6186/8000)
[Test]  Epoch: 58	Loss: 0.011187	Acc: 77.1% (6168/8000)
[Test]  Epoch: 59	Loss: 0.011091	Acc: 77.4% (6195/8000)
[Test]  Epoch: 60	Loss: 0.011130	Acc: 77.2% (6174/8000)
[Test]  Epoch: 61	Loss: 0.011145	Acc: 77.1% (6170/8000)
[Test]  Epoch: 62	Loss: 0.011144	Acc: 77.3% (6188/8000)
[Test]  Epoch: 63	Loss: 0.011091	Acc: 77.3% (6183/8000)
[Test]  Epoch: 64	Loss: 0.011126	Acc: 77.2% (6179/8000)
[Test]  Epoch: 65	Loss: 0.011092	Acc: 77.2% (6180/8000)
[Test]  Epoch: 66	Loss: 0.011123	Acc: 77.3% (6181/8000)
[Test]  Epoch: 67	Loss: 0.011088	Acc: 77.5% (6197/8000)
[Test]  Epoch: 68	Loss: 0.011119	Acc: 77.3% (6187/8000)
[Test]  Epoch: 69	Loss: 0.011112	Acc: 77.4% (6192/8000)
[Test]  Epoch: 70	Loss: 0.011119	Acc: 77.4% (6191/8000)
[Test]  Epoch: 71	Loss: 0.011108	Acc: 77.3% (6188/8000)
[Test]  Epoch: 72	Loss: 0.011101	Acc: 77.5% (6197/8000)
[Test]  Epoch: 73	Loss: 0.011106	Acc: 77.3% (6187/8000)
[Test]  Epoch: 74	Loss: 0.011129	Acc: 77.3% (6184/8000)
[Test]  Epoch: 75	Loss: 0.011121	Acc: 77.4% (6192/8000)
[Test]  Epoch: 76	Loss: 0.011100	Acc: 77.3% (6188/8000)
[Test]  Epoch: 77	Loss: 0.011113	Acc: 77.5% (6199/8000)
[Test]  Epoch: 78	Loss: 0.011117	Acc: 77.5% (6198/8000)
[Test]  Epoch: 79	Loss: 0.011128	Acc: 77.4% (6190/8000)
[Test]  Epoch: 80	Loss: 0.011123	Acc: 77.4% (6194/8000)
[Test]  Epoch: 81	Loss: 0.011111	Acc: 77.5% (6204/8000)
[Test]  Epoch: 82	Loss: 0.011155	Acc: 77.3% (6186/8000)
[Test]  Epoch: 83	Loss: 0.011118	Acc: 77.5% (6199/8000)
[Test]  Epoch: 84	Loss: 0.011103	Acc: 77.3% (6188/8000)
[Test]  Epoch: 85	Loss: 0.011124	Acc: 77.4% (6192/8000)
[Test]  Epoch: 86	Loss: 0.011099	Acc: 77.5% (6198/8000)
[Test]  Epoch: 87	Loss: 0.011049	Acc: 77.6% (6208/8000)
[Test]  Epoch: 88	Loss: 0.011130	Acc: 77.5% (6196/8000)
[Test]  Epoch: 89	Loss: 0.011152	Acc: 77.3% (6184/8000)
[Test]  Epoch: 90	Loss: 0.011138	Acc: 77.2% (6173/8000)
[Test]  Epoch: 91	Loss: 0.011134	Acc: 77.4% (6192/8000)
[Test]  Epoch: 92	Loss: 0.011113	Acc: 77.5% (6197/8000)
[Test]  Epoch: 93	Loss: 0.011098	Acc: 77.4% (6195/8000)
[Test]  Epoch: 94	Loss: 0.011100	Acc: 77.3% (6185/8000)
[Test]  Epoch: 95	Loss: 0.011128	Acc: 77.2% (6174/8000)
[Test]  Epoch: 96	Loss: 0.011177	Acc: 77.3% (6184/8000)
[Test]  Epoch: 97	Loss: 0.011112	Acc: 77.5% (6204/8000)
[Test]  Epoch: 98	Loss: 0.011113	Acc: 77.6% (6207/8000)
[Test]  Epoch: 99	Loss: 0.011159	Acc: 77.3% (6186/8000)
[Test]  Epoch: 100	Loss: 0.011094	Acc: 77.5% (6204/8000)
===========finish==========
['2024-08-19', '21:03:13.753984', '100', 'test', '0.011094121985137463', '77.55', '77.925']
