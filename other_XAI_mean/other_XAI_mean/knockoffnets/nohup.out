result path:  /home/gpu2/jbw/other_XAI_mean/knockoffnets/ms_elastictrainer_result_resnet18_vgg16_mobilenetv2.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.029811	Acc: 48.3% (4831/10000)
[Test]  Epoch: 2	Loss: 0.029688	Acc: 48.8% (4875/10000)
[Test]  Epoch: 3	Loss: 0.029691	Acc: 48.6% (4863/10000)
[Test]  Epoch: 4	Loss: 0.029696	Acc: 48.5% (4855/10000)
[Test]  Epoch: 5	Loss: 0.029717	Acc: 48.5% (4848/10000)
[Test]  Epoch: 6	Loss: 0.029706	Acc: 48.4% (4844/10000)
[Test]  Epoch: 7	Loss: 0.029596	Acc: 48.9% (4885/10000)
[Test]  Epoch: 8	Loss: 0.029622	Acc: 49.0% (4897/10000)
[Test]  Epoch: 9	Loss: 0.029534	Acc: 48.5% (4854/10000)
[Test]  Epoch: 10	Loss: 0.029673	Acc: 48.6% (4857/10000)
[Test]  Epoch: 11	Loss: 0.029579	Acc: 48.8% (4883/10000)
[Test]  Epoch: 12	Loss: 0.029548	Acc: 49.0% (4896/10000)
[Test]  Epoch: 13	Loss: 0.029555	Acc: 48.9% (4894/10000)
[Test]  Epoch: 14	Loss: 0.029517	Acc: 49.2% (4918/10000)
[Test]  Epoch: 15	Loss: 0.029646	Acc: 48.5% (4853/10000)
[Test]  Epoch: 16	Loss: 0.029472	Acc: 49.1% (4913/10000)
[Test]  Epoch: 17	Loss: 0.029577	Acc: 48.9% (4892/10000)
[Test]  Epoch: 18	Loss: 0.029639	Acc: 48.8% (4878/10000)
[Test]  Epoch: 19	Loss: 0.029562	Acc: 48.9% (4892/10000)
[Test]  Epoch: 20	Loss: 0.029530	Acc: 48.8% (4884/10000)
[Test]  Epoch: 21	Loss: 0.029501	Acc: 48.8% (4876/10000)
[Test]  Epoch: 22	Loss: 0.029458	Acc: 49.3% (4928/10000)
[Test]  Epoch: 23	Loss: 0.029553	Acc: 48.9% (4886/10000)
[Test]  Epoch: 24	Loss: 0.029544	Acc: 49.0% (4903/10000)
[Test]  Epoch: 25	Loss: 0.029531	Acc: 49.2% (4922/10000)
[Test]  Epoch: 26	Loss: 0.029477	Acc: 49.2% (4925/10000)
[Test]  Epoch: 27	Loss: 0.029562	Acc: 48.9% (4885/10000)
[Test]  Epoch: 28	Loss: 0.029662	Acc: 48.7% (4873/10000)
[Test]  Epoch: 29	Loss: 0.029596	Acc: 48.9% (4894/10000)
[Test]  Epoch: 30	Loss: 0.029527	Acc: 49.1% (4907/10000)
[Test]  Epoch: 31	Loss: 0.029476	Acc: 49.4% (4937/10000)
[Test]  Epoch: 32	Loss: 0.029599	Acc: 48.7% (4866/10000)
[Test]  Epoch: 33	Loss: 0.029536	Acc: 49.0% (4905/10000)
[Test]  Epoch: 34	Loss: 0.029542	Acc: 49.0% (4895/10000)
[Test]  Epoch: 35	Loss: 0.029596	Acc: 48.8% (4879/10000)
[Test]  Epoch: 36	Loss: 0.029684	Acc: 48.7% (4870/10000)
[Test]  Epoch: 37	Loss: 0.029544	Acc: 49.2% (4916/10000)
[Test]  Epoch: 38	Loss: 0.029463	Acc: 49.1% (4911/10000)
[Test]  Epoch: 39	Loss: 0.029563	Acc: 49.1% (4913/10000)
[Test]  Epoch: 40	Loss: 0.029539	Acc: 49.0% (4904/10000)
[Test]  Epoch: 41	Loss: 0.029512	Acc: 49.1% (4914/10000)
[Test]  Epoch: 42	Loss: 0.029695	Acc: 48.7% (4872/10000)
[Test]  Epoch: 43	Loss: 0.029642	Acc: 49.0% (4902/10000)
[Test]  Epoch: 44	Loss: 0.029547	Acc: 49.1% (4910/10000)
[Test]  Epoch: 45	Loss: 0.029702	Acc: 48.9% (4888/10000)
[Test]  Epoch: 46	Loss: 0.029549	Acc: 49.0% (4898/10000)
[Test]  Epoch: 47	Loss: 0.029563	Acc: 49.1% (4914/10000)
[Test]  Epoch: 48	Loss: 0.029585	Acc: 48.8% (4876/10000)
[Test]  Epoch: 49	Loss: 0.029601	Acc: 48.9% (4892/10000)
[Test]  Epoch: 50	Loss: 0.029591	Acc: 48.9% (4888/10000)
[Test]  Epoch: 51	Loss: 0.029630	Acc: 49.0% (4903/10000)
[Test]  Epoch: 52	Loss: 0.029522	Acc: 49.0% (4896/10000)
[Test]  Epoch: 53	Loss: 0.029623	Acc: 48.9% (4894/10000)
[Test]  Epoch: 54	Loss: 0.029665	Acc: 49.0% (4903/10000)
[Test]  Epoch: 55	Loss: 0.029581	Acc: 49.1% (4913/10000)
[Test]  Epoch: 56	Loss: 0.029631	Acc: 49.0% (4904/10000)
[Test]  Epoch: 57	Loss: 0.029653	Acc: 48.5% (4850/10000)
[Test]  Epoch: 58	Loss: 0.029502	Acc: 49.0% (4897/10000)
[Test]  Epoch: 59	Loss: 0.029618	Acc: 48.8% (4878/10000)
[Test]  Epoch: 60	Loss: 0.029779	Acc: 48.5% (4846/10000)
[Test]  Epoch: 61	Loss: 0.029737	Acc: 48.4% (4838/10000)
[Test]  Epoch: 62	Loss: 0.029652	Acc: 48.7% (4874/10000)
[Test]  Epoch: 63	Loss: 0.029647	Acc: 48.8% (4884/10000)
[Test]  Epoch: 64	Loss: 0.029632	Acc: 48.9% (4886/10000)
[Test]  Epoch: 65	Loss: 0.029561	Acc: 48.9% (4891/10000)
[Test]  Epoch: 66	Loss: 0.029585	Acc: 48.9% (4887/10000)
[Test]  Epoch: 67	Loss: 0.029664	Acc: 48.6% (4863/10000)
[Test]  Epoch: 68	Loss: 0.029578	Acc: 49.1% (4910/10000)
[Test]  Epoch: 69	Loss: 0.029593	Acc: 48.8% (4881/10000)
[Test]  Epoch: 70	Loss: 0.029614	Acc: 48.9% (4893/10000)
[Test]  Epoch: 71	Loss: 0.029568	Acc: 48.9% (4893/10000)
[Test]  Epoch: 72	Loss: 0.029601	Acc: 48.8% (4881/10000)
[Test]  Epoch: 73	Loss: 0.029536	Acc: 49.0% (4899/10000)
[Test]  Epoch: 74	Loss: 0.029536	Acc: 48.9% (4887/10000)
[Test]  Epoch: 75	Loss: 0.029557	Acc: 49.0% (4896/10000)
[Test]  Epoch: 76	Loss: 0.029635	Acc: 49.0% (4895/10000)
[Test]  Epoch: 77	Loss: 0.029568	Acc: 49.0% (4901/10000)
[Test]  Epoch: 78	Loss: 0.029628	Acc: 48.8% (4878/10000)
[Test]  Epoch: 79	Loss: 0.029605	Acc: 48.9% (4890/10000)
[Test]  Epoch: 80	Loss: 0.029579	Acc: 49.0% (4898/10000)
[Test]  Epoch: 81	Loss: 0.029522	Acc: 49.0% (4896/10000)
[Test]  Epoch: 82	Loss: 0.029629	Acc: 48.9% (4892/10000)
[Test]  Epoch: 83	Loss: 0.029563	Acc: 49.0% (4899/10000)
[Test]  Epoch: 84	Loss: 0.029565	Acc: 49.1% (4907/10000)
[Test]  Epoch: 85	Loss: 0.029530	Acc: 49.0% (4896/10000)
[Test]  Epoch: 86	Loss: 0.029592	Acc: 49.1% (4907/10000)
[Test]  Epoch: 87	Loss: 0.029556	Acc: 49.1% (4910/10000)
[Test]  Epoch: 88	Loss: 0.029638	Acc: 48.8% (4882/10000)
[Test]  Epoch: 89	Loss: 0.029618	Acc: 48.9% (4890/10000)
[Test]  Epoch: 90	Loss: 0.029544	Acc: 49.0% (4902/10000)
[Test]  Epoch: 91	Loss: 0.029528	Acc: 49.0% (4905/10000)
[Test]  Epoch: 92	Loss: 0.029613	Acc: 49.1% (4908/10000)
[Test]  Epoch: 93	Loss: 0.029572	Acc: 49.2% (4917/10000)
[Test]  Epoch: 94	Loss: 0.029550	Acc: 49.2% (4919/10000)
[Test]  Epoch: 95	Loss: 0.029620	Acc: 49.1% (4910/10000)
[Test]  Epoch: 96	Loss: 0.029535	Acc: 49.2% (4920/10000)
[Test]  Epoch: 97	Loss: 0.029667	Acc: 48.9% (4887/10000)
[Test]  Epoch: 98	Loss: 0.029597	Acc: 49.1% (4914/10000)
[Test]  Epoch: 99	Loss: 0.029630	Acc: 49.0% (4897/10000)
[Test]  Epoch: 100	Loss: 0.029636	Acc: 49.0% (4905/10000)
===========finish==========
['2024-08-19', '00:19:43.514679', '100', 'test', '0.02963573920726776', '49.05', '49.37']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.076744	Acc: 8.3% (831/10000)
[Test]  Epoch: 2	Loss: 0.064644	Acc: 11.1% (1107/10000)
[Test]  Epoch: 3	Loss: 0.058122	Acc: 14.8% (1483/10000)
[Test]  Epoch: 4	Loss: 0.056916	Acc: 16.7% (1671/10000)
[Test]  Epoch: 5	Loss: 0.055495	Acc: 17.4% (1742/10000)
[Test]  Epoch: 6	Loss: 0.055493	Acc: 17.8% (1784/10000)
[Test]  Epoch: 7	Loss: 0.054257	Acc: 18.6% (1862/10000)
[Test]  Epoch: 8	Loss: 0.055321	Acc: 17.7% (1773/10000)
[Test]  Epoch: 9	Loss: 0.053641	Acc: 19.1% (1911/10000)
[Test]  Epoch: 10	Loss: 0.053582	Acc: 19.9% (1987/10000)
[Test]  Epoch: 11	Loss: 0.053431	Acc: 20.0% (2002/10000)
[Test]  Epoch: 12	Loss: 0.052776	Acc: 20.3% (2027/10000)
[Test]  Epoch: 13	Loss: 0.053850	Acc: 19.4% (1940/10000)
[Test]  Epoch: 14	Loss: 0.053063	Acc: 19.8% (1976/10000)
[Test]  Epoch: 15	Loss: 0.053608	Acc: 19.8% (1976/10000)
[Test]  Epoch: 16	Loss: 0.052903	Acc: 20.3% (2029/10000)
[Test]  Epoch: 17	Loss: 0.052776	Acc: 20.1% (2006/10000)
[Test]  Epoch: 18	Loss: 0.052762	Acc: 20.1% (2011/10000)
[Test]  Epoch: 19	Loss: 0.052682	Acc: 20.4% (2039/10000)
[Test]  Epoch: 20	Loss: 0.052414	Acc: 20.8% (2076/10000)
[Test]  Epoch: 21	Loss: 0.052421	Acc: 20.3% (2029/10000)
[Test]  Epoch: 22	Loss: 0.052181	Acc: 20.9% (2087/10000)
[Test]  Epoch: 23	Loss: 0.052175	Acc: 20.7% (2071/10000)
[Test]  Epoch: 24	Loss: 0.052172	Acc: 20.7% (2066/10000)
[Test]  Epoch: 25	Loss: 0.052337	Acc: 20.6% (2065/10000)
[Test]  Epoch: 26	Loss: 0.052315	Acc: 20.8% (2078/10000)
[Test]  Epoch: 27	Loss: 0.051792	Acc: 21.4% (2139/10000)
[Test]  Epoch: 28	Loss: 0.051921	Acc: 21.1% (2107/10000)
[Test]  Epoch: 29	Loss: 0.052285	Acc: 21.0% (2103/10000)
[Test]  Epoch: 30	Loss: 0.051638	Acc: 21.9% (2194/10000)
[Test]  Epoch: 31	Loss: 0.051432	Acc: 21.8% (2179/10000)
[Test]  Epoch: 32	Loss: 0.051741	Acc: 21.2% (2119/10000)
[Test]  Epoch: 33	Loss: 0.052019	Acc: 21.1% (2113/10000)
[Test]  Epoch: 34	Loss: 0.051743	Acc: 21.4% (2145/10000)
[Test]  Epoch: 35	Loss: 0.051616	Acc: 21.0% (2104/10000)
[Test]  Epoch: 36	Loss: 0.051694	Acc: 21.1% (2111/10000)
[Test]  Epoch: 37	Loss: 0.051382	Acc: 21.6% (2161/10000)
[Test]  Epoch: 38	Loss: 0.051879	Acc: 21.3% (2130/10000)
[Test]  Epoch: 39	Loss: 0.051304	Acc: 21.3% (2133/10000)
[Test]  Epoch: 40	Loss: 0.051534	Acc: 21.1% (2109/10000)
[Test]  Epoch: 41	Loss: 0.051458	Acc: 21.5% (2152/10000)
[Test]  Epoch: 42	Loss: 0.051706	Acc: 21.5% (2151/10000)
[Test]  Epoch: 43	Loss: 0.051558	Acc: 21.3% (2127/10000)
[Test]  Epoch: 44	Loss: 0.051074	Acc: 22.0% (2196/10000)
[Test]  Epoch: 45	Loss: 0.051384	Acc: 21.8% (2176/10000)
[Test]  Epoch: 46	Loss: 0.051106	Acc: 21.7% (2173/10000)
[Test]  Epoch: 47	Loss: 0.050915	Acc: 21.9% (2185/10000)
[Test]  Epoch: 48	Loss: 0.051149	Acc: 22.2% (2216/10000)
[Test]  Epoch: 49	Loss: 0.051059	Acc: 21.8% (2180/10000)
[Test]  Epoch: 50	Loss: 0.051087	Acc: 22.3% (2229/10000)
[Test]  Epoch: 51	Loss: 0.050997	Acc: 22.0% (2198/10000)
[Test]  Epoch: 52	Loss: 0.050971	Acc: 21.6% (2164/10000)
[Test]  Epoch: 53	Loss: 0.051004	Acc: 21.8% (2184/10000)
[Test]  Epoch: 54	Loss: 0.050657	Acc: 22.4% (2243/10000)
[Test]  Epoch: 55	Loss: 0.050742	Acc: 22.1% (2213/10000)
[Test]  Epoch: 56	Loss: 0.051034	Acc: 21.9% (2192/10000)
[Test]  Epoch: 57	Loss: 0.050649	Acc: 21.9% (2188/10000)
[Test]  Epoch: 58	Loss: 0.050740	Acc: 21.9% (2185/10000)
[Test]  Epoch: 59	Loss: 0.050545	Acc: 22.3% (2234/10000)
[Test]  Epoch: 60	Loss: 0.050765	Acc: 21.9% (2188/10000)
[Test]  Epoch: 61	Loss: 0.050551	Acc: 22.3% (2230/10000)
[Test]  Epoch: 62	Loss: 0.050482	Acc: 22.4% (2238/10000)
[Test]  Epoch: 63	Loss: 0.050464	Acc: 22.3% (2232/10000)
[Test]  Epoch: 64	Loss: 0.050454	Acc: 22.4% (2237/10000)
[Test]  Epoch: 65	Loss: 0.050430	Acc: 22.6% (2259/10000)
[Test]  Epoch: 66	Loss: 0.050401	Acc: 22.4% (2239/10000)
[Test]  Epoch: 67	Loss: 0.050442	Acc: 22.3% (2228/10000)
[Test]  Epoch: 68	Loss: 0.050417	Acc: 22.2% (2222/10000)
[Test]  Epoch: 69	Loss: 0.050373	Acc: 22.4% (2237/10000)
[Test]  Epoch: 70	Loss: 0.050382	Acc: 22.1% (2209/10000)
[Test]  Epoch: 71	Loss: 0.050366	Acc: 22.3% (2229/10000)
[Test]  Epoch: 72	Loss: 0.050377	Acc: 22.3% (2226/10000)
[Test]  Epoch: 73	Loss: 0.050380	Acc: 22.3% (2228/10000)
[Test]  Epoch: 74	Loss: 0.050280	Acc: 22.4% (2242/10000)
[Test]  Epoch: 75	Loss: 0.050314	Acc: 22.3% (2230/10000)
[Test]  Epoch: 76	Loss: 0.050366	Acc: 22.4% (2235/10000)
[Test]  Epoch: 77	Loss: 0.050338	Acc: 22.3% (2233/10000)
[Test]  Epoch: 78	Loss: 0.050355	Acc: 22.3% (2233/10000)
[Test]  Epoch: 79	Loss: 0.050347	Acc: 22.1% (2207/10000)
[Test]  Epoch: 80	Loss: 0.050306	Acc: 22.4% (2240/10000)
[Test]  Epoch: 81	Loss: 0.050241	Acc: 22.5% (2248/10000)
[Test]  Epoch: 82	Loss: 0.050273	Acc: 22.5% (2247/10000)
[Test]  Epoch: 83	Loss: 0.050270	Acc: 22.4% (2238/10000)
[Test]  Epoch: 84	Loss: 0.050348	Acc: 22.3% (2231/10000)
[Test]  Epoch: 85	Loss: 0.050263	Acc: 22.3% (2230/10000)
[Test]  Epoch: 86	Loss: 0.050283	Acc: 22.4% (2236/10000)
[Test]  Epoch: 87	Loss: 0.050307	Acc: 22.4% (2237/10000)
[Test]  Epoch: 88	Loss: 0.050369	Acc: 22.4% (2237/10000)
[Test]  Epoch: 89	Loss: 0.050301	Acc: 22.3% (2233/10000)
[Test]  Epoch: 90	Loss: 0.050231	Acc: 22.5% (2251/10000)
[Test]  Epoch: 91	Loss: 0.050242	Acc: 22.6% (2260/10000)
[Test]  Epoch: 92	Loss: 0.050231	Acc: 22.4% (2239/10000)
[Test]  Epoch: 93	Loss: 0.050324	Acc: 22.4% (2242/10000)
[Test]  Epoch: 94	Loss: 0.050254	Acc: 22.6% (2259/10000)
[Test]  Epoch: 95	Loss: 0.050359	Acc: 22.4% (2238/10000)
[Test]  Epoch: 96	Loss: 0.050347	Acc: 22.5% (2254/10000)
[Test]  Epoch: 97	Loss: 0.050280	Acc: 22.4% (2236/10000)
[Test]  Epoch: 98	Loss: 0.050238	Acc: 22.5% (2254/10000)
[Test]  Epoch: 99	Loss: 0.050332	Acc: 22.4% (2237/10000)
[Test]  Epoch: 100	Loss: 0.050320	Acc: 22.4% (2238/10000)
===========finish==========
['2024-08-19', '00:21:36.209926', '100', 'test', '0.05032044668197632', '22.38', '22.6']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.082508	Acc: 7.1% (710/10000)
[Test]  Epoch: 2	Loss: 0.073940	Acc: 8.4% (842/10000)
[Test]  Epoch: 3	Loss: 0.060169	Acc: 13.6% (1359/10000)
[Test]  Epoch: 4	Loss: 0.057170	Acc: 15.9% (1594/10000)
[Test]  Epoch: 5	Loss: 0.057951	Acc: 15.5% (1551/10000)
[Test]  Epoch: 6	Loss: 0.057417	Acc: 16.1% (1606/10000)
[Test]  Epoch: 7	Loss: 0.056182	Acc: 17.3% (1731/10000)
[Test]  Epoch: 8	Loss: 0.055258	Acc: 17.7% (1769/10000)
[Test]  Epoch: 9	Loss: 0.056713	Acc: 16.4% (1639/10000)
[Test]  Epoch: 10	Loss: 0.054351	Acc: 18.5% (1852/10000)
[Test]  Epoch: 11	Loss: 0.054626	Acc: 18.4% (1839/10000)
[Test]  Epoch: 12	Loss: 0.054079	Acc: 19.1% (1910/10000)
[Test]  Epoch: 13	Loss: 0.055386	Acc: 17.8% (1775/10000)
[Test]  Epoch: 14	Loss: 0.053426	Acc: 19.5% (1949/10000)
[Test]  Epoch: 15	Loss: 0.055815	Acc: 17.7% (1771/10000)
[Test]  Epoch: 16	Loss: 0.053493	Acc: 19.5% (1949/10000)
[Test]  Epoch: 17	Loss: 0.052792	Acc: 20.0% (1997/10000)
[Test]  Epoch: 18	Loss: 0.054028	Acc: 19.0% (1898/10000)
[Test]  Epoch: 19	Loss: 0.053384	Acc: 19.2% (1918/10000)
[Test]  Epoch: 20	Loss: 0.053132	Acc: 19.4% (1944/10000)
[Test]  Epoch: 21	Loss: 0.053922	Acc: 19.6% (1957/10000)
[Test]  Epoch: 22	Loss: 0.053161	Acc: 20.2% (2020/10000)
[Test]  Epoch: 23	Loss: 0.052886	Acc: 20.2% (2021/10000)
[Test]  Epoch: 24	Loss: 0.052442	Acc: 20.3% (2034/10000)
[Test]  Epoch: 25	Loss: 0.052855	Acc: 20.5% (2048/10000)
[Test]  Epoch: 26	Loss: 0.052580	Acc: 20.2% (2018/10000)
[Test]  Epoch: 27	Loss: 0.054385	Acc: 18.7% (1871/10000)
[Test]  Epoch: 28	Loss: 0.052764	Acc: 20.2% (2016/10000)
[Test]  Epoch: 29	Loss: 0.052823	Acc: 20.1% (2013/10000)
[Test]  Epoch: 30	Loss: 0.052878	Acc: 20.2% (2021/10000)
[Test]  Epoch: 31	Loss: 0.052957	Acc: 20.1% (2007/10000)
[Test]  Epoch: 32	Loss: 0.052292	Acc: 20.9% (2088/10000)
[Test]  Epoch: 33	Loss: 0.052569	Acc: 20.2% (2023/10000)
[Test]  Epoch: 34	Loss: 0.052187	Acc: 20.9% (2089/10000)
[Test]  Epoch: 35	Loss: 0.052500	Acc: 20.4% (2040/10000)
[Test]  Epoch: 36	Loss: 0.052366	Acc: 20.8% (2081/10000)
[Test]  Epoch: 37	Loss: 0.052188	Acc: 20.8% (2075/10000)
[Test]  Epoch: 38	Loss: 0.052026	Acc: 21.0% (2097/10000)
[Test]  Epoch: 39	Loss: 0.051614	Acc: 21.3% (2134/10000)
[Test]  Epoch: 40	Loss: 0.052391	Acc: 20.5% (2050/10000)
[Test]  Epoch: 41	Loss: 0.051918	Acc: 21.0% (2097/10000)
[Test]  Epoch: 42	Loss: 0.052082	Acc: 20.7% (2074/10000)
[Test]  Epoch: 43	Loss: 0.052126	Acc: 20.6% (2062/10000)
[Test]  Epoch: 44	Loss: 0.051914	Acc: 20.9% (2093/10000)
[Test]  Epoch: 45	Loss: 0.051756	Acc: 21.1% (2105/10000)
[Test]  Epoch: 46	Loss: 0.051781	Acc: 20.7% (2073/10000)
[Test]  Epoch: 47	Loss: 0.051996	Acc: 20.9% (2087/10000)
[Test]  Epoch: 48	Loss: 0.051461	Acc: 21.4% (2136/10000)
[Test]  Epoch: 49	Loss: 0.051879	Acc: 21.0% (2102/10000)
[Test]  Epoch: 50	Loss: 0.051537	Acc: 20.9% (2087/10000)
[Test]  Epoch: 51	Loss: 0.051367	Acc: 21.3% (2127/10000)
[Test]  Epoch: 52	Loss: 0.051399	Acc: 21.2% (2121/10000)
[Test]  Epoch: 53	Loss: 0.051365	Acc: 21.3% (2126/10000)
[Test]  Epoch: 54	Loss: 0.051153	Acc: 21.6% (2158/10000)
[Test]  Epoch: 55	Loss: 0.051332	Acc: 21.4% (2139/10000)
[Test]  Epoch: 56	Loss: 0.051039	Acc: 21.7% (2170/10000)
[Test]  Epoch: 57	Loss: 0.051464	Acc: 21.0% (2101/10000)
[Test]  Epoch: 58	Loss: 0.051673	Acc: 21.1% (2111/10000)
[Test]  Epoch: 59	Loss: 0.051119	Acc: 21.5% (2149/10000)
[Test]  Epoch: 60	Loss: 0.051816	Acc: 21.3% (2128/10000)
[Test]  Epoch: 61	Loss: 0.051160	Acc: 21.6% (2164/10000)
[Test]  Epoch: 62	Loss: 0.050969	Acc: 21.8% (2178/10000)
[Test]  Epoch: 63	Loss: 0.050897	Acc: 21.9% (2187/10000)
[Test]  Epoch: 64	Loss: 0.050861	Acc: 21.8% (2176/10000)
[Test]  Epoch: 65	Loss: 0.050825	Acc: 22.0% (2204/10000)
[Test]  Epoch: 66	Loss: 0.050831	Acc: 22.0% (2201/10000)
[Test]  Epoch: 67	Loss: 0.050868	Acc: 21.7% (2172/10000)
[Test]  Epoch: 68	Loss: 0.050868	Acc: 21.9% (2185/10000)
[Test]  Epoch: 69	Loss: 0.050726	Acc: 22.0% (2201/10000)
[Test]  Epoch: 70	Loss: 0.050825	Acc: 21.8% (2183/10000)
[Test]  Epoch: 71	Loss: 0.050716	Acc: 22.2% (2220/10000)
[Test]  Epoch: 72	Loss: 0.050759	Acc: 22.1% (2212/10000)
[Test]  Epoch: 73	Loss: 0.050762	Acc: 22.1% (2205/10000)
[Test]  Epoch: 74	Loss: 0.050706	Acc: 21.9% (2192/10000)
[Test]  Epoch: 75	Loss: 0.050716	Acc: 21.9% (2192/10000)
[Test]  Epoch: 76	Loss: 0.050804	Acc: 21.9% (2190/10000)
[Test]  Epoch: 77	Loss: 0.050697	Acc: 21.9% (2190/10000)
[Test]  Epoch: 78	Loss: 0.050796	Acc: 22.1% (2208/10000)
[Test]  Epoch: 79	Loss: 0.050783	Acc: 21.9% (2187/10000)
[Test]  Epoch: 80	Loss: 0.050745	Acc: 22.1% (2205/10000)
[Test]  Epoch: 81	Loss: 0.050715	Acc: 22.0% (2196/10000)
[Test]  Epoch: 82	Loss: 0.050734	Acc: 21.9% (2193/10000)
[Test]  Epoch: 83	Loss: 0.050723	Acc: 21.8% (2182/10000)
[Test]  Epoch: 84	Loss: 0.050781	Acc: 21.8% (2175/10000)
[Test]  Epoch: 85	Loss: 0.050721	Acc: 22.0% (2197/10000)
[Test]  Epoch: 86	Loss: 0.050707	Acc: 22.0% (2196/10000)
[Test]  Epoch: 87	Loss: 0.050740	Acc: 21.9% (2187/10000)
[Test]  Epoch: 88	Loss: 0.050796	Acc: 21.9% (2192/10000)
[Test]  Epoch: 89	Loss: 0.050773	Acc: 22.0% (2199/10000)
[Test]  Epoch: 90	Loss: 0.050793	Acc: 21.8% (2182/10000)
[Test]  Epoch: 91	Loss: 0.050738	Acc: 21.8% (2183/10000)
[Test]  Epoch: 92	Loss: 0.050689	Acc: 22.2% (2224/10000)
[Test]  Epoch: 93	Loss: 0.050633	Acc: 22.1% (2211/10000)
[Test]  Epoch: 94	Loss: 0.050600	Acc: 21.9% (2190/10000)
[Test]  Epoch: 95	Loss: 0.050722	Acc: 21.9% (2193/10000)
[Test]  Epoch: 96	Loss: 0.050727	Acc: 22.0% (2202/10000)
[Test]  Epoch: 97	Loss: 0.050741	Acc: 21.9% (2195/10000)
[Test]  Epoch: 98	Loss: 0.050641	Acc: 21.9% (2193/10000)
[Test]  Epoch: 99	Loss: 0.050728	Acc: 22.0% (2198/10000)
[Test]  Epoch: 100	Loss: 0.050708	Acc: 21.7% (2170/10000)
===========finish==========
['2024-08-19', '00:23:40.770942', '100', 'test', '0.05070827631950378', '21.7', '22.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.2.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.5.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.148754	Acc: 1.7% (171/10000)
[Test]  Epoch: 2	Loss: 0.076062	Acc: 6.7% (670/10000)
[Test]  Epoch: 3	Loss: 0.067888	Acc: 9.3% (929/10000)
[Test]  Epoch: 4	Loss: 0.062067	Acc: 12.0% (1196/10000)
[Test]  Epoch: 5	Loss: 0.058878	Acc: 13.3% (1329/10000)
[Test]  Epoch: 6	Loss: 0.063367	Acc: 12.6% (1264/10000)
[Test]  Epoch: 7	Loss: 0.057853	Acc: 14.7% (1468/10000)
[Test]  Epoch: 8	Loss: 0.059272	Acc: 14.7% (1465/10000)
[Test]  Epoch: 9	Loss: 0.056913	Acc: 16.4% (1637/10000)
[Test]  Epoch: 10	Loss: 0.056105	Acc: 16.8% (1676/10000)
[Test]  Epoch: 11	Loss: 0.057536	Acc: 16.3% (1630/10000)
[Test]  Epoch: 12	Loss: 0.056497	Acc: 17.4% (1738/10000)
[Test]  Epoch: 13	Loss: 0.056096	Acc: 17.1% (1707/10000)
[Test]  Epoch: 14	Loss: 0.055984	Acc: 17.3% (1733/10000)
[Test]  Epoch: 15	Loss: 0.055739	Acc: 17.6% (1762/10000)
[Test]  Epoch: 16	Loss: 0.056585	Acc: 16.8% (1677/10000)
[Test]  Epoch: 17	Loss: 0.056176	Acc: 17.4% (1735/10000)
[Test]  Epoch: 18	Loss: 0.055543	Acc: 17.2% (1717/10000)
[Test]  Epoch: 19	Loss: 0.055725	Acc: 17.9% (1788/10000)
[Test]  Epoch: 20	Loss: 0.055905	Acc: 17.6% (1755/10000)
[Test]  Epoch: 21	Loss: 0.054626	Acc: 18.5% (1846/10000)
[Test]  Epoch: 22	Loss: 0.055585	Acc: 17.5% (1754/10000)
[Test]  Epoch: 23	Loss: 0.054454	Acc: 17.9% (1790/10000)
[Test]  Epoch: 24	Loss: 0.054028	Acc: 18.6% (1863/10000)
[Test]  Epoch: 25	Loss: 0.054649	Acc: 18.1% (1815/10000)
[Test]  Epoch: 26	Loss: 0.054092	Acc: 18.3% (1833/10000)
[Test]  Epoch: 27	Loss: 0.054571	Acc: 18.2% (1819/10000)
[Test]  Epoch: 28	Loss: 0.054352	Acc: 18.5% (1850/10000)
[Test]  Epoch: 29	Loss: 0.054595	Acc: 18.6% (1860/10000)
[Test]  Epoch: 30	Loss: 0.053889	Acc: 18.3% (1832/10000)
[Test]  Epoch: 31	Loss: 0.054345	Acc: 18.6% (1859/10000)
[Test]  Epoch: 32	Loss: 0.053741	Acc: 19.1% (1905/10000)
[Test]  Epoch: 33	Loss: 0.053638	Acc: 18.7% (1872/10000)
[Test]  Epoch: 34	Loss: 0.054263	Acc: 18.2% (1822/10000)
[Test]  Epoch: 35	Loss: 0.053614	Acc: 19.0% (1899/10000)
[Test]  Epoch: 36	Loss: 0.053688	Acc: 18.3% (1826/10000)
[Test]  Epoch: 37	Loss: 0.053338	Acc: 19.0% (1896/10000)
[Test]  Epoch: 38	Loss: 0.053294	Acc: 19.3% (1933/10000)
[Test]  Epoch: 39	Loss: 0.053593	Acc: 19.2% (1919/10000)
[Test]  Epoch: 40	Loss: 0.053136	Acc: 19.2% (1924/10000)
[Test]  Epoch: 41	Loss: 0.053503	Acc: 19.1% (1914/10000)
[Test]  Epoch: 42	Loss: 0.053631	Acc: 18.6% (1855/10000)
[Test]  Epoch: 43	Loss: 0.052926	Acc: 19.5% (1952/10000)
[Test]  Epoch: 44	Loss: 0.053250	Acc: 19.2% (1920/10000)
[Test]  Epoch: 45	Loss: 0.053453	Acc: 19.2% (1919/10000)
[Test]  Epoch: 46	Loss: 0.053187	Acc: 19.1% (1912/10000)
[Test]  Epoch: 47	Loss: 0.052868	Acc: 19.6% (1963/10000)
[Test]  Epoch: 48	Loss: 0.052818	Acc: 19.7% (1970/10000)
[Test]  Epoch: 49	Loss: 0.052685	Acc: 19.7% (1968/10000)
[Test]  Epoch: 50	Loss: 0.052963	Acc: 19.2% (1920/10000)
[Test]  Epoch: 51	Loss: 0.053647	Acc: 19.1% (1906/10000)
[Test]  Epoch: 52	Loss: 0.052801	Acc: 19.4% (1937/10000)
[Test]  Epoch: 53	Loss: 0.052958	Acc: 19.3% (1930/10000)
[Test]  Epoch: 54	Loss: 0.052789	Acc: 19.4% (1942/10000)
[Test]  Epoch: 55	Loss: 0.052697	Acc: 19.6% (1963/10000)
[Test]  Epoch: 56	Loss: 0.052710	Acc: 19.6% (1955/10000)
[Test]  Epoch: 57	Loss: 0.052545	Acc: 19.6% (1962/10000)
[Test]  Epoch: 58	Loss: 0.052854	Acc: 19.3% (1931/10000)
[Test]  Epoch: 59	Loss: 0.052998	Acc: 19.2% (1921/10000)
[Test]  Epoch: 60	Loss: 0.053258	Acc: 19.1% (1915/10000)
[Test]  Epoch: 61	Loss: 0.052493	Acc: 19.6% (1960/10000)
[Test]  Epoch: 62	Loss: 0.052249	Acc: 19.8% (1977/10000)
[Test]  Epoch: 63	Loss: 0.052124	Acc: 19.9% (1986/10000)
[Test]  Epoch: 64	Loss: 0.052158	Acc: 20.0% (2000/10000)
[Test]  Epoch: 65	Loss: 0.052067	Acc: 20.2% (2016/10000)
[Test]  Epoch: 66	Loss: 0.052035	Acc: 20.1% (2009/10000)
[Test]  Epoch: 67	Loss: 0.052097	Acc: 20.2% (2019/10000)
[Test]  Epoch: 68	Loss: 0.052047	Acc: 20.1% (2012/10000)
[Test]  Epoch: 69	Loss: 0.051980	Acc: 20.2% (2017/10000)
[Test]  Epoch: 70	Loss: 0.052028	Acc: 20.1% (2013/10000)
[Test]  Epoch: 71	Loss: 0.052000	Acc: 20.1% (2015/10000)
[Test]  Epoch: 72	Loss: 0.052023	Acc: 20.2% (2020/10000)
[Test]  Epoch: 73	Loss: 0.052096	Acc: 20.1% (2011/10000)
[Test]  Epoch: 74	Loss: 0.051991	Acc: 20.2% (2022/10000)
[Test]  Epoch: 75	Loss: 0.051983	Acc: 20.1% (2014/10000)
[Test]  Epoch: 76	Loss: 0.052019	Acc: 20.1% (2013/10000)
[Test]  Epoch: 77	Loss: 0.052012	Acc: 20.2% (2022/10000)
[Test]  Epoch: 78	Loss: 0.052113	Acc: 20.0% (2001/10000)
[Test]  Epoch: 79	Loss: 0.052047	Acc: 20.0% (1996/10000)
[Test]  Epoch: 80	Loss: 0.052004	Acc: 20.1% (2008/10000)
[Test]  Epoch: 81	Loss: 0.051980	Acc: 20.2% (2020/10000)
[Test]  Epoch: 82	Loss: 0.051970	Acc: 20.4% (2040/10000)
[Test]  Epoch: 83	Loss: 0.051949	Acc: 20.1% (2012/10000)
[Test]  Epoch: 84	Loss: 0.052016	Acc: 20.0% (2002/10000)
[Test]  Epoch: 85	Loss: 0.051944	Acc: 20.3% (2026/10000)
[Test]  Epoch: 86	Loss: 0.051977	Acc: 20.1% (2011/10000)
[Test]  Epoch: 87	Loss: 0.052072	Acc: 20.1% (2014/10000)
[Test]  Epoch: 88	Loss: 0.052081	Acc: 19.9% (1987/10000)
[Test]  Epoch: 89	Loss: 0.052044	Acc: 20.2% (2019/10000)
[Test]  Epoch: 90	Loss: 0.052044	Acc: 20.0% (2004/10000)
[Test]  Epoch: 91	Loss: 0.051995	Acc: 20.1% (2014/10000)
[Test]  Epoch: 92	Loss: 0.051939	Acc: 20.2% (2024/10000)
[Test]  Epoch: 93	Loss: 0.051966	Acc: 20.0% (2004/10000)
[Test]  Epoch: 94	Loss: 0.051904	Acc: 20.2% (2017/10000)
[Test]  Epoch: 95	Loss: 0.052021	Acc: 20.1% (2005/10000)
[Test]  Epoch: 96	Loss: 0.051904	Acc: 20.0% (1998/10000)
[Test]  Epoch: 97	Loss: 0.051991	Acc: 20.1% (2013/10000)
[Test]  Epoch: 98	Loss: 0.051878	Acc: 20.2% (2016/10000)
[Test]  Epoch: 99	Loss: 0.051968	Acc: 20.2% (2020/10000)
[Test]  Epoch: 100	Loss: 0.051987	Acc: 20.1% (2006/10000)
===========finish==========
['2024-08-19', '00:26:03.883387', '100', 'test', '0.051986923193931577', '20.06', '20.4']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.2.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.5.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.9.conv.3.weight', '_features.16.conv.0.1.weight', '_features.7.conv.0.1.weight', '_features.0.0.weight', '_features.15.conv.3.weight', '_features.16.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.11.conv.3.weight', '_features.2.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.1.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.134887	Acc: 1.2% (119/10000)
[Test]  Epoch: 2	Loss: 0.070331	Acc: 5.9% (588/10000)
[Test]  Epoch: 3	Loss: 0.065589	Acc: 7.9% (788/10000)
[Test]  Epoch: 4	Loss: 0.064358	Acc: 8.3% (826/10000)
[Test]  Epoch: 5	Loss: 0.062018	Acc: 10.6% (1058/10000)
[Test]  Epoch: 6	Loss: 0.062838	Acc: 10.4% (1043/10000)
[Test]  Epoch: 7	Loss: 0.062174	Acc: 11.5% (1149/10000)
[Test]  Epoch: 8	Loss: 0.061833	Acc: 11.4% (1145/10000)
[Test]  Epoch: 9	Loss: 0.062023	Acc: 11.6% (1155/10000)
[Test]  Epoch: 10	Loss: 0.060617	Acc: 12.5% (1249/10000)
[Test]  Epoch: 11	Loss: 0.062191	Acc: 12.0% (1196/10000)
[Test]  Epoch: 12	Loss: 0.060447	Acc: 12.6% (1260/10000)
[Test]  Epoch: 13	Loss: 0.062439	Acc: 12.1% (1208/10000)
[Test]  Epoch: 14	Loss: 0.060516	Acc: 13.4% (1336/10000)
[Test]  Epoch: 15	Loss: 0.061792	Acc: 12.7% (1274/10000)
[Test]  Epoch: 16	Loss: 0.061968	Acc: 12.8% (1278/10000)
[Test]  Epoch: 17	Loss: 0.060895	Acc: 13.0% (1303/10000)
[Test]  Epoch: 18	Loss: 0.060492	Acc: 13.9% (1387/10000)
[Test]  Epoch: 19	Loss: 0.060687	Acc: 13.0% (1304/10000)
[Test]  Epoch: 20	Loss: 0.060460	Acc: 13.3% (1332/10000)
[Test]  Epoch: 21	Loss: 0.059993	Acc: 13.9% (1386/10000)
[Test]  Epoch: 22	Loss: 0.059555	Acc: 14.3% (1428/10000)
[Test]  Epoch: 23	Loss: 0.059537	Acc: 14.4% (1436/10000)
[Test]  Epoch: 24	Loss: 0.059941	Acc: 13.8% (1378/10000)
[Test]  Epoch: 25	Loss: 0.060013	Acc: 14.1% (1408/10000)
[Test]  Epoch: 26	Loss: 0.059334	Acc: 14.1% (1409/10000)
[Test]  Epoch: 27	Loss: 0.059970	Acc: 14.2% (1416/10000)
[Test]  Epoch: 28	Loss: 0.059627	Acc: 14.2% (1424/10000)
[Test]  Epoch: 29	Loss: 0.060345	Acc: 14.2% (1419/10000)
[Test]  Epoch: 30	Loss: 0.059507	Acc: 14.1% (1412/10000)
[Test]  Epoch: 31	Loss: 0.060340	Acc: 13.6% (1364/10000)
[Test]  Epoch: 32	Loss: 0.059215	Acc: 14.0% (1399/10000)
[Test]  Epoch: 33	Loss: 0.059197	Acc: 14.5% (1446/10000)
[Test]  Epoch: 34	Loss: 0.059255	Acc: 14.1% (1412/10000)
[Test]  Epoch: 35	Loss: 0.059728	Acc: 13.9% (1387/10000)
[Test]  Epoch: 36	Loss: 0.059082	Acc: 14.4% (1444/10000)
[Test]  Epoch: 37	Loss: 0.058791	Acc: 14.3% (1432/10000)
[Test]  Epoch: 38	Loss: 0.058762	Acc: 14.7% (1465/10000)
[Test]  Epoch: 39	Loss: 0.059356	Acc: 14.6% (1459/10000)
[Test]  Epoch: 40	Loss: 0.058580	Acc: 14.5% (1454/10000)
[Test]  Epoch: 41	Loss: 0.059045	Acc: 14.1% (1414/10000)
[Test]  Epoch: 42	Loss: 0.059143	Acc: 14.8% (1478/10000)
[Test]  Epoch: 43	Loss: 0.058159	Acc: 14.7% (1466/10000)
[Test]  Epoch: 44	Loss: 0.058532	Acc: 14.8% (1475/10000)
[Test]  Epoch: 45	Loss: 0.058146	Acc: 15.1% (1512/10000)
[Test]  Epoch: 46	Loss: 0.058287	Acc: 15.1% (1506/10000)
[Test]  Epoch: 47	Loss: 0.058372	Acc: 15.0% (1500/10000)
[Test]  Epoch: 48	Loss: 0.058579	Acc: 14.9% (1488/10000)
[Test]  Epoch: 49	Loss: 0.058475	Acc: 14.7% (1474/10000)
[Test]  Epoch: 50	Loss: 0.058312	Acc: 14.8% (1483/10000)
[Test]  Epoch: 51	Loss: 0.058373	Acc: 14.6% (1463/10000)
[Test]  Epoch: 52	Loss: 0.059000	Acc: 14.3% (1435/10000)
[Test]  Epoch: 53	Loss: 0.058513	Acc: 14.6% (1463/10000)
[Test]  Epoch: 54	Loss: 0.057749	Acc: 15.1% (1512/10000)
[Test]  Epoch: 55	Loss: 0.058129	Acc: 14.7% (1471/10000)
[Test]  Epoch: 56	Loss: 0.057941	Acc: 14.8% (1477/10000)
[Test]  Epoch: 57	Loss: 0.057767	Acc: 15.2% (1515/10000)
[Test]  Epoch: 58	Loss: 0.057690	Acc: 14.8% (1483/10000)
[Test]  Epoch: 59	Loss: 0.058224	Acc: 14.6% (1460/10000)
[Test]  Epoch: 60	Loss: 0.059785	Acc: 13.3% (1335/10000)
[Test]  Epoch: 61	Loss: 0.057989	Acc: 15.0% (1496/10000)
[Test]  Epoch: 62	Loss: 0.057704	Acc: 15.3% (1528/10000)
[Test]  Epoch: 63	Loss: 0.057624	Acc: 15.5% (1550/10000)
[Test]  Epoch: 64	Loss: 0.057551	Acc: 15.4% (1545/10000)
[Test]  Epoch: 65	Loss: 0.057300	Acc: 15.6% (1558/10000)
[Test]  Epoch: 66	Loss: 0.057323	Acc: 15.4% (1541/10000)
[Test]  Epoch: 67	Loss: 0.057475	Acc: 15.3% (1531/10000)
[Test]  Epoch: 68	Loss: 0.057464	Acc: 15.4% (1542/10000)
[Test]  Epoch: 69	Loss: 0.057372	Acc: 15.3% (1527/10000)
[Test]  Epoch: 70	Loss: 0.057506	Acc: 15.4% (1540/10000)
[Test]  Epoch: 71	Loss: 0.057418	Acc: 15.2% (1523/10000)
[Test]  Epoch: 72	Loss: 0.057411	Acc: 15.4% (1540/10000)
[Test]  Epoch: 73	Loss: 0.057501	Acc: 15.0% (1503/10000)
[Test]  Epoch: 74	Loss: 0.057328	Acc: 15.2% (1525/10000)
[Test]  Epoch: 75	Loss: 0.057318	Acc: 15.3% (1531/10000)
[Test]  Epoch: 76	Loss: 0.057425	Acc: 15.2% (1525/10000)
[Test]  Epoch: 77	Loss: 0.057324	Acc: 15.3% (1528/10000)
[Test]  Epoch: 78	Loss: 0.057381	Acc: 15.3% (1528/10000)
[Test]  Epoch: 79	Loss: 0.057405	Acc: 15.2% (1517/10000)
[Test]  Epoch: 80	Loss: 0.057440	Acc: 15.0% (1504/10000)
[Test]  Epoch: 81	Loss: 0.057180	Acc: 15.3% (1534/10000)
[Test]  Epoch: 82	Loss: 0.057268	Acc: 15.2% (1520/10000)
[Test]  Epoch: 83	Loss: 0.057315	Acc: 15.1% (1513/10000)
[Test]  Epoch: 84	Loss: 0.057355	Acc: 15.2% (1522/10000)
[Test]  Epoch: 85	Loss: 0.057250	Acc: 15.4% (1539/10000)
[Test]  Epoch: 86	Loss: 0.057405	Acc: 15.2% (1524/10000)
[Test]  Epoch: 87	Loss: 0.057594	Acc: 15.0% (1504/10000)
[Test]  Epoch: 88	Loss: 0.057431	Acc: 15.2% (1521/10000)
[Test]  Epoch: 89	Loss: 0.057369	Acc: 15.4% (1540/10000)
[Test]  Epoch: 90	Loss: 0.057372	Acc: 15.5% (1547/10000)
[Test]  Epoch: 91	Loss: 0.057397	Acc: 15.2% (1521/10000)
[Test]  Epoch: 92	Loss: 0.057344	Acc: 15.2% (1516/10000)
[Test]  Epoch: 93	Loss: 0.057409	Acc: 15.2% (1520/10000)
[Test]  Epoch: 94	Loss: 0.057291	Acc: 15.3% (1533/10000)
[Test]  Epoch: 95	Loss: 0.057571	Acc: 15.2% (1524/10000)
[Test]  Epoch: 96	Loss: 0.057424	Acc: 15.2% (1521/10000)
[Test]  Epoch: 97	Loss: 0.057382	Acc: 15.2% (1523/10000)
[Test]  Epoch: 98	Loss: 0.057302	Acc: 15.4% (1538/10000)
[Test]  Epoch: 99	Loss: 0.057403	Acc: 15.2% (1519/10000)
[Test]  Epoch: 100	Loss: 0.057316	Acc: 15.5% (1546/10000)
===========finish==========
['2024-08-19', '00:28:32.716663', '100', 'test', '0.05731593556404114', '15.46', '15.58']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.2.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.5.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.9.conv.3.weight', '_features.16.conv.0.1.weight', '_features.7.conv.0.1.weight', '_features.0.0.weight', '_features.15.conv.3.weight', '_features.16.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.11.conv.3.weight', '_features.2.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.10.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.15.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.319409	Acc: 1.4% (142/10000)
[Test]  Epoch: 2	Loss: 0.074874	Acc: 6.6% (664/10000)
[Test]  Epoch: 3	Loss: 0.066171	Acc: 9.0% (899/10000)
[Test]  Epoch: 4	Loss: 0.061981	Acc: 10.9% (1089/10000)
[Test]  Epoch: 5	Loss: 0.062732	Acc: 10.8% (1084/10000)
[Test]  Epoch: 6	Loss: 0.061828	Acc: 11.2% (1117/10000)
[Test]  Epoch: 7	Loss: 0.060455	Acc: 12.0% (1201/10000)
[Test]  Epoch: 8	Loss: 0.061430	Acc: 12.3% (1233/10000)
[Test]  Epoch: 9	Loss: 0.060532	Acc: 13.4% (1344/10000)
[Test]  Epoch: 10	Loss: 0.060874	Acc: 12.8% (1278/10000)
[Test]  Epoch: 11	Loss: 0.060816	Acc: 13.6% (1361/10000)
[Test]  Epoch: 12	Loss: 0.061204	Acc: 12.9% (1294/10000)
[Test]  Epoch: 13	Loss: 0.060601	Acc: 13.0% (1296/10000)
[Test]  Epoch: 14	Loss: 0.060745	Acc: 13.7% (1371/10000)
[Test]  Epoch: 15	Loss: 0.060652	Acc: 13.7% (1370/10000)
[Test]  Epoch: 16	Loss: 0.060840	Acc: 13.3% (1334/10000)
[Test]  Epoch: 17	Loss: 0.059939	Acc: 14.2% (1423/10000)
[Test]  Epoch: 18	Loss: 0.059434	Acc: 14.4% (1437/10000)
[Test]  Epoch: 19	Loss: 0.059889	Acc: 14.5% (1449/10000)
[Test]  Epoch: 20	Loss: 0.059308	Acc: 15.0% (1496/10000)
[Test]  Epoch: 21	Loss: 0.058791	Acc: 15.4% (1539/10000)
[Test]  Epoch: 22	Loss: 0.058610	Acc: 15.2% (1520/10000)
[Test]  Epoch: 23	Loss: 0.059107	Acc: 14.7% (1471/10000)
[Test]  Epoch: 24	Loss: 0.058447	Acc: 15.5% (1547/10000)
[Test]  Epoch: 25	Loss: 0.058774	Acc: 14.9% (1493/10000)
[Test]  Epoch: 26	Loss: 0.058345	Acc: 15.6% (1559/10000)
[Test]  Epoch: 27	Loss: 0.058735	Acc: 14.6% (1459/10000)
[Test]  Epoch: 28	Loss: 0.059453	Acc: 14.7% (1471/10000)
[Test]  Epoch: 29	Loss: 0.058396	Acc: 15.6% (1557/10000)
[Test]  Epoch: 30	Loss: 0.059123	Acc: 14.8% (1477/10000)
[Test]  Epoch: 31	Loss: 0.057941	Acc: 16.2% (1617/10000)
[Test]  Epoch: 32	Loss: 0.058228	Acc: 15.0% (1498/10000)
[Test]  Epoch: 33	Loss: 0.058492	Acc: 15.2% (1524/10000)
[Test]  Epoch: 34	Loss: 0.057570	Acc: 15.8% (1583/10000)
[Test]  Epoch: 35	Loss: 0.058363	Acc: 15.1% (1511/10000)
[Test]  Epoch: 36	Loss: 0.057535	Acc: 15.8% (1576/10000)
[Test]  Epoch: 37	Loss: 0.057994	Acc: 15.7% (1568/10000)
[Test]  Epoch: 38	Loss: 0.057654	Acc: 16.1% (1606/10000)
[Test]  Epoch: 39	Loss: 0.057644	Acc: 15.9% (1588/10000)
[Test]  Epoch: 40	Loss: 0.057342	Acc: 16.1% (1609/10000)
[Test]  Epoch: 41	Loss: 0.057608	Acc: 15.6% (1560/10000)
[Test]  Epoch: 42	Loss: 0.057756	Acc: 16.1% (1606/10000)
[Test]  Epoch: 43	Loss: 0.057638	Acc: 15.6% (1557/10000)
[Test]  Epoch: 44	Loss: 0.057625	Acc: 15.8% (1580/10000)
[Test]  Epoch: 45	Loss: 0.057177	Acc: 16.3% (1629/10000)
[Test]  Epoch: 46	Loss: 0.057190	Acc: 16.5% (1648/10000)
[Test]  Epoch: 47	Loss: 0.057094	Acc: 16.4% (1637/10000)
[Test]  Epoch: 48	Loss: 0.056929	Acc: 16.4% (1637/10000)
[Test]  Epoch: 49	Loss: 0.057014	Acc: 16.3% (1627/10000)
[Test]  Epoch: 50	Loss: 0.056969	Acc: 16.3% (1632/10000)
[Test]  Epoch: 51	Loss: 0.057160	Acc: 16.2% (1617/10000)
[Test]  Epoch: 52	Loss: 0.056927	Acc: 16.2% (1624/10000)
[Test]  Epoch: 53	Loss: 0.057132	Acc: 16.1% (1613/10000)
[Test]  Epoch: 54	Loss: 0.057474	Acc: 16.0% (1601/10000)
[Test]  Epoch: 55	Loss: 0.056870	Acc: 16.4% (1640/10000)
[Test]  Epoch: 56	Loss: 0.056636	Acc: 16.4% (1639/10000)
[Test]  Epoch: 57	Loss: 0.056804	Acc: 16.1% (1614/10000)
[Test]  Epoch: 58	Loss: 0.056852	Acc: 16.3% (1630/10000)
[Test]  Epoch: 59	Loss: 0.056976	Acc: 16.2% (1621/10000)
[Test]  Epoch: 60	Loss: 0.057037	Acc: 16.0% (1598/10000)
[Test]  Epoch: 61	Loss: 0.056629	Acc: 16.4% (1635/10000)
[Test]  Epoch: 62	Loss: 0.056478	Acc: 16.7% (1673/10000)
[Test]  Epoch: 63	Loss: 0.056515	Acc: 16.5% (1653/10000)
[Test]  Epoch: 64	Loss: 0.056431	Acc: 16.4% (1638/10000)
[Test]  Epoch: 65	Loss: 0.056232	Acc: 16.7% (1671/10000)
[Test]  Epoch: 66	Loss: 0.056251	Acc: 16.5% (1651/10000)
[Test]  Epoch: 67	Loss: 0.056233	Acc: 16.7% (1667/10000)
[Test]  Epoch: 68	Loss: 0.056240	Acc: 16.7% (1667/10000)
[Test]  Epoch: 69	Loss: 0.056175	Acc: 16.7% (1672/10000)
[Test]  Epoch: 70	Loss: 0.056247	Acc: 16.7% (1674/10000)
[Test]  Epoch: 71	Loss: 0.056216	Acc: 16.8% (1676/10000)
[Test]  Epoch: 72	Loss: 0.056186	Acc: 16.9% (1687/10000)
[Test]  Epoch: 73	Loss: 0.056276	Acc: 16.6% (1662/10000)
[Test]  Epoch: 74	Loss: 0.056101	Acc: 16.7% (1673/10000)
[Test]  Epoch: 75	Loss: 0.056113	Acc: 16.6% (1665/10000)
[Test]  Epoch: 76	Loss: 0.056174	Acc: 16.8% (1675/10000)
[Test]  Epoch: 77	Loss: 0.056112	Acc: 16.9% (1688/10000)
[Test]  Epoch: 78	Loss: 0.056214	Acc: 16.6% (1662/10000)
[Test]  Epoch: 79	Loss: 0.056218	Acc: 16.6% (1665/10000)
[Test]  Epoch: 80	Loss: 0.056104	Acc: 16.6% (1664/10000)
[Test]  Epoch: 81	Loss: 0.056136	Acc: 16.7% (1673/10000)
[Test]  Epoch: 82	Loss: 0.056063	Acc: 16.8% (1679/10000)
[Test]  Epoch: 83	Loss: 0.056069	Acc: 16.7% (1671/10000)
[Test]  Epoch: 84	Loss: 0.056197	Acc: 16.7% (1666/10000)
[Test]  Epoch: 85	Loss: 0.056120	Acc: 16.6% (1661/10000)
[Test]  Epoch: 86	Loss: 0.056130	Acc: 16.7% (1666/10000)
[Test]  Epoch: 87	Loss: 0.056213	Acc: 16.5% (1653/10000)
[Test]  Epoch: 88	Loss: 0.056248	Acc: 16.4% (1645/10000)
[Test]  Epoch: 89	Loss: 0.056122	Acc: 16.6% (1662/10000)
[Test]  Epoch: 90	Loss: 0.056189	Acc: 16.7% (1669/10000)
[Test]  Epoch: 91	Loss: 0.056181	Acc: 16.4% (1636/10000)
[Test]  Epoch: 92	Loss: 0.056142	Acc: 16.5% (1651/10000)
[Test]  Epoch: 93	Loss: 0.056135	Acc: 16.7% (1668/10000)
[Test]  Epoch: 94	Loss: 0.056021	Acc: 16.8% (1676/10000)
[Test]  Epoch: 95	Loss: 0.056227	Acc: 16.7% (1672/10000)
[Test]  Epoch: 96	Loss: 0.056120	Acc: 16.7% (1668/10000)
[Test]  Epoch: 97	Loss: 0.056035	Acc: 17.0% (1696/10000)
[Test]  Epoch: 98	Loss: 0.055999	Acc: 16.9% (1685/10000)
[Test]  Epoch: 99	Loss: 0.056089	Acc: 16.7% (1670/10000)
[Test]  Epoch: 100	Loss: 0.056081	Acc: 16.7% (1671/10000)
===========finish==========
['2024-08-19', '00:31:04.134390', '100', 'test', '0.056081475734710694', '16.71', '16.96']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.2.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.5.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.9.conv.3.weight', '_features.16.conv.0.1.weight', '_features.7.conv.0.1.weight', '_features.0.0.weight', '_features.15.conv.3.weight', '_features.16.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.11.conv.3.weight', '_features.2.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.10.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.2.conv.1.0.weight', '_features.14.conv.3.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.18.1.weight', '_features.11.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.12.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.205880	Acc: 1.5% (151/10000)
[Test]  Epoch: 2	Loss: 0.101983	Acc: 2.0% (199/10000)
[Test]  Epoch: 3	Loss: 0.245885	Acc: 1.1% (112/10000)
[Test]  Epoch: 4	Loss: 0.074322	Acc: 3.7% (373/10000)
[Test]  Epoch: 5	Loss: 0.072827	Acc: 4.1% (414/10000)
[Test]  Epoch: 6	Loss: 0.078191	Acc: 2.8% (278/10000)
[Test]  Epoch: 7	Loss: 0.104507	Acc: 2.3% (231/10000)
[Test]  Epoch: 8	Loss: 0.070358	Acc: 4.7% (469/10000)
[Test]  Epoch: 9	Loss: 0.066464	Acc: 5.7% (569/10000)
[Test]  Epoch: 10	Loss: 0.064568	Acc: 6.8% (682/10000)
[Test]  Epoch: 11	Loss: 0.065874	Acc: 5.8% (578/10000)
[Test]  Epoch: 12	Loss: 0.066586	Acc: 5.1% (506/10000)
[Test]  Epoch: 13	Loss: 0.064404	Acc: 6.5% (647/10000)
[Test]  Epoch: 14	Loss: 0.065411	Acc: 7.2% (716/10000)
[Test]  Epoch: 15	Loss: 0.063601	Acc: 7.6% (761/10000)
[Test]  Epoch: 16	Loss: 0.066088	Acc: 7.4% (737/10000)
[Test]  Epoch: 17	Loss: 0.062731	Acc: 8.4% (836/10000)
[Test]  Epoch: 18	Loss: 0.061934	Acc: 9.1% (911/10000)
[Test]  Epoch: 19	Loss: 0.061974	Acc: 9.3% (933/10000)
[Test]  Epoch: 20	Loss: 0.062588	Acc: 9.1% (906/10000)
[Test]  Epoch: 21	Loss: 0.062914	Acc: 9.7% (965/10000)
[Test]  Epoch: 22	Loss: 0.062723	Acc: 9.9% (991/10000)
[Test]  Epoch: 23	Loss: 0.062334	Acc: 9.9% (989/10000)
[Test]  Epoch: 24	Loss: 0.062250	Acc: 10.1% (1013/10000)
[Test]  Epoch: 25	Loss: 0.065364	Acc: 7.5% (751/10000)
[Test]  Epoch: 26	Loss: 0.063269	Acc: 9.0% (901/10000)
[Test]  Epoch: 27	Loss: 0.063864	Acc: 9.1% (906/10000)
[Test]  Epoch: 28	Loss: 0.064438	Acc: 9.5% (948/10000)
[Test]  Epoch: 29	Loss: 0.064561	Acc: 9.5% (950/10000)
[Test]  Epoch: 30	Loss: 0.067089	Acc: 8.5% (852/10000)
[Test]  Epoch: 31	Loss: 0.063499	Acc: 9.4% (944/10000)
[Test]  Epoch: 32	Loss: 0.063459	Acc: 9.6% (960/10000)
[Test]  Epoch: 33	Loss: 0.063593	Acc: 9.2% (918/10000)
[Test]  Epoch: 34	Loss: 0.064806	Acc: 9.7% (972/10000)
[Test]  Epoch: 35	Loss: 0.064018	Acc: 10.1% (1008/10000)
[Test]  Epoch: 36	Loss: 0.064590	Acc: 9.8% (978/10000)
[Test]  Epoch: 37	Loss: 0.063918	Acc: 10.6% (1057/10000)
[Test]  Epoch: 38	Loss: 0.066688	Acc: 8.0% (800/10000)
[Test]  Epoch: 39	Loss: 0.062329	Acc: 11.3% (1131/10000)
[Test]  Epoch: 40	Loss: 0.063494	Acc: 10.0% (999/10000)
[Test]  Epoch: 41	Loss: 0.063426	Acc: 9.7% (972/10000)
[Test]  Epoch: 42	Loss: 0.063244	Acc: 10.8% (1085/10000)
[Test]  Epoch: 43	Loss: 0.062337	Acc: 12.1% (1208/10000)
[Test]  Epoch: 44	Loss: 0.063279	Acc: 11.4% (1144/10000)
[Test]  Epoch: 45	Loss: 0.062952	Acc: 11.8% (1177/10000)
[Test]  Epoch: 46	Loss: 0.063812	Acc: 11.0% (1096/10000)
[Test]  Epoch: 47	Loss: 0.062772	Acc: 12.2% (1222/10000)
[Test]  Epoch: 48	Loss: 0.067551	Acc: 9.5% (952/10000)
[Test]  Epoch: 49	Loss: 0.064601	Acc: 11.4% (1140/10000)
[Test]  Epoch: 50	Loss: 0.062284	Acc: 12.1% (1208/10000)
[Test]  Epoch: 51	Loss: 0.065309	Acc: 9.9% (992/10000)
[Test]  Epoch: 52	Loss: 0.064438	Acc: 10.0% (997/10000)
[Test]  Epoch: 53	Loss: 0.065715	Acc: 9.5% (953/10000)
[Test]  Epoch: 54	Loss: 0.064450	Acc: 9.5% (951/10000)
[Test]  Epoch: 55	Loss: 0.061667	Acc: 11.6% (1164/10000)
[Test]  Epoch: 56	Loss: 0.062170	Acc: 11.7% (1170/10000)
[Test]  Epoch: 57	Loss: 0.061336	Acc: 12.3% (1234/10000)
[Test]  Epoch: 58	Loss: 0.063530	Acc: 10.9% (1094/10000)
[Test]  Epoch: 59	Loss: 0.062466	Acc: 12.2% (1216/10000)
[Test]  Epoch: 60	Loss: 0.068117	Acc: 10.1% (1005/10000)
[Test]  Epoch: 61	Loss: 0.061586	Acc: 13.1% (1306/10000)
[Test]  Epoch: 62	Loss: 0.061181	Acc: 13.2% (1325/10000)
[Test]  Epoch: 63	Loss: 0.061081	Acc: 13.5% (1346/10000)
[Test]  Epoch: 64	Loss: 0.061026	Acc: 13.5% (1351/10000)
[Test]  Epoch: 65	Loss: 0.060871	Acc: 13.4% (1341/10000)
[Test]  Epoch: 66	Loss: 0.060901	Acc: 13.4% (1343/10000)
[Test]  Epoch: 67	Loss: 0.061209	Acc: 13.3% (1326/10000)
[Test]  Epoch: 68	Loss: 0.061206	Acc: 13.3% (1333/10000)
[Test]  Epoch: 69	Loss: 0.061280	Acc: 13.3% (1331/10000)
[Test]  Epoch: 70	Loss: 0.061509	Acc: 13.5% (1350/10000)
[Test]  Epoch: 71	Loss: 0.061796	Acc: 13.3% (1329/10000)
[Test]  Epoch: 72	Loss: 0.061463	Acc: 13.6% (1355/10000)
[Test]  Epoch: 73	Loss: 0.061544	Acc: 13.6% (1359/10000)
[Test]  Epoch: 74	Loss: 0.061584	Acc: 13.4% (1342/10000)
[Test]  Epoch: 75	Loss: 0.061447	Acc: 13.3% (1334/10000)
[Test]  Epoch: 76	Loss: 0.061691	Acc: 13.2% (1324/10000)
[Test]  Epoch: 77	Loss: 0.061764	Acc: 13.4% (1345/10000)
[Test]  Epoch: 78	Loss: 0.061578	Acc: 13.5% (1351/10000)
[Test]  Epoch: 79	Loss: 0.061668	Acc: 13.4% (1342/10000)
[Test]  Epoch: 80	Loss: 0.061621	Acc: 13.6% (1356/10000)
[Test]  Epoch: 81	Loss: 0.061761	Acc: 13.3% (1333/10000)
[Test]  Epoch: 82	Loss: 0.061668	Acc: 13.4% (1342/10000)
[Test]  Epoch: 83	Loss: 0.061776	Acc: 13.4% (1342/10000)
[Test]  Epoch: 84	Loss: 0.061743	Acc: 13.5% (1347/10000)
[Test]  Epoch: 85	Loss: 0.061892	Acc: 13.3% (1335/10000)
[Test]  Epoch: 86	Loss: 0.061871	Acc: 13.4% (1340/10000)
[Test]  Epoch: 87	Loss: 0.061907	Acc: 13.4% (1345/10000)
[Test]  Epoch: 88	Loss: 0.061781	Acc: 13.3% (1330/10000)
[Test]  Epoch: 89	Loss: 0.061816	Acc: 13.3% (1328/10000)
[Test]  Epoch: 90	Loss: 0.061973	Acc: 13.3% (1332/10000)
[Test]  Epoch: 91	Loss: 0.061972	Acc: 13.3% (1328/10000)
[Test]  Epoch: 92	Loss: 0.062007	Acc: 13.2% (1322/10000)
[Test]  Epoch: 93	Loss: 0.062329	Acc: 12.9% (1293/10000)
[Test]  Epoch: 94	Loss: 0.062475	Acc: 13.0% (1299/10000)
[Test]  Epoch: 95	Loss: 0.062156	Acc: 13.2% (1325/10000)
[Test]  Epoch: 96	Loss: 0.062035	Acc: 13.4% (1344/10000)
[Test]  Epoch: 97	Loss: 0.062009	Acc: 13.2% (1316/10000)
[Test]  Epoch: 98	Loss: 0.062206	Acc: 13.1% (1305/10000)
[Test]  Epoch: 99	Loss: 0.062014	Acc: 13.2% (1318/10000)
[Test]  Epoch: 100	Loss: 0.062128	Acc: 13.4% (1343/10000)
===========finish==========
['2024-08-19', '00:33:32.280294', '100', 'test', '0.06212784495353699', '13.43', '13.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.2.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.5.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.9.conv.3.weight', '_features.16.conv.0.1.weight', '_features.7.conv.0.1.weight', '_features.0.0.weight', '_features.15.conv.3.weight', '_features.16.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.11.conv.3.weight', '_features.2.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.10.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.2.conv.1.0.weight', '_features.14.conv.3.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.18.1.weight', '_features.11.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.13.conv.1.0.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.8.conv.1.0.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.13.conv.0.0.weight', '_features.4.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.354759	Acc: 1.5% (148/10000)
[Test]  Epoch: 2	Loss: 0.130158	Acc: 2.9% (285/10000)
[Test]  Epoch: 3	Loss: 0.071636	Acc: 5.6% (563/10000)
[Test]  Epoch: 4	Loss: 0.101489	Acc: 6.0% (595/10000)
[Test]  Epoch: 5	Loss: 0.065919	Acc: 7.1% (713/10000)
[Test]  Epoch: 6	Loss: 0.065872	Acc: 6.8% (681/10000)
[Test]  Epoch: 7	Loss: 0.065633	Acc: 7.4% (742/10000)
[Test]  Epoch: 8	Loss: 0.102589	Acc: 2.0% (198/10000)
[Test]  Epoch: 9	Loss: 0.142417	Acc: 2.1% (209/10000)
[Test]  Epoch: 10	Loss: 0.073041	Acc: 4.2% (422/10000)
[Test]  Epoch: 11	Loss: 0.071577	Acc: 5.5% (546/10000)
[Test]  Epoch: 12	Loss: 0.065395	Acc: 6.9% (686/10000)
[Test]  Epoch: 13	Loss: 0.077322	Acc: 3.5% (351/10000)
[Test]  Epoch: 14	Loss: 0.065492	Acc: 6.7% (665/10000)
[Test]  Epoch: 15	Loss: 0.064022	Acc: 7.3% (733/10000)
[Test]  Epoch: 16	Loss: 0.063820	Acc: 7.7% (769/10000)
[Test]  Epoch: 17	Loss: 0.064324	Acc: 7.2% (721/10000)
[Test]  Epoch: 18	Loss: 0.063836	Acc: 8.5% (848/10000)
[Test]  Epoch: 19	Loss: 0.063654	Acc: 8.3% (829/10000)
[Test]  Epoch: 20	Loss: 0.064129	Acc: 7.9% (794/10000)
[Test]  Epoch: 21	Loss: 0.064942	Acc: 8.6% (857/10000)
[Test]  Epoch: 22	Loss: 0.076416	Acc: 4.9% (492/10000)
[Test]  Epoch: 23	Loss: 0.062918	Acc: 8.9% (892/10000)
[Test]  Epoch: 24	Loss: 0.065227	Acc: 8.8% (881/10000)
[Test]  Epoch: 25	Loss: 0.068244	Acc: 8.0% (796/10000)
[Test]  Epoch: 26	Loss: 0.064095	Acc: 9.1% (905/10000)
[Test]  Epoch: 27	Loss: 0.062781	Acc: 9.8% (975/10000)
[Test]  Epoch: 28	Loss: 0.066249	Acc: 7.7% (773/10000)
[Test]  Epoch: 29	Loss: 0.062374	Acc: 9.9% (990/10000)
[Test]  Epoch: 30	Loss: 0.061673	Acc: 10.6% (1064/10000)
[Test]  Epoch: 31	Loss: 0.061156	Acc: 11.1% (1108/10000)
[Test]  Epoch: 32	Loss: 0.061761	Acc: 11.2% (1125/10000)
[Test]  Epoch: 33	Loss: 0.062416	Acc: 11.1% (1106/10000)
[Test]  Epoch: 34	Loss: 0.061556	Acc: 12.0% (1196/10000)
[Test]  Epoch: 35	Loss: 0.067127	Acc: 9.4% (942/10000)
[Test]  Epoch: 36	Loss: 0.062059	Acc: 11.2% (1117/10000)
[Test]  Epoch: 37	Loss: 0.061347	Acc: 11.3% (1130/10000)
[Test]  Epoch: 38	Loss: 0.062513	Acc: 11.3% (1135/10000)
[Test]  Epoch: 39	Loss: 0.061898	Acc: 11.9% (1193/10000)
[Test]  Epoch: 40	Loss: 0.062669	Acc: 11.4% (1145/10000)
[Test]  Epoch: 41	Loss: 0.063026	Acc: 11.7% (1169/10000)
[Test]  Epoch: 42	Loss: 0.063910	Acc: 11.1% (1108/10000)
[Test]  Epoch: 43	Loss: 0.063623	Acc: 11.8% (1180/10000)
[Test]  Epoch: 44	Loss: 0.063549	Acc: 10.9% (1095/10000)
[Test]  Epoch: 45	Loss: 0.094873	Acc: 2.7% (272/10000)
[Test]  Epoch: 46	Loss: 0.062362	Acc: 10.4% (1039/10000)
[Test]  Epoch: 47	Loss: 0.061538	Acc: 11.6% (1163/10000)
[Test]  Epoch: 48	Loss: 0.060875	Acc: 12.3% (1226/10000)
[Test]  Epoch: 49	Loss: 0.060978	Acc: 11.9% (1194/10000)
[Test]  Epoch: 50	Loss: 0.061143	Acc: 12.1% (1212/10000)
[Test]  Epoch: 51	Loss: 0.061798	Acc: 12.7% (1272/10000)
[Test]  Epoch: 52	Loss: 0.063771	Acc: 11.6% (1158/10000)
[Test]  Epoch: 53	Loss: 0.062083	Acc: 12.5% (1252/10000)
[Test]  Epoch: 54	Loss: 0.061358	Acc: 12.6% (1259/10000)
[Test]  Epoch: 55	Loss: 0.062502	Acc: 12.7% (1273/10000)
[Test]  Epoch: 56	Loss: 0.062484	Acc: 13.2% (1315/10000)
[Test]  Epoch: 57	Loss: 0.064959	Acc: 11.6% (1156/10000)
[Test]  Epoch: 58	Loss: 0.063562	Acc: 12.6% (1262/10000)
[Test]  Epoch: 59	Loss: 0.063006	Acc: 12.2% (1223/10000)
[Test]  Epoch: 60	Loss: 0.063438	Acc: 12.2% (1216/10000)
[Test]  Epoch: 61	Loss: 0.061925	Acc: 13.1% (1307/10000)
[Test]  Epoch: 62	Loss: 0.061761	Acc: 13.2% (1324/10000)
[Test]  Epoch: 63	Loss: 0.061714	Acc: 13.3% (1326/10000)
[Test]  Epoch: 64	Loss: 0.061643	Acc: 13.5% (1352/10000)
[Test]  Epoch: 65	Loss: 0.061654	Acc: 13.3% (1328/10000)
[Test]  Epoch: 66	Loss: 0.061708	Acc: 13.2% (1322/10000)
[Test]  Epoch: 67	Loss: 0.061821	Acc: 13.4% (1341/10000)
[Test]  Epoch: 68	Loss: 0.061795	Acc: 13.2% (1323/10000)
[Test]  Epoch: 69	Loss: 0.061809	Acc: 13.5% (1348/10000)
[Test]  Epoch: 70	Loss: 0.061823	Acc: 13.4% (1340/10000)
[Test]  Epoch: 71	Loss: 0.061745	Acc: 13.2% (1317/10000)
[Test]  Epoch: 72	Loss: 0.061785	Acc: 13.4% (1343/10000)
[Test]  Epoch: 73	Loss: 0.061706	Acc: 13.6% (1359/10000)
[Test]  Epoch: 74	Loss: 0.061711	Acc: 13.6% (1356/10000)
[Test]  Epoch: 75	Loss: 0.061826	Acc: 13.3% (1331/10000)
[Test]  Epoch: 76	Loss: 0.061750	Acc: 13.5% (1351/10000)
[Test]  Epoch: 77	Loss: 0.062214	Acc: 12.9% (1290/10000)
[Test]  Epoch: 78	Loss: 0.061875	Acc: 13.2% (1323/10000)
[Test]  Epoch: 79	Loss: 0.061891	Acc: 13.1% (1310/10000)
[Test]  Epoch: 80	Loss: 0.061683	Acc: 13.3% (1332/10000)
[Test]  Epoch: 81	Loss: 0.062244	Acc: 12.8% (1284/10000)
[Test]  Epoch: 82	Loss: 0.061873	Acc: 13.5% (1348/10000)
[Test]  Epoch: 83	Loss: 0.061830	Acc: 13.5% (1346/10000)
[Test]  Epoch: 84	Loss: 0.062077	Acc: 13.2% (1324/10000)
[Test]  Epoch: 85	Loss: 0.061850	Acc: 13.1% (1305/10000)
[Test]  Epoch: 86	Loss: 0.062072	Acc: 13.0% (1304/10000)
[Test]  Epoch: 87	Loss: 0.061947	Acc: 13.2% (1320/10000)
[Test]  Epoch: 88	Loss: 0.062012	Acc: 13.3% (1331/10000)
[Test]  Epoch: 89	Loss: 0.062450	Acc: 13.1% (1309/10000)
[Test]  Epoch: 90	Loss: 0.062137	Acc: 13.2% (1325/10000)
[Test]  Epoch: 91	Loss: 0.061938	Acc: 13.2% (1325/10000)
[Test]  Epoch: 92	Loss: 0.061828	Acc: 13.5% (1350/10000)
[Test]  Epoch: 93	Loss: 0.061989	Acc: 13.4% (1337/10000)
[Test]  Epoch: 94	Loss: 0.062036	Acc: 13.6% (1362/10000)
[Test]  Epoch: 95	Loss: 0.061939	Acc: 13.4% (1336/10000)
[Test]  Epoch: 96	Loss: 0.061805	Acc: 13.5% (1346/10000)
[Test]  Epoch: 97	Loss: 0.061961	Acc: 13.6% (1362/10000)
[Test]  Epoch: 98	Loss: 0.061756	Acc: 13.5% (1352/10000)
[Test]  Epoch: 99	Loss: 0.061832	Acc: 13.5% (1353/10000)
[Test]  Epoch: 100	Loss: 0.062125	Acc: 13.6% (1356/10000)
===========finish==========
['2024-08-19', '00:35:59.019686', '100', 'test', '0.06212506194114685', '13.56', '13.62']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.2.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.5.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.9.conv.3.weight', '_features.16.conv.0.1.weight', '_features.7.conv.0.1.weight', '_features.0.0.weight', '_features.15.conv.3.weight', '_features.16.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.11.conv.3.weight', '_features.2.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.10.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.2.conv.1.0.weight', '_features.14.conv.3.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.18.1.weight', '_features.11.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.13.conv.1.0.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.8.conv.1.0.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.13.conv.0.0.weight', '_features.4.conv.2.weight', '_features.14.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.10.conv.1.0.weight', '_features.9.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.7.conv.2.weight', '_features.8.conv.0.0.weight', '_features.16.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.435350	Acc: 1.1% (115/10000)
[Test]  Epoch: 2	Loss: 0.083951	Acc: 2.7% (272/10000)
[Test]  Epoch: 3	Loss: 0.075003	Acc: 2.6% (256/10000)
[Test]  Epoch: 4	Loss: 0.130264	Acc: 1.6% (163/10000)
[Test]  Epoch: 5	Loss: 0.076621	Acc: 2.2% (222/10000)
[Test]  Epoch: 6	Loss: 0.072979	Acc: 2.7% (266/10000)
[Test]  Epoch: 7	Loss: 0.085945	Acc: 2.8% (281/10000)
[Test]  Epoch: 8	Loss: 0.070273	Acc: 3.8% (377/10000)
[Test]  Epoch: 9	Loss: 0.068814	Acc: 4.6% (458/10000)
[Test]  Epoch: 10	Loss: 0.067143	Acc: 5.2% (522/10000)
[Test]  Epoch: 11	Loss: 0.081484	Acc: 3.7% (369/10000)
[Test]  Epoch: 12	Loss: 0.067877	Acc: 4.6% (456/10000)
[Test]  Epoch: 13	Loss: 0.067712	Acc: 5.5% (552/10000)
[Test]  Epoch: 14	Loss: 0.065341	Acc: 6.5% (652/10000)
[Test]  Epoch: 15	Loss: 0.065082	Acc: 7.0% (705/10000)
[Test]  Epoch: 16	Loss: 0.064222	Acc: 7.3% (732/10000)
[Test]  Epoch: 17	Loss: 0.063002	Acc: 8.3% (831/10000)
[Test]  Epoch: 18	Loss: 0.062813	Acc: 8.4% (844/10000)
[Test]  Epoch: 19	Loss: 0.063287	Acc: 8.0% (798/10000)
[Test]  Epoch: 20	Loss: 0.062496	Acc: 8.6% (863/10000)
[Test]  Epoch: 21	Loss: 0.062796	Acc: 9.2% (922/10000)
[Test]  Epoch: 22	Loss: 0.062651	Acc: 9.5% (947/10000)
[Test]  Epoch: 23	Loss: 0.062868	Acc: 9.3% (927/10000)
[Test]  Epoch: 24	Loss: 0.063041	Acc: 9.6% (960/10000)
[Test]  Epoch: 25	Loss: 0.064103	Acc: 9.0% (899/10000)
[Test]  Epoch: 26	Loss: 0.063516	Acc: 10.2% (1025/10000)
[Test]  Epoch: 27	Loss: 0.065215	Acc: 9.2% (922/10000)
[Test]  Epoch: 28	Loss: 0.064045	Acc: 9.5% (947/10000)
[Test]  Epoch: 29	Loss: 0.065115	Acc: 8.9% (891/10000)
[Test]  Epoch: 30	Loss: 0.063690	Acc: 9.9% (991/10000)
[Test]  Epoch: 31	Loss: 0.063969	Acc: 9.5% (949/10000)
[Test]  Epoch: 32	Loss: 0.064391	Acc: 9.7% (967/10000)
[Test]  Epoch: 33	Loss: 0.064729	Acc: 9.8% (980/10000)
[Test]  Epoch: 34	Loss: 0.065412	Acc: 9.2% (919/10000)
[Test]  Epoch: 35	Loss: 0.066893	Acc: 9.5% (949/10000)
[Test]  Epoch: 36	Loss: 0.065749	Acc: 10.3% (1027/10000)
[Test]  Epoch: 37	Loss: 0.064643	Acc: 10.2% (1023/10000)
[Test]  Epoch: 38	Loss: 0.064908	Acc: 9.9% (986/10000)
[Test]  Epoch: 39	Loss: 0.068543	Acc: 8.5% (849/10000)
[Test]  Epoch: 40	Loss: 0.066424	Acc: 9.9% (990/10000)
[Test]  Epoch: 41	Loss: 0.065032	Acc: 10.7% (1069/10000)
[Test]  Epoch: 42	Loss: 0.064594	Acc: 10.8% (1085/10000)
[Test]  Epoch: 43	Loss: 0.064633	Acc: 11.1% (1108/10000)
[Test]  Epoch: 44	Loss: 0.072344	Acc: 7.4% (740/10000)
[Test]  Epoch: 45	Loss: 0.065986	Acc: 10.0% (1003/10000)
[Test]  Epoch: 46	Loss: 0.064814	Acc: 10.6% (1064/10000)
[Test]  Epoch: 47	Loss: 0.065142	Acc: 11.3% (1128/10000)
[Test]  Epoch: 48	Loss: 0.064781	Acc: 10.3% (1031/10000)
[Test]  Epoch: 49	Loss: 0.064776	Acc: 11.0% (1100/10000)
[Test]  Epoch: 50	Loss: 0.064705	Acc: 10.5% (1050/10000)
[Test]  Epoch: 51	Loss: 0.064428	Acc: 11.3% (1128/10000)
[Test]  Epoch: 52	Loss: 0.065356	Acc: 10.3% (1034/10000)
[Test]  Epoch: 53	Loss: 0.064717	Acc: 10.2% (1023/10000)
[Test]  Epoch: 54	Loss: 0.064257	Acc: 10.8% (1082/10000)
[Test]  Epoch: 55	Loss: 0.065386	Acc: 10.9% (1090/10000)
[Test]  Epoch: 56	Loss: 0.065637	Acc: 10.6% (1064/10000)
[Test]  Epoch: 57	Loss: 0.064166	Acc: 10.9% (1095/10000)
[Test]  Epoch: 58	Loss: 0.066422	Acc: 9.4% (942/10000)
[Test]  Epoch: 59	Loss: 0.064413	Acc: 10.9% (1090/10000)
[Test]  Epoch: 60	Loss: 0.063647	Acc: 11.2% (1120/10000)
[Test]  Epoch: 61	Loss: 0.063625	Acc: 11.5% (1148/10000)
[Test]  Epoch: 62	Loss: 0.063435	Acc: 11.6% (1163/10000)
[Test]  Epoch: 63	Loss: 0.063310	Acc: 11.7% (1166/10000)
[Test]  Epoch: 64	Loss: 0.063298	Acc: 11.2% (1125/10000)
[Test]  Epoch: 65	Loss: 0.063279	Acc: 11.2% (1125/10000)
[Test]  Epoch: 66	Loss: 0.063308	Acc: 11.3% (1129/10000)
[Test]  Epoch: 67	Loss: 0.063269	Acc: 11.4% (1136/10000)
[Test]  Epoch: 68	Loss: 0.063167	Acc: 11.4% (1145/10000)
[Test]  Epoch: 69	Loss: 0.063246	Acc: 11.3% (1131/10000)
[Test]  Epoch: 70	Loss: 0.063113	Acc: 11.3% (1129/10000)
[Test]  Epoch: 71	Loss: 0.063307	Acc: 11.3% (1130/10000)
[Test]  Epoch: 72	Loss: 0.063065	Acc: 11.3% (1130/10000)
[Test]  Epoch: 73	Loss: 0.063364	Acc: 11.2% (1122/10000)
[Test]  Epoch: 74	Loss: 0.063061	Acc: 11.5% (1154/10000)
[Test]  Epoch: 75	Loss: 0.063069	Acc: 11.4% (1144/10000)
[Test]  Epoch: 76	Loss: 0.063112	Acc: 11.5% (1150/10000)
[Test]  Epoch: 77	Loss: 0.063127	Acc: 11.5% (1154/10000)
[Test]  Epoch: 78	Loss: 0.063073	Acc: 11.6% (1159/10000)
[Test]  Epoch: 79	Loss: 0.062962	Acc: 11.5% (1150/10000)
[Test]  Epoch: 80	Loss: 0.063846	Acc: 10.7% (1072/10000)
[Test]  Epoch: 81	Loss: 0.063421	Acc: 11.2% (1119/10000)
[Test]  Epoch: 82	Loss: 0.063108	Acc: 11.5% (1150/10000)
[Test]  Epoch: 83	Loss: 0.062994	Acc: 11.6% (1163/10000)
[Test]  Epoch: 84	Loss: 0.063417	Acc: 11.4% (1140/10000)
[Test]  Epoch: 85	Loss: 0.063186	Acc: 11.3% (1134/10000)
[Test]  Epoch: 86	Loss: 0.063005	Acc: 11.4% (1144/10000)
[Test]  Epoch: 87	Loss: 0.063138	Acc: 11.6% (1162/10000)
[Test]  Epoch: 88	Loss: 0.063447	Acc: 11.3% (1130/10000)
[Test]  Epoch: 89	Loss: 0.063039	Acc: 11.4% (1144/10000)
[Test]  Epoch: 90	Loss: 0.063205	Acc: 11.4% (1144/10000)
[Test]  Epoch: 91	Loss: 0.063058	Acc: 11.8% (1179/10000)
[Test]  Epoch: 92	Loss: 0.063013	Acc: 11.6% (1155/10000)
[Test]  Epoch: 93	Loss: 0.063342	Acc: 11.3% (1129/10000)
[Test]  Epoch: 94	Loss: 0.063232	Acc: 11.4% (1143/10000)
[Test]  Epoch: 95	Loss: 0.063199	Acc: 11.3% (1134/10000)
[Test]  Epoch: 96	Loss: 0.063069	Acc: 11.5% (1148/10000)
[Test]  Epoch: 97	Loss: 0.063089	Acc: 11.3% (1129/10000)
[Test]  Epoch: 98	Loss: 0.063080	Acc: 11.2% (1119/10000)
[Test]  Epoch: 99	Loss: 0.063264	Acc: 11.1% (1105/10000)
[Test]  Epoch: 100	Loss: 0.063296	Acc: 11.2% (1124/10000)
===========finish==========
['2024-08-19', '00:38:22.773999', '100', 'test', '0.06329589371681213', '11.24', '11.79']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.2.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.5.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.9.conv.3.weight', '_features.16.conv.0.1.weight', '_features.7.conv.0.1.weight', '_features.0.0.weight', '_features.15.conv.3.weight', '_features.16.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.11.conv.3.weight', '_features.2.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.10.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.2.conv.1.0.weight', '_features.14.conv.3.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.18.1.weight', '_features.11.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.13.conv.1.0.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.8.conv.1.0.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.13.conv.0.0.weight', '_features.4.conv.2.weight', '_features.14.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.10.conv.1.0.weight', '_features.9.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.7.conv.2.weight', '_features.8.conv.0.0.weight', '_features.16.conv.2.weight', '_features.17.conv.1.1.weight', '_features.16.conv.0.0.weight', '_features.13.conv.2.weight', '_features.12.conv.2.weight', '_features.9.conv.0.0.weight', '_features.15.conv.2.weight', '_features.8.conv.2.weight', '_features.14.conv.1.0.weight', '_features.17.conv.3.weight', '_features.10.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.702263	Acc: 1.1% (105/10000)
[Test]  Epoch: 2	Loss: 0.086751	Acc: 4.8% (485/10000)
[Test]  Epoch: 3	Loss: 0.081444	Acc: 5.5% (546/10000)
[Test]  Epoch: 4	Loss: 0.066190	Acc: 8.4% (838/10000)
[Test]  Epoch: 5	Loss: 0.064794	Acc: 9.8% (979/10000)
[Test]  Epoch: 6	Loss: 0.068243	Acc: 9.4% (941/10000)
[Test]  Epoch: 7	Loss: 0.060370	Acc: 11.3% (1131/10000)
[Test]  Epoch: 8	Loss: 0.061187	Acc: 11.0% (1100/10000)
[Test]  Epoch: 9	Loss: 0.061643	Acc: 10.6% (1064/10000)
[Test]  Epoch: 10	Loss: 0.060319	Acc: 12.0% (1200/10000)
[Test]  Epoch: 11	Loss: 0.061183	Acc: 11.8% (1185/10000)
[Test]  Epoch: 12	Loss: 0.060362	Acc: 12.5% (1246/10000)
[Test]  Epoch: 13	Loss: 0.061203	Acc: 12.2% (1217/10000)
[Test]  Epoch: 14	Loss: 0.106047	Acc: 5.5% (552/10000)
[Test]  Epoch: 15	Loss: 0.062194	Acc: 12.0% (1203/10000)
[Test]  Epoch: 16	Loss: 0.061485	Acc: 12.6% (1255/10000)
[Test]  Epoch: 17	Loss: 0.059538	Acc: 12.5% (1247/10000)
[Test]  Epoch: 18	Loss: 0.058166	Acc: 14.5% (1450/10000)
[Test]  Epoch: 19	Loss: 0.057503	Acc: 14.7% (1466/10000)
[Test]  Epoch: 20	Loss: 0.058644	Acc: 14.2% (1419/10000)
[Test]  Epoch: 21	Loss: 0.059588	Acc: 14.0% (1404/10000)
[Test]  Epoch: 22	Loss: 0.057697	Acc: 14.8% (1483/10000)
[Test]  Epoch: 23	Loss: 0.057015	Acc: 15.8% (1581/10000)
[Test]  Epoch: 24	Loss: 0.058975	Acc: 14.4% (1439/10000)
[Test]  Epoch: 25	Loss: 0.105313	Acc: 4.8% (483/10000)
[Test]  Epoch: 26	Loss: 0.065840	Acc: 9.1% (913/10000)
[Test]  Epoch: 27	Loss: 0.250529	Acc: 3.5% (354/10000)
[Test]  Epoch: 28	Loss: 0.063099	Acc: 9.3% (932/10000)
[Test]  Epoch: 29	Loss: 0.060966	Acc: 11.3% (1128/10000)
[Test]  Epoch: 30	Loss: 0.059314	Acc: 11.8% (1176/10000)
[Test]  Epoch: 31	Loss: 0.059022	Acc: 12.4% (1237/10000)
[Test]  Epoch: 32	Loss: 0.059343	Acc: 12.0% (1203/10000)
[Test]  Epoch: 33	Loss: 0.060270	Acc: 12.7% (1265/10000)
[Test]  Epoch: 34	Loss: 0.058449	Acc: 13.1% (1309/10000)
[Test]  Epoch: 35	Loss: 0.059062	Acc: 13.7% (1371/10000)
[Test]  Epoch: 36	Loss: 0.060696	Acc: 12.2% (1221/10000)
[Test]  Epoch: 37	Loss: 0.059509	Acc: 13.5% (1350/10000)
[Test]  Epoch: 38	Loss: 0.058959	Acc: 13.8% (1381/10000)
[Test]  Epoch: 39	Loss: 0.059201	Acc: 13.9% (1388/10000)
[Test]  Epoch: 40	Loss: 0.059612	Acc: 13.7% (1366/10000)
[Test]  Epoch: 41	Loss: 0.058532	Acc: 14.6% (1456/10000)
[Test]  Epoch: 42	Loss: 0.059277	Acc: 14.6% (1457/10000)
[Test]  Epoch: 43	Loss: 0.059242	Acc: 14.3% (1430/10000)
[Test]  Epoch: 44	Loss: 0.059486	Acc: 14.0% (1397/10000)
[Test]  Epoch: 45	Loss: 0.061434	Acc: 13.6% (1362/10000)
[Test]  Epoch: 46	Loss: 0.059399	Acc: 14.6% (1463/10000)
[Test]  Epoch: 47	Loss: 0.060440	Acc: 13.6% (1361/10000)
[Test]  Epoch: 48	Loss: 0.059490	Acc: 14.4% (1441/10000)
[Test]  Epoch: 49	Loss: 0.059306	Acc: 14.2% (1421/10000)
[Test]  Epoch: 50	Loss: 0.058581	Acc: 14.6% (1457/10000)
[Test]  Epoch: 51	Loss: 0.059737	Acc: 14.8% (1481/10000)
[Test]  Epoch: 52	Loss: 0.058357	Acc: 14.5% (1454/10000)
[Test]  Epoch: 53	Loss: 0.058604	Acc: 15.1% (1514/10000)
[Test]  Epoch: 54	Loss: 0.058579	Acc: 14.4% (1437/10000)
[Test]  Epoch: 55	Loss: 0.057838	Acc: 15.3% (1530/10000)
[Test]  Epoch: 56	Loss: 0.058585	Acc: 14.6% (1462/10000)
[Test]  Epoch: 57	Loss: 0.058667	Acc: 14.8% (1484/10000)
[Test]  Epoch: 58	Loss: 0.058103	Acc: 14.8% (1479/10000)
[Test]  Epoch: 59	Loss: 0.058553	Acc: 15.0% (1502/10000)
[Test]  Epoch: 60	Loss: 0.058171	Acc: 15.4% (1538/10000)
[Test]  Epoch: 61	Loss: 0.057786	Acc: 15.5% (1549/10000)
[Test]  Epoch: 62	Loss: 0.057734	Acc: 15.5% (1552/10000)
[Test]  Epoch: 63	Loss: 0.057407	Acc: 15.4% (1537/10000)
[Test]  Epoch: 64	Loss: 0.057344	Acc: 15.5% (1553/10000)
[Test]  Epoch: 65	Loss: 0.057278	Acc: 15.7% (1569/10000)
[Test]  Epoch: 66	Loss: 0.057505	Acc: 15.4% (1543/10000)
[Test]  Epoch: 67	Loss: 0.057361	Acc: 15.5% (1549/10000)
[Test]  Epoch: 68	Loss: 0.057418	Acc: 15.7% (1565/10000)
[Test]  Epoch: 69	Loss: 0.057349	Acc: 15.6% (1559/10000)
[Test]  Epoch: 70	Loss: 0.057284	Acc: 15.9% (1590/10000)
[Test]  Epoch: 71	Loss: 0.057381	Acc: 15.6% (1555/10000)
[Test]  Epoch: 72	Loss: 0.057238	Acc: 15.8% (1575/10000)
[Test]  Epoch: 73	Loss: 0.057230	Acc: 15.9% (1588/10000)
[Test]  Epoch: 74	Loss: 0.057162	Acc: 15.6% (1558/10000)
[Test]  Epoch: 75	Loss: 0.057023	Acc: 15.9% (1587/10000)
[Test]  Epoch: 76	Loss: 0.057346	Acc: 15.6% (1555/10000)
[Test]  Epoch: 77	Loss: 0.057284	Acc: 15.6% (1558/10000)
[Test]  Epoch: 78	Loss: 0.057251	Acc: 15.5% (1546/10000)
[Test]  Epoch: 79	Loss: 0.057189	Acc: 15.5% (1546/10000)
[Test]  Epoch: 80	Loss: 0.057217	Acc: 15.7% (1565/10000)
[Test]  Epoch: 81	Loss: 0.057311	Acc: 15.7% (1568/10000)
[Test]  Epoch: 82	Loss: 0.057245	Acc: 15.6% (1555/10000)
[Test]  Epoch: 83	Loss: 0.057340	Acc: 15.6% (1564/10000)
[Test]  Epoch: 84	Loss: 0.057287	Acc: 15.6% (1555/10000)
[Test]  Epoch: 85	Loss: 0.057207	Acc: 15.7% (1566/10000)
[Test]  Epoch: 86	Loss: 0.057171	Acc: 15.8% (1580/10000)
[Test]  Epoch: 87	Loss: 0.057362	Acc: 15.7% (1566/10000)
[Test]  Epoch: 88	Loss: 0.057288	Acc: 15.5% (1551/10000)
[Test]  Epoch: 89	Loss: 0.057240	Acc: 15.7% (1571/10000)
[Test]  Epoch: 90	Loss: 0.057300	Acc: 15.6% (1561/10000)
[Test]  Epoch: 91	Loss: 0.057393	Acc: 15.4% (1536/10000)
[Test]  Epoch: 92	Loss: 0.057368	Acc: 15.5% (1554/10000)
[Test]  Epoch: 93	Loss: 0.057364	Acc: 15.4% (1543/10000)
[Test]  Epoch: 94	Loss: 0.057451	Acc: 15.4% (1539/10000)
[Test]  Epoch: 95	Loss: 0.057348	Acc: 15.6% (1562/10000)
[Test]  Epoch: 96	Loss: 0.057197	Acc: 15.5% (1550/10000)
[Test]  Epoch: 97	Loss: 0.057299	Acc: 15.7% (1573/10000)
[Test]  Epoch: 98	Loss: 0.057383	Acc: 15.4% (1538/10000)
[Test]  Epoch: 99	Loss: 0.057295	Acc: 15.4% (1537/10000)
[Test]  Epoch: 100	Loss: 0.057254	Acc: 15.6% (1560/10000)
===========finish==========
['2024-08-19', '00:40:40.731144', '100', 'test', '0.05725408222675323', '15.6', '15.9']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.2.conv.3.weight', '_features.5.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.7.conv.3.weight', '_features.6.conv.1.1.weight', '_features.4.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.8.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.2.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.5.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.9.conv.3.weight', '_features.16.conv.0.1.weight', '_features.7.conv.0.1.weight', '_features.0.0.weight', '_features.15.conv.3.weight', '_features.16.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.11.conv.3.weight', '_features.2.conv.0.0.weight', '_features.8.conv.0.1.weight', '_features.1.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.9.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.10.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.2.conv.1.0.weight', '_features.14.conv.3.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.18.1.weight', '_features.11.conv.0.1.weight', '_features.6.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.13.conv.1.0.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.8.conv.1.0.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.13.conv.0.0.weight', '_features.4.conv.2.weight', '_features.14.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.10.conv.1.0.weight', '_features.9.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.7.conv.2.weight', '_features.8.conv.0.0.weight', '_features.16.conv.2.weight', '_features.17.conv.1.1.weight', '_features.16.conv.0.0.weight', '_features.13.conv.2.weight', '_features.12.conv.2.weight', '_features.9.conv.0.0.weight', '_features.15.conv.2.weight', '_features.8.conv.2.weight', '_features.14.conv.1.0.weight', '_features.17.conv.3.weight', '_features.10.conv.0.0.weight', '_features.10.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.11.conv.2.weight', '_features.17.conv.1.0.weight', '_features.14.conv.0.0.weight', '_features.14.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.2.weight', 'last_linear.weight', '_features.18.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.091649	Acc: 1.6% (164/10000)
[Test]  Epoch: 2	Loss: 0.068418	Acc: 5.5% (552/10000)
[Test]  Epoch: 3	Loss: 0.065688	Acc: 7.8% (781/10000)
[Test]  Epoch: 4	Loss: 0.063783	Acc: 8.9% (895/10000)
[Test]  Epoch: 5	Loss: 0.062184	Acc: 9.8% (975/10000)
[Test]  Epoch: 6	Loss: 0.061014	Acc: 10.5% (1050/10000)
[Test]  Epoch: 7	Loss: 0.059806	Acc: 11.6% (1158/10000)
[Test]  Epoch: 8	Loss: 0.059361	Acc: 12.0% (1203/10000)
[Test]  Epoch: 9	Loss: 0.058854	Acc: 12.1% (1211/10000)
[Test]  Epoch: 10	Loss: 0.058542	Acc: 12.2% (1224/10000)
[Test]  Epoch: 11	Loss: 0.058015	Acc: 13.1% (1312/10000)
[Test]  Epoch: 12	Loss: 0.057445	Acc: 13.3% (1329/10000)
[Test]  Epoch: 13	Loss: 0.057502	Acc: 13.1% (1308/10000)
[Test]  Epoch: 14	Loss: 0.056994	Acc: 13.8% (1375/10000)
[Test]  Epoch: 15	Loss: 0.056687	Acc: 13.4% (1345/10000)
[Test]  Epoch: 16	Loss: 0.057196	Acc: 13.7% (1366/10000)
[Test]  Epoch: 17	Loss: 0.057049	Acc: 14.2% (1419/10000)
[Test]  Epoch: 18	Loss: 0.057620	Acc: 13.7% (1371/10000)
[Test]  Epoch: 19	Loss: 0.056349	Acc: 14.6% (1455/10000)
[Test]  Epoch: 20	Loss: 0.056230	Acc: 14.7% (1470/10000)
[Test]  Epoch: 21	Loss: 0.055982	Acc: 14.6% (1462/10000)
[Test]  Epoch: 22	Loss: 0.055952	Acc: 14.8% (1479/10000)
[Test]  Epoch: 23	Loss: 0.055632	Acc: 15.0% (1496/10000)
[Test]  Epoch: 24	Loss: 0.055792	Acc: 14.8% (1484/10000)
[Test]  Epoch: 25	Loss: 0.055587	Acc: 15.3% (1532/10000)
[Test]  Epoch: 26	Loss: 0.055372	Acc: 15.8% (1580/10000)
[Test]  Epoch: 27	Loss: 0.055636	Acc: 15.1% (1513/10000)
[Test]  Epoch: 28	Loss: 0.055461	Acc: 15.2% (1524/10000)
[Test]  Epoch: 29	Loss: 0.055455	Acc: 15.3% (1534/10000)
[Test]  Epoch: 30	Loss: 0.055308	Acc: 15.4% (1539/10000)
[Test]  Epoch: 31	Loss: 0.055145	Acc: 15.8% (1584/10000)
[Test]  Epoch: 32	Loss: 0.055134	Acc: 15.8% (1585/10000)
[Test]  Epoch: 33	Loss: 0.054833	Acc: 15.9% (1587/10000)
[Test]  Epoch: 34	Loss: 0.054715	Acc: 16.5% (1646/10000)
[Test]  Epoch: 35	Loss: 0.054661	Acc: 15.8% (1581/10000)
[Test]  Epoch: 36	Loss: 0.054602	Acc: 16.2% (1619/10000)
[Test]  Epoch: 37	Loss: 0.054831	Acc: 16.1% (1608/10000)
[Test]  Epoch: 38	Loss: 0.054474	Acc: 16.6% (1663/10000)
[Test]  Epoch: 39	Loss: 0.054462	Acc: 16.2% (1616/10000)
[Test]  Epoch: 40	Loss: 0.054481	Acc: 16.6% (1656/10000)
[Test]  Epoch: 41	Loss: 0.054530	Acc: 16.2% (1625/10000)
[Test]  Epoch: 42	Loss: 0.054360	Acc: 17.0% (1702/10000)
[Test]  Epoch: 43	Loss: 0.054438	Acc: 16.7% (1670/10000)
[Test]  Epoch: 44	Loss: 0.054460	Acc: 16.5% (1649/10000)
[Test]  Epoch: 45	Loss: 0.054109	Acc: 16.7% (1673/10000)
[Test]  Epoch: 46	Loss: 0.054076	Acc: 16.8% (1681/10000)
[Test]  Epoch: 47	Loss: 0.053735	Acc: 17.1% (1705/10000)
[Test]  Epoch: 48	Loss: 0.059806	Acc: 13.1% (1313/10000)
[Test]  Epoch: 49	Loss: 0.058482	Acc: 15.1% (1505/10000)
[Test]  Epoch: 50	Loss: 0.054294	Acc: 17.7% (1774/10000)
[Test]  Epoch: 51	Loss: 0.053966	Acc: 17.2% (1716/10000)
[Test]  Epoch: 52	Loss: 0.053641	Acc: 17.7% (1768/10000)
[Test]  Epoch: 53	Loss: 0.053778	Acc: 17.8% (1778/10000)
[Test]  Epoch: 54	Loss: 0.053466	Acc: 17.7% (1771/10000)
[Test]  Epoch: 55	Loss: 0.053397	Acc: 18.1% (1806/10000)
[Test]  Epoch: 56	Loss: 0.053128	Acc: 18.2% (1818/10000)
[Test]  Epoch: 57	Loss: 0.053562	Acc: 17.7% (1767/10000)
[Test]  Epoch: 58	Loss: 0.052908	Acc: 18.8% (1882/10000)
[Test]  Epoch: 59	Loss: 0.053329	Acc: 18.2% (1823/10000)
[Test]  Epoch: 60	Loss: 0.052719	Acc: 18.7% (1868/10000)
[Test]  Epoch: 61	Loss: 0.052766	Acc: 18.8% (1879/10000)
[Test]  Epoch: 62	Loss: 0.052732	Acc: 18.7% (1874/10000)
[Test]  Epoch: 63	Loss: 0.052668	Acc: 18.9% (1889/10000)
[Test]  Epoch: 64	Loss: 0.052631	Acc: 19.1% (1914/10000)
[Test]  Epoch: 65	Loss: 0.052579	Acc: 18.9% (1889/10000)
[Test]  Epoch: 66	Loss: 0.052610	Acc: 18.7% (1866/10000)
[Test]  Epoch: 67	Loss: 0.052585	Acc: 18.8% (1882/10000)
[Test]  Epoch: 68	Loss: 0.052660	Acc: 18.9% (1888/10000)
[Test]  Epoch: 69	Loss: 0.052573	Acc: 19.2% (1919/10000)
[Test]  Epoch: 70	Loss: 0.052604	Acc: 19.0% (1900/10000)
[Test]  Epoch: 71	Loss: 0.052548	Acc: 19.2% (1918/10000)
[Test]  Epoch: 72	Loss: 0.052674	Acc: 18.7% (1870/10000)
[Test]  Epoch: 73	Loss: 0.052518	Acc: 19.0% (1899/10000)
[Test]  Epoch: 74	Loss: 0.052465	Acc: 19.1% (1905/10000)
[Test]  Epoch: 75	Loss: 0.052558	Acc: 18.8% (1880/10000)
[Test]  Epoch: 76	Loss: 0.052530	Acc: 19.1% (1909/10000)
[Test]  Epoch: 77	Loss: 0.052479	Acc: 18.9% (1889/10000)
[Test]  Epoch: 78	Loss: 0.052485	Acc: 19.0% (1897/10000)
[Test]  Epoch: 79	Loss: 0.052496	Acc: 19.2% (1925/10000)
[Test]  Epoch: 80	Loss: 0.052535	Acc: 18.9% (1890/10000)
[Test]  Epoch: 81	Loss: 0.052433	Acc: 19.0% (1903/10000)
[Test]  Epoch: 82	Loss: 0.052326	Acc: 19.0% (1900/10000)
[Test]  Epoch: 83	Loss: 0.052375	Acc: 19.0% (1897/10000)
[Test]  Epoch: 84	Loss: 0.052462	Acc: 19.1% (1915/10000)
[Test]  Epoch: 85	Loss: 0.052378	Acc: 19.1% (1908/10000)
[Test]  Epoch: 86	Loss: 0.052429	Acc: 19.2% (1916/10000)
[Test]  Epoch: 87	Loss: 0.052455	Acc: 19.1% (1911/10000)
[Test]  Epoch: 88	Loss: 0.052394	Acc: 19.1% (1914/10000)
[Test]  Epoch: 89	Loss: 0.052530	Acc: 18.8% (1880/10000)
[Test]  Epoch: 90	Loss: 0.052477	Acc: 18.9% (1887/10000)
[Test]  Epoch: 91	Loss: 0.052436	Acc: 18.9% (1886/10000)
[Test]  Epoch: 92	Loss: 0.052426	Acc: 18.8% (1880/10000)
[Test]  Epoch: 93	Loss: 0.052403	Acc: 19.2% (1925/10000)
[Test]  Epoch: 94	Loss: 0.052339	Acc: 18.9% (1894/10000)
[Test]  Epoch: 95	Loss: 0.052388	Acc: 19.0% (1900/10000)
[Test]  Epoch: 96	Loss: 0.052421	Acc: 18.8% (1880/10000)
[Test]  Epoch: 97	Loss: 0.052357	Acc: 19.0% (1899/10000)
[Test]  Epoch: 98	Loss: 0.052239	Acc: 19.2% (1920/10000)
[Test]  Epoch: 99	Loss: 0.052235	Acc: 19.1% (1911/10000)
[Test]  Epoch: 100	Loss: 0.052316	Acc: 19.1% (1908/10000)
===========finish==========
['2024-08-19', '00:43:02.226005', '100', 'test', '0.052315875005722046', '19.08', '19.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.022280	Acc: 70.9% (7088/10000)
[Test]  Epoch: 2	Loss: 0.022285	Acc: 70.9% (7092/10000)
[Test]  Epoch: 3	Loss: 0.022081	Acc: 70.8% (7084/10000)
[Test]  Epoch: 4	Loss: 0.022159	Acc: 70.7% (7067/10000)
[Test]  Epoch: 5	Loss: 0.022167	Acc: 70.9% (7094/10000)
[Test]  Epoch: 6	Loss: 0.022124	Acc: 70.9% (7089/10000)
[Test]  Epoch: 7	Loss: 0.021713	Acc: 71.0% (7103/10000)
[Test]  Epoch: 8	Loss: 0.021886	Acc: 71.0% (7101/10000)
[Test]  Epoch: 9	Loss: 0.021908	Acc: 71.3% (7128/10000)
[Test]  Epoch: 10	Loss: 0.021826	Acc: 71.0% (7099/10000)
[Test]  Epoch: 11	Loss: 0.022078	Acc: 70.9% (7094/10000)
[Test]  Epoch: 12	Loss: 0.022044	Acc: 70.6% (7057/10000)
[Test]  Epoch: 13	Loss: 0.022128	Acc: 70.4% (7038/10000)
[Test]  Epoch: 14	Loss: 0.022019	Acc: 70.7% (7071/10000)
[Test]  Epoch: 15	Loss: 0.022230	Acc: 70.6% (7057/10000)
[Test]  Epoch: 16	Loss: 0.022209	Acc: 70.6% (7060/10000)
[Test]  Epoch: 17	Loss: 0.021906	Acc: 70.8% (7079/10000)
[Test]  Epoch: 18	Loss: 0.021981	Acc: 70.7% (7070/10000)
[Test]  Epoch: 19	Loss: 0.022337	Acc: 70.8% (7078/10000)
[Test]  Epoch: 20	Loss: 0.021927	Acc: 71.0% (7095/10000)
[Test]  Epoch: 21	Loss: 0.022083	Acc: 70.4% (7041/10000)
[Test]  Epoch: 22	Loss: 0.022259	Acc: 70.6% (7060/10000)
[Test]  Epoch: 23	Loss: 0.021849	Acc: 70.4% (7042/10000)
[Test]  Epoch: 24	Loss: 0.022142	Acc: 70.8% (7078/10000)
[Test]  Epoch: 25	Loss: 0.022065	Acc: 70.4% (7037/10000)
[Test]  Epoch: 26	Loss: 0.021969	Acc: 70.7% (7072/10000)
[Test]  Epoch: 27	Loss: 0.022131	Acc: 70.7% (7068/10000)
[Test]  Epoch: 28	Loss: 0.022203	Acc: 70.5% (7052/10000)
[Test]  Epoch: 29	Loss: 0.021883	Acc: 70.3% (7033/10000)
[Test]  Epoch: 30	Loss: 0.022223	Acc: 70.7% (7071/10000)
[Test]  Epoch: 31	Loss: 0.022182	Acc: 70.5% (7048/10000)
[Test]  Epoch: 32	Loss: 0.021937	Acc: 70.8% (7076/10000)
[Test]  Epoch: 33	Loss: 0.022073	Acc: 71.0% (7099/10000)
[Test]  Epoch: 34	Loss: 0.021819	Acc: 71.2% (7118/10000)
[Test]  Epoch: 35	Loss: 0.022108	Acc: 70.6% (7063/10000)
[Test]  Epoch: 36	Loss: 0.021977	Acc: 70.7% (7072/10000)
[Test]  Epoch: 37	Loss: 0.021867	Acc: 70.8% (7079/10000)
[Test]  Epoch: 38	Loss: 0.022326	Acc: 70.1% (7011/10000)
[Test]  Epoch: 39	Loss: 0.022028	Acc: 70.5% (7047/10000)
[Test]  Epoch: 40	Loss: 0.021953	Acc: 70.8% (7079/10000)
[Test]  Epoch: 41	Loss: 0.022091	Acc: 70.5% (7055/10000)
[Test]  Epoch: 42	Loss: 0.021919	Acc: 70.4% (7041/10000)
[Test]  Epoch: 43	Loss: 0.021995	Acc: 70.4% (7036/10000)
[Test]  Epoch: 44	Loss: 0.021871	Acc: 71.0% (7097/10000)
[Test]  Epoch: 45	Loss: 0.021749	Acc: 71.0% (7098/10000)
[Test]  Epoch: 46	Loss: 0.021956	Acc: 70.8% (7079/10000)
[Test]  Epoch: 47	Loss: 0.021902	Acc: 70.6% (7056/10000)
[Test]  Epoch: 48	Loss: 0.021695	Acc: 70.8% (7076/10000)
[Test]  Epoch: 49	Loss: 0.021502	Acc: 71.2% (7118/10000)
[Test]  Epoch: 50	Loss: 0.021722	Acc: 70.6% (7058/10000)
[Test]  Epoch: 51	Loss: 0.021754	Acc: 70.9% (7093/10000)
[Test]  Epoch: 52	Loss: 0.021761	Acc: 70.6% (7058/10000)
[Test]  Epoch: 53	Loss: 0.021903	Acc: 70.2% (7021/10000)
[Test]  Epoch: 54	Loss: 0.021973	Acc: 70.6% (7062/10000)
[Test]  Epoch: 55	Loss: 0.021940	Acc: 70.8% (7079/10000)
[Test]  Epoch: 56	Loss: 0.021788	Acc: 70.5% (7055/10000)
[Test]  Epoch: 57	Loss: 0.021726	Acc: 70.7% (7069/10000)
[Test]  Epoch: 58	Loss: 0.022229	Acc: 70.5% (7049/10000)
[Test]  Epoch: 59	Loss: 0.021809	Acc: 71.1% (7112/10000)
[Test]  Epoch: 60	Loss: 0.022043	Acc: 70.4% (7039/10000)
[Test]  Epoch: 61	Loss: 0.021827	Acc: 70.4% (7042/10000)
[Test]  Epoch: 62	Loss: 0.021890	Acc: 70.7% (7065/10000)
[Test]  Epoch: 63	Loss: 0.021960	Acc: 70.5% (7054/10000)
[Test]  Epoch: 64	Loss: 0.021782	Acc: 70.5% (7052/10000)
[Test]  Epoch: 65	Loss: 0.022041	Acc: 70.4% (7039/10000)
[Test]  Epoch: 66	Loss: 0.021835	Acc: 70.8% (7078/10000)
[Test]  Epoch: 67	Loss: 0.021970	Acc: 70.9% (7093/10000)
[Test]  Epoch: 68	Loss: 0.021851	Acc: 71.0% (7103/10000)
[Test]  Epoch: 69	Loss: 0.021870	Acc: 70.7% (7069/10000)
[Test]  Epoch: 70	Loss: 0.022092	Acc: 70.5% (7050/10000)
[Test]  Epoch: 71	Loss: 0.021662	Acc: 70.2% (7020/10000)
[Test]  Epoch: 72	Loss: 0.022002	Acc: 70.8% (7078/10000)
[Test]  Epoch: 73	Loss: 0.021857	Acc: 70.6% (7056/10000)
[Test]  Epoch: 74	Loss: 0.021679	Acc: 70.9% (7089/10000)
[Test]  Epoch: 75	Loss: 0.021588	Acc: 71.3% (7128/10000)
[Test]  Epoch: 76	Loss: 0.022096	Acc: 69.9% (6992/10000)
[Test]  Epoch: 77	Loss: 0.021889	Acc: 70.6% (7059/10000)
[Test]  Epoch: 78	Loss: 0.022238	Acc: 70.1% (7008/10000)
[Test]  Epoch: 79	Loss: 0.021605	Acc: 70.9% (7093/10000)
[Test]  Epoch: 80	Loss: 0.021897	Acc: 70.7% (7066/10000)
[Test]  Epoch: 81	Loss: 0.021758	Acc: 70.5% (7046/10000)
[Test]  Epoch: 82	Loss: 0.021790	Acc: 70.8% (7081/10000)
[Test]  Epoch: 83	Loss: 0.021771	Acc: 70.8% (7079/10000)
[Test]  Epoch: 84	Loss: 0.021570	Acc: 70.9% (7088/10000)
[Test]  Epoch: 85	Loss: 0.021849	Acc: 70.9% (7088/10000)
[Test]  Epoch: 86	Loss: 0.021663	Acc: 70.6% (7058/10000)
[Test]  Epoch: 87	Loss: 0.021728	Acc: 70.5% (7049/10000)
[Test]  Epoch: 88	Loss: 0.021786	Acc: 70.7% (7071/10000)
[Test]  Epoch: 89	Loss: 0.021488	Acc: 70.7% (7067/10000)
[Test]  Epoch: 90	Loss: 0.021749	Acc: 70.7% (7070/10000)
[Test]  Epoch: 91	Loss: 0.021973	Acc: 70.6% (7062/10000)
[Test]  Epoch: 92	Loss: 0.021710	Acc: 70.7% (7068/10000)
[Test]  Epoch: 93	Loss: 0.021874	Acc: 70.7% (7073/10000)
[Test]  Epoch: 94	Loss: 0.021711	Acc: 70.7% (7073/10000)
[Test]  Epoch: 95	Loss: 0.021791	Acc: 70.4% (7042/10000)
[Test]  Epoch: 96	Loss: 0.021737	Acc: 70.7% (7066/10000)
[Test]  Epoch: 97	Loss: 0.021838	Acc: 70.7% (7071/10000)
[Test]  Epoch: 98	Loss: 0.021845	Acc: 70.5% (7049/10000)
[Test]  Epoch: 99	Loss: 0.021727	Acc: 71.1% (7110/10000)
[Test]  Epoch: 100	Loss: 0.021926	Acc: 70.5% (7055/10000)
===========finish==========
['2024-08-19', '00:45:24.092901', '100', 'test', '0.021925929141044617', '70.55', '71.28']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.1.weight', 'features.41.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.045641	Acc: 66.6% (6662/10000)
[Test]  Epoch: 2	Loss: 0.043036	Acc: 67.8% (6781/10000)
[Test]  Epoch: 3	Loss: 0.042207	Acc: 67.5% (6753/10000)
[Test]  Epoch: 4	Loss: 0.044269	Acc: 66.5% (6646/10000)
[Test]  Epoch: 5	Loss: 0.046960	Acc: 65.6% (6561/10000)
[Test]  Epoch: 6	Loss: 0.044939	Acc: 66.0% (6604/10000)
[Test]  Epoch: 7	Loss: 0.043320	Acc: 66.6% (6663/10000)
[Test]  Epoch: 8	Loss: 0.043425	Acc: 66.5% (6645/10000)
[Test]  Epoch: 9	Loss: 0.042826	Acc: 66.5% (6653/10000)
[Test]  Epoch: 10	Loss: 0.041035	Acc: 66.6% (6659/10000)
[Test]  Epoch: 11	Loss: 0.041396	Acc: 66.4% (6636/10000)
[Test]  Epoch: 12	Loss: 0.041915	Acc: 66.3% (6634/10000)
[Test]  Epoch: 13	Loss: 0.042397	Acc: 66.0% (6600/10000)
[Test]  Epoch: 14	Loss: 0.040797	Acc: 66.4% (6644/10000)
[Test]  Epoch: 15	Loss: 0.041660	Acc: 65.9% (6586/10000)
[Test]  Epoch: 16	Loss: 0.041879	Acc: 65.9% (6591/10000)
[Test]  Epoch: 17	Loss: 0.040266	Acc: 66.5% (6645/10000)
[Test]  Epoch: 18	Loss: 0.040213	Acc: 66.8% (6682/10000)
[Test]  Epoch: 19	Loss: 0.040901	Acc: 66.2% (6618/10000)
[Test]  Epoch: 20	Loss: 0.040665	Acc: 66.0% (6596/10000)
[Test]  Epoch: 21	Loss: 0.040844	Acc: 66.1% (6610/10000)
[Test]  Epoch: 22	Loss: 0.041024	Acc: 66.2% (6616/10000)
[Test]  Epoch: 23	Loss: 0.040488	Acc: 66.0% (6597/10000)
[Test]  Epoch: 24	Loss: 0.040726	Acc: 66.0% (6600/10000)
[Test]  Epoch: 25	Loss: 0.040584	Acc: 65.7% (6572/10000)
[Test]  Epoch: 26	Loss: 0.039802	Acc: 66.4% (6638/10000)
[Test]  Epoch: 27	Loss: 0.040463	Acc: 66.0% (6599/10000)
[Test]  Epoch: 28	Loss: 0.040469	Acc: 66.0% (6595/10000)
[Test]  Epoch: 29	Loss: 0.040148	Acc: 65.6% (6563/10000)
[Test]  Epoch: 30	Loss: 0.040258	Acc: 66.4% (6643/10000)
[Test]  Epoch: 31	Loss: 0.039798	Acc: 66.4% (6638/10000)
[Test]  Epoch: 32	Loss: 0.039297	Acc: 66.1% (6610/10000)
[Test]  Epoch: 33	Loss: 0.039774	Acc: 66.1% (6612/10000)
[Test]  Epoch: 34	Loss: 0.039895	Acc: 66.2% (6619/10000)
[Test]  Epoch: 35	Loss: 0.040289	Acc: 65.5% (6555/10000)
[Test]  Epoch: 36	Loss: 0.039434	Acc: 65.7% (6574/10000)
[Test]  Epoch: 37	Loss: 0.039359	Acc: 65.8% (6579/10000)
[Test]  Epoch: 38	Loss: 0.040320	Acc: 65.2% (6517/10000)
[Test]  Epoch: 39	Loss: 0.039490	Acc: 66.0% (6600/10000)
[Test]  Epoch: 40	Loss: 0.038765	Acc: 66.0% (6605/10000)
[Test]  Epoch: 41	Loss: 0.039409	Acc: 66.0% (6599/10000)
[Test]  Epoch: 42	Loss: 0.038831	Acc: 65.7% (6566/10000)
[Test]  Epoch: 43	Loss: 0.038951	Acc: 66.0% (6605/10000)
[Test]  Epoch: 44	Loss: 0.038602	Acc: 66.3% (6629/10000)
[Test]  Epoch: 45	Loss: 0.039115	Acc: 65.8% (6581/10000)
[Test]  Epoch: 46	Loss: 0.038739	Acc: 66.1% (6607/10000)
[Test]  Epoch: 47	Loss: 0.039310	Acc: 65.5% (6552/10000)
[Test]  Epoch: 48	Loss: 0.038703	Acc: 66.0% (6596/10000)
[Test]  Epoch: 49	Loss: 0.037790	Acc: 66.3% (6632/10000)
[Test]  Epoch: 50	Loss: 0.038180	Acc: 66.3% (6628/10000)
[Test]  Epoch: 51	Loss: 0.038542	Acc: 65.8% (6582/10000)
[Test]  Epoch: 52	Loss: 0.038440	Acc: 66.1% (6607/10000)
[Test]  Epoch: 53	Loss: 0.038760	Acc: 65.8% (6578/10000)
[Test]  Epoch: 54	Loss: 0.038649	Acc: 65.9% (6589/10000)
[Test]  Epoch: 55	Loss: 0.038302	Acc: 65.8% (6576/10000)
[Test]  Epoch: 56	Loss: 0.038416	Acc: 65.8% (6575/10000)
[Test]  Epoch: 57	Loss: 0.038287	Acc: 65.4% (6540/10000)
[Test]  Epoch: 58	Loss: 0.039289	Acc: 65.3% (6535/10000)
[Test]  Epoch: 59	Loss: 0.038652	Acc: 65.7% (6570/10000)
[Test]  Epoch: 60	Loss: 0.038469	Acc: 65.8% (6581/10000)
[Test]  Epoch: 61	Loss: 0.038508	Acc: 65.4% (6544/10000)
[Test]  Epoch: 62	Loss: 0.038450	Acc: 65.6% (6562/10000)
[Test]  Epoch: 63	Loss: 0.038426	Acc: 65.7% (6570/10000)
[Test]  Epoch: 64	Loss: 0.037897	Acc: 65.8% (6581/10000)
[Test]  Epoch: 65	Loss: 0.039091	Acc: 65.5% (6554/10000)
[Test]  Epoch: 66	Loss: 0.038147	Acc: 65.9% (6588/10000)
[Test]  Epoch: 67	Loss: 0.038318	Acc: 65.5% (6548/10000)
[Test]  Epoch: 68	Loss: 0.037821	Acc: 65.8% (6579/10000)
[Test]  Epoch: 69	Loss: 0.038053	Acc: 65.8% (6583/10000)
[Test]  Epoch: 70	Loss: 0.038208	Acc: 65.5% (6552/10000)
[Test]  Epoch: 71	Loss: 0.037613	Acc: 66.0% (6601/10000)
[Test]  Epoch: 72	Loss: 0.037909	Acc: 66.1% (6610/10000)
[Test]  Epoch: 73	Loss: 0.037948	Acc: 65.4% (6540/10000)
[Test]  Epoch: 74	Loss: 0.037661	Acc: 66.1% (6614/10000)
[Test]  Epoch: 75	Loss: 0.037855	Acc: 66.2% (6621/10000)
[Test]  Epoch: 76	Loss: 0.038436	Acc: 65.5% (6547/10000)
[Test]  Epoch: 77	Loss: 0.037511	Acc: 66.2% (6616/10000)
[Test]  Epoch: 78	Loss: 0.038295	Acc: 65.8% (6575/10000)
[Test]  Epoch: 79	Loss: 0.037944	Acc: 66.1% (6609/10000)
[Test]  Epoch: 80	Loss: 0.037845	Acc: 65.6% (6557/10000)
[Test]  Epoch: 81	Loss: 0.037685	Acc: 66.0% (6602/10000)
[Test]  Epoch: 82	Loss: 0.038061	Acc: 65.8% (6576/10000)
[Test]  Epoch: 83	Loss: 0.037975	Acc: 65.9% (6592/10000)
[Test]  Epoch: 84	Loss: 0.037892	Acc: 65.6% (6561/10000)
[Test]  Epoch: 85	Loss: 0.037776	Acc: 65.8% (6579/10000)
[Test]  Epoch: 86	Loss: 0.037477	Acc: 66.0% (6596/10000)
[Test]  Epoch: 87	Loss: 0.037549	Acc: 65.8% (6581/10000)
[Test]  Epoch: 88	Loss: 0.037730	Acc: 65.7% (6569/10000)
[Test]  Epoch: 89	Loss: 0.037264	Acc: 66.2% (6620/10000)
[Test]  Epoch: 90	Loss: 0.037843	Acc: 66.0% (6598/10000)
[Test]  Epoch: 91	Loss: 0.038234	Acc: 65.8% (6579/10000)
[Test]  Epoch: 92	Loss: 0.037339	Acc: 66.4% (6641/10000)
[Test]  Epoch: 93	Loss: 0.038017	Acc: 65.6% (6556/10000)
[Test]  Epoch: 94	Loss: 0.037361	Acc: 65.9% (6587/10000)
[Test]  Epoch: 95	Loss: 0.037458	Acc: 65.7% (6571/10000)
[Test]  Epoch: 96	Loss: 0.037763	Acc: 65.8% (6580/10000)
[Test]  Epoch: 97	Loss: 0.037568	Acc: 65.6% (6564/10000)
[Test]  Epoch: 98	Loss: 0.037913	Acc: 66.0% (6598/10000)
[Test]  Epoch: 99	Loss: 0.037708	Acc: 65.6% (6557/10000)
[Test]  Epoch: 100	Loss: 0.037409	Acc: 66.2% (6615/10000)
===========finish==========
['2024-08-19', '00:48:05.195119', '100', 'test', '0.03740874907970428', '66.15', '67.81']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.055937	Acc: 59.6% (5960/10000)
[Test]  Epoch: 2	Loss: 0.047933	Acc: 64.2% (6423/10000)
[Test]  Epoch: 3	Loss: 0.046283	Acc: 65.3% (6526/10000)
[Test]  Epoch: 4	Loss: 0.045630	Acc: 64.9% (6487/10000)
[Test]  Epoch: 5	Loss: 0.046280	Acc: 64.6% (6457/10000)
[Test]  Epoch: 6	Loss: 0.047115	Acc: 64.2% (6424/10000)
[Test]  Epoch: 7	Loss: 0.044967	Acc: 64.4% (6442/10000)
[Test]  Epoch: 8	Loss: 0.045557	Acc: 64.4% (6441/10000)
[Test]  Epoch: 9	Loss: 0.045416	Acc: 64.6% (6457/10000)
[Test]  Epoch: 10	Loss: 0.043898	Acc: 64.7% (6470/10000)
[Test]  Epoch: 11	Loss: 0.043138	Acc: 65.0% (6500/10000)
[Test]  Epoch: 12	Loss: 0.042854	Acc: 65.1% (6508/10000)
[Test]  Epoch: 13	Loss: 0.043323	Acc: 64.7% (6473/10000)
[Test]  Epoch: 14	Loss: 0.042121	Acc: 65.3% (6526/10000)
[Test]  Epoch: 15	Loss: 0.042057	Acc: 64.7% (6473/10000)
[Test]  Epoch: 16	Loss: 0.042529	Acc: 65.0% (6503/10000)
[Test]  Epoch: 17	Loss: 0.041012	Acc: 65.6% (6558/10000)
[Test]  Epoch: 18	Loss: 0.041656	Acc: 65.2% (6516/10000)
[Test]  Epoch: 19	Loss: 0.042440	Acc: 65.0% (6495/10000)
[Test]  Epoch: 20	Loss: 0.040702	Acc: 65.8% (6575/10000)
[Test]  Epoch: 21	Loss: 0.041563	Acc: 65.2% (6524/10000)
[Test]  Epoch: 22	Loss: 0.041560	Acc: 64.9% (6488/10000)
[Test]  Epoch: 23	Loss: 0.040928	Acc: 65.7% (6565/10000)
[Test]  Epoch: 24	Loss: 0.041532	Acc: 65.4% (6544/10000)
[Test]  Epoch: 25	Loss: 0.041102	Acc: 64.8% (6480/10000)
[Test]  Epoch: 26	Loss: 0.041457	Acc: 65.2% (6516/10000)
[Test]  Epoch: 27	Loss: 0.040899	Acc: 65.3% (6530/10000)
[Test]  Epoch: 28	Loss: 0.040872	Acc: 64.8% (6478/10000)
[Test]  Epoch: 29	Loss: 0.041240	Acc: 65.0% (6499/10000)
[Test]  Epoch: 30	Loss: 0.042300	Acc: 64.4% (6443/10000)
[Test]  Epoch: 31	Loss: 0.041654	Acc: 65.0% (6505/10000)
[Test]  Epoch: 32	Loss: 0.040280	Acc: 64.9% (6489/10000)
[Test]  Epoch: 33	Loss: 0.040814	Acc: 65.2% (6518/10000)
[Test]  Epoch: 34	Loss: 0.040980	Acc: 65.1% (6508/10000)
[Test]  Epoch: 35	Loss: 0.040377	Acc: 64.7% (6467/10000)
[Test]  Epoch: 36	Loss: 0.040714	Acc: 64.8% (6481/10000)
[Test]  Epoch: 37	Loss: 0.040001	Acc: 65.2% (6524/10000)
[Test]  Epoch: 38	Loss: 0.040111	Acc: 64.5% (6455/10000)
[Test]  Epoch: 39	Loss: 0.039726	Acc: 65.0% (6500/10000)
[Test]  Epoch: 40	Loss: 0.039936	Acc: 64.6% (6463/10000)
[Test]  Epoch: 41	Loss: 0.039453	Acc: 64.9% (6491/10000)
[Test]  Epoch: 42	Loss: 0.039909	Acc: 64.6% (6460/10000)
[Test]  Epoch: 43	Loss: 0.039685	Acc: 64.9% (6489/10000)
[Test]  Epoch: 44	Loss: 0.040470	Acc: 64.8% (6478/10000)
[Test]  Epoch: 45	Loss: 0.039851	Acc: 64.7% (6466/10000)
[Test]  Epoch: 46	Loss: 0.041093	Acc: 64.1% (6406/10000)
[Test]  Epoch: 47	Loss: 0.040277	Acc: 64.3% (6427/10000)
[Test]  Epoch: 48	Loss: 0.038899	Acc: 65.1% (6509/10000)
[Test]  Epoch: 49	Loss: 0.038522	Acc: 65.1% (6506/10000)
[Test]  Epoch: 50	Loss: 0.038579	Acc: 64.9% (6491/10000)
[Test]  Epoch: 51	Loss: 0.039420	Acc: 64.9% (6490/10000)
[Test]  Epoch: 52	Loss: 0.038711	Acc: 64.5% (6452/10000)
[Test]  Epoch: 53	Loss: 0.038950	Acc: 64.8% (6482/10000)
[Test]  Epoch: 54	Loss: 0.038598	Acc: 64.9% (6486/10000)
[Test]  Epoch: 55	Loss: 0.038458	Acc: 64.8% (6482/10000)
[Test]  Epoch: 56	Loss: 0.038658	Acc: 65.0% (6498/10000)
[Test]  Epoch: 57	Loss: 0.038381	Acc: 65.0% (6500/10000)
[Test]  Epoch: 58	Loss: 0.039359	Acc: 64.7% (6472/10000)
[Test]  Epoch: 59	Loss: 0.037874	Acc: 65.6% (6564/10000)
[Test]  Epoch: 60	Loss: 0.038252	Acc: 65.2% (6524/10000)
[Test]  Epoch: 61	Loss: 0.037867	Acc: 64.9% (6486/10000)
[Test]  Epoch: 62	Loss: 0.038191	Acc: 64.7% (6470/10000)
[Test]  Epoch: 63	Loss: 0.038311	Acc: 65.2% (6523/10000)
[Test]  Epoch: 64	Loss: 0.038245	Acc: 64.6% (6463/10000)
[Test]  Epoch: 65	Loss: 0.038696	Acc: 65.3% (6526/10000)
[Test]  Epoch: 66	Loss: 0.038003	Acc: 65.1% (6513/10000)
[Test]  Epoch: 67	Loss: 0.037760	Acc: 65.3% (6527/10000)
[Test]  Epoch: 68	Loss: 0.037987	Acc: 65.7% (6567/10000)
[Test]  Epoch: 69	Loss: 0.038322	Acc: 64.7% (6470/10000)
[Test]  Epoch: 70	Loss: 0.038292	Acc: 65.3% (6535/10000)
[Test]  Epoch: 71	Loss: 0.038054	Acc: 65.0% (6502/10000)
[Test]  Epoch: 72	Loss: 0.038151	Acc: 65.2% (6515/10000)
[Test]  Epoch: 73	Loss: 0.037744	Acc: 64.7% (6471/10000)
[Test]  Epoch: 74	Loss: 0.037557	Acc: 65.5% (6554/10000)
[Test]  Epoch: 75	Loss: 0.037689	Acc: 64.9% (6487/10000)
[Test]  Epoch: 76	Loss: 0.038662	Acc: 64.7% (6472/10000)
[Test]  Epoch: 77	Loss: 0.037517	Acc: 65.2% (6520/10000)
[Test]  Epoch: 78	Loss: 0.037997	Acc: 64.6% (6458/10000)
[Test]  Epoch: 79	Loss: 0.037753	Acc: 65.5% (6552/10000)
[Test]  Epoch: 80	Loss: 0.037807	Acc: 64.9% (6490/10000)
[Test]  Epoch: 81	Loss: 0.037741	Acc: 64.8% (6477/10000)
[Test]  Epoch: 82	Loss: 0.037803	Acc: 65.3% (6534/10000)
[Test]  Epoch: 83	Loss: 0.037984	Acc: 64.8% (6482/10000)
[Test]  Epoch: 84	Loss: 0.037942	Acc: 64.8% (6484/10000)
[Test]  Epoch: 85	Loss: 0.037704	Acc: 65.3% (6530/10000)
[Test]  Epoch: 86	Loss: 0.037466	Acc: 65.0% (6504/10000)
[Test]  Epoch: 87	Loss: 0.037508	Acc: 65.0% (6495/10000)
[Test]  Epoch: 88	Loss: 0.037674	Acc: 65.2% (6517/10000)
[Test]  Epoch: 89	Loss: 0.037361	Acc: 65.2% (6523/10000)
[Test]  Epoch: 90	Loss: 0.038117	Acc: 65.0% (6503/10000)
[Test]  Epoch: 91	Loss: 0.038328	Acc: 65.0% (6495/10000)
[Test]  Epoch: 92	Loss: 0.037345	Acc: 65.3% (6528/10000)
[Test]  Epoch: 93	Loss: 0.037682	Acc: 64.9% (6489/10000)
[Test]  Epoch: 94	Loss: 0.037549	Acc: 65.2% (6519/10000)
[Test]  Epoch: 95	Loss: 0.037499	Acc: 65.3% (6533/10000)
[Test]  Epoch: 96	Loss: 0.037630	Acc: 65.1% (6514/10000)
[Test]  Epoch: 97	Loss: 0.038110	Acc: 64.5% (6445/10000)
[Test]  Epoch: 98	Loss: 0.037853	Acc: 65.3% (6529/10000)
[Test]  Epoch: 99	Loss: 0.037207	Acc: 65.5% (6549/10000)
[Test]  Epoch: 100	Loss: 0.037420	Acc: 65.2% (6519/10000)
===========finish==========
['2024-08-19', '00:50:42.686104', '100', 'test', '0.03741963803768158', '65.19', '65.75']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.15.weight', 'features.21.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.068803	Acc: 50.7% (5071/10000)
[Test]  Epoch: 2	Loss: 0.051089	Acc: 60.8% (6080/10000)
[Test]  Epoch: 3	Loss: 0.048035	Acc: 61.5% (6148/10000)
[Test]  Epoch: 4	Loss: 0.047266	Acc: 62.0% (6202/10000)
[Test]  Epoch: 5	Loss: 0.048103	Acc: 61.2% (6120/10000)
[Test]  Epoch: 6	Loss: 0.048559	Acc: 61.8% (6175/10000)
[Test]  Epoch: 7	Loss: 0.045207	Acc: 62.7% (6269/10000)
[Test]  Epoch: 8	Loss: 0.044894	Acc: 62.9% (6290/10000)
[Test]  Epoch: 9	Loss: 0.044553	Acc: 62.9% (6291/10000)
[Test]  Epoch: 10	Loss: 0.044505	Acc: 63.0% (6303/10000)
[Test]  Epoch: 11	Loss: 0.043876	Acc: 63.2% (6325/10000)
[Test]  Epoch: 12	Loss: 0.042707	Acc: 63.6% (6360/10000)
[Test]  Epoch: 13	Loss: 0.043571	Acc: 62.9% (6285/10000)
[Test]  Epoch: 14	Loss: 0.043395	Acc: 63.3% (6328/10000)
[Test]  Epoch: 15	Loss: 0.042550	Acc: 63.5% (6347/10000)
[Test]  Epoch: 16	Loss: 0.042231	Acc: 63.8% (6384/10000)
[Test]  Epoch: 17	Loss: 0.042094	Acc: 64.0% (6398/10000)
[Test]  Epoch: 18	Loss: 0.041635	Acc: 63.8% (6384/10000)
[Test]  Epoch: 19	Loss: 0.042401	Acc: 63.4% (6342/10000)
[Test]  Epoch: 20	Loss: 0.041890	Acc: 63.2% (6325/10000)
[Test]  Epoch: 21	Loss: 0.041587	Acc: 63.5% (6349/10000)
[Test]  Epoch: 22	Loss: 0.041649	Acc: 63.9% (6391/10000)
[Test]  Epoch: 23	Loss: 0.041275	Acc: 63.1% (6312/10000)
[Test]  Epoch: 24	Loss: 0.041597	Acc: 63.8% (6380/10000)
[Test]  Epoch: 25	Loss: 0.041280	Acc: 63.6% (6358/10000)
[Test]  Epoch: 26	Loss: 0.040709	Acc: 63.8% (6376/10000)
[Test]  Epoch: 27	Loss: 0.041476	Acc: 63.4% (6338/10000)
[Test]  Epoch: 28	Loss: 0.040476	Acc: 64.0% (6403/10000)
[Test]  Epoch: 29	Loss: 0.041387	Acc: 63.5% (6351/10000)
[Test]  Epoch: 30	Loss: 0.040843	Acc: 63.9% (6394/10000)
[Test]  Epoch: 31	Loss: 0.041280	Acc: 63.5% (6349/10000)
[Test]  Epoch: 32	Loss: 0.040438	Acc: 64.1% (6412/10000)
[Test]  Epoch: 33	Loss: 0.040195	Acc: 63.9% (6391/10000)
[Test]  Epoch: 34	Loss: 0.040400	Acc: 63.8% (6378/10000)
[Test]  Epoch: 35	Loss: 0.040120	Acc: 64.2% (6420/10000)
[Test]  Epoch: 36	Loss: 0.039876	Acc: 63.9% (6385/10000)
[Test]  Epoch: 37	Loss: 0.039765	Acc: 63.7% (6371/10000)
[Test]  Epoch: 38	Loss: 0.040703	Acc: 63.9% (6389/10000)
[Test]  Epoch: 39	Loss: 0.040742	Acc: 63.5% (6355/10000)
[Test]  Epoch: 40	Loss: 0.040427	Acc: 63.5% (6355/10000)
[Test]  Epoch: 41	Loss: 0.040450	Acc: 63.6% (6358/10000)
[Test]  Epoch: 42	Loss: 0.039765	Acc: 63.8% (6376/10000)
[Test]  Epoch: 43	Loss: 0.040147	Acc: 64.1% (6410/10000)
[Test]  Epoch: 44	Loss: 0.039499	Acc: 64.2% (6419/10000)
[Test]  Epoch: 45	Loss: 0.039511	Acc: 63.7% (6372/10000)
[Test]  Epoch: 46	Loss: 0.040160	Acc: 63.5% (6353/10000)
[Test]  Epoch: 47	Loss: 0.039927	Acc: 63.5% (6351/10000)
[Test]  Epoch: 48	Loss: 0.039491	Acc: 63.8% (6382/10000)
[Test]  Epoch: 49	Loss: 0.038926	Acc: 63.8% (6383/10000)
[Test]  Epoch: 50	Loss: 0.039185	Acc: 63.7% (6368/10000)
[Test]  Epoch: 51	Loss: 0.040040	Acc: 63.6% (6357/10000)
[Test]  Epoch: 52	Loss: 0.039760	Acc: 63.4% (6343/10000)
[Test]  Epoch: 53	Loss: 0.038840	Acc: 63.7% (6374/10000)
[Test]  Epoch: 54	Loss: 0.039178	Acc: 63.6% (6362/10000)
[Test]  Epoch: 55	Loss: 0.039058	Acc: 63.8% (6378/10000)
[Test]  Epoch: 56	Loss: 0.038739	Acc: 64.1% (6412/10000)
[Test]  Epoch: 57	Loss: 0.038579	Acc: 63.7% (6372/10000)
[Test]  Epoch: 58	Loss: 0.039143	Acc: 63.7% (6366/10000)
[Test]  Epoch: 59	Loss: 0.038805	Acc: 63.8% (6381/10000)
[Test]  Epoch: 60	Loss: 0.039136	Acc: 64.0% (6397/10000)
[Test]  Epoch: 61	Loss: 0.038606	Acc: 63.8% (6378/10000)
[Test]  Epoch: 62	Loss: 0.038792	Acc: 63.3% (6333/10000)
[Test]  Epoch: 63	Loss: 0.039056	Acc: 63.2% (6323/10000)
[Test]  Epoch: 64	Loss: 0.039179	Acc: 63.3% (6327/10000)
[Test]  Epoch: 65	Loss: 0.039124	Acc: 63.5% (6355/10000)
[Test]  Epoch: 66	Loss: 0.038465	Acc: 63.8% (6382/10000)
[Test]  Epoch: 67	Loss: 0.038388	Acc: 64.2% (6418/10000)
[Test]  Epoch: 68	Loss: 0.038951	Acc: 63.5% (6346/10000)
[Test]  Epoch: 69	Loss: 0.039046	Acc: 63.6% (6364/10000)
[Test]  Epoch: 70	Loss: 0.038577	Acc: 63.8% (6384/10000)
[Test]  Epoch: 71	Loss: 0.038533	Acc: 63.7% (6374/10000)
[Test]  Epoch: 72	Loss: 0.038740	Acc: 64.1% (6412/10000)
[Test]  Epoch: 73	Loss: 0.038887	Acc: 63.5% (6345/10000)
[Test]  Epoch: 74	Loss: 0.038433	Acc: 64.1% (6414/10000)
[Test]  Epoch: 75	Loss: 0.038621	Acc: 63.9% (6387/10000)
[Test]  Epoch: 76	Loss: 0.039202	Acc: 63.5% (6350/10000)
[Test]  Epoch: 77	Loss: 0.038685	Acc: 64.0% (6404/10000)
[Test]  Epoch: 78	Loss: 0.038967	Acc: 63.4% (6342/10000)
[Test]  Epoch: 79	Loss: 0.038753	Acc: 64.2% (6418/10000)
[Test]  Epoch: 80	Loss: 0.038403	Acc: 63.6% (6360/10000)
[Test]  Epoch: 81	Loss: 0.038461	Acc: 63.6% (6365/10000)
[Test]  Epoch: 82	Loss: 0.038283	Acc: 63.9% (6393/10000)
[Test]  Epoch: 83	Loss: 0.038774	Acc: 63.7% (6372/10000)
[Test]  Epoch: 84	Loss: 0.038413	Acc: 64.1% (6414/10000)
[Test]  Epoch: 85	Loss: 0.038439	Acc: 63.7% (6372/10000)
[Test]  Epoch: 86	Loss: 0.038119	Acc: 64.2% (6422/10000)
[Test]  Epoch: 87	Loss: 0.038093	Acc: 64.0% (6397/10000)
[Test]  Epoch: 88	Loss: 0.038325	Acc: 64.0% (6401/10000)
[Test]  Epoch: 89	Loss: 0.038082	Acc: 63.6% (6365/10000)
[Test]  Epoch: 90	Loss: 0.038793	Acc: 63.9% (6385/10000)
[Test]  Epoch: 91	Loss: 0.038658	Acc: 63.2% (6324/10000)
[Test]  Epoch: 92	Loss: 0.038319	Acc: 64.1% (6412/10000)
[Test]  Epoch: 93	Loss: 0.038342	Acc: 63.7% (6367/10000)
[Test]  Epoch: 94	Loss: 0.038353	Acc: 64.0% (6398/10000)
[Test]  Epoch: 95	Loss: 0.038184	Acc: 63.7% (6372/10000)
[Test]  Epoch: 96	Loss: 0.038608	Acc: 63.7% (6372/10000)
[Test]  Epoch: 97	Loss: 0.038668	Acc: 63.5% (6350/10000)
[Test]  Epoch: 98	Loss: 0.038452	Acc: 64.3% (6430/10000)
[Test]  Epoch: 99	Loss: 0.038002	Acc: 64.0% (6402/10000)
[Test]  Epoch: 100	Loss: 0.037887	Acc: 64.0% (6396/10000)
===========finish==========
['2024-08-19', '00:53:21.670102', '100', 'test', '0.03788725436925888', '63.96', '64.3']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.15.weight', 'features.21.weight', 'features.35.weight', 'features.28.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.048686	Acc: 52.9% (5286/10000)
[Test]  Epoch: 2	Loss: 0.041230	Acc: 62.5% (6255/10000)
[Test]  Epoch: 3	Loss: 0.041156	Acc: 63.2% (6325/10000)
[Test]  Epoch: 4	Loss: 0.040851	Acc: 63.6% (6363/10000)
[Test]  Epoch: 5	Loss: 0.040157	Acc: 64.4% (6436/10000)
[Test]  Epoch: 6	Loss: 0.039400	Acc: 63.9% (6392/10000)
[Test]  Epoch: 7	Loss: 0.038393	Acc: 64.2% (6424/10000)
[Test]  Epoch: 8	Loss: 0.039281	Acc: 64.1% (6414/10000)
[Test]  Epoch: 9	Loss: 0.038183	Acc: 64.9% (6490/10000)
[Test]  Epoch: 10	Loss: 0.037726	Acc: 64.7% (6472/10000)
[Test]  Epoch: 11	Loss: 0.038302	Acc: 64.0% (6397/10000)
[Test]  Epoch: 12	Loss: 0.037490	Acc: 64.6% (6463/10000)
[Test]  Epoch: 13	Loss: 0.038117	Acc: 64.2% (6420/10000)
[Test]  Epoch: 14	Loss: 0.038641	Acc: 64.2% (6423/10000)
[Test]  Epoch: 15	Loss: 0.038686	Acc: 64.1% (6412/10000)
[Test]  Epoch: 16	Loss: 0.038056	Acc: 64.0% (6404/10000)
[Test]  Epoch: 17	Loss: 0.037628	Acc: 64.5% (6454/10000)
[Test]  Epoch: 18	Loss: 0.037071	Acc: 64.7% (6470/10000)
[Test]  Epoch: 19	Loss: 0.037463	Acc: 64.0% (6401/10000)
[Test]  Epoch: 20	Loss: 0.037068	Acc: 64.5% (6453/10000)
[Test]  Epoch: 21	Loss: 0.037320	Acc: 64.8% (6484/10000)
[Test]  Epoch: 22	Loss: 0.036800	Acc: 65.0% (6502/10000)
[Test]  Epoch: 23	Loss: 0.036762	Acc: 64.8% (6480/10000)
[Test]  Epoch: 24	Loss: 0.037167	Acc: 64.6% (6456/10000)
[Test]  Epoch: 25	Loss: 0.036878	Acc: 64.4% (6438/10000)
[Test]  Epoch: 26	Loss: 0.036403	Acc: 64.4% (6437/10000)
[Test]  Epoch: 27	Loss: 0.036660	Acc: 65.1% (6509/10000)
[Test]  Epoch: 28	Loss: 0.035825	Acc: 64.8% (6475/10000)
[Test]  Epoch: 29	Loss: 0.036558	Acc: 64.2% (6419/10000)
[Test]  Epoch: 30	Loss: 0.036145	Acc: 64.9% (6488/10000)
[Test]  Epoch: 31	Loss: 0.036100	Acc: 64.7% (6468/10000)
[Test]  Epoch: 32	Loss: 0.036207	Acc: 64.5% (6451/10000)
[Test]  Epoch: 33	Loss: 0.035916	Acc: 65.0% (6497/10000)
[Test]  Epoch: 34	Loss: 0.035664	Acc: 64.7% (6468/10000)
[Test]  Epoch: 35	Loss: 0.035983	Acc: 64.6% (6459/10000)
[Test]  Epoch: 36	Loss: 0.035292	Acc: 65.0% (6501/10000)
[Test]  Epoch: 37	Loss: 0.035339	Acc: 64.7% (6472/10000)
[Test]  Epoch: 38	Loss: 0.035640	Acc: 64.9% (6489/10000)
[Test]  Epoch: 39	Loss: 0.035497	Acc: 65.1% (6509/10000)
[Test]  Epoch: 40	Loss: 0.035966	Acc: 64.3% (6426/10000)
[Test]  Epoch: 41	Loss: 0.035616	Acc: 64.6% (6461/10000)
[Test]  Epoch: 42	Loss: 0.035823	Acc: 64.2% (6415/10000)
[Test]  Epoch: 43	Loss: 0.035786	Acc: 64.5% (6445/10000)
[Test]  Epoch: 44	Loss: 0.034816	Acc: 65.0% (6500/10000)
[Test]  Epoch: 45	Loss: 0.034761	Acc: 64.8% (6483/10000)
[Test]  Epoch: 46	Loss: 0.035489	Acc: 64.6% (6459/10000)
[Test]  Epoch: 47	Loss: 0.035132	Acc: 64.4% (6441/10000)
[Test]  Epoch: 48	Loss: 0.034662	Acc: 64.6% (6462/10000)
[Test]  Epoch: 49	Loss: 0.034756	Acc: 64.5% (6450/10000)
[Test]  Epoch: 50	Loss: 0.034300	Acc: 64.8% (6484/10000)
[Test]  Epoch: 51	Loss: 0.034642	Acc: 64.9% (6486/10000)
[Test]  Epoch: 52	Loss: 0.034970	Acc: 64.2% (6424/10000)
[Test]  Epoch: 53	Loss: 0.034644	Acc: 64.2% (6425/10000)
[Test]  Epoch: 54	Loss: 0.034940	Acc: 64.7% (6470/10000)
[Test]  Epoch: 55	Loss: 0.033959	Acc: 64.7% (6469/10000)
[Test]  Epoch: 56	Loss: 0.034418	Acc: 64.6% (6463/10000)
[Test]  Epoch: 57	Loss: 0.034182	Acc: 64.7% (6466/10000)
[Test]  Epoch: 58	Loss: 0.034689	Acc: 64.7% (6469/10000)
[Test]  Epoch: 59	Loss: 0.034243	Acc: 65.0% (6497/10000)
[Test]  Epoch: 60	Loss: 0.034097	Acc: 64.7% (6465/10000)
[Test]  Epoch: 61	Loss: 0.034046	Acc: 64.8% (6482/10000)
[Test]  Epoch: 62	Loss: 0.034382	Acc: 64.7% (6474/10000)
[Test]  Epoch: 63	Loss: 0.034178	Acc: 64.7% (6473/10000)
[Test]  Epoch: 64	Loss: 0.034165	Acc: 65.0% (6505/10000)
[Test]  Epoch: 65	Loss: 0.034092	Acc: 64.8% (6482/10000)
[Test]  Epoch: 66	Loss: 0.033914	Acc: 65.1% (6507/10000)
[Test]  Epoch: 67	Loss: 0.034234	Acc: 64.7% (6465/10000)
[Test]  Epoch: 68	Loss: 0.034095	Acc: 64.5% (6447/10000)
[Test]  Epoch: 69	Loss: 0.034206	Acc: 64.6% (6457/10000)
[Test]  Epoch: 70	Loss: 0.034584	Acc: 65.0% (6499/10000)
[Test]  Epoch: 71	Loss: 0.034160	Acc: 64.8% (6478/10000)
[Test]  Epoch: 72	Loss: 0.034191	Acc: 65.2% (6515/10000)
[Test]  Epoch: 73	Loss: 0.034133	Acc: 64.7% (6472/10000)
[Test]  Epoch: 74	Loss: 0.033586	Acc: 65.4% (6539/10000)
[Test]  Epoch: 75	Loss: 0.033975	Acc: 65.3% (6532/10000)
[Test]  Epoch: 76	Loss: 0.034264	Acc: 64.5% (6445/10000)
[Test]  Epoch: 77	Loss: 0.034117	Acc: 64.8% (6478/10000)
[Test]  Epoch: 78	Loss: 0.034282	Acc: 64.5% (6447/10000)
[Test]  Epoch: 79	Loss: 0.034046	Acc: 65.3% (6531/10000)
[Test]  Epoch: 80	Loss: 0.033863	Acc: 64.4% (6437/10000)
[Test]  Epoch: 81	Loss: 0.033965	Acc: 65.0% (6496/10000)
[Test]  Epoch: 82	Loss: 0.033855	Acc: 64.9% (6492/10000)
[Test]  Epoch: 83	Loss: 0.033835	Acc: 64.6% (6464/10000)
[Test]  Epoch: 84	Loss: 0.034327	Acc: 65.0% (6498/10000)
[Test]  Epoch: 85	Loss: 0.034064	Acc: 64.9% (6487/10000)
[Test]  Epoch: 86	Loss: 0.033698	Acc: 64.8% (6485/10000)
[Test]  Epoch: 87	Loss: 0.033762	Acc: 64.8% (6479/10000)
[Test]  Epoch: 88	Loss: 0.033481	Acc: 65.1% (6512/10000)
[Test]  Epoch: 89	Loss: 0.033653	Acc: 64.8% (6479/10000)
[Test]  Epoch: 90	Loss: 0.034060	Acc: 64.7% (6474/10000)
[Test]  Epoch: 91	Loss: 0.034251	Acc: 64.8% (6485/10000)
[Test]  Epoch: 92	Loss: 0.033473	Acc: 65.1% (6514/10000)
[Test]  Epoch: 93	Loss: 0.033758	Acc: 65.0% (6503/10000)
[Test]  Epoch: 94	Loss: 0.034008	Acc: 64.8% (6477/10000)
[Test]  Epoch: 95	Loss: 0.033941	Acc: 64.3% (6431/10000)
[Test]  Epoch: 96	Loss: 0.034158	Acc: 64.6% (6464/10000)
[Test]  Epoch: 97	Loss: 0.034104	Acc: 64.6% (6464/10000)
[Test]  Epoch: 98	Loss: 0.033733	Acc: 64.8% (6476/10000)
[Test]  Epoch: 99	Loss: 0.033644	Acc: 64.7% (6471/10000)
[Test]  Epoch: 100	Loss: 0.033337	Acc: 64.9% (6489/10000)
===========finish==========
['2024-08-19', '00:55:51.815110', '100', 'test', '0.03333705942630768', '64.89', '65.39']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.15.weight', 'features.21.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.31.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.055276	Acc: 53.1% (5315/10000)
[Test]  Epoch: 2	Loss: 0.038944	Acc: 62.4% (6243/10000)
[Test]  Epoch: 3	Loss: 0.036637	Acc: 64.0% (6396/10000)
[Test]  Epoch: 4	Loss: 0.036008	Acc: 64.1% (6413/10000)
[Test]  Epoch: 5	Loss: 0.036239	Acc: 64.0% (6399/10000)
[Test]  Epoch: 6	Loss: 0.035419	Acc: 65.2% (6516/10000)
[Test]  Epoch: 7	Loss: 0.034861	Acc: 65.2% (6520/10000)
[Test]  Epoch: 8	Loss: 0.035277	Acc: 64.4% (6437/10000)
[Test]  Epoch: 9	Loss: 0.034665	Acc: 65.3% (6533/10000)
[Test]  Epoch: 10	Loss: 0.034848	Acc: 64.7% (6473/10000)
[Test]  Epoch: 11	Loss: 0.034199	Acc: 64.9% (6492/10000)
[Test]  Epoch: 12	Loss: 0.034706	Acc: 65.0% (6495/10000)
[Test]  Epoch: 13	Loss: 0.034894	Acc: 64.8% (6484/10000)
[Test]  Epoch: 14	Loss: 0.034402	Acc: 65.4% (6543/10000)
[Test]  Epoch: 15	Loss: 0.034568	Acc: 64.9% (6493/10000)
[Test]  Epoch: 16	Loss: 0.034513	Acc: 65.0% (6496/10000)
[Test]  Epoch: 17	Loss: 0.033868	Acc: 65.2% (6523/10000)
[Test]  Epoch: 18	Loss: 0.033842	Acc: 65.2% (6516/10000)
[Test]  Epoch: 19	Loss: 0.033830	Acc: 65.5% (6549/10000)
[Test]  Epoch: 20	Loss: 0.034073	Acc: 65.3% (6528/10000)
[Test]  Epoch: 21	Loss: 0.034032	Acc: 65.4% (6542/10000)
[Test]  Epoch: 22	Loss: 0.034030	Acc: 65.3% (6535/10000)
[Test]  Epoch: 23	Loss: 0.033711	Acc: 65.3% (6534/10000)
[Test]  Epoch: 24	Loss: 0.034685	Acc: 64.6% (6463/10000)
[Test]  Epoch: 25	Loss: 0.033940	Acc: 65.2% (6520/10000)
[Test]  Epoch: 26	Loss: 0.033629	Acc: 64.8% (6475/10000)
[Test]  Epoch: 27	Loss: 0.033790	Acc: 65.2% (6517/10000)
[Test]  Epoch: 28	Loss: 0.033625	Acc: 64.7% (6473/10000)
[Test]  Epoch: 29	Loss: 0.034057	Acc: 64.2% (6422/10000)
[Test]  Epoch: 30	Loss: 0.033645	Acc: 65.0% (6499/10000)
[Test]  Epoch: 31	Loss: 0.033733	Acc: 64.8% (6480/10000)
[Test]  Epoch: 32	Loss: 0.033517	Acc: 65.0% (6498/10000)
[Test]  Epoch: 33	Loss: 0.033363	Acc: 65.1% (6514/10000)
[Test]  Epoch: 34	Loss: 0.033268	Acc: 65.0% (6502/10000)
[Test]  Epoch: 35	Loss: 0.033050	Acc: 65.3% (6529/10000)
[Test]  Epoch: 36	Loss: 0.032952	Acc: 64.8% (6484/10000)
[Test]  Epoch: 37	Loss: 0.033063	Acc: 64.9% (6494/10000)
[Test]  Epoch: 38	Loss: 0.033208	Acc: 64.6% (6459/10000)
[Test]  Epoch: 39	Loss: 0.033003	Acc: 65.3% (6532/10000)
[Test]  Epoch: 40	Loss: 0.032793	Acc: 65.1% (6506/10000)
[Test]  Epoch: 41	Loss: 0.033003	Acc: 65.0% (6504/10000)
[Test]  Epoch: 42	Loss: 0.032766	Acc: 65.3% (6526/10000)
[Test]  Epoch: 43	Loss: 0.032848	Acc: 65.5% (6545/10000)
[Test]  Epoch: 44	Loss: 0.032302	Acc: 65.4% (6540/10000)
[Test]  Epoch: 45	Loss: 0.032595	Acc: 65.6% (6563/10000)
[Test]  Epoch: 46	Loss: 0.033030	Acc: 65.2% (6524/10000)
[Test]  Epoch: 47	Loss: 0.032569	Acc: 65.2% (6520/10000)
[Test]  Epoch: 48	Loss: 0.032375	Acc: 65.3% (6526/10000)
[Test]  Epoch: 49	Loss: 0.032484	Acc: 65.5% (6551/10000)
[Test]  Epoch: 50	Loss: 0.032570	Acc: 64.6% (6461/10000)
[Test]  Epoch: 51	Loss: 0.032441	Acc: 65.0% (6500/10000)
[Test]  Epoch: 52	Loss: 0.032409	Acc: 64.9% (6487/10000)
[Test]  Epoch: 53	Loss: 0.031996	Acc: 65.0% (6498/10000)
[Test]  Epoch: 54	Loss: 0.032477	Acc: 64.5% (6445/10000)
[Test]  Epoch: 55	Loss: 0.032222	Acc: 65.5% (6550/10000)
[Test]  Epoch: 56	Loss: 0.032199	Acc: 65.2% (6525/10000)
[Test]  Epoch: 57	Loss: 0.032078	Acc: 64.8% (6478/10000)
[Test]  Epoch: 58	Loss: 0.032243	Acc: 65.0% (6501/10000)
[Test]  Epoch: 59	Loss: 0.031935	Acc: 64.8% (6485/10000)
[Test]  Epoch: 60	Loss: 0.031783	Acc: 65.1% (6513/10000)
[Test]  Epoch: 61	Loss: 0.031781	Acc: 65.0% (6504/10000)
[Test]  Epoch: 62	Loss: 0.032029	Acc: 65.0% (6498/10000)
[Test]  Epoch: 63	Loss: 0.031943	Acc: 64.9% (6486/10000)
[Test]  Epoch: 64	Loss: 0.032144	Acc: 64.8% (6484/10000)
[Test]  Epoch: 65	Loss: 0.031845	Acc: 65.0% (6502/10000)
[Test]  Epoch: 66	Loss: 0.031780	Acc: 64.8% (6480/10000)
[Test]  Epoch: 67	Loss: 0.032223	Acc: 65.2% (6516/10000)
[Test]  Epoch: 68	Loss: 0.031968	Acc: 65.0% (6498/10000)
[Test]  Epoch: 69	Loss: 0.031964	Acc: 64.7% (6471/10000)
[Test]  Epoch: 70	Loss: 0.032064	Acc: 64.8% (6480/10000)
[Test]  Epoch: 71	Loss: 0.031943	Acc: 65.1% (6510/10000)
[Test]  Epoch: 72	Loss: 0.031634	Acc: 65.4% (6537/10000)
[Test]  Epoch: 73	Loss: 0.031841	Acc: 64.8% (6482/10000)
[Test]  Epoch: 74	Loss: 0.031424	Acc: 65.2% (6525/10000)
[Test]  Epoch: 75	Loss: 0.031727	Acc: 65.6% (6563/10000)
[Test]  Epoch: 76	Loss: 0.031891	Acc: 64.8% (6476/10000)
[Test]  Epoch: 77	Loss: 0.031749	Acc: 65.0% (6500/10000)
[Test]  Epoch: 78	Loss: 0.031933	Acc: 64.8% (6478/10000)
[Test]  Epoch: 79	Loss: 0.031596	Acc: 65.4% (6537/10000)
[Test]  Epoch: 80	Loss: 0.031889	Acc: 64.9% (6490/10000)
[Test]  Epoch: 81	Loss: 0.031697	Acc: 64.7% (6472/10000)
[Test]  Epoch: 82	Loss: 0.031620	Acc: 64.8% (6481/10000)
[Test]  Epoch: 83	Loss: 0.031595	Acc: 65.3% (6532/10000)
[Test]  Epoch: 84	Loss: 0.031909	Acc: 65.0% (6503/10000)
[Test]  Epoch: 85	Loss: 0.031521	Acc: 64.8% (6479/10000)
[Test]  Epoch: 86	Loss: 0.031572	Acc: 65.0% (6499/10000)
[Test]  Epoch: 87	Loss: 0.031537	Acc: 65.2% (6516/10000)
[Test]  Epoch: 88	Loss: 0.031373	Acc: 65.1% (6511/10000)
[Test]  Epoch: 89	Loss: 0.031703	Acc: 65.1% (6514/10000)
[Test]  Epoch: 90	Loss: 0.031867	Acc: 64.7% (6465/10000)
[Test]  Epoch: 91	Loss: 0.031888	Acc: 64.8% (6484/10000)
[Test]  Epoch: 92	Loss: 0.031317	Acc: 65.0% (6502/10000)
[Test]  Epoch: 93	Loss: 0.031703	Acc: 65.7% (6568/10000)
[Test]  Epoch: 94	Loss: 0.031616	Acc: 64.7% (6466/10000)
[Test]  Epoch: 95	Loss: 0.031505	Acc: 64.8% (6483/10000)
[Test]  Epoch: 96	Loss: 0.031762	Acc: 64.8% (6482/10000)
[Test]  Epoch: 97	Loss: 0.031620	Acc: 64.9% (6493/10000)
[Test]  Epoch: 98	Loss: 0.031713	Acc: 65.3% (6531/10000)
[Test]  Epoch: 99	Loss: 0.031423	Acc: 65.1% (6511/10000)
[Test]  Epoch: 100	Loss: 0.031248	Acc: 65.5% (6550/10000)
===========finish==========
['2024-08-19', '00:58:22.695271', '100', 'test', '0.03124829124212265', '65.5', '65.68']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.15.weight', 'features.21.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.31.weight', 'features.38.weight', 'features.0.weight', 'classifier.weight', 'features.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.071639	Acc: 2.3% (232/10000)
[Test]  Epoch: 2	Loss: 0.068364	Acc: 5.7% (574/10000)
[Test]  Epoch: 3	Loss: 0.066667	Acc: 6.3% (633/10000)
[Test]  Epoch: 4	Loss: 0.064384	Acc: 8.5% (847/10000)
[Test]  Epoch: 5	Loss: 0.063112	Acc: 10.2% (1018/10000)
[Test]  Epoch: 6	Loss: 0.061889	Acc: 11.1% (1106/10000)
[Test]  Epoch: 7	Loss: 0.061394	Acc: 11.4% (1137/10000)
[Test]  Epoch: 8	Loss: 0.059970	Acc: 13.4% (1336/10000)
[Test]  Epoch: 9	Loss: 0.059000	Acc: 13.6% (1355/10000)
[Test]  Epoch: 10	Loss: 0.058239	Acc: 14.6% (1461/10000)
[Test]  Epoch: 11	Loss: 0.058136	Acc: 15.7% (1568/10000)
[Test]  Epoch: 12	Loss: 0.056272	Acc: 16.5% (1646/10000)
[Test]  Epoch: 13	Loss: 0.057466	Acc: 16.1% (1606/10000)
[Test]  Epoch: 14	Loss: 0.055621	Acc: 17.8% (1780/10000)
[Test]  Epoch: 15	Loss: 0.055904	Acc: 17.5% (1746/10000)
[Test]  Epoch: 16	Loss: 0.054907	Acc: 18.5% (1854/10000)
[Test]  Epoch: 17	Loss: 0.053943	Acc: 19.6% (1958/10000)
[Test]  Epoch: 18	Loss: 0.054452	Acc: 19.9% (1994/10000)
[Test]  Epoch: 19	Loss: 0.054123	Acc: 20.4% (2038/10000)
[Test]  Epoch: 20	Loss: 0.054155	Acc: 20.1% (2012/10000)
[Test]  Epoch: 21	Loss: 0.053276	Acc: 21.6% (2158/10000)
[Test]  Epoch: 22	Loss: 0.052918	Acc: 22.5% (2254/10000)
[Test]  Epoch: 23	Loss: 0.052645	Acc: 22.6% (2263/10000)
[Test]  Epoch: 24	Loss: 0.052607	Acc: 23.1% (2313/10000)
[Test]  Epoch: 25	Loss: 0.052648	Acc: 23.0% (2304/10000)
[Test]  Epoch: 26	Loss: 0.053138	Acc: 23.1% (2305/10000)
[Test]  Epoch: 27	Loss: 0.052795	Acc: 23.5% (2350/10000)
[Test]  Epoch: 28	Loss: 0.053687	Acc: 23.1% (2312/10000)
[Test]  Epoch: 29	Loss: 0.052389	Acc: 24.6% (2459/10000)
[Test]  Epoch: 30	Loss: 0.052417	Acc: 25.1% (2509/10000)
[Test]  Epoch: 31	Loss: 0.052344	Acc: 25.5% (2547/10000)
[Test]  Epoch: 32	Loss: 0.052961	Acc: 25.0% (2503/10000)
[Test]  Epoch: 33	Loss: 0.053279	Acc: 25.2% (2523/10000)
[Test]  Epoch: 34	Loss: 0.053146	Acc: 25.6% (2559/10000)
[Test]  Epoch: 35	Loss: 0.052703	Acc: 25.9% (2586/10000)
[Test]  Epoch: 36	Loss: 0.053161	Acc: 25.4% (2537/10000)
[Test]  Epoch: 37	Loss: 0.052802	Acc: 25.9% (2589/10000)
[Test]  Epoch: 38	Loss: 0.052677	Acc: 26.0% (2597/10000)
[Test]  Epoch: 39	Loss: 0.053491	Acc: 26.1% (2605/10000)
[Test]  Epoch: 40	Loss: 0.053159	Acc: 26.7% (2668/10000)
[Test]  Epoch: 41	Loss: 0.053877	Acc: 25.9% (2588/10000)
[Test]  Epoch: 42	Loss: 0.053701	Acc: 26.2% (2621/10000)
[Test]  Epoch: 43	Loss: 0.053310	Acc: 26.9% (2689/10000)
[Test]  Epoch: 44	Loss: 0.053306	Acc: 26.9% (2690/10000)
[Test]  Epoch: 45	Loss: 0.053538	Acc: 26.3% (2626/10000)
[Test]  Epoch: 46	Loss: 0.053584	Acc: 26.5% (2649/10000)
[Test]  Epoch: 47	Loss: 0.053530	Acc: 26.6% (2662/10000)
[Test]  Epoch: 48	Loss: 0.053976	Acc: 26.9% (2687/10000)
[Test]  Epoch: 49	Loss: 0.054386	Acc: 26.1% (2606/10000)
[Test]  Epoch: 50	Loss: 0.053774	Acc: 27.3% (2727/10000)
[Test]  Epoch: 51	Loss: 0.054223	Acc: 26.7% (2674/10000)
[Test]  Epoch: 52	Loss: 0.054172	Acc: 26.7% (2669/10000)
[Test]  Epoch: 53	Loss: 0.054850	Acc: 26.6% (2661/10000)
[Test]  Epoch: 54	Loss: 0.053874	Acc: 27.4% (2743/10000)
[Test]  Epoch: 55	Loss: 0.053874	Acc: 27.7% (2773/10000)
[Test]  Epoch: 56	Loss: 0.053973	Acc: 27.0% (2701/10000)
[Test]  Epoch: 57	Loss: 0.054351	Acc: 27.7% (2766/10000)
[Test]  Epoch: 58	Loss: 0.054390	Acc: 27.5% (2748/10000)
[Test]  Epoch: 59	Loss: 0.054369	Acc: 27.3% (2734/10000)
[Test]  Epoch: 60	Loss: 0.054213	Acc: 27.6% (2755/10000)
[Test]  Epoch: 61	Loss: 0.054303	Acc: 27.5% (2751/10000)
[Test]  Epoch: 62	Loss: 0.054149	Acc: 27.8% (2782/10000)
[Test]  Epoch: 63	Loss: 0.054302	Acc: 27.6% (2765/10000)
[Test]  Epoch: 64	Loss: 0.054256	Acc: 27.9% (2785/10000)
[Test]  Epoch: 65	Loss: 0.054352	Acc: 27.9% (2795/10000)
[Test]  Epoch: 66	Loss: 0.054229	Acc: 27.8% (2784/10000)
[Test]  Epoch: 67	Loss: 0.053907	Acc: 28.0% (2800/10000)
[Test]  Epoch: 68	Loss: 0.053974	Acc: 27.8% (2782/10000)
[Test]  Epoch: 69	Loss: 0.054205	Acc: 27.8% (2782/10000)
[Test]  Epoch: 70	Loss: 0.054020	Acc: 28.2% (2819/10000)
[Test]  Epoch: 71	Loss: 0.053776	Acc: 28.1% (2806/10000)
[Test]  Epoch: 72	Loss: 0.054206	Acc: 27.7% (2768/10000)
[Test]  Epoch: 73	Loss: 0.054103	Acc: 28.3% (2826/10000)
[Test]  Epoch: 74	Loss: 0.054017	Acc: 27.6% (2763/10000)
[Test]  Epoch: 75	Loss: 0.054118	Acc: 27.8% (2776/10000)
[Test]  Epoch: 76	Loss: 0.054329	Acc: 27.8% (2775/10000)
[Test]  Epoch: 77	Loss: 0.054232	Acc: 27.7% (2766/10000)
[Test]  Epoch: 78	Loss: 0.054183	Acc: 28.4% (2844/10000)
[Test]  Epoch: 79	Loss: 0.054044	Acc: 27.6% (2765/10000)
[Test]  Epoch: 80	Loss: 0.054089	Acc: 27.8% (2781/10000)
[Test]  Epoch: 81	Loss: 0.054214	Acc: 28.0% (2802/10000)
[Test]  Epoch: 82	Loss: 0.054005	Acc: 28.4% (2835/10000)
[Test]  Epoch: 83	Loss: 0.054382	Acc: 27.9% (2795/10000)
[Test]  Epoch: 84	Loss: 0.054156	Acc: 28.0% (2802/10000)
[Test]  Epoch: 85	Loss: 0.054157	Acc: 28.0% (2796/10000)
[Test]  Epoch: 86	Loss: 0.053984	Acc: 27.8% (2780/10000)
[Test]  Epoch: 87	Loss: 0.054239	Acc: 27.8% (2777/10000)
[Test]  Epoch: 88	Loss: 0.054075	Acc: 27.8% (2782/10000)
[Test]  Epoch: 89	Loss: 0.054080	Acc: 28.5% (2852/10000)
[Test]  Epoch: 90	Loss: 0.054139	Acc: 28.1% (2809/10000)
[Test]  Epoch: 91	Loss: 0.054529	Acc: 27.6% (2756/10000)
[Test]  Epoch: 92	Loss: 0.054237	Acc: 28.0% (2798/10000)
[Test]  Epoch: 93	Loss: 0.054023	Acc: 28.4% (2838/10000)
[Test]  Epoch: 94	Loss: 0.054090	Acc: 28.0% (2798/10000)
[Test]  Epoch: 95	Loss: 0.054306	Acc: 28.0% (2797/10000)
[Test]  Epoch: 96	Loss: 0.054396	Acc: 27.5% (2746/10000)
[Test]  Epoch: 97	Loss: 0.054188	Acc: 27.8% (2784/10000)
[Test]  Epoch: 98	Loss: 0.054141	Acc: 27.9% (2790/10000)
[Test]  Epoch: 99	Loss: 0.054191	Acc: 27.7% (2770/10000)
[Test]  Epoch: 100	Loss: 0.054057	Acc: 28.1% (2815/10000)
===========finish==========
['2024-08-19', '01:00:53.835023', '100', 'test', '0.05405697736740112', '28.15', '28.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.15.weight', 'features.21.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.31.weight', 'features.38.weight', 'features.0.weight', 'classifier.weight', 'features.3.weight', 'features.7.weight', 'features.10.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.071787	Acc: 2.0% (197/10000)
[Test]  Epoch: 2	Loss: 0.068912	Acc: 5.2% (520/10000)
[Test]  Epoch: 3	Loss: 0.067358	Acc: 6.1% (610/10000)
[Test]  Epoch: 4	Loss: 0.065968	Acc: 6.7% (674/10000)
[Test]  Epoch: 5	Loss: 0.065049	Acc: 7.8% (776/10000)
[Test]  Epoch: 6	Loss: 0.064554	Acc: 8.2% (825/10000)
[Test]  Epoch: 7	Loss: 0.063337	Acc: 9.4% (944/10000)
[Test]  Epoch: 8	Loss: 0.063497	Acc: 9.1% (911/10000)
[Test]  Epoch: 9	Loss: 0.063373	Acc: 9.8% (976/10000)
[Test]  Epoch: 10	Loss: 0.062698	Acc: 10.8% (1075/10000)
[Test]  Epoch: 11	Loss: 0.062671	Acc: 10.9% (1095/10000)
[Test]  Epoch: 12	Loss: 0.062687	Acc: 11.4% (1136/10000)
[Test]  Epoch: 13	Loss: 0.063075	Acc: 11.0% (1104/10000)
[Test]  Epoch: 14	Loss: 0.062874	Acc: 11.1% (1114/10000)
[Test]  Epoch: 15	Loss: 0.063486	Acc: 11.0% (1103/10000)
[Test]  Epoch: 16	Loss: 0.063658	Acc: 11.2% (1116/10000)
[Test]  Epoch: 17	Loss: 0.063488	Acc: 11.4% (1139/10000)
[Test]  Epoch: 18	Loss: 0.063534	Acc: 12.3% (1229/10000)
[Test]  Epoch: 19	Loss: 0.064690	Acc: 11.1% (1111/10000)
[Test]  Epoch: 20	Loss: 0.064455	Acc: 11.9% (1187/10000)
[Test]  Epoch: 21	Loss: 0.063797	Acc: 12.6% (1260/10000)
[Test]  Epoch: 22	Loss: 0.064937	Acc: 12.3% (1231/10000)
[Test]  Epoch: 23	Loss: 0.064700	Acc: 12.8% (1279/10000)
[Test]  Epoch: 24	Loss: 0.065376	Acc: 12.3% (1232/10000)
[Test]  Epoch: 25	Loss: 0.065537	Acc: 12.8% (1285/10000)
[Test]  Epoch: 26	Loss: 0.065716	Acc: 12.3% (1234/10000)
[Test]  Epoch: 27	Loss: 0.065674	Acc: 13.1% (1308/10000)
[Test]  Epoch: 28	Loss: 0.066995	Acc: 12.9% (1287/10000)
[Test]  Epoch: 29	Loss: 0.067283	Acc: 12.7% (1270/10000)
[Test]  Epoch: 30	Loss: 0.067349	Acc: 13.1% (1307/10000)
[Test]  Epoch: 31	Loss: 0.068023	Acc: 12.8% (1276/10000)
[Test]  Epoch: 32	Loss: 0.068234	Acc: 13.2% (1323/10000)
[Test]  Epoch: 33	Loss: 0.069635	Acc: 12.3% (1232/10000)
[Test]  Epoch: 34	Loss: 0.069377	Acc: 12.8% (1284/10000)
[Test]  Epoch: 35	Loss: 0.069544	Acc: 12.4% (1243/10000)
[Test]  Epoch: 36	Loss: 0.070464	Acc: 13.2% (1319/10000)
[Test]  Epoch: 37	Loss: 0.072937	Acc: 12.6% (1264/10000)
[Test]  Epoch: 38	Loss: 0.071118	Acc: 12.7% (1271/10000)
[Test]  Epoch: 39	Loss: 0.070710	Acc: 13.3% (1334/10000)
[Test]  Epoch: 40	Loss: 0.070909	Acc: 14.0% (1403/10000)
[Test]  Epoch: 41	Loss: 0.071946	Acc: 13.1% (1311/10000)
[Test]  Epoch: 42	Loss: 0.072826	Acc: 13.3% (1331/10000)
[Test]  Epoch: 43	Loss: 0.073056	Acc: 13.9% (1389/10000)
[Test]  Epoch: 44	Loss: 0.073913	Acc: 13.1% (1308/10000)
[Test]  Epoch: 45	Loss: 0.073326	Acc: 13.2% (1324/10000)
[Test]  Epoch: 46	Loss: 0.073392	Acc: 13.6% (1362/10000)
[Test]  Epoch: 47	Loss: 0.073662	Acc: 13.9% (1386/10000)
[Test]  Epoch: 48	Loss: 0.073501	Acc: 14.0% (1400/10000)
[Test]  Epoch: 49	Loss: 0.073679	Acc: 13.6% (1355/10000)
[Test]  Epoch: 50	Loss: 0.074261	Acc: 14.0% (1400/10000)
[Test]  Epoch: 51	Loss: 0.074950	Acc: 14.1% (1406/10000)
[Test]  Epoch: 52	Loss: 0.075505	Acc: 13.9% (1391/10000)
[Test]  Epoch: 53	Loss: 0.075207	Acc: 13.8% (1376/10000)
[Test]  Epoch: 54	Loss: 0.076329	Acc: 13.7% (1366/10000)
[Test]  Epoch: 55	Loss: 0.075038	Acc: 14.4% (1440/10000)
[Test]  Epoch: 56	Loss: 0.075582	Acc: 14.3% (1435/10000)
[Test]  Epoch: 57	Loss: 0.075595	Acc: 14.6% (1462/10000)
[Test]  Epoch: 58	Loss: 0.075874	Acc: 14.6% (1458/10000)
[Test]  Epoch: 59	Loss: 0.074949	Acc: 14.5% (1448/10000)
[Test]  Epoch: 60	Loss: 0.076463	Acc: 13.6% (1358/10000)
[Test]  Epoch: 61	Loss: 0.075738	Acc: 14.1% (1414/10000)
[Test]  Epoch: 62	Loss: 0.075787	Acc: 14.3% (1430/10000)
[Test]  Epoch: 63	Loss: 0.075810	Acc: 14.5% (1450/10000)
[Test]  Epoch: 64	Loss: 0.075552	Acc: 14.7% (1466/10000)
[Test]  Epoch: 65	Loss: 0.075657	Acc: 14.7% (1472/10000)
[Test]  Epoch: 66	Loss: 0.075573	Acc: 14.3% (1432/10000)
[Test]  Epoch: 67	Loss: 0.075565	Acc: 14.8% (1476/10000)
[Test]  Epoch: 68	Loss: 0.076087	Acc: 14.5% (1447/10000)
[Test]  Epoch: 69	Loss: 0.075474	Acc: 14.8% (1478/10000)
[Test]  Epoch: 70	Loss: 0.075847	Acc: 14.5% (1446/10000)
[Test]  Epoch: 71	Loss: 0.075741	Acc: 14.4% (1438/10000)
[Test]  Epoch: 72	Loss: 0.075241	Acc: 14.8% (1485/10000)
[Test]  Epoch: 73	Loss: 0.075394	Acc: 14.7% (1472/10000)
[Test]  Epoch: 74	Loss: 0.075227	Acc: 14.3% (1431/10000)
[Test]  Epoch: 75	Loss: 0.075502	Acc: 14.7% (1465/10000)
[Test]  Epoch: 76	Loss: 0.075776	Acc: 14.8% (1479/10000)
[Test]  Epoch: 77	Loss: 0.075536	Acc: 14.8% (1480/10000)
[Test]  Epoch: 78	Loss: 0.075582	Acc: 14.7% (1472/10000)
[Test]  Epoch: 79	Loss: 0.075537	Acc: 15.1% (1506/10000)
[Test]  Epoch: 80	Loss: 0.075510	Acc: 14.7% (1471/10000)
[Test]  Epoch: 81	Loss: 0.075647	Acc: 15.0% (1504/10000)
[Test]  Epoch: 82	Loss: 0.075570	Acc: 14.7% (1468/10000)
[Test]  Epoch: 83	Loss: 0.075689	Acc: 14.7% (1468/10000)
[Test]  Epoch: 84	Loss: 0.075880	Acc: 14.7% (1473/10000)
[Test]  Epoch: 85	Loss: 0.075721	Acc: 15.1% (1505/10000)
[Test]  Epoch: 86	Loss: 0.075875	Acc: 14.3% (1433/10000)
[Test]  Epoch: 87	Loss: 0.076000	Acc: 14.8% (1485/10000)
[Test]  Epoch: 88	Loss: 0.075897	Acc: 14.7% (1474/10000)
[Test]  Epoch: 89	Loss: 0.075617	Acc: 15.0% (1498/10000)
[Test]  Epoch: 90	Loss: 0.075720	Acc: 14.4% (1444/10000)
[Test]  Epoch: 91	Loss: 0.076150	Acc: 14.5% (1452/10000)
[Test]  Epoch: 92	Loss: 0.075945	Acc: 14.7% (1467/10000)
[Test]  Epoch: 93	Loss: 0.076099	Acc: 14.4% (1437/10000)
[Test]  Epoch: 94	Loss: 0.075635	Acc: 14.7% (1471/10000)
[Test]  Epoch: 95	Loss: 0.076008	Acc: 14.7% (1470/10000)
[Test]  Epoch: 96	Loss: 0.075964	Acc: 14.4% (1437/10000)
[Test]  Epoch: 97	Loss: 0.075685	Acc: 14.5% (1449/10000)
[Test]  Epoch: 98	Loss: 0.076004	Acc: 14.4% (1442/10000)
[Test]  Epoch: 99	Loss: 0.075933	Acc: 15.0% (1500/10000)
[Test]  Epoch: 100	Loss: 0.076176	Acc: 14.5% (1449/10000)
===========finish==========
['2024-08-19', '01:03:20.568262', '100', 'test', '0.0761757276058197', '14.49', '15.06']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.15.weight', 'features.21.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.31.weight', 'features.38.weight', 'features.0.weight', 'classifier.weight', 'features.3.weight', 'features.7.weight', 'features.10.weight', 'features.14.weight', 'features.17.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.072466	Acc: 1.7% (169/10000)
[Test]  Epoch: 2	Loss: 0.069543	Acc: 4.9% (491/10000)
[Test]  Epoch: 3	Loss: 0.067842	Acc: 5.6% (564/10000)
[Test]  Epoch: 4	Loss: 0.066766	Acc: 6.0% (598/10000)
[Test]  Epoch: 5	Loss: 0.065784	Acc: 7.4% (737/10000)
[Test]  Epoch: 6	Loss: 0.065096	Acc: 7.6% (761/10000)
[Test]  Epoch: 7	Loss: 0.064363	Acc: 8.4% (840/10000)
[Test]  Epoch: 8	Loss: 0.064038	Acc: 9.2% (915/10000)
[Test]  Epoch: 9	Loss: 0.064145	Acc: 9.1% (910/10000)
[Test]  Epoch: 10	Loss: 0.063545	Acc: 9.8% (982/10000)
[Test]  Epoch: 11	Loss: 0.063997	Acc: 9.9% (992/10000)
[Test]  Epoch: 12	Loss: 0.064017	Acc: 9.4% (945/10000)
[Test]  Epoch: 13	Loss: 0.063661	Acc: 10.4% (1042/10000)
[Test]  Epoch: 14	Loss: 0.063890	Acc: 10.6% (1063/10000)
[Test]  Epoch: 15	Loss: 0.064001	Acc: 10.4% (1039/10000)
[Test]  Epoch: 16	Loss: 0.064463	Acc: 9.6% (964/10000)
[Test]  Epoch: 17	Loss: 0.065623	Acc: 10.7% (1074/10000)
[Test]  Epoch: 18	Loss: 0.063994	Acc: 11.6% (1156/10000)
[Test]  Epoch: 19	Loss: 0.065277	Acc: 10.8% (1082/10000)
[Test]  Epoch: 20	Loss: 0.065547	Acc: 10.8% (1075/10000)
[Test]  Epoch: 21	Loss: 0.066614	Acc: 10.9% (1088/10000)
[Test]  Epoch: 22	Loss: 0.066561	Acc: 10.5% (1048/10000)
[Test]  Epoch: 23	Loss: 0.066272	Acc: 12.3% (1234/10000)
[Test]  Epoch: 24	Loss: 0.067156	Acc: 11.8% (1178/10000)
[Test]  Epoch: 25	Loss: 0.068460	Acc: 11.7% (1169/10000)
[Test]  Epoch: 26	Loss: 0.067604	Acc: 11.9% (1189/10000)
[Test]  Epoch: 27	Loss: 0.070562	Acc: 10.7% (1071/10000)
[Test]  Epoch: 28	Loss: 0.069785	Acc: 11.1% (1107/10000)
[Test]  Epoch: 29	Loss: 0.070692	Acc: 11.3% (1129/10000)
[Test]  Epoch: 30	Loss: 0.070058	Acc: 11.3% (1127/10000)
[Test]  Epoch: 31	Loss: 0.070709	Acc: 12.5% (1251/10000)
[Test]  Epoch: 32	Loss: 0.071624	Acc: 12.3% (1227/10000)
[Test]  Epoch: 33	Loss: 0.071835	Acc: 11.6% (1163/10000)
[Test]  Epoch: 34	Loss: 0.072660	Acc: 12.0% (1201/10000)
[Test]  Epoch: 35	Loss: 0.073162	Acc: 11.7% (1170/10000)
[Test]  Epoch: 36	Loss: 0.073897	Acc: 12.1% (1210/10000)
[Test]  Epoch: 37	Loss: 0.074822	Acc: 12.0% (1200/10000)
[Test]  Epoch: 38	Loss: 0.075105	Acc: 10.8% (1085/10000)
[Test]  Epoch: 39	Loss: 0.075005	Acc: 11.7% (1170/10000)
[Test]  Epoch: 40	Loss: 0.076562	Acc: 11.9% (1190/10000)
[Test]  Epoch: 41	Loss: 0.075944	Acc: 12.2% (1221/10000)
[Test]  Epoch: 42	Loss: 0.076734	Acc: 11.9% (1189/10000)
[Test]  Epoch: 43	Loss: 0.077444	Acc: 12.4% (1241/10000)
[Test]  Epoch: 44	Loss: 0.077450	Acc: 12.0% (1201/10000)
[Test]  Epoch: 45	Loss: 0.078291	Acc: 11.6% (1163/10000)
[Test]  Epoch: 46	Loss: 0.077576	Acc: 12.5% (1254/10000)
[Test]  Epoch: 47	Loss: 0.078132	Acc: 12.1% (1214/10000)
[Test]  Epoch: 48	Loss: 0.079342	Acc: 12.4% (1241/10000)
[Test]  Epoch: 49	Loss: 0.079631	Acc: 12.3% (1226/10000)
[Test]  Epoch: 50	Loss: 0.078233	Acc: 12.7% (1274/10000)
[Test]  Epoch: 51	Loss: 0.079283	Acc: 12.8% (1276/10000)
[Test]  Epoch: 52	Loss: 0.080921	Acc: 12.3% (1228/10000)
[Test]  Epoch: 53	Loss: 0.079623	Acc: 12.7% (1269/10000)
[Test]  Epoch: 54	Loss: 0.080186	Acc: 12.8% (1278/10000)
[Test]  Epoch: 55	Loss: 0.079594	Acc: 12.9% (1292/10000)
[Test]  Epoch: 56	Loss: 0.080765	Acc: 12.4% (1236/10000)
[Test]  Epoch: 57	Loss: 0.080399	Acc: 12.4% (1239/10000)
[Test]  Epoch: 58	Loss: 0.081650	Acc: 12.5% (1246/10000)
[Test]  Epoch: 59	Loss: 0.080898	Acc: 12.9% (1290/10000)
[Test]  Epoch: 60	Loss: 0.080690	Acc: 13.3% (1333/10000)
[Test]  Epoch: 61	Loss: 0.079843	Acc: 13.7% (1367/10000)
[Test]  Epoch: 62	Loss: 0.079905	Acc: 13.5% (1347/10000)
[Test]  Epoch: 63	Loss: 0.080060	Acc: 13.6% (1356/10000)
[Test]  Epoch: 64	Loss: 0.079735	Acc: 13.7% (1366/10000)
[Test]  Epoch: 65	Loss: 0.079959	Acc: 13.7% (1366/10000)
[Test]  Epoch: 66	Loss: 0.079606	Acc: 13.8% (1384/10000)
[Test]  Epoch: 67	Loss: 0.079850	Acc: 13.9% (1387/10000)
[Test]  Epoch: 68	Loss: 0.079744	Acc: 14.0% (1398/10000)
[Test]  Epoch: 69	Loss: 0.079602	Acc: 13.4% (1344/10000)
[Test]  Epoch: 70	Loss: 0.079839	Acc: 13.7% (1365/10000)
[Test]  Epoch: 71	Loss: 0.080132	Acc: 13.5% (1352/10000)
[Test]  Epoch: 72	Loss: 0.080033	Acc: 13.6% (1362/10000)
[Test]  Epoch: 73	Loss: 0.079968	Acc: 13.9% (1386/10000)
[Test]  Epoch: 74	Loss: 0.079997	Acc: 13.5% (1349/10000)
[Test]  Epoch: 75	Loss: 0.079939	Acc: 14.0% (1396/10000)
[Test]  Epoch: 76	Loss: 0.080126	Acc: 13.8% (1377/10000)
[Test]  Epoch: 77	Loss: 0.080060	Acc: 13.6% (1361/10000)
[Test]  Epoch: 78	Loss: 0.080248	Acc: 13.2% (1321/10000)
[Test]  Epoch: 79	Loss: 0.079736	Acc: 13.6% (1360/10000)
[Test]  Epoch: 80	Loss: 0.080193	Acc: 13.7% (1374/10000)
[Test]  Epoch: 81	Loss: 0.080199	Acc: 13.7% (1368/10000)
[Test]  Epoch: 82	Loss: 0.080211	Acc: 13.7% (1366/10000)
[Test]  Epoch: 83	Loss: 0.080247	Acc: 13.8% (1384/10000)
[Test]  Epoch: 84	Loss: 0.079990	Acc: 13.8% (1385/10000)
[Test]  Epoch: 85	Loss: 0.080399	Acc: 13.9% (1394/10000)
[Test]  Epoch: 86	Loss: 0.080196	Acc: 13.8% (1376/10000)
[Test]  Epoch: 87	Loss: 0.080076	Acc: 13.7% (1367/10000)
[Test]  Epoch: 88	Loss: 0.080172	Acc: 13.4% (1344/10000)
[Test]  Epoch: 89	Loss: 0.079951	Acc: 13.5% (1346/10000)
[Test]  Epoch: 90	Loss: 0.080262	Acc: 13.9% (1394/10000)
[Test]  Epoch: 91	Loss: 0.080203	Acc: 13.8% (1376/10000)
[Test]  Epoch: 92	Loss: 0.080591	Acc: 13.8% (1379/10000)
[Test]  Epoch: 93	Loss: 0.080203	Acc: 14.0% (1400/10000)
[Test]  Epoch: 94	Loss: 0.080005	Acc: 13.7% (1372/10000)
[Test]  Epoch: 95	Loss: 0.080155	Acc: 13.9% (1391/10000)
[Test]  Epoch: 96	Loss: 0.080103	Acc: 14.0% (1402/10000)
[Test]  Epoch: 97	Loss: 0.080385	Acc: 13.8% (1377/10000)
[Test]  Epoch: 98	Loss: 0.080025	Acc: 14.0% (1401/10000)
[Test]  Epoch: 99	Loss: 0.080397	Acc: 14.0% (1399/10000)
[Test]  Epoch: 100	Loss: 0.080531	Acc: 13.7% (1371/10000)
===========finish==========
['2024-08-19', '01:05:44.365305', '100', 'test', '0.08053068680763245', '13.71', '14.02']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.15.weight', 'features.21.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.31.weight', 'features.38.weight', 'features.0.weight', 'classifier.weight', 'features.3.weight', 'features.7.weight', 'features.10.weight', 'features.14.weight', 'features.17.weight', 'features.20.weight', 'features.24.weight', 'features.40.weight', 'features.37.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.074263	Acc: 1.1% (113/10000)
[Test]  Epoch: 2	Loss: 0.071030	Acc: 3.2% (317/10000)
[Test]  Epoch: 3	Loss: 0.069460	Acc: 4.0% (398/10000)
[Test]  Epoch: 4	Loss: 0.068524	Acc: 4.4% (438/10000)
[Test]  Epoch: 5	Loss: 0.067278	Acc: 5.4% (541/10000)
[Test]  Epoch: 6	Loss: 0.066495	Acc: 6.1% (609/10000)
[Test]  Epoch: 7	Loss: 0.065886	Acc: 6.2% (625/10000)
[Test]  Epoch: 8	Loss: 0.065328	Acc: 6.9% (692/10000)
[Test]  Epoch: 9	Loss: 0.064822	Acc: 7.6% (760/10000)
[Test]  Epoch: 10	Loss: 0.064580	Acc: 7.1% (710/10000)
[Test]  Epoch: 11	Loss: 0.064506	Acc: 8.1% (808/10000)
[Test]  Epoch: 12	Loss: 0.064000	Acc: 8.7% (867/10000)
[Test]  Epoch: 13	Loss: 0.063327	Acc: 8.9% (890/10000)
[Test]  Epoch: 14	Loss: 0.063647	Acc: 8.7% (871/10000)
[Test]  Epoch: 15	Loss: 0.063283	Acc: 9.0% (898/10000)
[Test]  Epoch: 16	Loss: 0.064049	Acc: 9.2% (923/10000)
[Test]  Epoch: 17	Loss: 0.062985	Acc: 9.8% (983/10000)
[Test]  Epoch: 18	Loss: 0.064810	Acc: 9.2% (925/10000)
[Test]  Epoch: 19	Loss: 0.064028	Acc: 9.5% (952/10000)
[Test]  Epoch: 20	Loss: 0.064343	Acc: 9.2% (925/10000)
[Test]  Epoch: 21	Loss: 0.063555	Acc: 10.3% (1031/10000)
[Test]  Epoch: 22	Loss: 0.063503	Acc: 10.6% (1061/10000)
[Test]  Epoch: 23	Loss: 0.064190	Acc: 10.7% (1067/10000)
[Test]  Epoch: 24	Loss: 0.063890	Acc: 10.4% (1039/10000)
[Test]  Epoch: 25	Loss: 0.063672	Acc: 11.2% (1125/10000)
[Test]  Epoch: 26	Loss: 0.064446	Acc: 10.4% (1036/10000)
[Test]  Epoch: 27	Loss: 0.064239	Acc: 10.5% (1050/10000)
[Test]  Epoch: 28	Loss: 0.065854	Acc: 10.2% (1016/10000)
[Test]  Epoch: 29	Loss: 0.066357	Acc: 10.9% (1093/10000)
[Test]  Epoch: 30	Loss: 0.066012	Acc: 10.6% (1063/10000)
[Test]  Epoch: 31	Loss: 0.065748	Acc: 10.8% (1080/10000)
[Test]  Epoch: 32	Loss: 0.064947	Acc: 11.2% (1122/10000)
[Test]  Epoch: 33	Loss: 0.066028	Acc: 11.2% (1117/10000)
[Test]  Epoch: 34	Loss: 0.068098	Acc: 10.6% (1062/10000)
[Test]  Epoch: 35	Loss: 0.067121	Acc: 11.1% (1113/10000)
[Test]  Epoch: 36	Loss: 0.068410	Acc: 10.5% (1047/10000)
[Test]  Epoch: 37	Loss: 0.070454	Acc: 11.3% (1126/10000)
[Test]  Epoch: 38	Loss: 0.067815	Acc: 11.8% (1176/10000)
[Test]  Epoch: 39	Loss: 0.068965	Acc: 12.2% (1220/10000)
[Test]  Epoch: 40	Loss: 0.069875	Acc: 11.2% (1123/10000)
[Test]  Epoch: 41	Loss: 0.068489	Acc: 12.0% (1202/10000)
[Test]  Epoch: 42	Loss: 0.070880	Acc: 10.9% (1094/10000)
[Test]  Epoch: 43	Loss: 0.070859	Acc: 11.6% (1160/10000)
[Test]  Epoch: 44	Loss: 0.070221	Acc: 11.6% (1158/10000)
[Test]  Epoch: 45	Loss: 0.071682	Acc: 11.2% (1122/10000)
[Test]  Epoch: 46	Loss: 0.072242	Acc: 11.3% (1134/10000)
[Test]  Epoch: 47	Loss: 0.071840	Acc: 12.3% (1233/10000)
[Test]  Epoch: 48	Loss: 0.071348	Acc: 11.7% (1174/10000)
[Test]  Epoch: 49	Loss: 0.073669	Acc: 11.1% (1108/10000)
[Test]  Epoch: 50	Loss: 0.072368	Acc: 11.6% (1164/10000)
[Test]  Epoch: 51	Loss: 0.071221	Acc: 12.3% (1230/10000)
[Test]  Epoch: 52	Loss: 0.073076	Acc: 12.8% (1282/10000)
[Test]  Epoch: 53	Loss: 0.073843	Acc: 11.9% (1190/10000)
[Test]  Epoch: 54	Loss: 0.073273	Acc: 12.2% (1225/10000)
[Test]  Epoch: 55	Loss: 0.074474	Acc: 12.2% (1222/10000)
[Test]  Epoch: 56	Loss: 0.076118	Acc: 12.0% (1198/10000)
[Test]  Epoch: 57	Loss: 0.074677	Acc: 12.0% (1199/10000)
[Test]  Epoch: 58	Loss: 0.074938	Acc: 12.5% (1247/10000)
[Test]  Epoch: 59	Loss: 0.074812	Acc: 12.7% (1269/10000)
[Test]  Epoch: 60	Loss: 0.075574	Acc: 12.6% (1259/10000)
[Test]  Epoch: 61	Loss: 0.073525	Acc: 13.3% (1332/10000)
[Test]  Epoch: 62	Loss: 0.073655	Acc: 13.3% (1332/10000)
[Test]  Epoch: 63	Loss: 0.073523	Acc: 13.2% (1320/10000)
[Test]  Epoch: 64	Loss: 0.073398	Acc: 13.1% (1314/10000)
[Test]  Epoch: 65	Loss: 0.073246	Acc: 13.6% (1356/10000)
[Test]  Epoch: 66	Loss: 0.073367	Acc: 13.2% (1317/10000)
[Test]  Epoch: 67	Loss: 0.072920	Acc: 13.3% (1331/10000)
[Test]  Epoch: 68	Loss: 0.073517	Acc: 13.2% (1318/10000)
[Test]  Epoch: 69	Loss: 0.073266	Acc: 13.3% (1329/10000)
[Test]  Epoch: 70	Loss: 0.073573	Acc: 13.0% (1300/10000)
[Test]  Epoch: 71	Loss: 0.073303	Acc: 13.5% (1350/10000)
[Test]  Epoch: 72	Loss: 0.073383	Acc: 13.3% (1332/10000)
[Test]  Epoch: 73	Loss: 0.073402	Acc: 13.3% (1334/10000)
[Test]  Epoch: 74	Loss: 0.073450	Acc: 13.2% (1320/10000)
[Test]  Epoch: 75	Loss: 0.073574	Acc: 13.3% (1331/10000)
[Test]  Epoch: 76	Loss: 0.073582	Acc: 13.2% (1318/10000)
[Test]  Epoch: 77	Loss: 0.073457	Acc: 13.1% (1310/10000)
[Test]  Epoch: 78	Loss: 0.073404	Acc: 13.4% (1339/10000)
[Test]  Epoch: 79	Loss: 0.073610	Acc: 13.4% (1341/10000)
[Test]  Epoch: 80	Loss: 0.073559	Acc: 13.1% (1310/10000)
[Test]  Epoch: 81	Loss: 0.073759	Acc: 13.3% (1332/10000)
[Test]  Epoch: 82	Loss: 0.073535	Acc: 13.2% (1319/10000)
[Test]  Epoch: 83	Loss: 0.073881	Acc: 13.3% (1333/10000)
[Test]  Epoch: 84	Loss: 0.073422	Acc: 13.7% (1370/10000)
[Test]  Epoch: 85	Loss: 0.073698	Acc: 13.3% (1329/10000)
[Test]  Epoch: 86	Loss: 0.073661	Acc: 13.5% (1354/10000)
[Test]  Epoch: 87	Loss: 0.073591	Acc: 13.5% (1350/10000)
[Test]  Epoch: 88	Loss: 0.073678	Acc: 13.4% (1339/10000)
[Test]  Epoch: 89	Loss: 0.073582	Acc: 13.3% (1332/10000)
[Test]  Epoch: 90	Loss: 0.073860	Acc: 13.3% (1331/10000)
[Test]  Epoch: 91	Loss: 0.073865	Acc: 13.3% (1335/10000)
[Test]  Epoch: 92	Loss: 0.073703	Acc: 13.5% (1353/10000)
[Test]  Epoch: 93	Loss: 0.073818	Acc: 13.3% (1326/10000)
[Test]  Epoch: 94	Loss: 0.073625	Acc: 13.3% (1330/10000)
[Test]  Epoch: 95	Loss: 0.073737	Acc: 13.2% (1323/10000)
[Test]  Epoch: 96	Loss: 0.074075	Acc: 13.3% (1329/10000)
[Test]  Epoch: 97	Loss: 0.074060	Acc: 13.4% (1339/10000)
[Test]  Epoch: 98	Loss: 0.073784	Acc: 13.2% (1324/10000)
[Test]  Epoch: 99	Loss: 0.073896	Acc: 12.9% (1291/10000)
[Test]  Epoch: 100	Loss: 0.074042	Acc: 13.1% (1310/10000)
===========finish==========
['2024-08-19', '01:08:10.944939', '100', 'test', '0.07404170134067535', '13.1', '13.7']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.1.weight', 'features.41.weight', 'features.4.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.15.weight', 'features.21.weight', 'features.35.weight', 'features.28.weight', 'features.25.weight', 'features.31.weight', 'features.38.weight', 'features.0.weight', 'classifier.weight', 'features.3.weight', 'features.7.weight', 'features.10.weight', 'features.14.weight', 'features.17.weight', 'features.20.weight', 'features.24.weight', 'features.40.weight', 'features.37.weight', 'features.34.weight', 'features.30.weight', 'features.27.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.135011	Acc: 1.0% (100/10000)
[Test]  Epoch: 2	Loss: 0.072417	Acc: 2.2% (223/10000)
[Test]  Epoch: 3	Loss: 0.071620	Acc: 3.0% (295/10000)
[Test]  Epoch: 4	Loss: 0.071205	Acc: 3.1% (313/10000)
[Test]  Epoch: 5	Loss: 0.070406	Acc: 3.5% (355/10000)
[Test]  Epoch: 6	Loss: 0.069780	Acc: 4.0% (398/10000)
[Test]  Epoch: 7	Loss: 0.069037	Acc: 3.7% (373/10000)
[Test]  Epoch: 8	Loss: 0.068830	Acc: 5.2% (518/10000)
[Test]  Epoch: 9	Loss: 0.068282	Acc: 5.6% (560/10000)
[Test]  Epoch: 10	Loss: 0.068092	Acc: 5.5% (547/10000)
[Test]  Epoch: 11	Loss: 0.067297	Acc: 6.1% (606/10000)
[Test]  Epoch: 12	Loss: 0.066737	Acc: 6.7% (668/10000)
[Test]  Epoch: 13	Loss: 0.066228	Acc: 6.3% (632/10000)
[Test]  Epoch: 14	Loss: 0.066177	Acc: 7.4% (738/10000)
[Test]  Epoch: 15	Loss: 0.065599	Acc: 7.3% (735/10000)
[Test]  Epoch: 16	Loss: 0.064832	Acc: 7.7% (765/10000)
[Test]  Epoch: 17	Loss: 0.065282	Acc: 8.2% (818/10000)
[Test]  Epoch: 18	Loss: 0.064414	Acc: 8.5% (851/10000)
[Test]  Epoch: 19	Loss: 0.063493	Acc: 9.1% (910/10000)
[Test]  Epoch: 20	Loss: 0.063282	Acc: 8.6% (863/10000)
[Test]  Epoch: 21	Loss: 0.063311	Acc: 9.3% (928/10000)
[Test]  Epoch: 22	Loss: 0.062975	Acc: 9.2% (916/10000)
[Test]  Epoch: 23	Loss: 0.064320	Acc: 9.6% (964/10000)
[Test]  Epoch: 24	Loss: 0.063061	Acc: 9.5% (950/10000)
[Test]  Epoch: 25	Loss: 0.062966	Acc: 9.9% (994/10000)
[Test]  Epoch: 26	Loss: 0.062884	Acc: 10.4% (1042/10000)
[Test]  Epoch: 27	Loss: 0.062704	Acc: 10.2% (1023/10000)
[Test]  Epoch: 28	Loss: 0.063105	Acc: 10.4% (1045/10000)
[Test]  Epoch: 29	Loss: 0.064407	Acc: 10.1% (1014/10000)
[Test]  Epoch: 30	Loss: 0.063022	Acc: 10.6% (1064/10000)
[Test]  Epoch: 31	Loss: 0.062719	Acc: 10.4% (1044/10000)
[Test]  Epoch: 32	Loss: 0.063208	Acc: 10.4% (1038/10000)
[Test]  Epoch: 33	Loss: 0.063052	Acc: 10.8% (1079/10000)
[Test]  Epoch: 34	Loss: 0.064401	Acc: 10.6% (1060/10000)
[Test]  Epoch: 35	Loss: 0.065994	Acc: 10.7% (1074/10000)
[Test]  Epoch: 36	Loss: 0.064741	Acc: 10.6% (1056/10000)
[Test]  Epoch: 37	Loss: 0.063880	Acc: 11.3% (1135/10000)
[Test]  Epoch: 38	Loss: 0.063284	Acc: 11.4% (1145/10000)
[Test]  Epoch: 39	Loss: 0.065075	Acc: 11.8% (1184/10000)
[Test]  Epoch: 40	Loss: 0.064629	Acc: 11.5% (1147/10000)
[Test]  Epoch: 41	Loss: 0.064513	Acc: 12.0% (1201/10000)
[Test]  Epoch: 42	Loss: 0.066148	Acc: 11.1% (1106/10000)
[Test]  Epoch: 43	Loss: 0.065432	Acc: 11.5% (1154/10000)
[Test]  Epoch: 44	Loss: 0.066084	Acc: 11.4% (1137/10000)
[Test]  Epoch: 45	Loss: 0.065218	Acc: 11.5% (1154/10000)
[Test]  Epoch: 46	Loss: 0.065656	Acc: 12.1% (1209/10000)
[Test]  Epoch: 47	Loss: 0.066027	Acc: 12.5% (1248/10000)
[Test]  Epoch: 48	Loss: 0.066753	Acc: 12.0% (1203/10000)
[Test]  Epoch: 49	Loss: 0.068286	Acc: 11.6% (1155/10000)
[Test]  Epoch: 50	Loss: 0.067842	Acc: 12.2% (1217/10000)
[Test]  Epoch: 51	Loss: 0.067455	Acc: 12.8% (1278/10000)
[Test]  Epoch: 52	Loss: 0.071929	Acc: 11.0% (1096/10000)
[Test]  Epoch: 53	Loss: 0.069678	Acc: 11.5% (1154/10000)
[Test]  Epoch: 54	Loss: 0.068424	Acc: 12.6% (1263/10000)
[Test]  Epoch: 55	Loss: 0.069101	Acc: 12.5% (1251/10000)
[Test]  Epoch: 56	Loss: 0.070550	Acc: 12.1% (1213/10000)
[Test]  Epoch: 57	Loss: 0.069915	Acc: 12.6% (1263/10000)
[Test]  Epoch: 58	Loss: 0.070520	Acc: 12.1% (1212/10000)
[Test]  Epoch: 59	Loss: 0.069933	Acc: 12.3% (1229/10000)
[Test]  Epoch: 60	Loss: 0.069923	Acc: 12.4% (1238/10000)
[Test]  Epoch: 61	Loss: 0.069355	Acc: 12.8% (1282/10000)
[Test]  Epoch: 62	Loss: 0.069514	Acc: 13.2% (1319/10000)
[Test]  Epoch: 63	Loss: 0.069559	Acc: 13.0% (1298/10000)
[Test]  Epoch: 64	Loss: 0.069461	Acc: 13.4% (1343/10000)
[Test]  Epoch: 65	Loss: 0.069554	Acc: 13.2% (1318/10000)
[Test]  Epoch: 66	Loss: 0.069385	Acc: 13.0% (1304/10000)
[Test]  Epoch: 67	Loss: 0.069602	Acc: 13.2% (1321/10000)
[Test]  Epoch: 68	Loss: 0.069607	Acc: 13.3% (1326/10000)
[Test]  Epoch: 69	Loss: 0.069533	Acc: 13.5% (1350/10000)
[Test]  Epoch: 70	Loss: 0.069686	Acc: 13.1% (1312/10000)
[Test]  Epoch: 71	Loss: 0.069769	Acc: 13.7% (1365/10000)
[Test]  Epoch: 72	Loss: 0.069640	Acc: 13.7% (1368/10000)
[Test]  Epoch: 73	Loss: 0.069608	Acc: 13.4% (1336/10000)
[Test]  Epoch: 74	Loss: 0.069652	Acc: 13.0% (1300/10000)
[Test]  Epoch: 75	Loss: 0.069711	Acc: 13.4% (1340/10000)
[Test]  Epoch: 76	Loss: 0.069776	Acc: 13.3% (1332/10000)
[Test]  Epoch: 77	Loss: 0.069635	Acc: 13.4% (1343/10000)
[Test]  Epoch: 78	Loss: 0.069405	Acc: 13.7% (1369/10000)
[Test]  Epoch: 79	Loss: 0.069613	Acc: 13.3% (1326/10000)
[Test]  Epoch: 80	Loss: 0.069830	Acc: 13.7% (1374/10000)
[Test]  Epoch: 81	Loss: 0.069774	Acc: 13.6% (1357/10000)
[Test]  Epoch: 82	Loss: 0.069889	Acc: 13.0% (1302/10000)
[Test]  Epoch: 83	Loss: 0.069791	Acc: 13.6% (1362/10000)
[Test]  Epoch: 84	Loss: 0.069867	Acc: 13.4% (1338/10000)
[Test]  Epoch: 85	Loss: 0.070013	Acc: 13.9% (1393/10000)
[Test]  Epoch: 86	Loss: 0.070151	Acc: 13.2% (1316/10000)
[Test]  Epoch: 87	Loss: 0.069866	Acc: 13.6% (1361/10000)
[Test]  Epoch: 88	Loss: 0.070004	Acc: 13.5% (1354/10000)
[Test]  Epoch: 89	Loss: 0.069932	Acc: 13.6% (1359/10000)
[Test]  Epoch: 90	Loss: 0.070372	Acc: 13.6% (1360/10000)
[Test]  Epoch: 91	Loss: 0.070206	Acc: 13.6% (1361/10000)
[Test]  Epoch: 92	Loss: 0.069993	Acc: 13.3% (1328/10000)
[Test]  Epoch: 93	Loss: 0.070139	Acc: 13.6% (1358/10000)
[Test]  Epoch: 94	Loss: 0.070155	Acc: 13.9% (1386/10000)
[Test]  Epoch: 95	Loss: 0.070360	Acc: 13.6% (1361/10000)
[Test]  Epoch: 96	Loss: 0.070200	Acc: 13.8% (1375/10000)
[Test]  Epoch: 97	Loss: 0.070127	Acc: 13.3% (1327/10000)
[Test]  Epoch: 98	Loss: 0.070149	Acc: 13.8% (1383/10000)
[Test]  Epoch: 99	Loss: 0.070282	Acc: 13.7% (1370/10000)
[Test]  Epoch: 100	Loss: 0.070525	Acc: 13.2% (1315/10000)
===========finish==========
['2024-08-19', '01:10:36.975341', '100', 'test', '0.07052538459300994', '13.15', '13.93']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=0
get_sample_layers not_random
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007630	Acc: 85.2% (8523/10000)
[Test]  Epoch: 2	Loss: 0.007453	Acc: 85.4% (8540/10000)
[Test]  Epoch: 3	Loss: 0.007349	Acc: 85.7% (8570/10000)
[Test]  Epoch: 4	Loss: 0.007240	Acc: 85.7% (8574/10000)
[Test]  Epoch: 5	Loss: 0.007204	Acc: 85.8% (8578/10000)
[Test]  Epoch: 6	Loss: 0.007149	Acc: 85.8% (8580/10000)
[Test]  Epoch: 7	Loss: 0.007093	Acc: 85.8% (8583/10000)
[Test]  Epoch: 8	Loss: 0.007075	Acc: 85.8% (8579/10000)
[Test]  Epoch: 9	Loss: 0.007023	Acc: 85.8% (8575/10000)
[Test]  Epoch: 10	Loss: 0.006998	Acc: 85.9% (8590/10000)
[Test]  Epoch: 11	Loss: 0.006983	Acc: 85.9% (8587/10000)
[Test]  Epoch: 12	Loss: 0.006965	Acc: 85.9% (8587/10000)
[Test]  Epoch: 13	Loss: 0.006902	Acc: 85.9% (8594/10000)
[Test]  Epoch: 14	Loss: 0.006891	Acc: 86.0% (8597/10000)
[Test]  Epoch: 15	Loss: 0.006895	Acc: 86.0% (8605/10000)
[Test]  Epoch: 16	Loss: 0.006864	Acc: 86.0% (8602/10000)
[Test]  Epoch: 17	Loss: 0.006839	Acc: 86.0% (8602/10000)
[Test]  Epoch: 18	Loss: 0.006839	Acc: 86.0% (8604/10000)
[Test]  Epoch: 19	Loss: 0.006821	Acc: 86.2% (8617/10000)
[Test]  Epoch: 20	Loss: 0.006827	Acc: 86.0% (8599/10000)
[Test]  Epoch: 21	Loss: 0.006830	Acc: 86.0% (8596/10000)
[Test]  Epoch: 22	Loss: 0.006828	Acc: 86.0% (8603/10000)
[Test]  Epoch: 23	Loss: 0.006798	Acc: 86.2% (8615/10000)
[Test]  Epoch: 24	Loss: 0.006782	Acc: 86.1% (8613/10000)
[Test]  Epoch: 25	Loss: 0.006786	Acc: 86.1% (8612/10000)
[Test]  Epoch: 26	Loss: 0.006767	Acc: 86.1% (8610/10000)
[Test]  Epoch: 27	Loss: 0.006779	Acc: 86.1% (8609/10000)
[Test]  Epoch: 28	Loss: 0.006741	Acc: 86.2% (8619/10000)
[Test]  Epoch: 29	Loss: 0.006741	Acc: 86.2% (8618/10000)
[Test]  Epoch: 30	Loss: 0.006763	Acc: 86.0% (8599/10000)
[Test]  Epoch: 31	Loss: 0.006742	Acc: 86.2% (8623/10000)
[Test]  Epoch: 32	Loss: 0.006745	Acc: 86.2% (8616/10000)
[Test]  Epoch: 33	Loss: 0.006745	Acc: 86.2% (8615/10000)
[Test]  Epoch: 34	Loss: 0.006718	Acc: 86.1% (8610/10000)
[Test]  Epoch: 35	Loss: 0.006705	Acc: 86.3% (8630/10000)
[Test]  Epoch: 36	Loss: 0.006698	Acc: 86.2% (8622/10000)
[Test]  Epoch: 37	Loss: 0.006729	Acc: 86.0% (8603/10000)
[Test]  Epoch: 38	Loss: 0.006714	Acc: 86.1% (8611/10000)
[Test]  Epoch: 39	Loss: 0.006708	Acc: 86.1% (8611/10000)
[Test]  Epoch: 40	Loss: 0.006703	Acc: 86.1% (8611/10000)
[Test]  Epoch: 41	Loss: 0.006708	Acc: 86.1% (8608/10000)
[Test]  Epoch: 42	Loss: 0.006719	Acc: 86.1% (8607/10000)
[Test]  Epoch: 43	Loss: 0.006696	Acc: 86.2% (8618/10000)
[Test]  Epoch: 44	Loss: 0.006699	Acc: 86.1% (8611/10000)
[Test]  Epoch: 45	Loss: 0.006703	Acc: 86.1% (8608/10000)
[Test]  Epoch: 46	Loss: 0.006681	Acc: 86.2% (8616/10000)
[Test]  Epoch: 47	Loss: 0.006711	Acc: 86.1% (8612/10000)
[Test]  Epoch: 48	Loss: 0.006659	Acc: 86.1% (8612/10000)
[Test]  Epoch: 49	Loss: 0.006701	Acc: 86.2% (8615/10000)
[Test]  Epoch: 50	Loss: 0.006697	Acc: 86.1% (8606/10000)
[Test]  Epoch: 51	Loss: 0.006660	Acc: 86.0% (8595/10000)
[Test]  Epoch: 52	Loss: 0.006650	Acc: 86.2% (8616/10000)
[Test]  Epoch: 53	Loss: 0.006648	Acc: 86.1% (8611/10000)
[Test]  Epoch: 54	Loss: 0.006653	Acc: 86.1% (8613/10000)
[Test]  Epoch: 55	Loss: 0.006652	Acc: 86.1% (8612/10000)
[Test]  Epoch: 56	Loss: 0.006640	Acc: 86.1% (8607/10000)
[Test]  Epoch: 57	Loss: 0.006668	Acc: 86.1% (8609/10000)
[Test]  Epoch: 58	Loss: 0.006673	Acc: 86.1% (8607/10000)
[Test]  Epoch: 59	Loss: 0.006668	Acc: 86.1% (8609/10000)
[Test]  Epoch: 60	Loss: 0.006687	Acc: 86.2% (8620/10000)
[Test]  Epoch: 61	Loss: 0.006670	Acc: 86.1% (8612/10000)
[Test]  Epoch: 62	Loss: 0.006670	Acc: 86.1% (8611/10000)
[Test]  Epoch: 63	Loss: 0.006661	Acc: 86.0% (8605/10000)
[Test]  Epoch: 64	Loss: 0.006644	Acc: 86.1% (8610/10000)
[Test]  Epoch: 65	Loss: 0.006645	Acc: 86.2% (8618/10000)
[Test]  Epoch: 66	Loss: 0.006649	Acc: 86.1% (8608/10000)
[Test]  Epoch: 67	Loss: 0.006639	Acc: 86.1% (8611/10000)
[Test]  Epoch: 68	Loss: 0.006664	Acc: 86.1% (8607/10000)
[Test]  Epoch: 69	Loss: 0.006664	Acc: 86.2% (8617/10000)
[Test]  Epoch: 70	Loss: 0.006653	Acc: 86.1% (8606/10000)
[Test]  Epoch: 71	Loss: 0.006637	Acc: 86.1% (8609/10000)
[Test]  Epoch: 72	Loss: 0.006618	Acc: 86.1% (8613/10000)
[Test]  Epoch: 73	Loss: 0.006636	Acc: 86.1% (8609/10000)
[Test]  Epoch: 74	Loss: 0.006631	Acc: 86.0% (8604/10000)
[Test]  Epoch: 75	Loss: 0.006634	Acc: 86.1% (8614/10000)
[Test]  Epoch: 76	Loss: 0.006643	Acc: 86.1% (8608/10000)
[Test]  Epoch: 77	Loss: 0.006644	Acc: 86.1% (8614/10000)
[Test]  Epoch: 78	Loss: 0.006644	Acc: 86.2% (8617/10000)
[Test]  Epoch: 79	Loss: 0.006657	Acc: 86.2% (8615/10000)
[Test]  Epoch: 80	Loss: 0.006643	Acc: 86.0% (8605/10000)
[Test]  Epoch: 81	Loss: 0.006659	Acc: 86.0% (8605/10000)
[Test]  Epoch: 82	Loss: 0.006656	Acc: 86.1% (8606/10000)
[Test]  Epoch: 83	Loss: 0.006632	Acc: 86.1% (8609/10000)
[Test]  Epoch: 84	Loss: 0.006638	Acc: 86.1% (8608/10000)
[Test]  Epoch: 85	Loss: 0.006645	Acc: 86.1% (8607/10000)
[Test]  Epoch: 86	Loss: 0.006640	Acc: 86.2% (8615/10000)
[Test]  Epoch: 87	Loss: 0.006638	Acc: 86.1% (8614/10000)
[Test]  Epoch: 88	Loss: 0.006629	Acc: 86.1% (8609/10000)
[Test]  Epoch: 89	Loss: 0.006628	Acc: 86.1% (8612/10000)
[Test]  Epoch: 90	Loss: 0.006631	Acc: 86.1% (8608/10000)
[Test]  Epoch: 91	Loss: 0.006636	Acc: 86.1% (8607/10000)
[Test]  Epoch: 92	Loss: 0.006626	Acc: 86.2% (8621/10000)
[Test]  Epoch: 93	Loss: 0.006630	Acc: 86.1% (8610/10000)
[Test]  Epoch: 94	Loss: 0.006631	Acc: 86.1% (8607/10000)
[Test]  Epoch: 95	Loss: 0.006623	Acc: 86.1% (8613/10000)
[Test]  Epoch: 96	Loss: 0.006621	Acc: 86.2% (8624/10000)
[Test]  Epoch: 97	Loss: 0.006637	Acc: 86.1% (8610/10000)
[Test]  Epoch: 98	Loss: 0.006651	Acc: 86.0% (8596/10000)
[Test]  Epoch: 99	Loss: 0.006627	Acc: 86.2% (8615/10000)
[Test]  Epoch: 100	Loss: 0.006647	Acc: 86.1% (8611/10000)
===========finish==========
['2024-08-19', '01:13:08.049558', '100', 'test', '0.006647412545979023', '86.11', '86.3']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.013955	Acc: 79.3% (7927/10000)
[Test]  Epoch: 2	Loss: 0.008777	Acc: 82.8% (8283/10000)
[Test]  Epoch: 3	Loss: 0.008237	Acc: 83.5% (8349/10000)
[Test]  Epoch: 4	Loss: 0.007878	Acc: 84.1% (8406/10000)
[Test]  Epoch: 5	Loss: 0.007752	Acc: 84.3% (8430/10000)
[Test]  Epoch: 6	Loss: 0.007654	Acc: 84.3% (8433/10000)
[Test]  Epoch: 7	Loss: 0.007521	Acc: 84.4% (8436/10000)
[Test]  Epoch: 8	Loss: 0.007418	Acc: 84.4% (8439/10000)
[Test]  Epoch: 9	Loss: 0.007339	Acc: 84.4% (8441/10000)
[Test]  Epoch: 10	Loss: 0.007435	Acc: 84.6% (8459/10000)
[Test]  Epoch: 11	Loss: 0.007358	Acc: 84.7% (8471/10000)
[Test]  Epoch: 12	Loss: 0.007340	Acc: 84.6% (8463/10000)
[Test]  Epoch: 13	Loss: 0.007326	Acc: 84.6% (8456/10000)
[Test]  Epoch: 14	Loss: 0.007364	Acc: 84.6% (8460/10000)
[Test]  Epoch: 15	Loss: 0.007316	Acc: 84.8% (8476/10000)
[Test]  Epoch: 16	Loss: 0.007206	Acc: 84.9% (8486/10000)
[Test]  Epoch: 17	Loss: 0.007269	Acc: 84.8% (8480/10000)
[Test]  Epoch: 18	Loss: 0.007268	Acc: 84.8% (8477/10000)
[Test]  Epoch: 19	Loss: 0.007203	Acc: 84.9% (8493/10000)
[Test]  Epoch: 20	Loss: 0.007193	Acc: 84.9% (8486/10000)
[Test]  Epoch: 21	Loss: 0.007150	Acc: 84.8% (8483/10000)
[Test]  Epoch: 22	Loss: 0.007183	Acc: 84.9% (8493/10000)
[Test]  Epoch: 23	Loss: 0.007201	Acc: 85.0% (8496/10000)
[Test]  Epoch: 24	Loss: 0.007127	Acc: 85.2% (8516/10000)
[Test]  Epoch: 25	Loss: 0.007153	Acc: 85.1% (8513/10000)
[Test]  Epoch: 26	Loss: 0.007153	Acc: 85.0% (8501/10000)
[Test]  Epoch: 27	Loss: 0.007124	Acc: 85.2% (8518/10000)
[Test]  Epoch: 28	Loss: 0.007060	Acc: 85.2% (8521/10000)
[Test]  Epoch: 29	Loss: 0.007094	Acc: 85.2% (8525/10000)
[Test]  Epoch: 30	Loss: 0.007081	Acc: 85.1% (8512/10000)
[Test]  Epoch: 31	Loss: 0.007071	Acc: 85.3% (8533/10000)
[Test]  Epoch: 32	Loss: 0.007090	Acc: 85.4% (8536/10000)
[Test]  Epoch: 33	Loss: 0.007014	Acc: 85.3% (8535/10000)
[Test]  Epoch: 34	Loss: 0.007045	Acc: 85.2% (8524/10000)
[Test]  Epoch: 35	Loss: 0.007883	Acc: 83.0% (8303/10000)
[Test]  Epoch: 36	Loss: 0.007036	Acc: 85.2% (8519/10000)
[Test]  Epoch: 37	Loss: 0.007047	Acc: 85.3% (8529/10000)
[Test]  Epoch: 38	Loss: 0.007054	Acc: 85.4% (8536/10000)
[Test]  Epoch: 39	Loss: 0.006995	Acc: 85.3% (8535/10000)
[Test]  Epoch: 40	Loss: 0.006990	Acc: 85.4% (8544/10000)
[Test]  Epoch: 41	Loss: 0.006987	Acc: 85.3% (8531/10000)
[Test]  Epoch: 42	Loss: 0.006968	Acc: 85.3% (8530/10000)
[Test]  Epoch: 43	Loss: 0.006962	Acc: 85.4% (8537/10000)
[Test]  Epoch: 44	Loss: 0.006960	Acc: 85.4% (8540/10000)
[Test]  Epoch: 45	Loss: 0.006999	Acc: 85.3% (8531/10000)
[Test]  Epoch: 46	Loss: 0.006937	Acc: 85.4% (8539/10000)
[Test]  Epoch: 47	Loss: 0.006994	Acc: 85.4% (8538/10000)
[Test]  Epoch: 48	Loss: 0.006958	Acc: 85.2% (8525/10000)
[Test]  Epoch: 49	Loss: 0.007000	Acc: 85.3% (8533/10000)
[Test]  Epoch: 50	Loss: 0.006971	Acc: 85.3% (8526/10000)
[Test]  Epoch: 51	Loss: 0.006927	Acc: 85.4% (8542/10000)
[Test]  Epoch: 52	Loss: 0.006930	Acc: 85.4% (8540/10000)
[Test]  Epoch: 53	Loss: 0.006952	Acc: 85.4% (8542/10000)
[Test]  Epoch: 54	Loss: 0.006961	Acc: 85.3% (8528/10000)
[Test]  Epoch: 55	Loss: 0.006939	Acc: 85.4% (8539/10000)
[Test]  Epoch: 56	Loss: 0.006937	Acc: 85.4% (8538/10000)
[Test]  Epoch: 57	Loss: 0.006953	Acc: 85.4% (8542/10000)
[Test]  Epoch: 58	Loss: 0.006917	Acc: 85.4% (8541/10000)
[Test]  Epoch: 59	Loss: 0.006938	Acc: 85.4% (8541/10000)
[Test]  Epoch: 60	Loss: 0.006961	Acc: 85.3% (8533/10000)
[Test]  Epoch: 61	Loss: 0.006966	Acc: 85.3% (8532/10000)
[Test]  Epoch: 62	Loss: 0.006961	Acc: 85.3% (8534/10000)
[Test]  Epoch: 63	Loss: 0.006962	Acc: 85.4% (8536/10000)
[Test]  Epoch: 64	Loss: 0.006936	Acc: 85.4% (8539/10000)
[Test]  Epoch: 65	Loss: 0.006917	Acc: 85.5% (8545/10000)
[Test]  Epoch: 66	Loss: 0.006928	Acc: 85.4% (8541/10000)
[Test]  Epoch: 67	Loss: 0.006959	Acc: 85.3% (8528/10000)
[Test]  Epoch: 68	Loss: 0.006936	Acc: 85.4% (8539/10000)
[Test]  Epoch: 69	Loss: 0.006936	Acc: 85.4% (8542/10000)
[Test]  Epoch: 70	Loss: 0.006940	Acc: 85.3% (8529/10000)
[Test]  Epoch: 71	Loss: 0.006924	Acc: 85.3% (8535/10000)
[Test]  Epoch: 72	Loss: 0.006910	Acc: 85.3% (8533/10000)
[Test]  Epoch: 73	Loss: 0.006916	Acc: 85.5% (8548/10000)
[Test]  Epoch: 74	Loss: 0.006911	Acc: 85.4% (8543/10000)
[Test]  Epoch: 75	Loss: 0.006914	Acc: 85.5% (8547/10000)
[Test]  Epoch: 76	Loss: 0.006918	Acc: 85.3% (8529/10000)
[Test]  Epoch: 77	Loss: 0.006937	Acc: 85.4% (8544/10000)
[Test]  Epoch: 78	Loss: 0.006944	Acc: 85.4% (8543/10000)
[Test]  Epoch: 79	Loss: 0.006931	Acc: 85.4% (8539/10000)
[Test]  Epoch: 80	Loss: 0.006930	Acc: 85.3% (8532/10000)
[Test]  Epoch: 81	Loss: 0.006947	Acc: 85.5% (8546/10000)
[Test]  Epoch: 82	Loss: 0.006936	Acc: 85.4% (8542/10000)
[Test]  Epoch: 83	Loss: 0.006908	Acc: 85.4% (8544/10000)
[Test]  Epoch: 84	Loss: 0.006914	Acc: 85.4% (8540/10000)
[Test]  Epoch: 85	Loss: 0.006934	Acc: 85.4% (8537/10000)
[Test]  Epoch: 86	Loss: 0.006922	Acc: 85.4% (8543/10000)
[Test]  Epoch: 87	Loss: 0.006899	Acc: 85.3% (8535/10000)
[Test]  Epoch: 88	Loss: 0.006912	Acc: 85.3% (8533/10000)
[Test]  Epoch: 89	Loss: 0.006903	Acc: 85.3% (8534/10000)
[Test]  Epoch: 90	Loss: 0.006928	Acc: 85.5% (8549/10000)
[Test]  Epoch: 91	Loss: 0.006927	Acc: 85.4% (8543/10000)
[Test]  Epoch: 92	Loss: 0.006943	Acc: 85.4% (8540/10000)
[Test]  Epoch: 93	Loss: 0.006951	Acc: 85.4% (8539/10000)
[Test]  Epoch: 94	Loss: 0.006912	Acc: 85.5% (8548/10000)
[Test]  Epoch: 95	Loss: 0.006921	Acc: 85.5% (8550/10000)
[Test]  Epoch: 96	Loss: 0.006905	Acc: 85.5% (8551/10000)
[Test]  Epoch: 97	Loss: 0.006918	Acc: 85.4% (8541/10000)
[Test]  Epoch: 98	Loss: 0.006960	Acc: 85.4% (8539/10000)
[Test]  Epoch: 99	Loss: 0.006908	Acc: 85.4% (8543/10000)
[Test]  Epoch: 100	Loss: 0.006941	Acc: 85.5% (8550/10000)
===========finish==========
['2024-08-19', '01:14:55.578848', '100', 'test', '0.006940637321770192', '85.5', '85.51']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.014186	Acc: 71.7% (7168/10000)
[Test]  Epoch: 2	Loss: 0.009866	Acc: 79.1% (7913/10000)
[Test]  Epoch: 3	Loss: 0.009441	Acc: 80.5% (8046/10000)
[Test]  Epoch: 4	Loss: 0.009421	Acc: 80.8% (8078/10000)
[Test]  Epoch: 5	Loss: 0.009372	Acc: 80.8% (8080/10000)
[Test]  Epoch: 6	Loss: 0.009425	Acc: 80.4% (8044/10000)
[Test]  Epoch: 7	Loss: 0.009187	Acc: 80.9% (8092/10000)
[Test]  Epoch: 8	Loss: 0.009018	Acc: 81.4% (8140/10000)
[Test]  Epoch: 9	Loss: 0.008960	Acc: 81.4% (8138/10000)
[Test]  Epoch: 10	Loss: 0.009038	Acc: 81.2% (8123/10000)
[Test]  Epoch: 11	Loss: 0.008993	Acc: 81.3% (8129/10000)
[Test]  Epoch: 12	Loss: 0.009633	Acc: 80.5% (8050/10000)
[Test]  Epoch: 13	Loss: 0.009307	Acc: 80.7% (8073/10000)
[Test]  Epoch: 14	Loss: 0.009164	Acc: 81.1% (8109/10000)
[Test]  Epoch: 15	Loss: 0.009142	Acc: 81.0% (8100/10000)
[Test]  Epoch: 16	Loss: 0.009017	Acc: 81.2% (8124/10000)
[Test]  Epoch: 17	Loss: 0.009071	Acc: 81.4% (8143/10000)
[Test]  Epoch: 18	Loss: 0.009019	Acc: 81.2% (8123/10000)
[Test]  Epoch: 19	Loss: 0.009014	Acc: 81.3% (8134/10000)
[Test]  Epoch: 20	Loss: 0.008906	Acc: 81.5% (8151/10000)
[Test]  Epoch: 21	Loss: 0.008907	Acc: 81.6% (8161/10000)
[Test]  Epoch: 22	Loss: 0.008871	Acc: 81.7% (8165/10000)
[Test]  Epoch: 23	Loss: 0.008858	Acc: 81.5% (8150/10000)
[Test]  Epoch: 24	Loss: 0.008896	Acc: 81.5% (8147/10000)
[Test]  Epoch: 25	Loss: 0.008824	Acc: 81.7% (8166/10000)
[Test]  Epoch: 26	Loss: 0.008860	Acc: 81.5% (8145/10000)
[Test]  Epoch: 27	Loss: 0.008823	Acc: 81.7% (8168/10000)
[Test]  Epoch: 28	Loss: 0.008811	Acc: 81.7% (8166/10000)
[Test]  Epoch: 29	Loss: 0.008812	Acc: 81.7% (8171/10000)
[Test]  Epoch: 30	Loss: 0.008833	Acc: 81.7% (8170/10000)
[Test]  Epoch: 31	Loss: 0.008759	Acc: 81.8% (8185/10000)
[Test]  Epoch: 32	Loss: 0.008785	Acc: 81.9% (8192/10000)
[Test]  Epoch: 33	Loss: 0.008703	Acc: 81.9% (8187/10000)
[Test]  Epoch: 34	Loss: 0.008689	Acc: 82.0% (8195/10000)
[Test]  Epoch: 35	Loss: 0.009381	Acc: 80.6% (8056/10000)
[Test]  Epoch: 36	Loss: 0.008862	Acc: 81.5% (8152/10000)
[Test]  Epoch: 37	Loss: 0.009164	Acc: 81.0% (8095/10000)
[Test]  Epoch: 38	Loss: 0.008897	Acc: 81.5% (8148/10000)
[Test]  Epoch: 39	Loss: 0.008793	Acc: 81.6% (8157/10000)
[Test]  Epoch: 40	Loss: 0.008746	Acc: 81.9% (8187/10000)
[Test]  Epoch: 41	Loss: 0.008718	Acc: 81.8% (8181/10000)
[Test]  Epoch: 42	Loss: 0.008764	Acc: 81.8% (8182/10000)
[Test]  Epoch: 43	Loss: 0.008707	Acc: 81.8% (8179/10000)
[Test]  Epoch: 44	Loss: 0.008682	Acc: 81.9% (8186/10000)
[Test]  Epoch: 45	Loss: 0.008624	Acc: 81.9% (8191/10000)
[Test]  Epoch: 46	Loss: 0.008659	Acc: 81.9% (8192/10000)
[Test]  Epoch: 47	Loss: 0.008624	Acc: 81.9% (8193/10000)
[Test]  Epoch: 48	Loss: 0.008615	Acc: 82.0% (8198/10000)
[Test]  Epoch: 49	Loss: 0.009067	Acc: 81.2% (8116/10000)
[Test]  Epoch: 50	Loss: 0.008646	Acc: 82.0% (8200/10000)
[Test]  Epoch: 51	Loss: 0.008651	Acc: 82.0% (8197/10000)
[Test]  Epoch: 52	Loss: 0.008573	Acc: 82.3% (8227/10000)
[Test]  Epoch: 53	Loss: 0.008601	Acc: 82.2% (8217/10000)
[Test]  Epoch: 54	Loss: 0.008552	Acc: 82.1% (8206/10000)
[Test]  Epoch: 55	Loss: 0.008575	Acc: 82.1% (8214/10000)
[Test]  Epoch: 56	Loss: 0.008513	Acc: 82.2% (8219/10000)
[Test]  Epoch: 57	Loss: 0.008554	Acc: 82.2% (8223/10000)
[Test]  Epoch: 58	Loss: 0.008494	Acc: 82.1% (8213/10000)
[Test]  Epoch: 59	Loss: 0.008530	Acc: 82.0% (8202/10000)
[Test]  Epoch: 60	Loss: 0.008626	Acc: 82.1% (8208/10000)
[Test]  Epoch: 61	Loss: 0.008618	Acc: 81.9% (8188/10000)
[Test]  Epoch: 62	Loss: 0.008593	Acc: 82.0% (8203/10000)
[Test]  Epoch: 63	Loss: 0.008589	Acc: 82.0% (8202/10000)
[Test]  Epoch: 64	Loss: 0.008571	Acc: 82.1% (8210/10000)
[Test]  Epoch: 65	Loss: 0.008551	Acc: 82.2% (8220/10000)
[Test]  Epoch: 66	Loss: 0.008539	Acc: 82.2% (8222/10000)
[Test]  Epoch: 67	Loss: 0.008561	Acc: 82.1% (8206/10000)
[Test]  Epoch: 68	Loss: 0.008534	Acc: 82.2% (8218/10000)
[Test]  Epoch: 69	Loss: 0.008546	Acc: 82.3% (8227/10000)
[Test]  Epoch: 70	Loss: 0.008549	Acc: 82.1% (8209/10000)
[Test]  Epoch: 71	Loss: 0.008543	Acc: 82.2% (8220/10000)
[Test]  Epoch: 72	Loss: 0.008534	Acc: 82.1% (8209/10000)
[Test]  Epoch: 73	Loss: 0.008505	Acc: 82.1% (8210/10000)
[Test]  Epoch: 74	Loss: 0.008523	Acc: 82.2% (8220/10000)
[Test]  Epoch: 75	Loss: 0.008519	Acc: 82.3% (8229/10000)
[Test]  Epoch: 76	Loss: 0.008518	Acc: 82.1% (8213/10000)
[Test]  Epoch: 77	Loss: 0.008545	Acc: 82.3% (8228/10000)
[Test]  Epoch: 78	Loss: 0.008522	Acc: 82.2% (8217/10000)
[Test]  Epoch: 79	Loss: 0.008508	Acc: 82.3% (8229/10000)
[Test]  Epoch: 80	Loss: 0.008518	Acc: 82.2% (8220/10000)
[Test]  Epoch: 81	Loss: 0.008554	Acc: 82.2% (8222/10000)
[Test]  Epoch: 82	Loss: 0.008535	Acc: 82.2% (8224/10000)
[Test]  Epoch: 83	Loss: 0.008515	Acc: 82.3% (8229/10000)
[Test]  Epoch: 84	Loss: 0.008508	Acc: 82.2% (8221/10000)
[Test]  Epoch: 85	Loss: 0.008517	Acc: 82.3% (8234/10000)
[Test]  Epoch: 86	Loss: 0.008542	Acc: 82.3% (8233/10000)
[Test]  Epoch: 87	Loss: 0.008514	Acc: 82.3% (8232/10000)
[Test]  Epoch: 88	Loss: 0.008537	Acc: 82.4% (8239/10000)
[Test]  Epoch: 89	Loss: 0.008521	Acc: 82.2% (8224/10000)
[Test]  Epoch: 90	Loss: 0.008540	Acc: 82.1% (8213/10000)
[Test]  Epoch: 91	Loss: 0.008516	Acc: 82.3% (8229/10000)
[Test]  Epoch: 92	Loss: 0.008528	Acc: 82.2% (8222/10000)
[Test]  Epoch: 93	Loss: 0.008520	Acc: 82.3% (8232/10000)
[Test]  Epoch: 94	Loss: 0.008526	Acc: 82.3% (8226/10000)
[Test]  Epoch: 95	Loss: 0.008532	Acc: 82.2% (8215/10000)
[Test]  Epoch: 96	Loss: 0.008513	Acc: 82.3% (8227/10000)
[Test]  Epoch: 97	Loss: 0.008544	Acc: 82.3% (8226/10000)
[Test]  Epoch: 98	Loss: 0.008560	Acc: 82.3% (8228/10000)
[Test]  Epoch: 99	Loss: 0.008493	Acc: 82.2% (8224/10000)
[Test]  Epoch: 100	Loss: 0.008506	Acc: 82.3% (8227/10000)
===========finish==========
['2024-08-19', '01:16:47.583730', '100', 'test', '0.008506324470043182', '82.27', '82.39']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.024590	Acc: 61.1% (6114/10000)
[Test]  Epoch: 2	Loss: 0.012269	Acc: 74.2% (7425/10000)
[Test]  Epoch: 3	Loss: 0.011509	Acc: 76.0% (7596/10000)
[Test]  Epoch: 4	Loss: 0.011402	Acc: 76.5% (7652/10000)
[Test]  Epoch: 5	Loss: 0.011061	Acc: 77.2% (7717/10000)
[Test]  Epoch: 6	Loss: 0.011023	Acc: 77.3% (7734/10000)
[Test]  Epoch: 7	Loss: 0.010768	Acc: 77.4% (7744/10000)
[Test]  Epoch: 8	Loss: 0.010847	Acc: 77.7% (7773/10000)
[Test]  Epoch: 9	Loss: 0.010549	Acc: 78.1% (7811/10000)
[Test]  Epoch: 10	Loss: 0.010684	Acc: 78.0% (7801/10000)
[Test]  Epoch: 11	Loss: 0.010530	Acc: 78.4% (7839/10000)
[Test]  Epoch: 12	Loss: 0.010908	Acc: 77.2% (7720/10000)
[Test]  Epoch: 13	Loss: 0.010503	Acc: 78.2% (7819/10000)
[Test]  Epoch: 14	Loss: 0.010286	Acc: 78.4% (7836/10000)
[Test]  Epoch: 15	Loss: 0.010375	Acc: 78.3% (7835/10000)
[Test]  Epoch: 16	Loss: 0.010194	Acc: 78.8% (7879/10000)
[Test]  Epoch: 17	Loss: 0.010251	Acc: 78.7% (7871/10000)
[Test]  Epoch: 18	Loss: 0.010374	Acc: 78.4% (7842/10000)
[Test]  Epoch: 19	Loss: 0.010211	Acc: 78.6% (7859/10000)
[Test]  Epoch: 20	Loss: 0.010223	Acc: 78.9% (7889/10000)
[Test]  Epoch: 21	Loss: 0.010156	Acc: 78.7% (7870/10000)
[Test]  Epoch: 22	Loss: 0.010036	Acc: 79.3% (7932/10000)
[Test]  Epoch: 23	Loss: 0.010072	Acc: 79.0% (7902/10000)
[Test]  Epoch: 24	Loss: 0.010063	Acc: 79.3% (7932/10000)
[Test]  Epoch: 25	Loss: 0.010051	Acc: 79.1% (7907/10000)
[Test]  Epoch: 26	Loss: 0.009988	Acc: 79.2% (7915/10000)
[Test]  Epoch: 27	Loss: 0.009920	Acc: 79.3% (7927/10000)
[Test]  Epoch: 28	Loss: 0.010072	Acc: 79.0% (7903/10000)
[Test]  Epoch: 29	Loss: 0.010304	Acc: 78.4% (7838/10000)
[Test]  Epoch: 30	Loss: 0.009917	Acc: 79.2% (7920/10000)
[Test]  Epoch: 31	Loss: 0.009849	Acc: 79.7% (7972/10000)
[Test]  Epoch: 32	Loss: 0.009942	Acc: 79.5% (7945/10000)
[Test]  Epoch: 33	Loss: 0.009770	Acc: 79.8% (7975/10000)
[Test]  Epoch: 34	Loss: 0.009724	Acc: 79.8% (7985/10000)
[Test]  Epoch: 35	Loss: 0.010326	Acc: 78.0% (7801/10000)
[Test]  Epoch: 36	Loss: 0.009779	Acc: 79.7% (7966/10000)
[Test]  Epoch: 37	Loss: 0.009768	Acc: 79.7% (7965/10000)
[Test]  Epoch: 38	Loss: 0.010629	Acc: 77.8% (7775/10000)
[Test]  Epoch: 39	Loss: 0.010193	Acc: 79.0% (7901/10000)
[Test]  Epoch: 40	Loss: 0.010055	Acc: 79.3% (7933/10000)
[Test]  Epoch: 41	Loss: 0.009997	Acc: 79.3% (7933/10000)
[Test]  Epoch: 42	Loss: 0.009982	Acc: 79.2% (7920/10000)
[Test]  Epoch: 43	Loss: 0.009911	Acc: 79.5% (7948/10000)
[Test]  Epoch: 44	Loss: 0.009886	Acc: 79.5% (7948/10000)
[Test]  Epoch: 45	Loss: 0.009870	Acc: 79.3% (7935/10000)
[Test]  Epoch: 46	Loss: 0.009747	Acc: 79.7% (7970/10000)
[Test]  Epoch: 47	Loss: 0.009828	Acc: 79.5% (7953/10000)
[Test]  Epoch: 48	Loss: 0.009838	Acc: 79.6% (7960/10000)
[Test]  Epoch: 49	Loss: 0.009772	Acc: 79.5% (7946/10000)
[Test]  Epoch: 50	Loss: 0.009805	Acc: 79.6% (7961/10000)
[Test]  Epoch: 51	Loss: 0.009705	Acc: 79.6% (7959/10000)
[Test]  Epoch: 52	Loss: 0.009685	Acc: 79.8% (7979/10000)
[Test]  Epoch: 53	Loss: 0.009720	Acc: 79.9% (7988/10000)
[Test]  Epoch: 54	Loss: 0.009717	Acc: 79.6% (7963/10000)
[Test]  Epoch: 55	Loss: 0.009683	Acc: 79.8% (7980/10000)
[Test]  Epoch: 56	Loss: 0.009652	Acc: 79.8% (7980/10000)
[Test]  Epoch: 57	Loss: 0.009577	Acc: 80.1% (8014/10000)
[Test]  Epoch: 58	Loss: 0.009556	Acc: 79.7% (7973/10000)
[Test]  Epoch: 59	Loss: 0.009590	Acc: 79.7% (7974/10000)
[Test]  Epoch: 60	Loss: 0.009669	Acc: 79.7% (7974/10000)
[Test]  Epoch: 61	Loss: 0.009659	Acc: 79.7% (7969/10000)
[Test]  Epoch: 62	Loss: 0.009638	Acc: 79.7% (7971/10000)
[Test]  Epoch: 63	Loss: 0.009608	Acc: 79.8% (7980/10000)
[Test]  Epoch: 64	Loss: 0.009619	Acc: 79.9% (7990/10000)
[Test]  Epoch: 65	Loss: 0.009575	Acc: 79.8% (7984/10000)
[Test]  Epoch: 66	Loss: 0.009565	Acc: 79.8% (7977/10000)
[Test]  Epoch: 67	Loss: 0.009627	Acc: 79.7% (7972/10000)
[Test]  Epoch: 68	Loss: 0.009596	Acc: 79.8% (7985/10000)
[Test]  Epoch: 69	Loss: 0.009585	Acc: 79.8% (7985/10000)
[Test]  Epoch: 70	Loss: 0.009575	Acc: 80.0% (7996/10000)
[Test]  Epoch: 71	Loss: 0.009573	Acc: 79.7% (7970/10000)
[Test]  Epoch: 72	Loss: 0.009603	Acc: 79.8% (7980/10000)
[Test]  Epoch: 73	Loss: 0.009581	Acc: 79.8% (7981/10000)
[Test]  Epoch: 74	Loss: 0.009571	Acc: 79.9% (7987/10000)
[Test]  Epoch: 75	Loss: 0.009582	Acc: 79.8% (7985/10000)
[Test]  Epoch: 76	Loss: 0.009561	Acc: 79.9% (7987/10000)
[Test]  Epoch: 77	Loss: 0.009564	Acc: 79.8% (7975/10000)
[Test]  Epoch: 78	Loss: 0.009583	Acc: 79.8% (7983/10000)
[Test]  Epoch: 79	Loss: 0.009549	Acc: 79.9% (7991/10000)
[Test]  Epoch: 80	Loss: 0.009568	Acc: 79.8% (7984/10000)
[Test]  Epoch: 81	Loss: 0.009591	Acc: 79.7% (7974/10000)
[Test]  Epoch: 82	Loss: 0.009593	Acc: 79.8% (7975/10000)
[Test]  Epoch: 83	Loss: 0.009546	Acc: 79.8% (7983/10000)
[Test]  Epoch: 84	Loss: 0.009573	Acc: 79.9% (7986/10000)
[Test]  Epoch: 85	Loss: 0.009557	Acc: 79.8% (7983/10000)
[Test]  Epoch: 86	Loss: 0.009531	Acc: 79.9% (7988/10000)
[Test]  Epoch: 87	Loss: 0.009557	Acc: 79.9% (7989/10000)
[Test]  Epoch: 88	Loss: 0.009593	Acc: 79.8% (7982/10000)
[Test]  Epoch: 89	Loss: 0.009600	Acc: 79.9% (7988/10000)
[Test]  Epoch: 90	Loss: 0.009599	Acc: 79.8% (7985/10000)
[Test]  Epoch: 91	Loss: 0.009580	Acc: 79.9% (7988/10000)
[Test]  Epoch: 92	Loss: 0.009571	Acc: 79.9% (7993/10000)
[Test]  Epoch: 93	Loss: 0.009592	Acc: 80.0% (8004/10000)
[Test]  Epoch: 94	Loss: 0.009580	Acc: 79.9% (7993/10000)
[Test]  Epoch: 95	Loss: 0.009540	Acc: 80.0% (8000/10000)
[Test]  Epoch: 96	Loss: 0.009497	Acc: 80.1% (8008/10000)
[Test]  Epoch: 97	Loss: 0.009551	Acc: 79.9% (7993/10000)
[Test]  Epoch: 98	Loss: 0.009601	Acc: 79.9% (7986/10000)
[Test]  Epoch: 99	Loss: 0.009527	Acc: 79.8% (7983/10000)
[Test]  Epoch: 100	Loss: 0.009527	Acc: 79.9% (7988/10000)
===========finish==========
['2024-08-19', '01:18:53.228348', '100', 'test', '0.009526920604705811', '79.88', '80.14']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035177	Acc: 35.1% (3506/10000)
[Test]  Epoch: 2	Loss: 0.021141	Acc: 53.8% (5379/10000)
[Test]  Epoch: 3	Loss: 0.019898	Acc: 57.3% (5732/10000)
[Test]  Epoch: 4	Loss: 0.019295	Acc: 60.5% (6046/10000)
[Test]  Epoch: 5	Loss: 0.019579	Acc: 61.0% (6097/10000)
[Test]  Epoch: 6	Loss: 0.019292	Acc: 62.6% (6264/10000)
[Test]  Epoch: 7	Loss: 0.019523	Acc: 62.8% (6279/10000)
[Test]  Epoch: 8	Loss: 0.018898	Acc: 64.0% (6395/10000)
[Test]  Epoch: 9	Loss: 0.018914	Acc: 63.7% (6367/10000)
[Test]  Epoch: 10	Loss: 0.019317	Acc: 63.7% (6369/10000)
[Test]  Epoch: 11	Loss: 0.018895	Acc: 64.6% (6457/10000)
[Test]  Epoch: 12	Loss: 0.019800	Acc: 63.0% (6305/10000)
[Test]  Epoch: 13	Loss: 0.018226	Acc: 64.9% (6492/10000)
[Test]  Epoch: 14	Loss: 0.018412	Acc: 64.3% (6432/10000)
[Test]  Epoch: 15	Loss: 0.018946	Acc: 64.7% (6469/10000)
[Test]  Epoch: 16	Loss: 0.018818	Acc: 64.4% (6441/10000)
[Test]  Epoch: 17	Loss: 0.018226	Acc: 65.5% (6553/10000)
[Test]  Epoch: 18	Loss: 0.017772	Acc: 66.2% (6615/10000)
[Test]  Epoch: 19	Loss: 0.017796	Acc: 65.5% (6552/10000)
[Test]  Epoch: 20	Loss: 0.019736	Acc: 63.3% (6327/10000)
[Test]  Epoch: 21	Loss: 0.018246	Acc: 65.4% (6540/10000)
[Test]  Epoch: 22	Loss: 0.018209	Acc: 65.5% (6547/10000)
[Test]  Epoch: 23	Loss: 0.017864	Acc: 65.4% (6540/10000)
[Test]  Epoch: 24	Loss: 0.018274	Acc: 65.1% (6510/10000)
[Test]  Epoch: 25	Loss: 0.018264	Acc: 65.4% (6539/10000)
[Test]  Epoch: 26	Loss: 0.017935	Acc: 65.5% (6555/10000)
[Test]  Epoch: 27	Loss: 0.017884	Acc: 65.4% (6539/10000)
[Test]  Epoch: 28	Loss: 0.017807	Acc: 65.7% (6572/10000)
[Test]  Epoch: 29	Loss: 0.017634	Acc: 65.8% (6581/10000)
[Test]  Epoch: 30	Loss: 0.018099	Acc: 65.6% (6557/10000)
[Test]  Epoch: 31	Loss: 0.018235	Acc: 65.0% (6503/10000)
[Test]  Epoch: 32	Loss: 0.017860	Acc: 65.7% (6569/10000)
[Test]  Epoch: 33	Loss: 0.017521	Acc: 66.2% (6623/10000)
[Test]  Epoch: 34	Loss: 0.017501	Acc: 66.2% (6618/10000)
[Test]  Epoch: 35	Loss: 0.019449	Acc: 63.3% (6328/10000)
[Test]  Epoch: 36	Loss: 0.017626	Acc: 66.0% (6596/10000)
[Test]  Epoch: 37	Loss: 0.017593	Acc: 66.2% (6616/10000)
[Test]  Epoch: 38	Loss: 0.018873	Acc: 65.1% (6508/10000)
[Test]  Epoch: 39	Loss: 0.017553	Acc: 66.5% (6646/10000)
[Test]  Epoch: 40	Loss: 0.017073	Acc: 66.8% (6682/10000)
[Test]  Epoch: 41	Loss: 0.017120	Acc: 66.5% (6655/10000)
[Test]  Epoch: 42	Loss: 0.017186	Acc: 66.7% (6670/10000)
[Test]  Epoch: 43	Loss: 0.017235	Acc: 66.7% (6667/10000)
[Test]  Epoch: 44	Loss: 0.016931	Acc: 66.7% (6672/10000)
[Test]  Epoch: 45	Loss: 0.016764	Acc: 67.0% (6700/10000)
[Test]  Epoch: 46	Loss: 0.016762	Acc: 67.1% (6710/10000)
[Test]  Epoch: 47	Loss: 0.016663	Acc: 67.3% (6733/10000)
[Test]  Epoch: 48	Loss: 0.016869	Acc: 67.0% (6701/10000)
[Test]  Epoch: 49	Loss: 0.016995	Acc: 67.1% (6714/10000)
[Test]  Epoch: 50	Loss: 0.016974	Acc: 67.3% (6731/10000)
[Test]  Epoch: 51	Loss: 0.016878	Acc: 67.0% (6703/10000)
[Test]  Epoch: 52	Loss: 0.017039	Acc: 67.0% (6700/10000)
[Test]  Epoch: 53	Loss: 0.017043	Acc: 66.8% (6678/10000)
[Test]  Epoch: 54	Loss: 0.016764	Acc: 67.2% (6720/10000)
[Test]  Epoch: 55	Loss: 0.016843	Acc: 67.2% (6718/10000)
[Test]  Epoch: 56	Loss: 0.017076	Acc: 66.9% (6691/10000)
[Test]  Epoch: 57	Loss: 0.016839	Acc: 67.0% (6705/10000)
[Test]  Epoch: 58	Loss: 0.016426	Acc: 67.6% (6758/10000)
[Test]  Epoch: 59	Loss: 0.016579	Acc: 67.4% (6739/10000)
[Test]  Epoch: 60	Loss: 0.016502	Acc: 67.5% (6750/10000)
[Test]  Epoch: 61	Loss: 0.016525	Acc: 67.6% (6757/10000)
[Test]  Epoch: 62	Loss: 0.016500	Acc: 67.7% (6766/10000)
[Test]  Epoch: 63	Loss: 0.016451	Acc: 67.6% (6757/10000)
[Test]  Epoch: 64	Loss: 0.016496	Acc: 67.5% (6753/10000)
[Test]  Epoch: 65	Loss: 0.016449	Acc: 67.5% (6751/10000)
[Test]  Epoch: 66	Loss: 0.016463	Acc: 67.5% (6755/10000)
[Test]  Epoch: 67	Loss: 0.016531	Acc: 67.4% (6744/10000)
[Test]  Epoch: 68	Loss: 0.016509	Acc: 67.6% (6759/10000)
[Test]  Epoch: 69	Loss: 0.016563	Acc: 67.5% (6747/10000)
[Test]  Epoch: 70	Loss: 0.016507	Acc: 67.5% (6753/10000)
[Test]  Epoch: 71	Loss: 0.016468	Acc: 67.4% (6743/10000)
[Test]  Epoch: 72	Loss: 0.016466	Acc: 67.5% (6747/10000)
[Test]  Epoch: 73	Loss: 0.016493	Acc: 67.5% (6754/10000)
[Test]  Epoch: 74	Loss: 0.016497	Acc: 67.5% (6753/10000)
[Test]  Epoch: 75	Loss: 0.016566	Acc: 67.5% (6746/10000)
[Test]  Epoch: 76	Loss: 0.016538	Acc: 67.7% (6767/10000)
[Test]  Epoch: 77	Loss: 0.016561	Acc: 67.6% (6764/10000)
[Test]  Epoch: 78	Loss: 0.016533	Acc: 67.6% (6764/10000)
[Test]  Epoch: 79	Loss: 0.016520	Acc: 67.6% (6764/10000)
[Test]  Epoch: 80	Loss: 0.016496	Acc: 67.6% (6756/10000)
[Test]  Epoch: 81	Loss: 0.016529	Acc: 67.6% (6757/10000)
[Test]  Epoch: 82	Loss: 0.016654	Acc: 67.6% (6759/10000)
[Test]  Epoch: 83	Loss: 0.016527	Acc: 67.6% (6761/10000)
[Test]  Epoch: 84	Loss: 0.016520	Acc: 67.6% (6756/10000)
[Test]  Epoch: 85	Loss: 0.016507	Acc: 67.7% (6767/10000)
[Test]  Epoch: 86	Loss: 0.016500	Acc: 67.7% (6771/10000)
[Test]  Epoch: 87	Loss: 0.016529	Acc: 67.7% (6770/10000)
[Test]  Epoch: 88	Loss: 0.016665	Acc: 67.4% (6738/10000)
[Test]  Epoch: 89	Loss: 0.016640	Acc: 67.3% (6733/10000)
[Test]  Epoch: 90	Loss: 0.016568	Acc: 67.4% (6740/10000)
[Test]  Epoch: 91	Loss: 0.016609	Acc: 67.3% (6727/10000)
[Test]  Epoch: 92	Loss: 0.016562	Acc: 67.4% (6741/10000)
[Test]  Epoch: 93	Loss: 0.016538	Acc: 67.5% (6750/10000)
[Test]  Epoch: 94	Loss: 0.016589	Acc: 67.6% (6759/10000)
[Test]  Epoch: 95	Loss: 0.016576	Acc: 67.5% (6754/10000)
[Test]  Epoch: 96	Loss: 0.016528	Acc: 67.6% (6757/10000)
[Test]  Epoch: 97	Loss: 0.016554	Acc: 67.5% (6750/10000)
[Test]  Epoch: 98	Loss: 0.016686	Acc: 67.5% (6754/10000)
[Test]  Epoch: 99	Loss: 0.016628	Acc: 67.6% (6761/10000)
[Test]  Epoch: 100	Loss: 0.016553	Acc: 67.6% (6764/10000)
===========finish==========
['2024-08-19', '01:21:00.448274', '100', 'test', '0.016553499937057493', '67.64', '67.71']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.049674	Acc: 31.7% (3171/10000)
[Test]  Epoch: 2	Loss: 0.022222	Acc: 51.6% (5165/10000)
[Test]  Epoch: 3	Loss: 0.018525	Acc: 60.1% (6015/10000)
[Test]  Epoch: 4	Loss: 0.017539	Acc: 63.1% (6306/10000)
[Test]  Epoch: 5	Loss: 0.017182	Acc: 65.2% (6519/10000)
[Test]  Epoch: 6	Loss: 0.017044	Acc: 65.8% (6579/10000)
[Test]  Epoch: 7	Loss: 0.017791	Acc: 64.8% (6481/10000)
[Test]  Epoch: 8	Loss: 0.017475	Acc: 65.9% (6590/10000)
[Test]  Epoch: 9	Loss: 0.017065	Acc: 66.8% (6676/10000)
[Test]  Epoch: 10	Loss: 0.017032	Acc: 67.3% (6729/10000)
[Test]  Epoch: 11	Loss: 0.017583	Acc: 66.9% (6686/10000)
[Test]  Epoch: 12	Loss: 0.016862	Acc: 67.2% (6718/10000)
[Test]  Epoch: 13	Loss: 0.016183	Acc: 68.1% (6811/10000)
[Test]  Epoch: 14	Loss: 0.017118	Acc: 67.0% (6697/10000)
[Test]  Epoch: 15	Loss: 0.018324	Acc: 67.3% (6732/10000)
[Test]  Epoch: 16	Loss: 0.016229	Acc: 68.5% (6849/10000)
[Test]  Epoch: 17	Loss: 0.016156	Acc: 68.8% (6875/10000)
[Test]  Epoch: 18	Loss: 0.015944	Acc: 69.3% (6931/10000)
[Test]  Epoch: 19	Loss: 0.015659	Acc: 69.2% (6920/10000)
[Test]  Epoch: 20	Loss: 0.016127	Acc: 68.8% (6884/10000)
[Test]  Epoch: 21	Loss: 0.015509	Acc: 69.5% (6950/10000)
[Test]  Epoch: 22	Loss: 0.015990	Acc: 69.2% (6918/10000)
[Test]  Epoch: 23	Loss: 0.016003	Acc: 68.8% (6882/10000)
[Test]  Epoch: 24	Loss: 0.015693	Acc: 69.2% (6921/10000)
[Test]  Epoch: 25	Loss: 0.016128	Acc: 69.0% (6898/10000)
[Test]  Epoch: 26	Loss: 0.016893	Acc: 67.5% (6751/10000)
[Test]  Epoch: 27	Loss: 0.016478	Acc: 67.7% (6770/10000)
[Test]  Epoch: 28	Loss: 0.015667	Acc: 69.7% (6966/10000)
[Test]  Epoch: 29	Loss: 0.015689	Acc: 69.5% (6946/10000)
[Test]  Epoch: 30	Loss: 0.016005	Acc: 69.0% (6903/10000)
[Test]  Epoch: 31	Loss: 0.015229	Acc: 70.2% (7018/10000)
[Test]  Epoch: 32	Loss: 0.015112	Acc: 70.0% (7002/10000)
[Test]  Epoch: 33	Loss: 0.014804	Acc: 70.3% (7033/10000)
[Test]  Epoch: 34	Loss: 0.015196	Acc: 70.1% (7014/10000)
[Test]  Epoch: 35	Loss: 0.016511	Acc: 68.1% (6809/10000)
[Test]  Epoch: 36	Loss: 0.015046	Acc: 70.3% (7033/10000)
[Test]  Epoch: 37	Loss: 0.014876	Acc: 70.5% (7053/10000)
[Test]  Epoch: 38	Loss: 0.015035	Acc: 70.5% (7051/10000)
[Test]  Epoch: 39	Loss: 0.015040	Acc: 70.8% (7075/10000)
[Test]  Epoch: 40	Loss: 0.014898	Acc: 70.6% (7063/10000)
[Test]  Epoch: 41	Loss: 0.014990	Acc: 71.0% (7101/10000)
[Test]  Epoch: 42	Loss: 0.014811	Acc: 70.7% (7074/10000)
[Test]  Epoch: 43	Loss: 0.014715	Acc: 71.2% (7124/10000)
[Test]  Epoch: 44	Loss: 0.014832	Acc: 70.8% (7078/10000)
[Test]  Epoch: 45	Loss: 0.014662	Acc: 71.2% (7120/10000)
[Test]  Epoch: 46	Loss: 0.014618	Acc: 71.1% (7114/10000)
[Test]  Epoch: 47	Loss: 0.014556	Acc: 71.2% (7115/10000)
[Test]  Epoch: 48	Loss: 0.014513	Acc: 71.3% (7132/10000)
[Test]  Epoch: 49	Loss: 0.014733	Acc: 71.0% (7105/10000)
[Test]  Epoch: 50	Loss: 0.014733	Acc: 70.9% (7094/10000)
[Test]  Epoch: 51	Loss: 0.014413	Acc: 71.3% (7126/10000)
[Test]  Epoch: 52	Loss: 0.014614	Acc: 71.3% (7134/10000)
[Test]  Epoch: 53	Loss: 0.014760	Acc: 70.8% (7085/10000)
[Test]  Epoch: 54	Loss: 0.014639	Acc: 71.2% (7120/10000)
[Test]  Epoch: 55	Loss: 0.014283	Acc: 71.4% (7137/10000)
[Test]  Epoch: 56	Loss: 0.014455	Acc: 71.4% (7143/10000)
[Test]  Epoch: 57	Loss: 0.014549	Acc: 71.3% (7126/10000)
[Test]  Epoch: 58	Loss: 0.014212	Acc: 71.8% (7184/10000)
[Test]  Epoch: 59	Loss: 0.014283	Acc: 71.4% (7139/10000)
[Test]  Epoch: 60	Loss: 0.014371	Acc: 71.6% (7164/10000)
[Test]  Epoch: 61	Loss: 0.014322	Acc: 71.8% (7176/10000)
[Test]  Epoch: 62	Loss: 0.014297	Acc: 71.8% (7177/10000)
[Test]  Epoch: 63	Loss: 0.014189	Acc: 71.8% (7178/10000)
[Test]  Epoch: 64	Loss: 0.014221	Acc: 71.8% (7176/10000)
[Test]  Epoch: 65	Loss: 0.014203	Acc: 71.9% (7186/10000)
[Test]  Epoch: 66	Loss: 0.014157	Acc: 71.9% (7190/10000)
[Test]  Epoch: 67	Loss: 0.014244	Acc: 71.6% (7157/10000)
[Test]  Epoch: 68	Loss: 0.014259	Acc: 71.8% (7179/10000)
[Test]  Epoch: 69	Loss: 0.014280	Acc: 71.7% (7165/10000)
[Test]  Epoch: 70	Loss: 0.014167	Acc: 71.6% (7159/10000)
[Test]  Epoch: 71	Loss: 0.014146	Acc: 71.6% (7161/10000)
[Test]  Epoch: 72	Loss: 0.014130	Acc: 71.8% (7185/10000)
[Test]  Epoch: 73	Loss: 0.014123	Acc: 71.8% (7175/10000)
[Test]  Epoch: 74	Loss: 0.014146	Acc: 71.7% (7165/10000)
[Test]  Epoch: 75	Loss: 0.014183	Acc: 71.7% (7173/10000)
[Test]  Epoch: 76	Loss: 0.014178	Acc: 71.8% (7177/10000)
[Test]  Epoch: 77	Loss: 0.014244	Acc: 71.8% (7176/10000)
[Test]  Epoch: 78	Loss: 0.014269	Acc: 71.8% (7183/10000)
[Test]  Epoch: 79	Loss: 0.014180	Acc: 71.9% (7189/10000)
[Test]  Epoch: 80	Loss: 0.014202	Acc: 71.9% (7187/10000)
[Test]  Epoch: 81	Loss: 0.014146	Acc: 71.8% (7182/10000)
[Test]  Epoch: 82	Loss: 0.014179	Acc: 71.9% (7186/10000)
[Test]  Epoch: 83	Loss: 0.014163	Acc: 72.0% (7202/10000)
[Test]  Epoch: 84	Loss: 0.014225	Acc: 71.9% (7187/10000)
[Test]  Epoch: 85	Loss: 0.014224	Acc: 71.9% (7191/10000)
[Test]  Epoch: 86	Loss: 0.014199	Acc: 72.0% (7197/10000)
[Test]  Epoch: 87	Loss: 0.014290	Acc: 71.7% (7170/10000)
[Test]  Epoch: 88	Loss: 0.014313	Acc: 71.6% (7163/10000)
[Test]  Epoch: 89	Loss: 0.014249	Acc: 71.8% (7183/10000)
[Test]  Epoch: 90	Loss: 0.014237	Acc: 71.9% (7190/10000)
[Test]  Epoch: 91	Loss: 0.014202	Acc: 71.7% (7173/10000)
[Test]  Epoch: 92	Loss: 0.014186	Acc: 71.8% (7182/10000)
[Test]  Epoch: 93	Loss: 0.014198	Acc: 71.8% (7178/10000)
[Test]  Epoch: 94	Loss: 0.014228	Acc: 71.9% (7189/10000)
[Test]  Epoch: 95	Loss: 0.014182	Acc: 72.0% (7197/10000)
[Test]  Epoch: 96	Loss: 0.014125	Acc: 72.0% (7200/10000)
[Test]  Epoch: 97	Loss: 0.014193	Acc: 72.0% (7195/10000)
[Test]  Epoch: 98	Loss: 0.014228	Acc: 71.9% (7187/10000)
[Test]  Epoch: 99	Loss: 0.014237	Acc: 71.8% (7182/10000)
[Test]  Epoch: 100	Loss: 0.014237	Acc: 71.9% (7186/10000)
===========finish==========
['2024-08-19', '01:23:05.177229', '100', 'test', '0.014237045165896415', '71.86', '72.02']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.026351	Acc: 56.6% (5664/10000)
[Test]  Epoch: 2	Loss: 0.017998	Acc: 66.5% (6648/10000)
[Test]  Epoch: 3	Loss: 0.015045	Acc: 70.9% (7089/10000)
[Test]  Epoch: 4	Loss: 0.015511	Acc: 70.5% (7047/10000)
[Test]  Epoch: 5	Loss: 0.015065	Acc: 71.1% (7111/10000)
[Test]  Epoch: 6	Loss: 0.015121	Acc: 71.2% (7119/10000)
[Test]  Epoch: 7	Loss: 0.013964	Acc: 72.7% (7265/10000)
[Test]  Epoch: 8	Loss: 0.013901	Acc: 72.8% (7275/10000)
[Test]  Epoch: 9	Loss: 0.013760	Acc: 72.8% (7284/10000)
[Test]  Epoch: 10	Loss: 0.013721	Acc: 73.2% (7319/10000)
[Test]  Epoch: 11	Loss: 0.013315	Acc: 73.8% (7384/10000)
[Test]  Epoch: 12	Loss: 0.013623	Acc: 72.8% (7275/10000)
[Test]  Epoch: 13	Loss: 0.013256	Acc: 73.6% (7361/10000)
[Test]  Epoch: 14	Loss: 0.013340	Acc: 73.3% (7331/10000)
[Test]  Epoch: 15	Loss: 0.013107	Acc: 73.6% (7361/10000)
[Test]  Epoch: 16	Loss: 0.013033	Acc: 74.1% (7408/10000)
[Test]  Epoch: 17	Loss: 0.013175	Acc: 73.6% (7363/10000)
[Test]  Epoch: 18	Loss: 0.013127	Acc: 74.0% (7402/10000)
[Test]  Epoch: 19	Loss: 0.013065	Acc: 73.9% (7388/10000)
[Test]  Epoch: 20	Loss: 0.013398	Acc: 74.0% (7402/10000)
[Test]  Epoch: 21	Loss: 0.013043	Acc: 73.9% (7394/10000)
[Test]  Epoch: 22	Loss: 0.013068	Acc: 74.2% (7425/10000)
[Test]  Epoch: 23	Loss: 0.012974	Acc: 74.1% (7409/10000)
[Test]  Epoch: 24	Loss: 0.013087	Acc: 74.2% (7418/10000)
[Test]  Epoch: 25	Loss: 0.012961	Acc: 74.3% (7430/10000)
[Test]  Epoch: 26	Loss: 0.012797	Acc: 74.3% (7427/10000)
[Test]  Epoch: 27	Loss: 0.012912	Acc: 74.3% (7427/10000)
[Test]  Epoch: 28	Loss: 0.012891	Acc: 74.6% (7463/10000)
[Test]  Epoch: 29	Loss: 0.013095	Acc: 74.2% (7420/10000)
[Test]  Epoch: 30	Loss: 0.012921	Acc: 74.4% (7443/10000)
[Test]  Epoch: 31	Loss: 0.012838	Acc: 74.3% (7433/10000)
[Test]  Epoch: 32	Loss: 0.012771	Acc: 74.6% (7459/10000)
[Test]  Epoch: 33	Loss: 0.012762	Acc: 74.7% (7471/10000)
[Test]  Epoch: 34	Loss: 0.012747	Acc: 74.6% (7459/10000)
[Test]  Epoch: 35	Loss: 0.013125	Acc: 74.1% (7409/10000)
[Test]  Epoch: 36	Loss: 0.012926	Acc: 74.7% (7468/10000)
[Test]  Epoch: 37	Loss: 0.012849	Acc: 74.7% (7466/10000)
[Test]  Epoch: 38	Loss: 0.012825	Acc: 74.5% (7449/10000)
[Test]  Epoch: 39	Loss: 0.012858	Acc: 74.4% (7439/10000)
[Test]  Epoch: 40	Loss: 0.012806	Acc: 74.5% (7445/10000)
[Test]  Epoch: 41	Loss: 0.012769	Acc: 74.8% (7482/10000)
[Test]  Epoch: 42	Loss: 0.012786	Acc: 74.7% (7469/10000)
[Test]  Epoch: 43	Loss: 0.012718	Acc: 75.0% (7502/10000)
[Test]  Epoch: 44	Loss: 0.012732	Acc: 74.7% (7468/10000)
[Test]  Epoch: 45	Loss: 0.012789	Acc: 74.9% (7489/10000)
[Test]  Epoch: 46	Loss: 0.012578	Acc: 74.9% (7486/10000)
[Test]  Epoch: 47	Loss: 0.012610	Acc: 74.7% (7468/10000)
[Test]  Epoch: 48	Loss: 0.012649	Acc: 74.8% (7480/10000)
[Test]  Epoch: 49	Loss: 0.012808	Acc: 74.8% (7481/10000)
[Test]  Epoch: 50	Loss: 0.012801	Acc: 74.7% (7474/10000)
[Test]  Epoch: 51	Loss: 0.012643	Acc: 74.7% (7470/10000)
[Test]  Epoch: 52	Loss: 0.012671	Acc: 74.8% (7475/10000)
[Test]  Epoch: 53	Loss: 0.012730	Acc: 74.8% (7481/10000)
[Test]  Epoch: 54	Loss: 0.012657	Acc: 74.7% (7469/10000)
[Test]  Epoch: 55	Loss: 0.012676	Acc: 74.8% (7480/10000)
[Test]  Epoch: 56	Loss: 0.012648	Acc: 74.5% (7452/10000)
[Test]  Epoch: 57	Loss: 0.012723	Acc: 74.6% (7464/10000)
[Test]  Epoch: 58	Loss: 0.012748	Acc: 74.7% (7466/10000)
[Test]  Epoch: 59	Loss: 0.012709	Acc: 74.6% (7463/10000)
[Test]  Epoch: 60	Loss: 0.012798	Acc: 74.7% (7466/10000)
[Test]  Epoch: 61	Loss: 0.012816	Acc: 74.8% (7483/10000)
[Test]  Epoch: 62	Loss: 0.012810	Acc: 74.7% (7467/10000)
[Test]  Epoch: 63	Loss: 0.012738	Acc: 74.8% (7476/10000)
[Test]  Epoch: 64	Loss: 0.012745	Acc: 74.9% (7491/10000)
[Test]  Epoch: 65	Loss: 0.012691	Acc: 74.9% (7492/10000)
[Test]  Epoch: 66	Loss: 0.012647	Acc: 74.8% (7485/10000)
[Test]  Epoch: 67	Loss: 0.012705	Acc: 74.9% (7486/10000)
[Test]  Epoch: 68	Loss: 0.012702	Acc: 74.9% (7488/10000)
[Test]  Epoch: 69	Loss: 0.012759	Acc: 74.8% (7482/10000)
[Test]  Epoch: 70	Loss: 0.012691	Acc: 74.9% (7487/10000)
[Test]  Epoch: 71	Loss: 0.012670	Acc: 74.8% (7482/10000)
[Test]  Epoch: 72	Loss: 0.012683	Acc: 74.9% (7493/10000)
[Test]  Epoch: 73	Loss: 0.012674	Acc: 74.8% (7481/10000)
[Test]  Epoch: 74	Loss: 0.012627	Acc: 74.9% (7489/10000)
[Test]  Epoch: 75	Loss: 0.012658	Acc: 75.0% (7499/10000)
[Test]  Epoch: 76	Loss: 0.012645	Acc: 74.8% (7483/10000)
[Test]  Epoch: 77	Loss: 0.012644	Acc: 74.8% (7480/10000)
[Test]  Epoch: 78	Loss: 0.012677	Acc: 74.8% (7485/10000)
[Test]  Epoch: 79	Loss: 0.012659	Acc: 74.9% (7493/10000)
[Test]  Epoch: 80	Loss: 0.012649	Acc: 74.8% (7484/10000)
[Test]  Epoch: 81	Loss: 0.012673	Acc: 74.8% (7480/10000)
[Test]  Epoch: 82	Loss: 0.012687	Acc: 74.8% (7478/10000)
[Test]  Epoch: 83	Loss: 0.012666	Acc: 74.9% (7491/10000)
[Test]  Epoch: 84	Loss: 0.012685	Acc: 75.0% (7496/10000)
[Test]  Epoch: 85	Loss: 0.012649	Acc: 74.8% (7482/10000)
[Test]  Epoch: 86	Loss: 0.012621	Acc: 75.1% (7508/10000)
[Test]  Epoch: 87	Loss: 0.012646	Acc: 74.9% (7492/10000)
[Test]  Epoch: 88	Loss: 0.012713	Acc: 74.8% (7479/10000)
[Test]  Epoch: 89	Loss: 0.012671	Acc: 74.8% (7485/10000)
[Test]  Epoch: 90	Loss: 0.012677	Acc: 74.8% (7480/10000)
[Test]  Epoch: 91	Loss: 0.012690	Acc: 74.9% (7491/10000)
[Test]  Epoch: 92	Loss: 0.012699	Acc: 74.9% (7488/10000)
[Test]  Epoch: 93	Loss: 0.012657	Acc: 75.0% (7500/10000)
[Test]  Epoch: 94	Loss: 0.012711	Acc: 74.8% (7481/10000)
[Test]  Epoch: 95	Loss: 0.012656	Acc: 74.9% (7489/10000)
[Test]  Epoch: 96	Loss: 0.012647	Acc: 74.8% (7479/10000)
[Test]  Epoch: 97	Loss: 0.012693	Acc: 74.8% (7479/10000)
[Test]  Epoch: 98	Loss: 0.012756	Acc: 74.9% (7487/10000)
[Test]  Epoch: 99	Loss: 0.012659	Acc: 75.0% (7496/10000)
[Test]  Epoch: 100	Loss: 0.012663	Acc: 74.8% (7483/10000)
===========finish==========
['2024-08-19', '01:25:08.197749', '100', 'test', '0.012662574255466462', '74.83', '75.08']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer2.0.downsample.0.weight', 'conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer2.0.downsample.0.weight', 'conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.031383	Acc: 40.2% (4016/10000)
[Test]  Epoch: 2	Loss: 0.026602	Acc: 48.6% (4859/10000)
[Test]  Epoch: 3	Loss: 0.024452	Acc: 54.7% (5472/10000)
[Test]  Epoch: 4	Loss: 0.024666	Acc: 55.6% (5565/10000)
[Test]  Epoch: 5	Loss: 0.022329	Acc: 59.1% (5909/10000)
[Test]  Epoch: 6	Loss: 0.022490	Acc: 59.2% (5917/10000)
[Test]  Epoch: 7	Loss: 0.020863	Acc: 60.3% (6034/10000)
[Test]  Epoch: 8	Loss: 0.020610	Acc: 59.9% (5994/10000)
[Test]  Epoch: 9	Loss: 0.019964	Acc: 61.6% (6156/10000)
[Test]  Epoch: 10	Loss: 0.020245	Acc: 61.6% (6161/10000)
[Test]  Epoch: 11	Loss: 0.019689	Acc: 62.1% (6207/10000)
[Test]  Epoch: 12	Loss: 0.020612	Acc: 60.6% (6056/10000)
[Test]  Epoch: 13	Loss: 0.020221	Acc: 61.8% (6182/10000)
[Test]  Epoch: 14	Loss: 0.019716	Acc: 61.3% (6133/10000)
[Test]  Epoch: 15	Loss: 0.019466	Acc: 62.2% (6216/10000)
[Test]  Epoch: 16	Loss: 0.019098	Acc: 62.3% (6229/10000)
[Test]  Epoch: 17	Loss: 0.019186	Acc: 62.2% (6219/10000)
[Test]  Epoch: 18	Loss: 0.019153	Acc: 62.2% (6223/10000)
[Test]  Epoch: 19	Loss: 0.019056	Acc: 62.5% (6253/10000)
[Test]  Epoch: 20	Loss: 0.018911	Acc: 62.9% (6294/10000)
[Test]  Epoch: 21	Loss: 0.018921	Acc: 62.8% (6280/10000)
[Test]  Epoch: 22	Loss: 0.018913	Acc: 62.9% (6288/10000)
[Test]  Epoch: 23	Loss: 0.019026	Acc: 62.7% (6271/10000)
[Test]  Epoch: 24	Loss: 0.018794	Acc: 63.1% (6311/10000)
[Test]  Epoch: 25	Loss: 0.019429	Acc: 62.6% (6261/10000)
[Test]  Epoch: 26	Loss: 0.019047	Acc: 63.3% (6329/10000)
[Test]  Epoch: 27	Loss: 0.018977	Acc: 63.1% (6312/10000)
[Test]  Epoch: 28	Loss: 0.018953	Acc: 63.2% (6325/10000)
[Test]  Epoch: 29	Loss: 0.018811	Acc: 63.6% (6361/10000)
[Test]  Epoch: 30	Loss: 0.018679	Acc: 63.9% (6388/10000)
[Test]  Epoch: 31	Loss: 0.018641	Acc: 63.9% (6392/10000)
[Test]  Epoch: 32	Loss: 0.019232	Acc: 62.9% (6287/10000)
[Test]  Epoch: 33	Loss: 0.018740	Acc: 63.3% (6334/10000)
[Test]  Epoch: 34	Loss: 0.018639	Acc: 63.6% (6360/10000)
[Test]  Epoch: 35	Loss: 0.018967	Acc: 63.5% (6349/10000)
[Test]  Epoch: 36	Loss: 0.018869	Acc: 63.6% (6362/10000)
[Test]  Epoch: 37	Loss: 0.018622	Acc: 63.5% (6345/10000)
[Test]  Epoch: 38	Loss: 0.021762	Acc: 59.2% (5918/10000)
[Test]  Epoch: 39	Loss: 0.020680	Acc: 61.2% (6119/10000)
[Test]  Epoch: 40	Loss: 0.019279	Acc: 62.9% (6288/10000)
[Test]  Epoch: 41	Loss: 0.019046	Acc: 63.2% (6322/10000)
[Test]  Epoch: 42	Loss: 0.019160	Acc: 63.0% (6300/10000)
[Test]  Epoch: 43	Loss: 0.019174	Acc: 63.1% (6314/10000)
[Test]  Epoch: 44	Loss: 0.018813	Acc: 63.4% (6344/10000)
[Test]  Epoch: 45	Loss: 0.019046	Acc: 62.9% (6286/10000)
[Test]  Epoch: 46	Loss: 0.018559	Acc: 63.9% (6386/10000)
[Test]  Epoch: 47	Loss: 0.018596	Acc: 63.3% (6327/10000)
[Test]  Epoch: 48	Loss: 0.018688	Acc: 63.8% (6384/10000)
[Test]  Epoch: 49	Loss: 0.018901	Acc: 63.4% (6336/10000)
[Test]  Epoch: 50	Loss: 0.018998	Acc: 63.5% (6353/10000)
[Test]  Epoch: 51	Loss: 0.018514	Acc: 63.8% (6375/10000)
[Test]  Epoch: 52	Loss: 0.018690	Acc: 64.2% (6416/10000)
[Test]  Epoch: 53	Loss: 0.018733	Acc: 64.0% (6404/10000)
[Test]  Epoch: 54	Loss: 0.018532	Acc: 64.0% (6403/10000)
[Test]  Epoch: 55	Loss: 0.018518	Acc: 64.3% (6428/10000)
[Test]  Epoch: 56	Loss: 0.018693	Acc: 64.1% (6407/10000)
[Test]  Epoch: 57	Loss: 0.018362	Acc: 64.2% (6418/10000)
[Test]  Epoch: 58	Loss: 0.018370	Acc: 64.2% (6425/10000)
[Test]  Epoch: 59	Loss: 0.018606	Acc: 63.9% (6394/10000)
[Test]  Epoch: 60	Loss: 0.018709	Acc: 64.0% (6398/10000)
[Test]  Epoch: 61	Loss: 0.018574	Acc: 64.4% (6436/10000)
[Test]  Epoch: 62	Loss: 0.018637	Acc: 64.2% (6423/10000)
[Test]  Epoch: 63	Loss: 0.018525	Acc: 64.1% (6411/10000)
[Test]  Epoch: 64	Loss: 0.018612	Acc: 64.3% (6432/10000)
[Test]  Epoch: 65	Loss: 0.018480	Acc: 64.1% (6410/10000)
[Test]  Epoch: 66	Loss: 0.018453	Acc: 64.4% (6436/10000)
[Test]  Epoch: 67	Loss: 0.018492	Acc: 64.2% (6422/10000)
[Test]  Epoch: 68	Loss: 0.018406	Acc: 64.5% (6448/10000)
[Test]  Epoch: 69	Loss: 0.018525	Acc: 64.3% (6427/10000)
[Test]  Epoch: 70	Loss: 0.018483	Acc: 64.3% (6427/10000)
[Test]  Epoch: 71	Loss: 0.018430	Acc: 64.0% (6397/10000)
[Test]  Epoch: 72	Loss: 0.018396	Acc: 64.4% (6436/10000)
[Test]  Epoch: 73	Loss: 0.018461	Acc: 64.3% (6430/10000)
[Test]  Epoch: 74	Loss: 0.018434	Acc: 64.3% (6433/10000)
[Test]  Epoch: 75	Loss: 0.018368	Acc: 64.5% (6451/10000)
[Test]  Epoch: 76	Loss: 0.018402	Acc: 64.3% (6429/10000)
[Test]  Epoch: 77	Loss: 0.018360	Acc: 64.4% (6438/10000)
[Test]  Epoch: 78	Loss: 0.018395	Acc: 64.3% (6430/10000)
[Test]  Epoch: 79	Loss: 0.018418	Acc: 64.5% (6455/10000)
[Test]  Epoch: 80	Loss: 0.018475	Acc: 64.4% (6440/10000)
[Test]  Epoch: 81	Loss: 0.018418	Acc: 64.3% (6426/10000)
[Test]  Epoch: 82	Loss: 0.018457	Acc: 64.3% (6432/10000)
[Test]  Epoch: 83	Loss: 0.018377	Acc: 64.5% (6453/10000)
[Test]  Epoch: 84	Loss: 0.018413	Acc: 64.5% (6452/10000)
[Test]  Epoch: 85	Loss: 0.018430	Acc: 64.4% (6438/10000)
[Test]  Epoch: 86	Loss: 0.018440	Acc: 64.5% (6449/10000)
[Test]  Epoch: 87	Loss: 0.018377	Acc: 64.4% (6442/10000)
[Test]  Epoch: 88	Loss: 0.018445	Acc: 64.5% (6451/10000)
[Test]  Epoch: 89	Loss: 0.018464	Acc: 64.3% (6429/10000)
[Test]  Epoch: 90	Loss: 0.018556	Acc: 64.5% (6448/10000)
[Test]  Epoch: 91	Loss: 0.018528	Acc: 64.3% (6435/10000)
[Test]  Epoch: 92	Loss: 0.018463	Acc: 64.5% (6453/10000)
[Test]  Epoch: 93	Loss: 0.018400	Acc: 64.4% (6442/10000)
[Test]  Epoch: 94	Loss: 0.018438	Acc: 64.6% (6460/10000)
[Test]  Epoch: 95	Loss: 0.018431	Acc: 64.4% (6443/10000)
[Test]  Epoch: 96	Loss: 0.018456	Acc: 64.4% (6438/10000)
[Test]  Epoch: 97	Loss: 0.018459	Acc: 64.5% (6453/10000)
[Test]  Epoch: 98	Loss: 0.018621	Acc: 64.4% (6436/10000)
[Test]  Epoch: 99	Loss: 0.018449	Acc: 64.5% (6448/10000)
[Test]  Epoch: 100	Loss: 0.018438	Acc: 64.6% (6463/10000)
===========finish==========
['2024-08-19', '01:27:07.213846', '100', 'test', '0.018438396179676055', '64.63', '64.63']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer2.0.downsample.0.weight', 'conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer4.1.conv2.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer2.0.downsample.0.weight', 'conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer4.1.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.048197	Acc: 36.8% (3682/10000)
[Test]  Epoch: 2	Loss: 0.027240	Acc: 49.1% (4910/10000)
[Test]  Epoch: 3	Loss: 0.023522	Acc: 54.5% (5446/10000)
[Test]  Epoch: 4	Loss: 0.023706	Acc: 55.2% (5516/10000)
[Test]  Epoch: 5	Loss: 0.023153	Acc: 56.1% (5607/10000)
[Test]  Epoch: 6	Loss: 0.022714	Acc: 57.9% (5785/10000)
[Test]  Epoch: 7	Loss: 0.021436	Acc: 58.2% (5817/10000)
[Test]  Epoch: 8	Loss: 0.021390	Acc: 58.4% (5842/10000)
[Test]  Epoch: 9	Loss: 0.021293	Acc: 58.9% (5890/10000)
[Test]  Epoch: 10	Loss: 0.021417	Acc: 58.7% (5874/10000)
[Test]  Epoch: 11	Loss: 0.021422	Acc: 59.1% (5914/10000)
[Test]  Epoch: 12	Loss: 0.022623	Acc: 56.4% (5640/10000)
[Test]  Epoch: 13	Loss: 0.020720	Acc: 59.4% (5939/10000)
[Test]  Epoch: 14	Loss: 0.020342	Acc: 59.7% (5966/10000)
[Test]  Epoch: 15	Loss: 0.020639	Acc: 59.8% (5984/10000)
[Test]  Epoch: 16	Loss: 0.020354	Acc: 60.0% (6003/10000)
[Test]  Epoch: 17	Loss: 0.020190	Acc: 60.5% (6054/10000)
[Test]  Epoch: 18	Loss: 0.020496	Acc: 59.7% (5973/10000)
[Test]  Epoch: 19	Loss: 0.020239	Acc: 60.2% (6018/10000)
[Test]  Epoch: 20	Loss: 0.020178	Acc: 60.7% (6069/10000)
[Test]  Epoch: 21	Loss: 0.020138	Acc: 61.0% (6096/10000)
[Test]  Epoch: 22	Loss: 0.020428	Acc: 60.3% (6031/10000)
[Test]  Epoch: 23	Loss: 0.020279	Acc: 60.0% (6000/10000)
[Test]  Epoch: 24	Loss: 0.020308	Acc: 60.4% (6043/10000)
[Test]  Epoch: 25	Loss: 0.020466	Acc: 60.8% (6081/10000)
[Test]  Epoch: 26	Loss: 0.020158	Acc: 60.8% (6077/10000)
[Test]  Epoch: 27	Loss: 0.020499	Acc: 60.5% (6048/10000)
[Test]  Epoch: 28	Loss: 0.020563	Acc: 60.7% (6074/10000)
[Test]  Epoch: 29	Loss: 0.020697	Acc: 60.3% (6026/10000)
[Test]  Epoch: 30	Loss: 0.020322	Acc: 60.6% (6058/10000)
[Test]  Epoch: 31	Loss: 0.020334	Acc: 60.9% (6089/10000)
[Test]  Epoch: 32	Loss: 0.020534	Acc: 60.8% (6075/10000)
[Test]  Epoch: 33	Loss: 0.020240	Acc: 61.1% (6108/10000)
[Test]  Epoch: 34	Loss: 0.020382	Acc: 60.8% (6084/10000)
[Test]  Epoch: 35	Loss: 0.020742	Acc: 60.6% (6056/10000)
[Test]  Epoch: 36	Loss: 0.020920	Acc: 60.3% (6034/10000)
[Test]  Epoch: 37	Loss: 0.020389	Acc: 60.7% (6068/10000)
[Test]  Epoch: 38	Loss: 0.021387	Acc: 59.8% (5979/10000)
[Test]  Epoch: 39	Loss: 0.021306	Acc: 59.5% (5951/10000)
[Test]  Epoch: 40	Loss: 0.021272	Acc: 59.7% (5971/10000)
[Test]  Epoch: 41	Loss: 0.020638	Acc: 60.5% (6047/10000)
[Test]  Epoch: 42	Loss: 0.020567	Acc: 60.5% (6047/10000)
[Test]  Epoch: 43	Loss: 0.020585	Acc: 60.7% (6073/10000)
[Test]  Epoch: 44	Loss: 0.020590	Acc: 60.5% (6052/10000)
[Test]  Epoch: 45	Loss: 0.020484	Acc: 60.7% (6067/10000)
[Test]  Epoch: 46	Loss: 0.020489	Acc: 60.7% (6066/10000)
[Test]  Epoch: 47	Loss: 0.020388	Acc: 60.5% (6052/10000)
[Test]  Epoch: 48	Loss: 0.020325	Acc: 61.0% (6100/10000)
[Test]  Epoch: 49	Loss: 0.020632	Acc: 60.7% (6066/10000)
[Test]  Epoch: 50	Loss: 0.020571	Acc: 60.6% (6062/10000)
[Test]  Epoch: 51	Loss: 0.020477	Acc: 60.6% (6056/10000)
[Test]  Epoch: 52	Loss: 0.020424	Acc: 60.8% (6084/10000)
[Test]  Epoch: 53	Loss: 0.020580	Acc: 60.9% (6088/10000)
[Test]  Epoch: 54	Loss: 0.020488	Acc: 60.9% (6086/10000)
[Test]  Epoch: 55	Loss: 0.020471	Acc: 60.8% (6084/10000)
[Test]  Epoch: 56	Loss: 0.020563	Acc: 60.7% (6074/10000)
[Test]  Epoch: 57	Loss: 0.020306	Acc: 61.1% (6114/10000)
[Test]  Epoch: 58	Loss: 0.020676	Acc: 60.5% (6046/10000)
[Test]  Epoch: 59	Loss: 0.020688	Acc: 60.8% (6083/10000)
[Test]  Epoch: 60	Loss: 0.020689	Acc: 60.8% (6075/10000)
[Test]  Epoch: 61	Loss: 0.020740	Acc: 60.9% (6088/10000)
[Test]  Epoch: 62	Loss: 0.020681	Acc: 61.1% (6106/10000)
[Test]  Epoch: 63	Loss: 0.020680	Acc: 60.9% (6093/10000)
[Test]  Epoch: 64	Loss: 0.020669	Acc: 61.0% (6098/10000)
[Test]  Epoch: 65	Loss: 0.020545	Acc: 61.0% (6097/10000)
[Test]  Epoch: 66	Loss: 0.020565	Acc: 60.9% (6092/10000)
[Test]  Epoch: 67	Loss: 0.020637	Acc: 60.7% (6074/10000)
[Test]  Epoch: 68	Loss: 0.020555	Acc: 61.2% (6117/10000)
[Test]  Epoch: 69	Loss: 0.020610	Acc: 61.0% (6097/10000)
[Test]  Epoch: 70	Loss: 0.020605	Acc: 61.2% (6116/10000)
[Test]  Epoch: 71	Loss: 0.020583	Acc: 60.8% (6079/10000)
[Test]  Epoch: 72	Loss: 0.020596	Acc: 61.0% (6100/10000)
[Test]  Epoch: 73	Loss: 0.020568	Acc: 61.0% (6101/10000)
[Test]  Epoch: 74	Loss: 0.020541	Acc: 60.8% (6081/10000)
[Test]  Epoch: 75	Loss: 0.020508	Acc: 60.9% (6091/10000)
[Test]  Epoch: 76	Loss: 0.020488	Acc: 61.0% (6100/10000)
[Test]  Epoch: 77	Loss: 0.020543	Acc: 61.0% (6095/10000)
[Test]  Epoch: 78	Loss: 0.020520	Acc: 61.1% (6110/10000)
[Test]  Epoch: 79	Loss: 0.020466	Acc: 61.1% (6112/10000)
[Test]  Epoch: 80	Loss: 0.020492	Acc: 61.1% (6109/10000)
[Test]  Epoch: 81	Loss: 0.020607	Acc: 60.8% (6083/10000)
[Test]  Epoch: 82	Loss: 0.020576	Acc: 60.8% (6079/10000)
[Test]  Epoch: 83	Loss: 0.020496	Acc: 61.2% (6122/10000)
[Test]  Epoch: 84	Loss: 0.020568	Acc: 61.3% (6129/10000)
[Test]  Epoch: 85	Loss: 0.020515	Acc: 61.0% (6102/10000)
[Test]  Epoch: 86	Loss: 0.020515	Acc: 61.2% (6120/10000)
[Test]  Epoch: 87	Loss: 0.020560	Acc: 61.1% (6113/10000)
[Test]  Epoch: 88	Loss: 0.020525	Acc: 61.1% (6106/10000)
[Test]  Epoch: 89	Loss: 0.020602	Acc: 61.0% (6096/10000)
[Test]  Epoch: 90	Loss: 0.020547	Acc: 61.2% (6121/10000)
[Test]  Epoch: 91	Loss: 0.020591	Acc: 61.0% (6104/10000)
[Test]  Epoch: 92	Loss: 0.020595	Acc: 60.9% (6091/10000)
[Test]  Epoch: 93	Loss: 0.020526	Acc: 61.1% (6110/10000)
[Test]  Epoch: 94	Loss: 0.020516	Acc: 61.1% (6112/10000)
[Test]  Epoch: 95	Loss: 0.020586	Acc: 61.1% (6111/10000)
[Test]  Epoch: 96	Loss: 0.020540	Acc: 61.0% (6095/10000)
[Test]  Epoch: 97	Loss: 0.020587	Acc: 61.1% (6106/10000)
[Test]  Epoch: 98	Loss: 0.020627	Acc: 60.9% (6091/10000)
[Test]  Epoch: 99	Loss: 0.020574	Acc: 60.9% (6089/10000)
[Test]  Epoch: 100	Loss: 0.020584	Acc: 60.9% (6091/10000)
===========finish==========
['2024-08-19', '01:29:13.250776', '100', 'test', '0.0205839200258255', '60.91', '61.29']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer2.0.downsample.0.weight', 'conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv2.weight', 'layer2.1.conv2.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer2.0.downsample.0.weight', 'conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv2.weight', 'layer2.1.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.043524	Acc: 27.8% (2779/10000)
[Test]  Epoch: 2	Loss: 0.027136	Acc: 44.3% (4430/10000)
[Test]  Epoch: 3	Loss: 0.025524	Acc: 50.6% (5061/10000)
[Test]  Epoch: 4	Loss: 0.027304	Acc: 48.4% (4840/10000)
[Test]  Epoch: 5	Loss: 0.025666	Acc: 50.6% (5057/10000)
[Test]  Epoch: 6	Loss: 0.026866	Acc: 50.4% (5038/10000)
[Test]  Epoch: 7	Loss: 0.025262	Acc: 51.6% (5161/10000)
[Test]  Epoch: 8	Loss: 0.025451	Acc: 52.4% (5244/10000)
[Test]  Epoch: 9	Loss: 0.025896	Acc: 51.6% (5156/10000)
[Test]  Epoch: 10	Loss: 0.025354	Acc: 53.4% (5337/10000)
[Test]  Epoch: 11	Loss: 0.025237	Acc: 52.4% (5238/10000)
[Test]  Epoch: 12	Loss: 0.025346	Acc: 52.3% (5227/10000)
[Test]  Epoch: 13	Loss: 0.025312	Acc: 52.6% (5262/10000)
[Test]  Epoch: 14	Loss: 0.025237	Acc: 53.1% (5309/10000)
[Test]  Epoch: 15	Loss: 0.025505	Acc: 52.6% (5259/10000)
[Test]  Epoch: 16	Loss: 0.025233	Acc: 53.2% (5319/10000)
[Test]  Epoch: 17	Loss: 0.025352	Acc: 52.8% (5278/10000)
[Test]  Epoch: 18	Loss: 0.025224	Acc: 53.2% (5320/10000)
[Test]  Epoch: 19	Loss: 0.025178	Acc: 53.1% (5309/10000)
[Test]  Epoch: 20	Loss: 0.025168	Acc: 53.4% (5339/10000)
[Test]  Epoch: 21	Loss: 0.025273	Acc: 52.9% (5292/10000)
[Test]  Epoch: 22	Loss: 0.025273	Acc: 53.0% (5300/10000)
[Test]  Epoch: 23	Loss: 0.025043	Acc: 53.1% (5314/10000)
[Test]  Epoch: 24	Loss: 0.025262	Acc: 53.1% (5314/10000)
[Test]  Epoch: 25	Loss: 0.025426	Acc: 53.1% (5313/10000)
[Test]  Epoch: 26	Loss: 0.025083	Acc: 53.4% (5341/10000)
[Test]  Epoch: 27	Loss: 0.025244	Acc: 53.3% (5334/10000)
[Test]  Epoch: 28	Loss: 0.025375	Acc: 53.2% (5318/10000)
[Test]  Epoch: 29	Loss: 0.025490	Acc: 53.2% (5321/10000)
[Test]  Epoch: 30	Loss: 0.025342	Acc: 53.5% (5353/10000)
[Test]  Epoch: 31	Loss: 0.025281	Acc: 53.4% (5337/10000)
[Test]  Epoch: 32	Loss: 0.025542	Acc: 53.5% (5353/10000)
[Test]  Epoch: 33	Loss: 0.025537	Acc: 53.4% (5340/10000)
[Test]  Epoch: 34	Loss: 0.025462	Acc: 53.4% (5338/10000)
[Test]  Epoch: 35	Loss: 0.025193	Acc: 53.3% (5329/10000)
[Test]  Epoch: 36	Loss: 0.025332	Acc: 53.2% (5322/10000)
[Test]  Epoch: 37	Loss: 0.025247	Acc: 53.5% (5348/10000)
[Test]  Epoch: 38	Loss: 0.025902	Acc: 52.9% (5287/10000)
[Test]  Epoch: 39	Loss: 0.025674	Acc: 53.1% (5307/10000)
[Test]  Epoch: 40	Loss: 0.025481	Acc: 53.6% (5361/10000)
[Test]  Epoch: 41	Loss: 0.025330	Acc: 53.2% (5324/10000)
[Test]  Epoch: 42	Loss: 0.025421	Acc: 53.4% (5336/10000)
[Test]  Epoch: 43	Loss: 0.025658	Acc: 53.2% (5317/10000)
[Test]  Epoch: 44	Loss: 0.025658	Acc: 53.1% (5313/10000)
[Test]  Epoch: 45	Loss: 0.025582	Acc: 53.1% (5307/10000)
[Test]  Epoch: 46	Loss: 0.025508	Acc: 53.5% (5355/10000)
[Test]  Epoch: 47	Loss: 0.025380	Acc: 53.5% (5349/10000)
[Test]  Epoch: 48	Loss: 0.025581	Acc: 53.2% (5322/10000)
[Test]  Epoch: 49	Loss: 0.025671	Acc: 53.1% (5306/10000)
[Test]  Epoch: 50	Loss: 0.025887	Acc: 53.2% (5321/10000)
[Test]  Epoch: 51	Loss: 0.025571	Acc: 53.4% (5341/10000)
[Test]  Epoch: 52	Loss: 0.025483	Acc: 53.3% (5333/10000)
[Test]  Epoch: 53	Loss: 0.025732	Acc: 53.5% (5345/10000)
[Test]  Epoch: 54	Loss: 0.025463	Acc: 53.2% (5324/10000)
[Test]  Epoch: 55	Loss: 0.025493	Acc: 53.4% (5344/10000)
[Test]  Epoch: 56	Loss: 0.025565	Acc: 53.3% (5329/10000)
[Test]  Epoch: 57	Loss: 0.025428	Acc: 53.5% (5352/10000)
[Test]  Epoch: 58	Loss: 0.025605	Acc: 53.4% (5339/10000)
[Test]  Epoch: 59	Loss: 0.025578	Acc: 53.4% (5340/10000)
[Test]  Epoch: 60	Loss: 0.025767	Acc: 53.0% (5301/10000)
[Test]  Epoch: 61	Loss: 0.025685	Acc: 53.1% (5310/10000)
[Test]  Epoch: 62	Loss: 0.025535	Acc: 53.4% (5342/10000)
[Test]  Epoch: 63	Loss: 0.025497	Acc: 53.2% (5318/10000)
[Test]  Epoch: 64	Loss: 0.025535	Acc: 53.4% (5339/10000)
[Test]  Epoch: 65	Loss: 0.025498	Acc: 53.5% (5353/10000)
[Test]  Epoch: 66	Loss: 0.025481	Acc: 53.3% (5332/10000)
[Test]  Epoch: 67	Loss: 0.025597	Acc: 52.9% (5292/10000)
[Test]  Epoch: 68	Loss: 0.025570	Acc: 53.4% (5336/10000)
[Test]  Epoch: 69	Loss: 0.025630	Acc: 53.3% (5334/10000)
[Test]  Epoch: 70	Loss: 0.025558	Acc: 53.4% (5336/10000)
[Test]  Epoch: 71	Loss: 0.025504	Acc: 53.3% (5332/10000)
[Test]  Epoch: 72	Loss: 0.025621	Acc: 53.4% (5337/10000)
[Test]  Epoch: 73	Loss: 0.025600	Acc: 53.2% (5318/10000)
[Test]  Epoch: 74	Loss: 0.025488	Acc: 53.3% (5332/10000)
[Test]  Epoch: 75	Loss: 0.025523	Acc: 53.4% (5336/10000)
[Test]  Epoch: 76	Loss: 0.025480	Acc: 53.2% (5322/10000)
[Test]  Epoch: 77	Loss: 0.025525	Acc: 53.4% (5341/10000)
[Test]  Epoch: 78	Loss: 0.025479	Acc: 53.4% (5340/10000)
[Test]  Epoch: 79	Loss: 0.025428	Acc: 53.4% (5341/10000)
[Test]  Epoch: 80	Loss: 0.025484	Acc: 53.3% (5333/10000)
[Test]  Epoch: 81	Loss: 0.025558	Acc: 53.5% (5348/10000)
[Test]  Epoch: 82	Loss: 0.025531	Acc: 53.3% (5334/10000)
[Test]  Epoch: 83	Loss: 0.025541	Acc: 53.4% (5337/10000)
[Test]  Epoch: 84	Loss: 0.025486	Acc: 53.3% (5332/10000)
[Test]  Epoch: 85	Loss: 0.025499	Acc: 53.2% (5325/10000)
[Test]  Epoch: 86	Loss: 0.025530	Acc: 53.4% (5344/10000)
[Test]  Epoch: 87	Loss: 0.025496	Acc: 53.4% (5342/10000)
[Test]  Epoch: 88	Loss: 0.025473	Acc: 53.4% (5337/10000)
[Test]  Epoch: 89	Loss: 0.025574	Acc: 53.3% (5334/10000)
[Test]  Epoch: 90	Loss: 0.025506	Acc: 53.4% (5337/10000)
[Test]  Epoch: 91	Loss: 0.025505	Acc: 53.4% (5336/10000)
[Test]  Epoch: 92	Loss: 0.025479	Acc: 53.2% (5321/10000)
[Test]  Epoch: 93	Loss: 0.025482	Acc: 53.3% (5331/10000)
[Test]  Epoch: 94	Loss: 0.025515	Acc: 53.4% (5341/10000)
[Test]  Epoch: 95	Loss: 0.025494	Acc: 53.3% (5331/10000)
[Test]  Epoch: 96	Loss: 0.025468	Acc: 53.4% (5336/10000)
[Test]  Epoch: 97	Loss: 0.025578	Acc: 53.4% (5335/10000)
[Test]  Epoch: 98	Loss: 0.025661	Acc: 53.2% (5318/10000)
[Test]  Epoch: 99	Loss: 0.025556	Acc: 53.4% (5337/10000)
[Test]  Epoch: 100	Loss: 0.025473	Acc: 53.5% (5345/10000)
===========finish==========
['2024-08-19', '01:31:16.067411', '100', 'test', '0.025472702705860138', '53.45', '53.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer2.0.downsample.0.weight', 'conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv2.weight', 'layer2.1.conv2.weight', 'layer4.0.conv1.weight', 'layer2.1.conv1.weight', 'layer2.0.conv2.weight', 'layer3.0.conv2.weight', 'layer3.0.conv1.weight']
['layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer4.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.downsample.1.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'last_linear.weight', 'layer3.1.conv1.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer2.0.downsample.0.weight', 'conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv2.weight', 'layer2.1.conv2.weight', 'layer4.0.conv1.weight', 'layer2.1.conv1.weight', 'layer2.0.conv2.weight', 'layer3.0.conv2.weight', 'layer3.0.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039597	Acc: 30.0% (2996/10000)
[Test]  Epoch: 2	Loss: 0.024274	Acc: 47.2% (4722/10000)
[Test]  Epoch: 3	Loss: 0.023406	Acc: 50.7% (5071/10000)
[Test]  Epoch: 4	Loss: 0.023834	Acc: 50.5% (5045/10000)
[Test]  Epoch: 5	Loss: 0.024021	Acc: 51.4% (5143/10000)
[Test]  Epoch: 6	Loss: 0.024341	Acc: 51.5% (5148/10000)
[Test]  Epoch: 7	Loss: 0.024273	Acc: 51.7% (5167/10000)
[Test]  Epoch: 8	Loss: 0.024463	Acc: 52.2% (5224/10000)
[Test]  Epoch: 9	Loss: 0.024171	Acc: 51.9% (5188/10000)
[Test]  Epoch: 10	Loss: 0.024311	Acc: 52.2% (5216/10000)
[Test]  Epoch: 11	Loss: 0.024092	Acc: 52.6% (5265/10000)
[Test]  Epoch: 12	Loss: 0.024124	Acc: 52.5% (5245/10000)
[Test]  Epoch: 13	Loss: 0.024161	Acc: 52.7% (5274/10000)
[Test]  Epoch: 14	Loss: 0.024116	Acc: 52.7% (5267/10000)
[Test]  Epoch: 15	Loss: 0.024217	Acc: 52.6% (5263/10000)
[Test]  Epoch: 16	Loss: 0.024267	Acc: 52.7% (5268/10000)
[Test]  Epoch: 17	Loss: 0.024178	Acc: 53.1% (5310/10000)
[Test]  Epoch: 18	Loss: 0.024146	Acc: 52.8% (5283/10000)
[Test]  Epoch: 19	Loss: 0.024087	Acc: 53.2% (5318/10000)
[Test]  Epoch: 20	Loss: 0.023889	Acc: 53.7% (5374/10000)
[Test]  Epoch: 21	Loss: 0.023859	Acc: 53.6% (5358/10000)
[Test]  Epoch: 22	Loss: 0.023741	Acc: 53.8% (5376/10000)
[Test]  Epoch: 23	Loss: 0.023688	Acc: 54.0% (5395/10000)
[Test]  Epoch: 24	Loss: 0.023801	Acc: 53.9% (5385/10000)
[Test]  Epoch: 25	Loss: 0.023833	Acc: 53.6% (5360/10000)
[Test]  Epoch: 26	Loss: 0.023748	Acc: 53.9% (5394/10000)
[Test]  Epoch: 27	Loss: 0.023643	Acc: 53.8% (5381/10000)
[Test]  Epoch: 28	Loss: 0.023797	Acc: 54.0% (5397/10000)
[Test]  Epoch: 29	Loss: 0.023892	Acc: 53.9% (5390/10000)
[Test]  Epoch: 30	Loss: 0.023689	Acc: 54.4% (5439/10000)
[Test]  Epoch: 31	Loss: 0.023429	Acc: 54.6% (5457/10000)
[Test]  Epoch: 32	Loss: 0.023559	Acc: 54.2% (5421/10000)
[Test]  Epoch: 33	Loss: 0.023550	Acc: 54.1% (5412/10000)
[Test]  Epoch: 34	Loss: 0.023849	Acc: 54.2% (5423/10000)
[Test]  Epoch: 35	Loss: 0.023706	Acc: 54.2% (5416/10000)
[Test]  Epoch: 36	Loss: 0.023506	Acc: 54.4% (5436/10000)
[Test]  Epoch: 37	Loss: 0.023678	Acc: 54.1% (5407/10000)
[Test]  Epoch: 38	Loss: 0.023700	Acc: 54.2% (5420/10000)
[Test]  Epoch: 39	Loss: 0.023631	Acc: 54.0% (5403/10000)
[Test]  Epoch: 40	Loss: 0.023646	Acc: 54.0% (5398/10000)
[Test]  Epoch: 41	Loss: 0.023445	Acc: 54.4% (5437/10000)
[Test]  Epoch: 42	Loss: 0.023499	Acc: 54.2% (5417/10000)
[Test]  Epoch: 43	Loss: 0.023530	Acc: 54.3% (5426/10000)
[Test]  Epoch: 44	Loss: 0.023455	Acc: 54.4% (5444/10000)
[Test]  Epoch: 45	Loss: 0.023602	Acc: 54.3% (5431/10000)
[Test]  Epoch: 46	Loss: 0.023416	Acc: 54.7% (5472/10000)
[Test]  Epoch: 47	Loss: 0.023580	Acc: 54.3% (5434/10000)
[Test]  Epoch: 48	Loss: 0.023603	Acc: 54.5% (5451/10000)
[Test]  Epoch: 49	Loss: 0.023618	Acc: 54.6% (5460/10000)
[Test]  Epoch: 50	Loss: 0.023569	Acc: 54.6% (5464/10000)
[Test]  Epoch: 51	Loss: 0.023451	Acc: 54.4% (5436/10000)
[Test]  Epoch: 52	Loss: 0.023556	Acc: 54.5% (5450/10000)
[Test]  Epoch: 53	Loss: 0.023490	Acc: 54.6% (5460/10000)
[Test]  Epoch: 54	Loss: 0.023508	Acc: 54.5% (5455/10000)
[Test]  Epoch: 55	Loss: 0.023354	Acc: 54.5% (5448/10000)
[Test]  Epoch: 56	Loss: 0.023446	Acc: 54.3% (5433/10000)
[Test]  Epoch: 57	Loss: 0.023262	Acc: 54.9% (5485/10000)
[Test]  Epoch: 58	Loss: 0.023349	Acc: 54.6% (5458/10000)
[Test]  Epoch: 59	Loss: 0.023414	Acc: 54.6% (5459/10000)
[Test]  Epoch: 60	Loss: 0.023467	Acc: 54.7% (5472/10000)
[Test]  Epoch: 61	Loss: 0.023369	Acc: 54.8% (5478/10000)
[Test]  Epoch: 62	Loss: 0.023336	Acc: 54.8% (5476/10000)
[Test]  Epoch: 63	Loss: 0.023363	Acc: 54.8% (5481/10000)
[Test]  Epoch: 64	Loss: 0.023434	Acc: 54.6% (5456/10000)
[Test]  Epoch: 65	Loss: 0.023363	Acc: 54.6% (5461/10000)
[Test]  Epoch: 66	Loss: 0.023283	Acc: 54.8% (5478/10000)
[Test]  Epoch: 67	Loss: 0.023337	Acc: 54.7% (5473/10000)
[Test]  Epoch: 68	Loss: 0.023274	Acc: 55.0% (5500/10000)
[Test]  Epoch: 69	Loss: 0.023342	Acc: 54.7% (5474/10000)
[Test]  Epoch: 70	Loss: 0.023287	Acc: 54.9% (5487/10000)
[Test]  Epoch: 71	Loss: 0.023321	Acc: 54.8% (5475/10000)
[Test]  Epoch: 72	Loss: 0.023348	Acc: 54.8% (5479/10000)
[Test]  Epoch: 73	Loss: 0.023286	Acc: 54.7% (5471/10000)
[Test]  Epoch: 74	Loss: 0.023206	Acc: 54.9% (5491/10000)
[Test]  Epoch: 75	Loss: 0.023207	Acc: 54.9% (5488/10000)
[Test]  Epoch: 76	Loss: 0.023205	Acc: 54.9% (5493/10000)
[Test]  Epoch: 77	Loss: 0.023238	Acc: 55.0% (5495/10000)
[Test]  Epoch: 78	Loss: 0.023263	Acc: 54.9% (5494/10000)
[Test]  Epoch: 79	Loss: 0.023275	Acc: 54.9% (5494/10000)
[Test]  Epoch: 80	Loss: 0.023260	Acc: 54.7% (5469/10000)
[Test]  Epoch: 81	Loss: 0.023295	Acc: 54.9% (5490/10000)
[Test]  Epoch: 82	Loss: 0.023283	Acc: 55.1% (5508/10000)
[Test]  Epoch: 83	Loss: 0.023279	Acc: 54.8% (5479/10000)
[Test]  Epoch: 84	Loss: 0.023255	Acc: 54.8% (5479/10000)
[Test]  Epoch: 85	Loss: 0.023258	Acc: 55.0% (5505/10000)
[Test]  Epoch: 86	Loss: 0.023266	Acc: 55.0% (5495/10000)
[Test]  Epoch: 87	Loss: 0.023269	Acc: 54.8% (5476/10000)
[Test]  Epoch: 88	Loss: 0.023318	Acc: 54.9% (5492/10000)
[Test]  Epoch: 89	Loss: 0.023279	Acc: 54.9% (5490/10000)
[Test]  Epoch: 90	Loss: 0.023302	Acc: 54.7% (5474/10000)
[Test]  Epoch: 91	Loss: 0.023346	Acc: 54.8% (5477/10000)
[Test]  Epoch: 92	Loss: 0.023247	Acc: 54.9% (5485/10000)
[Test]  Epoch: 93	Loss: 0.023293	Acc: 54.8% (5483/10000)
[Test]  Epoch: 94	Loss: 0.023284	Acc: 54.9% (5491/10000)
[Test]  Epoch: 95	Loss: 0.023234	Acc: 54.8% (5482/10000)
[Test]  Epoch: 96	Loss: 0.023172	Acc: 55.0% (5498/10000)
[Test]  Epoch: 97	Loss: 0.023267	Acc: 54.9% (5492/10000)
[Test]  Epoch: 98	Loss: 0.023359	Acc: 54.9% (5491/10000)
[Test]  Epoch: 99	Loss: 0.023274	Acc: 55.0% (5500/10000)
[Test]  Epoch: 100	Loss: 0.023241	Acc: 54.9% (5486/10000)
===========finish==========
['2024-08-19', '01:33:16.365656', '100', 'test', '0.023240885543823243', '54.86', '55.08']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.008443	Acc: 81.6% (8157/10000)
[Test]  Epoch: 2	Loss: 0.008329	Acc: 81.9% (8187/10000)
[Test]  Epoch: 3	Loss: 0.008350	Acc: 81.8% (8183/10000)
[Test]  Epoch: 4	Loss: 0.008339	Acc: 82.0% (8199/10000)
[Test]  Epoch: 5	Loss: 0.008293	Acc: 82.2% (8219/10000)
[Test]  Epoch: 6	Loss: 0.008306	Acc: 82.1% (8211/10000)
[Test]  Epoch: 7	Loss: 0.008334	Acc: 82.2% (8224/10000)
[Test]  Epoch: 8	Loss: 0.008325	Acc: 82.0% (8197/10000)
[Test]  Epoch: 9	Loss: 0.008355	Acc: 82.1% (8207/10000)
[Test]  Epoch: 10	Loss: 0.008250	Acc: 82.3% (8227/10000)
[Test]  Epoch: 11	Loss: 0.008266	Acc: 82.2% (8216/10000)
[Test]  Epoch: 12	Loss: 0.008228	Acc: 82.4% (8242/10000)
[Test]  Epoch: 13	Loss: 0.008224	Acc: 82.4% (8237/10000)
[Test]  Epoch: 14	Loss: 0.008214	Acc: 82.4% (8243/10000)
[Test]  Epoch: 15	Loss: 0.008204	Acc: 82.5% (8247/10000)
[Test]  Epoch: 16	Loss: 0.008295	Acc: 82.2% (8223/10000)
[Test]  Epoch: 17	Loss: 0.008200	Acc: 82.2% (8222/10000)
[Test]  Epoch: 18	Loss: 0.008259	Acc: 82.1% (8207/10000)
[Test]  Epoch: 19	Loss: 0.008193	Acc: 82.3% (8233/10000)
[Test]  Epoch: 20	Loss: 0.008178	Acc: 82.3% (8231/10000)
[Test]  Epoch: 21	Loss: 0.008199	Acc: 82.4% (8242/10000)
[Test]  Epoch: 22	Loss: 0.008195	Acc: 82.4% (8238/10000)
[Test]  Epoch: 23	Loss: 0.008232	Acc: 82.3% (8230/10000)
[Test]  Epoch: 24	Loss: 0.008231	Acc: 82.4% (8241/10000)
[Test]  Epoch: 25	Loss: 0.008179	Acc: 82.4% (8241/10000)
[Test]  Epoch: 26	Loss: 0.008220	Acc: 82.4% (8237/10000)
[Test]  Epoch: 27	Loss: 0.008272	Acc: 82.3% (8227/10000)
[Test]  Epoch: 28	Loss: 0.008250	Acc: 82.3% (8227/10000)
[Test]  Epoch: 29	Loss: 0.008253	Acc: 82.3% (8234/10000)
[Test]  Epoch: 30	Loss: 0.008214	Acc: 82.5% (8248/10000)
[Test]  Epoch: 31	Loss: 0.008180	Acc: 82.4% (8241/10000)
[Test]  Epoch: 32	Loss: 0.008218	Acc: 82.4% (8240/10000)
[Test]  Epoch: 33	Loss: 0.008251	Acc: 82.4% (8236/10000)
[Test]  Epoch: 34	Loss: 0.008180	Acc: 82.5% (8251/10000)
[Test]  Epoch: 35	Loss: 0.008203	Acc: 82.3% (8229/10000)
[Test]  Epoch: 36	Loss: 0.008217	Acc: 82.5% (8248/10000)
[Test]  Epoch: 37	Loss: 0.008218	Acc: 82.3% (8234/10000)
[Test]  Epoch: 38	Loss: 0.008248	Acc: 82.5% (8247/10000)
[Test]  Epoch: 39	Loss: 0.008245	Acc: 82.2% (8219/10000)
[Test]  Epoch: 40	Loss: 0.008232	Acc: 82.1% (8211/10000)
[Test]  Epoch: 41	Loss: 0.008206	Acc: 82.3% (8230/10000)
[Test]  Epoch: 42	Loss: 0.008257	Acc: 82.2% (8217/10000)
[Test]  Epoch: 43	Loss: 0.008251	Acc: 82.2% (8225/10000)
[Test]  Epoch: 44	Loss: 0.008263	Acc: 82.2% (8223/10000)
[Test]  Epoch: 45	Loss: 0.008243	Acc: 82.3% (8233/10000)
[Test]  Epoch: 46	Loss: 0.008264	Acc: 82.3% (8227/10000)
[Test]  Epoch: 47	Loss: 0.008232	Acc: 82.5% (8247/10000)
[Test]  Epoch: 48	Loss: 0.008231	Acc: 82.5% (8246/10000)
[Test]  Epoch: 49	Loss: 0.008210	Acc: 82.3% (8233/10000)
[Test]  Epoch: 50	Loss: 0.008227	Acc: 82.4% (8241/10000)
[Test]  Epoch: 51	Loss: 0.008238	Acc: 82.3% (8229/10000)
[Test]  Epoch: 52	Loss: 0.008240	Acc: 82.3% (8228/10000)
[Test]  Epoch: 53	Loss: 0.008266	Acc: 82.3% (8226/10000)
[Test]  Epoch: 54	Loss: 0.008221	Acc: 82.5% (8246/10000)
[Test]  Epoch: 55	Loss: 0.008270	Acc: 82.2% (8225/10000)
[Test]  Epoch: 56	Loss: 0.008250	Acc: 82.2% (8223/10000)
[Test]  Epoch: 57	Loss: 0.008279	Acc: 82.5% (8252/10000)
[Test]  Epoch: 58	Loss: 0.008270	Acc: 82.4% (8240/10000)
[Test]  Epoch: 59	Loss: 0.008276	Acc: 82.1% (8211/10000)
[Test]  Epoch: 60	Loss: 0.008267	Acc: 82.2% (8218/10000)
[Test]  Epoch: 61	Loss: 0.008263	Acc: 82.1% (8214/10000)
[Test]  Epoch: 62	Loss: 0.008276	Acc: 82.1% (8214/10000)
[Test]  Epoch: 63	Loss: 0.008285	Acc: 82.2% (8215/10000)
[Test]  Epoch: 64	Loss: 0.008236	Acc: 82.2% (8225/10000)
[Test]  Epoch: 65	Loss: 0.008243	Acc: 82.3% (8228/10000)
[Test]  Epoch: 66	Loss: 0.008249	Acc: 82.3% (8232/10000)
[Test]  Epoch: 67	Loss: 0.008242	Acc: 82.3% (8228/10000)
[Test]  Epoch: 68	Loss: 0.008241	Acc: 82.3% (8227/10000)
[Test]  Epoch: 69	Loss: 0.008267	Acc: 82.1% (8207/10000)
[Test]  Epoch: 70	Loss: 0.008239	Acc: 82.3% (8230/10000)
[Test]  Epoch: 71	Loss: 0.008236	Acc: 82.3% (8232/10000)
[Test]  Epoch: 72	Loss: 0.008224	Acc: 82.2% (8225/10000)
[Test]  Epoch: 73	Loss: 0.008254	Acc: 82.3% (8231/10000)
[Test]  Epoch: 74	Loss: 0.008249	Acc: 82.4% (8236/10000)
[Test]  Epoch: 75	Loss: 0.008248	Acc: 82.2% (8225/10000)
[Test]  Epoch: 76	Loss: 0.008235	Acc: 82.3% (8227/10000)
[Test]  Epoch: 77	Loss: 0.008241	Acc: 82.3% (8233/10000)
[Test]  Epoch: 78	Loss: 0.008221	Acc: 82.4% (8241/10000)
[Test]  Epoch: 79	Loss: 0.008240	Acc: 82.4% (8241/10000)
[Test]  Epoch: 80	Loss: 0.008252	Acc: 82.4% (8236/10000)
[Test]  Epoch: 81	Loss: 0.008231	Acc: 82.3% (8233/10000)
[Test]  Epoch: 82	Loss: 0.008247	Acc: 82.3% (8232/10000)
[Test]  Epoch: 83	Loss: 0.008233	Acc: 82.4% (8239/10000)
[Test]  Epoch: 84	Loss: 0.008237	Acc: 82.4% (8241/10000)
[Test]  Epoch: 85	Loss: 0.008267	Acc: 82.3% (8231/10000)
[Test]  Epoch: 86	Loss: 0.008236	Acc: 82.5% (8247/10000)
[Test]  Epoch: 87	Loss: 0.008256	Acc: 82.3% (8229/10000)
[Test]  Epoch: 88	Loss: 0.008244	Acc: 82.3% (8233/10000)
[Test]  Epoch: 89	Loss: 0.008234	Acc: 82.3% (8231/10000)
[Test]  Epoch: 90	Loss: 0.008243	Acc: 82.3% (8231/10000)
[Test]  Epoch: 91	Loss: 0.008245	Acc: 82.2% (8220/10000)
[Test]  Epoch: 92	Loss: 0.008228	Acc: 82.3% (8231/10000)
[Test]  Epoch: 93	Loss: 0.008243	Acc: 82.2% (8222/10000)
[Test]  Epoch: 94	Loss: 0.008258	Acc: 82.3% (8228/10000)
[Test]  Epoch: 95	Loss: 0.008242	Acc: 82.3% (8226/10000)
[Test]  Epoch: 96	Loss: 0.008232	Acc: 82.4% (8241/10000)
[Test]  Epoch: 97	Loss: 0.008211	Acc: 82.3% (8235/10000)
[Test]  Epoch: 98	Loss: 0.008247	Acc: 82.1% (8209/10000)
[Test]  Epoch: 99	Loss: 0.008253	Acc: 82.3% (8231/10000)
[Test]  Epoch: 100	Loss: 0.008268	Acc: 82.3% (8231/10000)
===========finish==========
['2024-08-19', '01:35:16.663118', '100', 'test', '0.008267580264806748', '82.31', '82.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039809	Acc: 32.7% (3270/10000)
[Test]  Epoch: 2	Loss: 0.027295	Acc: 42.4% (4242/10000)
[Test]  Epoch: 3	Loss: 0.025646	Acc: 44.0% (4402/10000)
[Test]  Epoch: 4	Loss: 0.026563	Acc: 43.9% (4391/10000)
[Test]  Epoch: 5	Loss: 0.026290	Acc: 45.2% (4520/10000)
[Test]  Epoch: 6	Loss: 0.025294	Acc: 46.7% (4674/10000)
[Test]  Epoch: 7	Loss: 0.025363	Acc: 46.3% (4633/10000)
[Test]  Epoch: 8	Loss: 0.026337	Acc: 45.6% (4564/10000)
[Test]  Epoch: 9	Loss: 0.025761	Acc: 45.8% (4580/10000)
[Test]  Epoch: 10	Loss: 0.026069	Acc: 46.3% (4628/10000)
[Test]  Epoch: 11	Loss: 0.025993	Acc: 46.6% (4662/10000)
[Test]  Epoch: 12	Loss: 0.025655	Acc: 47.3% (4730/10000)
[Test]  Epoch: 13	Loss: 0.025757	Acc: 47.5% (4755/10000)
[Test]  Epoch: 14	Loss: 0.025808	Acc: 46.9% (4691/10000)
[Test]  Epoch: 15	Loss: 0.025790	Acc: 47.3% (4731/10000)
[Test]  Epoch: 16	Loss: 0.026097	Acc: 47.0% (4699/10000)
[Test]  Epoch: 17	Loss: 0.025600	Acc: 47.7% (4768/10000)
[Test]  Epoch: 18	Loss: 0.025819	Acc: 47.1% (4714/10000)
[Test]  Epoch: 19	Loss: 0.026125	Acc: 46.7% (4668/10000)
[Test]  Epoch: 20	Loss: 0.025443	Acc: 47.8% (4782/10000)
[Test]  Epoch: 21	Loss: 0.025815	Acc: 47.4% (4735/10000)
[Test]  Epoch: 22	Loss: 0.025543	Acc: 47.8% (4778/10000)
[Test]  Epoch: 23	Loss: 0.026024	Acc: 46.8% (4681/10000)
[Test]  Epoch: 24	Loss: 0.025392	Acc: 48.2% (4820/10000)
[Test]  Epoch: 25	Loss: 0.025904	Acc: 46.9% (4690/10000)
[Test]  Epoch: 26	Loss: 0.025561	Acc: 47.7% (4766/10000)
[Test]  Epoch: 27	Loss: 0.025383	Acc: 47.5% (4746/10000)
[Test]  Epoch: 28	Loss: 0.025296	Acc: 47.6% (4756/10000)
[Test]  Epoch: 29	Loss: 0.025192	Acc: 48.3% (4834/10000)
[Test]  Epoch: 30	Loss: 0.025571	Acc: 47.8% (4777/10000)
[Test]  Epoch: 31	Loss: 0.025047	Acc: 48.0% (4801/10000)
[Test]  Epoch: 32	Loss: 0.024808	Acc: 48.9% (4887/10000)
[Test]  Epoch: 33	Loss: 0.025292	Acc: 47.9% (4791/10000)
[Test]  Epoch: 34	Loss: 0.025208	Acc: 47.7% (4768/10000)
[Test]  Epoch: 35	Loss: 0.025128	Acc: 47.8% (4781/10000)
[Test]  Epoch: 36	Loss: 0.025033	Acc: 48.1% (4811/10000)
[Test]  Epoch: 37	Loss: 0.024955	Acc: 48.1% (4814/10000)
[Test]  Epoch: 38	Loss: 0.025038	Acc: 48.5% (4847/10000)
[Test]  Epoch: 39	Loss: 0.025040	Acc: 48.1% (4810/10000)
[Test]  Epoch: 40	Loss: 0.024849	Acc: 48.1% (4815/10000)
[Test]  Epoch: 41	Loss: 0.024656	Acc: 48.5% (4853/10000)
[Test]  Epoch: 42	Loss: 0.024948	Acc: 48.4% (4839/10000)
[Test]  Epoch: 43	Loss: 0.024955	Acc: 48.1% (4811/10000)
[Test]  Epoch: 44	Loss: 0.024735	Acc: 48.0% (4799/10000)
[Test]  Epoch: 45	Loss: 0.024785	Acc: 48.4% (4843/10000)
[Test]  Epoch: 46	Loss: 0.024697	Acc: 48.1% (4815/10000)
[Test]  Epoch: 47	Loss: 0.025274	Acc: 48.1% (4807/10000)
[Test]  Epoch: 48	Loss: 0.024789	Acc: 48.8% (4883/10000)
[Test]  Epoch: 49	Loss: 0.024811	Acc: 48.1% (4815/10000)
[Test]  Epoch: 50	Loss: 0.024689	Acc: 48.2% (4821/10000)
[Test]  Epoch: 51	Loss: 0.024435	Acc: 48.8% (4883/10000)
[Test]  Epoch: 52	Loss: 0.024421	Acc: 48.3% (4834/10000)
[Test]  Epoch: 53	Loss: 0.024661	Acc: 48.0% (4799/10000)
[Test]  Epoch: 54	Loss: 0.024546	Acc: 48.1% (4814/10000)
[Test]  Epoch: 55	Loss: 0.024283	Acc: 49.0% (4900/10000)
[Test]  Epoch: 56	Loss: 0.024519	Acc: 48.3% (4828/10000)
[Test]  Epoch: 57	Loss: 0.024401	Acc: 48.4% (4836/10000)
[Test]  Epoch: 58	Loss: 0.024629	Acc: 48.2% (4819/10000)
[Test]  Epoch: 59	Loss: 0.024471	Acc: 48.5% (4855/10000)
[Test]  Epoch: 60	Loss: 0.024292	Acc: 48.7% (4874/10000)
[Test]  Epoch: 61	Loss: 0.024178	Acc: 49.2% (4918/10000)
[Test]  Epoch: 62	Loss: 0.024188	Acc: 49.0% (4902/10000)
[Test]  Epoch: 63	Loss: 0.024113	Acc: 49.1% (4911/10000)
[Test]  Epoch: 64	Loss: 0.024125	Acc: 49.1% (4912/10000)
[Test]  Epoch: 65	Loss: 0.024121	Acc: 48.9% (4894/10000)
[Test]  Epoch: 66	Loss: 0.024126	Acc: 49.0% (4899/10000)
[Test]  Epoch: 67	Loss: 0.024238	Acc: 48.6% (4865/10000)
[Test]  Epoch: 68	Loss: 0.024177	Acc: 48.9% (4889/10000)
[Test]  Epoch: 69	Loss: 0.024125	Acc: 48.9% (4887/10000)
[Test]  Epoch: 70	Loss: 0.024115	Acc: 48.8% (4875/10000)
[Test]  Epoch: 71	Loss: 0.024121	Acc: 48.9% (4891/10000)
[Test]  Epoch: 72	Loss: 0.024194	Acc: 49.2% (4916/10000)
[Test]  Epoch: 73	Loss: 0.024182	Acc: 49.0% (4898/10000)
[Test]  Epoch: 74	Loss: 0.024131	Acc: 49.1% (4910/10000)
[Test]  Epoch: 75	Loss: 0.024044	Acc: 49.2% (4922/10000)
[Test]  Epoch: 76	Loss: 0.024143	Acc: 48.9% (4887/10000)
[Test]  Epoch: 77	Loss: 0.024093	Acc: 49.1% (4910/10000)
[Test]  Epoch: 78	Loss: 0.024165	Acc: 49.0% (4900/10000)
[Test]  Epoch: 79	Loss: 0.024182	Acc: 49.1% (4908/10000)
[Test]  Epoch: 80	Loss: 0.024074	Acc: 48.8% (4883/10000)
[Test]  Epoch: 81	Loss: 0.024104	Acc: 49.0% (4902/10000)
[Test]  Epoch: 82	Loss: 0.024148	Acc: 49.0% (4902/10000)
[Test]  Epoch: 83	Loss: 0.024106	Acc: 49.0% (4897/10000)
[Test]  Epoch: 84	Loss: 0.024047	Acc: 49.1% (4907/10000)
[Test]  Epoch: 85	Loss: 0.024101	Acc: 49.1% (4907/10000)
[Test]  Epoch: 86	Loss: 0.024104	Acc: 49.1% (4912/10000)
[Test]  Epoch: 87	Loss: 0.024066	Acc: 48.8% (4881/10000)
[Test]  Epoch: 88	Loss: 0.024034	Acc: 49.1% (4906/10000)
[Test]  Epoch: 89	Loss: 0.024062	Acc: 49.0% (4904/10000)
[Test]  Epoch: 90	Loss: 0.024061	Acc: 49.1% (4909/10000)
[Test]  Epoch: 91	Loss: 0.024023	Acc: 48.8% (4875/10000)
[Test]  Epoch: 92	Loss: 0.024046	Acc: 49.1% (4907/10000)
[Test]  Epoch: 93	Loss: 0.024041	Acc: 49.0% (4905/10000)
[Test]  Epoch: 94	Loss: 0.024024	Acc: 48.9% (4890/10000)
[Test]  Epoch: 95	Loss: 0.023980	Acc: 49.0% (4898/10000)
[Test]  Epoch: 96	Loss: 0.024088	Acc: 49.0% (4898/10000)
[Test]  Epoch: 97	Loss: 0.024096	Acc: 49.0% (4897/10000)
[Test]  Epoch: 98	Loss: 0.024133	Acc: 48.9% (4888/10000)
[Test]  Epoch: 99	Loss: 0.024059	Acc: 49.0% (4899/10000)
[Test]  Epoch: 100	Loss: 0.023999	Acc: 48.8% (4882/10000)
===========finish==========
['2024-08-19', '01:37:40.647728', '100', 'test', '0.023999274730682373', '48.82', '49.22']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.075527	Acc: 21.9% (2189/10000)
[Test]  Epoch: 2	Loss: 0.029959	Acc: 38.2% (3822/10000)
[Test]  Epoch: 3	Loss: 0.026216	Acc: 42.6% (4257/10000)
[Test]  Epoch: 4	Loss: 0.026822	Acc: 42.7% (4266/10000)
[Test]  Epoch: 5	Loss: 0.026725	Acc: 42.5% (4247/10000)
[Test]  Epoch: 6	Loss: 0.028257	Acc: 42.8% (4279/10000)
[Test]  Epoch: 7	Loss: 0.026966	Acc: 44.2% (4421/10000)
[Test]  Epoch: 8	Loss: 0.027644	Acc: 43.6% (4364/10000)
[Test]  Epoch: 9	Loss: 0.027721	Acc: 43.8% (4379/10000)
[Test]  Epoch: 10	Loss: 0.030353	Acc: 41.5% (4150/10000)
[Test]  Epoch: 11	Loss: 0.027858	Acc: 44.1% (4408/10000)
[Test]  Epoch: 12	Loss: 0.027947	Acc: 45.1% (4508/10000)
[Test]  Epoch: 13	Loss: 0.027186	Acc: 46.5% (4653/10000)
[Test]  Epoch: 14	Loss: 0.027431	Acc: 45.4% (4542/10000)
[Test]  Epoch: 15	Loss: 0.027080	Acc: 45.4% (4543/10000)
[Test]  Epoch: 16	Loss: 0.027266	Acc: 45.3% (4533/10000)
[Test]  Epoch: 17	Loss: 0.026583	Acc: 45.9% (4589/10000)
[Test]  Epoch: 18	Loss: 0.026792	Acc: 45.9% (4589/10000)
[Test]  Epoch: 19	Loss: 0.026333	Acc: 46.5% (4646/10000)
[Test]  Epoch: 20	Loss: 0.026695	Acc: 46.3% (4627/10000)
[Test]  Epoch: 21	Loss: 0.026778	Acc: 46.3% (4632/10000)
[Test]  Epoch: 22	Loss: 0.026326	Acc: 46.9% (4694/10000)
[Test]  Epoch: 23	Loss: 0.026566	Acc: 46.5% (4651/10000)
[Test]  Epoch: 24	Loss: 0.027614	Acc: 45.2% (4520/10000)
[Test]  Epoch: 25	Loss: 0.026963	Acc: 46.6% (4664/10000)
[Test]  Epoch: 26	Loss: 0.026402	Acc: 46.3% (4633/10000)
[Test]  Epoch: 27	Loss: 0.026567	Acc: 45.9% (4587/10000)
[Test]  Epoch: 28	Loss: 0.026003	Acc: 46.8% (4676/10000)
[Test]  Epoch: 29	Loss: 0.025702	Acc: 47.2% (4720/10000)
[Test]  Epoch: 30	Loss: 0.025740	Acc: 47.0% (4701/10000)
[Test]  Epoch: 31	Loss: 0.025571	Acc: 47.5% (4746/10000)
[Test]  Epoch: 32	Loss: 0.025865	Acc: 47.8% (4781/10000)
[Test]  Epoch: 33	Loss: 0.026078	Acc: 46.9% (4688/10000)
[Test]  Epoch: 34	Loss: 0.025489	Acc: 47.1% (4706/10000)
[Test]  Epoch: 35	Loss: 0.025319	Acc: 47.9% (4793/10000)
[Test]  Epoch: 36	Loss: 0.026017	Acc: 46.8% (4681/10000)
[Test]  Epoch: 37	Loss: 0.025606	Acc: 47.2% (4722/10000)
[Test]  Epoch: 38	Loss: 0.026506	Acc: 46.5% (4650/10000)
[Test]  Epoch: 39	Loss: 0.025714	Acc: 47.2% (4719/10000)
[Test]  Epoch: 40	Loss: 0.025417	Acc: 47.7% (4769/10000)
[Test]  Epoch: 41	Loss: 0.025348	Acc: 47.5% (4752/10000)
[Test]  Epoch: 42	Loss: 0.025379	Acc: 47.7% (4769/10000)
[Test]  Epoch: 43	Loss: 0.025586	Acc: 47.6% (4760/10000)
[Test]  Epoch: 44	Loss: 0.025314	Acc: 47.8% (4780/10000)
[Test]  Epoch: 45	Loss: 0.025324	Acc: 47.8% (4784/10000)
[Test]  Epoch: 46	Loss: 0.025638	Acc: 47.4% (4737/10000)
[Test]  Epoch: 47	Loss: 0.025707	Acc: 47.2% (4716/10000)
[Test]  Epoch: 48	Loss: 0.025429	Acc: 47.5% (4749/10000)
[Test]  Epoch: 49	Loss: 0.025366	Acc: 47.2% (4723/10000)
[Test]  Epoch: 50	Loss: 0.025347	Acc: 47.9% (4790/10000)
[Test]  Epoch: 51	Loss: 0.025329	Acc: 47.9% (4785/10000)
[Test]  Epoch: 52	Loss: 0.025266	Acc: 48.1% (4807/10000)
[Test]  Epoch: 53	Loss: 0.025583	Acc: 47.4% (4741/10000)
[Test]  Epoch: 54	Loss: 0.024845	Acc: 48.0% (4801/10000)
[Test]  Epoch: 55	Loss: 0.025281	Acc: 47.2% (4721/10000)
[Test]  Epoch: 56	Loss: 0.024969	Acc: 48.0% (4798/10000)
[Test]  Epoch: 57	Loss: 0.025769	Acc: 46.9% (4694/10000)
[Test]  Epoch: 58	Loss: 0.025441	Acc: 46.9% (4693/10000)
[Test]  Epoch: 59	Loss: 0.026064	Acc: 46.4% (4639/10000)
[Test]  Epoch: 60	Loss: 0.025336	Acc: 48.0% (4803/10000)
[Test]  Epoch: 61	Loss: 0.025049	Acc: 47.7% (4773/10000)
[Test]  Epoch: 62	Loss: 0.024918	Acc: 48.0% (4803/10000)
[Test]  Epoch: 63	Loss: 0.024850	Acc: 48.1% (4814/10000)
[Test]  Epoch: 64	Loss: 0.024843	Acc: 48.1% (4807/10000)
[Test]  Epoch: 65	Loss: 0.024836	Acc: 47.9% (4790/10000)
[Test]  Epoch: 66	Loss: 0.024776	Acc: 47.7% (4773/10000)
[Test]  Epoch: 67	Loss: 0.024786	Acc: 47.8% (4784/10000)
[Test]  Epoch: 68	Loss: 0.024818	Acc: 47.8% (4784/10000)
[Test]  Epoch: 69	Loss: 0.024810	Acc: 48.0% (4803/10000)
[Test]  Epoch: 70	Loss: 0.024768	Acc: 48.1% (4811/10000)
[Test]  Epoch: 71	Loss: 0.024780	Acc: 47.9% (4792/10000)
[Test]  Epoch: 72	Loss: 0.024873	Acc: 48.1% (4807/10000)
[Test]  Epoch: 73	Loss: 0.024719	Acc: 48.1% (4812/10000)
[Test]  Epoch: 74	Loss: 0.024700	Acc: 48.1% (4813/10000)
[Test]  Epoch: 75	Loss: 0.024666	Acc: 48.1% (4809/10000)
[Test]  Epoch: 76	Loss: 0.024692	Acc: 47.8% (4781/10000)
[Test]  Epoch: 77	Loss: 0.024749	Acc: 48.1% (4810/10000)
[Test]  Epoch: 78	Loss: 0.024741	Acc: 48.1% (4812/10000)
[Test]  Epoch: 79	Loss: 0.024768	Acc: 48.2% (4816/10000)
[Test]  Epoch: 80	Loss: 0.024664	Acc: 48.0% (4796/10000)
[Test]  Epoch: 81	Loss: 0.024837	Acc: 47.9% (4791/10000)
[Test]  Epoch: 82	Loss: 0.024815	Acc: 48.1% (4810/10000)
[Test]  Epoch: 83	Loss: 0.024816	Acc: 47.9% (4791/10000)
[Test]  Epoch: 84	Loss: 0.024755	Acc: 48.0% (4799/10000)
[Test]  Epoch: 85	Loss: 0.024806	Acc: 47.8% (4784/10000)
[Test]  Epoch: 86	Loss: 0.024740	Acc: 48.1% (4807/10000)
[Test]  Epoch: 87	Loss: 0.024677	Acc: 48.2% (4818/10000)
[Test]  Epoch: 88	Loss: 0.024583	Acc: 48.2% (4816/10000)
[Test]  Epoch: 89	Loss: 0.024735	Acc: 47.9% (4787/10000)
[Test]  Epoch: 90	Loss: 0.024711	Acc: 48.1% (4806/10000)
[Test]  Epoch: 91	Loss: 0.024704	Acc: 48.0% (4804/10000)
[Test]  Epoch: 92	Loss: 0.024780	Acc: 48.2% (4816/10000)
[Test]  Epoch: 93	Loss: 0.024710	Acc: 48.1% (4808/10000)
[Test]  Epoch: 94	Loss: 0.024639	Acc: 48.0% (4805/10000)
[Test]  Epoch: 95	Loss: 0.024697	Acc: 48.2% (4819/10000)
[Test]  Epoch: 96	Loss: 0.024740	Acc: 48.1% (4807/10000)
[Test]  Epoch: 97	Loss: 0.024667	Acc: 48.1% (4810/10000)
[Test]  Epoch: 98	Loss: 0.024765	Acc: 48.1% (4814/10000)
[Test]  Epoch: 99	Loss: 0.024658	Acc: 48.1% (4814/10000)
[Test]  Epoch: 100	Loss: 0.024660	Acc: 48.0% (4803/10000)
===========finish==========
['2024-08-19', '01:40:02.085987', '100', 'test', '0.02466038398742676', '48.03', '48.19']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.1.1.weight', '_features.11.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.051364	Acc: 29.8% (2979/10000)
[Test]  Epoch: 2	Loss: 0.028709	Acc: 39.0% (3905/10000)
[Test]  Epoch: 3	Loss: 0.028440	Acc: 38.4% (3837/10000)
[Test]  Epoch: 4	Loss: 0.031618	Acc: 36.4% (3640/10000)
[Test]  Epoch: 5	Loss: 0.028384	Acc: 39.9% (3990/10000)
[Test]  Epoch: 6	Loss: 0.027523	Acc: 41.7% (4168/10000)
[Test]  Epoch: 7	Loss: 0.027437	Acc: 41.5% (4146/10000)
[Test]  Epoch: 8	Loss: 0.030677	Acc: 40.7% (4072/10000)
[Test]  Epoch: 9	Loss: 0.028635	Acc: 42.3% (4226/10000)
[Test]  Epoch: 10	Loss: 0.029830	Acc: 40.3% (4028/10000)
[Test]  Epoch: 11	Loss: 0.028761	Acc: 42.8% (4282/10000)
[Test]  Epoch: 12	Loss: 0.029239	Acc: 41.9% (4194/10000)
[Test]  Epoch: 13	Loss: 0.028117	Acc: 42.8% (4275/10000)
[Test]  Epoch: 14	Loss: 0.027854	Acc: 42.8% (4281/10000)
[Test]  Epoch: 15	Loss: 0.028701	Acc: 42.7% (4266/10000)
[Test]  Epoch: 16	Loss: 0.027852	Acc: 43.5% (4348/10000)
[Test]  Epoch: 17	Loss: 0.028090	Acc: 43.4% (4339/10000)
[Test]  Epoch: 18	Loss: 0.028351	Acc: 42.3% (4231/10000)
[Test]  Epoch: 19	Loss: 0.028158	Acc: 43.2% (4325/10000)
[Test]  Epoch: 20	Loss: 0.028013	Acc: 43.5% (4348/10000)
[Test]  Epoch: 21	Loss: 0.027940	Acc: 44.2% (4423/10000)
[Test]  Epoch: 22	Loss: 0.028019	Acc: 44.1% (4408/10000)
[Test]  Epoch: 23	Loss: 0.027768	Acc: 43.5% (4353/10000)
[Test]  Epoch: 24	Loss: 0.028258	Acc: 42.9% (4287/10000)
[Test]  Epoch: 25	Loss: 0.028534	Acc: 42.8% (4281/10000)
[Test]  Epoch: 26	Loss: 0.027652	Acc: 43.4% (4337/10000)
[Test]  Epoch: 27	Loss: 0.027724	Acc: 43.2% (4320/10000)
[Test]  Epoch: 28	Loss: 0.028060	Acc: 42.9% (4286/10000)
[Test]  Epoch: 29	Loss: 0.027571	Acc: 43.6% (4359/10000)
[Test]  Epoch: 30	Loss: 0.027015	Acc: 44.5% (4447/10000)
[Test]  Epoch: 31	Loss: 0.027705	Acc: 42.7% (4268/10000)
[Test]  Epoch: 32	Loss: 0.027302	Acc: 44.3% (4434/10000)
[Test]  Epoch: 33	Loss: 0.027497	Acc: 43.6% (4362/10000)
[Test]  Epoch: 34	Loss: 0.026935	Acc: 43.9% (4389/10000)
[Test]  Epoch: 35	Loss: 0.027214	Acc: 43.9% (4385/10000)
[Test]  Epoch: 36	Loss: 0.027282	Acc: 43.5% (4355/10000)
[Test]  Epoch: 37	Loss: 0.027055	Acc: 44.3% (4430/10000)
[Test]  Epoch: 38	Loss: 0.027290	Acc: 44.5% (4453/10000)
[Test]  Epoch: 39	Loss: 0.027418	Acc: 43.5% (4346/10000)
[Test]  Epoch: 40	Loss: 0.026799	Acc: 44.0% (4399/10000)
[Test]  Epoch: 41	Loss: 0.026905	Acc: 43.8% (4377/10000)
[Test]  Epoch: 42	Loss: 0.026947	Acc: 44.1% (4408/10000)
[Test]  Epoch: 43	Loss: 0.027046	Acc: 44.1% (4411/10000)
[Test]  Epoch: 44	Loss: 0.026635	Acc: 44.8% (4482/10000)
[Test]  Epoch: 45	Loss: 0.026665	Acc: 44.8% (4481/10000)
[Test]  Epoch: 46	Loss: 0.026907	Acc: 44.5% (4448/10000)
[Test]  Epoch: 47	Loss: 0.026877	Acc: 43.9% (4390/10000)
[Test]  Epoch: 48	Loss: 0.026767	Acc: 44.4% (4443/10000)
[Test]  Epoch: 49	Loss: 0.027218	Acc: 43.3% (4326/10000)
[Test]  Epoch: 50	Loss: 0.027168	Acc: 44.0% (4397/10000)
[Test]  Epoch: 51	Loss: 0.026714	Acc: 44.4% (4435/10000)
[Test]  Epoch: 52	Loss: 0.026787	Acc: 44.8% (4478/10000)
[Test]  Epoch: 53	Loss: 0.026661	Acc: 44.2% (4420/10000)
[Test]  Epoch: 54	Loss: 0.026463	Acc: 44.0% (4405/10000)
[Test]  Epoch: 55	Loss: 0.026890	Acc: 44.5% (4451/10000)
[Test]  Epoch: 56	Loss: 0.026947	Acc: 44.1% (4413/10000)
[Test]  Epoch: 57	Loss: 0.026933	Acc: 44.2% (4417/10000)
[Test]  Epoch: 58	Loss: 0.026749	Acc: 44.8% (4481/10000)
[Test]  Epoch: 59	Loss: 0.026410	Acc: 44.7% (4468/10000)
[Test]  Epoch: 60	Loss: 0.026679	Acc: 44.7% (4470/10000)
[Test]  Epoch: 61	Loss: 0.026363	Acc: 45.1% (4511/10000)
[Test]  Epoch: 62	Loss: 0.026293	Acc: 45.1% (4509/10000)
[Test]  Epoch: 63	Loss: 0.026310	Acc: 45.1% (4507/10000)
[Test]  Epoch: 64	Loss: 0.026279	Acc: 45.1% (4510/10000)
[Test]  Epoch: 65	Loss: 0.026224	Acc: 45.0% (4495/10000)
[Test]  Epoch: 66	Loss: 0.026245	Acc: 44.8% (4475/10000)
[Test]  Epoch: 67	Loss: 0.026358	Acc: 44.9% (4488/10000)
[Test]  Epoch: 68	Loss: 0.026272	Acc: 45.0% (4495/10000)
[Test]  Epoch: 69	Loss: 0.026287	Acc: 44.9% (4493/10000)
[Test]  Epoch: 70	Loss: 0.026236	Acc: 44.8% (4484/10000)
[Test]  Epoch: 71	Loss: 0.026225	Acc: 44.8% (4483/10000)
[Test]  Epoch: 72	Loss: 0.026285	Acc: 45.0% (4501/10000)
[Test]  Epoch: 73	Loss: 0.026252	Acc: 45.0% (4504/10000)
[Test]  Epoch: 74	Loss: 0.026260	Acc: 44.8% (4475/10000)
[Test]  Epoch: 75	Loss: 0.026219	Acc: 44.8% (4475/10000)
[Test]  Epoch: 76	Loss: 0.026244	Acc: 44.7% (4471/10000)
[Test]  Epoch: 77	Loss: 0.026230	Acc: 44.9% (4493/10000)
[Test]  Epoch: 78	Loss: 0.026275	Acc: 45.0% (4503/10000)
[Test]  Epoch: 79	Loss: 0.026239	Acc: 44.8% (4479/10000)
[Test]  Epoch: 80	Loss: 0.026233	Acc: 44.9% (4489/10000)
[Test]  Epoch: 81	Loss: 0.026297	Acc: 44.6% (4463/10000)
[Test]  Epoch: 82	Loss: 0.026271	Acc: 44.7% (4472/10000)
[Test]  Epoch: 83	Loss: 0.026211	Acc: 44.8% (4484/10000)
[Test]  Epoch: 84	Loss: 0.026248	Acc: 45.1% (4508/10000)
[Test]  Epoch: 85	Loss: 0.026300	Acc: 44.8% (4479/10000)
[Test]  Epoch: 86	Loss: 0.026274	Acc: 45.1% (4515/10000)
[Test]  Epoch: 87	Loss: 0.026236	Acc: 45.3% (4533/10000)
[Test]  Epoch: 88	Loss: 0.026144	Acc: 44.9% (4487/10000)
[Test]  Epoch: 89	Loss: 0.026239	Acc: 45.1% (4514/10000)
[Test]  Epoch: 90	Loss: 0.026265	Acc: 45.1% (4506/10000)
[Test]  Epoch: 91	Loss: 0.026263	Acc: 44.9% (4492/10000)
[Test]  Epoch: 92	Loss: 0.026249	Acc: 44.9% (4490/10000)
[Test]  Epoch: 93	Loss: 0.026282	Acc: 45.0% (4502/10000)
[Test]  Epoch: 94	Loss: 0.026236	Acc: 45.0% (4498/10000)
[Test]  Epoch: 95	Loss: 0.026257	Acc: 44.8% (4481/10000)
[Test]  Epoch: 96	Loss: 0.026321	Acc: 44.9% (4489/10000)
[Test]  Epoch: 97	Loss: 0.026285	Acc: 44.9% (4485/10000)
[Test]  Epoch: 98	Loss: 0.026365	Acc: 45.0% (4496/10000)
[Test]  Epoch: 99	Loss: 0.026247	Acc: 44.8% (4475/10000)
[Test]  Epoch: 100	Loss: 0.026232	Acc: 44.8% (4480/10000)
===========finish==========
['2024-08-19', '01:42:25.228640', '100', 'test', '0.026231737101078033', '44.8', '45.33']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.1.1.weight', '_features.11.conv.3.weight', '_features.9.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.3.weight', '_features.0.0.weight', '_features.11.conv.1.1.weight', '_features.15.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.135046	Acc: 18.5% (1854/10000)
[Test]  Epoch: 2	Loss: 0.033500	Acc: 29.9% (2988/10000)
[Test]  Epoch: 3	Loss: 0.029341	Acc: 34.6% (3461/10000)
[Test]  Epoch: 4	Loss: 0.032180	Acc: 31.8% (3183/10000)
[Test]  Epoch: 5	Loss: 0.028713	Acc: 36.9% (3688/10000)
[Test]  Epoch: 6	Loss: 0.030336	Acc: 37.1% (3714/10000)
[Test]  Epoch: 7	Loss: 0.030216	Acc: 36.3% (3631/10000)
[Test]  Epoch: 8	Loss: 0.033769	Acc: 34.7% (3473/10000)
[Test]  Epoch: 9	Loss: 0.030666	Acc: 35.4% (3544/10000)
[Test]  Epoch: 10	Loss: 0.029857	Acc: 37.9% (3792/10000)
[Test]  Epoch: 11	Loss: 0.030686	Acc: 38.8% (3880/10000)
[Test]  Epoch: 12	Loss: 0.031339	Acc: 37.5% (3749/10000)
[Test]  Epoch: 13	Loss: 0.032657	Acc: 37.1% (3712/10000)
[Test]  Epoch: 14	Loss: 0.030527	Acc: 38.8% (3880/10000)
[Test]  Epoch: 15	Loss: 0.030132	Acc: 39.7% (3973/10000)
[Test]  Epoch: 16	Loss: 0.030606	Acc: 38.5% (3852/10000)
[Test]  Epoch: 17	Loss: 0.030321	Acc: 37.8% (3777/10000)
[Test]  Epoch: 18	Loss: 0.031978	Acc: 38.3% (3831/10000)
[Test]  Epoch: 19	Loss: 0.030670	Acc: 38.3% (3827/10000)
[Test]  Epoch: 20	Loss: 0.029314	Acc: 40.8% (4082/10000)
[Test]  Epoch: 21	Loss: 0.029621	Acc: 39.3% (3929/10000)
[Test]  Epoch: 22	Loss: 0.029394	Acc: 39.9% (3985/10000)
[Test]  Epoch: 23	Loss: 0.029258	Acc: 39.9% (3990/10000)
[Test]  Epoch: 24	Loss: 0.030039	Acc: 38.9% (3890/10000)
[Test]  Epoch: 25	Loss: 0.029470	Acc: 40.4% (4039/10000)
[Test]  Epoch: 26	Loss: 0.029811	Acc: 39.8% (3980/10000)
[Test]  Epoch: 27	Loss: 0.029132	Acc: 39.6% (3961/10000)
[Test]  Epoch: 28	Loss: 0.029487	Acc: 40.6% (4062/10000)
[Test]  Epoch: 29	Loss: 0.028791	Acc: 41.0% (4096/10000)
[Test]  Epoch: 30	Loss: 0.029086	Acc: 40.2% (4016/10000)
[Test]  Epoch: 31	Loss: 0.028820	Acc: 40.6% (4063/10000)
[Test]  Epoch: 32	Loss: 0.028623	Acc: 41.2% (4124/10000)
[Test]  Epoch: 33	Loss: 0.028854	Acc: 39.8% (3983/10000)
[Test]  Epoch: 34	Loss: 0.028699	Acc: 40.6% (4059/10000)
[Test]  Epoch: 35	Loss: 0.028303	Acc: 40.8% (4075/10000)
[Test]  Epoch: 36	Loss: 0.028361	Acc: 41.1% (4115/10000)
[Test]  Epoch: 37	Loss: 0.028491	Acc: 40.0% (4004/10000)
[Test]  Epoch: 38	Loss: 0.028873	Acc: 41.0% (4098/10000)
[Test]  Epoch: 39	Loss: 0.028496	Acc: 40.6% (4057/10000)
[Test]  Epoch: 40	Loss: 0.028151	Acc: 41.2% (4116/10000)
[Test]  Epoch: 41	Loss: 0.028499	Acc: 40.8% (4077/10000)
[Test]  Epoch: 42	Loss: 0.028178	Acc: 41.4% (4136/10000)
[Test]  Epoch: 43	Loss: 0.028783	Acc: 40.7% (4071/10000)
[Test]  Epoch: 44	Loss: 0.028091	Acc: 41.1% (4115/10000)
[Test]  Epoch: 45	Loss: 0.028255	Acc: 41.1% (4113/10000)
[Test]  Epoch: 46	Loss: 0.027910	Acc: 41.4% (4135/10000)
[Test]  Epoch: 47	Loss: 0.028092	Acc: 41.5% (4154/10000)
[Test]  Epoch: 48	Loss: 0.028097	Acc: 41.4% (4137/10000)
[Test]  Epoch: 49	Loss: 0.028167	Acc: 40.8% (4075/10000)
[Test]  Epoch: 50	Loss: 0.028039	Acc: 41.7% (4173/10000)
[Test]  Epoch: 51	Loss: 0.027944	Acc: 41.6% (4158/10000)
[Test]  Epoch: 52	Loss: 0.027922	Acc: 41.3% (4128/10000)
[Test]  Epoch: 53	Loss: 0.027860	Acc: 41.1% (4110/10000)
[Test]  Epoch: 54	Loss: 0.027625	Acc: 41.4% (4139/10000)
[Test]  Epoch: 55	Loss: 0.028114	Acc: 41.4% (4144/10000)
[Test]  Epoch: 56	Loss: 0.027894	Acc: 41.7% (4166/10000)
[Test]  Epoch: 57	Loss: 0.027659	Acc: 41.5% (4148/10000)
[Test]  Epoch: 58	Loss: 0.027806	Acc: 41.7% (4169/10000)
[Test]  Epoch: 59	Loss: 0.028149	Acc: 40.8% (4081/10000)
[Test]  Epoch: 60	Loss: 0.028076	Acc: 41.0% (4099/10000)
[Test]  Epoch: 61	Loss: 0.027846	Acc: 41.5% (4153/10000)
[Test]  Epoch: 62	Loss: 0.027803	Acc: 41.6% (4159/10000)
[Test]  Epoch: 63	Loss: 0.027684	Acc: 41.6% (4161/10000)
[Test]  Epoch: 64	Loss: 0.027661	Acc: 41.7% (4173/10000)
[Test]  Epoch: 65	Loss: 0.027636	Acc: 41.6% (4158/10000)
[Test]  Epoch: 66	Loss: 0.027567	Acc: 41.6% (4157/10000)
[Test]  Epoch: 67	Loss: 0.027661	Acc: 41.6% (4162/10000)
[Test]  Epoch: 68	Loss: 0.027589	Acc: 41.6% (4162/10000)
[Test]  Epoch: 69	Loss: 0.027635	Acc: 41.5% (4148/10000)
[Test]  Epoch: 70	Loss: 0.027624	Acc: 41.7% (4167/10000)
[Test]  Epoch: 71	Loss: 0.027645	Acc: 41.5% (4152/10000)
[Test]  Epoch: 72	Loss: 0.027672	Acc: 41.8% (4181/10000)
[Test]  Epoch: 73	Loss: 0.027632	Acc: 41.4% (4141/10000)
[Test]  Epoch: 74	Loss: 0.027589	Acc: 41.6% (4157/10000)
[Test]  Epoch: 75	Loss: 0.027536	Acc: 41.6% (4163/10000)
[Test]  Epoch: 76	Loss: 0.027616	Acc: 41.4% (4141/10000)
[Test]  Epoch: 77	Loss: 0.027553	Acc: 41.6% (4160/10000)
[Test]  Epoch: 78	Loss: 0.027588	Acc: 41.7% (4167/10000)
[Test]  Epoch: 79	Loss: 0.027718	Acc: 41.6% (4163/10000)
[Test]  Epoch: 80	Loss: 0.027616	Acc: 41.6% (4162/10000)
[Test]  Epoch: 81	Loss: 0.027629	Acc: 41.6% (4158/10000)
[Test]  Epoch: 82	Loss: 0.027600	Acc: 41.5% (4149/10000)
[Test]  Epoch: 83	Loss: 0.027629	Acc: 41.4% (4135/10000)
[Test]  Epoch: 84	Loss: 0.027564	Acc: 41.5% (4155/10000)
[Test]  Epoch: 85	Loss: 0.027531	Acc: 41.7% (4167/10000)
[Test]  Epoch: 86	Loss: 0.027525	Acc: 41.9% (4188/10000)
[Test]  Epoch: 87	Loss: 0.027597	Acc: 41.5% (4155/10000)
[Test]  Epoch: 88	Loss: 0.027483	Acc: 41.7% (4167/10000)
[Test]  Epoch: 89	Loss: 0.027617	Acc: 41.5% (4146/10000)
[Test]  Epoch: 90	Loss: 0.027592	Acc: 41.6% (4158/10000)
[Test]  Epoch: 91	Loss: 0.027542	Acc: 41.6% (4164/10000)
[Test]  Epoch: 92	Loss: 0.027650	Acc: 41.6% (4158/10000)
[Test]  Epoch: 93	Loss: 0.027598	Acc: 41.5% (4154/10000)
[Test]  Epoch: 94	Loss: 0.027588	Acc: 41.5% (4150/10000)
[Test]  Epoch: 95	Loss: 0.027612	Acc: 41.6% (4158/10000)
[Test]  Epoch: 96	Loss: 0.027613	Acc: 41.6% (4160/10000)
[Test]  Epoch: 97	Loss: 0.027604	Acc: 41.7% (4172/10000)
[Test]  Epoch: 98	Loss: 0.027673	Acc: 41.6% (4164/10000)
[Test]  Epoch: 99	Loss: 0.027603	Acc: 41.7% (4173/10000)
[Test]  Epoch: 100	Loss: 0.027619	Acc: 41.8% (4178/10000)
===========finish==========
['2024-08-19', '01:44:32.112653', '100', 'test', '0.027619112896919252', '41.78', '41.88']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.1.1.weight', '_features.11.conv.3.weight', '_features.9.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.3.weight', '_features.0.0.weight', '_features.11.conv.1.1.weight', '_features.15.conv.3.weight', '_features.9.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.1.0.weight', '_features.16.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.102494	Acc: 19.3% (1927/10000)
[Test]  Epoch: 2	Loss: 0.039321	Acc: 25.3% (2531/10000)
[Test]  Epoch: 3	Loss: 0.031060	Acc: 30.8% (3075/10000)
[Test]  Epoch: 4	Loss: 0.029942	Acc: 31.5% (3149/10000)
[Test]  Epoch: 5	Loss: 0.031837	Acc: 31.7% (3170/10000)
[Test]  Epoch: 6	Loss: 0.030098	Acc: 34.9% (3494/10000)
[Test]  Epoch: 7	Loss: 0.032004	Acc: 34.7% (3468/10000)
[Test]  Epoch: 8	Loss: 0.033104	Acc: 34.3% (3433/10000)
[Test]  Epoch: 9	Loss: 0.033055	Acc: 34.4% (3438/10000)
[Test]  Epoch: 10	Loss: 0.033053	Acc: 36.8% (3679/10000)
[Test]  Epoch: 11	Loss: 0.033383	Acc: 34.8% (3482/10000)
[Test]  Epoch: 12	Loss: 0.031876	Acc: 36.4% (3641/10000)
[Test]  Epoch: 13	Loss: 0.031713	Acc: 36.9% (3686/10000)
[Test]  Epoch: 14	Loss: 0.032592	Acc: 36.0% (3601/10000)
[Test]  Epoch: 15	Loss: 0.032152	Acc: 37.0% (3696/10000)
[Test]  Epoch: 16	Loss: 0.032049	Acc: 36.8% (3683/10000)
[Test]  Epoch: 17	Loss: 0.032992	Acc: 35.9% (3588/10000)
[Test]  Epoch: 18	Loss: 0.031010	Acc: 38.2% (3821/10000)
[Test]  Epoch: 19	Loss: 0.031609	Acc: 38.1% (3813/10000)
[Test]  Epoch: 20	Loss: 0.030845	Acc: 38.9% (3893/10000)
[Test]  Epoch: 21	Loss: 0.031313	Acc: 37.5% (3753/10000)
[Test]  Epoch: 22	Loss: 0.030676	Acc: 38.8% (3883/10000)
[Test]  Epoch: 23	Loss: 0.030433	Acc: 37.9% (3790/10000)
[Test]  Epoch: 24	Loss: 0.031399	Acc: 37.6% (3760/10000)
[Test]  Epoch: 25	Loss: 0.031498	Acc: 38.0% (3798/10000)
[Test]  Epoch: 26	Loss: 0.030736	Acc: 38.3% (3830/10000)
[Test]  Epoch: 27	Loss: 0.030831	Acc: 38.3% (3829/10000)
[Test]  Epoch: 28	Loss: 0.030377	Acc: 38.8% (3884/10000)
[Test]  Epoch: 29	Loss: 0.030508	Acc: 38.5% (3849/10000)
[Test]  Epoch: 30	Loss: 0.029979	Acc: 39.4% (3941/10000)
[Test]  Epoch: 31	Loss: 0.030143	Acc: 39.0% (3901/10000)
[Test]  Epoch: 32	Loss: 0.030169	Acc: 38.8% (3882/10000)
[Test]  Epoch: 33	Loss: 0.030019	Acc: 39.2% (3916/10000)
[Test]  Epoch: 34	Loss: 0.030491	Acc: 38.1% (3811/10000)
[Test]  Epoch: 35	Loss: 0.031197	Acc: 38.2% (3824/10000)
[Test]  Epoch: 36	Loss: 0.030374	Acc: 38.7% (3868/10000)
[Test]  Epoch: 37	Loss: 0.030143	Acc: 39.2% (3918/10000)
[Test]  Epoch: 38	Loss: 0.030968	Acc: 37.9% (3788/10000)
[Test]  Epoch: 39	Loss: 0.030570	Acc: 39.0% (3896/10000)
[Test]  Epoch: 40	Loss: 0.029984	Acc: 39.0% (3899/10000)
[Test]  Epoch: 41	Loss: 0.030236	Acc: 39.1% (3907/10000)
[Test]  Epoch: 42	Loss: 0.030533	Acc: 38.7% (3869/10000)
[Test]  Epoch: 43	Loss: 0.029509	Acc: 39.6% (3956/10000)
[Test]  Epoch: 44	Loss: 0.029464	Acc: 39.7% (3971/10000)
[Test]  Epoch: 45	Loss: 0.029610	Acc: 40.0% (4002/10000)
[Test]  Epoch: 46	Loss: 0.029449	Acc: 40.0% (3999/10000)
[Test]  Epoch: 47	Loss: 0.029497	Acc: 38.9% (3891/10000)
[Test]  Epoch: 48	Loss: 0.030058	Acc: 39.4% (3941/10000)
[Test]  Epoch: 49	Loss: 0.029356	Acc: 39.8% (3980/10000)
[Test]  Epoch: 50	Loss: 0.029495	Acc: 39.2% (3925/10000)
[Test]  Epoch: 51	Loss: 0.029509	Acc: 39.4% (3940/10000)
[Test]  Epoch: 52	Loss: 0.029631	Acc: 40.0% (3998/10000)
[Test]  Epoch: 53	Loss: 0.029283	Acc: 39.6% (3957/10000)
[Test]  Epoch: 54	Loss: 0.029365	Acc: 39.9% (3992/10000)
[Test]  Epoch: 55	Loss: 0.029335	Acc: 40.0% (3998/10000)
[Test]  Epoch: 56	Loss: 0.029467	Acc: 39.0% (3905/10000)
[Test]  Epoch: 57	Loss: 0.029393	Acc: 39.5% (3955/10000)
[Test]  Epoch: 58	Loss: 0.029375	Acc: 40.2% (4025/10000)
[Test]  Epoch: 59	Loss: 0.029247	Acc: 40.1% (4010/10000)
[Test]  Epoch: 60	Loss: 0.029469	Acc: 39.9% (3988/10000)
[Test]  Epoch: 61	Loss: 0.029245	Acc: 40.5% (4051/10000)
[Test]  Epoch: 62	Loss: 0.029113	Acc: 40.4% (4041/10000)
[Test]  Epoch: 63	Loss: 0.029057	Acc: 40.2% (4016/10000)
[Test]  Epoch: 64	Loss: 0.029089	Acc: 40.1% (4012/10000)
[Test]  Epoch: 65	Loss: 0.029012	Acc: 40.2% (4020/10000)
[Test]  Epoch: 66	Loss: 0.028986	Acc: 40.3% (4033/10000)
[Test]  Epoch: 67	Loss: 0.029044	Acc: 40.2% (4017/10000)
[Test]  Epoch: 68	Loss: 0.029020	Acc: 40.3% (4030/10000)
[Test]  Epoch: 69	Loss: 0.029093	Acc: 40.2% (4022/10000)
[Test]  Epoch: 70	Loss: 0.029053	Acc: 40.2% (4020/10000)
[Test]  Epoch: 71	Loss: 0.029046	Acc: 40.3% (4027/10000)
[Test]  Epoch: 72	Loss: 0.029092	Acc: 40.2% (4017/10000)
[Test]  Epoch: 73	Loss: 0.028972	Acc: 40.1% (4014/10000)
[Test]  Epoch: 74	Loss: 0.028929	Acc: 40.2% (4023/10000)
[Test]  Epoch: 75	Loss: 0.029005	Acc: 40.4% (4035/10000)
[Test]  Epoch: 76	Loss: 0.028978	Acc: 40.3% (4032/10000)
[Test]  Epoch: 77	Loss: 0.028970	Acc: 40.4% (4039/10000)
[Test]  Epoch: 78	Loss: 0.028916	Acc: 40.5% (4049/10000)
[Test]  Epoch: 79	Loss: 0.028979	Acc: 40.3% (4034/10000)
[Test]  Epoch: 80	Loss: 0.028949	Acc: 40.3% (4034/10000)
[Test]  Epoch: 81	Loss: 0.029013	Acc: 40.3% (4030/10000)
[Test]  Epoch: 82	Loss: 0.028988	Acc: 40.4% (4043/10000)
[Test]  Epoch: 83	Loss: 0.028987	Acc: 40.5% (4052/10000)
[Test]  Epoch: 84	Loss: 0.028990	Acc: 40.4% (4037/10000)
[Test]  Epoch: 85	Loss: 0.028986	Acc: 40.3% (4028/10000)
[Test]  Epoch: 86	Loss: 0.029020	Acc: 40.4% (4042/10000)
[Test]  Epoch: 87	Loss: 0.028998	Acc: 40.4% (4039/10000)
[Test]  Epoch: 88	Loss: 0.029011	Acc: 40.2% (4019/10000)
[Test]  Epoch: 89	Loss: 0.029082	Acc: 40.4% (4039/10000)
[Test]  Epoch: 90	Loss: 0.029032	Acc: 40.1% (4013/10000)
[Test]  Epoch: 91	Loss: 0.028975	Acc: 40.5% (4055/10000)
[Test]  Epoch: 92	Loss: 0.028972	Acc: 40.4% (4036/10000)
[Test]  Epoch: 93	Loss: 0.028940	Acc: 40.4% (4040/10000)
[Test]  Epoch: 94	Loss: 0.029106	Acc: 40.4% (4041/10000)
[Test]  Epoch: 95	Loss: 0.029086	Acc: 40.3% (4033/10000)
[Test]  Epoch: 96	Loss: 0.029123	Acc: 40.4% (4038/10000)
[Test]  Epoch: 97	Loss: 0.028991	Acc: 40.5% (4046/10000)
[Test]  Epoch: 98	Loss: 0.029063	Acc: 40.5% (4047/10000)
[Test]  Epoch: 99	Loss: 0.028973	Acc: 40.3% (4034/10000)
[Test]  Epoch: 100	Loss: 0.029043	Acc: 40.5% (4052/10000)
===========finish==========
['2024-08-19', '01:46:37.834069', '100', 'test', '0.029043374156951903', '40.52', '40.55']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.1.1.weight', '_features.11.conv.3.weight', '_features.9.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.3.weight', '_features.0.0.weight', '_features.11.conv.1.1.weight', '_features.15.conv.3.weight', '_features.9.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.1.conv.1.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.128380	Acc: 22.9% (2285/10000)
[Test]  Epoch: 2	Loss: 0.031938	Acc: 29.9% (2986/10000)
[Test]  Epoch: 3	Loss: 0.028595	Acc: 32.9% (3291/10000)
[Test]  Epoch: 4	Loss: 0.028179	Acc: 35.6% (3560/10000)
[Test]  Epoch: 5	Loss: 0.030127	Acc: 35.8% (3583/10000)
[Test]  Epoch: 6	Loss: 0.030350	Acc: 36.2% (3619/10000)
[Test]  Epoch: 7	Loss: 0.030199	Acc: 35.7% (3568/10000)
[Test]  Epoch: 8	Loss: 0.031881	Acc: 36.0% (3604/10000)
[Test]  Epoch: 9	Loss: 0.030586	Acc: 37.7% (3767/10000)
[Test]  Epoch: 10	Loss: 0.033108	Acc: 35.2% (3521/10000)
[Test]  Epoch: 11	Loss: 0.031116	Acc: 38.5% (3845/10000)
[Test]  Epoch: 12	Loss: 0.032403	Acc: 36.4% (3642/10000)
[Test]  Epoch: 13	Loss: 0.030796	Acc: 39.0% (3899/10000)
[Test]  Epoch: 14	Loss: 0.031378	Acc: 38.1% (3810/10000)
[Test]  Epoch: 15	Loss: 0.031429	Acc: 38.2% (3817/10000)
[Test]  Epoch: 16	Loss: 0.031761	Acc: 37.9% (3787/10000)
[Test]  Epoch: 17	Loss: 0.030963	Acc: 38.6% (3865/10000)
[Test]  Epoch: 18	Loss: 0.030048	Acc: 39.5% (3950/10000)
[Test]  Epoch: 19	Loss: 0.029280	Acc: 40.5% (4050/10000)
[Test]  Epoch: 20	Loss: 0.029189	Acc: 41.4% (4137/10000)
[Test]  Epoch: 21	Loss: 0.030903	Acc: 37.7% (3770/10000)
[Test]  Epoch: 22	Loss: 0.030621	Acc: 39.4% (3936/10000)
[Test]  Epoch: 23	Loss: 0.028892	Acc: 40.3% (4028/10000)
[Test]  Epoch: 24	Loss: 0.029090	Acc: 41.0% (4102/10000)
[Test]  Epoch: 25	Loss: 0.029101	Acc: 40.6% (4057/10000)
[Test]  Epoch: 26	Loss: 0.029057	Acc: 40.8% (4079/10000)
[Test]  Epoch: 27	Loss: 0.029411	Acc: 39.1% (3913/10000)
[Test]  Epoch: 28	Loss: 0.028549	Acc: 39.8% (3978/10000)
[Test]  Epoch: 29	Loss: 0.027990	Acc: 41.7% (4166/10000)
[Test]  Epoch: 30	Loss: 0.028336	Acc: 41.6% (4156/10000)
[Test]  Epoch: 31	Loss: 0.028368	Acc: 41.3% (4130/10000)
[Test]  Epoch: 32	Loss: 0.028691	Acc: 41.2% (4124/10000)
[Test]  Epoch: 33	Loss: 0.028115	Acc: 42.1% (4212/10000)
[Test]  Epoch: 34	Loss: 0.028055	Acc: 41.9% (4185/10000)
[Test]  Epoch: 35	Loss: 0.029518	Acc: 39.6% (3961/10000)
[Test]  Epoch: 36	Loss: 0.028156	Acc: 41.5% (4154/10000)
[Test]  Epoch: 37	Loss: 0.028770	Acc: 40.4% (4043/10000)
[Test]  Epoch: 38	Loss: 0.028794	Acc: 41.3% (4126/10000)
[Test]  Epoch: 39	Loss: 0.028222	Acc: 41.5% (4145/10000)
[Test]  Epoch: 40	Loss: 0.028059	Acc: 41.3% (4126/10000)
[Test]  Epoch: 41	Loss: 0.027568	Acc: 42.1% (4207/10000)
[Test]  Epoch: 42	Loss: 0.028806	Acc: 40.7% (4073/10000)
[Test]  Epoch: 43	Loss: 0.027919	Acc: 41.7% (4170/10000)
[Test]  Epoch: 44	Loss: 0.027772	Acc: 41.6% (4160/10000)
[Test]  Epoch: 45	Loss: 0.027686	Acc: 42.1% (4215/10000)
[Test]  Epoch: 46	Loss: 0.028345	Acc: 41.5% (4155/10000)
[Test]  Epoch: 47	Loss: 0.027919	Acc: 42.3% (4231/10000)
[Test]  Epoch: 48	Loss: 0.027622	Acc: 41.9% (4194/10000)
[Test]  Epoch: 49	Loss: 0.027844	Acc: 42.1% (4207/10000)
[Test]  Epoch: 50	Loss: 0.027523	Acc: 42.2% (4223/10000)
[Test]  Epoch: 51	Loss: 0.027632	Acc: 42.4% (4240/10000)
[Test]  Epoch: 52	Loss: 0.027446	Acc: 42.7% (4268/10000)
[Test]  Epoch: 53	Loss: 0.027836	Acc: 42.6% (4256/10000)
[Test]  Epoch: 54	Loss: 0.027975	Acc: 41.6% (4165/10000)
[Test]  Epoch: 55	Loss: 0.027684	Acc: 42.3% (4233/10000)
[Test]  Epoch: 56	Loss: 0.027553	Acc: 42.2% (4223/10000)
[Test]  Epoch: 57	Loss: 0.027784	Acc: 42.7% (4271/10000)
[Test]  Epoch: 58	Loss: 0.027843	Acc: 42.3% (4226/10000)
[Test]  Epoch: 59	Loss: 0.027733	Acc: 42.2% (4219/10000)
[Test]  Epoch: 60	Loss: 0.027739	Acc: 43.0% (4302/10000)
[Test]  Epoch: 61	Loss: 0.027408	Acc: 42.9% (4285/10000)
[Test]  Epoch: 62	Loss: 0.027354	Acc: 42.9% (4293/10000)
[Test]  Epoch: 63	Loss: 0.027240	Acc: 43.0% (4296/10000)
[Test]  Epoch: 64	Loss: 0.027248	Acc: 43.0% (4305/10000)
[Test]  Epoch: 65	Loss: 0.027230	Acc: 43.0% (4304/10000)
[Test]  Epoch: 66	Loss: 0.027280	Acc: 42.8% (4282/10000)
[Test]  Epoch: 67	Loss: 0.027325	Acc: 42.8% (4281/10000)
[Test]  Epoch: 68	Loss: 0.027284	Acc: 42.9% (4290/10000)
[Test]  Epoch: 69	Loss: 0.027406	Acc: 43.0% (4297/10000)
[Test]  Epoch: 70	Loss: 0.027323	Acc: 42.7% (4272/10000)
[Test]  Epoch: 71	Loss: 0.027292	Acc: 42.6% (4265/10000)
[Test]  Epoch: 72	Loss: 0.027267	Acc: 42.8% (4282/10000)
[Test]  Epoch: 73	Loss: 0.027308	Acc: 42.7% (4268/10000)
[Test]  Epoch: 74	Loss: 0.027175	Acc: 42.9% (4288/10000)
[Test]  Epoch: 75	Loss: 0.027194	Acc: 42.9% (4288/10000)
[Test]  Epoch: 76	Loss: 0.027241	Acc: 42.9% (4293/10000)
[Test]  Epoch: 77	Loss: 0.027294	Acc: 42.6% (4265/10000)
[Test]  Epoch: 78	Loss: 0.027323	Acc: 42.8% (4281/10000)
[Test]  Epoch: 79	Loss: 0.027309	Acc: 42.9% (4287/10000)
[Test]  Epoch: 80	Loss: 0.027292	Acc: 42.9% (4291/10000)
[Test]  Epoch: 81	Loss: 0.027364	Acc: 42.6% (4259/10000)
[Test]  Epoch: 82	Loss: 0.027379	Acc: 42.6% (4256/10000)
[Test]  Epoch: 83	Loss: 0.027328	Acc: 42.6% (4264/10000)
[Test]  Epoch: 84	Loss: 0.027290	Acc: 42.8% (4279/10000)
[Test]  Epoch: 85	Loss: 0.027344	Acc: 42.8% (4277/10000)
[Test]  Epoch: 86	Loss: 0.027301	Acc: 42.9% (4289/10000)
[Test]  Epoch: 87	Loss: 0.027392	Acc: 42.7% (4273/10000)
[Test]  Epoch: 88	Loss: 0.027359	Acc: 42.7% (4270/10000)
[Test]  Epoch: 89	Loss: 0.027371	Acc: 42.6% (4263/10000)
[Test]  Epoch: 90	Loss: 0.027369	Acc: 42.8% (4284/10000)
[Test]  Epoch: 91	Loss: 0.027279	Acc: 42.9% (4289/10000)
[Test]  Epoch: 92	Loss: 0.027304	Acc: 42.7% (4273/10000)
[Test]  Epoch: 93	Loss: 0.027312	Acc: 43.1% (4309/10000)
[Test]  Epoch: 94	Loss: 0.027365	Acc: 42.8% (4278/10000)
[Test]  Epoch: 95	Loss: 0.027371	Acc: 42.7% (4271/10000)
[Test]  Epoch: 96	Loss: 0.027315	Acc: 43.0% (4297/10000)
[Test]  Epoch: 97	Loss: 0.027298	Acc: 43.0% (4300/10000)
[Test]  Epoch: 98	Loss: 0.027433	Acc: 42.7% (4267/10000)
[Test]  Epoch: 99	Loss: 0.027336	Acc: 42.7% (4266/10000)
[Test]  Epoch: 100	Loss: 0.027285	Acc: 42.7% (4266/10000)
===========finish==========
['2024-08-19', '01:48:45.149800', '100', 'test', '0.027285318958759307', '42.66', '43.09']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.1.1.weight', '_features.11.conv.3.weight', '_features.9.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.3.weight', '_features.0.0.weight', '_features.11.conv.1.1.weight', '_features.15.conv.3.weight', '_features.9.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.1.conv.1.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.2.conv.2.weight', '_features.6.conv.0.0.weight', '_features.3.conv.2.weight', '_features.11.conv.1.0.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.190002	Acc: 14.1% (1405/10000)
[Test]  Epoch: 2	Loss: 0.035405	Acc: 30.8% (3082/10000)
[Test]  Epoch: 3	Loss: 0.027369	Acc: 37.9% (3794/10000)
[Test]  Epoch: 4	Loss: 0.029880	Acc: 37.1% (3706/10000)
[Test]  Epoch: 5	Loss: 0.030228	Acc: 39.5% (3952/10000)
[Test]  Epoch: 6	Loss: 0.026522	Acc: 43.5% (4351/10000)
[Test]  Epoch: 7	Loss: 0.026670	Acc: 43.0% (4304/10000)
[Test]  Epoch: 8	Loss: 0.032571	Acc: 39.1% (3907/10000)
[Test]  Epoch: 9	Loss: 0.029045	Acc: 42.3% (4229/10000)
[Test]  Epoch: 10	Loss: 0.025898	Acc: 44.9% (4491/10000)
[Test]  Epoch: 11	Loss: 0.026535	Acc: 44.4% (4437/10000)
[Test]  Epoch: 12	Loss: 0.027371	Acc: 42.9% (4288/10000)
[Test]  Epoch: 13	Loss: 0.026912	Acc: 43.6% (4365/10000)
[Test]  Epoch: 14	Loss: 0.026231	Acc: 44.8% (4475/10000)
[Test]  Epoch: 15	Loss: 0.025678	Acc: 45.9% (4585/10000)
[Test]  Epoch: 16	Loss: 0.026027	Acc: 45.1% (4512/10000)
[Test]  Epoch: 17	Loss: 0.025965	Acc: 45.2% (4524/10000)
[Test]  Epoch: 18	Loss: 0.025456	Acc: 46.7% (4673/10000)
[Test]  Epoch: 19	Loss: 0.025088	Acc: 46.7% (4673/10000)
[Test]  Epoch: 20	Loss: 0.024819	Acc: 47.0% (4696/10000)
[Test]  Epoch: 21	Loss: 0.025143	Acc: 46.5% (4649/10000)
[Test]  Epoch: 22	Loss: 0.024971	Acc: 46.8% (4681/10000)
[Test]  Epoch: 23	Loss: 0.025102	Acc: 46.3% (4632/10000)
[Test]  Epoch: 24	Loss: 0.024891	Acc: 46.7% (4674/10000)
[Test]  Epoch: 25	Loss: 0.024988	Acc: 47.3% (4733/10000)
[Test]  Epoch: 26	Loss: 0.024292	Acc: 47.5% (4748/10000)
[Test]  Epoch: 27	Loss: 0.024590	Acc: 47.2% (4716/10000)
[Test]  Epoch: 28	Loss: 0.024475	Acc: 47.6% (4756/10000)
[Test]  Epoch: 29	Loss: 0.024766	Acc: 46.4% (4644/10000)
[Test]  Epoch: 30	Loss: 0.024479	Acc: 47.7% (4772/10000)
[Test]  Epoch: 31	Loss: 0.024631	Acc: 47.6% (4760/10000)
[Test]  Epoch: 32	Loss: 0.024573	Acc: 47.3% (4730/10000)
[Test]  Epoch: 33	Loss: 0.024310	Acc: 48.1% (4806/10000)
[Test]  Epoch: 34	Loss: 0.024660	Acc: 47.0% (4696/10000)
[Test]  Epoch: 35	Loss: 0.024151	Acc: 47.4% (4741/10000)
[Test]  Epoch: 36	Loss: 0.024605	Acc: 47.6% (4756/10000)
[Test]  Epoch: 37	Loss: 0.024179	Acc: 47.9% (4785/10000)
[Test]  Epoch: 38	Loss: 0.026025	Acc: 44.9% (4492/10000)
[Test]  Epoch: 39	Loss: 0.024793	Acc: 47.4% (4737/10000)
[Test]  Epoch: 40	Loss: 0.024424	Acc: 47.5% (4749/10000)
[Test]  Epoch: 41	Loss: 0.024229	Acc: 48.1% (4810/10000)
[Test]  Epoch: 42	Loss: 0.024491	Acc: 47.2% (4718/10000)
[Test]  Epoch: 43	Loss: 0.024033	Acc: 48.2% (4822/10000)
[Test]  Epoch: 44	Loss: 0.024027	Acc: 48.6% (4858/10000)
[Test]  Epoch: 45	Loss: 0.024128	Acc: 47.9% (4791/10000)
[Test]  Epoch: 46	Loss: 0.024168	Acc: 47.8% (4781/10000)
[Test]  Epoch: 47	Loss: 0.024347	Acc: 47.7% (4768/10000)
[Test]  Epoch: 48	Loss: 0.024407	Acc: 48.0% (4802/10000)
[Test]  Epoch: 49	Loss: 0.023927	Acc: 48.4% (4839/10000)
[Test]  Epoch: 50	Loss: 0.024060	Acc: 47.9% (4785/10000)
[Test]  Epoch: 51	Loss: 0.024100	Acc: 48.5% (4850/10000)
[Test]  Epoch: 52	Loss: 0.023855	Acc: 48.4% (4835/10000)
[Test]  Epoch: 53	Loss: 0.023915	Acc: 48.5% (4851/10000)
[Test]  Epoch: 54	Loss: 0.023759	Acc: 48.8% (4879/10000)
[Test]  Epoch: 55	Loss: 0.023837	Acc: 48.3% (4830/10000)
[Test]  Epoch: 56	Loss: 0.023870	Acc: 48.2% (4821/10000)
[Test]  Epoch: 57	Loss: 0.023814	Acc: 48.4% (4844/10000)
[Test]  Epoch: 58	Loss: 0.023625	Acc: 48.0% (4803/10000)
[Test]  Epoch: 59	Loss: 0.023766	Acc: 48.3% (4833/10000)
[Test]  Epoch: 60	Loss: 0.023809	Acc: 48.2% (4819/10000)
[Test]  Epoch: 61	Loss: 0.023653	Acc: 48.5% (4849/10000)
[Test]  Epoch: 62	Loss: 0.023669	Acc: 48.3% (4833/10000)
[Test]  Epoch: 63	Loss: 0.023572	Acc: 48.7% (4867/10000)
[Test]  Epoch: 64	Loss: 0.023549	Acc: 48.5% (4850/10000)
[Test]  Epoch: 65	Loss: 0.023522	Acc: 48.5% (4854/10000)
[Test]  Epoch: 66	Loss: 0.023526	Acc: 48.5% (4849/10000)
[Test]  Epoch: 67	Loss: 0.023707	Acc: 48.3% (4828/10000)
[Test]  Epoch: 68	Loss: 0.023502	Acc: 48.8% (4878/10000)
[Test]  Epoch: 69	Loss: 0.023590	Acc: 48.9% (4886/10000)
[Test]  Epoch: 70	Loss: 0.023526	Acc: 48.8% (4880/10000)
[Test]  Epoch: 71	Loss: 0.023649	Acc: 48.4% (4841/10000)
[Test]  Epoch: 72	Loss: 0.023646	Acc: 48.5% (4846/10000)
[Test]  Epoch: 73	Loss: 0.023528	Acc: 48.4% (4841/10000)
[Test]  Epoch: 74	Loss: 0.023467	Acc: 48.6% (4865/10000)
[Test]  Epoch: 75	Loss: 0.023442	Acc: 48.8% (4883/10000)
[Test]  Epoch: 76	Loss: 0.023483	Acc: 48.8% (4878/10000)
[Test]  Epoch: 77	Loss: 0.023498	Acc: 48.8% (4878/10000)
[Test]  Epoch: 78	Loss: 0.023517	Acc: 48.9% (4889/10000)
[Test]  Epoch: 79	Loss: 0.023501	Acc: 48.9% (4890/10000)
[Test]  Epoch: 80	Loss: 0.023522	Acc: 48.7% (4866/10000)
[Test]  Epoch: 81	Loss: 0.023588	Acc: 48.6% (4864/10000)
[Test]  Epoch: 82	Loss: 0.023574	Acc: 48.7% (4870/10000)
[Test]  Epoch: 83	Loss: 0.023543	Acc: 48.9% (4887/10000)
[Test]  Epoch: 84	Loss: 0.023451	Acc: 48.9% (4886/10000)
[Test]  Epoch: 85	Loss: 0.023519	Acc: 48.8% (4879/10000)
[Test]  Epoch: 86	Loss: 0.023498	Acc: 49.0% (4901/10000)
[Test]  Epoch: 87	Loss: 0.023490	Acc: 48.9% (4894/10000)
[Test]  Epoch: 88	Loss: 0.023487	Acc: 48.8% (4884/10000)
[Test]  Epoch: 89	Loss: 0.023549	Acc: 49.0% (4898/10000)
[Test]  Epoch: 90	Loss: 0.023534	Acc: 48.9% (4885/10000)
[Test]  Epoch: 91	Loss: 0.023541	Acc: 48.7% (4872/10000)
[Test]  Epoch: 92	Loss: 0.023484	Acc: 48.8% (4883/10000)
[Test]  Epoch: 93	Loss: 0.023510	Acc: 49.0% (4897/10000)
[Test]  Epoch: 94	Loss: 0.023492	Acc: 48.9% (4885/10000)
[Test]  Epoch: 95	Loss: 0.023463	Acc: 48.9% (4886/10000)
[Test]  Epoch: 96	Loss: 0.023488	Acc: 48.9% (4890/10000)
[Test]  Epoch: 97	Loss: 0.023496	Acc: 49.0% (4903/10000)
[Test]  Epoch: 98	Loss: 0.023626	Acc: 48.7% (4866/10000)
[Test]  Epoch: 99	Loss: 0.023484	Acc: 48.7% (4873/10000)
[Test]  Epoch: 100	Loss: 0.023486	Acc: 48.6% (4859/10000)
===========finish==========
['2024-08-19', '01:50:53.410138', '100', 'test', '0.023485855734348297', '48.59', '49.03']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.1.1.weight', '_features.11.conv.3.weight', '_features.9.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.3.weight', '_features.0.0.weight', '_features.11.conv.1.1.weight', '_features.15.conv.3.weight', '_features.9.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.1.conv.1.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.2.conv.2.weight', '_features.6.conv.0.0.weight', '_features.3.conv.2.weight', '_features.11.conv.1.0.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.6.conv.2.weight', '_features.8.conv.0.0.weight', '_features.4.conv.2.weight', '_features.18.1.weight', '_features.10.conv.0.0.weight', '_features.7.conv.2.weight', '_features.8.conv.2.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.17.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.655285	Acc: 10.1% (1012/10000)
[Test]  Epoch: 2	Loss: 0.062799	Acc: 16.9% (1685/10000)
[Test]  Epoch: 3	Loss: 0.036699	Acc: 19.2% (1919/10000)
[Test]  Epoch: 4	Loss: 0.048097	Acc: 19.6% (1959/10000)
[Test]  Epoch: 5	Loss: 0.033988	Acc: 23.1% (2311/10000)
[Test]  Epoch: 6	Loss: 0.032141	Acc: 23.4% (2341/10000)
[Test]  Epoch: 7	Loss: 0.051704	Acc: 19.6% (1958/10000)
[Test]  Epoch: 8	Loss: 0.035931	Acc: 24.7% (2473/10000)
[Test]  Epoch: 9	Loss: 0.033755	Acc: 26.4% (2643/10000)
[Test]  Epoch: 10	Loss: 0.034259	Acc: 26.0% (2603/10000)
[Test]  Epoch: 11	Loss: 0.031368	Acc: 28.5% (2849/10000)
[Test]  Epoch: 12	Loss: 0.030446	Acc: 29.6% (2963/10000)
[Test]  Epoch: 13	Loss: 0.030000	Acc: 30.4% (3045/10000)
[Test]  Epoch: 14	Loss: 0.030232	Acc: 29.9% (2991/10000)
[Test]  Epoch: 15	Loss: 0.030500	Acc: 31.6% (3155/10000)
[Test]  Epoch: 16	Loss: 0.030584	Acc: 30.2% (3022/10000)
[Test]  Epoch: 17	Loss: 0.029525	Acc: 31.5% (3149/10000)
[Test]  Epoch: 18	Loss: 0.031040	Acc: 30.6% (3056/10000)
[Test]  Epoch: 19	Loss: 0.031326	Acc: 30.2% (3019/10000)
[Test]  Epoch: 20	Loss: 0.030869	Acc: 32.7% (3270/10000)
[Test]  Epoch: 21	Loss: 0.032355	Acc: 29.5% (2946/10000)
[Test]  Epoch: 22	Loss: 0.032017	Acc: 32.5% (3252/10000)
[Test]  Epoch: 23	Loss: 0.033333	Acc: 29.9% (2991/10000)
[Test]  Epoch: 24	Loss: 0.032241	Acc: 32.1% (3211/10000)
[Test]  Epoch: 25	Loss: 0.032698	Acc: 28.9% (2888/10000)
[Test]  Epoch: 26	Loss: 0.029542	Acc: 32.2% (3224/10000)
[Test]  Epoch: 27	Loss: 0.029357	Acc: 32.0% (3205/10000)
[Test]  Epoch: 28	Loss: 0.030567	Acc: 31.6% (3156/10000)
[Test]  Epoch: 29	Loss: 0.029806	Acc: 34.2% (3425/10000)
[Test]  Epoch: 30	Loss: 0.032433	Acc: 31.0% (3104/10000)
[Test]  Epoch: 31	Loss: 0.032078	Acc: 33.8% (3378/10000)
[Test]  Epoch: 32	Loss: 0.032736	Acc: 34.0% (3404/10000)
[Test]  Epoch: 33	Loss: 0.032036	Acc: 33.9% (3386/10000)
[Test]  Epoch: 34	Loss: 0.031562	Acc: 33.7% (3367/10000)
[Test]  Epoch: 35	Loss: 0.032803	Acc: 33.9% (3392/10000)
[Test]  Epoch: 36	Loss: 0.032492	Acc: 33.5% (3346/10000)
[Test]  Epoch: 37	Loss: 0.033074	Acc: 33.2% (3325/10000)
[Test]  Epoch: 38	Loss: 0.034112	Acc: 33.4% (3344/10000)
[Test]  Epoch: 39	Loss: 0.032714	Acc: 34.7% (3470/10000)
[Test]  Epoch: 40	Loss: 0.033889	Acc: 35.7% (3573/10000)
[Test]  Epoch: 41	Loss: 0.032381	Acc: 34.9% (3491/10000)
[Test]  Epoch: 42	Loss: 0.034112	Acc: 35.0% (3496/10000)
[Test]  Epoch: 43	Loss: 0.033219	Acc: 34.8% (3483/10000)
[Test]  Epoch: 44	Loss: 0.033899	Acc: 35.6% (3561/10000)
[Test]  Epoch: 45	Loss: 0.032683	Acc: 36.3% (3628/10000)
[Test]  Epoch: 46	Loss: 0.034183	Acc: 34.3% (3428/10000)
[Test]  Epoch: 47	Loss: 0.034052	Acc: 35.0% (3504/10000)
[Test]  Epoch: 48	Loss: 0.032987	Acc: 35.3% (3527/10000)
[Test]  Epoch: 49	Loss: 0.033093	Acc: 35.3% (3530/10000)
[Test]  Epoch: 50	Loss: 0.033554	Acc: 34.7% (3472/10000)
[Test]  Epoch: 51	Loss: 0.033083	Acc: 35.7% (3568/10000)
[Test]  Epoch: 52	Loss: 0.033216	Acc: 36.5% (3654/10000)
[Test]  Epoch: 53	Loss: 0.033836	Acc: 34.5% (3452/10000)
[Test]  Epoch: 54	Loss: 0.034728	Acc: 34.7% (3473/10000)
[Test]  Epoch: 55	Loss: 0.033317	Acc: 35.6% (3564/10000)
[Test]  Epoch: 56	Loss: 0.032771	Acc: 35.9% (3586/10000)
[Test]  Epoch: 57	Loss: 0.032324	Acc: 36.0% (3598/10000)
[Test]  Epoch: 58	Loss: 0.032389	Acc: 35.3% (3534/10000)
[Test]  Epoch: 59	Loss: 0.033502	Acc: 35.5% (3551/10000)
[Test]  Epoch: 60	Loss: 0.032986	Acc: 36.1% (3609/10000)
[Test]  Epoch: 61	Loss: 0.032368	Acc: 36.6% (3663/10000)
[Test]  Epoch: 62	Loss: 0.032151	Acc: 36.7% (3672/10000)
[Test]  Epoch: 63	Loss: 0.031936	Acc: 36.6% (3662/10000)
[Test]  Epoch: 64	Loss: 0.032170	Acc: 36.4% (3639/10000)
[Test]  Epoch: 65	Loss: 0.031972	Acc: 36.6% (3663/10000)
[Test]  Epoch: 66	Loss: 0.031846	Acc: 36.4% (3639/10000)
[Test]  Epoch: 67	Loss: 0.032123	Acc: 36.1% (3613/10000)
[Test]  Epoch: 68	Loss: 0.032151	Acc: 36.6% (3658/10000)
[Test]  Epoch: 69	Loss: 0.032193	Acc: 36.7% (3668/10000)
[Test]  Epoch: 70	Loss: 0.032129	Acc: 36.6% (3662/10000)
[Test]  Epoch: 71	Loss: 0.032300	Acc: 36.1% (3615/10000)
[Test]  Epoch: 72	Loss: 0.032183	Acc: 36.5% (3654/10000)
[Test]  Epoch: 73	Loss: 0.031934	Acc: 36.6% (3664/10000)
[Test]  Epoch: 74	Loss: 0.032120	Acc: 36.5% (3654/10000)
[Test]  Epoch: 75	Loss: 0.032236	Acc: 36.7% (3672/10000)
[Test]  Epoch: 76	Loss: 0.031896	Acc: 36.3% (3629/10000)
[Test]  Epoch: 77	Loss: 0.031924	Acc: 36.5% (3648/10000)
[Test]  Epoch: 78	Loss: 0.031994	Acc: 36.6% (3662/10000)
[Test]  Epoch: 79	Loss: 0.031947	Acc: 36.5% (3648/10000)
[Test]  Epoch: 80	Loss: 0.032125	Acc: 36.0% (3601/10000)
[Test]  Epoch: 81	Loss: 0.031848	Acc: 36.6% (3663/10000)
[Test]  Epoch: 82	Loss: 0.032026	Acc: 36.6% (3656/10000)
[Test]  Epoch: 83	Loss: 0.031984	Acc: 36.4% (3635/10000)
[Test]  Epoch: 84	Loss: 0.031925	Acc: 36.5% (3649/10000)
[Test]  Epoch: 85	Loss: 0.031976	Acc: 36.4% (3642/10000)
[Test]  Epoch: 86	Loss: 0.032097	Acc: 36.4% (3637/10000)
[Test]  Epoch: 87	Loss: 0.032055	Acc: 36.4% (3639/10000)
[Test]  Epoch: 88	Loss: 0.031963	Acc: 36.5% (3650/10000)
[Test]  Epoch: 89	Loss: 0.031979	Acc: 36.6% (3656/10000)
[Test]  Epoch: 90	Loss: 0.032004	Acc: 36.5% (3655/10000)
[Test]  Epoch: 91	Loss: 0.031885	Acc: 36.4% (3639/10000)
[Test]  Epoch: 92	Loss: 0.032028	Acc: 36.5% (3650/10000)
[Test]  Epoch: 93	Loss: 0.031898	Acc: 36.5% (3655/10000)
[Test]  Epoch: 94	Loss: 0.031859	Acc: 36.7% (3674/10000)
[Test]  Epoch: 95	Loss: 0.031869	Acc: 36.8% (3675/10000)
[Test]  Epoch: 96	Loss: 0.031862	Acc: 36.3% (3630/10000)
[Test]  Epoch: 97	Loss: 0.031961	Acc: 36.4% (3644/10000)
[Test]  Epoch: 98	Loss: 0.032043	Acc: 36.1% (3613/10000)
[Test]  Epoch: 99	Loss: 0.032215	Acc: 36.1% (3607/10000)
[Test]  Epoch: 100	Loss: 0.031928	Acc: 36.3% (3630/10000)
===========finish==========
['2024-08-19', '01:53:00.342719', '100', 'test', '0.0319284286737442', '36.3', '36.75']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.1.1.weight', '_features.11.conv.3.weight', '_features.9.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.3.weight', '_features.0.0.weight', '_features.11.conv.1.1.weight', '_features.15.conv.3.weight', '_features.9.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.1.conv.1.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.2.conv.2.weight', '_features.6.conv.0.0.weight', '_features.3.conv.2.weight', '_features.11.conv.1.0.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.6.conv.2.weight', '_features.8.conv.0.0.weight', '_features.4.conv.2.weight', '_features.18.1.weight', '_features.10.conv.0.0.weight', '_features.7.conv.2.weight', '_features.8.conv.2.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.13.conv.2.weight', '_features.10.conv.2.weight', '_features.12.conv.2.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.17.conv.0.1.weight', '_features.11.conv.0.0.weight', '_features.11.conv.2.weight', '_features.14.conv.0.0.weight', '_features.15.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.603178	Acc: 12.5% (1250/10000)
[Test]  Epoch: 2	Loss: 0.232086	Acc: 13.9% (1388/10000)
[Test]  Epoch: 3	Loss: 0.032946	Acc: 24.5% (2446/10000)
[Test]  Epoch: 4	Loss: 0.031211	Acc: 25.8% (2584/10000)
[Test]  Epoch: 5	Loss: 0.029982	Acc: 28.9% (2886/10000)
[Test]  Epoch: 6	Loss: 0.030736	Acc: 29.8% (2984/10000)
[Test]  Epoch: 7	Loss: 0.030262	Acc: 31.8% (3180/10000)
[Test]  Epoch: 8	Loss: 0.033762	Acc: 28.0% (2803/10000)
[Test]  Epoch: 9	Loss: 0.029578	Acc: 32.2% (3220/10000)
[Test]  Epoch: 10	Loss: 0.028540	Acc: 35.4% (3541/10000)
[Test]  Epoch: 11	Loss: 0.029034	Acc: 35.3% (3532/10000)
[Test]  Epoch: 12	Loss: 0.029662	Acc: 33.6% (3360/10000)
[Test]  Epoch: 13	Loss: 0.030356	Acc: 35.3% (3529/10000)
[Test]  Epoch: 14	Loss: 0.032273	Acc: 33.3% (3330/10000)
[Test]  Epoch: 15	Loss: 0.032286	Acc: 32.2% (3218/10000)
[Test]  Epoch: 16	Loss: 0.031559	Acc: 33.5% (3349/10000)
[Test]  Epoch: 17	Loss: 0.031236	Acc: 35.1% (3512/10000)
[Test]  Epoch: 18	Loss: 0.032412	Acc: 35.1% (3509/10000)
[Test]  Epoch: 19	Loss: 0.032374	Acc: 35.0% (3495/10000)
[Test]  Epoch: 20	Loss: 0.032232	Acc: 35.8% (3584/10000)
[Test]  Epoch: 21	Loss: 0.033885	Acc: 35.4% (3544/10000)
[Test]  Epoch: 22	Loss: 0.033738	Acc: 34.9% (3488/10000)
[Test]  Epoch: 23	Loss: 0.032317	Acc: 35.9% (3587/10000)
[Test]  Epoch: 24	Loss: 0.038541	Acc: 30.9% (3090/10000)
[Test]  Epoch: 25	Loss: 0.032857	Acc: 34.0% (3402/10000)
[Test]  Epoch: 26	Loss: 0.031583	Acc: 36.0% (3599/10000)
[Test]  Epoch: 27	Loss: 0.031434	Acc: 37.0% (3696/10000)
[Test]  Epoch: 28	Loss: 0.032289	Acc: 36.1% (3614/10000)
[Test]  Epoch: 29	Loss: 0.033200	Acc: 36.3% (3634/10000)
[Test]  Epoch: 30	Loss: 0.032405	Acc: 35.9% (3588/10000)
[Test]  Epoch: 31	Loss: 0.032307	Acc: 37.5% (3748/10000)
[Test]  Epoch: 32	Loss: 0.031956	Acc: 36.6% (3658/10000)
[Test]  Epoch: 33	Loss: 0.031918	Acc: 37.4% (3736/10000)
[Test]  Epoch: 34	Loss: 0.031605	Acc: 36.3% (3634/10000)
[Test]  Epoch: 35	Loss: 0.031781	Acc: 35.9% (3594/10000)
[Test]  Epoch: 36	Loss: 0.032043	Acc: 37.0% (3703/10000)
[Test]  Epoch: 37	Loss: 0.031758	Acc: 36.2% (3618/10000)
[Test]  Epoch: 38	Loss: 0.037656	Acc: 29.7% (2968/10000)
[Test]  Epoch: 39	Loss: 0.030813	Acc: 36.6% (3665/10000)
[Test]  Epoch: 40	Loss: 0.032158	Acc: 36.1% (3608/10000)
[Test]  Epoch: 41	Loss: 0.031598	Acc: 37.8% (3780/10000)
[Test]  Epoch: 42	Loss: 0.032163	Acc: 36.2% (3620/10000)
[Test]  Epoch: 43	Loss: 0.031845	Acc: 37.1% (3714/10000)
[Test]  Epoch: 44	Loss: 0.032235	Acc: 37.5% (3752/10000)
[Test]  Epoch: 45	Loss: 0.031403	Acc: 38.2% (3816/10000)
[Test]  Epoch: 46	Loss: 0.032519	Acc: 35.9% (3586/10000)
[Test]  Epoch: 47	Loss: 0.031898	Acc: 36.8% (3677/10000)
[Test]  Epoch: 48	Loss: 0.032088	Acc: 37.5% (3747/10000)
[Test]  Epoch: 49	Loss: 0.031587	Acc: 37.4% (3740/10000)
[Test]  Epoch: 50	Loss: 0.031939	Acc: 36.4% (3639/10000)
[Test]  Epoch: 51	Loss: 0.030923	Acc: 37.9% (3791/10000)
[Test]  Epoch: 52	Loss: 0.031173	Acc: 38.4% (3840/10000)
[Test]  Epoch: 53	Loss: 0.031115	Acc: 38.2% (3823/10000)
[Test]  Epoch: 54	Loss: 0.030969	Acc: 38.2% (3822/10000)
[Test]  Epoch: 55	Loss: 0.031238	Acc: 38.6% (3857/10000)
[Test]  Epoch: 56	Loss: 0.031235	Acc: 38.3% (3829/10000)
[Test]  Epoch: 57	Loss: 0.040313	Acc: 27.7% (2768/10000)
[Test]  Epoch: 58	Loss: 0.029901	Acc: 38.1% (3808/10000)
[Test]  Epoch: 59	Loss: 0.030166	Acc: 38.2% (3825/10000)
[Test]  Epoch: 60	Loss: 0.030922	Acc: 37.4% (3740/10000)
[Test]  Epoch: 61	Loss: 0.030919	Acc: 38.7% (3872/10000)
[Test]  Epoch: 62	Loss: 0.031005	Acc: 38.9% (3892/10000)
[Test]  Epoch: 63	Loss: 0.030840	Acc: 39.0% (3896/10000)
[Test]  Epoch: 64	Loss: 0.031009	Acc: 38.9% (3889/10000)
[Test]  Epoch: 65	Loss: 0.031062	Acc: 39.0% (3898/10000)
[Test]  Epoch: 66	Loss: 0.031230	Acc: 38.8% (3875/10000)
[Test]  Epoch: 67	Loss: 0.031154	Acc: 38.7% (3867/10000)
[Test]  Epoch: 68	Loss: 0.031223	Acc: 39.0% (3896/10000)
[Test]  Epoch: 69	Loss: 0.031241	Acc: 39.1% (3908/10000)
[Test]  Epoch: 70	Loss: 0.031035	Acc: 39.0% (3899/10000)
[Test]  Epoch: 71	Loss: 0.030980	Acc: 38.9% (3888/10000)
[Test]  Epoch: 72	Loss: 0.030787	Acc: 39.1% (3914/10000)
[Test]  Epoch: 73	Loss: 0.031091	Acc: 39.3% (3933/10000)
[Test]  Epoch: 74	Loss: 0.031067	Acc: 39.1% (3911/10000)
[Test]  Epoch: 75	Loss: 0.030984	Acc: 39.3% (3928/10000)
[Test]  Epoch: 76	Loss: 0.031025	Acc: 39.0% (3903/10000)
[Test]  Epoch: 77	Loss: 0.030857	Acc: 39.3% (3927/10000)
[Test]  Epoch: 78	Loss: 0.031006	Acc: 38.9% (3890/10000)
[Test]  Epoch: 79	Loss: 0.031101	Acc: 38.9% (3893/10000)
[Test]  Epoch: 80	Loss: 0.030978	Acc: 38.9% (3886/10000)
[Test]  Epoch: 81	Loss: 0.031126	Acc: 38.8% (3879/10000)
[Test]  Epoch: 82	Loss: 0.031085	Acc: 39.1% (3908/10000)
[Test]  Epoch: 83	Loss: 0.031162	Acc: 38.9% (3891/10000)
[Test]  Epoch: 84	Loss: 0.031014	Acc: 38.9% (3892/10000)
[Test]  Epoch: 85	Loss: 0.031116	Acc: 39.0% (3905/10000)
[Test]  Epoch: 86	Loss: 0.031208	Acc: 38.9% (3889/10000)
[Test]  Epoch: 87	Loss: 0.031190	Acc: 38.7% (3874/10000)
[Test]  Epoch: 88	Loss: 0.031220	Acc: 39.0% (3897/10000)
[Test]  Epoch: 89	Loss: 0.031075	Acc: 38.9% (3891/10000)
[Test]  Epoch: 90	Loss: 0.031047	Acc: 39.0% (3904/10000)
[Test]  Epoch: 91	Loss: 0.030940	Acc: 39.2% (3922/10000)
[Test]  Epoch: 92	Loss: 0.030909	Acc: 39.0% (3904/10000)
[Test]  Epoch: 93	Loss: 0.030902	Acc: 38.9% (3891/10000)
[Test]  Epoch: 94	Loss: 0.031022	Acc: 38.9% (3886/10000)
[Test]  Epoch: 95	Loss: 0.031019	Acc: 38.9% (3888/10000)
[Test]  Epoch: 96	Loss: 0.031085	Acc: 38.8% (3879/10000)
[Test]  Epoch: 97	Loss: 0.030993	Acc: 38.9% (3887/10000)
[Test]  Epoch: 98	Loss: 0.030880	Acc: 38.9% (3894/10000)
[Test]  Epoch: 99	Loss: 0.031105	Acc: 38.6% (3861/10000)
[Test]  Epoch: 100	Loss: 0.030824	Acc: 38.9% (3892/10000)
===========finish==========
['2024-08-19', '01:55:08.114757', '100', 'test', '0.03082374235391617', '38.92', '39.33']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.1.conv.2.weight', '_features.1.conv.0.1.weight', '_features.3.conv.3.weight', '_features.0.1.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.13.conv.3.weight', '_features.10.conv.3.weight', '_features.9.conv.3.weight', '_features.7.conv.3.weight', '_features.12.conv.3.weight', '_features.2.conv.1.1.weight', '_features.5.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.3.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.6.conv.1.1.weight', '_features.11.conv.3.weight', '_features.9.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.12.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.15.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.16.conv.3.weight', '_features.0.0.weight', '_features.11.conv.1.1.weight', '_features.15.conv.3.weight', '_features.9.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.2.conv.1.0.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.0.weight', '_features.14.conv.3.weight', '_features.5.conv.1.0.weight', '_features.16.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.1.conv.1.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.5.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.16.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.4.conv.0.0.weight', '_features.12.conv.1.0.weight', '_features.2.conv.2.weight', '_features.6.conv.0.0.weight', '_features.3.conv.2.weight', '_features.11.conv.1.0.weight', '_features.5.conv.2.weight', '_features.7.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.6.conv.2.weight', '_features.8.conv.0.0.weight', '_features.4.conv.2.weight', '_features.18.1.weight', '_features.10.conv.0.0.weight', '_features.7.conv.2.weight', '_features.8.conv.2.weight', '_features.14.conv.0.1.weight', '_features.14.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.13.conv.2.weight', '_features.10.conv.2.weight', '_features.12.conv.2.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.17.conv.0.1.weight', '_features.11.conv.0.0.weight', '_features.11.conv.2.weight', '_features.14.conv.0.0.weight', '_features.15.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.17.conv.3.weight', '_features.16.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.15.conv.2.weight', '_features.16.conv.2.weight', '_features.14.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.2.weight', 'last_linear.weight', '_features.18.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.060358	Acc: 13.4% (1344/10000)
[Test]  Epoch: 2	Loss: 0.029119	Acc: 35.2% (3520/10000)
[Test]  Epoch: 3	Loss: 0.026818	Acc: 40.2% (4025/10000)
[Test]  Epoch: 4	Loss: 0.026562	Acc: 41.3% (4128/10000)
[Test]  Epoch: 5	Loss: 0.025392	Acc: 45.0% (4502/10000)
[Test]  Epoch: 6	Loss: 0.025239	Acc: 45.5% (4545/10000)
[Test]  Epoch: 7	Loss: 0.025447	Acc: 45.7% (4566/10000)
[Test]  Epoch: 8	Loss: 0.024894	Acc: 46.8% (4681/10000)
[Test]  Epoch: 9	Loss: 0.025294	Acc: 46.4% (4635/10000)
[Test]  Epoch: 10	Loss: 0.024847	Acc: 47.6% (4761/10000)
[Test]  Epoch: 11	Loss: 0.024155	Acc: 48.6% (4860/10000)
[Test]  Epoch: 12	Loss: 0.024612	Acc: 48.1% (4814/10000)
[Test]  Epoch: 13	Loss: 0.024297	Acc: 48.4% (4843/10000)
[Test]  Epoch: 14	Loss: 0.024511	Acc: 48.2% (4818/10000)
[Test]  Epoch: 15	Loss: 0.024180	Acc: 49.1% (4913/10000)
[Test]  Epoch: 16	Loss: 0.023840	Acc: 49.6% (4962/10000)
[Test]  Epoch: 17	Loss: 0.023889	Acc: 49.1% (4909/10000)
[Test]  Epoch: 18	Loss: 0.023893	Acc: 49.5% (4948/10000)
[Test]  Epoch: 19	Loss: 0.023691	Acc: 49.9% (4986/10000)
[Test]  Epoch: 20	Loss: 0.023708	Acc: 49.2% (4924/10000)
[Test]  Epoch: 21	Loss: 0.023711	Acc: 49.1% (4913/10000)
[Test]  Epoch: 22	Loss: 0.023820	Acc: 49.9% (4986/10000)
[Test]  Epoch: 23	Loss: 0.023668	Acc: 49.7% (4967/10000)
[Test]  Epoch: 24	Loss: 0.023314	Acc: 50.2% (5024/10000)
[Test]  Epoch: 25	Loss: 0.023562	Acc: 50.0% (5003/10000)
[Test]  Epoch: 26	Loss: 0.023328	Acc: 50.2% (5018/10000)
[Test]  Epoch: 27	Loss: 0.023434	Acc: 50.0% (5005/10000)
[Test]  Epoch: 28	Loss: 0.023304	Acc: 50.2% (5018/10000)
[Test]  Epoch: 29	Loss: 0.023165	Acc: 50.5% (5045/10000)
[Test]  Epoch: 30	Loss: 0.023091	Acc: 50.1% (5013/10000)
[Test]  Epoch: 31	Loss: 0.023044	Acc: 50.7% (5067/10000)
[Test]  Epoch: 32	Loss: 0.023228	Acc: 50.9% (5088/10000)
[Test]  Epoch: 33	Loss: 0.023205	Acc: 50.5% (5050/10000)
[Test]  Epoch: 34	Loss: 0.023127	Acc: 50.3% (5034/10000)
[Test]  Epoch: 35	Loss: 0.022951	Acc: 50.9% (5088/10000)
[Test]  Epoch: 36	Loss: 0.022969	Acc: 51.1% (5111/10000)
[Test]  Epoch: 37	Loss: 0.022608	Acc: 51.6% (5156/10000)
[Test]  Epoch: 38	Loss: 0.023130	Acc: 50.8% (5084/10000)
[Test]  Epoch: 39	Loss: 0.022920	Acc: 51.3% (5133/10000)
[Test]  Epoch: 40	Loss: 0.022717	Acc: 51.2% (5123/10000)
[Test]  Epoch: 41	Loss: 0.022851	Acc: 51.2% (5117/10000)
[Test]  Epoch: 42	Loss: 0.022723	Acc: 51.7% (5166/10000)
[Test]  Epoch: 43	Loss: 0.022682	Acc: 50.9% (5088/10000)
[Test]  Epoch: 44	Loss: 0.022497	Acc: 51.5% (5152/10000)
[Test]  Epoch: 45	Loss: 0.022406	Acc: 52.3% (5227/10000)
[Test]  Epoch: 46	Loss: 0.022403	Acc: 51.7% (5173/10000)
[Test]  Epoch: 47	Loss: 0.022575	Acc: 51.1% (5111/10000)
[Test]  Epoch: 48	Loss: 0.022375	Acc: 51.8% (5184/10000)
[Test]  Epoch: 49	Loss: 0.022280	Acc: 52.4% (5235/10000)
[Test]  Epoch: 50	Loss: 0.022661	Acc: 51.7% (5168/10000)
[Test]  Epoch: 51	Loss: 0.022271	Acc: 51.9% (5190/10000)
[Test]  Epoch: 52	Loss: 0.022471	Acc: 51.4% (5141/10000)
[Test]  Epoch: 53	Loss: 0.022588	Acc: 51.5% (5153/10000)
[Test]  Epoch: 54	Loss: 0.022514	Acc: 51.6% (5159/10000)
[Test]  Epoch: 55	Loss: 0.022281	Acc: 52.4% (5237/10000)
[Test]  Epoch: 56	Loss: 0.022283	Acc: 52.1% (5211/10000)
[Test]  Epoch: 57	Loss: 0.022401	Acc: 52.1% (5214/10000)
[Test]  Epoch: 58	Loss: 0.022187	Acc: 52.0% (5204/10000)
[Test]  Epoch: 59	Loss: 0.022228	Acc: 51.9% (5193/10000)
[Test]  Epoch: 60	Loss: 0.022152	Acc: 52.3% (5231/10000)
[Test]  Epoch: 61	Loss: 0.022167	Acc: 52.3% (5229/10000)
[Test]  Epoch: 62	Loss: 0.022074	Acc: 52.6% (5256/10000)
[Test]  Epoch: 63	Loss: 0.021984	Acc: 52.4% (5241/10000)
[Test]  Epoch: 64	Loss: 0.021974	Acc: 52.5% (5247/10000)
[Test]  Epoch: 65	Loss: 0.021942	Acc: 52.5% (5247/10000)
[Test]  Epoch: 66	Loss: 0.021925	Acc: 52.4% (5238/10000)
[Test]  Epoch: 67	Loss: 0.022110	Acc: 52.1% (5215/10000)
[Test]  Epoch: 68	Loss: 0.021940	Acc: 52.3% (5233/10000)
[Test]  Epoch: 69	Loss: 0.021945	Acc: 52.5% (5245/10000)
[Test]  Epoch: 70	Loss: 0.021930	Acc: 52.4% (5240/10000)
[Test]  Epoch: 71	Loss: 0.022087	Acc: 52.0% (5199/10000)
[Test]  Epoch: 72	Loss: 0.022047	Acc: 52.2% (5216/10000)
[Test]  Epoch: 73	Loss: 0.021954	Acc: 52.2% (5216/10000)
[Test]  Epoch: 74	Loss: 0.021894	Acc: 52.0% (5202/10000)
[Test]  Epoch: 75	Loss: 0.021901	Acc: 52.4% (5243/10000)
[Test]  Epoch: 76	Loss: 0.021953	Acc: 52.6% (5260/10000)
[Test]  Epoch: 77	Loss: 0.021930	Acc: 52.4% (5242/10000)
[Test]  Epoch: 78	Loss: 0.022011	Acc: 52.2% (5223/10000)
[Test]  Epoch: 79	Loss: 0.021937	Acc: 52.5% (5248/10000)
[Test]  Epoch: 80	Loss: 0.021970	Acc: 52.4% (5244/10000)
[Test]  Epoch: 81	Loss: 0.022020	Acc: 52.2% (5222/10000)
[Test]  Epoch: 82	Loss: 0.021986	Acc: 52.2% (5225/10000)
[Test]  Epoch: 83	Loss: 0.022024	Acc: 52.3% (5230/10000)
[Test]  Epoch: 84	Loss: 0.021963	Acc: 52.2% (5225/10000)
[Test]  Epoch: 85	Loss: 0.021936	Acc: 52.1% (5215/10000)
[Test]  Epoch: 86	Loss: 0.021975	Acc: 52.3% (5230/10000)
[Test]  Epoch: 87	Loss: 0.021941	Acc: 52.4% (5237/10000)
[Test]  Epoch: 88	Loss: 0.021919	Acc: 52.6% (5260/10000)
[Test]  Epoch: 89	Loss: 0.021993	Acc: 52.4% (5241/10000)
[Test]  Epoch: 90	Loss: 0.021906	Acc: 52.5% (5248/10000)
[Test]  Epoch: 91	Loss: 0.021926	Acc: 52.3% (5230/10000)
[Test]  Epoch: 92	Loss: 0.021983	Acc: 52.3% (5228/10000)
[Test]  Epoch: 93	Loss: 0.022036	Acc: 52.3% (5229/10000)
[Test]  Epoch: 94	Loss: 0.021902	Acc: 52.4% (5238/10000)
[Test]  Epoch: 95	Loss: 0.021945	Acc: 52.4% (5240/10000)
[Test]  Epoch: 96	Loss: 0.021975	Acc: 52.1% (5213/10000)
[Test]  Epoch: 97	Loss: 0.021920	Acc: 52.1% (5211/10000)
[Test]  Epoch: 98	Loss: 0.021958	Acc: 52.2% (5218/10000)
[Test]  Epoch: 99	Loss: 0.021891	Acc: 52.4% (5240/10000)
[Test]  Epoch: 100	Loss: 0.021791	Acc: 52.5% (5252/10000)
===========finish==========
['2024-08-19', '01:57:13.949649', '100', 'test', '0.0217907457113266', '52.52', '52.6']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-vgg16_bn/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('features.0.bias', 0.0), ('features.1.bias', 0.0), ('features.3.bias', 0.0), ('features.4.bias', 0.0), ('features.7.bias', 0.0), ('features.8.bias', 0.0), ('features.10.bias', 0.0), ('features.11.bias', 0.0), ('features.14.bias', 0.0), ('features.15.bias', 0.0), ('features.17.bias', 0.0), ('features.18.bias', 0.0), ('features.20.bias', 0.0), ('features.21.bias', 0.0), ('features.24.bias', 0.0), ('features.25.bias', 0.0), ('features.27.bias', 0.0), ('features.28.bias', 0.0), ('features.30.bias', 0.0), ('features.31.bias', 0.0), ('features.34.bias', 0.0), ('features.35.bias', 0.0), ('features.37.bias', 0.0), ('features.38.bias', 0.0), ('features.40.bias', 0.0), ('features.41.bias', 0.0), ('classifier.bias', 0.0), ('features.38.weight', -11.69686222076416), ('features.35.weight', -11.860086441040039), ('features.1.weight', -14.800999641418457), ('features.31.weight', -26.411109924316406), ('features.41.weight', -28.222881317138672), ('features.28.weight', -36.04952621459961), ('features.4.weight', -55.10869598388672), ('features.25.weight', -82.29485321044922), ('features.8.weight', -92.28964233398438), ('features.11.weight', -184.2922821044922), ('features.18.weight', -253.5595245361328), ('features.21.weight', -282.8586730957031), ('features.15.weight', -282.9633483886719), ('classifier.weight', -3806.13916015625), ('features.0.weight', -4410.5888671875), ('features.3.weight', -311594.65625), ('features.7.weight', -790645.5625), ('features.10.weight', -1900770.875), ('features.40.weight', -2144488.25), ('features.37.weight', -2341541.25), ('features.34.weight', -3373331.5), ('features.30.weight', -5111660.0), ('features.14.weight', -5385439.0), ('features.20.weight', -7769905.0), ('features.27.weight', -9088770.0), ('features.17.weight', -10005436.0), ('features.24.weight', -10094132.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('features.0.bias', 0.0), ('features.1.bias', 0.0), ('features.3.bias', 0.0), ('features.4.bias', 0.0), ('features.7.bias', 0.0), ('features.8.bias', 0.0), ('features.10.bias', 0.0), ('features.11.bias', 0.0), ('features.14.bias', 0.0), ('features.15.bias', 0.0), ('features.17.bias', 0.0), ('features.18.bias', 0.0), ('features.20.bias', 0.0), ('features.21.bias', 0.0), ('features.24.bias', 0.0), ('features.25.bias', 0.0), ('features.27.bias', 0.0), ('features.28.bias', 0.0), ('features.30.bias', 0.0), ('features.31.bias', 0.0), ('features.34.bias', 0.0), ('features.35.bias', 0.0), ('features.37.bias', 0.0), ('features.38.bias', 0.0), ('features.40.bias', 0.0), ('features.41.bias', 0.0), ('classifier.bias', 0.0), ('features.38.weight', -11.69686222076416), ('features.35.weight', -11.860086441040039), ('features.1.weight', -14.800999641418457), ('features.31.weight', -26.411109924316406), ('features.41.weight', -28.222881317138672), ('features.28.weight', -36.04952621459961), ('features.4.weight', -55.10869598388672), ('features.25.weight', -82.29485321044922), ('features.8.weight', -92.28964233398438), ('features.11.weight', -184.2922821044922), ('features.18.weight', -253.5595245361328), ('features.21.weight', -282.8586730957031), ('features.15.weight', -282.9633483886719), ('classifier.weight', -3806.13916015625), ('features.0.weight', -4410.5888671875), ('features.3.weight', -311594.65625), ('features.7.weight', -790645.5625), ('features.10.weight', -1900770.875), ('features.40.weight', -2144488.25), ('features.37.weight', -2341541.25), ('features.34.weight', -3373331.5), ('features.30.weight', -5111660.0), ('features.14.weight', -5385439.0), ('features.20.weight', -7769905.0), ('features.27.weight', -9088770.0), ('features.17.weight', -10005436.0), ('features.24.weight', -10094132.0)]
--------------------------------------------
get_sample_layers n=27  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.005736	Acc: 87.7% (8765/10000)
[Test]  Epoch: 2	Loss: 0.005697	Acc: 87.8% (8780/10000)
[Test]  Epoch: 3	Loss: 0.005521	Acc: 88.3% (8829/10000)
[Test]  Epoch: 4	Loss: 0.005554	Acc: 88.2% (8815/10000)
[Test]  Epoch: 5	Loss: 0.005537	Acc: 88.1% (8810/10000)
[Test]  Epoch: 6	Loss: 0.005408	Acc: 88.5% (8854/10000)
[Test]  Epoch: 7	Loss: 0.005465	Acc: 88.4% (8844/10000)
[Test]  Epoch: 8	Loss: 0.005392	Acc: 88.8% (8879/10000)
[Test]  Epoch: 9	Loss: 0.005350	Acc: 88.6% (8862/10000)
[Test]  Epoch: 10	Loss: 0.005435	Acc: 88.6% (8862/10000)
[Test]  Epoch: 11	Loss: 0.005518	Acc: 88.3% (8827/10000)
[Test]  Epoch: 12	Loss: 0.005339	Acc: 88.9% (8889/10000)
[Test]  Epoch: 13	Loss: 0.005630	Acc: 88.2% (8819/10000)
[Test]  Epoch: 14	Loss: 0.005469	Acc: 88.7% (8869/10000)
[Test]  Epoch: 15	Loss: 0.005476	Acc: 88.5% (8855/10000)
[Test]  Epoch: 16	Loss: 0.005514	Acc: 88.7% (8873/10000)
[Test]  Epoch: 17	Loss: 0.005491	Acc: 88.6% (8864/10000)
[Test]  Epoch: 18	Loss: 0.005401	Acc: 89.0% (8895/10000)
[Test]  Epoch: 19	Loss: 0.005603	Acc: 88.8% (8877/10000)
[Test]  Epoch: 20	Loss: 0.005478	Acc: 89.1% (8906/10000)
[Test]  Epoch: 21	Loss: 0.005579	Acc: 88.8% (8884/10000)
[Test]  Epoch: 22	Loss: 0.005530	Acc: 89.0% (8899/10000)
[Test]  Epoch: 23	Loss: 0.005632	Acc: 88.7% (8867/10000)
[Test]  Epoch: 24	Loss: 0.005692	Acc: 88.6% (8860/10000)
[Test]  Epoch: 25	Loss: 0.005725	Acc: 88.6% (8860/10000)
[Test]  Epoch: 26	Loss: 0.005635	Acc: 89.1% (8912/10000)
[Test]  Epoch: 27	Loss: 0.005718	Acc: 88.7% (8867/10000)
[Test]  Epoch: 28	Loss: 0.005708	Acc: 88.9% (8888/10000)
[Test]  Epoch: 29	Loss: 0.005840	Acc: 88.6% (8858/10000)
[Test]  Epoch: 30	Loss: 0.005678	Acc: 88.8% (8885/10000)
[Test]  Epoch: 31	Loss: 0.005612	Acc: 89.0% (8895/10000)
[Test]  Epoch: 32	Loss: 0.005725	Acc: 88.6% (8864/10000)
[Test]  Epoch: 33	Loss: 0.005756	Acc: 88.6% (8860/10000)
[Test]  Epoch: 34	Loss: 0.005950	Acc: 88.3% (8830/10000)
[Test]  Epoch: 35	Loss: 0.005814	Acc: 88.9% (8886/10000)
[Test]  Epoch: 36	Loss: 0.005813	Acc: 88.4% (8844/10000)
[Test]  Epoch: 37	Loss: 0.005941	Acc: 88.3% (8834/10000)
[Test]  Epoch: 38	Loss: 0.005879	Acc: 88.7% (8865/10000)
[Test]  Epoch: 39	Loss: 0.005894	Acc: 88.5% (8849/10000)
[Test]  Epoch: 40	Loss: 0.005905	Acc: 88.6% (8857/10000)
[Test]  Epoch: 41	Loss: 0.005933	Acc: 88.5% (8845/10000)
[Test]  Epoch: 42	Loss: 0.005918	Acc: 88.6% (8858/10000)
[Test]  Epoch: 43	Loss: 0.005889	Acc: 88.8% (8880/10000)
[Test]  Epoch: 44	Loss: 0.006026	Acc: 88.9% (8890/10000)
[Test]  Epoch: 45	Loss: 0.005815	Acc: 88.6% (8861/10000)
[Test]  Epoch: 46	Loss: 0.006023	Acc: 88.4% (8837/10000)
[Test]  Epoch: 47	Loss: 0.006192	Acc: 88.4% (8843/10000)
[Test]  Epoch: 48	Loss: 0.006047	Acc: 88.7% (8866/10000)
[Test]  Epoch: 49	Loss: 0.006012	Acc: 88.7% (8867/10000)
[Test]  Epoch: 50	Loss: 0.006034	Acc: 88.6% (8860/10000)
[Test]  Epoch: 51	Loss: 0.005929	Acc: 88.9% (8887/10000)
[Test]  Epoch: 52	Loss: 0.005981	Acc: 88.7% (8871/10000)
[Test]  Epoch: 53	Loss: 0.005986	Acc: 88.4% (8841/10000)
[Test]  Epoch: 54	Loss: 0.006148	Acc: 88.6% (8858/10000)
[Test]  Epoch: 55	Loss: 0.006105	Acc: 88.6% (8863/10000)
[Test]  Epoch: 56	Loss: 0.006059	Acc: 88.3% (8830/10000)
[Test]  Epoch: 57	Loss: 0.006062	Acc: 88.4% (8844/10000)
[Test]  Epoch: 58	Loss: 0.006164	Acc: 88.2% (8823/10000)
[Test]  Epoch: 59	Loss: 0.006220	Acc: 88.3% (8835/10000)
[Test]  Epoch: 60	Loss: 0.006219	Acc: 88.6% (8862/10000)
[Test]  Epoch: 61	Loss: 0.005985	Acc: 88.7% (8867/10000)
[Test]  Epoch: 62	Loss: 0.006210	Acc: 88.5% (8854/10000)
[Test]  Epoch: 63	Loss: 0.006066	Acc: 88.9% (8888/10000)
[Test]  Epoch: 64	Loss: 0.006209	Acc: 88.3% (8834/10000)
[Test]  Epoch: 65	Loss: 0.006007	Acc: 88.8% (8881/10000)
[Test]  Epoch: 66	Loss: 0.006095	Acc: 88.5% (8848/10000)
[Test]  Epoch: 67	Loss: 0.006098	Acc: 88.7% (8868/10000)
[Test]  Epoch: 68	Loss: 0.006033	Acc: 88.7% (8869/10000)
[Test]  Epoch: 69	Loss: 0.005973	Acc: 88.9% (8887/10000)
[Test]  Epoch: 70	Loss: 0.006138	Acc: 89.0% (8895/10000)
[Test]  Epoch: 71	Loss: 0.006115	Acc: 88.5% (8851/10000)
[Test]  Epoch: 72	Loss: 0.006139	Acc: 88.5% (8853/10000)
[Test]  Epoch: 73	Loss: 0.006178	Acc: 88.7% (8871/10000)
[Test]  Epoch: 74	Loss: 0.006222	Acc: 88.4% (8842/10000)
[Test]  Epoch: 75	Loss: 0.006082	Acc: 88.7% (8872/10000)
[Test]  Epoch: 76	Loss: 0.006090	Acc: 88.5% (8848/10000)
[Test]  Epoch: 77	Loss: 0.006311	Acc: 88.5% (8851/10000)
[Test]  Epoch: 78	Loss: 0.006108	Acc: 88.8% (8882/10000)
[Test]  Epoch: 79	Loss: 0.006138	Acc: 88.6% (8856/10000)
[Test]  Epoch: 80	Loss: 0.006196	Acc: 88.5% (8846/10000)
[Test]  Epoch: 81	Loss: 0.006265	Acc: 88.7% (8865/10000)
[Test]  Epoch: 82	Loss: 0.006088	Acc: 88.4% (8840/10000)
[Test]  Epoch: 83	Loss: 0.006107	Acc: 88.8% (8878/10000)
[Test]  Epoch: 84	Loss: 0.006188	Acc: 88.8% (8882/10000)
[Test]  Epoch: 85	Loss: 0.006155	Acc: 88.7% (8868/10000)
[Test]  Epoch: 86	Loss: 0.006182	Acc: 88.4% (8841/10000)
[Test]  Epoch: 87	Loss: 0.006213	Acc: 88.5% (8851/10000)
[Test]  Epoch: 88	Loss: 0.006268	Acc: 88.5% (8849/10000)
[Test]  Epoch: 89	Loss: 0.006104	Acc: 88.7% (8873/10000)
[Test]  Epoch: 90	Loss: 0.006115	Acc: 88.8% (8881/10000)
[Test]  Epoch: 91	Loss: 0.006133	Acc: 88.9% (8889/10000)
[Test]  Epoch: 92	Loss: 0.006146	Acc: 88.6% (8861/10000)
[Test]  Epoch: 93	Loss: 0.006050	Acc: 88.6% (8861/10000)
[Test]  Epoch: 94	Loss: 0.006052	Acc: 88.8% (8880/10000)
[Test]  Epoch: 95	Loss: 0.006118	Acc: 88.8% (8876/10000)
[Test]  Epoch: 96	Loss: 0.005983	Acc: 88.7% (8873/10000)
[Test]  Epoch: 97	Loss: 0.006088	Acc: 88.6% (8863/10000)
[Test]  Epoch: 98	Loss: 0.006265	Acc: 88.7% (8873/10000)
[Test]  Epoch: 99	Loss: 0.006199	Acc: 88.7% (8871/10000)
[Test]  Epoch: 100	Loss: 0.006112	Acc: 88.7% (8868/10000)
===========finish==========
['2024-08-19', '02:00:32.675487', '100', 'test', '0.006112066599726677', '88.68', '89.12']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.38.weight', 'features.35.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.005877	Acc: 87.6% (8759/10000)
[Test]  Epoch: 2	Loss: 0.005697	Acc: 87.8% (8783/10000)
[Test]  Epoch: 3	Loss: 0.005511	Acc: 88.3% (8835/10000)
[Test]  Epoch: 4	Loss: 0.005539	Acc: 88.2% (8816/10000)
[Test]  Epoch: 5	Loss: 0.005520	Acc: 88.1% (8812/10000)
[Test]  Epoch: 6	Loss: 0.005393	Acc: 88.6% (8856/10000)
[Test]  Epoch: 7	Loss: 0.005447	Acc: 88.4% (8840/10000)
[Test]  Epoch: 8	Loss: 0.005377	Acc: 88.7% (8872/10000)
[Test]  Epoch: 9	Loss: 0.005331	Acc: 88.6% (8864/10000)
[Test]  Epoch: 10	Loss: 0.005413	Acc: 88.6% (8864/10000)
[Test]  Epoch: 11	Loss: 0.005493	Acc: 88.3% (8830/10000)
[Test]  Epoch: 12	Loss: 0.005319	Acc: 88.9% (8893/10000)
[Test]  Epoch: 13	Loss: 0.005607	Acc: 88.2% (8821/10000)
[Test]  Epoch: 14	Loss: 0.005449	Acc: 88.7% (8867/10000)
[Test]  Epoch: 15	Loss: 0.005455	Acc: 88.5% (8855/10000)
[Test]  Epoch: 16	Loss: 0.005494	Acc: 88.8% (8875/10000)
[Test]  Epoch: 17	Loss: 0.005471	Acc: 88.7% (8867/10000)
[Test]  Epoch: 18	Loss: 0.005386	Acc: 89.0% (8898/10000)
[Test]  Epoch: 19	Loss: 0.005589	Acc: 88.7% (8874/10000)
[Test]  Epoch: 20	Loss: 0.005464	Acc: 89.1% (8912/10000)
[Test]  Epoch: 21	Loss: 0.005565	Acc: 88.8% (8879/10000)
[Test]  Epoch: 22	Loss: 0.005516	Acc: 89.0% (8897/10000)
[Test]  Epoch: 23	Loss: 0.005620	Acc: 88.7% (8869/10000)
[Test]  Epoch: 24	Loss: 0.005679	Acc: 88.6% (8863/10000)
[Test]  Epoch: 25	Loss: 0.005711	Acc: 88.6% (8858/10000)
[Test]  Epoch: 26	Loss: 0.005626	Acc: 89.1% (8913/10000)
[Test]  Epoch: 27	Loss: 0.005713	Acc: 88.7% (8868/10000)
[Test]  Epoch: 28	Loss: 0.005704	Acc: 88.9% (8889/10000)
[Test]  Epoch: 29	Loss: 0.005837	Acc: 88.6% (8860/10000)
[Test]  Epoch: 30	Loss: 0.005677	Acc: 88.9% (8888/10000)
[Test]  Epoch: 31	Loss: 0.005614	Acc: 88.9% (8889/10000)
[Test]  Epoch: 32	Loss: 0.005726	Acc: 88.7% (8871/10000)
[Test]  Epoch: 33	Loss: 0.005757	Acc: 88.6% (8862/10000)
[Test]  Epoch: 34	Loss: 0.005953	Acc: 88.3% (8827/10000)
[Test]  Epoch: 35	Loss: 0.005820	Acc: 88.8% (8883/10000)
[Test]  Epoch: 36	Loss: 0.005819	Acc: 88.4% (8844/10000)
[Test]  Epoch: 37	Loss: 0.005945	Acc: 88.3% (8832/10000)
[Test]  Epoch: 38	Loss: 0.005888	Acc: 88.7% (8868/10000)
[Test]  Epoch: 39	Loss: 0.005898	Acc: 88.4% (8843/10000)
[Test]  Epoch: 40	Loss: 0.005915	Acc: 88.5% (8852/10000)
[Test]  Epoch: 41	Loss: 0.005939	Acc: 88.5% (8851/10000)
[Test]  Epoch: 42	Loss: 0.005930	Acc: 88.6% (8860/10000)
[Test]  Epoch: 43	Loss: 0.005899	Acc: 88.8% (8877/10000)
[Test]  Epoch: 44	Loss: 0.006033	Acc: 88.9% (8893/10000)
[Test]  Epoch: 45	Loss: 0.005826	Acc: 88.6% (8858/10000)
[Test]  Epoch: 46	Loss: 0.006037	Acc: 88.3% (8834/10000)
[Test]  Epoch: 47	Loss: 0.006209	Acc: 88.4% (8839/10000)
[Test]  Epoch: 48	Loss: 0.006063	Acc: 88.7% (8874/10000)
[Test]  Epoch: 49	Loss: 0.006028	Acc: 88.6% (8862/10000)
[Test]  Epoch: 50	Loss: 0.006047	Acc: 88.6% (8859/10000)
[Test]  Epoch: 51	Loss: 0.005942	Acc: 88.9% (8890/10000)
[Test]  Epoch: 52	Loss: 0.005996	Acc: 88.8% (8875/10000)
[Test]  Epoch: 53	Loss: 0.005998	Acc: 88.4% (8844/10000)
[Test]  Epoch: 54	Loss: 0.006164	Acc: 88.6% (8857/10000)
[Test]  Epoch: 55	Loss: 0.006121	Acc: 88.6% (8862/10000)
[Test]  Epoch: 56	Loss: 0.006079	Acc: 88.2% (8824/10000)
[Test]  Epoch: 57	Loss: 0.006076	Acc: 88.5% (8852/10000)
[Test]  Epoch: 58	Loss: 0.006183	Acc: 88.2% (8818/10000)
[Test]  Epoch: 59	Loss: 0.006241	Acc: 88.4% (8836/10000)
[Test]  Epoch: 60	Loss: 0.006242	Acc: 88.5% (8853/10000)
[Test]  Epoch: 61	Loss: 0.006003	Acc: 88.7% (8866/10000)
[Test]  Epoch: 62	Loss: 0.006231	Acc: 88.5% (8851/10000)
[Test]  Epoch: 63	Loss: 0.006085	Acc: 88.9% (8886/10000)
[Test]  Epoch: 64	Loss: 0.006229	Acc: 88.2% (8825/10000)
[Test]  Epoch: 65	Loss: 0.006021	Acc: 88.9% (8888/10000)
[Test]  Epoch: 66	Loss: 0.006108	Acc: 88.5% (8847/10000)
[Test]  Epoch: 67	Loss: 0.006117	Acc: 88.7% (8871/10000)
[Test]  Epoch: 68	Loss: 0.006049	Acc: 88.6% (8861/10000)
[Test]  Epoch: 69	Loss: 0.005995	Acc: 88.9% (8887/10000)
[Test]  Epoch: 70	Loss: 0.006159	Acc: 88.9% (8888/10000)
[Test]  Epoch: 71	Loss: 0.006140	Acc: 88.5% (8849/10000)
[Test]  Epoch: 72	Loss: 0.006158	Acc: 88.5% (8853/10000)
[Test]  Epoch: 73	Loss: 0.006194	Acc: 88.7% (8873/10000)
[Test]  Epoch: 74	Loss: 0.006240	Acc: 88.5% (8845/10000)
[Test]  Epoch: 75	Loss: 0.006099	Acc: 88.8% (8875/10000)
[Test]  Epoch: 76	Loss: 0.006108	Acc: 88.5% (8853/10000)
[Test]  Epoch: 77	Loss: 0.006332	Acc: 88.5% (8848/10000)
[Test]  Epoch: 78	Loss: 0.006125	Acc: 88.9% (8886/10000)
[Test]  Epoch: 79	Loss: 0.006157	Acc: 88.6% (8859/10000)
[Test]  Epoch: 80	Loss: 0.006219	Acc: 88.4% (8844/10000)
[Test]  Epoch: 81	Loss: 0.006283	Acc: 88.6% (8856/10000)
[Test]  Epoch: 82	Loss: 0.006110	Acc: 88.4% (8836/10000)
[Test]  Epoch: 83	Loss: 0.006128	Acc: 88.8% (8877/10000)
[Test]  Epoch: 84	Loss: 0.006207	Acc: 88.8% (8885/10000)
[Test]  Epoch: 85	Loss: 0.006176	Acc: 88.6% (8864/10000)
[Test]  Epoch: 86	Loss: 0.006202	Acc: 88.4% (8838/10000)
[Test]  Epoch: 87	Loss: 0.006233	Acc: 88.5% (8845/10000)
[Test]  Epoch: 88	Loss: 0.006289	Acc: 88.5% (8851/10000)
[Test]  Epoch: 89	Loss: 0.006124	Acc: 88.8% (8875/10000)
[Test]  Epoch: 90	Loss: 0.006138	Acc: 88.8% (8876/10000)
[Test]  Epoch: 91	Loss: 0.006153	Acc: 88.8% (8881/10000)
[Test]  Epoch: 92	Loss: 0.006167	Acc: 88.6% (8863/10000)
[Test]  Epoch: 93	Loss: 0.006065	Acc: 88.6% (8860/10000)
[Test]  Epoch: 94	Loss: 0.006070	Acc: 88.8% (8881/10000)
[Test]  Epoch: 95	Loss: 0.006137	Acc: 88.7% (8874/10000)
[Test]  Epoch: 96	Loss: 0.006000	Acc: 88.7% (8873/10000)
[Test]  Epoch: 97	Loss: 0.006107	Acc: 88.6% (8864/10000)
[Test]  Epoch: 98	Loss: 0.006286	Acc: 88.7% (8871/10000)
[Test]  Epoch: 99	Loss: 0.006217	Acc: 88.7% (8870/10000)
[Test]  Epoch: 100	Loss: 0.006131	Acc: 88.7% (8872/10000)
===========finish==========
['2024-08-19', '02:02:56.153567', '100', 'test', '0.006131159897893667', '88.72', '89.13']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.022101	Acc: 85.2% (8523/10000)
[Test]  Epoch: 2	Loss: 0.015951	Acc: 85.8% (8580/10000)
[Test]  Epoch: 3	Loss: 0.013268	Acc: 86.3% (8631/10000)
[Test]  Epoch: 4	Loss: 0.013386	Acc: 86.5% (8652/10000)
[Test]  Epoch: 5	Loss: 0.013152	Acc: 86.3% (8632/10000)
[Test]  Epoch: 6	Loss: 0.011717	Acc: 87.0% (8696/10000)
[Test]  Epoch: 7	Loss: 0.011849	Acc: 87.2% (8717/10000)
[Test]  Epoch: 8	Loss: 0.011933	Acc: 86.7% (8674/10000)
[Test]  Epoch: 9	Loss: 0.011226	Acc: 87.2% (8725/10000)
[Test]  Epoch: 10	Loss: 0.011693	Acc: 86.6% (8664/10000)
[Test]  Epoch: 11	Loss: 0.011886	Acc: 86.6% (8663/10000)
[Test]  Epoch: 12	Loss: 0.011327	Acc: 87.4% (8737/10000)
[Test]  Epoch: 13	Loss: 0.012183	Acc: 86.7% (8670/10000)
[Test]  Epoch: 14	Loss: 0.011533	Acc: 87.0% (8698/10000)
[Test]  Epoch: 15	Loss: 0.011228	Acc: 87.2% (8724/10000)
[Test]  Epoch: 16	Loss: 0.011459	Acc: 87.1% (8714/10000)
[Test]  Epoch: 17	Loss: 0.011208	Acc: 87.4% (8736/10000)
[Test]  Epoch: 18	Loss: 0.011206	Acc: 87.4% (8736/10000)
[Test]  Epoch: 19	Loss: 0.011175	Acc: 87.6% (8760/10000)
[Test]  Epoch: 20	Loss: 0.010959	Acc: 87.8% (8781/10000)
[Test]  Epoch: 21	Loss: 0.011271	Acc: 87.3% (8735/10000)
[Test]  Epoch: 22	Loss: 0.011103	Acc: 87.4% (8741/10000)
[Test]  Epoch: 23	Loss: 0.010861	Acc: 87.6% (8756/10000)
[Test]  Epoch: 24	Loss: 0.011216	Acc: 87.6% (8762/10000)
[Test]  Epoch: 25	Loss: 0.011362	Acc: 87.6% (8757/10000)
[Test]  Epoch: 26	Loss: 0.011053	Acc: 87.6% (8761/10000)
[Test]  Epoch: 27	Loss: 0.011427	Acc: 87.0% (8699/10000)
[Test]  Epoch: 28	Loss: 0.011301	Acc: 87.6% (8758/10000)
[Test]  Epoch: 29	Loss: 0.011295	Acc: 87.1% (8711/10000)
[Test]  Epoch: 30	Loss: 0.010880	Acc: 88.0% (8795/10000)
[Test]  Epoch: 31	Loss: 0.010599	Acc: 88.0% (8802/10000)
[Test]  Epoch: 32	Loss: 0.010924	Acc: 87.6% (8762/10000)
[Test]  Epoch: 33	Loss: 0.011014	Acc: 87.4% (8736/10000)
[Test]  Epoch: 34	Loss: 0.011285	Acc: 87.2% (8717/10000)
[Test]  Epoch: 35	Loss: 0.010928	Acc: 87.9% (8794/10000)
[Test]  Epoch: 36	Loss: 0.010916	Acc: 87.8% (8778/10000)
[Test]  Epoch: 37	Loss: 0.011084	Acc: 87.4% (8739/10000)
[Test]  Epoch: 38	Loss: 0.011316	Acc: 87.4% (8737/10000)
[Test]  Epoch: 39	Loss: 0.011193	Acc: 87.1% (8709/10000)
[Test]  Epoch: 40	Loss: 0.011473	Acc: 87.0% (8701/10000)
[Test]  Epoch: 41	Loss: 0.011166	Acc: 87.5% (8747/10000)
[Test]  Epoch: 42	Loss: 0.010943	Acc: 87.1% (8706/10000)
[Test]  Epoch: 43	Loss: 0.010880	Acc: 87.7% (8768/10000)
[Test]  Epoch: 44	Loss: 0.011108	Acc: 87.2% (8719/10000)
[Test]  Epoch: 45	Loss: 0.010721	Acc: 87.5% (8751/10000)
[Test]  Epoch: 46	Loss: 0.011376	Acc: 87.0% (8698/10000)
[Test]  Epoch: 47	Loss: 0.011569	Acc: 87.1% (8714/10000)
[Test]  Epoch: 48	Loss: 0.011283	Acc: 87.2% (8719/10000)
[Test]  Epoch: 49	Loss: 0.010985	Acc: 87.7% (8769/10000)
[Test]  Epoch: 50	Loss: 0.011014	Acc: 87.6% (8756/10000)
[Test]  Epoch: 51	Loss: 0.010765	Acc: 87.8% (8781/10000)
[Test]  Epoch: 52	Loss: 0.010809	Acc: 87.5% (8751/10000)
[Test]  Epoch: 53	Loss: 0.010720	Acc: 87.4% (8744/10000)
[Test]  Epoch: 54	Loss: 0.011017	Acc: 87.5% (8755/10000)
[Test]  Epoch: 55	Loss: 0.010659	Acc: 87.5% (8755/10000)
[Test]  Epoch: 56	Loss: 0.010749	Acc: 87.4% (8738/10000)
[Test]  Epoch: 57	Loss: 0.010915	Acc: 87.3% (8734/10000)
[Test]  Epoch: 58	Loss: 0.010760	Acc: 87.4% (8737/10000)
[Test]  Epoch: 59	Loss: 0.010936	Acc: 87.3% (8726/10000)
[Test]  Epoch: 60	Loss: 0.010963	Acc: 87.3% (8735/10000)
[Test]  Epoch: 61	Loss: 0.010583	Acc: 87.6% (8757/10000)
[Test]  Epoch: 62	Loss: 0.011138	Acc: 87.2% (8723/10000)
[Test]  Epoch: 63	Loss: 0.010591	Acc: 87.5% (8751/10000)
[Test]  Epoch: 64	Loss: 0.010816	Acc: 87.4% (8738/10000)
[Test]  Epoch: 65	Loss: 0.010477	Acc: 87.7% (8774/10000)
[Test]  Epoch: 66	Loss: 0.010824	Acc: 87.3% (8731/10000)
[Test]  Epoch: 67	Loss: 0.010675	Acc: 87.6% (8761/10000)
[Test]  Epoch: 68	Loss: 0.010576	Acc: 87.2% (8722/10000)
[Test]  Epoch: 69	Loss: 0.010406	Acc: 87.4% (8741/10000)
[Test]  Epoch: 70	Loss: 0.010812	Acc: 87.6% (8756/10000)
[Test]  Epoch: 71	Loss: 0.010616	Acc: 87.7% (8766/10000)
[Test]  Epoch: 72	Loss: 0.010872	Acc: 87.6% (8756/10000)
[Test]  Epoch: 73	Loss: 0.010988	Acc: 87.5% (8746/10000)
[Test]  Epoch: 74	Loss: 0.010819	Acc: 87.5% (8754/10000)
[Test]  Epoch: 75	Loss: 0.010638	Acc: 87.7% (8774/10000)
[Test]  Epoch: 76	Loss: 0.010566	Acc: 87.4% (8742/10000)
[Test]  Epoch: 77	Loss: 0.011346	Acc: 87.1% (8712/10000)
[Test]  Epoch: 78	Loss: 0.010967	Acc: 87.5% (8754/10000)
[Test]  Epoch: 79	Loss: 0.010660	Acc: 87.6% (8759/10000)
[Test]  Epoch: 80	Loss: 0.011025	Acc: 87.3% (8730/10000)
[Test]  Epoch: 81	Loss: 0.010877	Acc: 87.4% (8736/10000)
[Test]  Epoch: 82	Loss: 0.010686	Acc: 87.6% (8758/10000)
[Test]  Epoch: 83	Loss: 0.010858	Acc: 87.4% (8737/10000)
[Test]  Epoch: 84	Loss: 0.010804	Acc: 87.6% (8762/10000)
[Test]  Epoch: 85	Loss: 0.010874	Acc: 87.5% (8745/10000)
[Test]  Epoch: 86	Loss: 0.010813	Acc: 87.5% (8746/10000)
[Test]  Epoch: 87	Loss: 0.010849	Acc: 87.3% (8734/10000)
[Test]  Epoch: 88	Loss: 0.011005	Acc: 87.8% (8779/10000)
[Test]  Epoch: 89	Loss: 0.010422	Acc: 87.4% (8744/10000)
[Test]  Epoch: 90	Loss: 0.010531	Acc: 87.8% (8780/10000)
[Test]  Epoch: 91	Loss: 0.010787	Acc: 87.5% (8748/10000)
[Test]  Epoch: 92	Loss: 0.010683	Acc: 87.5% (8754/10000)
[Test]  Epoch: 93	Loss: 0.010576	Acc: 87.8% (8783/10000)
[Test]  Epoch: 94	Loss: 0.010446	Acc: 87.7% (8772/10000)
[Test]  Epoch: 95	Loss: 0.010661	Acc: 87.6% (8762/10000)
[Test]  Epoch: 96	Loss: 0.010339	Acc: 87.8% (8776/10000)
[Test]  Epoch: 97	Loss: 0.010498	Acc: 87.5% (8750/10000)
[Test]  Epoch: 98	Loss: 0.010879	Acc: 87.5% (8749/10000)
[Test]  Epoch: 99	Loss: 0.010702	Acc: 87.3% (8734/10000)
[Test]  Epoch: 100	Loss: 0.010657	Acc: 87.5% (8748/10000)
===========finish==========
['2024-08-19', '02:05:22.523848', '100', 'test', '0.010656523358821869', '87.48', '88.02']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight', 'features.28.weight', 'features.4.weight', 'features.25.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.028689	Acc: 83.5% (8348/10000)
[Test]  Epoch: 2	Loss: 0.017092	Acc: 85.5% (8548/10000)
[Test]  Epoch: 3	Loss: 0.014505	Acc: 86.0% (8598/10000)
[Test]  Epoch: 4	Loss: 0.013883	Acc: 85.8% (8581/10000)
[Test]  Epoch: 5	Loss: 0.013358	Acc: 85.8% (8581/10000)
[Test]  Epoch: 6	Loss: 0.012423	Acc: 86.7% (8669/10000)
[Test]  Epoch: 7	Loss: 0.011968	Acc: 87.0% (8704/10000)
[Test]  Epoch: 8	Loss: 0.012351	Acc: 86.4% (8640/10000)
[Test]  Epoch: 9	Loss: 0.011996	Acc: 86.7% (8669/10000)
[Test]  Epoch: 10	Loss: 0.012275	Acc: 86.2% (8624/10000)
[Test]  Epoch: 11	Loss: 0.012001	Acc: 86.6% (8656/10000)
[Test]  Epoch: 12	Loss: 0.011498	Acc: 87.2% (8715/10000)
[Test]  Epoch: 13	Loss: 0.012147	Acc: 86.7% (8671/10000)
[Test]  Epoch: 14	Loss: 0.011513	Acc: 87.0% (8695/10000)
[Test]  Epoch: 15	Loss: 0.011387	Acc: 87.1% (8707/10000)
[Test]  Epoch: 16	Loss: 0.011556	Acc: 87.1% (8709/10000)
[Test]  Epoch: 17	Loss: 0.011501	Acc: 87.1% (8711/10000)
[Test]  Epoch: 18	Loss: 0.011386	Acc: 87.2% (8724/10000)
[Test]  Epoch: 19	Loss: 0.011396	Acc: 87.3% (8731/10000)
[Test]  Epoch: 20	Loss: 0.011292	Acc: 87.6% (8762/10000)
[Test]  Epoch: 21	Loss: 0.011515	Acc: 87.3% (8728/10000)
[Test]  Epoch: 22	Loss: 0.011419	Acc: 87.1% (8708/10000)
[Test]  Epoch: 23	Loss: 0.011267	Acc: 87.3% (8734/10000)
[Test]  Epoch: 24	Loss: 0.011580	Acc: 87.3% (8727/10000)
[Test]  Epoch: 25	Loss: 0.011673	Acc: 87.1% (8708/10000)
[Test]  Epoch: 26	Loss: 0.011465	Acc: 87.3% (8735/10000)
[Test]  Epoch: 27	Loss: 0.011753	Acc: 86.8% (8676/10000)
[Test]  Epoch: 28	Loss: 0.011571	Acc: 87.4% (8741/10000)
[Test]  Epoch: 29	Loss: 0.011528	Acc: 86.8% (8678/10000)
[Test]  Epoch: 30	Loss: 0.011265	Acc: 87.4% (8744/10000)
[Test]  Epoch: 31	Loss: 0.010903	Acc: 87.7% (8770/10000)
[Test]  Epoch: 32	Loss: 0.011346	Acc: 87.4% (8739/10000)
[Test]  Epoch: 33	Loss: 0.011507	Acc: 87.0% (8703/10000)
[Test]  Epoch: 34	Loss: 0.011851	Acc: 86.6% (8661/10000)
[Test]  Epoch: 35	Loss: 0.011278	Acc: 87.2% (8724/10000)
[Test]  Epoch: 36	Loss: 0.011140	Acc: 87.2% (8725/10000)
[Test]  Epoch: 37	Loss: 0.011555	Acc: 86.8% (8680/10000)
[Test]  Epoch: 38	Loss: 0.011667	Acc: 87.1% (8708/10000)
[Test]  Epoch: 39	Loss: 0.011602	Acc: 86.9% (8691/10000)
[Test]  Epoch: 40	Loss: 0.011783	Acc: 86.8% (8677/10000)
[Test]  Epoch: 41	Loss: 0.011468	Acc: 87.0% (8697/10000)
[Test]  Epoch: 42	Loss: 0.011360	Acc: 86.8% (8684/10000)
[Test]  Epoch: 43	Loss: 0.011169	Acc: 87.3% (8735/10000)
[Test]  Epoch: 44	Loss: 0.011443	Acc: 86.9% (8689/10000)
[Test]  Epoch: 45	Loss: 0.011034	Acc: 87.2% (8715/10000)
[Test]  Epoch: 46	Loss: 0.011451	Acc: 86.9% (8692/10000)
[Test]  Epoch: 47	Loss: 0.011857	Acc: 87.0% (8695/10000)
[Test]  Epoch: 48	Loss: 0.011501	Acc: 86.9% (8692/10000)
[Test]  Epoch: 49	Loss: 0.011195	Acc: 87.1% (8708/10000)
[Test]  Epoch: 50	Loss: 0.011295	Acc: 87.2% (8724/10000)
[Test]  Epoch: 51	Loss: 0.010966	Acc: 87.3% (8727/10000)
[Test]  Epoch: 52	Loss: 0.011218	Acc: 87.4% (8736/10000)
[Test]  Epoch: 53	Loss: 0.011026	Acc: 87.1% (8708/10000)
[Test]  Epoch: 54	Loss: 0.011166	Acc: 87.3% (8734/10000)
[Test]  Epoch: 55	Loss: 0.011055	Acc: 87.3% (8731/10000)
[Test]  Epoch: 56	Loss: 0.011076	Acc: 87.3% (8727/10000)
[Test]  Epoch: 57	Loss: 0.011070	Acc: 87.2% (8716/10000)
[Test]  Epoch: 58	Loss: 0.010933	Acc: 87.3% (8731/10000)
[Test]  Epoch: 59	Loss: 0.010994	Acc: 87.0% (8704/10000)
[Test]  Epoch: 60	Loss: 0.011263	Acc: 87.2% (8725/10000)
[Test]  Epoch: 61	Loss: 0.010695	Acc: 87.8% (8778/10000)
[Test]  Epoch: 62	Loss: 0.011235	Acc: 87.3% (8726/10000)
[Test]  Epoch: 63	Loss: 0.010739	Acc: 87.3% (8730/10000)
[Test]  Epoch: 64	Loss: 0.011034	Acc: 86.9% (8694/10000)
[Test]  Epoch: 65	Loss: 0.010744	Acc: 87.5% (8746/10000)
[Test]  Epoch: 66	Loss: 0.010867	Acc: 87.3% (8734/10000)
[Test]  Epoch: 67	Loss: 0.010839	Acc: 87.3% (8731/10000)
[Test]  Epoch: 68	Loss: 0.010731	Acc: 87.2% (8718/10000)
[Test]  Epoch: 69	Loss: 0.010617	Acc: 87.5% (8750/10000)
[Test]  Epoch: 70	Loss: 0.010980	Acc: 87.4% (8737/10000)
[Test]  Epoch: 71	Loss: 0.010796	Acc: 87.4% (8737/10000)
[Test]  Epoch: 72	Loss: 0.011002	Acc: 87.2% (8722/10000)
[Test]  Epoch: 73	Loss: 0.011165	Acc: 87.3% (8729/10000)
[Test]  Epoch: 74	Loss: 0.011042	Acc: 87.0% (8700/10000)
[Test]  Epoch: 75	Loss: 0.010757	Acc: 87.5% (8748/10000)
[Test]  Epoch: 76	Loss: 0.010645	Acc: 87.4% (8739/10000)
[Test]  Epoch: 77	Loss: 0.011415	Acc: 87.0% (8702/10000)
[Test]  Epoch: 78	Loss: 0.011075	Acc: 87.4% (8737/10000)
[Test]  Epoch: 79	Loss: 0.010851	Acc: 87.3% (8732/10000)
[Test]  Epoch: 80	Loss: 0.011099	Acc: 87.1% (8713/10000)
[Test]  Epoch: 81	Loss: 0.011052	Acc: 87.2% (8716/10000)
[Test]  Epoch: 82	Loss: 0.010754	Acc: 87.3% (8732/10000)
[Test]  Epoch: 83	Loss: 0.011059	Acc: 87.2% (8718/10000)
[Test]  Epoch: 84	Loss: 0.010960	Acc: 87.7% (8768/10000)
[Test]  Epoch: 85	Loss: 0.011035	Acc: 87.2% (8724/10000)
[Test]  Epoch: 86	Loss: 0.011114	Acc: 87.3% (8732/10000)
[Test]  Epoch: 87	Loss: 0.010998	Acc: 87.3% (8734/10000)
[Test]  Epoch: 88	Loss: 0.011096	Acc: 87.5% (8755/10000)
[Test]  Epoch: 89	Loss: 0.010593	Acc: 87.1% (8712/10000)
[Test]  Epoch: 90	Loss: 0.010733	Acc: 87.6% (8758/10000)
[Test]  Epoch: 91	Loss: 0.010959	Acc: 87.5% (8745/10000)
[Test]  Epoch: 92	Loss: 0.010903	Acc: 87.3% (8730/10000)
[Test]  Epoch: 93	Loss: 0.010748	Acc: 87.4% (8736/10000)
[Test]  Epoch: 94	Loss: 0.010560	Acc: 87.4% (8744/10000)
[Test]  Epoch: 95	Loss: 0.010867	Acc: 87.4% (8738/10000)
[Test]  Epoch: 96	Loss: 0.010611	Acc: 87.9% (8789/10000)
[Test]  Epoch: 97	Loss: 0.010674	Acc: 87.4% (8743/10000)
[Test]  Epoch: 98	Loss: 0.011115	Acc: 87.1% (8706/10000)
[Test]  Epoch: 99	Loss: 0.010977	Acc: 87.2% (8722/10000)
[Test]  Epoch: 100	Loss: 0.010805	Acc: 87.1% (8713/10000)
===========finish==========
['2024-08-19', '02:07:44.403321', '100', 'test', '0.01080490599349141', '87.13', '87.89']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight', 'features.28.weight', 'features.4.weight', 'features.25.weight', 'features.8.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.040440	Acc: 79.0% (7903/10000)
[Test]  Epoch: 2	Loss: 0.016803	Acc: 85.3% (8527/10000)
[Test]  Epoch: 3	Loss: 0.014270	Acc: 85.8% (8585/10000)
[Test]  Epoch: 4	Loss: 0.014367	Acc: 85.5% (8551/10000)
[Test]  Epoch: 5	Loss: 0.013896	Acc: 85.3% (8528/10000)
[Test]  Epoch: 6	Loss: 0.012583	Acc: 86.7% (8670/10000)
[Test]  Epoch: 7	Loss: 0.012583	Acc: 86.5% (8655/10000)
[Test]  Epoch: 8	Loss: 0.012556	Acc: 86.3% (8635/10000)
[Test]  Epoch: 9	Loss: 0.012084	Acc: 86.5% (8653/10000)
[Test]  Epoch: 10	Loss: 0.012557	Acc: 85.8% (8580/10000)
[Test]  Epoch: 11	Loss: 0.012170	Acc: 86.3% (8629/10000)
[Test]  Epoch: 12	Loss: 0.011753	Acc: 86.8% (8677/10000)
[Test]  Epoch: 13	Loss: 0.012180	Acc: 86.2% (8618/10000)
[Test]  Epoch: 14	Loss: 0.011702	Acc: 86.9% (8686/10000)
[Test]  Epoch: 15	Loss: 0.011688	Acc: 86.7% (8671/10000)
[Test]  Epoch: 16	Loss: 0.011891	Acc: 86.9% (8690/10000)
[Test]  Epoch: 17	Loss: 0.011502	Acc: 86.9% (8692/10000)
[Test]  Epoch: 18	Loss: 0.011529	Acc: 86.9% (8687/10000)
[Test]  Epoch: 19	Loss: 0.011814	Acc: 86.9% (8689/10000)
[Test]  Epoch: 20	Loss: 0.011838	Acc: 87.2% (8719/10000)
[Test]  Epoch: 21	Loss: 0.011879	Acc: 86.7% (8674/10000)
[Test]  Epoch: 22	Loss: 0.011903	Acc: 86.9% (8691/10000)
[Test]  Epoch: 23	Loss: 0.011925	Acc: 86.6% (8656/10000)
[Test]  Epoch: 24	Loss: 0.012140	Acc: 86.8% (8675/10000)
[Test]  Epoch: 25	Loss: 0.011940	Acc: 87.0% (8700/10000)
[Test]  Epoch: 26	Loss: 0.011866	Acc: 86.9% (8694/10000)
[Test]  Epoch: 27	Loss: 0.012142	Acc: 86.8% (8683/10000)
[Test]  Epoch: 28	Loss: 0.011987	Acc: 87.0% (8705/10000)
[Test]  Epoch: 29	Loss: 0.012276	Acc: 86.3% (8635/10000)
[Test]  Epoch: 30	Loss: 0.012008	Acc: 86.5% (8655/10000)
[Test]  Epoch: 31	Loss: 0.011632	Acc: 87.1% (8710/10000)
[Test]  Epoch: 32	Loss: 0.012047	Acc: 87.0% (8695/10000)
[Test]  Epoch: 33	Loss: 0.012135	Acc: 86.9% (8692/10000)
[Test]  Epoch: 34	Loss: 0.012205	Acc: 86.3% (8635/10000)
[Test]  Epoch: 35	Loss: 0.011940	Acc: 86.9% (8693/10000)
[Test]  Epoch: 36	Loss: 0.011983	Acc: 86.5% (8654/10000)
[Test]  Epoch: 37	Loss: 0.012324	Acc: 86.6% (8662/10000)
[Test]  Epoch: 38	Loss: 0.012204	Acc: 86.5% (8648/10000)
[Test]  Epoch: 39	Loss: 0.012124	Acc: 86.7% (8667/10000)
[Test]  Epoch: 40	Loss: 0.012199	Acc: 86.7% (8667/10000)
[Test]  Epoch: 41	Loss: 0.011785	Acc: 87.0% (8697/10000)
[Test]  Epoch: 42	Loss: 0.011685	Acc: 86.8% (8680/10000)
[Test]  Epoch: 43	Loss: 0.011806	Acc: 86.8% (8675/10000)
[Test]  Epoch: 44	Loss: 0.011903	Acc: 86.6% (8660/10000)
[Test]  Epoch: 45	Loss: 0.011412	Acc: 87.0% (8705/10000)
[Test]  Epoch: 46	Loss: 0.011783	Acc: 87.0% (8696/10000)
[Test]  Epoch: 47	Loss: 0.012102	Acc: 86.8% (8677/10000)
[Test]  Epoch: 48	Loss: 0.012060	Acc: 86.8% (8684/10000)
[Test]  Epoch: 49	Loss: 0.011718	Acc: 87.0% (8697/10000)
[Test]  Epoch: 50	Loss: 0.011809	Acc: 86.9% (8689/10000)
[Test]  Epoch: 51	Loss: 0.011656	Acc: 86.9% (8691/10000)
[Test]  Epoch: 52	Loss: 0.011534	Acc: 86.9% (8691/10000)
[Test]  Epoch: 53	Loss: 0.011442	Acc: 87.2% (8720/10000)
[Test]  Epoch: 54	Loss: 0.011779	Acc: 87.2% (8719/10000)
[Test]  Epoch: 55	Loss: 0.011417	Acc: 86.8% (8675/10000)
[Test]  Epoch: 56	Loss: 0.011580	Acc: 86.7% (8665/10000)
[Test]  Epoch: 57	Loss: 0.011545	Acc: 87.0% (8701/10000)
[Test]  Epoch: 58	Loss: 0.011552	Acc: 86.9% (8687/10000)
[Test]  Epoch: 59	Loss: 0.011670	Acc: 86.4% (8640/10000)
[Test]  Epoch: 60	Loss: 0.011623	Acc: 86.7% (8674/10000)
[Test]  Epoch: 61	Loss: 0.011293	Acc: 87.2% (8719/10000)
[Test]  Epoch: 62	Loss: 0.011893	Acc: 87.0% (8701/10000)
[Test]  Epoch: 63	Loss: 0.011512	Acc: 86.6% (8660/10000)
[Test]  Epoch: 64	Loss: 0.011546	Acc: 87.1% (8710/10000)
[Test]  Epoch: 65	Loss: 0.011555	Acc: 86.7% (8671/10000)
[Test]  Epoch: 66	Loss: 0.011389	Acc: 86.8% (8682/10000)
[Test]  Epoch: 67	Loss: 0.011444	Acc: 87.1% (8707/10000)
[Test]  Epoch: 68	Loss: 0.011317	Acc: 86.7% (8666/10000)
[Test]  Epoch: 69	Loss: 0.011119	Acc: 87.2% (8718/10000)
[Test]  Epoch: 70	Loss: 0.011694	Acc: 87.3% (8731/10000)
[Test]  Epoch: 71	Loss: 0.011539	Acc: 87.1% (8707/10000)
[Test]  Epoch: 72	Loss: 0.011765	Acc: 86.8% (8678/10000)
[Test]  Epoch: 73	Loss: 0.011781	Acc: 86.8% (8679/10000)
[Test]  Epoch: 74	Loss: 0.011677	Acc: 87.0% (8704/10000)
[Test]  Epoch: 75	Loss: 0.011428	Acc: 87.0% (8705/10000)
[Test]  Epoch: 76	Loss: 0.011292	Acc: 87.0% (8699/10000)
[Test]  Epoch: 77	Loss: 0.011914	Acc: 86.7% (8671/10000)
[Test]  Epoch: 78	Loss: 0.011645	Acc: 87.1% (8709/10000)
[Test]  Epoch: 79	Loss: 0.011466	Acc: 86.9% (8690/10000)
[Test]  Epoch: 80	Loss: 0.011382	Acc: 87.1% (8711/10000)
[Test]  Epoch: 81	Loss: 0.011431	Acc: 87.1% (8709/10000)
[Test]  Epoch: 82	Loss: 0.011266	Acc: 86.7% (8669/10000)
[Test]  Epoch: 83	Loss: 0.011470	Acc: 86.9% (8694/10000)
[Test]  Epoch: 84	Loss: 0.011632	Acc: 87.2% (8721/10000)
[Test]  Epoch: 85	Loss: 0.011626	Acc: 87.0% (8696/10000)
[Test]  Epoch: 86	Loss: 0.011486	Acc: 86.6% (8663/10000)
[Test]  Epoch: 87	Loss: 0.011741	Acc: 86.8% (8682/10000)
[Test]  Epoch: 88	Loss: 0.011624	Acc: 86.9% (8691/10000)
[Test]  Epoch: 89	Loss: 0.011188	Acc: 87.0% (8699/10000)
[Test]  Epoch: 90	Loss: 0.011429	Acc: 87.1% (8713/10000)
[Test]  Epoch: 91	Loss: 0.011431	Acc: 86.8% (8676/10000)
[Test]  Epoch: 92	Loss: 0.011491	Acc: 86.8% (8684/10000)
[Test]  Epoch: 93	Loss: 0.011399	Acc: 87.0% (8700/10000)
[Test]  Epoch: 94	Loss: 0.011135	Acc: 86.9% (8693/10000)
[Test]  Epoch: 95	Loss: 0.011476	Acc: 87.0% (8702/10000)
[Test]  Epoch: 96	Loss: 0.011122	Acc: 87.1% (8712/10000)
[Test]  Epoch: 97	Loss: 0.011183	Acc: 87.0% (8701/10000)
[Test]  Epoch: 98	Loss: 0.011715	Acc: 86.6% (8660/10000)
[Test]  Epoch: 99	Loss: 0.011359	Acc: 87.0% (8696/10000)
[Test]  Epoch: 100	Loss: 0.011461	Acc: 86.8% (8676/10000)
===========finish==========
['2024-08-19', '02:10:14.444789', '100', 'test', '0.011460728786885739', '86.76', '87.31']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight', 'features.28.weight', 'features.4.weight', 'features.25.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.044769	Acc: 76.5% (7654/10000)
[Test]  Epoch: 2	Loss: 0.017333	Acc: 84.5% (8449/10000)
[Test]  Epoch: 3	Loss: 0.014135	Acc: 85.3% (8527/10000)
[Test]  Epoch: 4	Loss: 0.013850	Acc: 85.1% (8506/10000)
[Test]  Epoch: 5	Loss: 0.013936	Acc: 84.8% (8478/10000)
[Test]  Epoch: 6	Loss: 0.012243	Acc: 86.0% (8598/10000)
[Test]  Epoch: 7	Loss: 0.012634	Acc: 85.9% (8594/10000)
[Test]  Epoch: 8	Loss: 0.012453	Acc: 85.8% (8578/10000)
[Test]  Epoch: 9	Loss: 0.012102	Acc: 85.7% (8574/10000)
[Test]  Epoch: 10	Loss: 0.012286	Acc: 85.4% (8542/10000)
[Test]  Epoch: 11	Loss: 0.011998	Acc: 86.0% (8605/10000)
[Test]  Epoch: 12	Loss: 0.011860	Acc: 86.4% (8644/10000)
[Test]  Epoch: 13	Loss: 0.012019	Acc: 86.0% (8600/10000)
[Test]  Epoch: 14	Loss: 0.011744	Acc: 86.1% (8610/10000)
[Test]  Epoch: 15	Loss: 0.011573	Acc: 86.3% (8635/10000)
[Test]  Epoch: 16	Loss: 0.011735	Acc: 86.0% (8602/10000)
[Test]  Epoch: 17	Loss: 0.011470	Acc: 86.3% (8628/10000)
[Test]  Epoch: 18	Loss: 0.011882	Acc: 86.5% (8652/10000)
[Test]  Epoch: 19	Loss: 0.012317	Acc: 86.0% (8602/10000)
[Test]  Epoch: 20	Loss: 0.011974	Acc: 86.2% (8620/10000)
[Test]  Epoch: 21	Loss: 0.011764	Acc: 86.5% (8647/10000)
[Test]  Epoch: 22	Loss: 0.011972	Acc: 86.4% (8642/10000)
[Test]  Epoch: 23	Loss: 0.012166	Acc: 86.0% (8602/10000)
[Test]  Epoch: 24	Loss: 0.012259	Acc: 86.0% (8603/10000)
[Test]  Epoch: 25	Loss: 0.011932	Acc: 86.2% (8617/10000)
[Test]  Epoch: 26	Loss: 0.011807	Acc: 86.5% (8645/10000)
[Test]  Epoch: 27	Loss: 0.011914	Acc: 86.2% (8624/10000)
[Test]  Epoch: 28	Loss: 0.012003	Acc: 86.3% (8628/10000)
[Test]  Epoch: 29	Loss: 0.012108	Acc: 86.1% (8611/10000)
[Test]  Epoch: 30	Loss: 0.012042	Acc: 86.2% (8616/10000)
[Test]  Epoch: 31	Loss: 0.011766	Acc: 86.5% (8652/10000)
[Test]  Epoch: 32	Loss: 0.011952	Acc: 86.5% (8654/10000)
[Test]  Epoch: 33	Loss: 0.011731	Acc: 86.6% (8664/10000)
[Test]  Epoch: 34	Loss: 0.012007	Acc: 86.1% (8612/10000)
[Test]  Epoch: 35	Loss: 0.012177	Acc: 86.5% (8651/10000)
[Test]  Epoch: 36	Loss: 0.012022	Acc: 86.4% (8638/10000)
[Test]  Epoch: 37	Loss: 0.012446	Acc: 86.1% (8608/10000)
[Test]  Epoch: 38	Loss: 0.012440	Acc: 86.0% (8596/10000)
[Test]  Epoch: 39	Loss: 0.012349	Acc: 85.9% (8591/10000)
[Test]  Epoch: 40	Loss: 0.012292	Acc: 86.0% (8600/10000)
[Test]  Epoch: 41	Loss: 0.012046	Acc: 86.3% (8634/10000)
[Test]  Epoch: 42	Loss: 0.011964	Acc: 86.3% (8627/10000)
[Test]  Epoch: 43	Loss: 0.012006	Acc: 86.3% (8629/10000)
[Test]  Epoch: 44	Loss: 0.012184	Acc: 85.9% (8590/10000)
[Test]  Epoch: 45	Loss: 0.011884	Acc: 86.1% (8610/10000)
[Test]  Epoch: 46	Loss: 0.012166	Acc: 85.9% (8588/10000)
[Test]  Epoch: 47	Loss: 0.012255	Acc: 85.9% (8592/10000)
[Test]  Epoch: 48	Loss: 0.012225	Acc: 86.0% (8599/10000)
[Test]  Epoch: 49	Loss: 0.012011	Acc: 86.3% (8635/10000)
[Test]  Epoch: 50	Loss: 0.011909	Acc: 86.1% (8608/10000)
[Test]  Epoch: 51	Loss: 0.011853	Acc: 86.6% (8657/10000)
[Test]  Epoch: 52	Loss: 0.011895	Acc: 86.4% (8641/10000)
[Test]  Epoch: 53	Loss: 0.011756	Acc: 86.1% (8608/10000)
[Test]  Epoch: 54	Loss: 0.012127	Acc: 86.3% (8633/10000)
[Test]  Epoch: 55	Loss: 0.011567	Acc: 86.4% (8643/10000)
[Test]  Epoch: 56	Loss: 0.011819	Acc: 86.4% (8639/10000)
[Test]  Epoch: 57	Loss: 0.011640	Acc: 86.2% (8616/10000)
[Test]  Epoch: 58	Loss: 0.011732	Acc: 86.4% (8637/10000)
[Test]  Epoch: 59	Loss: 0.011939	Acc: 86.1% (8614/10000)
[Test]  Epoch: 60	Loss: 0.011895	Acc: 86.3% (8630/10000)
[Test]  Epoch: 61	Loss: 0.011697	Acc: 86.5% (8649/10000)
[Test]  Epoch: 62	Loss: 0.012078	Acc: 86.7% (8665/10000)
[Test]  Epoch: 63	Loss: 0.011851	Acc: 86.0% (8599/10000)
[Test]  Epoch: 64	Loss: 0.011917	Acc: 86.3% (8631/10000)
[Test]  Epoch: 65	Loss: 0.011602	Acc: 86.5% (8650/10000)
[Test]  Epoch: 66	Loss: 0.011799	Acc: 86.0% (8600/10000)
[Test]  Epoch: 67	Loss: 0.011583	Acc: 86.8% (8681/10000)
[Test]  Epoch: 68	Loss: 0.011553	Acc: 86.5% (8651/10000)
[Test]  Epoch: 69	Loss: 0.011472	Acc: 86.5% (8646/10000)
[Test]  Epoch: 70	Loss: 0.011927	Acc: 86.5% (8646/10000)
[Test]  Epoch: 71	Loss: 0.011641	Acc: 86.6% (8657/10000)
[Test]  Epoch: 72	Loss: 0.012096	Acc: 86.1% (8610/10000)
[Test]  Epoch: 73	Loss: 0.012023	Acc: 86.3% (8630/10000)
[Test]  Epoch: 74	Loss: 0.012021	Acc: 86.2% (8616/10000)
[Test]  Epoch: 75	Loss: 0.011689	Acc: 86.7% (8674/10000)
[Test]  Epoch: 76	Loss: 0.011751	Acc: 86.2% (8624/10000)
[Test]  Epoch: 77	Loss: 0.012215	Acc: 86.1% (8614/10000)
[Test]  Epoch: 78	Loss: 0.011920	Acc: 86.4% (8642/10000)
[Test]  Epoch: 79	Loss: 0.011830	Acc: 86.1% (8614/10000)
[Test]  Epoch: 80	Loss: 0.011739	Acc: 86.4% (8639/10000)
[Test]  Epoch: 81	Loss: 0.011826	Acc: 86.4% (8638/10000)
[Test]  Epoch: 82	Loss: 0.011630	Acc: 86.3% (8633/10000)
[Test]  Epoch: 83	Loss: 0.011812	Acc: 86.5% (8647/10000)
[Test]  Epoch: 84	Loss: 0.011763	Acc: 86.6% (8664/10000)
[Test]  Epoch: 85	Loss: 0.011847	Acc: 86.3% (8635/10000)
[Test]  Epoch: 86	Loss: 0.011535	Acc: 86.5% (8648/10000)
[Test]  Epoch: 87	Loss: 0.012122	Acc: 86.3% (8628/10000)
[Test]  Epoch: 88	Loss: 0.011954	Acc: 86.2% (8615/10000)
[Test]  Epoch: 89	Loss: 0.011432	Acc: 86.3% (8634/10000)
[Test]  Epoch: 90	Loss: 0.011649	Acc: 86.8% (8679/10000)
[Test]  Epoch: 91	Loss: 0.011533	Acc: 86.4% (8638/10000)
[Test]  Epoch: 92	Loss: 0.011856	Acc: 86.5% (8655/10000)
[Test]  Epoch: 93	Loss: 0.011696	Acc: 86.4% (8642/10000)
[Test]  Epoch: 94	Loss: 0.011385	Acc: 86.6% (8656/10000)
[Test]  Epoch: 95	Loss: 0.011609	Acc: 86.3% (8631/10000)
[Test]  Epoch: 96	Loss: 0.011353	Acc: 86.6% (8658/10000)
[Test]  Epoch: 97	Loss: 0.011501	Acc: 86.4% (8638/10000)
[Test]  Epoch: 98	Loss: 0.012032	Acc: 86.0% (8603/10000)
[Test]  Epoch: 99	Loss: 0.011729	Acc: 86.2% (8622/10000)
[Test]  Epoch: 100	Loss: 0.011838	Acc: 86.2% (8622/10000)
===========finish==========
['2024-08-19', '02:12:39.973050', '100', 'test', '0.011837919390201568', '86.22', '86.81']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight', 'features.28.weight', 'features.4.weight', 'features.25.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035907	Acc: 20.6% (2061/10000)
[Test]  Epoch: 2	Loss: 0.028973	Acc: 31.2% (3119/10000)
[Test]  Epoch: 3	Loss: 0.029819	Acc: 33.6% (3358/10000)
[Test]  Epoch: 4	Loss: 0.028828	Acc: 34.4% (3444/10000)
[Test]  Epoch: 5	Loss: 0.027098	Acc: 38.1% (3814/10000)
[Test]  Epoch: 6	Loss: 0.026860	Acc: 39.0% (3899/10000)
[Test]  Epoch: 7	Loss: 0.027395	Acc: 37.7% (3766/10000)
[Test]  Epoch: 8	Loss: 0.026352	Acc: 39.8% (3983/10000)
[Test]  Epoch: 9	Loss: 0.026073	Acc: 41.4% (4137/10000)
[Test]  Epoch: 10	Loss: 0.025806	Acc: 42.9% (4285/10000)
[Test]  Epoch: 11	Loss: 0.025920	Acc: 43.8% (4381/10000)
[Test]  Epoch: 12	Loss: 0.024627	Acc: 45.1% (4515/10000)
[Test]  Epoch: 13	Loss: 0.025708	Acc: 45.5% (4546/10000)
[Test]  Epoch: 14	Loss: 0.026131	Acc: 44.7% (4470/10000)
[Test]  Epoch: 15	Loss: 0.025058	Acc: 47.0% (4696/10000)
[Test]  Epoch: 16	Loss: 0.026399	Acc: 45.8% (4575/10000)
[Test]  Epoch: 17	Loss: 0.024963	Acc: 47.6% (4761/10000)
[Test]  Epoch: 18	Loss: 0.024679	Acc: 48.0% (4796/10000)
[Test]  Epoch: 19	Loss: 0.026446	Acc: 47.5% (4750/10000)
[Test]  Epoch: 20	Loss: 0.025373	Acc: 49.4% (4943/10000)
[Test]  Epoch: 21	Loss: 0.025316	Acc: 49.4% (4939/10000)
[Test]  Epoch: 22	Loss: 0.026890	Acc: 49.0% (4896/10000)
[Test]  Epoch: 23	Loss: 0.024496	Acc: 52.6% (5256/10000)
[Test]  Epoch: 24	Loss: 0.026137	Acc: 50.0% (5000/10000)
[Test]  Epoch: 25	Loss: 0.024788	Acc: 51.9% (5193/10000)
[Test]  Epoch: 26	Loss: 0.027470	Acc: 50.0% (5000/10000)
[Test]  Epoch: 27	Loss: 0.028208	Acc: 48.0% (4805/10000)
[Test]  Epoch: 28	Loss: 0.026242	Acc: 51.7% (5171/10000)
[Test]  Epoch: 29	Loss: 0.025680	Acc: 53.4% (5340/10000)
[Test]  Epoch: 30	Loss: 0.028116	Acc: 52.6% (5257/10000)
[Test]  Epoch: 31	Loss: 0.026602	Acc: 52.8% (5284/10000)
[Test]  Epoch: 32	Loss: 0.026851	Acc: 52.5% (5255/10000)
[Test]  Epoch: 33	Loss: 0.027108	Acc: 53.4% (5335/10000)
[Test]  Epoch: 34	Loss: 0.027159	Acc: 54.1% (5408/10000)
[Test]  Epoch: 35	Loss: 0.028789	Acc: 51.6% (5156/10000)
[Test]  Epoch: 36	Loss: 0.028807	Acc: 52.6% (5264/10000)
[Test]  Epoch: 37	Loss: 0.029074	Acc: 53.7% (5371/10000)
[Test]  Epoch: 38	Loss: 0.027862	Acc: 55.6% (5559/10000)
[Test]  Epoch: 39	Loss: 0.028574	Acc: 54.5% (5449/10000)
[Test]  Epoch: 40	Loss: 0.030004	Acc: 53.4% (5343/10000)
[Test]  Epoch: 41	Loss: 0.029198	Acc: 54.7% (5471/10000)
[Test]  Epoch: 42	Loss: 0.029232	Acc: 55.4% (5540/10000)
[Test]  Epoch: 43	Loss: 0.030776	Acc: 53.7% (5372/10000)
[Test]  Epoch: 44	Loss: 0.030278	Acc: 54.5% (5449/10000)
[Test]  Epoch: 45	Loss: 0.032009	Acc: 53.4% (5344/10000)
[Test]  Epoch: 46	Loss: 0.031710	Acc: 53.4% (5339/10000)
[Test]  Epoch: 47	Loss: 0.031918	Acc: 53.6% (5364/10000)
[Test]  Epoch: 48	Loss: 0.030414	Acc: 55.6% (5562/10000)
[Test]  Epoch: 49	Loss: 0.034029	Acc: 52.1% (5212/10000)
[Test]  Epoch: 50	Loss: 0.032923	Acc: 53.8% (5379/10000)
[Test]  Epoch: 51	Loss: 0.031081	Acc: 55.4% (5542/10000)
[Test]  Epoch: 52	Loss: 0.031597	Acc: 56.4% (5644/10000)
[Test]  Epoch: 53	Loss: 0.034086	Acc: 55.1% (5512/10000)
[Test]  Epoch: 54	Loss: 0.033780	Acc: 55.8% (5576/10000)
[Test]  Epoch: 55	Loss: 0.032984	Acc: 55.9% (5594/10000)
[Test]  Epoch: 56	Loss: 0.032737	Acc: 55.9% (5585/10000)
[Test]  Epoch: 57	Loss: 0.033379	Acc: 55.8% (5583/10000)
[Test]  Epoch: 58	Loss: 0.033522	Acc: 55.8% (5583/10000)
[Test]  Epoch: 59	Loss: 0.035333	Acc: 54.4% (5444/10000)
[Test]  Epoch: 60	Loss: 0.032283	Acc: 57.3% (5730/10000)
[Test]  Epoch: 61	Loss: 0.032344	Acc: 57.4% (5738/10000)
[Test]  Epoch: 62	Loss: 0.031996	Acc: 57.3% (5730/10000)
[Test]  Epoch: 63	Loss: 0.031792	Acc: 57.2% (5721/10000)
[Test]  Epoch: 64	Loss: 0.031631	Acc: 58.0% (5801/10000)
[Test]  Epoch: 65	Loss: 0.031962	Acc: 57.9% (5785/10000)
[Test]  Epoch: 66	Loss: 0.031661	Acc: 57.7% (5766/10000)
[Test]  Epoch: 67	Loss: 0.032005	Acc: 57.3% (5732/10000)
[Test]  Epoch: 68	Loss: 0.032570	Acc: 56.5% (5653/10000)
[Test]  Epoch: 69	Loss: 0.031800	Acc: 57.2% (5721/10000)
[Test]  Epoch: 70	Loss: 0.032034	Acc: 57.0% (5704/10000)
[Test]  Epoch: 71	Loss: 0.032200	Acc: 57.5% (5745/10000)
[Test]  Epoch: 72	Loss: 0.032225	Acc: 57.6% (5758/10000)
[Test]  Epoch: 73	Loss: 0.032090	Acc: 57.4% (5740/10000)
[Test]  Epoch: 74	Loss: 0.031948	Acc: 57.7% (5767/10000)
[Test]  Epoch: 75	Loss: 0.032132	Acc: 57.6% (5758/10000)
[Test]  Epoch: 76	Loss: 0.031691	Acc: 58.3% (5831/10000)
[Test]  Epoch: 77	Loss: 0.031758	Acc: 57.8% (5782/10000)
[Test]  Epoch: 78	Loss: 0.032012	Acc: 57.9% (5793/10000)
[Test]  Epoch: 79	Loss: 0.031787	Acc: 58.1% (5809/10000)
[Test]  Epoch: 80	Loss: 0.031877	Acc: 58.0% (5797/10000)
[Test]  Epoch: 81	Loss: 0.031419	Acc: 58.2% (5821/10000)
[Test]  Epoch: 82	Loss: 0.031806	Acc: 57.9% (5787/10000)
[Test]  Epoch: 83	Loss: 0.031964	Acc: 58.1% (5808/10000)
[Test]  Epoch: 84	Loss: 0.032167	Acc: 57.5% (5752/10000)
[Test]  Epoch: 85	Loss: 0.031227	Acc: 58.3% (5829/10000)
[Test]  Epoch: 86	Loss: 0.031971	Acc: 58.5% (5854/10000)
[Test]  Epoch: 87	Loss: 0.031709	Acc: 57.9% (5787/10000)
[Test]  Epoch: 88	Loss: 0.031894	Acc: 58.0% (5801/10000)
[Test]  Epoch: 89	Loss: 0.032161	Acc: 57.8% (5778/10000)
[Test]  Epoch: 90	Loss: 0.032124	Acc: 57.7% (5767/10000)
[Test]  Epoch: 91	Loss: 0.032075	Acc: 57.7% (5773/10000)
[Test]  Epoch: 92	Loss: 0.032298	Acc: 57.8% (5784/10000)
[Test]  Epoch: 93	Loss: 0.032169	Acc: 57.9% (5790/10000)
[Test]  Epoch: 94	Loss: 0.031953	Acc: 57.8% (5777/10000)
[Test]  Epoch: 95	Loss: 0.032034	Acc: 57.5% (5749/10000)
[Test]  Epoch: 96	Loss: 0.031929	Acc: 57.9% (5791/10000)
[Test]  Epoch: 97	Loss: 0.032613	Acc: 57.9% (5785/10000)
[Test]  Epoch: 98	Loss: 0.032502	Acc: 57.8% (5783/10000)
[Test]  Epoch: 99	Loss: 0.032260	Acc: 58.3% (5827/10000)
[Test]  Epoch: 100	Loss: 0.032585	Acc: 57.8% (5783/10000)
===========finish==========
['2024-08-19', '02:15:05.475425', '100', 'test', '0.03258537776470184', '57.83', '58.54']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight', 'features.28.weight', 'features.4.weight', 'features.25.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight', 'features.7.weight', 'features.10.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.040415	Acc: 20.3% (2032/10000)
[Test]  Epoch: 2	Loss: 0.030466	Acc: 30.2% (3019/10000)
[Test]  Epoch: 3	Loss: 0.031064	Acc: 31.1% (3105/10000)
[Test]  Epoch: 4	Loss: 0.031712	Acc: 28.5% (2852/10000)
[Test]  Epoch: 5	Loss: 0.029032	Acc: 34.6% (3456/10000)
[Test]  Epoch: 6	Loss: 0.028543	Acc: 34.8% (3478/10000)
[Test]  Epoch: 7	Loss: 0.030367	Acc: 33.9% (3386/10000)
[Test]  Epoch: 8	Loss: 0.028280	Acc: 36.3% (3629/10000)
[Test]  Epoch: 9	Loss: 0.026936	Acc: 39.4% (3936/10000)
[Test]  Epoch: 10	Loss: 0.030120	Acc: 35.5% (3552/10000)
[Test]  Epoch: 11	Loss: 0.030758	Acc: 36.8% (3675/10000)
[Test]  Epoch: 12	Loss: 0.026207	Acc: 42.6% (4265/10000)
[Test]  Epoch: 13	Loss: 0.026755	Acc: 41.6% (4158/10000)
[Test]  Epoch: 14	Loss: 0.029382	Acc: 39.7% (3968/10000)
[Test]  Epoch: 15	Loss: 0.031314	Acc: 40.9% (4086/10000)
[Test]  Epoch: 16	Loss: 0.026632	Acc: 44.1% (4412/10000)
[Test]  Epoch: 17	Loss: 0.028148	Acc: 43.3% (4329/10000)
[Test]  Epoch: 18	Loss: 0.030352	Acc: 42.7% (4274/10000)
[Test]  Epoch: 19	Loss: 0.031079	Acc: 42.9% (4292/10000)
[Test]  Epoch: 20	Loss: 0.029787	Acc: 42.0% (4205/10000)
[Test]  Epoch: 21	Loss: 0.029506	Acc: 43.2% (4320/10000)
[Test]  Epoch: 22	Loss: 0.029234	Acc: 44.5% (4455/10000)
[Test]  Epoch: 23	Loss: 0.030701	Acc: 43.6% (4363/10000)
[Test]  Epoch: 24	Loss: 0.030075	Acc: 44.9% (4494/10000)
[Test]  Epoch: 25	Loss: 0.033500	Acc: 44.0% (4404/10000)
[Test]  Epoch: 26	Loss: 0.030760	Acc: 46.2% (4618/10000)
[Test]  Epoch: 27	Loss: 0.030803	Acc: 44.9% (4491/10000)
[Test]  Epoch: 28	Loss: 0.033185	Acc: 44.8% (4480/10000)
[Test]  Epoch: 29	Loss: 0.033537	Acc: 44.0% (4398/10000)
[Test]  Epoch: 30	Loss: 0.034293	Acc: 45.2% (4523/10000)
[Test]  Epoch: 31	Loss: 0.033472	Acc: 47.1% (4712/10000)
[Test]  Epoch: 32	Loss: 0.031646	Acc: 48.2% (4820/10000)
[Test]  Epoch: 33	Loss: 0.036060	Acc: 44.9% (4490/10000)
[Test]  Epoch: 34	Loss: 0.032710	Acc: 47.5% (4749/10000)
[Test]  Epoch: 35	Loss: 0.035941	Acc: 47.7% (4771/10000)
[Test]  Epoch: 36	Loss: 0.035285	Acc: 46.7% (4671/10000)
[Test]  Epoch: 37	Loss: 0.035405	Acc: 46.1% (4606/10000)
[Test]  Epoch: 38	Loss: 0.034276	Acc: 48.1% (4815/10000)
[Test]  Epoch: 39	Loss: 0.035832	Acc: 47.2% (4721/10000)
[Test]  Epoch: 40	Loss: 0.034679	Acc: 47.4% (4744/10000)
[Test]  Epoch: 41	Loss: 0.035848	Acc: 48.4% (4844/10000)
[Test]  Epoch: 42	Loss: 0.037647	Acc: 45.9% (4590/10000)
[Test]  Epoch: 43	Loss: 0.040973	Acc: 46.8% (4681/10000)
[Test]  Epoch: 44	Loss: 0.040072	Acc: 45.6% (4559/10000)
[Test]  Epoch: 45	Loss: 0.037466	Acc: 48.7% (4869/10000)
[Test]  Epoch: 46	Loss: 0.038628	Acc: 47.1% (4712/10000)
[Test]  Epoch: 47	Loss: 0.037423	Acc: 47.5% (4746/10000)
[Test]  Epoch: 48	Loss: 0.037677	Acc: 49.0% (4900/10000)
[Test]  Epoch: 49	Loss: 0.037939	Acc: 47.3% (4727/10000)
[Test]  Epoch: 50	Loss: 0.037435	Acc: 49.4% (4937/10000)
[Test]  Epoch: 51	Loss: 0.038935	Acc: 49.4% (4938/10000)
[Test]  Epoch: 52	Loss: 0.036672	Acc: 49.1% (4915/10000)
[Test]  Epoch: 53	Loss: 0.039154	Acc: 47.6% (4761/10000)
[Test]  Epoch: 54	Loss: 0.037828	Acc: 49.3% (4934/10000)
[Test]  Epoch: 55	Loss: 0.039455	Acc: 48.8% (4883/10000)
[Test]  Epoch: 56	Loss: 0.038079	Acc: 49.7% (4971/10000)
[Test]  Epoch: 57	Loss: 0.039205	Acc: 48.9% (4885/10000)
[Test]  Epoch: 58	Loss: 0.039065	Acc: 49.8% (4978/10000)
[Test]  Epoch: 59	Loss: 0.038066	Acc: 50.8% (5081/10000)
[Test]  Epoch: 60	Loss: 0.040474	Acc: 49.1% (4913/10000)
[Test]  Epoch: 61	Loss: 0.037321	Acc: 51.5% (5148/10000)
[Test]  Epoch: 62	Loss: 0.036753	Acc: 51.3% (5131/10000)
[Test]  Epoch: 63	Loss: 0.037158	Acc: 51.6% (5163/10000)
[Test]  Epoch: 64	Loss: 0.037089	Acc: 51.5% (5154/10000)
[Test]  Epoch: 65	Loss: 0.036422	Acc: 51.8% (5182/10000)
[Test]  Epoch: 66	Loss: 0.037033	Acc: 51.5% (5154/10000)
[Test]  Epoch: 67	Loss: 0.037552	Acc: 51.1% (5112/10000)
[Test]  Epoch: 68	Loss: 0.037446	Acc: 51.7% (5170/10000)
[Test]  Epoch: 69	Loss: 0.037092	Acc: 51.6% (5161/10000)
[Test]  Epoch: 70	Loss: 0.036835	Acc: 51.9% (5191/10000)
[Test]  Epoch: 71	Loss: 0.036823	Acc: 51.6% (5163/10000)
[Test]  Epoch: 72	Loss: 0.037616	Acc: 50.9% (5089/10000)
[Test]  Epoch: 73	Loss: 0.036881	Acc: 51.6% (5160/10000)
[Test]  Epoch: 74	Loss: 0.037120	Acc: 51.5% (5146/10000)
[Test]  Epoch: 75	Loss: 0.037158	Acc: 51.3% (5130/10000)
[Test]  Epoch: 76	Loss: 0.037199	Acc: 51.7% (5174/10000)
[Test]  Epoch: 77	Loss: 0.037294	Acc: 51.9% (5187/10000)
[Test]  Epoch: 78	Loss: 0.037381	Acc: 51.8% (5180/10000)
[Test]  Epoch: 79	Loss: 0.037394	Acc: 52.1% (5211/10000)
[Test]  Epoch: 80	Loss: 0.037200	Acc: 51.7% (5173/10000)
[Test]  Epoch: 81	Loss: 0.037006	Acc: 51.7% (5167/10000)
[Test]  Epoch: 82	Loss: 0.036963	Acc: 52.0% (5198/10000)
[Test]  Epoch: 83	Loss: 0.037290	Acc: 52.1% (5214/10000)
[Test]  Epoch: 84	Loss: 0.036949	Acc: 51.9% (5188/10000)
[Test]  Epoch: 85	Loss: 0.036960	Acc: 52.2% (5216/10000)
[Test]  Epoch: 86	Loss: 0.037467	Acc: 51.8% (5178/10000)
[Test]  Epoch: 87	Loss: 0.037469	Acc: 51.5% (5147/10000)
[Test]  Epoch: 88	Loss: 0.037475	Acc: 51.0% (5102/10000)
[Test]  Epoch: 89	Loss: 0.037235	Acc: 51.9% (5194/10000)
[Test]  Epoch: 90	Loss: 0.037333	Acc: 52.2% (5216/10000)
[Test]  Epoch: 91	Loss: 0.037106	Acc: 52.0% (5199/10000)
[Test]  Epoch: 92	Loss: 0.037192	Acc: 52.1% (5211/10000)
[Test]  Epoch: 93	Loss: 0.037574	Acc: 51.9% (5190/10000)
[Test]  Epoch: 94	Loss: 0.037491	Acc: 51.7% (5174/10000)
[Test]  Epoch: 95	Loss: 0.037233	Acc: 51.6% (5164/10000)
[Test]  Epoch: 96	Loss: 0.037406	Acc: 51.4% (5142/10000)
[Test]  Epoch: 97	Loss: 0.037107	Acc: 51.9% (5186/10000)
[Test]  Epoch: 98	Loss: 0.037317	Acc: 51.9% (5185/10000)
[Test]  Epoch: 99	Loss: 0.037314	Acc: 51.7% (5172/10000)
[Test]  Epoch: 100	Loss: 0.037054	Acc: 52.5% (5252/10000)
===========finish==========
['2024-08-19', '02:17:37.537116', '100', 'test', '0.03705404604673385', '52.52', '52.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight', 'features.28.weight', 'features.4.weight', 'features.25.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight', 'features.7.weight', 'features.10.weight', 'features.40.weight', 'features.37.weight', 'features.34.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035807	Acc: 18.3% (1832/10000)
[Test]  Epoch: 2	Loss: 0.032507	Acc: 23.8% (2376/10000)
[Test]  Epoch: 3	Loss: 0.030827	Acc: 29.7% (2968/10000)
[Test]  Epoch: 4	Loss: 0.029708	Acc: 29.8% (2981/10000)
[Test]  Epoch: 5	Loss: 0.027821	Acc: 35.1% (3512/10000)
[Test]  Epoch: 6	Loss: 0.028060	Acc: 35.4% (3540/10000)
[Test]  Epoch: 7	Loss: 0.027949	Acc: 36.6% (3664/10000)
[Test]  Epoch: 8	Loss: 0.026860	Acc: 38.0% (3800/10000)
[Test]  Epoch: 9	Loss: 0.026990	Acc: 38.7% (3874/10000)
[Test]  Epoch: 10	Loss: 0.027424	Acc: 39.1% (3908/10000)
[Test]  Epoch: 11	Loss: 0.027922	Acc: 38.8% (3875/10000)
[Test]  Epoch: 12	Loss: 0.026442	Acc: 42.1% (4215/10000)
[Test]  Epoch: 13	Loss: 0.026801	Acc: 41.4% (4135/10000)
[Test]  Epoch: 14	Loss: 0.027953	Acc: 41.3% (4133/10000)
[Test]  Epoch: 15	Loss: 0.029561	Acc: 41.7% (4174/10000)
[Test]  Epoch: 16	Loss: 0.031123	Acc: 37.9% (3794/10000)
[Test]  Epoch: 17	Loss: 0.027045	Acc: 45.1% (4509/10000)
[Test]  Epoch: 18	Loss: 0.026125	Acc: 44.6% (4465/10000)
[Test]  Epoch: 19	Loss: 0.029310	Acc: 42.3% (4226/10000)
[Test]  Epoch: 20	Loss: 0.030034	Acc: 43.4% (4344/10000)
[Test]  Epoch: 21	Loss: 0.027460	Acc: 45.4% (4536/10000)
[Test]  Epoch: 22	Loss: 0.028159	Acc: 44.5% (4455/10000)
[Test]  Epoch: 23	Loss: 0.027429	Acc: 46.7% (4667/10000)
[Test]  Epoch: 24	Loss: 0.026798	Acc: 47.4% (4738/10000)
[Test]  Epoch: 25	Loss: 0.028512	Acc: 45.2% (4522/10000)
[Test]  Epoch: 26	Loss: 0.027517	Acc: 48.7% (4874/10000)
[Test]  Epoch: 27	Loss: 0.030671	Acc: 44.1% (4412/10000)
[Test]  Epoch: 28	Loss: 0.028403	Acc: 47.8% (4781/10000)
[Test]  Epoch: 29	Loss: 0.028004	Acc: 48.8% (4875/10000)
[Test]  Epoch: 30	Loss: 0.030001	Acc: 47.0% (4697/10000)
[Test]  Epoch: 31	Loss: 0.030566	Acc: 46.9% (4693/10000)
[Test]  Epoch: 32	Loss: 0.030187	Acc: 47.5% (4754/10000)
[Test]  Epoch: 33	Loss: 0.031317	Acc: 47.3% (4727/10000)
[Test]  Epoch: 34	Loss: 0.032356	Acc: 46.2% (4623/10000)
[Test]  Epoch: 35	Loss: 0.030666	Acc: 48.1% (4811/10000)
[Test]  Epoch: 36	Loss: 0.030483	Acc: 49.4% (4942/10000)
[Test]  Epoch: 37	Loss: 0.032018	Acc: 47.8% (4783/10000)
[Test]  Epoch: 38	Loss: 0.030795	Acc: 50.2% (5025/10000)
[Test]  Epoch: 39	Loss: 0.033300	Acc: 47.6% (4759/10000)
[Test]  Epoch: 40	Loss: 0.033146	Acc: 48.7% (4866/10000)
[Test]  Epoch: 41	Loss: 0.031509	Acc: 50.4% (5037/10000)
[Test]  Epoch: 42	Loss: 0.034356	Acc: 47.5% (4753/10000)
[Test]  Epoch: 43	Loss: 0.033579	Acc: 49.3% (4926/10000)
[Test]  Epoch: 44	Loss: 0.034259	Acc: 48.5% (4850/10000)
[Test]  Epoch: 45	Loss: 0.031484	Acc: 50.6% (5061/10000)
[Test]  Epoch: 46	Loss: 0.031761	Acc: 49.8% (4975/10000)
[Test]  Epoch: 47	Loss: 0.034002	Acc: 47.9% (4787/10000)
[Test]  Epoch: 48	Loss: 0.033014	Acc: 50.5% (5054/10000)
[Test]  Epoch: 49	Loss: 0.037029	Acc: 47.6% (4764/10000)
[Test]  Epoch: 50	Loss: 0.033795	Acc: 49.7% (4972/10000)
[Test]  Epoch: 51	Loss: 0.034579	Acc: 49.5% (4947/10000)
[Test]  Epoch: 52	Loss: 0.034266	Acc: 51.1% (5115/10000)
[Test]  Epoch: 53	Loss: 0.035448	Acc: 48.5% (4847/10000)
[Test]  Epoch: 54	Loss: 0.034517	Acc: 49.7% (4971/10000)
[Test]  Epoch: 55	Loss: 0.033870	Acc: 51.1% (5112/10000)
[Test]  Epoch: 56	Loss: 0.037636	Acc: 47.9% (4788/10000)
[Test]  Epoch: 57	Loss: 0.034518	Acc: 50.3% (5027/10000)
[Test]  Epoch: 58	Loss: 0.035869	Acc: 49.5% (4945/10000)
[Test]  Epoch: 59	Loss: 0.035239	Acc: 49.5% (4952/10000)
[Test]  Epoch: 60	Loss: 0.034534	Acc: 50.9% (5092/10000)
[Test]  Epoch: 61	Loss: 0.033333	Acc: 51.9% (5185/10000)
[Test]  Epoch: 62	Loss: 0.032920	Acc: 52.9% (5285/10000)
[Test]  Epoch: 63	Loss: 0.033458	Acc: 52.2% (5224/10000)
[Test]  Epoch: 64	Loss: 0.033825	Acc: 51.8% (5177/10000)
[Test]  Epoch: 65	Loss: 0.033296	Acc: 52.5% (5245/10000)
[Test]  Epoch: 66	Loss: 0.033126	Acc: 52.0% (5204/10000)
[Test]  Epoch: 67	Loss: 0.033301	Acc: 52.0% (5204/10000)
[Test]  Epoch: 68	Loss: 0.033237	Acc: 52.3% (5232/10000)
[Test]  Epoch: 69	Loss: 0.033372	Acc: 52.3% (5230/10000)
[Test]  Epoch: 70	Loss: 0.033028	Acc: 52.4% (5238/10000)
[Test]  Epoch: 71	Loss: 0.033494	Acc: 52.0% (5195/10000)
[Test]  Epoch: 72	Loss: 0.033816	Acc: 52.1% (5207/10000)
[Test]  Epoch: 73	Loss: 0.033560	Acc: 51.9% (5185/10000)
[Test]  Epoch: 74	Loss: 0.033849	Acc: 51.6% (5165/10000)
[Test]  Epoch: 75	Loss: 0.033328	Acc: 52.1% (5209/10000)
[Test]  Epoch: 76	Loss: 0.033612	Acc: 51.8% (5183/10000)
[Test]  Epoch: 77	Loss: 0.033782	Acc: 52.3% (5234/10000)
[Test]  Epoch: 78	Loss: 0.033859	Acc: 52.1% (5212/10000)
[Test]  Epoch: 79	Loss: 0.033806	Acc: 52.1% (5212/10000)
[Test]  Epoch: 80	Loss: 0.033739	Acc: 51.9% (5188/10000)
[Test]  Epoch: 81	Loss: 0.033495	Acc: 52.2% (5222/10000)
[Test]  Epoch: 82	Loss: 0.033748	Acc: 52.0% (5195/10000)
[Test]  Epoch: 83	Loss: 0.033768	Acc: 52.5% (5245/10000)
[Test]  Epoch: 84	Loss: 0.033515	Acc: 52.0% (5199/10000)
[Test]  Epoch: 85	Loss: 0.033293	Acc: 52.6% (5259/10000)
[Test]  Epoch: 86	Loss: 0.033849	Acc: 52.2% (5221/10000)
[Test]  Epoch: 87	Loss: 0.033605	Acc: 52.5% (5254/10000)
[Test]  Epoch: 88	Loss: 0.033614	Acc: 52.0% (5195/10000)
[Test]  Epoch: 89	Loss: 0.033295	Acc: 52.6% (5257/10000)
[Test]  Epoch: 90	Loss: 0.034161	Acc: 52.2% (5223/10000)
[Test]  Epoch: 91	Loss: 0.033656	Acc: 52.2% (5225/10000)
[Test]  Epoch: 92	Loss: 0.033852	Acc: 52.4% (5236/10000)
[Test]  Epoch: 93	Loss: 0.033983	Acc: 52.2% (5216/10000)
[Test]  Epoch: 94	Loss: 0.033854	Acc: 52.3% (5230/10000)
[Test]  Epoch: 95	Loss: 0.033845	Acc: 52.0% (5198/10000)
[Test]  Epoch: 96	Loss: 0.033826	Acc: 52.5% (5247/10000)
[Test]  Epoch: 97	Loss: 0.034060	Acc: 52.0% (5200/10000)
[Test]  Epoch: 98	Loss: 0.033805	Acc: 52.4% (5235/10000)
[Test]  Epoch: 99	Loss: 0.033647	Acc: 52.0% (5204/10000)
[Test]  Epoch: 100	Loss: 0.033404	Acc: 52.8% (5276/10000)
===========finish==========
['2024-08-19', '02:20:08.131294', '100', 'test', '0.033404015600681305', '52.76', '52.85']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight', 'features.28.weight', 'features.4.weight', 'features.25.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight', 'features.7.weight', 'features.10.weight', 'features.40.weight', 'features.37.weight', 'features.34.weight', 'features.30.weight', 'features.14.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035652	Acc: 20.6% (2055/10000)
[Test]  Epoch: 2	Loss: 0.032219	Acc: 22.6% (2260/10000)
[Test]  Epoch: 3	Loss: 0.029205	Acc: 31.9% (3192/10000)
[Test]  Epoch: 4	Loss: 0.030548	Acc: 29.3% (2933/10000)
[Test]  Epoch: 5	Loss: 0.028212	Acc: 34.7% (3468/10000)
[Test]  Epoch: 6	Loss: 0.030418	Acc: 33.2% (3321/10000)
[Test]  Epoch: 7	Loss: 0.031766	Acc: 29.7% (2972/10000)
[Test]  Epoch: 8	Loss: 0.030736	Acc: 30.3% (3031/10000)
[Test]  Epoch: 9	Loss: 0.027531	Acc: 36.1% (3615/10000)
[Test]  Epoch: 10	Loss: 0.029909	Acc: 34.2% (3422/10000)
[Test]  Epoch: 11	Loss: 0.027309	Acc: 38.1% (3813/10000)
[Test]  Epoch: 12	Loss: 0.028665	Acc: 36.9% (3690/10000)
[Test]  Epoch: 13	Loss: 0.027399	Acc: 40.8% (4076/10000)
[Test]  Epoch: 14	Loss: 0.029458	Acc: 36.7% (3672/10000)
[Test]  Epoch: 15	Loss: 0.031977	Acc: 38.5% (3849/10000)
[Test]  Epoch: 16	Loss: 0.027084	Acc: 42.6% (4262/10000)
[Test]  Epoch: 17	Loss: 0.027665	Acc: 42.5% (4255/10000)
[Test]  Epoch: 18	Loss: 0.032227	Acc: 38.9% (3889/10000)
[Test]  Epoch: 19	Loss: 0.028979	Acc: 42.9% (4290/10000)
[Test]  Epoch: 20	Loss: 0.030888	Acc: 39.0% (3903/10000)
[Test]  Epoch: 21	Loss: 0.031573	Acc: 38.2% (3820/10000)
[Test]  Epoch: 22	Loss: 0.029376	Acc: 41.2% (4119/10000)
[Test]  Epoch: 23	Loss: 0.030699	Acc: 41.5% (4151/10000)
[Test]  Epoch: 24	Loss: 0.029835	Acc: 43.2% (4321/10000)
[Test]  Epoch: 25	Loss: 0.032445	Acc: 41.5% (4149/10000)
[Test]  Epoch: 26	Loss: 0.032339	Acc: 42.7% (4273/10000)
[Test]  Epoch: 27	Loss: 0.033147	Acc: 41.1% (4106/10000)
[Test]  Epoch: 28	Loss: 0.034109	Acc: 40.4% (4037/10000)
[Test]  Epoch: 29	Loss: 0.036172	Acc: 38.9% (3891/10000)
[Test]  Epoch: 30	Loss: 0.031964	Acc: 44.8% (4483/10000)
[Test]  Epoch: 31	Loss: 0.035164	Acc: 40.2% (4022/10000)
[Test]  Epoch: 32	Loss: 0.031649	Acc: 44.8% (4476/10000)
[Test]  Epoch: 33	Loss: 0.032334	Acc: 45.8% (4583/10000)
[Test]  Epoch: 34	Loss: 0.036087	Acc: 41.2% (4124/10000)
[Test]  Epoch: 35	Loss: 0.034608	Acc: 44.8% (4481/10000)
[Test]  Epoch: 36	Loss: 0.040693	Acc: 38.9% (3893/10000)
[Test]  Epoch: 37	Loss: 0.037591	Acc: 42.1% (4213/10000)
[Test]  Epoch: 38	Loss: 0.037767	Acc: 41.6% (4159/10000)
[Test]  Epoch: 39	Loss: 0.033246	Acc: 46.2% (4619/10000)
[Test]  Epoch: 40	Loss: 0.034666	Acc: 44.5% (4449/10000)
[Test]  Epoch: 41	Loss: 0.034542	Acc: 45.5% (4547/10000)
[Test]  Epoch: 42	Loss: 0.038608	Acc: 45.0% (4496/10000)
[Test]  Epoch: 43	Loss: 0.036878	Acc: 45.2% (4525/10000)
[Test]  Epoch: 44	Loss: 0.036054	Acc: 44.6% (4459/10000)
[Test]  Epoch: 45	Loss: 0.037274	Acc: 44.3% (4429/10000)
[Test]  Epoch: 46	Loss: 0.033094	Acc: 46.5% (4655/10000)
[Test]  Epoch: 47	Loss: 0.036719	Acc: 46.5% (4651/10000)
[Test]  Epoch: 48	Loss: 0.038472	Acc: 45.1% (4509/10000)
[Test]  Epoch: 49	Loss: 0.036280	Acc: 47.2% (4723/10000)
[Test]  Epoch: 50	Loss: 0.039233	Acc: 44.8% (4481/10000)
[Test]  Epoch: 51	Loss: 0.035918	Acc: 47.5% (4747/10000)
[Test]  Epoch: 52	Loss: 0.039746	Acc: 46.4% (4641/10000)
[Test]  Epoch: 53	Loss: 0.035417	Acc: 48.2% (4822/10000)
[Test]  Epoch: 54	Loss: 0.036364	Acc: 47.3% (4728/10000)
[Test]  Epoch: 55	Loss: 0.036690	Acc: 48.0% (4795/10000)
[Test]  Epoch: 56	Loss: 0.038686	Acc: 45.8% (4577/10000)
[Test]  Epoch: 57	Loss: 0.038488	Acc: 47.3% (4732/10000)
[Test]  Epoch: 58	Loss: 0.037107	Acc: 47.6% (4762/10000)
[Test]  Epoch: 59	Loss: 0.039975	Acc: 46.3% (4634/10000)
[Test]  Epoch: 60	Loss: 0.036825	Acc: 48.7% (4871/10000)
[Test]  Epoch: 61	Loss: 0.035603	Acc: 49.3% (4934/10000)
[Test]  Epoch: 62	Loss: 0.035450	Acc: 49.3% (4933/10000)
[Test]  Epoch: 63	Loss: 0.035197	Acc: 50.1% (5010/10000)
[Test]  Epoch: 64	Loss: 0.035422	Acc: 49.2% (4919/10000)
[Test]  Epoch: 65	Loss: 0.035115	Acc: 49.8% (4983/10000)
[Test]  Epoch: 66	Loss: 0.034645	Acc: 50.0% (4995/10000)
[Test]  Epoch: 67	Loss: 0.034763	Acc: 49.9% (4991/10000)
[Test]  Epoch: 68	Loss: 0.035139	Acc: 49.7% (4966/10000)
[Test]  Epoch: 69	Loss: 0.035043	Acc: 50.0% (4999/10000)
[Test]  Epoch: 70	Loss: 0.034606	Acc: 50.2% (5024/10000)
[Test]  Epoch: 71	Loss: 0.034577	Acc: 50.3% (5028/10000)
[Test]  Epoch: 72	Loss: 0.034860	Acc: 49.9% (4986/10000)
[Test]  Epoch: 73	Loss: 0.034831	Acc: 50.0% (5003/10000)
[Test]  Epoch: 74	Loss: 0.034905	Acc: 49.9% (4988/10000)
[Test]  Epoch: 75	Loss: 0.034904	Acc: 50.3% (5032/10000)
[Test]  Epoch: 76	Loss: 0.035001	Acc: 50.5% (5047/10000)
[Test]  Epoch: 77	Loss: 0.034617	Acc: 50.8% (5077/10000)
[Test]  Epoch: 78	Loss: 0.034888	Acc: 50.0% (4999/10000)
[Test]  Epoch: 79	Loss: 0.034850	Acc: 50.7% (5066/10000)
[Test]  Epoch: 80	Loss: 0.034737	Acc: 50.5% (5051/10000)
[Test]  Epoch: 81	Loss: 0.034765	Acc: 50.4% (5041/10000)
[Test]  Epoch: 82	Loss: 0.034740	Acc: 50.2% (5024/10000)
[Test]  Epoch: 83	Loss: 0.034812	Acc: 50.4% (5036/10000)
[Test]  Epoch: 84	Loss: 0.034703	Acc: 50.7% (5072/10000)
[Test]  Epoch: 85	Loss: 0.034585	Acc: 50.1% (5015/10000)
[Test]  Epoch: 86	Loss: 0.034904	Acc: 50.3% (5029/10000)
[Test]  Epoch: 87	Loss: 0.034620	Acc: 50.1% (5007/10000)
[Test]  Epoch: 88	Loss: 0.034794	Acc: 50.6% (5061/10000)
[Test]  Epoch: 89	Loss: 0.034628	Acc: 50.8% (5079/10000)
[Test]  Epoch: 90	Loss: 0.034931	Acc: 49.9% (4990/10000)
[Test]  Epoch: 91	Loss: 0.035043	Acc: 49.9% (4993/10000)
[Test]  Epoch: 92	Loss: 0.035282	Acc: 49.8% (4977/10000)
[Test]  Epoch: 93	Loss: 0.034512	Acc: 50.1% (5011/10000)
[Test]  Epoch: 94	Loss: 0.034733	Acc: 49.8% (4981/10000)
[Test]  Epoch: 95	Loss: 0.034436	Acc: 50.3% (5033/10000)
[Test]  Epoch: 96	Loss: 0.034670	Acc: 50.8% (5078/10000)
[Test]  Epoch: 97	Loss: 0.035373	Acc: 49.7% (4974/10000)
[Test]  Epoch: 98	Loss: 0.035020	Acc: 50.3% (5029/10000)
[Test]  Epoch: 99	Loss: 0.034938	Acc: 50.0% (5001/10000)
[Test]  Epoch: 100	Loss: 0.034621	Acc: 50.9% (5086/10000)
===========finish==========
['2024-08-19', '02:22:40.245871', '100', 'test', '0.03462132071256638', '50.86', '50.86']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.38.weight', 'features.35.weight', 'features.1.weight', 'features.31.weight', 'features.41.weight', 'features.28.weight', 'features.4.weight', 'features.25.weight', 'features.8.weight', 'features.11.weight', 'features.18.weight', 'features.21.weight', 'features.15.weight', 'classifier.weight', 'features.0.weight', 'features.3.weight', 'features.7.weight', 'features.10.weight', 'features.40.weight', 'features.37.weight', 'features.34.weight', 'features.30.weight', 'features.14.weight', 'features.20.weight', 'features.27.weight', 'features.17.weight', 'features.24.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.056517	Acc: 10.0% (998/10000)
[Test]  Epoch: 2	Loss: 0.032556	Acc: 20.1% (2008/10000)
[Test]  Epoch: 3	Loss: 0.030754	Acc: 27.2% (2721/10000)
[Test]  Epoch: 4	Loss: 0.030990	Acc: 27.1% (2714/10000)
[Test]  Epoch: 5	Loss: 0.029373	Acc: 30.2% (3023/10000)
[Test]  Epoch: 6	Loss: 0.029375	Acc: 30.6% (3058/10000)
[Test]  Epoch: 7	Loss: 0.030021	Acc: 29.7% (2973/10000)
[Test]  Epoch: 8	Loss: 0.028961	Acc: 30.5% (3053/10000)
[Test]  Epoch: 9	Loss: 0.027975	Acc: 34.7% (3471/10000)
[Test]  Epoch: 10	Loss: 0.029920	Acc: 30.8% (3077/10000)
[Test]  Epoch: 11	Loss: 0.029666	Acc: 33.6% (3365/10000)
[Test]  Epoch: 12	Loss: 0.029186	Acc: 35.5% (3555/10000)
[Test]  Epoch: 13	Loss: 0.026820	Acc: 37.4% (3744/10000)
[Test]  Epoch: 14	Loss: 0.028626	Acc: 35.1% (3509/10000)
[Test]  Epoch: 15	Loss: 0.031834	Acc: 36.1% (3607/10000)
[Test]  Epoch: 16	Loss: 0.029172	Acc: 34.2% (3418/10000)
[Test]  Epoch: 17	Loss: 0.029230	Acc: 39.4% (3943/10000)
[Test]  Epoch: 18	Loss: 0.030776	Acc: 37.8% (3776/10000)
[Test]  Epoch: 19	Loss: 0.031246	Acc: 37.7% (3770/10000)
[Test]  Epoch: 20	Loss: 0.029350	Acc: 39.1% (3908/10000)
[Test]  Epoch: 21	Loss: 0.027543	Acc: 41.2% (4116/10000)
[Test]  Epoch: 22	Loss: 0.029171	Acc: 39.6% (3962/10000)
[Test]  Epoch: 23	Loss: 0.031735	Acc: 38.3% (3834/10000)
[Test]  Epoch: 24	Loss: 0.027687	Acc: 42.3% (4231/10000)
[Test]  Epoch: 25	Loss: 0.031403	Acc: 38.7% (3872/10000)
[Test]  Epoch: 26	Loss: 0.028254	Acc: 43.2% (4319/10000)
[Test]  Epoch: 27	Loss: 0.031527	Acc: 39.2% (3923/10000)
[Test]  Epoch: 28	Loss: 0.035352	Acc: 38.9% (3886/10000)
[Test]  Epoch: 29	Loss: 0.030067	Acc: 42.2% (4225/10000)
[Test]  Epoch: 30	Loss: 0.034967	Acc: 40.4% (4038/10000)
[Test]  Epoch: 31	Loss: 0.031715	Acc: 42.8% (4275/10000)
[Test]  Epoch: 32	Loss: 0.032580	Acc: 43.4% (4344/10000)
[Test]  Epoch: 33	Loss: 0.031971	Acc: 43.5% (4347/10000)
[Test]  Epoch: 34	Loss: 0.033487	Acc: 40.4% (4041/10000)
[Test]  Epoch: 35	Loss: 0.035137	Acc: 41.2% (4124/10000)
[Test]  Epoch: 36	Loss: 0.033113	Acc: 41.4% (4139/10000)
[Test]  Epoch: 37	Loss: 0.039281	Acc: 39.0% (3901/10000)
[Test]  Epoch: 38	Loss: 0.033156	Acc: 44.2% (4425/10000)
[Test]  Epoch: 39	Loss: 0.034534	Acc: 42.8% (4281/10000)
[Test]  Epoch: 40	Loss: 0.033661	Acc: 44.0% (4402/10000)
[Test]  Epoch: 41	Loss: 0.035382	Acc: 43.5% (4351/10000)
[Test]  Epoch: 42	Loss: 0.037916	Acc: 43.7% (4371/10000)
[Test]  Epoch: 43	Loss: 0.036769	Acc: 43.9% (4387/10000)
[Test]  Epoch: 44	Loss: 0.036215	Acc: 43.5% (4355/10000)
[Test]  Epoch: 45	Loss: 0.036607	Acc: 41.9% (4190/10000)
[Test]  Epoch: 46	Loss: 0.037881	Acc: 44.3% (4434/10000)
[Test]  Epoch: 47	Loss: 0.036773	Acc: 45.2% (4522/10000)
[Test]  Epoch: 48	Loss: 0.034905	Acc: 47.0% (4697/10000)
[Test]  Epoch: 49	Loss: 0.035507	Acc: 45.6% (4565/10000)
[Test]  Epoch: 50	Loss: 0.039349	Acc: 43.8% (4379/10000)
[Test]  Epoch: 51	Loss: 0.036620	Acc: 45.0% (4496/10000)
[Test]  Epoch: 52	Loss: 0.038193	Acc: 45.6% (4556/10000)
[Test]  Epoch: 53	Loss: 0.037237	Acc: 45.1% (4515/10000)
[Test]  Epoch: 54	Loss: 0.037992	Acc: 45.0% (4496/10000)
[Test]  Epoch: 55	Loss: 0.038401	Acc: 45.7% (4573/10000)
[Test]  Epoch: 56	Loss: 0.040270	Acc: 45.3% (4527/10000)
[Test]  Epoch: 57	Loss: 0.037767	Acc: 45.2% (4518/10000)
[Test]  Epoch: 58	Loss: 0.040654	Acc: 44.2% (4422/10000)
[Test]  Epoch: 59	Loss: 0.037673	Acc: 46.2% (4622/10000)
[Test]  Epoch: 60	Loss: 0.039338	Acc: 46.5% (4653/10000)
[Test]  Epoch: 61	Loss: 0.036256	Acc: 48.1% (4810/10000)
[Test]  Epoch: 62	Loss: 0.035975	Acc: 48.2% (4820/10000)
[Test]  Epoch: 63	Loss: 0.035513	Acc: 48.7% (4874/10000)
[Test]  Epoch: 64	Loss: 0.035647	Acc: 48.4% (4840/10000)
[Test]  Epoch: 65	Loss: 0.035605	Acc: 48.4% (4836/10000)
[Test]  Epoch: 66	Loss: 0.035728	Acc: 48.7% (4873/10000)
[Test]  Epoch: 67	Loss: 0.035606	Acc: 48.1% (4813/10000)
[Test]  Epoch: 68	Loss: 0.035283	Acc: 49.1% (4911/10000)
[Test]  Epoch: 69	Loss: 0.035634	Acc: 48.5% (4855/10000)
[Test]  Epoch: 70	Loss: 0.035066	Acc: 49.1% (4907/10000)
[Test]  Epoch: 71	Loss: 0.035411	Acc: 48.7% (4870/10000)
[Test]  Epoch: 72	Loss: 0.035221	Acc: 49.0% (4896/10000)
[Test]  Epoch: 73	Loss: 0.035343	Acc: 48.3% (4833/10000)
[Test]  Epoch: 74	Loss: 0.035694	Acc: 49.1% (4911/10000)
[Test]  Epoch: 75	Loss: 0.035455	Acc: 48.5% (4854/10000)
[Test]  Epoch: 76	Loss: 0.035910	Acc: 48.6% (4865/10000)
[Test]  Epoch: 77	Loss: 0.035626	Acc: 48.8% (4878/10000)
[Test]  Epoch: 78	Loss: 0.035698	Acc: 48.5% (4850/10000)
[Test]  Epoch: 79	Loss: 0.036019	Acc: 48.9% (4894/10000)
[Test]  Epoch: 80	Loss: 0.035429	Acc: 48.8% (4878/10000)
[Test]  Epoch: 81	Loss: 0.035727	Acc: 49.0% (4898/10000)
[Test]  Epoch: 82	Loss: 0.035685	Acc: 48.9% (4892/10000)
[Test]  Epoch: 83	Loss: 0.035474	Acc: 48.9% (4893/10000)
[Test]  Epoch: 84	Loss: 0.035727	Acc: 49.1% (4915/10000)
[Test]  Epoch: 85	Loss: 0.035299	Acc: 49.4% (4944/10000)
[Test]  Epoch: 86	Loss: 0.035450	Acc: 48.9% (4894/10000)
[Test]  Epoch: 87	Loss: 0.035459	Acc: 48.9% (4891/10000)
[Test]  Epoch: 88	Loss: 0.035371	Acc: 49.6% (4963/10000)
[Test]  Epoch: 89	Loss: 0.035444	Acc: 49.6% (4963/10000)
[Test]  Epoch: 90	Loss: 0.035479	Acc: 48.5% (4854/10000)
[Test]  Epoch: 91	Loss: 0.035433	Acc: 49.2% (4923/10000)
[Test]  Epoch: 92	Loss: 0.035464	Acc: 49.0% (4898/10000)
[Test]  Epoch: 93	Loss: 0.035828	Acc: 49.1% (4913/10000)
[Test]  Epoch: 94	Loss: 0.036050	Acc: 49.1% (4906/10000)
[Test]  Epoch: 95	Loss: 0.035564	Acc: 49.0% (4895/10000)
[Test]  Epoch: 96	Loss: 0.035590	Acc: 49.1% (4915/10000)
[Test]  Epoch: 97	Loss: 0.035751	Acc: 48.8% (4882/10000)
[Test]  Epoch: 98	Loss: 0.035872	Acc: 48.9% (4889/10000)
[Test]  Epoch: 99	Loss: 0.035747	Acc: 49.4% (4935/10000)
[Test]  Epoch: 100	Loss: 0.035388	Acc: 49.9% (4985/10000)
===========finish==========
['2024-08-19', '02:25:08.926900', '100', 'test', '0.03538802138566971', '49.85', '49.85']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=0
get_sample_layers not_random
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039889	Acc: 43.8% (4383/10000)
[Test]  Epoch: 2	Loss: 0.039520	Acc: 44.2% (4424/10000)
[Test]  Epoch: 3	Loss: 0.039682	Acc: 43.9% (4392/10000)
[Test]  Epoch: 4	Loss: 0.039572	Acc: 44.1% (4408/10000)
[Test]  Epoch: 5	Loss: 0.039677	Acc: 43.8% (4379/10000)
[Test]  Epoch: 6	Loss: 0.039751	Acc: 44.0% (4401/10000)
[Test]  Epoch: 7	Loss: 0.039859	Acc: 43.8% (4380/10000)
[Test]  Epoch: 8	Loss: 0.039848	Acc: 44.0% (4396/10000)
[Test]  Epoch: 9	Loss: 0.039922	Acc: 43.8% (4382/10000)
[Test]  Epoch: 10	Loss: 0.039993	Acc: 43.7% (4373/10000)
[Test]  Epoch: 11	Loss: 0.040026	Acc: 43.7% (4367/10000)
[Test]  Epoch: 12	Loss: 0.039994	Acc: 43.7% (4374/10000)
[Test]  Epoch: 13	Loss: 0.040033	Acc: 43.5% (4351/10000)
[Test]  Epoch: 14	Loss: 0.039996	Acc: 43.6% (4361/10000)
[Test]  Epoch: 15	Loss: 0.040072	Acc: 43.7% (4367/10000)
[Test]  Epoch: 16	Loss: 0.040096	Acc: 43.8% (4382/10000)
[Test]  Epoch: 17	Loss: 0.040070	Acc: 43.6% (4365/10000)
[Test]  Epoch: 18	Loss: 0.040097	Acc: 43.8% (4375/10000)
[Test]  Epoch: 19	Loss: 0.040128	Acc: 43.8% (4379/10000)
[Test]  Epoch: 20	Loss: 0.040221	Acc: 43.5% (4354/10000)
[Test]  Epoch: 21	Loss: 0.040257	Acc: 43.3% (4331/10000)
[Test]  Epoch: 22	Loss: 0.040214	Acc: 43.6% (4363/10000)
[Test]  Epoch: 23	Loss: 0.040217	Acc: 43.5% (4345/10000)
[Test]  Epoch: 24	Loss: 0.040235	Acc: 43.6% (4361/10000)
[Test]  Epoch: 25	Loss: 0.040239	Acc: 43.5% (4352/10000)
[Test]  Epoch: 26	Loss: 0.040390	Acc: 43.2% (4319/10000)
[Test]  Epoch: 27	Loss: 0.040248	Acc: 43.7% (4367/10000)
[Test]  Epoch: 28	Loss: 0.040382	Acc: 43.5% (4352/10000)
[Test]  Epoch: 29	Loss: 0.040376	Acc: 43.3% (4334/10000)
[Test]  Epoch: 30	Loss: 0.040329	Acc: 43.6% (4365/10000)
[Test]  Epoch: 31	Loss: 0.040464	Acc: 43.5% (4346/10000)
[Test]  Epoch: 32	Loss: 0.040444	Acc: 43.4% (4339/10000)
[Test]  Epoch: 33	Loss: 0.040337	Acc: 43.6% (4365/10000)
[Test]  Epoch: 34	Loss: 0.040417	Acc: 43.4% (4341/10000)
[Test]  Epoch: 35	Loss: 0.040420	Acc: 43.5% (4354/10000)
[Test]  Epoch: 36	Loss: 0.040361	Acc: 43.6% (4360/10000)
[Test]  Epoch: 37	Loss: 0.040400	Acc: 43.6% (4356/10000)
[Test]  Epoch: 38	Loss: 0.040445	Acc: 43.5% (4355/10000)
[Test]  Epoch: 39	Loss: 0.040498	Acc: 43.3% (4329/10000)
[Test]  Epoch: 40	Loss: 0.040443	Acc: 43.5% (4345/10000)
[Test]  Epoch: 41	Loss: 0.040546	Acc: 43.4% (4342/10000)
[Test]  Epoch: 42	Loss: 0.040509	Acc: 43.5% (4351/10000)
[Test]  Epoch: 43	Loss: 0.040487	Acc: 43.5% (4348/10000)
[Test]  Epoch: 44	Loss: 0.040483	Acc: 43.5% (4350/10000)
[Test]  Epoch: 45	Loss: 0.040494	Acc: 43.6% (4364/10000)
[Test]  Epoch: 46	Loss: 0.040596	Acc: 43.4% (4336/10000)
[Test]  Epoch: 47	Loss: 0.040512	Acc: 43.5% (4351/10000)
[Test]  Epoch: 48	Loss: 0.040481	Acc: 43.4% (4344/10000)
[Test]  Epoch: 49	Loss: 0.040526	Acc: 43.5% (4353/10000)
[Test]  Epoch: 50	Loss: 0.040518	Acc: 43.5% (4353/10000)
[Test]  Epoch: 51	Loss: 0.040535	Acc: 43.6% (4363/10000)
[Test]  Epoch: 52	Loss: 0.040558	Acc: 43.5% (4350/10000)
[Test]  Epoch: 53	Loss: 0.040573	Acc: 43.5% (4349/10000)
[Test]  Epoch: 54	Loss: 0.040576	Acc: 43.6% (4365/10000)
[Test]  Epoch: 55	Loss: 0.040628	Acc: 43.6% (4365/10000)
[Test]  Epoch: 56	Loss: 0.040624	Acc: 43.5% (4349/10000)
[Test]  Epoch: 57	Loss: 0.040649	Acc: 43.6% (4360/10000)
[Test]  Epoch: 58	Loss: 0.040573	Acc: 43.4% (4344/10000)
[Test]  Epoch: 59	Loss: 0.040636	Acc: 43.5% (4349/10000)
[Test]  Epoch: 60	Loss: 0.040723	Acc: 43.4% (4344/10000)
[Test]  Epoch: 61	Loss: 0.040749	Acc: 43.3% (4328/10000)
[Test]  Epoch: 62	Loss: 0.040686	Acc: 43.4% (4344/10000)
[Test]  Epoch: 63	Loss: 0.040642	Acc: 43.5% (4345/10000)
[Test]  Epoch: 64	Loss: 0.040606	Acc: 43.5% (4351/10000)
[Test]  Epoch: 65	Loss: 0.040658	Acc: 43.4% (4341/10000)
[Test]  Epoch: 66	Loss: 0.040668	Acc: 43.5% (4349/10000)
[Test]  Epoch: 67	Loss: 0.040690	Acc: 43.7% (4369/10000)
[Test]  Epoch: 68	Loss: 0.040717	Acc: 43.4% (4337/10000)
[Test]  Epoch: 69	Loss: 0.040690	Acc: 43.4% (4342/10000)
[Test]  Epoch: 70	Loss: 0.040647	Acc: 43.4% (4341/10000)
[Test]  Epoch: 71	Loss: 0.040662	Acc: 43.4% (4342/10000)
[Test]  Epoch: 72	Loss: 0.040677	Acc: 43.3% (4331/10000)
[Test]  Epoch: 73	Loss: 0.040651	Acc: 43.4% (4344/10000)
[Test]  Epoch: 74	Loss: 0.040644	Acc: 43.6% (4362/10000)
[Test]  Epoch: 75	Loss: 0.040706	Acc: 43.5% (4355/10000)
[Test]  Epoch: 76	Loss: 0.040637	Acc: 43.6% (4362/10000)
[Test]  Epoch: 77	Loss: 0.040636	Acc: 43.6% (4359/10000)
[Test]  Epoch: 78	Loss: 0.040667	Acc: 43.6% (4356/10000)
[Test]  Epoch: 79	Loss: 0.040683	Acc: 43.6% (4359/10000)
[Test]  Epoch: 80	Loss: 0.040623	Acc: 43.7% (4369/10000)
[Test]  Epoch: 81	Loss: 0.040633	Acc: 43.7% (4370/10000)
[Test]  Epoch: 82	Loss: 0.040683	Acc: 43.7% (4367/10000)
[Test]  Epoch: 83	Loss: 0.040723	Acc: 43.5% (4351/10000)
[Test]  Epoch: 84	Loss: 0.040732	Acc: 43.4% (4344/10000)
[Test]  Epoch: 85	Loss: 0.040703	Acc: 43.5% (4355/10000)
[Test]  Epoch: 86	Loss: 0.040663	Acc: 43.6% (4356/10000)
[Test]  Epoch: 87	Loss: 0.040634	Acc: 43.4% (4344/10000)
[Test]  Epoch: 88	Loss: 0.040733	Acc: 43.4% (4344/10000)
[Test]  Epoch: 89	Loss: 0.040671	Acc: 43.6% (4357/10000)
[Test]  Epoch: 90	Loss: 0.040673	Acc: 43.5% (4352/10000)
[Test]  Epoch: 91	Loss: 0.040690	Acc: 43.7% (4368/10000)
[Test]  Epoch: 92	Loss: 0.040635	Acc: 43.6% (4363/10000)
[Test]  Epoch: 93	Loss: 0.040690	Acc: 43.5% (4353/10000)
[Test]  Epoch: 94	Loss: 0.040665	Acc: 43.7% (4366/10000)
[Test]  Epoch: 95	Loss: 0.040652	Acc: 43.6% (4356/10000)
[Test]  Epoch: 96	Loss: 0.040667	Acc: 43.5% (4355/10000)
[Test]  Epoch: 97	Loss: 0.040675	Acc: 43.5% (4351/10000)
[Test]  Epoch: 98	Loss: 0.040652	Acc: 43.5% (4350/10000)
[Test]  Epoch: 99	Loss: 0.040673	Acc: 43.5% (4352/10000)
[Test]  Epoch: 100	Loss: 0.040714	Acc: 43.4% (4338/10000)
===========finish==========
['2024-08-19', '02:27:35.958399', '100', 'test', '0.04071366533637047', '43.38', '44.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.060330	Acc: 16.6% (1655/10000)
[Test]  Epoch: 2	Loss: 0.046452	Acc: 32.7% (3270/10000)
[Test]  Epoch: 3	Loss: 0.044521	Acc: 36.8% (3678/10000)
[Test]  Epoch: 4	Loss: 0.043787	Acc: 37.5% (3752/10000)
[Test]  Epoch: 5	Loss: 0.043992	Acc: 38.0% (3796/10000)
[Test]  Epoch: 6	Loss: 0.043622	Acc: 38.3% (3828/10000)
[Test]  Epoch: 7	Loss: 0.043386	Acc: 38.8% (3884/10000)
[Test]  Epoch: 8	Loss: 0.043574	Acc: 38.3% (3830/10000)
[Test]  Epoch: 9	Loss: 0.043422	Acc: 39.3% (3929/10000)
[Test]  Epoch: 10	Loss: 0.043624	Acc: 38.4% (3843/10000)
[Test]  Epoch: 11	Loss: 0.043684	Acc: 38.7% (3871/10000)
[Test]  Epoch: 12	Loss: 0.043619	Acc: 39.1% (3907/10000)
[Test]  Epoch: 13	Loss: 0.043434	Acc: 39.2% (3916/10000)
[Test]  Epoch: 14	Loss: 0.043333	Acc: 39.6% (3960/10000)
[Test]  Epoch: 15	Loss: 0.043287	Acc: 39.9% (3989/10000)
[Test]  Epoch: 16	Loss: 0.043327	Acc: 39.8% (3978/10000)
[Test]  Epoch: 17	Loss: 0.043267	Acc: 39.7% (3967/10000)
[Test]  Epoch: 18	Loss: 0.043240	Acc: 39.8% (3976/10000)
[Test]  Epoch: 19	Loss: 0.043498	Acc: 39.6% (3963/10000)
[Test]  Epoch: 20	Loss: 0.043483	Acc: 39.7% (3969/10000)
[Test]  Epoch: 21	Loss: 0.043207	Acc: 39.8% (3983/10000)
[Test]  Epoch: 22	Loss: 0.043691	Acc: 39.8% (3981/10000)
[Test]  Epoch: 23	Loss: 0.043382	Acc: 39.9% (3992/10000)
[Test]  Epoch: 24	Loss: 0.043247	Acc: 40.1% (4006/10000)
[Test]  Epoch: 25	Loss: 0.043252	Acc: 39.9% (3993/10000)
[Test]  Epoch: 26	Loss: 0.043398	Acc: 39.9% (3988/10000)
[Test]  Epoch: 27	Loss: 0.043295	Acc: 40.0% (3999/10000)
[Test]  Epoch: 28	Loss: 0.043365	Acc: 39.9% (3992/10000)
[Test]  Epoch: 29	Loss: 0.043305	Acc: 40.2% (4020/10000)
[Test]  Epoch: 30	Loss: 0.043381	Acc: 40.2% (4025/10000)
[Test]  Epoch: 31	Loss: 0.043354	Acc: 40.1% (4015/10000)
[Test]  Epoch: 32	Loss: 0.043168	Acc: 40.3% (4031/10000)
[Test]  Epoch: 33	Loss: 0.043402	Acc: 40.2% (4021/10000)
[Test]  Epoch: 34	Loss: 0.043449	Acc: 40.3% (4034/10000)
[Test]  Epoch: 35	Loss: 0.043270	Acc: 40.2% (4018/10000)
[Test]  Epoch: 36	Loss: 0.043199	Acc: 40.4% (4039/10000)
[Test]  Epoch: 37	Loss: 0.043289	Acc: 40.8% (4080/10000)
[Test]  Epoch: 38	Loss: 0.043194	Acc: 40.6% (4059/10000)
[Test]  Epoch: 39	Loss: 0.043489	Acc: 40.2% (4024/10000)
[Test]  Epoch: 40	Loss: 0.043536	Acc: 40.0% (4002/10000)
[Test]  Epoch: 41	Loss: 0.043314	Acc: 40.2% (4025/10000)
[Test]  Epoch: 42	Loss: 0.043310	Acc: 40.4% (4035/10000)
[Test]  Epoch: 43	Loss: 0.043204	Acc: 40.4% (4035/10000)
[Test]  Epoch: 44	Loss: 0.043383	Acc: 40.3% (4027/10000)
[Test]  Epoch: 45	Loss: 0.043230	Acc: 40.5% (4045/10000)
[Test]  Epoch: 46	Loss: 0.043219	Acc: 40.2% (4022/10000)
[Test]  Epoch: 47	Loss: 0.043407	Acc: 40.7% (4071/10000)
[Test]  Epoch: 48	Loss: 0.043480	Acc: 40.6% (4062/10000)
[Test]  Epoch: 49	Loss: 0.043473	Acc: 40.5% (4047/10000)
[Test]  Epoch: 50	Loss: 0.043313	Acc: 40.4% (4040/10000)
[Test]  Epoch: 51	Loss: 0.043362	Acc: 40.6% (4065/10000)
[Test]  Epoch: 52	Loss: 0.043351	Acc: 40.5% (4053/10000)
[Test]  Epoch: 53	Loss: 0.043489	Acc: 40.2% (4016/10000)
[Test]  Epoch: 54	Loss: 0.043452	Acc: 40.7% (4071/10000)
[Test]  Epoch: 55	Loss: 0.043355	Acc: 40.9% (4087/10000)
[Test]  Epoch: 56	Loss: 0.043450	Acc: 40.6% (4062/10000)
[Test]  Epoch: 57	Loss: 0.043254	Acc: 40.6% (4058/10000)
[Test]  Epoch: 58	Loss: 0.043409	Acc: 40.6% (4058/10000)
[Test]  Epoch: 59	Loss: 0.043441	Acc: 40.6% (4063/10000)
[Test]  Epoch: 60	Loss: 0.043483	Acc: 40.5% (4047/10000)
[Test]  Epoch: 61	Loss: 0.043537	Acc: 40.5% (4046/10000)
[Test]  Epoch: 62	Loss: 0.043506	Acc: 40.5% (4049/10000)
[Test]  Epoch: 63	Loss: 0.043449	Acc: 40.5% (4048/10000)
[Test]  Epoch: 64	Loss: 0.043439	Acc: 40.6% (4059/10000)
[Test]  Epoch: 65	Loss: 0.043481	Acc: 40.6% (4056/10000)
[Test]  Epoch: 66	Loss: 0.043482	Acc: 40.6% (4060/10000)
[Test]  Epoch: 67	Loss: 0.043526	Acc: 40.6% (4061/10000)
[Test]  Epoch: 68	Loss: 0.043565	Acc: 40.4% (4043/10000)
[Test]  Epoch: 69	Loss: 0.043523	Acc: 40.6% (4056/10000)
[Test]  Epoch: 70	Loss: 0.043500	Acc: 40.7% (4067/10000)
[Test]  Epoch: 71	Loss: 0.043505	Acc: 40.6% (4059/10000)
[Test]  Epoch: 72	Loss: 0.043530	Acc: 40.6% (4064/10000)
[Test]  Epoch: 73	Loss: 0.043452	Acc: 40.8% (4078/10000)
[Test]  Epoch: 74	Loss: 0.043441	Acc: 40.6% (4065/10000)
[Test]  Epoch: 75	Loss: 0.043515	Acc: 40.6% (4062/10000)
[Test]  Epoch: 76	Loss: 0.043452	Acc: 40.8% (4082/10000)
[Test]  Epoch: 77	Loss: 0.043454	Acc: 40.9% (4090/10000)
[Test]  Epoch: 78	Loss: 0.043498	Acc: 40.5% (4045/10000)
[Test]  Epoch: 79	Loss: 0.043524	Acc: 40.7% (4070/10000)
[Test]  Epoch: 80	Loss: 0.043455	Acc: 40.6% (4061/10000)
[Test]  Epoch: 81	Loss: 0.043454	Acc: 40.6% (4059/10000)
[Test]  Epoch: 82	Loss: 0.043446	Acc: 40.5% (4052/10000)
[Test]  Epoch: 83	Loss: 0.043521	Acc: 40.5% (4048/10000)
[Test]  Epoch: 84	Loss: 0.043544	Acc: 40.7% (4070/10000)
[Test]  Epoch: 85	Loss: 0.043535	Acc: 40.6% (4061/10000)
[Test]  Epoch: 86	Loss: 0.043503	Acc: 40.6% (4064/10000)
[Test]  Epoch: 87	Loss: 0.043474	Acc: 40.6% (4060/10000)
[Test]  Epoch: 88	Loss: 0.043514	Acc: 40.6% (4056/10000)
[Test]  Epoch: 89	Loss: 0.043439	Acc: 40.7% (4070/10000)
[Test]  Epoch: 90	Loss: 0.043468	Acc: 41.0% (4097/10000)
[Test]  Epoch: 91	Loss: 0.043480	Acc: 40.6% (4065/10000)
[Test]  Epoch: 92	Loss: 0.043445	Acc: 40.8% (4082/10000)
[Test]  Epoch: 93	Loss: 0.043508	Acc: 40.8% (4077/10000)
[Test]  Epoch: 94	Loss: 0.043487	Acc: 40.8% (4079/10000)
[Test]  Epoch: 95	Loss: 0.043443	Acc: 40.8% (4078/10000)
[Test]  Epoch: 96	Loss: 0.043409	Acc: 40.8% (4080/10000)
[Test]  Epoch: 97	Loss: 0.043472	Acc: 40.5% (4051/10000)
[Test]  Epoch: 98	Loss: 0.043434	Acc: 40.6% (4064/10000)
[Test]  Epoch: 99	Loss: 0.043468	Acc: 40.7% (4069/10000)
[Test]  Epoch: 100	Loss: 0.043497	Acc: 40.6% (4062/10000)
===========finish==========
['2024-08-19', '02:29:54.533232', '100', 'test', '0.04349663438796997', '40.62', '40.97']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.066477	Acc: 12.8% (1284/10000)
[Test]  Epoch: 2	Loss: 0.050686	Acc: 27.1% (2713/10000)
[Test]  Epoch: 3	Loss: 0.048063	Acc: 31.8% (3179/10000)
[Test]  Epoch: 4	Loss: 0.047273	Acc: 32.8% (3284/10000)
[Test]  Epoch: 5	Loss: 0.047158	Acc: 33.8% (3379/10000)
[Test]  Epoch: 6	Loss: 0.046560	Acc: 34.1% (3406/10000)
[Test]  Epoch: 7	Loss: 0.046046	Acc: 34.9% (3492/10000)
[Test]  Epoch: 8	Loss: 0.046282	Acc: 35.0% (3496/10000)
[Test]  Epoch: 9	Loss: 0.046092	Acc: 35.5% (3546/10000)
[Test]  Epoch: 10	Loss: 0.046078	Acc: 35.3% (3531/10000)
[Test]  Epoch: 11	Loss: 0.045994	Acc: 35.7% (3566/10000)
[Test]  Epoch: 12	Loss: 0.045964	Acc: 36.1% (3610/10000)
[Test]  Epoch: 13	Loss: 0.045570	Acc: 36.4% (3635/10000)
[Test]  Epoch: 14	Loss: 0.045564	Acc: 36.2% (3622/10000)
[Test]  Epoch: 15	Loss: 0.045685	Acc: 36.1% (3608/10000)
[Test]  Epoch: 16	Loss: 0.045600	Acc: 36.1% (3613/10000)
[Test]  Epoch: 17	Loss: 0.045338	Acc: 36.3% (3634/10000)
[Test]  Epoch: 18	Loss: 0.045297	Acc: 36.7% (3672/10000)
[Test]  Epoch: 19	Loss: 0.045775	Acc: 36.6% (3659/10000)
[Test]  Epoch: 20	Loss: 0.045676	Acc: 36.6% (3656/10000)
[Test]  Epoch: 21	Loss: 0.045161	Acc: 36.8% (3682/10000)
[Test]  Epoch: 22	Loss: 0.045788	Acc: 36.4% (3644/10000)
[Test]  Epoch: 23	Loss: 0.045221	Acc: 37.0% (3701/10000)
[Test]  Epoch: 24	Loss: 0.045355	Acc: 37.1% (3713/10000)
[Test]  Epoch: 25	Loss: 0.045172	Acc: 37.0% (3695/10000)
[Test]  Epoch: 26	Loss: 0.045317	Acc: 37.2% (3717/10000)
[Test]  Epoch: 27	Loss: 0.045249	Acc: 37.3% (3728/10000)
[Test]  Epoch: 28	Loss: 0.045175	Acc: 37.3% (3728/10000)
[Test]  Epoch: 29	Loss: 0.045225	Acc: 37.0% (3697/10000)
[Test]  Epoch: 30	Loss: 0.045404	Acc: 37.0% (3704/10000)
[Test]  Epoch: 31	Loss: 0.045163	Acc: 37.4% (3742/10000)
[Test]  Epoch: 32	Loss: 0.045238	Acc: 37.2% (3724/10000)
[Test]  Epoch: 33	Loss: 0.045304	Acc: 37.6% (3761/10000)
[Test]  Epoch: 34	Loss: 0.045380	Acc: 37.4% (3735/10000)
[Test]  Epoch: 35	Loss: 0.045184	Acc: 37.6% (3757/10000)
[Test]  Epoch: 36	Loss: 0.045018	Acc: 37.9% (3790/10000)
[Test]  Epoch: 37	Loss: 0.045105	Acc: 37.5% (3752/10000)
[Test]  Epoch: 38	Loss: 0.045018	Acc: 37.8% (3775/10000)
[Test]  Epoch: 39	Loss: 0.045349	Acc: 37.3% (3733/10000)
[Test]  Epoch: 40	Loss: 0.045318	Acc: 37.4% (3743/10000)
[Test]  Epoch: 41	Loss: 0.045172	Acc: 37.6% (3762/10000)
[Test]  Epoch: 42	Loss: 0.045122	Acc: 37.8% (3782/10000)
[Test]  Epoch: 43	Loss: 0.045053	Acc: 37.8% (3775/10000)
[Test]  Epoch: 44	Loss: 0.045233	Acc: 37.7% (3768/10000)
[Test]  Epoch: 45	Loss: 0.045136	Acc: 37.8% (3777/10000)
[Test]  Epoch: 46	Loss: 0.045109	Acc: 37.5% (3751/10000)
[Test]  Epoch: 47	Loss: 0.045149	Acc: 38.0% (3804/10000)
[Test]  Epoch: 48	Loss: 0.045324	Acc: 37.7% (3767/10000)
[Test]  Epoch: 49	Loss: 0.045233	Acc: 37.8% (3781/10000)
[Test]  Epoch: 50	Loss: 0.045144	Acc: 37.8% (3782/10000)
[Test]  Epoch: 51	Loss: 0.045081	Acc: 38.1% (3809/10000)
[Test]  Epoch: 52	Loss: 0.045145	Acc: 37.7% (3774/10000)
[Test]  Epoch: 53	Loss: 0.045202	Acc: 37.9% (3786/10000)
[Test]  Epoch: 54	Loss: 0.045243	Acc: 37.9% (3793/10000)
[Test]  Epoch: 55	Loss: 0.045083	Acc: 38.0% (3795/10000)
[Test]  Epoch: 56	Loss: 0.045195	Acc: 37.9% (3790/10000)
[Test]  Epoch: 57	Loss: 0.045096	Acc: 38.0% (3800/10000)
[Test]  Epoch: 58	Loss: 0.045112	Acc: 38.1% (3808/10000)
[Test]  Epoch: 59	Loss: 0.045238	Acc: 38.0% (3797/10000)
[Test]  Epoch: 60	Loss: 0.045159	Acc: 38.1% (3808/10000)
[Test]  Epoch: 61	Loss: 0.045249	Acc: 38.1% (3811/10000)
[Test]  Epoch: 62	Loss: 0.045255	Acc: 38.2% (3817/10000)
[Test]  Epoch: 63	Loss: 0.045191	Acc: 38.1% (3811/10000)
[Test]  Epoch: 64	Loss: 0.045186	Acc: 38.4% (3835/10000)
[Test]  Epoch: 65	Loss: 0.045198	Acc: 38.1% (3811/10000)
[Test]  Epoch: 66	Loss: 0.045200	Acc: 38.2% (3816/10000)
[Test]  Epoch: 67	Loss: 0.045269	Acc: 37.9% (3792/10000)
[Test]  Epoch: 68	Loss: 0.045336	Acc: 37.8% (3780/10000)
[Test]  Epoch: 69	Loss: 0.045265	Acc: 37.9% (3792/10000)
[Test]  Epoch: 70	Loss: 0.045258	Acc: 38.1% (3807/10000)
[Test]  Epoch: 71	Loss: 0.045230	Acc: 38.0% (3798/10000)
[Test]  Epoch: 72	Loss: 0.045255	Acc: 38.0% (3797/10000)
[Test]  Epoch: 73	Loss: 0.045207	Acc: 38.1% (3806/10000)
[Test]  Epoch: 74	Loss: 0.045190	Acc: 38.1% (3815/10000)
[Test]  Epoch: 75	Loss: 0.045232	Acc: 38.0% (3797/10000)
[Test]  Epoch: 76	Loss: 0.045191	Acc: 38.3% (3827/10000)
[Test]  Epoch: 77	Loss: 0.045194	Acc: 38.2% (3819/10000)
[Test]  Epoch: 78	Loss: 0.045250	Acc: 37.9% (3789/10000)
[Test]  Epoch: 79	Loss: 0.045261	Acc: 38.1% (3813/10000)
[Test]  Epoch: 80	Loss: 0.045200	Acc: 38.0% (3801/10000)
[Test]  Epoch: 81	Loss: 0.045204	Acc: 38.0% (3797/10000)
[Test]  Epoch: 82	Loss: 0.045206	Acc: 38.1% (3806/10000)
[Test]  Epoch: 83	Loss: 0.045289	Acc: 37.9% (3793/10000)
[Test]  Epoch: 84	Loss: 0.045289	Acc: 38.0% (3799/10000)
[Test]  Epoch: 85	Loss: 0.045287	Acc: 37.9% (3791/10000)
[Test]  Epoch: 86	Loss: 0.045284	Acc: 38.0% (3804/10000)
[Test]  Epoch: 87	Loss: 0.045219	Acc: 38.0% (3800/10000)
[Test]  Epoch: 88	Loss: 0.045253	Acc: 38.0% (3799/10000)
[Test]  Epoch: 89	Loss: 0.045212	Acc: 38.0% (3803/10000)
[Test]  Epoch: 90	Loss: 0.045234	Acc: 38.0% (3803/10000)
[Test]  Epoch: 91	Loss: 0.045219	Acc: 38.1% (3815/10000)
[Test]  Epoch: 92	Loss: 0.045195	Acc: 38.1% (3814/10000)
[Test]  Epoch: 93	Loss: 0.045241	Acc: 38.1% (3806/10000)
[Test]  Epoch: 94	Loss: 0.045210	Acc: 38.0% (3804/10000)
[Test]  Epoch: 95	Loss: 0.045168	Acc: 38.0% (3801/10000)
[Test]  Epoch: 96	Loss: 0.045166	Acc: 38.1% (3810/10000)
[Test]  Epoch: 97	Loss: 0.045221	Acc: 37.9% (3792/10000)
[Test]  Epoch: 98	Loss: 0.045167	Acc: 38.1% (3815/10000)
[Test]  Epoch: 99	Loss: 0.045200	Acc: 38.1% (3809/10000)
[Test]  Epoch: 100	Loss: 0.045230	Acc: 38.0% (3804/10000)
===========finish==========
['2024-08-19', '02:32:29.961345', '100', 'test', '0.04522990639209747', '38.04', '38.35']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.070478	Acc: 9.9% (987/10000)
[Test]  Epoch: 2	Loss: 0.051869	Acc: 25.7% (2570/10000)
[Test]  Epoch: 3	Loss: 0.049397	Acc: 29.8% (2975/10000)
[Test]  Epoch: 4	Loss: 0.048413	Acc: 31.6% (3156/10000)
[Test]  Epoch: 5	Loss: 0.048630	Acc: 31.7% (3166/10000)
[Test]  Epoch: 6	Loss: 0.047700	Acc: 32.5% (3255/10000)
[Test]  Epoch: 7	Loss: 0.047345	Acc: 32.9% (3294/10000)
[Test]  Epoch: 8	Loss: 0.047460	Acc: 32.9% (3286/10000)
[Test]  Epoch: 9	Loss: 0.047330	Acc: 33.8% (3382/10000)
[Test]  Epoch: 10	Loss: 0.047532	Acc: 33.2% (3320/10000)
[Test]  Epoch: 11	Loss: 0.047177	Acc: 34.1% (3406/10000)
[Test]  Epoch: 12	Loss: 0.047252	Acc: 34.1% (3408/10000)
[Test]  Epoch: 13	Loss: 0.046888	Acc: 34.9% (3488/10000)
[Test]  Epoch: 14	Loss: 0.046859	Acc: 34.4% (3439/10000)
[Test]  Epoch: 15	Loss: 0.047045	Acc: 34.8% (3480/10000)
[Test]  Epoch: 16	Loss: 0.046813	Acc: 34.9% (3488/10000)
[Test]  Epoch: 17	Loss: 0.046601	Acc: 35.0% (3499/10000)
[Test]  Epoch: 18	Loss: 0.046678	Acc: 35.2% (3517/10000)
[Test]  Epoch: 19	Loss: 0.046817	Acc: 35.5% (3547/10000)
[Test]  Epoch: 20	Loss: 0.046857	Acc: 35.4% (3537/10000)
[Test]  Epoch: 21	Loss: 0.046403	Acc: 35.6% (3557/10000)
[Test]  Epoch: 22	Loss: 0.047001	Acc: 35.2% (3518/10000)
[Test]  Epoch: 23	Loss: 0.046741	Acc: 35.6% (3562/10000)
[Test]  Epoch: 24	Loss: 0.046622	Acc: 35.9% (3585/10000)
[Test]  Epoch: 25	Loss: 0.046483	Acc: 35.8% (3580/10000)
[Test]  Epoch: 26	Loss: 0.046708	Acc: 35.4% (3544/10000)
[Test]  Epoch: 27	Loss: 0.046576	Acc: 35.9% (3590/10000)
[Test]  Epoch: 28	Loss: 0.046543	Acc: 35.8% (3575/10000)
[Test]  Epoch: 29	Loss: 0.046516	Acc: 35.9% (3587/10000)
[Test]  Epoch: 30	Loss: 0.046788	Acc: 35.6% (3557/10000)
[Test]  Epoch: 31	Loss: 0.046425	Acc: 36.1% (3614/10000)
[Test]  Epoch: 32	Loss: 0.046323	Acc: 36.3% (3629/10000)
[Test]  Epoch: 33	Loss: 0.046499	Acc: 36.3% (3626/10000)
[Test]  Epoch: 34	Loss: 0.046603	Acc: 36.2% (3618/10000)
[Test]  Epoch: 35	Loss: 0.046425	Acc: 35.6% (3559/10000)
[Test]  Epoch: 36	Loss: 0.046302	Acc: 36.3% (3634/10000)
[Test]  Epoch: 37	Loss: 0.046416	Acc: 36.6% (3659/10000)
[Test]  Epoch: 38	Loss: 0.046299	Acc: 36.4% (3635/10000)
[Test]  Epoch: 39	Loss: 0.046661	Acc: 36.2% (3624/10000)
[Test]  Epoch: 40	Loss: 0.046785	Acc: 36.1% (3613/10000)
[Test]  Epoch: 41	Loss: 0.046408	Acc: 36.5% (3653/10000)
[Test]  Epoch: 42	Loss: 0.046329	Acc: 36.5% (3653/10000)
[Test]  Epoch: 43	Loss: 0.046344	Acc: 36.2% (3622/10000)
[Test]  Epoch: 44	Loss: 0.046380	Acc: 36.7% (3669/10000)
[Test]  Epoch: 45	Loss: 0.046437	Acc: 36.6% (3656/10000)
[Test]  Epoch: 46	Loss: 0.046429	Acc: 36.4% (3636/10000)
[Test]  Epoch: 47	Loss: 0.046534	Acc: 36.4% (3643/10000)
[Test]  Epoch: 48	Loss: 0.046625	Acc: 36.4% (3642/10000)
[Test]  Epoch: 49	Loss: 0.046541	Acc: 36.6% (3664/10000)
[Test]  Epoch: 50	Loss: 0.046533	Acc: 36.5% (3647/10000)
[Test]  Epoch: 51	Loss: 0.046474	Acc: 36.6% (3658/10000)
[Test]  Epoch: 52	Loss: 0.046395	Acc: 36.5% (3649/10000)
[Test]  Epoch: 53	Loss: 0.046530	Acc: 36.5% (3649/10000)
[Test]  Epoch: 54	Loss: 0.046529	Acc: 36.4% (3643/10000)
[Test]  Epoch: 55	Loss: 0.046419	Acc: 36.5% (3655/10000)
[Test]  Epoch: 56	Loss: 0.046528	Acc: 36.4% (3644/10000)
[Test]  Epoch: 57	Loss: 0.046257	Acc: 36.3% (3631/10000)
[Test]  Epoch: 58	Loss: 0.046506	Acc: 36.7% (3671/10000)
[Test]  Epoch: 59	Loss: 0.046410	Acc: 36.5% (3651/10000)
[Test]  Epoch: 60	Loss: 0.046565	Acc: 36.4% (3636/10000)
[Test]  Epoch: 61	Loss: 0.046624	Acc: 36.3% (3633/10000)
[Test]  Epoch: 62	Loss: 0.046637	Acc: 36.5% (3653/10000)
[Test]  Epoch: 63	Loss: 0.046516	Acc: 36.3% (3631/10000)
[Test]  Epoch: 64	Loss: 0.046497	Acc: 36.8% (3677/10000)
[Test]  Epoch: 65	Loss: 0.046546	Acc: 36.6% (3660/10000)
[Test]  Epoch: 66	Loss: 0.046537	Acc: 36.5% (3651/10000)
[Test]  Epoch: 67	Loss: 0.046574	Acc: 36.6% (3658/10000)
[Test]  Epoch: 68	Loss: 0.046614	Acc: 36.4% (3642/10000)
[Test]  Epoch: 69	Loss: 0.046575	Acc: 36.5% (3655/10000)
[Test]  Epoch: 70	Loss: 0.046543	Acc: 36.6% (3658/10000)
[Test]  Epoch: 71	Loss: 0.046552	Acc: 36.8% (3677/10000)
[Test]  Epoch: 72	Loss: 0.046570	Acc: 36.8% (3677/10000)
[Test]  Epoch: 73	Loss: 0.046522	Acc: 36.7% (3670/10000)
[Test]  Epoch: 74	Loss: 0.046513	Acc: 36.7% (3667/10000)
[Test]  Epoch: 75	Loss: 0.046515	Acc: 36.9% (3687/10000)
[Test]  Epoch: 76	Loss: 0.046491	Acc: 36.8% (3676/10000)
[Test]  Epoch: 77	Loss: 0.046492	Acc: 36.8% (3675/10000)
[Test]  Epoch: 78	Loss: 0.046542	Acc: 36.5% (3651/10000)
[Test]  Epoch: 79	Loss: 0.046580	Acc: 36.5% (3652/10000)
[Test]  Epoch: 80	Loss: 0.046521	Acc: 36.8% (3681/10000)
[Test]  Epoch: 81	Loss: 0.046500	Acc: 36.8% (3677/10000)
[Test]  Epoch: 82	Loss: 0.046522	Acc: 36.5% (3654/10000)
[Test]  Epoch: 83	Loss: 0.046590	Acc: 36.6% (3656/10000)
[Test]  Epoch: 84	Loss: 0.046618	Acc: 36.8% (3675/10000)
[Test]  Epoch: 85	Loss: 0.046612	Acc: 36.6% (3658/10000)
[Test]  Epoch: 86	Loss: 0.046583	Acc: 36.7% (3668/10000)
[Test]  Epoch: 87	Loss: 0.046531	Acc: 36.8% (3684/10000)
[Test]  Epoch: 88	Loss: 0.046581	Acc: 36.7% (3671/10000)
[Test]  Epoch: 89	Loss: 0.046519	Acc: 36.8% (3679/10000)
[Test]  Epoch: 90	Loss: 0.046551	Acc: 36.8% (3681/10000)
[Test]  Epoch: 91	Loss: 0.046538	Acc: 36.8% (3676/10000)
[Test]  Epoch: 92	Loss: 0.046510	Acc: 36.7% (3670/10000)
[Test]  Epoch: 93	Loss: 0.046567	Acc: 36.6% (3665/10000)
[Test]  Epoch: 94	Loss: 0.046500	Acc: 36.8% (3679/10000)
[Test]  Epoch: 95	Loss: 0.046492	Acc: 36.8% (3682/10000)
[Test]  Epoch: 96	Loss: 0.046459	Acc: 36.7% (3671/10000)
[Test]  Epoch: 97	Loss: 0.046534	Acc: 36.7% (3673/10000)
[Test]  Epoch: 98	Loss: 0.046489	Acc: 36.8% (3682/10000)
[Test]  Epoch: 99	Loss: 0.046536	Acc: 36.8% (3676/10000)
[Test]  Epoch: 100	Loss: 0.046554	Acc: 36.7% (3669/10000)
===========finish==========
['2024-08-19', '02:35:31.157295', '100', 'test', '0.046554038071632386', '36.69', '36.87']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.077193	Acc: 8.4% (839/10000)
[Test]  Epoch: 2	Loss: 0.056974	Acc: 24.1% (2415/10000)
[Test]  Epoch: 3	Loss: 0.054673	Acc: 26.1% (2610/10000)
[Test]  Epoch: 4	Loss: 0.051164	Acc: 30.4% (3041/10000)
[Test]  Epoch: 5	Loss: 0.051450	Acc: 28.6% (2860/10000)
[Test]  Epoch: 6	Loss: 0.050886	Acc: 29.7% (2971/10000)
[Test]  Epoch: 7	Loss: 0.049856	Acc: 31.3% (3127/10000)
[Test]  Epoch: 8	Loss: 0.049261	Acc: 32.1% (3210/10000)
[Test]  Epoch: 9	Loss: 0.048645	Acc: 32.6% (3265/10000)
[Test]  Epoch: 10	Loss: 0.048601	Acc: 31.9% (3195/10000)
[Test]  Epoch: 11	Loss: 0.048256	Acc: 32.2% (3218/10000)
[Test]  Epoch: 12	Loss: 0.048223	Acc: 32.6% (3256/10000)
[Test]  Epoch: 13	Loss: 0.047936	Acc: 33.0% (3303/10000)
[Test]  Epoch: 14	Loss: 0.047905	Acc: 33.2% (3325/10000)
[Test]  Epoch: 15	Loss: 0.048030	Acc: 32.7% (3269/10000)
[Test]  Epoch: 16	Loss: 0.047663	Acc: 33.4% (3342/10000)
[Test]  Epoch: 17	Loss: 0.047415	Acc: 33.5% (3355/10000)
[Test]  Epoch: 18	Loss: 0.047942	Acc: 33.2% (3318/10000)
[Test]  Epoch: 19	Loss: 0.047959	Acc: 33.2% (3316/10000)
[Test]  Epoch: 20	Loss: 0.047806	Acc: 33.6% (3364/10000)
[Test]  Epoch: 21	Loss: 0.047595	Acc: 33.6% (3362/10000)
[Test]  Epoch: 22	Loss: 0.048331	Acc: 32.9% (3290/10000)
[Test]  Epoch: 23	Loss: 0.047590	Acc: 33.8% (3377/10000)
[Test]  Epoch: 24	Loss: 0.048264	Acc: 33.1% (3306/10000)
[Test]  Epoch: 25	Loss: 0.047506	Acc: 33.6% (3359/10000)
[Test]  Epoch: 26	Loss: 0.047684	Acc: 33.4% (3342/10000)
[Test]  Epoch: 27	Loss: 0.047712	Acc: 33.6% (3360/10000)
[Test]  Epoch: 28	Loss: 0.047544	Acc: 33.7% (3372/10000)
[Test]  Epoch: 29	Loss: 0.047723	Acc: 33.2% (3324/10000)
[Test]  Epoch: 30	Loss: 0.047608	Acc: 33.9% (3392/10000)
[Test]  Epoch: 31	Loss: 0.047418	Acc: 34.4% (3438/10000)
[Test]  Epoch: 32	Loss: 0.047454	Acc: 33.9% (3386/10000)
[Test]  Epoch: 33	Loss: 0.047534	Acc: 33.9% (3386/10000)
[Test]  Epoch: 34	Loss: 0.047423	Acc: 34.4% (3439/10000)
[Test]  Epoch: 35	Loss: 0.047657	Acc: 34.0% (3404/10000)
[Test]  Epoch: 36	Loss: 0.047226	Acc: 34.6% (3461/10000)
[Test]  Epoch: 37	Loss: 0.047325	Acc: 34.6% (3462/10000)
[Test]  Epoch: 38	Loss: 0.047394	Acc: 34.0% (3401/10000)
[Test]  Epoch: 39	Loss: 0.047668	Acc: 34.0% (3403/10000)
[Test]  Epoch: 40	Loss: 0.047360	Acc: 34.6% (3458/10000)
[Test]  Epoch: 41	Loss: 0.047192	Acc: 34.5% (3449/10000)
[Test]  Epoch: 42	Loss: 0.047329	Acc: 34.2% (3421/10000)
[Test]  Epoch: 43	Loss: 0.047188	Acc: 34.5% (3445/10000)
[Test]  Epoch: 44	Loss: 0.047342	Acc: 34.4% (3442/10000)
[Test]  Epoch: 45	Loss: 0.047539	Acc: 34.2% (3416/10000)
[Test]  Epoch: 46	Loss: 0.047611	Acc: 33.9% (3393/10000)
[Test]  Epoch: 47	Loss: 0.047426	Acc: 34.7% (3472/10000)
[Test]  Epoch: 48	Loss: 0.047309	Acc: 34.6% (3460/10000)
[Test]  Epoch: 49	Loss: 0.047212	Acc: 34.9% (3493/10000)
[Test]  Epoch: 50	Loss: 0.047223	Acc: 34.9% (3494/10000)
[Test]  Epoch: 51	Loss: 0.047438	Acc: 34.1% (3415/10000)
[Test]  Epoch: 52	Loss: 0.047243	Acc: 34.4% (3444/10000)
[Test]  Epoch: 53	Loss: 0.047296	Acc: 34.9% (3485/10000)
[Test]  Epoch: 54	Loss: 0.047349	Acc: 34.9% (3492/10000)
[Test]  Epoch: 55	Loss: 0.047246	Acc: 34.8% (3476/10000)
[Test]  Epoch: 56	Loss: 0.047454	Acc: 34.6% (3462/10000)
[Test]  Epoch: 57	Loss: 0.047412	Acc: 34.4% (3443/10000)
[Test]  Epoch: 58	Loss: 0.047035	Acc: 35.5% (3548/10000)
[Test]  Epoch: 59	Loss: 0.047356	Acc: 35.0% (3496/10000)
[Test]  Epoch: 60	Loss: 0.047364	Acc: 34.9% (3490/10000)
[Test]  Epoch: 61	Loss: 0.047446	Acc: 34.8% (3480/10000)
[Test]  Epoch: 62	Loss: 0.047443	Acc: 34.9% (3490/10000)
[Test]  Epoch: 63	Loss: 0.047390	Acc: 34.8% (3476/10000)
[Test]  Epoch: 64	Loss: 0.047313	Acc: 34.9% (3492/10000)
[Test]  Epoch: 65	Loss: 0.047429	Acc: 34.7% (3467/10000)
[Test]  Epoch: 66	Loss: 0.047290	Acc: 35.0% (3502/10000)
[Test]  Epoch: 67	Loss: 0.047421	Acc: 34.8% (3475/10000)
[Test]  Epoch: 68	Loss: 0.047448	Acc: 34.8% (3477/10000)
[Test]  Epoch: 69	Loss: 0.047336	Acc: 35.0% (3497/10000)
[Test]  Epoch: 70	Loss: 0.047372	Acc: 34.9% (3493/10000)
[Test]  Epoch: 71	Loss: 0.047320	Acc: 35.1% (3506/10000)
[Test]  Epoch: 72	Loss: 0.047437	Acc: 34.8% (3476/10000)
[Test]  Epoch: 73	Loss: 0.047295	Acc: 35.2% (3517/10000)
[Test]  Epoch: 74	Loss: 0.047273	Acc: 35.0% (3499/10000)
[Test]  Epoch: 75	Loss: 0.047378	Acc: 34.8% (3479/10000)
[Test]  Epoch: 76	Loss: 0.047378	Acc: 34.8% (3482/10000)
[Test]  Epoch: 77	Loss: 0.047335	Acc: 34.8% (3484/10000)
[Test]  Epoch: 78	Loss: 0.047388	Acc: 34.8% (3484/10000)
[Test]  Epoch: 79	Loss: 0.047317	Acc: 35.2% (3516/10000)
[Test]  Epoch: 80	Loss: 0.047200	Acc: 35.4% (3535/10000)
[Test]  Epoch: 81	Loss: 0.047302	Acc: 35.0% (3501/10000)
[Test]  Epoch: 82	Loss: 0.047392	Acc: 34.7% (3470/10000)
[Test]  Epoch: 83	Loss: 0.047395	Acc: 34.5% (3452/10000)
[Test]  Epoch: 84	Loss: 0.047422	Acc: 34.9% (3485/10000)
[Test]  Epoch: 85	Loss: 0.047329	Acc: 34.8% (3480/10000)
[Test]  Epoch: 86	Loss: 0.047398	Acc: 34.6% (3461/10000)
[Test]  Epoch: 87	Loss: 0.047408	Acc: 34.6% (3464/10000)
[Test]  Epoch: 88	Loss: 0.047378	Acc: 34.9% (3492/10000)
[Test]  Epoch: 89	Loss: 0.047292	Acc: 35.0% (3496/10000)
[Test]  Epoch: 90	Loss: 0.047287	Acc: 35.0% (3505/10000)
[Test]  Epoch: 91	Loss: 0.047310	Acc: 35.0% (3500/10000)
[Test]  Epoch: 92	Loss: 0.047226	Acc: 35.1% (3507/10000)
[Test]  Epoch: 93	Loss: 0.047307	Acc: 35.0% (3500/10000)
[Test]  Epoch: 94	Loss: 0.047339	Acc: 34.9% (3486/10000)
[Test]  Epoch: 95	Loss: 0.047388	Acc: 34.6% (3464/10000)
[Test]  Epoch: 96	Loss: 0.047287	Acc: 34.9% (3494/10000)
[Test]  Epoch: 97	Loss: 0.047313	Acc: 35.0% (3496/10000)
[Test]  Epoch: 98	Loss: 0.047314	Acc: 34.9% (3490/10000)
[Test]  Epoch: 99	Loss: 0.047352	Acc: 34.8% (3478/10000)
[Test]  Epoch: 100	Loss: 0.047315	Acc: 34.8% (3481/10000)
===========finish==========
['2024-08-19', '02:39:06.941403', '100', 'test', '0.04731543651819229', '34.81', '35.48']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.068060	Acc: 10.7% (1069/10000)
[Test]  Epoch: 2	Loss: 0.053842	Acc: 23.5% (2354/10000)
[Test]  Epoch: 3	Loss: 0.051300	Acc: 27.8% (2779/10000)
[Test]  Epoch: 4	Loss: 0.050167	Acc: 29.2% (2918/10000)
[Test]  Epoch: 5	Loss: 0.050097	Acc: 29.6% (2964/10000)
[Test]  Epoch: 6	Loss: 0.049100	Acc: 30.5% (3054/10000)
[Test]  Epoch: 7	Loss: 0.048846	Acc: 31.8% (3175/10000)
[Test]  Epoch: 8	Loss: 0.048622	Acc: 32.0% (3200/10000)
[Test]  Epoch: 9	Loss: 0.048402	Acc: 32.5% (3249/10000)
[Test]  Epoch: 10	Loss: 0.048555	Acc: 32.3% (3233/10000)
[Test]  Epoch: 11	Loss: 0.048384	Acc: 32.5% (3247/10000)
[Test]  Epoch: 12	Loss: 0.048161	Acc: 33.2% (3319/10000)
[Test]  Epoch: 13	Loss: 0.048260	Acc: 32.8% (3283/10000)
[Test]  Epoch: 14	Loss: 0.047808	Acc: 33.3% (3333/10000)
[Test]  Epoch: 15	Loss: 0.047986	Acc: 33.6% (3360/10000)
[Test]  Epoch: 16	Loss: 0.047870	Acc: 33.6% (3356/10000)
[Test]  Epoch: 17	Loss: 0.047858	Acc: 33.6% (3363/10000)
[Test]  Epoch: 18	Loss: 0.047691	Acc: 33.5% (3354/10000)
[Test]  Epoch: 19	Loss: 0.047857	Acc: 33.9% (3390/10000)
[Test]  Epoch: 20	Loss: 0.047796	Acc: 34.1% (3414/10000)
[Test]  Epoch: 21	Loss: 0.047415	Acc: 34.4% (3436/10000)
[Test]  Epoch: 22	Loss: 0.047897	Acc: 33.8% (3380/10000)
[Test]  Epoch: 23	Loss: 0.047620	Acc: 34.4% (3435/10000)
[Test]  Epoch: 24	Loss: 0.047475	Acc: 34.4% (3440/10000)
[Test]  Epoch: 25	Loss: 0.047472	Acc: 34.7% (3471/10000)
[Test]  Epoch: 26	Loss: 0.047565	Acc: 34.6% (3456/10000)
[Test]  Epoch: 27	Loss: 0.047562	Acc: 34.3% (3434/10000)
[Test]  Epoch: 28	Loss: 0.047221	Acc: 35.1% (3510/10000)
[Test]  Epoch: 29	Loss: 0.047450	Acc: 34.5% (3448/10000)
[Test]  Epoch: 30	Loss: 0.047591	Acc: 34.4% (3442/10000)
[Test]  Epoch: 31	Loss: 0.047351	Acc: 34.7% (3467/10000)
[Test]  Epoch: 32	Loss: 0.047261	Acc: 34.8% (3481/10000)
[Test]  Epoch: 33	Loss: 0.047410	Acc: 35.0% (3499/10000)
[Test]  Epoch: 34	Loss: 0.047460	Acc: 35.1% (3514/10000)
[Test]  Epoch: 35	Loss: 0.047426	Acc: 34.7% (3473/10000)
[Test]  Epoch: 36	Loss: 0.047152	Acc: 35.4% (3538/10000)
[Test]  Epoch: 37	Loss: 0.047271	Acc: 35.5% (3555/10000)
[Test]  Epoch: 38	Loss: 0.047245	Acc: 35.2% (3520/10000)
[Test]  Epoch: 39	Loss: 0.047486	Acc: 34.9% (3492/10000)
[Test]  Epoch: 40	Loss: 0.047541	Acc: 35.1% (3513/10000)
[Test]  Epoch: 41	Loss: 0.047345	Acc: 34.8% (3482/10000)
[Test]  Epoch: 42	Loss: 0.047084	Acc: 35.5% (3550/10000)
[Test]  Epoch: 43	Loss: 0.047168	Acc: 35.5% (3546/10000)
[Test]  Epoch: 44	Loss: 0.047297	Acc: 35.3% (3528/10000)
[Test]  Epoch: 45	Loss: 0.047265	Acc: 35.7% (3572/10000)
[Test]  Epoch: 46	Loss: 0.047198	Acc: 35.1% (3512/10000)
[Test]  Epoch: 47	Loss: 0.047341	Acc: 35.8% (3578/10000)
[Test]  Epoch: 48	Loss: 0.047421	Acc: 35.4% (3535/10000)
[Test]  Epoch: 49	Loss: 0.047418	Acc: 35.3% (3531/10000)
[Test]  Epoch: 50	Loss: 0.047138	Acc: 35.4% (3537/10000)
[Test]  Epoch: 51	Loss: 0.047253	Acc: 35.3% (3531/10000)
[Test]  Epoch: 52	Loss: 0.047245	Acc: 35.5% (3553/10000)
[Test]  Epoch: 53	Loss: 0.047286	Acc: 35.6% (3557/10000)
[Test]  Epoch: 54	Loss: 0.047260	Acc: 35.3% (3532/10000)
[Test]  Epoch: 55	Loss: 0.047300	Acc: 35.3% (3532/10000)
[Test]  Epoch: 56	Loss: 0.047319	Acc: 35.4% (3540/10000)
[Test]  Epoch: 57	Loss: 0.047167	Acc: 35.4% (3536/10000)
[Test]  Epoch: 58	Loss: 0.047181	Acc: 35.8% (3583/10000)
[Test]  Epoch: 59	Loss: 0.047313	Acc: 35.6% (3559/10000)
[Test]  Epoch: 60	Loss: 0.047383	Acc: 35.5% (3546/10000)
[Test]  Epoch: 61	Loss: 0.047408	Acc: 35.2% (3521/10000)
[Test]  Epoch: 62	Loss: 0.047444	Acc: 35.4% (3535/10000)
[Test]  Epoch: 63	Loss: 0.047318	Acc: 35.6% (3556/10000)
[Test]  Epoch: 64	Loss: 0.047311	Acc: 35.5% (3547/10000)
[Test]  Epoch: 65	Loss: 0.047355	Acc: 35.5% (3547/10000)
[Test]  Epoch: 66	Loss: 0.047360	Acc: 35.5% (3546/10000)
[Test]  Epoch: 67	Loss: 0.047377	Acc: 35.4% (3543/10000)
[Test]  Epoch: 68	Loss: 0.047463	Acc: 35.4% (3539/10000)
[Test]  Epoch: 69	Loss: 0.047404	Acc: 35.5% (3547/10000)
[Test]  Epoch: 70	Loss: 0.047359	Acc: 35.6% (3556/10000)
[Test]  Epoch: 71	Loss: 0.047310	Acc: 35.8% (3577/10000)
[Test]  Epoch: 72	Loss: 0.047352	Acc: 35.5% (3547/10000)
[Test]  Epoch: 73	Loss: 0.047288	Acc: 35.6% (3565/10000)
[Test]  Epoch: 74	Loss: 0.047302	Acc: 35.6% (3559/10000)
[Test]  Epoch: 75	Loss: 0.047337	Acc: 35.6% (3561/10000)
[Test]  Epoch: 76	Loss: 0.047278	Acc: 35.7% (3566/10000)
[Test]  Epoch: 77	Loss: 0.047313	Acc: 35.6% (3563/10000)
[Test]  Epoch: 78	Loss: 0.047333	Acc: 35.5% (3548/10000)
[Test]  Epoch: 79	Loss: 0.047361	Acc: 35.5% (3552/10000)
[Test]  Epoch: 80	Loss: 0.047299	Acc: 35.8% (3582/10000)
[Test]  Epoch: 81	Loss: 0.047269	Acc: 35.6% (3561/10000)
[Test]  Epoch: 82	Loss: 0.047285	Acc: 35.6% (3560/10000)
[Test]  Epoch: 83	Loss: 0.047381	Acc: 35.3% (3533/10000)
[Test]  Epoch: 84	Loss: 0.047383	Acc: 35.7% (3571/10000)
[Test]  Epoch: 85	Loss: 0.047401	Acc: 35.4% (3536/10000)
[Test]  Epoch: 86	Loss: 0.047367	Acc: 35.5% (3545/10000)
[Test]  Epoch: 87	Loss: 0.047335	Acc: 35.6% (3557/10000)
[Test]  Epoch: 88	Loss: 0.047387	Acc: 35.5% (3548/10000)
[Test]  Epoch: 89	Loss: 0.047316	Acc: 35.5% (3555/10000)
[Test]  Epoch: 90	Loss: 0.047392	Acc: 35.6% (3556/10000)
[Test]  Epoch: 91	Loss: 0.047315	Acc: 35.6% (3561/10000)
[Test]  Epoch: 92	Loss: 0.047301	Acc: 35.7% (3572/10000)
[Test]  Epoch: 93	Loss: 0.047348	Acc: 35.5% (3555/10000)
[Test]  Epoch: 94	Loss: 0.047283	Acc: 35.7% (3573/10000)
[Test]  Epoch: 95	Loss: 0.047288	Acc: 35.6% (3563/10000)
[Test]  Epoch: 96	Loss: 0.047226	Acc: 35.6% (3561/10000)
[Test]  Epoch: 97	Loss: 0.047323	Acc: 35.7% (3569/10000)
[Test]  Epoch: 98	Loss: 0.047264	Acc: 35.9% (3585/10000)
[Test]  Epoch: 99	Loss: 0.047330	Acc: 35.6% (3562/10000)
[Test]  Epoch: 100	Loss: 0.047310	Acc: 35.5% (3552/10000)
===========finish==========
['2024-08-19', '02:42:39.379463', '100', 'test', '0.04730977512598038', '35.52', '35.85']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.077114	Acc: 6.1% (606/10000)
[Test]  Epoch: 2	Loss: 0.057846	Acc: 19.1% (1910/10000)
[Test]  Epoch: 3	Loss: 0.055263	Acc: 22.3% (2231/10000)
[Test]  Epoch: 4	Loss: 0.054381	Acc: 23.8% (2378/10000)
[Test]  Epoch: 5	Loss: 0.053709	Acc: 24.7% (2474/10000)
[Test]  Epoch: 6	Loss: 0.053220	Acc: 25.2% (2516/10000)
[Test]  Epoch: 7	Loss: 0.052579	Acc: 26.3% (2632/10000)
[Test]  Epoch: 8	Loss: 0.052473	Acc: 26.3% (2634/10000)
[Test]  Epoch: 9	Loss: 0.052511	Acc: 26.6% (2657/10000)
[Test]  Epoch: 10	Loss: 0.052600	Acc: 26.3% (2634/10000)
[Test]  Epoch: 11	Loss: 0.052351	Acc: 27.1% (2707/10000)
[Test]  Epoch: 12	Loss: 0.052029	Acc: 27.7% (2771/10000)
[Test]  Epoch: 13	Loss: 0.051966	Acc: 27.4% (2744/10000)
[Test]  Epoch: 14	Loss: 0.051596	Acc: 27.9% (2788/10000)
[Test]  Epoch: 15	Loss: 0.051923	Acc: 27.8% (2779/10000)
[Test]  Epoch: 16	Loss: 0.051651	Acc: 28.2% (2817/10000)
[Test]  Epoch: 17	Loss: 0.051611	Acc: 28.0% (2801/10000)
[Test]  Epoch: 18	Loss: 0.051554	Acc: 28.4% (2836/10000)
[Test]  Epoch: 19	Loss: 0.052105	Acc: 27.6% (2757/10000)
[Test]  Epoch: 20	Loss: 0.051783	Acc: 28.4% (2841/10000)
[Test]  Epoch: 21	Loss: 0.051353	Acc: 28.9% (2885/10000)
[Test]  Epoch: 22	Loss: 0.051783	Acc: 28.4% (2835/10000)
[Test]  Epoch: 23	Loss: 0.051366	Acc: 28.8% (2882/10000)
[Test]  Epoch: 24	Loss: 0.051393	Acc: 28.7% (2869/10000)
[Test]  Epoch: 25	Loss: 0.051253	Acc: 29.1% (2914/10000)
[Test]  Epoch: 26	Loss: 0.051521	Acc: 28.8% (2875/10000)
[Test]  Epoch: 27	Loss: 0.051279	Acc: 28.7% (2867/10000)
[Test]  Epoch: 28	Loss: 0.051174	Acc: 29.2% (2924/10000)
[Test]  Epoch: 29	Loss: 0.051836	Acc: 28.3% (2832/10000)
[Test]  Epoch: 30	Loss: 0.051382	Acc: 28.7% (2867/10000)
[Test]  Epoch: 31	Loss: 0.051302	Acc: 29.2% (2925/10000)
[Test]  Epoch: 32	Loss: 0.051085	Acc: 29.7% (2968/10000)
[Test]  Epoch: 33	Loss: 0.051316	Acc: 29.2% (2918/10000)
[Test]  Epoch: 34	Loss: 0.051283	Acc: 29.3% (2930/10000)
[Test]  Epoch: 35	Loss: 0.051331	Acc: 29.2% (2917/10000)
[Test]  Epoch: 36	Loss: 0.051177	Acc: 29.6% (2957/10000)
[Test]  Epoch: 37	Loss: 0.051204	Acc: 29.5% (2949/10000)
[Test]  Epoch: 38	Loss: 0.051205	Acc: 29.6% (2956/10000)
[Test]  Epoch: 39	Loss: 0.051409	Acc: 29.0% (2900/10000)
[Test]  Epoch: 40	Loss: 0.051530	Acc: 29.2% (2924/10000)
[Test]  Epoch: 41	Loss: 0.051017	Acc: 29.7% (2968/10000)
[Test]  Epoch: 42	Loss: 0.051262	Acc: 29.4% (2945/10000)
[Test]  Epoch: 43	Loss: 0.051026	Acc: 29.6% (2962/10000)
[Test]  Epoch: 44	Loss: 0.051053	Acc: 29.8% (2982/10000)
[Test]  Epoch: 45	Loss: 0.051327	Acc: 29.4% (2944/10000)
[Test]  Epoch: 46	Loss: 0.051086	Acc: 29.5% (2952/10000)
[Test]  Epoch: 47	Loss: 0.051187	Acc: 29.7% (2974/10000)
[Test]  Epoch: 48	Loss: 0.051342	Acc: 29.9% (2986/10000)
[Test]  Epoch: 49	Loss: 0.051350	Acc: 29.7% (2966/10000)
[Test]  Epoch: 50	Loss: 0.051219	Acc: 29.8% (2977/10000)
[Test]  Epoch: 51	Loss: 0.051167	Acc: 29.9% (2985/10000)
[Test]  Epoch: 52	Loss: 0.051201	Acc: 29.9% (2987/10000)
[Test]  Epoch: 53	Loss: 0.051090	Acc: 29.7% (2968/10000)
[Test]  Epoch: 54	Loss: 0.051226	Acc: 29.9% (2995/10000)
[Test]  Epoch: 55	Loss: 0.051268	Acc: 29.6% (2965/10000)
[Test]  Epoch: 56	Loss: 0.051197	Acc: 30.2% (3020/10000)
[Test]  Epoch: 57	Loss: 0.051020	Acc: 30.1% (3006/10000)
[Test]  Epoch: 58	Loss: 0.050973	Acc: 30.1% (3012/10000)
[Test]  Epoch: 59	Loss: 0.051246	Acc: 29.7% (2971/10000)
[Test]  Epoch: 60	Loss: 0.051226	Acc: 29.9% (2991/10000)
[Test]  Epoch: 61	Loss: 0.051243	Acc: 29.9% (2990/10000)
[Test]  Epoch: 62	Loss: 0.051258	Acc: 30.0% (3001/10000)
[Test]  Epoch: 63	Loss: 0.051155	Acc: 30.0% (3004/10000)
[Test]  Epoch: 64	Loss: 0.051122	Acc: 30.0% (3001/10000)
[Test]  Epoch: 65	Loss: 0.051203	Acc: 30.1% (3008/10000)
[Test]  Epoch: 66	Loss: 0.051217	Acc: 30.1% (3011/10000)
[Test]  Epoch: 67	Loss: 0.051276	Acc: 30.0% (3001/10000)
[Test]  Epoch: 68	Loss: 0.051329	Acc: 29.9% (2985/10000)
[Test]  Epoch: 69	Loss: 0.051276	Acc: 29.9% (2991/10000)
[Test]  Epoch: 70	Loss: 0.051207	Acc: 30.2% (3016/10000)
[Test]  Epoch: 71	Loss: 0.051194	Acc: 30.0% (3000/10000)
[Test]  Epoch: 72	Loss: 0.051220	Acc: 30.0% (3002/10000)
[Test]  Epoch: 73	Loss: 0.051165	Acc: 30.2% (3023/10000)
[Test]  Epoch: 74	Loss: 0.051146	Acc: 30.2% (3019/10000)
[Test]  Epoch: 75	Loss: 0.051199	Acc: 30.0% (2996/10000)
[Test]  Epoch: 76	Loss: 0.051174	Acc: 30.1% (3007/10000)
[Test]  Epoch: 77	Loss: 0.051169	Acc: 30.1% (3014/10000)
[Test]  Epoch: 78	Loss: 0.051212	Acc: 30.0% (3000/10000)
[Test]  Epoch: 79	Loss: 0.051277	Acc: 30.2% (3016/10000)
[Test]  Epoch: 80	Loss: 0.051200	Acc: 30.2% (3019/10000)
[Test]  Epoch: 81	Loss: 0.051148	Acc: 29.9% (2989/10000)
[Test]  Epoch: 82	Loss: 0.051232	Acc: 30.0% (2998/10000)
[Test]  Epoch: 83	Loss: 0.051281	Acc: 29.9% (2994/10000)
[Test]  Epoch: 84	Loss: 0.051297	Acc: 30.2% (3016/10000)
[Test]  Epoch: 85	Loss: 0.051300	Acc: 30.1% (3012/10000)
[Test]  Epoch: 86	Loss: 0.051279	Acc: 30.0% (2997/10000)
[Test]  Epoch: 87	Loss: 0.051212	Acc: 29.9% (2987/10000)
[Test]  Epoch: 88	Loss: 0.051289	Acc: 29.9% (2994/10000)
[Test]  Epoch: 89	Loss: 0.051217	Acc: 29.9% (2995/10000)
[Test]  Epoch: 90	Loss: 0.051251	Acc: 30.0% (3004/10000)
[Test]  Epoch: 91	Loss: 0.051219	Acc: 30.0% (3003/10000)
[Test]  Epoch: 92	Loss: 0.051178	Acc: 30.1% (3010/10000)
[Test]  Epoch: 93	Loss: 0.051225	Acc: 30.1% (3007/10000)
[Test]  Epoch: 94	Loss: 0.051160	Acc: 30.1% (3012/10000)
[Test]  Epoch: 95	Loss: 0.051188	Acc: 30.0% (3002/10000)
[Test]  Epoch: 96	Loss: 0.051114	Acc: 30.2% (3018/10000)
[Test]  Epoch: 97	Loss: 0.051213	Acc: 30.1% (3011/10000)
[Test]  Epoch: 98	Loss: 0.051179	Acc: 29.9% (2992/10000)
[Test]  Epoch: 99	Loss: 0.051217	Acc: 30.1% (3013/10000)
[Test]  Epoch: 100	Loss: 0.051238	Acc: 30.0% (3003/10000)
===========finish==========
['2024-08-19', '02:46:21.293111', '100', 'test', '0.0512377410531044', '30.03', '30.23']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.080430	Acc: 6.4% (637/10000)
[Test]  Epoch: 2	Loss: 0.057511	Acc: 19.6% (1955/10000)
[Test]  Epoch: 3	Loss: 0.056260	Acc: 21.4% (2142/10000)
[Test]  Epoch: 4	Loss: 0.055512	Acc: 22.6% (2258/10000)
[Test]  Epoch: 5	Loss: 0.055257	Acc: 23.1% (2313/10000)
[Test]  Epoch: 6	Loss: 0.054854	Acc: 23.6% (2360/10000)
[Test]  Epoch: 7	Loss: 0.054382	Acc: 24.1% (2414/10000)
[Test]  Epoch: 8	Loss: 0.054344	Acc: 24.3% (2427/10000)
[Test]  Epoch: 9	Loss: 0.054375	Acc: 24.4% (2444/10000)
[Test]  Epoch: 10	Loss: 0.054373	Acc: 24.3% (2426/10000)
[Test]  Epoch: 11	Loss: 0.054340	Acc: 24.1% (2407/10000)
[Test]  Epoch: 12	Loss: 0.054206	Acc: 24.4% (2438/10000)
[Test]  Epoch: 13	Loss: 0.054308	Acc: 24.5% (2452/10000)
[Test]  Epoch: 14	Loss: 0.053836	Acc: 24.9% (2488/10000)
[Test]  Epoch: 15	Loss: 0.053977	Acc: 24.8% (2478/10000)
[Test]  Epoch: 16	Loss: 0.053965	Acc: 24.7% (2473/10000)
[Test]  Epoch: 17	Loss: 0.053731	Acc: 25.1% (2515/10000)
[Test]  Epoch: 18	Loss: 0.053558	Acc: 25.4% (2540/10000)
[Test]  Epoch: 19	Loss: 0.054045	Acc: 24.8% (2479/10000)
[Test]  Epoch: 20	Loss: 0.053960	Acc: 25.2% (2523/10000)
[Test]  Epoch: 21	Loss: 0.053598	Acc: 25.2% (2522/10000)
[Test]  Epoch: 22	Loss: 0.054050	Acc: 25.0% (2502/10000)
[Test]  Epoch: 23	Loss: 0.053701	Acc: 25.6% (2563/10000)
[Test]  Epoch: 24	Loss: 0.053894	Acc: 25.2% (2519/10000)
[Test]  Epoch: 25	Loss: 0.053663	Acc: 25.8% (2578/10000)
[Test]  Epoch: 26	Loss: 0.053816	Acc: 25.4% (2538/10000)
[Test]  Epoch: 27	Loss: 0.053638	Acc: 25.6% (2556/10000)
[Test]  Epoch: 28	Loss: 0.053695	Acc: 25.8% (2575/10000)
[Test]  Epoch: 29	Loss: 0.053877	Acc: 25.2% (2522/10000)
[Test]  Epoch: 30	Loss: 0.053752	Acc: 25.5% (2553/10000)
[Test]  Epoch: 31	Loss: 0.053762	Acc: 25.8% (2581/10000)
[Test]  Epoch: 32	Loss: 0.053567	Acc: 25.9% (2589/10000)
[Test]  Epoch: 33	Loss: 0.053613	Acc: 25.7% (2570/10000)
[Test]  Epoch: 34	Loss: 0.053688	Acc: 25.3% (2526/10000)
[Test]  Epoch: 35	Loss: 0.053585	Acc: 25.9% (2594/10000)
[Test]  Epoch: 36	Loss: 0.053557	Acc: 25.9% (2588/10000)
[Test]  Epoch: 37	Loss: 0.053697	Acc: 26.0% (2600/10000)
[Test]  Epoch: 38	Loss: 0.053550	Acc: 25.9% (2589/10000)
[Test]  Epoch: 39	Loss: 0.053866	Acc: 25.7% (2567/10000)
[Test]  Epoch: 40	Loss: 0.053947	Acc: 25.6% (2563/10000)
[Test]  Epoch: 41	Loss: 0.053682	Acc: 25.9% (2591/10000)
[Test]  Epoch: 42	Loss: 0.053639	Acc: 26.1% (2613/10000)
[Test]  Epoch: 43	Loss: 0.053538	Acc: 26.1% (2605/10000)
[Test]  Epoch: 44	Loss: 0.053626	Acc: 26.0% (2600/10000)
[Test]  Epoch: 45	Loss: 0.053712	Acc: 25.8% (2582/10000)
[Test]  Epoch: 46	Loss: 0.053687	Acc: 26.0% (2598/10000)
[Test]  Epoch: 47	Loss: 0.053712	Acc: 26.3% (2627/10000)
[Test]  Epoch: 48	Loss: 0.053843	Acc: 26.0% (2604/10000)
[Test]  Epoch: 49	Loss: 0.053654	Acc: 26.4% (2636/10000)
[Test]  Epoch: 50	Loss: 0.053582	Acc: 26.3% (2634/10000)
[Test]  Epoch: 51	Loss: 0.053650	Acc: 26.2% (2624/10000)
[Test]  Epoch: 52	Loss: 0.053721	Acc: 26.0% (2599/10000)
[Test]  Epoch: 53	Loss: 0.053630	Acc: 26.1% (2609/10000)
[Test]  Epoch: 54	Loss: 0.053783	Acc: 26.0% (2603/10000)
[Test]  Epoch: 55	Loss: 0.053568	Acc: 26.2% (2623/10000)
[Test]  Epoch: 56	Loss: 0.053707	Acc: 26.4% (2639/10000)
[Test]  Epoch: 57	Loss: 0.053554	Acc: 26.3% (2631/10000)
[Test]  Epoch: 58	Loss: 0.053624	Acc: 26.6% (2661/10000)
[Test]  Epoch: 59	Loss: 0.053708	Acc: 26.4% (2642/10000)
[Test]  Epoch: 60	Loss: 0.053651	Acc: 25.9% (2594/10000)
[Test]  Epoch: 61	Loss: 0.053758	Acc: 25.9% (2594/10000)
[Test]  Epoch: 62	Loss: 0.053793	Acc: 26.0% (2599/10000)
[Test]  Epoch: 63	Loss: 0.053713	Acc: 26.0% (2597/10000)
[Test]  Epoch: 64	Loss: 0.053684	Acc: 26.1% (2612/10000)
[Test]  Epoch: 65	Loss: 0.053735	Acc: 26.2% (2616/10000)
[Test]  Epoch: 66	Loss: 0.053740	Acc: 26.2% (2621/10000)
[Test]  Epoch: 67	Loss: 0.053785	Acc: 26.3% (2626/10000)
[Test]  Epoch: 68	Loss: 0.053856	Acc: 26.0% (2599/10000)
[Test]  Epoch: 69	Loss: 0.053777	Acc: 26.2% (2623/10000)
[Test]  Epoch: 70	Loss: 0.053748	Acc: 26.1% (2608/10000)
[Test]  Epoch: 71	Loss: 0.053769	Acc: 26.1% (2609/10000)
[Test]  Epoch: 72	Loss: 0.053781	Acc: 26.1% (2613/10000)
[Test]  Epoch: 73	Loss: 0.053742	Acc: 26.1% (2608/10000)
[Test]  Epoch: 74	Loss: 0.053708	Acc: 26.2% (2625/10000)
[Test]  Epoch: 75	Loss: 0.053748	Acc: 26.2% (2625/10000)
[Test]  Epoch: 76	Loss: 0.053721	Acc: 26.2% (2619/10000)
[Test]  Epoch: 77	Loss: 0.053712	Acc: 26.2% (2625/10000)
[Test]  Epoch: 78	Loss: 0.053778	Acc: 26.1% (2609/10000)
[Test]  Epoch: 79	Loss: 0.053796	Acc: 26.1% (2608/10000)
[Test]  Epoch: 80	Loss: 0.053777	Acc: 26.2% (2617/10000)
[Test]  Epoch: 81	Loss: 0.053697	Acc: 26.2% (2618/10000)
[Test]  Epoch: 82	Loss: 0.053736	Acc: 26.3% (2629/10000)
[Test]  Epoch: 83	Loss: 0.053849	Acc: 26.3% (2631/10000)
[Test]  Epoch: 84	Loss: 0.053839	Acc: 26.1% (2611/10000)
[Test]  Epoch: 85	Loss: 0.053795	Acc: 26.2% (2623/10000)
[Test]  Epoch: 86	Loss: 0.053772	Acc: 26.2% (2621/10000)
[Test]  Epoch: 87	Loss: 0.053745	Acc: 26.1% (2612/10000)
[Test]  Epoch: 88	Loss: 0.053815	Acc: 26.0% (2599/10000)
[Test]  Epoch: 89	Loss: 0.053784	Acc: 26.0% (2600/10000)
[Test]  Epoch: 90	Loss: 0.053819	Acc: 26.1% (2612/10000)
[Test]  Epoch: 91	Loss: 0.053711	Acc: 26.4% (2640/10000)
[Test]  Epoch: 92	Loss: 0.053688	Acc: 26.2% (2620/10000)
[Test]  Epoch: 93	Loss: 0.053773	Acc: 26.2% (2618/10000)
[Test]  Epoch: 94	Loss: 0.053704	Acc: 26.3% (2634/10000)
[Test]  Epoch: 95	Loss: 0.053753	Acc: 26.2% (2623/10000)
[Test]  Epoch: 96	Loss: 0.053700	Acc: 26.4% (2636/10000)
[Test]  Epoch: 97	Loss: 0.053750	Acc: 26.3% (2632/10000)
[Test]  Epoch: 98	Loss: 0.053703	Acc: 26.4% (2635/10000)
[Test]  Epoch: 99	Loss: 0.053747	Acc: 26.2% (2621/10000)
[Test]  Epoch: 100	Loss: 0.053729	Acc: 26.4% (2638/10000)
===========finish==========
['2024-08-19', '02:49:54.402241', '100', 'test', '0.05372883687019348', '26.38', '26.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.conv1.weight', 'last_linear.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.conv1.weight', 'last_linear.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.080903	Acc: 2.9% (291/10000)
[Test]  Epoch: 2	Loss: 0.074965	Acc: 6.3% (632/10000)
[Test]  Epoch: 3	Loss: 0.072309	Acc: 7.7% (767/10000)
[Test]  Epoch: 4	Loss: 0.070799	Acc: 8.6% (863/10000)
[Test]  Epoch: 5	Loss: 0.069836	Acc: 8.7% (865/10000)
[Test]  Epoch: 6	Loss: 0.069015	Acc: 9.1% (905/10000)
[Test]  Epoch: 7	Loss: 0.067975	Acc: 9.6% (960/10000)
[Test]  Epoch: 8	Loss: 0.067689	Acc: 10.0% (1003/10000)
[Test]  Epoch: 9	Loss: 0.067278	Acc: 10.2% (1024/10000)
[Test]  Epoch: 10	Loss: 0.066960	Acc: 10.2% (1019/10000)
[Test]  Epoch: 11	Loss: 0.066598	Acc: 10.4% (1039/10000)
[Test]  Epoch: 12	Loss: 0.066181	Acc: 10.6% (1059/10000)
[Test]  Epoch: 13	Loss: 0.066319	Acc: 10.6% (1056/10000)
[Test]  Epoch: 14	Loss: 0.065867	Acc: 10.8% (1082/10000)
[Test]  Epoch: 15	Loss: 0.065772	Acc: 10.9% (1088/10000)
[Test]  Epoch: 16	Loss: 0.065439	Acc: 10.8% (1076/10000)
[Test]  Epoch: 17	Loss: 0.065446	Acc: 11.0% (1102/10000)
[Test]  Epoch: 18	Loss: 0.065116	Acc: 11.3% (1128/10000)
[Test]  Epoch: 19	Loss: 0.065418	Acc: 11.2% (1116/10000)
[Test]  Epoch: 20	Loss: 0.064959	Acc: 11.5% (1153/10000)
[Test]  Epoch: 21	Loss: 0.064805	Acc: 11.6% (1155/10000)
[Test]  Epoch: 22	Loss: 0.065011	Acc: 11.5% (1151/10000)
[Test]  Epoch: 23	Loss: 0.064760	Acc: 11.3% (1132/10000)
[Test]  Epoch: 24	Loss: 0.064672	Acc: 11.7% (1173/10000)
[Test]  Epoch: 25	Loss: 0.064508	Acc: 11.9% (1191/10000)
[Test]  Epoch: 26	Loss: 0.064892	Acc: 11.8% (1177/10000)
[Test]  Epoch: 27	Loss: 0.064412	Acc: 11.8% (1180/10000)
[Test]  Epoch: 28	Loss: 0.064887	Acc: 11.3% (1133/10000)
[Test]  Epoch: 29	Loss: 0.064594	Acc: 11.6% (1160/10000)
[Test]  Epoch: 30	Loss: 0.064384	Acc: 12.1% (1211/10000)
[Test]  Epoch: 31	Loss: 0.064291	Acc: 12.0% (1199/10000)
[Test]  Epoch: 32	Loss: 0.064332	Acc: 11.8% (1185/10000)
[Test]  Epoch: 33	Loss: 0.064460	Acc: 11.8% (1179/10000)
[Test]  Epoch: 34	Loss: 0.064357	Acc: 11.8% (1184/10000)
[Test]  Epoch: 35	Loss: 0.064513	Acc: 11.9% (1189/10000)
[Test]  Epoch: 36	Loss: 0.063747	Acc: 12.4% (1245/10000)
[Test]  Epoch: 37	Loss: 0.064265	Acc: 12.1% (1210/10000)
[Test]  Epoch: 38	Loss: 0.064102	Acc: 12.0% (1196/10000)
[Test]  Epoch: 39	Loss: 0.063869	Acc: 12.6% (1261/10000)
[Test]  Epoch: 40	Loss: 0.063991	Acc: 12.5% (1246/10000)
[Test]  Epoch: 41	Loss: 0.064114	Acc: 12.2% (1222/10000)
[Test]  Epoch: 42	Loss: 0.063966	Acc: 12.7% (1272/10000)
[Test]  Epoch: 43	Loss: 0.063904	Acc: 12.3% (1234/10000)
[Test]  Epoch: 44	Loss: 0.063921	Acc: 12.6% (1262/10000)
[Test]  Epoch: 45	Loss: 0.063892	Acc: 12.8% (1276/10000)
[Test]  Epoch: 46	Loss: 0.063981	Acc: 12.3% (1232/10000)
[Test]  Epoch: 47	Loss: 0.063906	Acc: 12.7% (1273/10000)
[Test]  Epoch: 48	Loss: 0.063696	Acc: 12.7% (1272/10000)
[Test]  Epoch: 49	Loss: 0.063834	Acc: 12.6% (1256/10000)
[Test]  Epoch: 50	Loss: 0.064241	Acc: 12.4% (1237/10000)
[Test]  Epoch: 51	Loss: 0.063641	Acc: 12.6% (1263/10000)
[Test]  Epoch: 52	Loss: 0.063568	Acc: 12.8% (1285/10000)
[Test]  Epoch: 53	Loss: 0.063662	Acc: 12.8% (1284/10000)
[Test]  Epoch: 54	Loss: 0.063678	Acc: 12.6% (1263/10000)
[Test]  Epoch: 55	Loss: 0.063712	Acc: 12.6% (1258/10000)
[Test]  Epoch: 56	Loss: 0.063791	Acc: 12.6% (1261/10000)
[Test]  Epoch: 57	Loss: 0.063690	Acc: 12.8% (1285/10000)
[Test]  Epoch: 58	Loss: 0.063616	Acc: 12.9% (1290/10000)
[Test]  Epoch: 59	Loss: 0.063596	Acc: 12.9% (1286/10000)
[Test]  Epoch: 60	Loss: 0.063593	Acc: 13.0% (1299/10000)
[Test]  Epoch: 61	Loss: 0.063595	Acc: 12.8% (1279/10000)
[Test]  Epoch: 62	Loss: 0.063582	Acc: 13.0% (1298/10000)
[Test]  Epoch: 63	Loss: 0.063446	Acc: 13.0% (1297/10000)
[Test]  Epoch: 64	Loss: 0.063501	Acc: 12.9% (1287/10000)
[Test]  Epoch: 65	Loss: 0.063562	Acc: 12.7% (1269/10000)
[Test]  Epoch: 66	Loss: 0.063566	Acc: 12.8% (1281/10000)
[Test]  Epoch: 67	Loss: 0.063559	Acc: 12.9% (1291/10000)
[Test]  Epoch: 68	Loss: 0.063618	Acc: 12.8% (1280/10000)
[Test]  Epoch: 69	Loss: 0.063554	Acc: 12.9% (1292/10000)
[Test]  Epoch: 70	Loss: 0.063544	Acc: 13.0% (1302/10000)
[Test]  Epoch: 71	Loss: 0.063518	Acc: 12.8% (1281/10000)
[Test]  Epoch: 72	Loss: 0.063569	Acc: 12.9% (1295/10000)
[Test]  Epoch: 73	Loss: 0.063490	Acc: 12.9% (1289/10000)
[Test]  Epoch: 74	Loss: 0.063482	Acc: 12.9% (1289/10000)
[Test]  Epoch: 75	Loss: 0.063574	Acc: 12.7% (1271/10000)
[Test]  Epoch: 76	Loss: 0.063505	Acc: 12.8% (1284/10000)
[Test]  Epoch: 77	Loss: 0.063489	Acc: 12.9% (1294/10000)
[Test]  Epoch: 78	Loss: 0.063547	Acc: 12.8% (1279/10000)
[Test]  Epoch: 79	Loss: 0.063537	Acc: 13.0% (1301/10000)
[Test]  Epoch: 80	Loss: 0.063460	Acc: 13.0% (1300/10000)
[Test]  Epoch: 81	Loss: 0.063470	Acc: 12.9% (1294/10000)
[Test]  Epoch: 82	Loss: 0.063517	Acc: 13.1% (1308/10000)
[Test]  Epoch: 83	Loss: 0.063555	Acc: 13.0% (1300/10000)
[Test]  Epoch: 84	Loss: 0.063537	Acc: 13.0% (1297/10000)
[Test]  Epoch: 85	Loss: 0.063531	Acc: 13.0% (1296/10000)
[Test]  Epoch: 86	Loss: 0.063524	Acc: 12.9% (1288/10000)
[Test]  Epoch: 87	Loss: 0.063504	Acc: 13.0% (1298/10000)
[Test]  Epoch: 88	Loss: 0.063609	Acc: 12.8% (1285/10000)
[Test]  Epoch: 89	Loss: 0.063516	Acc: 12.9% (1288/10000)
[Test]  Epoch: 90	Loss: 0.063542	Acc: 12.8% (1281/10000)
[Test]  Epoch: 91	Loss: 0.063521	Acc: 13.0% (1300/10000)
[Test]  Epoch: 92	Loss: 0.063477	Acc: 12.9% (1287/10000)
[Test]  Epoch: 93	Loss: 0.063513	Acc: 12.9% (1290/10000)
[Test]  Epoch: 94	Loss: 0.063466	Acc: 13.0% (1304/10000)
[Test]  Epoch: 95	Loss: 0.063519	Acc: 13.0% (1302/10000)
[Test]  Epoch: 96	Loss: 0.063479	Acc: 13.0% (1303/10000)
[Test]  Epoch: 97	Loss: 0.063508	Acc: 13.0% (1301/10000)
[Test]  Epoch: 98	Loss: 0.063503	Acc: 12.9% (1286/10000)
[Test]  Epoch: 99	Loss: 0.063480	Acc: 13.0% (1300/10000)
[Test]  Epoch: 100	Loss: 0.063524	Acc: 13.0% (1298/10000)
===========finish==========
['2024-08-19', '02:53:32.481528', '100', 'test', '0.0635244487285614', '12.98', '13.08']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.conv1.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer2.0.conv2.weight', 'layer3.0.conv1.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.conv1.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer2.0.conv2.weight', 'layer3.0.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.081651	Acc: 2.4% (244/10000)
[Test]  Epoch: 2	Loss: 0.074971	Acc: 6.1% (614/10000)
[Test]  Epoch: 3	Loss: 0.072190	Acc: 7.9% (792/10000)
[Test]  Epoch: 4	Loss: 0.070412	Acc: 8.9% (889/10000)
[Test]  Epoch: 5	Loss: 0.069190	Acc: 9.2% (917/10000)
[Test]  Epoch: 6	Loss: 0.068400	Acc: 9.6% (958/10000)
[Test]  Epoch: 7	Loss: 0.067615	Acc: 10.3% (1029/10000)
[Test]  Epoch: 8	Loss: 0.066824	Acc: 10.8% (1076/10000)
[Test]  Epoch: 9	Loss: 0.066634	Acc: 11.0% (1099/10000)
[Test]  Epoch: 10	Loss: 0.065941	Acc: 11.1% (1105/10000)
[Test]  Epoch: 11	Loss: 0.065938	Acc: 11.3% (1133/10000)
[Test]  Epoch: 12	Loss: 0.065489	Acc: 11.7% (1169/10000)
[Test]  Epoch: 13	Loss: 0.065279	Acc: 11.9% (1188/10000)
[Test]  Epoch: 14	Loss: 0.065080	Acc: 11.6% (1163/10000)
[Test]  Epoch: 15	Loss: 0.064994	Acc: 11.7% (1173/10000)
[Test]  Epoch: 16	Loss: 0.064679	Acc: 12.1% (1211/10000)
[Test]  Epoch: 17	Loss: 0.064641	Acc: 12.2% (1224/10000)
[Test]  Epoch: 18	Loss: 0.064243	Acc: 12.3% (1231/10000)
[Test]  Epoch: 19	Loss: 0.064249	Acc: 12.5% (1250/10000)
[Test]  Epoch: 20	Loss: 0.064097	Acc: 12.9% (1291/10000)
[Test]  Epoch: 21	Loss: 0.063912	Acc: 12.6% (1259/10000)
[Test]  Epoch: 22	Loss: 0.064165	Acc: 12.6% (1261/10000)
[Test]  Epoch: 23	Loss: 0.063850	Acc: 12.8% (1277/10000)
[Test]  Epoch: 24	Loss: 0.063891	Acc: 12.8% (1276/10000)
[Test]  Epoch: 25	Loss: 0.063633	Acc: 13.1% (1308/10000)
[Test]  Epoch: 26	Loss: 0.063771	Acc: 12.9% (1289/10000)
[Test]  Epoch: 27	Loss: 0.063483	Acc: 13.2% (1322/10000)
[Test]  Epoch: 28	Loss: 0.063362	Acc: 12.9% (1286/10000)
[Test]  Epoch: 29	Loss: 0.063499	Acc: 12.9% (1293/10000)
[Test]  Epoch: 30	Loss: 0.063461	Acc: 12.9% (1295/10000)
[Test]  Epoch: 31	Loss: 0.063571	Acc: 12.9% (1293/10000)
[Test]  Epoch: 32	Loss: 0.063142	Acc: 13.3% (1328/10000)
[Test]  Epoch: 33	Loss: 0.063273	Acc: 13.1% (1311/10000)
[Test]  Epoch: 34	Loss: 0.063074	Acc: 13.3% (1326/10000)
[Test]  Epoch: 35	Loss: 0.063131	Acc: 13.1% (1314/10000)
[Test]  Epoch: 36	Loss: 0.062931	Acc: 13.8% (1379/10000)
[Test]  Epoch: 37	Loss: 0.063223	Acc: 13.3% (1333/10000)
[Test]  Epoch: 38	Loss: 0.062897	Acc: 13.9% (1390/10000)
[Test]  Epoch: 39	Loss: 0.062944	Acc: 13.3% (1335/10000)
[Test]  Epoch: 40	Loss: 0.062869	Acc: 13.5% (1348/10000)
[Test]  Epoch: 41	Loss: 0.063028	Acc: 13.6% (1356/10000)
[Test]  Epoch: 42	Loss: 0.062972	Acc: 13.5% (1349/10000)
[Test]  Epoch: 43	Loss: 0.062601	Acc: 13.8% (1383/10000)
[Test]  Epoch: 44	Loss: 0.062796	Acc: 13.7% (1369/10000)
[Test]  Epoch: 45	Loss: 0.062715	Acc: 13.9% (1393/10000)
[Test]  Epoch: 46	Loss: 0.062738	Acc: 13.9% (1391/10000)
[Test]  Epoch: 47	Loss: 0.062768	Acc: 13.9% (1392/10000)
[Test]  Epoch: 48	Loss: 0.062629	Acc: 14.2% (1415/10000)
[Test]  Epoch: 49	Loss: 0.062638	Acc: 13.9% (1389/10000)
[Test]  Epoch: 50	Loss: 0.062988	Acc: 13.6% (1358/10000)
[Test]  Epoch: 51	Loss: 0.062706	Acc: 13.8% (1385/10000)
[Test]  Epoch: 52	Loss: 0.062617	Acc: 14.2% (1424/10000)
[Test]  Epoch: 53	Loss: 0.062647	Acc: 13.9% (1390/10000)
[Test]  Epoch: 54	Loss: 0.062686	Acc: 14.0% (1400/10000)
[Test]  Epoch: 55	Loss: 0.062604	Acc: 13.9% (1393/10000)
[Test]  Epoch: 56	Loss: 0.062534	Acc: 14.0% (1402/10000)
[Test]  Epoch: 57	Loss: 0.062666	Acc: 14.2% (1419/10000)
[Test]  Epoch: 58	Loss: 0.062450	Acc: 14.0% (1402/10000)
[Test]  Epoch: 59	Loss: 0.062627	Acc: 13.9% (1395/10000)
[Test]  Epoch: 60	Loss: 0.062617	Acc: 14.1% (1414/10000)
[Test]  Epoch: 61	Loss: 0.062510	Acc: 14.1% (1412/10000)
[Test]  Epoch: 62	Loss: 0.062489	Acc: 14.1% (1405/10000)
[Test]  Epoch: 63	Loss: 0.062381	Acc: 14.1% (1413/10000)
[Test]  Epoch: 64	Loss: 0.062426	Acc: 14.2% (1421/10000)
[Test]  Epoch: 65	Loss: 0.062468	Acc: 14.1% (1411/10000)
[Test]  Epoch: 66	Loss: 0.062446	Acc: 14.2% (1423/10000)
[Test]  Epoch: 67	Loss: 0.062483	Acc: 14.1% (1412/10000)
[Test]  Epoch: 68	Loss: 0.062516	Acc: 14.2% (1415/10000)
[Test]  Epoch: 69	Loss: 0.062450	Acc: 14.1% (1408/10000)
[Test]  Epoch: 70	Loss: 0.062428	Acc: 14.2% (1420/10000)
[Test]  Epoch: 71	Loss: 0.062416	Acc: 14.2% (1421/10000)
[Test]  Epoch: 72	Loss: 0.062486	Acc: 14.2% (1418/10000)
[Test]  Epoch: 73	Loss: 0.062420	Acc: 14.3% (1426/10000)
[Test]  Epoch: 74	Loss: 0.062378	Acc: 14.2% (1419/10000)
[Test]  Epoch: 75	Loss: 0.062456	Acc: 14.2% (1420/10000)
[Test]  Epoch: 76	Loss: 0.062409	Acc: 14.2% (1424/10000)
[Test]  Epoch: 77	Loss: 0.062430	Acc: 14.2% (1415/10000)
[Test]  Epoch: 78	Loss: 0.062508	Acc: 14.1% (1412/10000)
[Test]  Epoch: 79	Loss: 0.062443	Acc: 14.2% (1418/10000)
[Test]  Epoch: 80	Loss: 0.062423	Acc: 14.1% (1413/10000)
[Test]  Epoch: 81	Loss: 0.062413	Acc: 14.1% (1413/10000)
[Test]  Epoch: 82	Loss: 0.062455	Acc: 14.0% (1404/10000)
[Test]  Epoch: 83	Loss: 0.062477	Acc: 14.1% (1411/10000)
[Test]  Epoch: 84	Loss: 0.062437	Acc: 14.1% (1411/10000)
[Test]  Epoch: 85	Loss: 0.062442	Acc: 14.2% (1415/10000)
[Test]  Epoch: 86	Loss: 0.062454	Acc: 14.2% (1416/10000)
[Test]  Epoch: 87	Loss: 0.062461	Acc: 14.1% (1412/10000)
[Test]  Epoch: 88	Loss: 0.062463	Acc: 14.1% (1408/10000)
[Test]  Epoch: 89	Loss: 0.062440	Acc: 14.0% (1403/10000)
[Test]  Epoch: 90	Loss: 0.062437	Acc: 14.2% (1419/10000)
[Test]  Epoch: 91	Loss: 0.062417	Acc: 14.3% (1429/10000)
[Test]  Epoch: 92	Loss: 0.062361	Acc: 14.2% (1420/10000)
[Test]  Epoch: 93	Loss: 0.062418	Acc: 14.2% (1424/10000)
[Test]  Epoch: 94	Loss: 0.062420	Acc: 14.3% (1433/10000)
[Test]  Epoch: 95	Loss: 0.062461	Acc: 14.2% (1419/10000)
[Test]  Epoch: 96	Loss: 0.062401	Acc: 14.3% (1428/10000)
[Test]  Epoch: 97	Loss: 0.062437	Acc: 14.2% (1423/10000)
[Test]  Epoch: 98	Loss: 0.062408	Acc: 14.2% (1422/10000)
[Test]  Epoch: 99	Loss: 0.062393	Acc: 14.2% (1415/10000)
[Test]  Epoch: 100	Loss: 0.062412	Acc: 14.2% (1415/10000)
===========finish==========
['2024-08-19', '02:57:06.419647', '100', 'test', '0.0624118212223053', '14.15', '14.33']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.conv1.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer2.0.conv2.weight', 'layer3.0.conv1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer3.0.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight']
['layer4.1.bn2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn2.weight', 'layer1.1.bn1.weight', 'layer4.0.bn2.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer4.0.downsample.1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'bn1.weight', 'layer2.0.downsample.1.weight', 'layer3.0.downsample.1.weight', 'layer4.1.bn1.weight', 'layer3.0.bn2.weight', 'layer3.0.bn1.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.0.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'conv1.weight', 'layer1.1.conv2.weight', 'layer3.1.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.1.conv2.weight', 'layer2.0.conv1.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer4.0.downsample.0.weight', 'layer2.0.conv2.weight', 'layer3.0.conv1.weight', 'layer4.1.conv2.weight', 'layer4.1.conv1.weight', 'layer3.0.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.087220	Acc: 0.8% (75/10000)
[Test]  Epoch: 2	Loss: 0.082001	Acc: 2.5% (250/10000)
[Test]  Epoch: 3	Loss: 0.079225	Acc: 4.1% (406/10000)
[Test]  Epoch: 4	Loss: 0.077104	Acc: 5.2% (520/10000)
[Test]  Epoch: 5	Loss: 0.075470	Acc: 6.2% (623/10000)
[Test]  Epoch: 6	Loss: 0.074180	Acc: 6.8% (681/10000)
[Test]  Epoch: 7	Loss: 0.073089	Acc: 7.2% (722/10000)
[Test]  Epoch: 8	Loss: 0.071958	Acc: 7.9% (787/10000)
[Test]  Epoch: 9	Loss: 0.071081	Acc: 8.0% (799/10000)
[Test]  Epoch: 10	Loss: 0.070200	Acc: 8.6% (857/10000)
[Test]  Epoch: 11	Loss: 0.069458	Acc: 8.9% (893/10000)
[Test]  Epoch: 12	Loss: 0.068812	Acc: 9.3% (929/10000)
[Test]  Epoch: 13	Loss: 0.068239	Acc: 9.7% (971/10000)
[Test]  Epoch: 14	Loss: 0.067760	Acc: 9.8% (984/10000)
[Test]  Epoch: 15	Loss: 0.067361	Acc: 10.2% (1023/10000)
[Test]  Epoch: 16	Loss: 0.066991	Acc: 10.5% (1046/10000)
[Test]  Epoch: 17	Loss: 0.066607	Acc: 10.7% (1065/10000)
[Test]  Epoch: 18	Loss: 0.066398	Acc: 10.8% (1075/10000)
[Test]  Epoch: 19	Loss: 0.066092	Acc: 11.0% (1099/10000)
[Test]  Epoch: 20	Loss: 0.065680	Acc: 11.2% (1116/10000)
[Test]  Epoch: 21	Loss: 0.065447	Acc: 11.4% (1136/10000)
[Test]  Epoch: 22	Loss: 0.065269	Acc: 11.2% (1119/10000)
[Test]  Epoch: 23	Loss: 0.064957	Acc: 11.5% (1148/10000)
[Test]  Epoch: 24	Loss: 0.064770	Acc: 11.5% (1150/10000)
[Test]  Epoch: 25	Loss: 0.064675	Acc: 11.7% (1172/10000)
[Test]  Epoch: 26	Loss: 0.064438	Acc: 11.9% (1192/10000)
[Test]  Epoch: 27	Loss: 0.064242	Acc: 12.2% (1220/10000)
[Test]  Epoch: 28	Loss: 0.064104	Acc: 12.1% (1207/10000)
[Test]  Epoch: 29	Loss: 0.063833	Acc: 12.4% (1239/10000)
[Test]  Epoch: 30	Loss: 0.063835	Acc: 12.3% (1227/10000)
[Test]  Epoch: 31	Loss: 0.063784	Acc: 12.4% (1238/10000)
[Test]  Epoch: 32	Loss: 0.063613	Acc: 12.5% (1252/10000)
[Test]  Epoch: 33	Loss: 0.063457	Acc: 12.5% (1247/10000)
[Test]  Epoch: 34	Loss: 0.063341	Acc: 12.7% (1272/10000)
[Test]  Epoch: 35	Loss: 0.063169	Acc: 12.7% (1271/10000)
[Test]  Epoch: 36	Loss: 0.063116	Acc: 12.8% (1283/10000)
[Test]  Epoch: 37	Loss: 0.063005	Acc: 12.9% (1291/10000)
[Test]  Epoch: 38	Loss: 0.062854	Acc: 13.0% (1302/10000)
[Test]  Epoch: 39	Loss: 0.062832	Acc: 12.9% (1287/10000)
[Test]  Epoch: 40	Loss: 0.062712	Acc: 13.1% (1310/10000)
[Test]  Epoch: 41	Loss: 0.062792	Acc: 13.1% (1308/10000)
[Test]  Epoch: 42	Loss: 0.062568	Acc: 12.9% (1295/10000)
[Test]  Epoch: 43	Loss: 0.062529	Acc: 13.2% (1315/10000)
[Test]  Epoch: 44	Loss: 0.062441	Acc: 13.3% (1327/10000)
[Test]  Epoch: 45	Loss: 0.062407	Acc: 13.2% (1321/10000)
[Test]  Epoch: 46	Loss: 0.062348	Acc: 13.1% (1312/10000)
[Test]  Epoch: 47	Loss: 0.062272	Acc: 13.3% (1333/10000)
[Test]  Epoch: 48	Loss: 0.062095	Acc: 13.7% (1368/10000)
[Test]  Epoch: 49	Loss: 0.062129	Acc: 13.7% (1373/10000)
[Test]  Epoch: 50	Loss: 0.062107	Acc: 13.6% (1358/10000)
[Test]  Epoch: 51	Loss: 0.061981	Acc: 13.5% (1352/10000)
[Test]  Epoch: 52	Loss: 0.061897	Acc: 13.7% (1373/10000)
[Test]  Epoch: 53	Loss: 0.061933	Acc: 13.8% (1376/10000)
[Test]  Epoch: 54	Loss: 0.061920	Acc: 13.9% (1390/10000)
[Test]  Epoch: 55	Loss: 0.061905	Acc: 13.7% (1371/10000)
[Test]  Epoch: 56	Loss: 0.061816	Acc: 13.7% (1372/10000)
[Test]  Epoch: 57	Loss: 0.061895	Acc: 13.7% (1371/10000)
[Test]  Epoch: 58	Loss: 0.061620	Acc: 14.1% (1408/10000)
[Test]  Epoch: 59	Loss: 0.061553	Acc: 14.1% (1412/10000)
[Test]  Epoch: 60	Loss: 0.061620	Acc: 14.1% (1406/10000)
[Test]  Epoch: 61	Loss: 0.061653	Acc: 14.0% (1401/10000)
[Test]  Epoch: 62	Loss: 0.061618	Acc: 14.1% (1406/10000)
[Test]  Epoch: 63	Loss: 0.061542	Acc: 14.1% (1408/10000)
[Test]  Epoch: 64	Loss: 0.061538	Acc: 14.2% (1424/10000)
[Test]  Epoch: 65	Loss: 0.061591	Acc: 14.0% (1404/10000)
[Test]  Epoch: 66	Loss: 0.061571	Acc: 14.0% (1401/10000)
[Test]  Epoch: 67	Loss: 0.061616	Acc: 14.1% (1408/10000)
[Test]  Epoch: 68	Loss: 0.061624	Acc: 14.0% (1403/10000)
[Test]  Epoch: 69	Loss: 0.061541	Acc: 14.1% (1414/10000)
[Test]  Epoch: 70	Loss: 0.061536	Acc: 14.2% (1421/10000)
[Test]  Epoch: 71	Loss: 0.061542	Acc: 14.1% (1408/10000)
[Test]  Epoch: 72	Loss: 0.061601	Acc: 13.9% (1395/10000)
[Test]  Epoch: 73	Loss: 0.061545	Acc: 14.1% (1407/10000)
[Test]  Epoch: 74	Loss: 0.061469	Acc: 14.2% (1416/10000)
[Test]  Epoch: 75	Loss: 0.061496	Acc: 14.2% (1422/10000)
[Test]  Epoch: 76	Loss: 0.061519	Acc: 14.1% (1413/10000)
[Test]  Epoch: 77	Loss: 0.061520	Acc: 14.2% (1416/10000)
[Test]  Epoch: 78	Loss: 0.061524	Acc: 14.0% (1398/10000)
[Test]  Epoch: 79	Loss: 0.061509	Acc: 14.0% (1402/10000)
[Test]  Epoch: 80	Loss: 0.061453	Acc: 14.1% (1409/10000)
[Test]  Epoch: 81	Loss: 0.061432	Acc: 14.1% (1408/10000)
[Test]  Epoch: 82	Loss: 0.061498	Acc: 14.2% (1424/10000)
[Test]  Epoch: 83	Loss: 0.061463	Acc: 14.1% (1413/10000)
[Test]  Epoch: 84	Loss: 0.061475	Acc: 14.1% (1405/10000)
[Test]  Epoch: 85	Loss: 0.061500	Acc: 14.1% (1409/10000)
[Test]  Epoch: 86	Loss: 0.061448	Acc: 14.1% (1409/10000)
[Test]  Epoch: 87	Loss: 0.061431	Acc: 14.1% (1414/10000)
[Test]  Epoch: 88	Loss: 0.061500	Acc: 14.2% (1424/10000)
[Test]  Epoch: 89	Loss: 0.061459	Acc: 14.1% (1412/10000)
[Test]  Epoch: 90	Loss: 0.061461	Acc: 14.0% (1403/10000)
[Test]  Epoch: 91	Loss: 0.061441	Acc: 14.2% (1422/10000)
[Test]  Epoch: 92	Loss: 0.061379	Acc: 14.2% (1423/10000)
[Test]  Epoch: 93	Loss: 0.061407	Acc: 14.1% (1412/10000)
[Test]  Epoch: 94	Loss: 0.061386	Acc: 14.0% (1402/10000)
[Test]  Epoch: 95	Loss: 0.061492	Acc: 14.1% (1411/10000)
[Test]  Epoch: 96	Loss: 0.061433	Acc: 14.1% (1414/10000)
[Test]  Epoch: 97	Loss: 0.061440	Acc: 14.1% (1407/10000)
[Test]  Epoch: 98	Loss: 0.061486	Acc: 14.1% (1411/10000)
[Test]  Epoch: 99	Loss: 0.061400	Acc: 14.2% (1416/10000)
[Test]  Epoch: 100	Loss: 0.061386	Acc: 14.2% (1420/10000)
===========finish==========
['2024-08-19', '03:01:24.554823', '100', 'test', '0.061385748791694644', '14.2', '14.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=0
get_sample_layers not_random
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039146	Acc: 45.2% (4520/10000)
[Test]  Epoch: 2	Loss: 0.038964	Acc: 45.8% (4580/10000)
[Test]  Epoch: 3	Loss: 0.039226	Acc: 45.6% (4557/10000)
[Test]  Epoch: 4	Loss: 0.039423	Acc: 45.3% (4532/10000)
[Test]  Epoch: 5	Loss: 0.039468	Acc: 45.4% (4542/10000)
[Test]  Epoch: 6	Loss: 0.039667	Acc: 45.5% (4546/10000)
[Test]  Epoch: 7	Loss: 0.039652	Acc: 45.4% (4537/10000)
[Test]  Epoch: 8	Loss: 0.039844	Acc: 44.9% (4490/10000)
[Test]  Epoch: 9	Loss: 0.039945	Acc: 45.0% (4498/10000)
[Test]  Epoch: 10	Loss: 0.040072	Acc: 45.1% (4506/10000)
[Test]  Epoch: 11	Loss: 0.040159	Acc: 45.0% (4497/10000)
[Test]  Epoch: 12	Loss: 0.040050	Acc: 45.1% (4512/10000)
[Test]  Epoch: 13	Loss: 0.040198	Acc: 45.1% (4515/10000)
[Test]  Epoch: 14	Loss: 0.040236	Acc: 45.1% (4508/10000)
[Test]  Epoch: 15	Loss: 0.040413	Acc: 45.0% (4501/10000)
[Test]  Epoch: 16	Loss: 0.040167	Acc: 45.2% (4520/10000)
[Test]  Epoch: 17	Loss: 0.040335	Acc: 45.0% (4501/10000)
[Test]  Epoch: 18	Loss: 0.040377	Acc: 45.0% (4501/10000)
[Test]  Epoch: 19	Loss: 0.040374	Acc: 44.8% (4484/10000)
[Test]  Epoch: 20	Loss: 0.040493	Acc: 45.1% (4511/10000)
[Test]  Epoch: 21	Loss: 0.040516	Acc: 45.2% (4520/10000)
[Test]  Epoch: 22	Loss: 0.040608	Acc: 44.8% (4479/10000)
[Test]  Epoch: 23	Loss: 0.040611	Acc: 44.9% (4487/10000)
[Test]  Epoch: 24	Loss: 0.040670	Acc: 44.8% (4484/10000)
[Test]  Epoch: 25	Loss: 0.040605	Acc: 45.2% (4517/10000)
[Test]  Epoch: 26	Loss: 0.040761	Acc: 44.7% (4474/10000)
[Test]  Epoch: 27	Loss: 0.040594	Acc: 45.0% (4504/10000)
[Test]  Epoch: 28	Loss: 0.040844	Acc: 44.6% (4462/10000)
[Test]  Epoch: 29	Loss: 0.040748	Acc: 44.7% (4473/10000)
[Test]  Epoch: 30	Loss: 0.040874	Acc: 44.7% (4470/10000)
[Test]  Epoch: 31	Loss: 0.041028	Acc: 44.9% (4487/10000)
[Test]  Epoch: 32	Loss: 0.040928	Acc: 44.8% (4480/10000)
[Test]  Epoch: 33	Loss: 0.040786	Acc: 45.0% (4496/10000)
[Test]  Epoch: 34	Loss: 0.040866	Acc: 44.8% (4479/10000)
[Test]  Epoch: 35	Loss: 0.040858	Acc: 44.9% (4488/10000)
[Test]  Epoch: 36	Loss: 0.040806	Acc: 44.9% (4487/10000)
[Test]  Epoch: 37	Loss: 0.040919	Acc: 45.0% (4496/10000)
[Test]  Epoch: 38	Loss: 0.041046	Acc: 44.7% (4468/10000)
[Test]  Epoch: 39	Loss: 0.041061	Acc: 44.7% (4468/10000)
[Test]  Epoch: 40	Loss: 0.040999	Acc: 44.7% (4469/10000)
[Test]  Epoch: 41	Loss: 0.041061	Acc: 44.8% (4476/10000)
[Test]  Epoch: 42	Loss: 0.040998	Acc: 44.9% (4492/10000)
[Test]  Epoch: 43	Loss: 0.040999	Acc: 44.8% (4481/10000)
[Test]  Epoch: 44	Loss: 0.041049	Acc: 44.8% (4480/10000)
[Test]  Epoch: 45	Loss: 0.041100	Acc: 44.9% (4485/10000)
[Test]  Epoch: 46	Loss: 0.041142	Acc: 44.7% (4470/10000)
[Test]  Epoch: 47	Loss: 0.041217	Acc: 44.9% (4486/10000)
[Test]  Epoch: 48	Loss: 0.041162	Acc: 44.8% (4481/10000)
[Test]  Epoch: 49	Loss: 0.041197	Acc: 44.8% (4477/10000)
[Test]  Epoch: 50	Loss: 0.041250	Acc: 44.6% (4459/10000)
[Test]  Epoch: 51	Loss: 0.041140	Acc: 44.8% (4475/10000)
[Test]  Epoch: 52	Loss: 0.041060	Acc: 44.9% (4487/10000)
[Test]  Epoch: 53	Loss: 0.041181	Acc: 44.8% (4479/10000)
[Test]  Epoch: 54	Loss: 0.041326	Acc: 44.7% (4469/10000)
[Test]  Epoch: 55	Loss: 0.041291	Acc: 44.6% (4462/10000)
[Test]  Epoch: 56	Loss: 0.041276	Acc: 44.6% (4460/10000)
[Test]  Epoch: 57	Loss: 0.041369	Acc: 44.4% (4436/10000)
[Test]  Epoch: 58	Loss: 0.041150	Acc: 44.7% (4470/10000)
[Test]  Epoch: 59	Loss: 0.041283	Acc: 44.6% (4460/10000)
[Test]  Epoch: 60	Loss: 0.041239	Acc: 44.7% (4467/10000)
[Test]  Epoch: 61	Loss: 0.041351	Acc: 44.7% (4470/10000)
[Test]  Epoch: 62	Loss: 0.041358	Acc: 44.5% (4454/10000)
[Test]  Epoch: 63	Loss: 0.041147	Acc: 45.0% (4495/10000)
[Test]  Epoch: 64	Loss: 0.041179	Acc: 44.8% (4479/10000)
[Test]  Epoch: 65	Loss: 0.041288	Acc: 44.6% (4458/10000)
[Test]  Epoch: 66	Loss: 0.041250	Acc: 44.8% (4482/10000)
[Test]  Epoch: 67	Loss: 0.041313	Acc: 44.6% (4463/10000)
[Test]  Epoch: 68	Loss: 0.041421	Acc: 44.4% (4442/10000)
[Test]  Epoch: 69	Loss: 0.041355	Acc: 44.5% (4447/10000)
[Test]  Epoch: 70	Loss: 0.041336	Acc: 44.6% (4458/10000)
[Test]  Epoch: 71	Loss: 0.041290	Acc: 44.6% (4464/10000)
[Test]  Epoch: 72	Loss: 0.041368	Acc: 44.4% (4443/10000)
[Test]  Epoch: 73	Loss: 0.041213	Acc: 44.6% (4461/10000)
[Test]  Epoch: 74	Loss: 0.041311	Acc: 44.5% (4449/10000)
[Test]  Epoch: 75	Loss: 0.041385	Acc: 44.6% (4462/10000)
[Test]  Epoch: 76	Loss: 0.041229	Acc: 44.7% (4474/10000)
[Test]  Epoch: 77	Loss: 0.041345	Acc: 44.6% (4459/10000)
[Test]  Epoch: 78	Loss: 0.041398	Acc: 44.4% (4436/10000)
[Test]  Epoch: 79	Loss: 0.041463	Acc: 44.4% (4442/10000)
[Test]  Epoch: 80	Loss: 0.041282	Acc: 44.9% (4485/10000)
[Test]  Epoch: 81	Loss: 0.041381	Acc: 44.6% (4464/10000)
[Test]  Epoch: 82	Loss: 0.041392	Acc: 44.6% (4458/10000)
[Test]  Epoch: 83	Loss: 0.041413	Acc: 44.4% (4444/10000)
[Test]  Epoch: 84	Loss: 0.041516	Acc: 44.5% (4454/10000)
[Test]  Epoch: 85	Loss: 0.041406	Acc: 44.5% (4445/10000)
[Test]  Epoch: 86	Loss: 0.041467	Acc: 44.4% (4439/10000)
[Test]  Epoch: 87	Loss: 0.041370	Acc: 44.4% (4442/10000)
[Test]  Epoch: 88	Loss: 0.041371	Acc: 44.5% (4450/10000)
[Test]  Epoch: 89	Loss: 0.041342	Acc: 44.5% (4448/10000)
[Test]  Epoch: 90	Loss: 0.041426	Acc: 44.4% (4444/10000)
[Test]  Epoch: 91	Loss: 0.041446	Acc: 44.6% (4459/10000)
[Test]  Epoch: 92	Loss: 0.041310	Acc: 44.6% (4462/10000)
[Test]  Epoch: 93	Loss: 0.041392	Acc: 44.4% (4442/10000)
[Test]  Epoch: 94	Loss: 0.041427	Acc: 44.5% (4451/10000)
[Test]  Epoch: 95	Loss: 0.041377	Acc: 44.5% (4445/10000)
[Test]  Epoch: 96	Loss: 0.041343	Acc: 44.8% (4477/10000)
[Test]  Epoch: 97	Loss: 0.041426	Acc: 44.6% (4460/10000)
[Test]  Epoch: 98	Loss: 0.041392	Acc: 44.5% (4447/10000)
[Test]  Epoch: 99	Loss: 0.041423	Acc: 44.4% (4439/10000)
[Test]  Epoch: 100	Loss: 0.041362	Acc: 44.4% (4440/10000)
===========finish==========
['2024-08-19', '03:05:27.849360', '100', 'test', '0.04136173195838928', '44.4', '45.8']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.045311	Acc: 35.8% (3579/10000)
[Test]  Epoch: 2	Loss: 0.041756	Acc: 41.6% (4165/10000)
[Test]  Epoch: 3	Loss: 0.041428	Acc: 42.3% (4229/10000)
[Test]  Epoch: 4	Loss: 0.041379	Acc: 42.4% (4242/10000)
[Test]  Epoch: 5	Loss: 0.041332	Acc: 42.7% (4274/10000)
[Test]  Epoch: 6	Loss: 0.041440	Acc: 42.5% (4255/10000)
[Test]  Epoch: 7	Loss: 0.041342	Acc: 42.9% (4287/10000)
[Test]  Epoch: 8	Loss: 0.041406	Acc: 42.9% (4294/10000)
[Test]  Epoch: 9	Loss: 0.041550	Acc: 42.8% (4276/10000)
[Test]  Epoch: 10	Loss: 0.041559	Acc: 42.8% (4283/10000)
[Test]  Epoch: 11	Loss: 0.041630	Acc: 42.8% (4278/10000)
[Test]  Epoch: 12	Loss: 0.041468	Acc: 43.1% (4310/10000)
[Test]  Epoch: 13	Loss: 0.041523	Acc: 43.0% (4300/10000)
[Test]  Epoch: 14	Loss: 0.041641	Acc: 43.0% (4295/10000)
[Test]  Epoch: 15	Loss: 0.041785	Acc: 43.0% (4297/10000)
[Test]  Epoch: 16	Loss: 0.041526	Acc: 43.2% (4320/10000)
[Test]  Epoch: 17	Loss: 0.041636	Acc: 43.0% (4304/10000)
[Test]  Epoch: 18	Loss: 0.041675	Acc: 43.1% (4309/10000)
[Test]  Epoch: 19	Loss: 0.041593	Acc: 43.1% (4312/10000)
[Test]  Epoch: 20	Loss: 0.041751	Acc: 43.1% (4308/10000)
[Test]  Epoch: 21	Loss: 0.041774	Acc: 43.1% (4314/10000)
[Test]  Epoch: 22	Loss: 0.041784	Acc: 43.0% (4305/10000)
[Test]  Epoch: 23	Loss: 0.041759	Acc: 42.9% (4287/10000)
[Test]  Epoch: 24	Loss: 0.041854	Acc: 43.1% (4308/10000)
[Test]  Epoch: 25	Loss: 0.041760	Acc: 43.3% (4334/10000)
[Test]  Epoch: 26	Loss: 0.041895	Acc: 43.1% (4309/10000)
[Test]  Epoch: 27	Loss: 0.041729	Acc: 43.3% (4326/10000)
[Test]  Epoch: 28	Loss: 0.041945	Acc: 42.9% (4290/10000)
[Test]  Epoch: 29	Loss: 0.041934	Acc: 43.2% (4318/10000)
[Test]  Epoch: 30	Loss: 0.041998	Acc: 43.1% (4313/10000)
[Test]  Epoch: 31	Loss: 0.042126	Acc: 43.3% (4328/10000)
[Test]  Epoch: 32	Loss: 0.041949	Acc: 43.3% (4329/10000)
[Test]  Epoch: 33	Loss: 0.041854	Acc: 43.4% (4339/10000)
[Test]  Epoch: 34	Loss: 0.041956	Acc: 43.5% (4345/10000)
[Test]  Epoch: 35	Loss: 0.041977	Acc: 43.3% (4332/10000)
[Test]  Epoch: 36	Loss: 0.041886	Acc: 43.2% (4316/10000)
[Test]  Epoch: 37	Loss: 0.042023	Acc: 43.5% (4346/10000)
[Test]  Epoch: 38	Loss: 0.042159	Acc: 43.2% (4323/10000)
[Test]  Epoch: 39	Loss: 0.042091	Acc: 43.1% (4312/10000)
[Test]  Epoch: 40	Loss: 0.042033	Acc: 43.2% (4319/10000)
[Test]  Epoch: 41	Loss: 0.042113	Acc: 43.1% (4311/10000)
[Test]  Epoch: 42	Loss: 0.042031	Acc: 43.4% (4343/10000)
[Test]  Epoch: 43	Loss: 0.041974	Acc: 43.7% (4367/10000)
[Test]  Epoch: 44	Loss: 0.042023	Acc: 43.6% (4362/10000)
[Test]  Epoch: 45	Loss: 0.042097	Acc: 43.4% (4336/10000)
[Test]  Epoch: 46	Loss: 0.042124	Acc: 43.3% (4327/10000)
[Test]  Epoch: 47	Loss: 0.042177	Acc: 43.4% (4335/10000)
[Test]  Epoch: 48	Loss: 0.042106	Acc: 43.4% (4342/10000)
[Test]  Epoch: 49	Loss: 0.042168	Acc: 43.2% (4322/10000)
[Test]  Epoch: 50	Loss: 0.042231	Acc: 43.2% (4317/10000)
[Test]  Epoch: 51	Loss: 0.042112	Acc: 43.3% (4327/10000)
[Test]  Epoch: 52	Loss: 0.042008	Acc: 43.5% (4354/10000)
[Test]  Epoch: 53	Loss: 0.042156	Acc: 43.3% (4329/10000)
[Test]  Epoch: 54	Loss: 0.042320	Acc: 43.0% (4304/10000)
[Test]  Epoch: 55	Loss: 0.042241	Acc: 43.3% (4330/10000)
[Test]  Epoch: 56	Loss: 0.042253	Acc: 43.1% (4308/10000)
[Test]  Epoch: 57	Loss: 0.042326	Acc: 42.9% (4292/10000)
[Test]  Epoch: 58	Loss: 0.042052	Acc: 43.6% (4359/10000)
[Test]  Epoch: 59	Loss: 0.042244	Acc: 43.4% (4344/10000)
[Test]  Epoch: 60	Loss: 0.042209	Acc: 43.4% (4342/10000)
[Test]  Epoch: 61	Loss: 0.042301	Acc: 43.2% (4320/10000)
[Test]  Epoch: 62	Loss: 0.042296	Acc: 43.4% (4336/10000)
[Test]  Epoch: 63	Loss: 0.042077	Acc: 43.6% (4357/10000)
[Test]  Epoch: 64	Loss: 0.042114	Acc: 43.6% (4357/10000)
[Test]  Epoch: 65	Loss: 0.042216	Acc: 43.5% (4352/10000)
[Test]  Epoch: 66	Loss: 0.042191	Acc: 43.4% (4342/10000)
[Test]  Epoch: 67	Loss: 0.042240	Acc: 43.5% (4352/10000)
[Test]  Epoch: 68	Loss: 0.042362	Acc: 43.2% (4320/10000)
[Test]  Epoch: 69	Loss: 0.042287	Acc: 43.3% (4332/10000)
[Test]  Epoch: 70	Loss: 0.042286	Acc: 43.1% (4312/10000)
[Test]  Epoch: 71	Loss: 0.042233	Acc: 43.5% (4345/10000)
[Test]  Epoch: 72	Loss: 0.042298	Acc: 43.2% (4321/10000)
[Test]  Epoch: 73	Loss: 0.042157	Acc: 43.4% (4338/10000)
[Test]  Epoch: 74	Loss: 0.042265	Acc: 43.2% (4320/10000)
[Test]  Epoch: 75	Loss: 0.042318	Acc: 43.2% (4321/10000)
[Test]  Epoch: 76	Loss: 0.042164	Acc: 43.6% (4357/10000)
[Test]  Epoch: 77	Loss: 0.042283	Acc: 43.3% (4330/10000)
[Test]  Epoch: 78	Loss: 0.042326	Acc: 43.0% (4297/10000)
[Test]  Epoch: 79	Loss: 0.042398	Acc: 43.0% (4302/10000)
[Test]  Epoch: 80	Loss: 0.042217	Acc: 43.2% (4325/10000)
[Test]  Epoch: 81	Loss: 0.042313	Acc: 43.4% (4339/10000)
[Test]  Epoch: 82	Loss: 0.042324	Acc: 43.0% (4300/10000)
[Test]  Epoch: 83	Loss: 0.042359	Acc: 43.1% (4314/10000)
[Test]  Epoch: 84	Loss: 0.042436	Acc: 43.3% (4326/10000)
[Test]  Epoch: 85	Loss: 0.042328	Acc: 43.1% (4308/10000)
[Test]  Epoch: 86	Loss: 0.042400	Acc: 43.0% (4297/10000)
[Test]  Epoch: 87	Loss: 0.042316	Acc: 43.1% (4308/10000)
[Test]  Epoch: 88	Loss: 0.042314	Acc: 43.3% (4326/10000)
[Test]  Epoch: 89	Loss: 0.042280	Acc: 43.3% (4328/10000)
[Test]  Epoch: 90	Loss: 0.042376	Acc: 43.3% (4327/10000)
[Test]  Epoch: 91	Loss: 0.042362	Acc: 43.2% (4322/10000)
[Test]  Epoch: 92	Loss: 0.042231	Acc: 43.4% (4343/10000)
[Test]  Epoch: 93	Loss: 0.042327	Acc: 43.3% (4332/10000)
[Test]  Epoch: 94	Loss: 0.042369	Acc: 43.6% (4357/10000)
[Test]  Epoch: 95	Loss: 0.042297	Acc: 43.5% (4348/10000)
[Test]  Epoch: 96	Loss: 0.042257	Acc: 43.4% (4338/10000)
[Test]  Epoch: 97	Loss: 0.042349	Acc: 43.1% (4309/10000)
[Test]  Epoch: 98	Loss: 0.042319	Acc: 43.1% (4310/10000)
[Test]  Epoch: 99	Loss: 0.042350	Acc: 43.0% (4302/10000)
[Test]  Epoch: 100	Loss: 0.042293	Acc: 43.2% (4317/10000)
===========finish==========
['2024-08-19', '03:10:15.494570', '100', 'test', '0.042293318569660183', '43.17', '43.67']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.052839	Acc: 25.0% (2499/10000)
[Test]  Epoch: 2	Loss: 0.044762	Acc: 36.4% (3639/10000)
[Test]  Epoch: 3	Loss: 0.044086	Acc: 37.9% (3792/10000)
[Test]  Epoch: 4	Loss: 0.043832	Acc: 38.4% (3844/10000)
[Test]  Epoch: 5	Loss: 0.043758	Acc: 38.6% (3863/10000)
[Test]  Epoch: 6	Loss: 0.043700	Acc: 39.0% (3905/10000)
[Test]  Epoch: 7	Loss: 0.043515	Acc: 39.4% (3939/10000)
[Test]  Epoch: 8	Loss: 0.043653	Acc: 38.9% (3889/10000)
[Test]  Epoch: 9	Loss: 0.043565	Acc: 39.4% (3939/10000)
[Test]  Epoch: 10	Loss: 0.043603	Acc: 39.5% (3952/10000)
[Test]  Epoch: 11	Loss: 0.043742	Acc: 39.5% (3945/10000)
[Test]  Epoch: 12	Loss: 0.043556	Acc: 39.5% (3945/10000)
[Test]  Epoch: 13	Loss: 0.043491	Acc: 39.8% (3983/10000)
[Test]  Epoch: 14	Loss: 0.043594	Acc: 39.7% (3970/10000)
[Test]  Epoch: 15	Loss: 0.043657	Acc: 39.7% (3972/10000)
[Test]  Epoch: 16	Loss: 0.043450	Acc: 40.4% (4039/10000)
[Test]  Epoch: 17	Loss: 0.043529	Acc: 40.4% (4036/10000)
[Test]  Epoch: 18	Loss: 0.043468	Acc: 40.4% (4035/10000)
[Test]  Epoch: 19	Loss: 0.043446	Acc: 40.0% (4005/10000)
[Test]  Epoch: 20	Loss: 0.043532	Acc: 40.2% (4021/10000)
[Test]  Epoch: 21	Loss: 0.043526	Acc: 40.2% (4025/10000)
[Test]  Epoch: 22	Loss: 0.043558	Acc: 40.2% (4017/10000)
[Test]  Epoch: 23	Loss: 0.043538	Acc: 40.3% (4029/10000)
[Test]  Epoch: 24	Loss: 0.043622	Acc: 40.2% (4018/10000)
[Test]  Epoch: 25	Loss: 0.043494	Acc: 40.5% (4050/10000)
[Test]  Epoch: 26	Loss: 0.043645	Acc: 40.3% (4033/10000)
[Test]  Epoch: 27	Loss: 0.043453	Acc: 40.4% (4036/10000)
[Test]  Epoch: 28	Loss: 0.043617	Acc: 40.5% (4046/10000)
[Test]  Epoch: 29	Loss: 0.043610	Acc: 40.3% (4031/10000)
[Test]  Epoch: 30	Loss: 0.043714	Acc: 40.6% (4062/10000)
[Test]  Epoch: 31	Loss: 0.043801	Acc: 40.4% (4038/10000)
[Test]  Epoch: 32	Loss: 0.043640	Acc: 40.5% (4053/10000)
[Test]  Epoch: 33	Loss: 0.043580	Acc: 40.8% (4076/10000)
[Test]  Epoch: 34	Loss: 0.043670	Acc: 41.1% (4106/10000)
[Test]  Epoch: 35	Loss: 0.043671	Acc: 40.7% (4066/10000)
[Test]  Epoch: 36	Loss: 0.043607	Acc: 40.8% (4077/10000)
[Test]  Epoch: 37	Loss: 0.043727	Acc: 40.9% (4091/10000)
[Test]  Epoch: 38	Loss: 0.043826	Acc: 40.8% (4082/10000)
[Test]  Epoch: 39	Loss: 0.043789	Acc: 40.6% (4058/10000)
[Test]  Epoch: 40	Loss: 0.043698	Acc: 40.8% (4079/10000)
[Test]  Epoch: 41	Loss: 0.043742	Acc: 40.8% (4076/10000)
[Test]  Epoch: 42	Loss: 0.043715	Acc: 41.0% (4098/10000)
[Test]  Epoch: 43	Loss: 0.043633	Acc: 41.1% (4112/10000)
[Test]  Epoch: 44	Loss: 0.043675	Acc: 41.1% (4114/10000)
[Test]  Epoch: 45	Loss: 0.043727	Acc: 41.0% (4097/10000)
[Test]  Epoch: 46	Loss: 0.043726	Acc: 40.9% (4090/10000)
[Test]  Epoch: 47	Loss: 0.043769	Acc: 41.1% (4107/10000)
[Test]  Epoch: 48	Loss: 0.043752	Acc: 41.2% (4122/10000)
[Test]  Epoch: 49	Loss: 0.043787	Acc: 41.3% (4130/10000)
[Test]  Epoch: 50	Loss: 0.043792	Acc: 41.1% (4106/10000)
[Test]  Epoch: 51	Loss: 0.043680	Acc: 41.2% (4121/10000)
[Test]  Epoch: 52	Loss: 0.043618	Acc: 41.4% (4140/10000)
[Test]  Epoch: 53	Loss: 0.043786	Acc: 41.3% (4130/10000)
[Test]  Epoch: 54	Loss: 0.043902	Acc: 41.3% (4131/10000)
[Test]  Epoch: 55	Loss: 0.043803	Acc: 41.2% (4123/10000)
[Test]  Epoch: 56	Loss: 0.043848	Acc: 41.0% (4097/10000)
[Test]  Epoch: 57	Loss: 0.043867	Acc: 40.9% (4089/10000)
[Test]  Epoch: 58	Loss: 0.043584	Acc: 41.6% (4159/10000)
[Test]  Epoch: 59	Loss: 0.043829	Acc: 41.2% (4123/10000)
[Test]  Epoch: 60	Loss: 0.043793	Acc: 41.1% (4115/10000)
[Test]  Epoch: 61	Loss: 0.043855	Acc: 41.1% (4114/10000)
[Test]  Epoch: 62	Loss: 0.043877	Acc: 41.2% (4118/10000)
[Test]  Epoch: 63	Loss: 0.043634	Acc: 41.4% (4140/10000)
[Test]  Epoch: 64	Loss: 0.043691	Acc: 41.4% (4138/10000)
[Test]  Epoch: 65	Loss: 0.043800	Acc: 41.3% (4126/10000)
[Test]  Epoch: 66	Loss: 0.043765	Acc: 41.4% (4136/10000)
[Test]  Epoch: 67	Loss: 0.043794	Acc: 41.4% (4139/10000)
[Test]  Epoch: 68	Loss: 0.043941	Acc: 41.0% (4095/10000)
[Test]  Epoch: 69	Loss: 0.043857	Acc: 41.2% (4122/10000)
[Test]  Epoch: 70	Loss: 0.043849	Acc: 41.3% (4128/10000)
[Test]  Epoch: 71	Loss: 0.043803	Acc: 41.5% (4146/10000)
[Test]  Epoch: 72	Loss: 0.043870	Acc: 41.1% (4106/10000)
[Test]  Epoch: 73	Loss: 0.043707	Acc: 41.3% (4134/10000)
[Test]  Epoch: 74	Loss: 0.043804	Acc: 41.3% (4132/10000)
[Test]  Epoch: 75	Loss: 0.043863	Acc: 41.3% (4126/10000)
[Test]  Epoch: 76	Loss: 0.043733	Acc: 41.5% (4150/10000)
[Test]  Epoch: 77	Loss: 0.043862	Acc: 41.4% (4135/10000)
[Test]  Epoch: 78	Loss: 0.043894	Acc: 41.2% (4119/10000)
[Test]  Epoch: 79	Loss: 0.043968	Acc: 40.9% (4091/10000)
[Test]  Epoch: 80	Loss: 0.043817	Acc: 41.2% (4121/10000)
[Test]  Epoch: 81	Loss: 0.043858	Acc: 41.3% (4127/10000)
[Test]  Epoch: 82	Loss: 0.043887	Acc: 41.3% (4133/10000)
[Test]  Epoch: 83	Loss: 0.043940	Acc: 41.2% (4119/10000)
[Test]  Epoch: 84	Loss: 0.044023	Acc: 41.2% (4117/10000)
[Test]  Epoch: 85	Loss: 0.043900	Acc: 41.3% (4132/10000)
[Test]  Epoch: 86	Loss: 0.043966	Acc: 41.1% (4114/10000)
[Test]  Epoch: 87	Loss: 0.043906	Acc: 41.4% (4139/10000)
[Test]  Epoch: 88	Loss: 0.043860	Acc: 41.2% (4124/10000)
[Test]  Epoch: 89	Loss: 0.043837	Acc: 41.4% (4136/10000)
[Test]  Epoch: 90	Loss: 0.043916	Acc: 41.4% (4139/10000)
[Test]  Epoch: 91	Loss: 0.043909	Acc: 41.3% (4128/10000)
[Test]  Epoch: 92	Loss: 0.043791	Acc: 41.4% (4138/10000)
[Test]  Epoch: 93	Loss: 0.043901	Acc: 41.3% (4129/10000)
[Test]  Epoch: 94	Loss: 0.043942	Acc: 41.2% (4122/10000)
[Test]  Epoch: 95	Loss: 0.043862	Acc: 41.3% (4126/10000)
[Test]  Epoch: 96	Loss: 0.043799	Acc: 41.5% (4145/10000)
[Test]  Epoch: 97	Loss: 0.043914	Acc: 41.2% (4116/10000)
[Test]  Epoch: 98	Loss: 0.043865	Acc: 41.4% (4142/10000)
[Test]  Epoch: 99	Loss: 0.043899	Acc: 41.1% (4111/10000)
[Test]  Epoch: 100	Loss: 0.043831	Acc: 41.4% (4135/10000)
===========finish==========
['2024-08-19', '03:15:06.715138', '100', 'test', '0.04383114740848541', '41.35', '41.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.061014	Acc: 15.7% (1570/10000)
[Test]  Epoch: 2	Loss: 0.048671	Acc: 30.7% (3069/10000)
[Test]  Epoch: 3	Loss: 0.046701	Acc: 33.9% (3392/10000)
[Test]  Epoch: 4	Loss: 0.046366	Acc: 34.9% (3493/10000)
[Test]  Epoch: 5	Loss: 0.045904	Acc: 35.9% (3587/10000)
[Test]  Epoch: 6	Loss: 0.045838	Acc: 36.0% (3603/10000)
[Test]  Epoch: 7	Loss: 0.045651	Acc: 36.8% (3679/10000)
[Test]  Epoch: 8	Loss: 0.045449	Acc: 37.0% (3699/10000)
[Test]  Epoch: 9	Loss: 0.045505	Acc: 37.3% (3728/10000)
[Test]  Epoch: 10	Loss: 0.045292	Acc: 37.3% (3734/10000)
[Test]  Epoch: 11	Loss: 0.045394	Acc: 37.5% (3750/10000)
[Test]  Epoch: 12	Loss: 0.045201	Acc: 38.0% (3800/10000)
[Test]  Epoch: 13	Loss: 0.045159	Acc: 38.1% (3806/10000)
[Test]  Epoch: 14	Loss: 0.045226	Acc: 37.9% (3786/10000)
[Test]  Epoch: 15	Loss: 0.045358	Acc: 37.8% (3784/10000)
[Test]  Epoch: 16	Loss: 0.044984	Acc: 38.3% (3828/10000)
[Test]  Epoch: 17	Loss: 0.045082	Acc: 38.2% (3819/10000)
[Test]  Epoch: 18	Loss: 0.045100	Acc: 38.3% (3832/10000)
[Test]  Epoch: 19	Loss: 0.044951	Acc: 38.4% (3844/10000)
[Test]  Epoch: 20	Loss: 0.045133	Acc: 38.2% (3822/10000)
[Test]  Epoch: 21	Loss: 0.044989	Acc: 38.4% (3843/10000)
[Test]  Epoch: 22	Loss: 0.045040	Acc: 38.4% (3839/10000)
[Test]  Epoch: 23	Loss: 0.045002	Acc: 38.8% (3884/10000)
[Test]  Epoch: 24	Loss: 0.045110	Acc: 38.7% (3874/10000)
[Test]  Epoch: 25	Loss: 0.044948	Acc: 38.7% (3871/10000)
[Test]  Epoch: 26	Loss: 0.044987	Acc: 38.6% (3863/10000)
[Test]  Epoch: 27	Loss: 0.044891	Acc: 38.8% (3883/10000)
[Test]  Epoch: 28	Loss: 0.045070	Acc: 38.9% (3888/10000)
[Test]  Epoch: 29	Loss: 0.044951	Acc: 39.0% (3895/10000)
[Test]  Epoch: 30	Loss: 0.045094	Acc: 39.1% (3911/10000)
[Test]  Epoch: 31	Loss: 0.045144	Acc: 39.1% (3906/10000)
[Test]  Epoch: 32	Loss: 0.044895	Acc: 39.2% (3916/10000)
[Test]  Epoch: 33	Loss: 0.044889	Acc: 39.2% (3921/10000)
[Test]  Epoch: 34	Loss: 0.044951	Acc: 39.4% (3943/10000)
[Test]  Epoch: 35	Loss: 0.044953	Acc: 39.3% (3927/10000)
[Test]  Epoch: 36	Loss: 0.044845	Acc: 39.6% (3959/10000)
[Test]  Epoch: 37	Loss: 0.044895	Acc: 39.5% (3955/10000)
[Test]  Epoch: 38	Loss: 0.045005	Acc: 39.3% (3932/10000)
[Test]  Epoch: 39	Loss: 0.044968	Acc: 39.4% (3943/10000)
[Test]  Epoch: 40	Loss: 0.044850	Acc: 39.6% (3958/10000)
[Test]  Epoch: 41	Loss: 0.044940	Acc: 39.2% (3924/10000)
[Test]  Epoch: 42	Loss: 0.044957	Acc: 39.8% (3979/10000)
[Test]  Epoch: 43	Loss: 0.044799	Acc: 39.8% (3984/10000)
[Test]  Epoch: 44	Loss: 0.044849	Acc: 39.8% (3975/10000)
[Test]  Epoch: 45	Loss: 0.045034	Acc: 39.4% (3944/10000)
[Test]  Epoch: 46	Loss: 0.044982	Acc: 39.3% (3934/10000)
[Test]  Epoch: 47	Loss: 0.044985	Acc: 39.6% (3956/10000)
[Test]  Epoch: 48	Loss: 0.044930	Acc: 39.8% (3975/10000)
[Test]  Epoch: 49	Loss: 0.044921	Acc: 39.9% (3991/10000)
[Test]  Epoch: 50	Loss: 0.045043	Acc: 39.5% (3955/10000)
[Test]  Epoch: 51	Loss: 0.044920	Acc: 39.6% (3961/10000)
[Test]  Epoch: 52	Loss: 0.044752	Acc: 39.8% (3977/10000)
[Test]  Epoch: 53	Loss: 0.045037	Acc: 39.8% (3979/10000)
[Test]  Epoch: 54	Loss: 0.045090	Acc: 39.7% (3974/10000)
[Test]  Epoch: 55	Loss: 0.044961	Acc: 39.9% (3987/10000)
[Test]  Epoch: 56	Loss: 0.045012	Acc: 39.6% (3960/10000)
[Test]  Epoch: 57	Loss: 0.044998	Acc: 39.6% (3961/10000)
[Test]  Epoch: 58	Loss: 0.044754	Acc: 40.2% (4016/10000)
[Test]  Epoch: 59	Loss: 0.045032	Acc: 40.1% (4008/10000)
[Test]  Epoch: 60	Loss: 0.045037	Acc: 39.9% (3992/10000)
[Test]  Epoch: 61	Loss: 0.045014	Acc: 39.6% (3958/10000)
[Test]  Epoch: 62	Loss: 0.045091	Acc: 39.4% (3944/10000)
[Test]  Epoch: 63	Loss: 0.044828	Acc: 40.1% (4011/10000)
[Test]  Epoch: 64	Loss: 0.044821	Acc: 40.1% (4011/10000)
[Test]  Epoch: 65	Loss: 0.044879	Acc: 39.9% (3987/10000)
[Test]  Epoch: 66	Loss: 0.044876	Acc: 40.0% (4003/10000)
[Test]  Epoch: 67	Loss: 0.044948	Acc: 39.8% (3983/10000)
[Test]  Epoch: 68	Loss: 0.045098	Acc: 39.7% (3968/10000)
[Test]  Epoch: 69	Loss: 0.044993	Acc: 39.8% (3983/10000)
[Test]  Epoch: 70	Loss: 0.044993	Acc: 39.7% (3966/10000)
[Test]  Epoch: 71	Loss: 0.044915	Acc: 39.9% (3988/10000)
[Test]  Epoch: 72	Loss: 0.045007	Acc: 39.6% (3964/10000)
[Test]  Epoch: 73	Loss: 0.044893	Acc: 39.8% (3977/10000)
[Test]  Epoch: 74	Loss: 0.045010	Acc: 39.6% (3964/10000)
[Test]  Epoch: 75	Loss: 0.045025	Acc: 39.8% (3981/10000)
[Test]  Epoch: 76	Loss: 0.044871	Acc: 40.0% (3999/10000)
[Test]  Epoch: 77	Loss: 0.045058	Acc: 39.7% (3974/10000)
[Test]  Epoch: 78	Loss: 0.045026	Acc: 39.6% (3959/10000)
[Test]  Epoch: 79	Loss: 0.045095	Acc: 39.6% (3961/10000)
[Test]  Epoch: 80	Loss: 0.044932	Acc: 39.8% (3980/10000)
[Test]  Epoch: 81	Loss: 0.044999	Acc: 39.9% (3989/10000)
[Test]  Epoch: 82	Loss: 0.045005	Acc: 39.8% (3984/10000)
[Test]  Epoch: 83	Loss: 0.045054	Acc: 39.7% (3966/10000)
[Test]  Epoch: 84	Loss: 0.045144	Acc: 39.6% (3965/10000)
[Test]  Epoch: 85	Loss: 0.045036	Acc: 39.8% (3981/10000)
[Test]  Epoch: 86	Loss: 0.045085	Acc: 39.8% (3983/10000)
[Test]  Epoch: 87	Loss: 0.045001	Acc: 39.9% (3994/10000)
[Test]  Epoch: 88	Loss: 0.045019	Acc: 39.7% (3974/10000)
[Test]  Epoch: 89	Loss: 0.044983	Acc: 39.9% (3988/10000)
[Test]  Epoch: 90	Loss: 0.045037	Acc: 39.7% (3969/10000)
[Test]  Epoch: 91	Loss: 0.045025	Acc: 39.8% (3983/10000)
[Test]  Epoch: 92	Loss: 0.044966	Acc: 39.8% (3983/10000)
[Test]  Epoch: 93	Loss: 0.045066	Acc: 39.9% (3990/10000)
[Test]  Epoch: 94	Loss: 0.045094	Acc: 39.7% (3971/10000)
[Test]  Epoch: 95	Loss: 0.045038	Acc: 39.9% (3994/10000)
[Test]  Epoch: 96	Loss: 0.044956	Acc: 39.8% (3982/10000)
[Test]  Epoch: 97	Loss: 0.045045	Acc: 39.7% (3970/10000)
[Test]  Epoch: 98	Loss: 0.045008	Acc: 39.7% (3971/10000)
[Test]  Epoch: 99	Loss: 0.045070	Acc: 39.5% (3948/10000)
[Test]  Epoch: 100	Loss: 0.045050	Acc: 39.8% (3976/10000)
===========finish==========
['2024-08-19', '03:19:48.943204', '100', 'test', '0.04505035258531571', '39.76', '40.16']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.0.1.weight', '_features.15.conv.1.1.weight', '_features.14.conv.3.weight', '_features.12.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.18.1.weight', '_features.15.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.090940	Acc: 5.7% (568/10000)
[Test]  Epoch: 2	Loss: 0.054713	Acc: 22.6% (2265/10000)
[Test]  Epoch: 3	Loss: 0.050141	Acc: 28.5% (2852/10000)
[Test]  Epoch: 4	Loss: 0.049149	Acc: 29.7% (2973/10000)
[Test]  Epoch: 5	Loss: 0.049240	Acc: 30.3% (3031/10000)
[Test]  Epoch: 6	Loss: 0.048278	Acc: 32.2% (3216/10000)
[Test]  Epoch: 7	Loss: 0.048138	Acc: 32.1% (3209/10000)
[Test]  Epoch: 8	Loss: 0.048092	Acc: 32.4% (3244/10000)
[Test]  Epoch: 9	Loss: 0.048153	Acc: 33.0% (3299/10000)
[Test]  Epoch: 10	Loss: 0.048485	Acc: 32.5% (3254/10000)
[Test]  Epoch: 11	Loss: 0.048056	Acc: 33.4% (3335/10000)
[Test]  Epoch: 12	Loss: 0.048124	Acc: 33.5% (3349/10000)
[Test]  Epoch: 13	Loss: 0.048081	Acc: 34.1% (3412/10000)
[Test]  Epoch: 14	Loss: 0.047877	Acc: 34.1% (3408/10000)
[Test]  Epoch: 15	Loss: 0.047954	Acc: 33.9% (3392/10000)
[Test]  Epoch: 16	Loss: 0.047831	Acc: 34.2% (3425/10000)
[Test]  Epoch: 17	Loss: 0.047853	Acc: 34.4% (3436/10000)
[Test]  Epoch: 18	Loss: 0.047551	Acc: 34.8% (3476/10000)
[Test]  Epoch: 19	Loss: 0.047665	Acc: 34.9% (3488/10000)
[Test]  Epoch: 20	Loss: 0.047821	Acc: 34.8% (3476/10000)
[Test]  Epoch: 21	Loss: 0.047549	Acc: 34.8% (3477/10000)
[Test]  Epoch: 22	Loss: 0.047877	Acc: 35.1% (3512/10000)
[Test]  Epoch: 23	Loss: 0.047684	Acc: 35.2% (3519/10000)
[Test]  Epoch: 24	Loss: 0.047676	Acc: 35.3% (3531/10000)
[Test]  Epoch: 25	Loss: 0.047366	Acc: 35.4% (3542/10000)
[Test]  Epoch: 26	Loss: 0.047655	Acc: 35.2% (3525/10000)
[Test]  Epoch: 27	Loss: 0.047286	Acc: 35.7% (3570/10000)
[Test]  Epoch: 28	Loss: 0.047538	Acc: 35.6% (3561/10000)
[Test]  Epoch: 29	Loss: 0.047359	Acc: 35.8% (3581/10000)
[Test]  Epoch: 30	Loss: 0.047423	Acc: 36.0% (3600/10000)
[Test]  Epoch: 31	Loss: 0.047539	Acc: 36.0% (3605/10000)
[Test]  Epoch: 32	Loss: 0.047456	Acc: 35.7% (3566/10000)
[Test]  Epoch: 33	Loss: 0.047373	Acc: 36.1% (3609/10000)
[Test]  Epoch: 34	Loss: 0.047413	Acc: 36.3% (3628/10000)
[Test]  Epoch: 35	Loss: 0.047536	Acc: 35.8% (3579/10000)
[Test]  Epoch: 36	Loss: 0.047415	Acc: 36.1% (3615/10000)
[Test]  Epoch: 37	Loss: 0.047442	Acc: 36.6% (3656/10000)
[Test]  Epoch: 38	Loss: 0.047624	Acc: 36.3% (3631/10000)
[Test]  Epoch: 39	Loss: 0.047576	Acc: 36.1% (3612/10000)
[Test]  Epoch: 40	Loss: 0.047392	Acc: 36.4% (3641/10000)
[Test]  Epoch: 41	Loss: 0.047373	Acc: 36.2% (3623/10000)
[Test]  Epoch: 42	Loss: 0.047122	Acc: 36.8% (3676/10000)
[Test]  Epoch: 43	Loss: 0.047086	Acc: 36.8% (3678/10000)
[Test]  Epoch: 44	Loss: 0.047615	Acc: 36.6% (3659/10000)
[Test]  Epoch: 45	Loss: 0.047472	Acc: 36.6% (3658/10000)
[Test]  Epoch: 46	Loss: 0.047260	Acc: 36.7% (3674/10000)
[Test]  Epoch: 47	Loss: 0.047489	Acc: 36.8% (3682/10000)
[Test]  Epoch: 48	Loss: 0.047590	Acc: 36.6% (3664/10000)
[Test]  Epoch: 49	Loss: 0.047544	Acc: 36.8% (3676/10000)
[Test]  Epoch: 50	Loss: 0.047268	Acc: 36.7% (3671/10000)
[Test]  Epoch: 51	Loss: 0.047387	Acc: 36.9% (3689/10000)
[Test]  Epoch: 52	Loss: 0.047293	Acc: 37.0% (3704/10000)
[Test]  Epoch: 53	Loss: 0.047498	Acc: 36.7% (3670/10000)
[Test]  Epoch: 54	Loss: 0.047435	Acc: 37.2% (3716/10000)
[Test]  Epoch: 55	Loss: 0.047280	Acc: 37.1% (3714/10000)
[Test]  Epoch: 56	Loss: 0.047346	Acc: 36.9% (3687/10000)
[Test]  Epoch: 57	Loss: 0.047370	Acc: 36.7% (3670/10000)
[Test]  Epoch: 58	Loss: 0.047106	Acc: 37.4% (3738/10000)
[Test]  Epoch: 59	Loss: 0.047366	Acc: 37.0% (3700/10000)
[Test]  Epoch: 60	Loss: 0.047274	Acc: 37.2% (3717/10000)
[Test]  Epoch: 61	Loss: 0.047254	Acc: 37.4% (3737/10000)
[Test]  Epoch: 62	Loss: 0.047391	Acc: 37.4% (3736/10000)
[Test]  Epoch: 63	Loss: 0.047167	Acc: 37.4% (3740/10000)
[Test]  Epoch: 64	Loss: 0.047207	Acc: 37.5% (3745/10000)
[Test]  Epoch: 65	Loss: 0.047270	Acc: 37.3% (3732/10000)
[Test]  Epoch: 66	Loss: 0.047255	Acc: 37.4% (3743/10000)
[Test]  Epoch: 67	Loss: 0.047353	Acc: 37.1% (3708/10000)
[Test]  Epoch: 68	Loss: 0.047400	Acc: 37.2% (3718/10000)
[Test]  Epoch: 69	Loss: 0.047386	Acc: 37.2% (3720/10000)
[Test]  Epoch: 70	Loss: 0.047393	Acc: 37.4% (3735/10000)
[Test]  Epoch: 71	Loss: 0.047287	Acc: 37.6% (3761/10000)
[Test]  Epoch: 72	Loss: 0.047387	Acc: 37.4% (3737/10000)
[Test]  Epoch: 73	Loss: 0.047262	Acc: 37.6% (3764/10000)
[Test]  Epoch: 74	Loss: 0.047366	Acc: 37.6% (3757/10000)
[Test]  Epoch: 75	Loss: 0.047409	Acc: 37.4% (3737/10000)
[Test]  Epoch: 76	Loss: 0.047212	Acc: 37.7% (3771/10000)
[Test]  Epoch: 77	Loss: 0.047385	Acc: 37.4% (3742/10000)
[Test]  Epoch: 78	Loss: 0.047338	Acc: 37.4% (3739/10000)
[Test]  Epoch: 79	Loss: 0.047374	Acc: 37.4% (3743/10000)
[Test]  Epoch: 80	Loss: 0.047274	Acc: 37.6% (3756/10000)
[Test]  Epoch: 81	Loss: 0.047358	Acc: 37.5% (3754/10000)
[Test]  Epoch: 82	Loss: 0.047365	Acc: 37.6% (3757/10000)
[Test]  Epoch: 83	Loss: 0.047383	Acc: 37.3% (3734/10000)
[Test]  Epoch: 84	Loss: 0.047457	Acc: 37.4% (3740/10000)
[Test]  Epoch: 85	Loss: 0.047417	Acc: 37.3% (3734/10000)
[Test]  Epoch: 86	Loss: 0.047405	Acc: 37.4% (3743/10000)
[Test]  Epoch: 87	Loss: 0.047336	Acc: 37.5% (3747/10000)
[Test]  Epoch: 88	Loss: 0.047333	Acc: 37.5% (3752/10000)
[Test]  Epoch: 89	Loss: 0.047297	Acc: 37.7% (3768/10000)
[Test]  Epoch: 90	Loss: 0.047386	Acc: 37.4% (3743/10000)
[Test]  Epoch: 91	Loss: 0.047351	Acc: 37.6% (3760/10000)
[Test]  Epoch: 92	Loss: 0.047247	Acc: 37.7% (3767/10000)
[Test]  Epoch: 93	Loss: 0.047374	Acc: 37.4% (3739/10000)
[Test]  Epoch: 94	Loss: 0.047455	Acc: 37.6% (3764/10000)
[Test]  Epoch: 95	Loss: 0.047323	Acc: 37.5% (3752/10000)
[Test]  Epoch: 96	Loss: 0.047252	Acc: 37.7% (3768/10000)
[Test]  Epoch: 97	Loss: 0.047297	Acc: 37.5% (3749/10000)
[Test]  Epoch: 98	Loss: 0.047243	Acc: 37.4% (3740/10000)
[Test]  Epoch: 99	Loss: 0.047363	Acc: 37.5% (3745/10000)
[Test]  Epoch: 100	Loss: 0.047319	Acc: 37.6% (3756/10000)
===========finish==========
['2024-08-19', '03:24:35.449646', '100', 'test', '0.047318822705745694', '37.56', '37.71']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.0.1.weight', '_features.15.conv.1.1.weight', '_features.14.conv.3.weight', '_features.12.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.18.1.weight', '_features.15.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.16.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.14.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.102558	Acc: 4.2% (416/10000)
[Test]  Epoch: 2	Loss: 0.053358	Acc: 24.2% (2420/10000)
[Test]  Epoch: 3	Loss: 0.049809	Acc: 29.1% (2915/10000)
[Test]  Epoch: 4	Loss: 0.049042	Acc: 31.1% (3115/10000)
[Test]  Epoch: 5	Loss: 0.048698	Acc: 32.1% (3214/10000)
[Test]  Epoch: 6	Loss: 0.050792	Acc: 28.9% (2895/10000)
[Test]  Epoch: 7	Loss: 0.048444	Acc: 32.6% (3264/10000)
[Test]  Epoch: 8	Loss: 0.048238	Acc: 33.2% (3319/10000)
[Test]  Epoch: 9	Loss: 0.048341	Acc: 33.3% (3327/10000)
[Test]  Epoch: 10	Loss: 0.048072	Acc: 33.5% (3345/10000)
[Test]  Epoch: 11	Loss: 0.048026	Acc: 34.2% (3418/10000)
[Test]  Epoch: 12	Loss: 0.047812	Acc: 34.4% (3442/10000)
[Test]  Epoch: 13	Loss: 0.047812	Acc: 34.8% (3481/10000)
[Test]  Epoch: 14	Loss: 0.047643	Acc: 35.3% (3527/10000)
[Test]  Epoch: 15	Loss: 0.047650	Acc: 35.3% (3526/10000)
[Test]  Epoch: 16	Loss: 0.047567	Acc: 35.0% (3504/10000)
[Test]  Epoch: 17	Loss: 0.047636	Acc: 35.5% (3548/10000)
[Test]  Epoch: 18	Loss: 0.047368	Acc: 35.8% (3578/10000)
[Test]  Epoch: 19	Loss: 0.047469	Acc: 35.8% (3579/10000)
[Test]  Epoch: 20	Loss: 0.047635	Acc: 36.1% (3611/10000)
[Test]  Epoch: 21	Loss: 0.047108	Acc: 36.5% (3645/10000)
[Test]  Epoch: 22	Loss: 0.047437	Acc: 36.0% (3598/10000)
[Test]  Epoch: 23	Loss: 0.047484	Acc: 35.6% (3564/10000)
[Test]  Epoch: 24	Loss: 0.047474	Acc: 36.1% (3606/10000)
[Test]  Epoch: 25	Loss: 0.047185	Acc: 36.6% (3659/10000)
[Test]  Epoch: 26	Loss: 0.047378	Acc: 36.2% (3624/10000)
[Test]  Epoch: 27	Loss: 0.047170	Acc: 36.5% (3654/10000)
[Test]  Epoch: 28	Loss: 0.047193	Acc: 36.8% (3680/10000)
[Test]  Epoch: 29	Loss: 0.047126	Acc: 36.7% (3674/10000)
[Test]  Epoch: 30	Loss: 0.047200	Acc: 37.1% (3707/10000)
[Test]  Epoch: 31	Loss: 0.047379	Acc: 36.8% (3681/10000)
[Test]  Epoch: 32	Loss: 0.047266	Acc: 36.5% (3652/10000)
[Test]  Epoch: 33	Loss: 0.047282	Acc: 36.6% (3664/10000)
[Test]  Epoch: 34	Loss: 0.047060	Acc: 36.8% (3676/10000)
[Test]  Epoch: 35	Loss: 0.047085	Acc: 36.9% (3693/10000)
[Test]  Epoch: 36	Loss: 0.047032	Acc: 37.5% (3748/10000)
[Test]  Epoch: 37	Loss: 0.047104	Acc: 37.3% (3729/10000)
[Test]  Epoch: 38	Loss: 0.047163	Acc: 37.0% (3705/10000)
[Test]  Epoch: 39	Loss: 0.047136	Acc: 37.2% (3722/10000)
[Test]  Epoch: 40	Loss: 0.047031	Acc: 37.1% (3711/10000)
[Test]  Epoch: 41	Loss: 0.047064	Acc: 36.9% (3692/10000)
[Test]  Epoch: 42	Loss: 0.046976	Acc: 37.4% (3739/10000)
[Test]  Epoch: 43	Loss: 0.046664	Acc: 37.9% (3788/10000)
[Test]  Epoch: 44	Loss: 0.047075	Acc: 37.6% (3763/10000)
[Test]  Epoch: 45	Loss: 0.046996	Acc: 37.6% (3765/10000)
[Test]  Epoch: 46	Loss: 0.046948	Acc: 37.5% (3752/10000)
[Test]  Epoch: 47	Loss: 0.047042	Acc: 37.7% (3773/10000)
[Test]  Epoch: 48	Loss: 0.047002	Acc: 37.7% (3769/10000)
[Test]  Epoch: 49	Loss: 0.047049	Acc: 37.7% (3768/10000)
[Test]  Epoch: 50	Loss: 0.047243	Acc: 37.2% (3719/10000)
[Test]  Epoch: 51	Loss: 0.046844	Acc: 37.9% (3790/10000)
[Test]  Epoch: 52	Loss: 0.046801	Acc: 38.1% (3806/10000)
[Test]  Epoch: 53	Loss: 0.047002	Acc: 37.9% (3786/10000)
[Test]  Epoch: 54	Loss: 0.047020	Acc: 37.9% (3791/10000)
[Test]  Epoch: 55	Loss: 0.046756	Acc: 38.1% (3812/10000)
[Test]  Epoch: 56	Loss: 0.047255	Acc: 37.4% (3740/10000)
[Test]  Epoch: 57	Loss: 0.046860	Acc: 37.9% (3792/10000)
[Test]  Epoch: 58	Loss: 0.046741	Acc: 38.5% (3848/10000)
[Test]  Epoch: 59	Loss: 0.050530	Acc: 32.6% (3257/10000)
[Test]  Epoch: 60	Loss: 0.047264	Acc: 36.4% (3636/10000)
[Test]  Epoch: 61	Loss: 0.047139	Acc: 37.0% (3701/10000)
[Test]  Epoch: 62	Loss: 0.047151	Acc: 37.0% (3698/10000)
[Test]  Epoch: 63	Loss: 0.046992	Acc: 37.2% (3719/10000)
[Test]  Epoch: 64	Loss: 0.047010	Acc: 37.3% (3733/10000)
[Test]  Epoch: 65	Loss: 0.047083	Acc: 37.3% (3727/10000)
[Test]  Epoch: 66	Loss: 0.047086	Acc: 37.3% (3732/10000)
[Test]  Epoch: 67	Loss: 0.047036	Acc: 37.4% (3740/10000)
[Test]  Epoch: 68	Loss: 0.047080	Acc: 37.2% (3723/10000)
[Test]  Epoch: 69	Loss: 0.047055	Acc: 37.4% (3741/10000)
[Test]  Epoch: 70	Loss: 0.047123	Acc: 37.5% (3751/10000)
[Test]  Epoch: 71	Loss: 0.046966	Acc: 37.7% (3766/10000)
[Test]  Epoch: 72	Loss: 0.047032	Acc: 37.5% (3752/10000)
[Test]  Epoch: 73	Loss: 0.047013	Acc: 37.6% (3760/10000)
[Test]  Epoch: 74	Loss: 0.047043	Acc: 37.5% (3750/10000)
[Test]  Epoch: 75	Loss: 0.047094	Acc: 37.6% (3763/10000)
[Test]  Epoch: 76	Loss: 0.046968	Acc: 37.7% (3768/10000)
[Test]  Epoch: 77	Loss: 0.047080	Acc: 37.4% (3740/10000)
[Test]  Epoch: 78	Loss: 0.047115	Acc: 37.5% (3746/10000)
[Test]  Epoch: 79	Loss: 0.047047	Acc: 37.4% (3741/10000)
[Test]  Epoch: 80	Loss: 0.046942	Acc: 37.6% (3761/10000)
[Test]  Epoch: 81	Loss: 0.047029	Acc: 37.6% (3757/10000)
[Test]  Epoch: 82	Loss: 0.046974	Acc: 37.5% (3755/10000)
[Test]  Epoch: 83	Loss: 0.047070	Acc: 37.6% (3759/10000)
[Test]  Epoch: 84	Loss: 0.047129	Acc: 37.6% (3758/10000)
[Test]  Epoch: 85	Loss: 0.047137	Acc: 37.6% (3764/10000)
[Test]  Epoch: 86	Loss: 0.047114	Acc: 37.5% (3749/10000)
[Test]  Epoch: 87	Loss: 0.047003	Acc: 37.7% (3768/10000)
[Test]  Epoch: 88	Loss: 0.046994	Acc: 37.8% (3780/10000)
[Test]  Epoch: 89	Loss: 0.046982	Acc: 37.7% (3767/10000)
[Test]  Epoch: 90	Loss: 0.047046	Acc: 37.6% (3758/10000)
[Test]  Epoch: 91	Loss: 0.047018	Acc: 37.7% (3771/10000)
[Test]  Epoch: 92	Loss: 0.046896	Acc: 37.7% (3772/10000)
[Test]  Epoch: 93	Loss: 0.047014	Acc: 37.8% (3780/10000)
[Test]  Epoch: 94	Loss: 0.047099	Acc: 37.8% (3776/10000)
[Test]  Epoch: 95	Loss: 0.046996	Acc: 37.7% (3773/10000)
[Test]  Epoch: 96	Loss: 0.046849	Acc: 37.8% (3784/10000)
[Test]  Epoch: 97	Loss: 0.047014	Acc: 37.6% (3759/10000)
[Test]  Epoch: 98	Loss: 0.046953	Acc: 37.9% (3788/10000)
[Test]  Epoch: 99	Loss: 0.046976	Acc: 37.8% (3780/10000)
[Test]  Epoch: 100	Loss: 0.046943	Acc: 37.9% (3793/10000)
===========finish==========
['2024-08-19', '03:29:10.861406', '100', 'test', '0.046943246150016785', '37.93', '38.48']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.0.1.weight', '_features.15.conv.1.1.weight', '_features.14.conv.3.weight', '_features.12.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.18.1.weight', '_features.15.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.16.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.14.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.17.conv.3.weight', '_features.8.conv.1.0.weight', '_features.12.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.4.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.3.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.118398	Acc: 3.1% (308/10000)
[Test]  Epoch: 2	Loss: 0.052394	Acc: 25.9% (2594/10000)
[Test]  Epoch: 3	Loss: 0.049467	Acc: 29.7% (2970/10000)
[Test]  Epoch: 4	Loss: 0.048513	Acc: 31.4% (3142/10000)
[Test]  Epoch: 5	Loss: 0.048031	Acc: 32.5% (3251/10000)
[Test]  Epoch: 6	Loss: 0.048259	Acc: 32.9% (3285/10000)
[Test]  Epoch: 7	Loss: 0.047769	Acc: 33.4% (3335/10000)
[Test]  Epoch: 8	Loss: 0.048124	Acc: 32.7% (3268/10000)
[Test]  Epoch: 9	Loss: 0.047854	Acc: 33.8% (3382/10000)
[Test]  Epoch: 10	Loss: 0.047648	Acc: 34.4% (3435/10000)
[Test]  Epoch: 11	Loss: 0.047504	Acc: 34.4% (3436/10000)
[Test]  Epoch: 12	Loss: 0.047693	Acc: 34.6% (3461/10000)
[Test]  Epoch: 13	Loss: 0.047328	Acc: 34.9% (3491/10000)
[Test]  Epoch: 14	Loss: 0.047337	Acc: 35.1% (3513/10000)
[Test]  Epoch: 15	Loss: 0.047368	Acc: 35.0% (3505/10000)
[Test]  Epoch: 16	Loss: 0.047269	Acc: 35.2% (3524/10000)
[Test]  Epoch: 17	Loss: 0.047376	Acc: 35.4% (3540/10000)
[Test]  Epoch: 18	Loss: 0.047200	Acc: 35.4% (3543/10000)
[Test]  Epoch: 19	Loss: 0.047052	Acc: 36.0% (3600/10000)
[Test]  Epoch: 20	Loss: 0.047371	Acc: 35.5% (3545/10000)
[Test]  Epoch: 21	Loss: 0.047037	Acc: 35.9% (3587/10000)
[Test]  Epoch: 22	Loss: 0.047278	Acc: 35.6% (3556/10000)
[Test]  Epoch: 23	Loss: 0.047331	Acc: 35.6% (3562/10000)
[Test]  Epoch: 24	Loss: 0.047170	Acc: 36.1% (3611/10000)
[Test]  Epoch: 25	Loss: 0.046972	Acc: 36.5% (3648/10000)
[Test]  Epoch: 26	Loss: 0.047080	Acc: 36.5% (3654/10000)
[Test]  Epoch: 27	Loss: 0.046896	Acc: 36.7% (3669/10000)
[Test]  Epoch: 28	Loss: 0.047119	Acc: 36.6% (3661/10000)
[Test]  Epoch: 29	Loss: 0.046935	Acc: 36.4% (3638/10000)
[Test]  Epoch: 30	Loss: 0.047092	Acc: 36.4% (3644/10000)
[Test]  Epoch: 31	Loss: 0.047100	Acc: 36.6% (3662/10000)
[Test]  Epoch: 32	Loss: 0.046988	Acc: 36.6% (3656/10000)
[Test]  Epoch: 33	Loss: 0.047063	Acc: 36.7% (3674/10000)
[Test]  Epoch: 34	Loss: 0.047033	Acc: 36.6% (3662/10000)
[Test]  Epoch: 35	Loss: 0.046871	Acc: 36.7% (3668/10000)
[Test]  Epoch: 36	Loss: 0.046953	Acc: 36.9% (3689/10000)
[Test]  Epoch: 37	Loss: 0.047041	Acc: 37.0% (3696/10000)
[Test]  Epoch: 38	Loss: 0.047024	Acc: 36.8% (3683/10000)
[Test]  Epoch: 39	Loss: 0.047162	Acc: 36.8% (3682/10000)
[Test]  Epoch: 40	Loss: 0.047058	Acc: 37.0% (3696/10000)
[Test]  Epoch: 41	Loss: 0.047053	Acc: 37.0% (3700/10000)
[Test]  Epoch: 42	Loss: 0.047012	Acc: 37.0% (3703/10000)
[Test]  Epoch: 43	Loss: 0.046685	Acc: 37.4% (3742/10000)
[Test]  Epoch: 44	Loss: 0.047096	Acc: 37.2% (3725/10000)
[Test]  Epoch: 45	Loss: 0.046944	Acc: 37.2% (3723/10000)
[Test]  Epoch: 46	Loss: 0.047063	Acc: 37.1% (3713/10000)
[Test]  Epoch: 47	Loss: 0.046922	Acc: 37.5% (3754/10000)
[Test]  Epoch: 48	Loss: 0.047001	Acc: 37.5% (3748/10000)
[Test]  Epoch: 49	Loss: 0.047068	Acc: 37.5% (3747/10000)
[Test]  Epoch: 50	Loss: 0.046861	Acc: 37.6% (3761/10000)
[Test]  Epoch: 51	Loss: 0.046821	Acc: 37.8% (3775/10000)
[Test]  Epoch: 52	Loss: 0.046771	Acc: 37.8% (3776/10000)
[Test]  Epoch: 53	Loss: 0.046997	Acc: 37.4% (3742/10000)
[Test]  Epoch: 54	Loss: 0.047113	Acc: 37.1% (3714/10000)
[Test]  Epoch: 55	Loss: 0.046890	Acc: 37.7% (3766/10000)
[Test]  Epoch: 56	Loss: 0.046951	Acc: 37.6% (3756/10000)
[Test]  Epoch: 57	Loss: 0.046955	Acc: 37.5% (3746/10000)
[Test]  Epoch: 58	Loss: 0.046773	Acc: 37.6% (3764/10000)
[Test]  Epoch: 59	Loss: 0.046901	Acc: 37.6% (3756/10000)
[Test]  Epoch: 60	Loss: 0.046884	Acc: 37.5% (3754/10000)
[Test]  Epoch: 61	Loss: 0.046890	Acc: 37.7% (3768/10000)
[Test]  Epoch: 62	Loss: 0.046947	Acc: 37.8% (3778/10000)
[Test]  Epoch: 63	Loss: 0.046797	Acc: 37.9% (3792/10000)
[Test]  Epoch: 64	Loss: 0.046763	Acc: 38.0% (3801/10000)
[Test]  Epoch: 65	Loss: 0.046858	Acc: 37.7% (3774/10000)
[Test]  Epoch: 66	Loss: 0.046850	Acc: 37.8% (3784/10000)
[Test]  Epoch: 67	Loss: 0.046858	Acc: 37.8% (3784/10000)
[Test]  Epoch: 68	Loss: 0.046994	Acc: 37.5% (3745/10000)
[Test]  Epoch: 69	Loss: 0.046918	Acc: 37.7% (3774/10000)
[Test]  Epoch: 70	Loss: 0.046913	Acc: 37.7% (3772/10000)
[Test]  Epoch: 71	Loss: 0.046848	Acc: 37.8% (3775/10000)
[Test]  Epoch: 72	Loss: 0.046993	Acc: 37.7% (3774/10000)
[Test]  Epoch: 73	Loss: 0.046882	Acc: 37.7% (3773/10000)
[Test]  Epoch: 74	Loss: 0.046935	Acc: 37.6% (3765/10000)
[Test]  Epoch: 75	Loss: 0.046948	Acc: 37.8% (3783/10000)
[Test]  Epoch: 76	Loss: 0.046848	Acc: 38.2% (3820/10000)
[Test]  Epoch: 77	Loss: 0.046918	Acc: 37.8% (3782/10000)
[Test]  Epoch: 78	Loss: 0.046987	Acc: 37.6% (3756/10000)
[Test]  Epoch: 79	Loss: 0.047027	Acc: 37.7% (3768/10000)
[Test]  Epoch: 80	Loss: 0.046853	Acc: 38.0% (3795/10000)
[Test]  Epoch: 81	Loss: 0.046874	Acc: 37.8% (3781/10000)
[Test]  Epoch: 82	Loss: 0.046842	Acc: 37.7% (3771/10000)
[Test]  Epoch: 83	Loss: 0.046972	Acc: 37.8% (3776/10000)
[Test]  Epoch: 84	Loss: 0.047051	Acc: 37.6% (3760/10000)
[Test]  Epoch: 85	Loss: 0.046950	Acc: 37.8% (3775/10000)
[Test]  Epoch: 86	Loss: 0.046984	Acc: 37.8% (3783/10000)
[Test]  Epoch: 87	Loss: 0.046901	Acc: 37.8% (3784/10000)
[Test]  Epoch: 88	Loss: 0.046948	Acc: 38.0% (3802/10000)
[Test]  Epoch: 89	Loss: 0.046935	Acc: 37.6% (3762/10000)
[Test]  Epoch: 90	Loss: 0.047018	Acc: 37.7% (3770/10000)
[Test]  Epoch: 91	Loss: 0.046955	Acc: 37.8% (3780/10000)
[Test]  Epoch: 92	Loss: 0.046862	Acc: 38.0% (3804/10000)
[Test]  Epoch: 93	Loss: 0.046942	Acc: 38.0% (3800/10000)
[Test]  Epoch: 94	Loss: 0.047009	Acc: 37.9% (3785/10000)
[Test]  Epoch: 95	Loss: 0.046967	Acc: 37.7% (3771/10000)
[Test]  Epoch: 96	Loss: 0.046882	Acc: 38.0% (3802/10000)
[Test]  Epoch: 97	Loss: 0.046876	Acc: 37.8% (3784/10000)
[Test]  Epoch: 98	Loss: 0.046870	Acc: 37.9% (3793/10000)
[Test]  Epoch: 99	Loss: 0.046914	Acc: 37.7% (3772/10000)
[Test]  Epoch: 100	Loss: 0.046738	Acc: 37.8% (3783/10000)
===========finish==========
['2024-08-19', '03:33:50.413521', '100', 'test', '0.046738302958011624', '37.83', '38.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.0.1.weight', '_features.15.conv.1.1.weight', '_features.14.conv.3.weight', '_features.12.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.18.1.weight', '_features.15.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.16.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.14.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.17.conv.3.weight', '_features.8.conv.1.0.weight', '_features.12.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.4.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.3.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.1.conv.1.weight', '_features.2.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.3.conv.0.0.weight', '_features.5.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.137315	Acc: 2.5% (250/10000)
[Test]  Epoch: 2	Loss: 0.054546	Acc: 23.4% (2337/10000)
[Test]  Epoch: 3	Loss: 0.049925	Acc: 28.8% (2876/10000)
[Test]  Epoch: 4	Loss: 0.049097	Acc: 30.6% (3061/10000)
[Test]  Epoch: 5	Loss: 0.048927	Acc: 31.4% (3144/10000)
[Test]  Epoch: 6	Loss: 0.048121	Acc: 32.4% (3235/10000)
[Test]  Epoch: 7	Loss: 0.048211	Acc: 32.6% (3256/10000)
[Test]  Epoch: 8	Loss: 0.047957	Acc: 32.9% (3294/10000)
[Test]  Epoch: 9	Loss: 0.048157	Acc: 33.1% (3308/10000)
[Test]  Epoch: 10	Loss: 0.047935	Acc: 33.5% (3345/10000)
[Test]  Epoch: 11	Loss: 0.047900	Acc: 33.3% (3333/10000)
[Test]  Epoch: 12	Loss: 0.048129	Acc: 33.9% (3393/10000)
[Test]  Epoch: 13	Loss: 0.047802	Acc: 34.3% (3432/10000)
[Test]  Epoch: 14	Loss: 0.047716	Acc: 34.2% (3424/10000)
[Test]  Epoch: 15	Loss: 0.047725	Acc: 34.5% (3450/10000)
[Test]  Epoch: 16	Loss: 0.047602	Acc: 34.3% (3433/10000)
[Test]  Epoch: 17	Loss: 0.048116	Acc: 34.1% (3407/10000)
[Test]  Epoch: 18	Loss: 0.047522	Acc: 34.7% (3474/10000)
[Test]  Epoch: 19	Loss: 0.047530	Acc: 35.0% (3500/10000)
[Test]  Epoch: 20	Loss: 0.047803	Acc: 35.1% (3506/10000)
[Test]  Epoch: 21	Loss: 0.047519	Acc: 34.9% (3488/10000)
[Test]  Epoch: 22	Loss: 0.047934	Acc: 34.5% (3453/10000)
[Test]  Epoch: 23	Loss: 0.047604	Acc: 35.2% (3523/10000)
[Test]  Epoch: 24	Loss: 0.047602	Acc: 35.3% (3526/10000)
[Test]  Epoch: 25	Loss: 0.047521	Acc: 35.4% (3542/10000)
[Test]  Epoch: 26	Loss: 0.047698	Acc: 35.4% (3543/10000)
[Test]  Epoch: 27	Loss: 0.047411	Acc: 35.5% (3546/10000)
[Test]  Epoch: 28	Loss: 0.047698	Acc: 35.6% (3557/10000)
[Test]  Epoch: 29	Loss: 0.047504	Acc: 35.4% (3539/10000)
[Test]  Epoch: 30	Loss: 0.047743	Acc: 35.5% (3553/10000)
[Test]  Epoch: 31	Loss: 0.047689	Acc: 35.5% (3549/10000)
[Test]  Epoch: 32	Loss: 0.047534	Acc: 35.6% (3560/10000)
[Test]  Epoch: 33	Loss: 0.047689	Acc: 35.5% (3546/10000)
[Test]  Epoch: 34	Loss: 0.047519	Acc: 35.9% (3590/10000)
[Test]  Epoch: 35	Loss: 0.047511	Acc: 36.0% (3597/10000)
[Test]  Epoch: 36	Loss: 0.047534	Acc: 35.9% (3587/10000)
[Test]  Epoch: 37	Loss: 0.047566	Acc: 36.4% (3642/10000)
[Test]  Epoch: 38	Loss: 0.047509	Acc: 36.3% (3629/10000)
[Test]  Epoch: 39	Loss: 0.047609	Acc: 36.3% (3634/10000)
[Test]  Epoch: 40	Loss: 0.047497	Acc: 36.2% (3619/10000)
[Test]  Epoch: 41	Loss: 0.047593	Acc: 36.3% (3630/10000)
[Test]  Epoch: 42	Loss: 0.047314	Acc: 36.6% (3660/10000)
[Test]  Epoch: 43	Loss: 0.047171	Acc: 36.7% (3670/10000)
[Test]  Epoch: 44	Loss: 0.047563	Acc: 36.4% (3640/10000)
[Test]  Epoch: 45	Loss: 0.047492	Acc: 36.6% (3660/10000)
[Test]  Epoch: 46	Loss: 0.047515	Acc: 36.4% (3640/10000)
[Test]  Epoch: 47	Loss: 0.047506	Acc: 36.3% (3626/10000)
[Test]  Epoch: 48	Loss: 0.047890	Acc: 35.8% (3575/10000)
[Test]  Epoch: 49	Loss: 0.047517	Acc: 36.5% (3646/10000)
[Test]  Epoch: 50	Loss: 0.047444	Acc: 36.7% (3674/10000)
[Test]  Epoch: 51	Loss: 0.047416	Acc: 36.7% (3667/10000)
[Test]  Epoch: 52	Loss: 0.047394	Acc: 37.0% (3704/10000)
[Test]  Epoch: 53	Loss: 0.047605	Acc: 36.8% (3678/10000)
[Test]  Epoch: 54	Loss: 0.050526	Acc: 33.1% (3308/10000)
[Test]  Epoch: 55	Loss: 0.047554	Acc: 36.1% (3606/10000)
[Test]  Epoch: 56	Loss: 0.047805	Acc: 35.4% (3536/10000)
[Test]  Epoch: 57	Loss: 0.047707	Acc: 35.7% (3570/10000)
[Test]  Epoch: 58	Loss: 0.047473	Acc: 36.5% (3649/10000)
[Test]  Epoch: 59	Loss: 0.047526	Acc: 36.6% (3660/10000)
[Test]  Epoch: 60	Loss: 0.047469	Acc: 36.6% (3661/10000)
[Test]  Epoch: 61	Loss: 0.047487	Acc: 36.6% (3658/10000)
[Test]  Epoch: 62	Loss: 0.047565	Acc: 36.5% (3652/10000)
[Test]  Epoch: 63	Loss: 0.047416	Acc: 36.7% (3668/10000)
[Test]  Epoch: 64	Loss: 0.047364	Acc: 36.8% (3677/10000)
[Test]  Epoch: 65	Loss: 0.047461	Acc: 36.6% (3660/10000)
[Test]  Epoch: 66	Loss: 0.047451	Acc: 36.9% (3690/10000)
[Test]  Epoch: 67	Loss: 0.047463	Acc: 36.8% (3680/10000)
[Test]  Epoch: 68	Loss: 0.047604	Acc: 36.5% (3655/10000)
[Test]  Epoch: 69	Loss: 0.047535	Acc: 36.9% (3688/10000)
[Test]  Epoch: 70	Loss: 0.047547	Acc: 36.8% (3678/10000)
[Test]  Epoch: 71	Loss: 0.047416	Acc: 36.7% (3669/10000)
[Test]  Epoch: 72	Loss: 0.047578	Acc: 36.4% (3643/10000)
[Test]  Epoch: 73	Loss: 0.047429	Acc: 36.9% (3694/10000)
[Test]  Epoch: 74	Loss: 0.047489	Acc: 36.8% (3679/10000)
[Test]  Epoch: 75	Loss: 0.047580	Acc: 36.7% (3666/10000)
[Test]  Epoch: 76	Loss: 0.047462	Acc: 37.0% (3702/10000)
[Test]  Epoch: 77	Loss: 0.047553	Acc: 36.8% (3684/10000)
[Test]  Epoch: 78	Loss: 0.047534	Acc: 36.5% (3654/10000)
[Test]  Epoch: 79	Loss: 0.047597	Acc: 36.7% (3666/10000)
[Test]  Epoch: 80	Loss: 0.047415	Acc: 36.8% (3677/10000)
[Test]  Epoch: 81	Loss: 0.047423	Acc: 36.7% (3672/10000)
[Test]  Epoch: 82	Loss: 0.047470	Acc: 36.6% (3661/10000)
[Test]  Epoch: 83	Loss: 0.047573	Acc: 36.7% (3666/10000)
[Test]  Epoch: 84	Loss: 0.047600	Acc: 36.9% (3690/10000)
[Test]  Epoch: 85	Loss: 0.047539	Acc: 36.8% (3682/10000)
[Test]  Epoch: 86	Loss: 0.047551	Acc: 36.6% (3660/10000)
[Test]  Epoch: 87	Loss: 0.047507	Acc: 36.8% (3679/10000)
[Test]  Epoch: 88	Loss: 0.047564	Acc: 36.8% (3678/10000)
[Test]  Epoch: 89	Loss: 0.047498	Acc: 36.6% (3660/10000)
[Test]  Epoch: 90	Loss: 0.047566	Acc: 36.7% (3669/10000)
[Test]  Epoch: 91	Loss: 0.047565	Acc: 36.7% (3670/10000)
[Test]  Epoch: 92	Loss: 0.047421	Acc: 37.0% (3702/10000)
[Test]  Epoch: 93	Loss: 0.047539	Acc: 36.8% (3681/10000)
[Test]  Epoch: 94	Loss: 0.047639	Acc: 36.7% (3668/10000)
[Test]  Epoch: 95	Loss: 0.047509	Acc: 36.9% (3688/10000)
[Test]  Epoch: 96	Loss: 0.047386	Acc: 37.0% (3705/10000)
[Test]  Epoch: 97	Loss: 0.047427	Acc: 36.7% (3669/10000)
[Test]  Epoch: 98	Loss: 0.047470	Acc: 36.9% (3692/10000)
[Test]  Epoch: 99	Loss: 0.047437	Acc: 36.7% (3667/10000)
[Test]  Epoch: 100	Loss: 0.047391	Acc: 36.9% (3685/10000)
===========finish==========
['2024-08-19', '03:38:33.047254', '100', 'test', '0.04739060664176941', '36.85', '37.05']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.0.1.weight', '_features.15.conv.1.1.weight', '_features.14.conv.3.weight', '_features.12.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.18.1.weight', '_features.15.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.16.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.14.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.17.conv.3.weight', '_features.8.conv.1.0.weight', '_features.12.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.4.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.3.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.1.conv.1.weight', '_features.2.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.3.conv.0.0.weight', '_features.5.conv.0.0.weight', '_features.2.conv.0.0.weight', '_features.5.conv.2.weight', '_features.3.conv.2.weight', '_features.0.0.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.8.conv.0.0.weight', '_features.2.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.119015	Acc: 2.5% (252/10000)
[Test]  Epoch: 2	Loss: 0.053768	Acc: 24.4% (2438/10000)
[Test]  Epoch: 3	Loss: 0.050294	Acc: 28.6% (2860/10000)
[Test]  Epoch: 4	Loss: 0.049421	Acc: 30.4% (3040/10000)
[Test]  Epoch: 5	Loss: 0.048847	Acc: 31.3% (3130/10000)
[Test]  Epoch: 6	Loss: 0.048417	Acc: 32.4% (3239/10000)
[Test]  Epoch: 7	Loss: 0.048578	Acc: 32.2% (3222/10000)
[Test]  Epoch: 8	Loss: 0.048425	Acc: 33.0% (3301/10000)
[Test]  Epoch: 9	Loss: 0.048253	Acc: 33.2% (3320/10000)
[Test]  Epoch: 10	Loss: 0.048270	Acc: 33.1% (3313/10000)
[Test]  Epoch: 11	Loss: 0.048405	Acc: 33.2% (3322/10000)
[Test]  Epoch: 12	Loss: 0.048330	Acc: 33.8% (3378/10000)
[Test]  Epoch: 13	Loss: 0.048092	Acc: 33.8% (3382/10000)
[Test]  Epoch: 14	Loss: 0.048298	Acc: 33.9% (3385/10000)
[Test]  Epoch: 15	Loss: 0.048000	Acc: 34.1% (3409/10000)
[Test]  Epoch: 16	Loss: 0.047981	Acc: 34.1% (3412/10000)
[Test]  Epoch: 17	Loss: 0.048320	Acc: 34.1% (3407/10000)
[Test]  Epoch: 18	Loss: 0.047987	Acc: 34.4% (3437/10000)
[Test]  Epoch: 19	Loss: 0.047968	Acc: 34.5% (3447/10000)
[Test]  Epoch: 20	Loss: 0.048027	Acc: 34.8% (3475/10000)
[Test]  Epoch: 21	Loss: 0.047967	Acc: 34.7% (3466/10000)
[Test]  Epoch: 22	Loss: 0.048094	Acc: 34.7% (3467/10000)
[Test]  Epoch: 23	Loss: 0.048261	Acc: 34.6% (3462/10000)
[Test]  Epoch: 24	Loss: 0.048052	Acc: 34.7% (3471/10000)
[Test]  Epoch: 25	Loss: 0.047896	Acc: 35.2% (3525/10000)
[Test]  Epoch: 26	Loss: 0.048014	Acc: 35.0% (3499/10000)
[Test]  Epoch: 27	Loss: 0.047786	Acc: 35.0% (3502/10000)
[Test]  Epoch: 28	Loss: 0.048067	Acc: 35.0% (3496/10000)
[Test]  Epoch: 29	Loss: 0.047907	Acc: 34.8% (3481/10000)
[Test]  Epoch: 30	Loss: 0.047895	Acc: 35.2% (3524/10000)
[Test]  Epoch: 31	Loss: 0.048217	Acc: 35.1% (3512/10000)
[Test]  Epoch: 32	Loss: 0.047731	Acc: 35.4% (3544/10000)
[Test]  Epoch: 33	Loss: 0.047987	Acc: 35.5% (3551/10000)
[Test]  Epoch: 34	Loss: 0.047974	Acc: 35.5% (3553/10000)
[Test]  Epoch: 35	Loss: 0.047858	Acc: 35.6% (3561/10000)
[Test]  Epoch: 36	Loss: 0.047844	Acc: 35.6% (3565/10000)
[Test]  Epoch: 37	Loss: 0.047787	Acc: 35.9% (3588/10000)
[Test]  Epoch: 38	Loss: 0.047828	Acc: 36.0% (3595/10000)
[Test]  Epoch: 39	Loss: 0.048174	Acc: 35.4% (3535/10000)
[Test]  Epoch: 40	Loss: 0.048008	Acc: 35.6% (3557/10000)
[Test]  Epoch: 41	Loss: 0.047721	Acc: 36.1% (3610/10000)
[Test]  Epoch: 42	Loss: 0.047792	Acc: 35.9% (3589/10000)
[Test]  Epoch: 43	Loss: 0.047870	Acc: 36.0% (3596/10000)
[Test]  Epoch: 44	Loss: 0.047775	Acc: 35.8% (3582/10000)
[Test]  Epoch: 45	Loss: 0.047814	Acc: 35.9% (3594/10000)
[Test]  Epoch: 46	Loss: 0.047878	Acc: 35.7% (3569/10000)
[Test]  Epoch: 47	Loss: 0.048043	Acc: 36.0% (3596/10000)
[Test]  Epoch: 48	Loss: 0.048005	Acc: 36.0% (3597/10000)
[Test]  Epoch: 49	Loss: 0.047720	Acc: 36.6% (3658/10000)
[Test]  Epoch: 50	Loss: 0.047831	Acc: 36.2% (3621/10000)
[Test]  Epoch: 51	Loss: 0.047734	Acc: 36.2% (3621/10000)
[Test]  Epoch: 52	Loss: 0.047657	Acc: 36.4% (3640/10000)
[Test]  Epoch: 53	Loss: 0.047971	Acc: 36.0% (3602/10000)
[Test]  Epoch: 54	Loss: 0.047825	Acc: 36.3% (3627/10000)
[Test]  Epoch: 55	Loss: 0.047672	Acc: 36.4% (3643/10000)
[Test]  Epoch: 56	Loss: 0.047819	Acc: 35.9% (3590/10000)
[Test]  Epoch: 57	Loss: 0.047921	Acc: 36.1% (3615/10000)
[Test]  Epoch: 58	Loss: 0.047639	Acc: 36.4% (3641/10000)
[Test]  Epoch: 59	Loss: 0.047904	Acc: 36.4% (3636/10000)
[Test]  Epoch: 60	Loss: 0.048274	Acc: 35.6% (3560/10000)
[Test]  Epoch: 61	Loss: 0.048004	Acc: 35.9% (3592/10000)
[Test]  Epoch: 62	Loss: 0.047971	Acc: 36.1% (3610/10000)
[Test]  Epoch: 63	Loss: 0.047868	Acc: 36.4% (3643/10000)
[Test]  Epoch: 64	Loss: 0.047802	Acc: 36.5% (3648/10000)
[Test]  Epoch: 65	Loss: 0.047860	Acc: 36.2% (3623/10000)
[Test]  Epoch: 66	Loss: 0.047822	Acc: 36.3% (3626/10000)
[Test]  Epoch: 67	Loss: 0.047798	Acc: 36.6% (3658/10000)
[Test]  Epoch: 68	Loss: 0.047938	Acc: 36.3% (3626/10000)
[Test]  Epoch: 69	Loss: 0.047880	Acc: 36.4% (3636/10000)
[Test]  Epoch: 70	Loss: 0.047858	Acc: 36.3% (3627/10000)
[Test]  Epoch: 71	Loss: 0.047868	Acc: 36.4% (3639/10000)
[Test]  Epoch: 72	Loss: 0.047969	Acc: 36.5% (3652/10000)
[Test]  Epoch: 73	Loss: 0.047889	Acc: 36.3% (3634/10000)
[Test]  Epoch: 74	Loss: 0.047872	Acc: 36.5% (3648/10000)
[Test]  Epoch: 75	Loss: 0.047937	Acc: 36.1% (3614/10000)
[Test]  Epoch: 76	Loss: 0.047812	Acc: 36.6% (3661/10000)
[Test]  Epoch: 77	Loss: 0.047909	Acc: 36.5% (3655/10000)
[Test]  Epoch: 78	Loss: 0.047872	Acc: 36.0% (3605/10000)
[Test]  Epoch: 79	Loss: 0.047963	Acc: 36.2% (3616/10000)
[Test]  Epoch: 80	Loss: 0.047851	Acc: 36.4% (3639/10000)
[Test]  Epoch: 81	Loss: 0.047868	Acc: 36.3% (3632/10000)
[Test]  Epoch: 82	Loss: 0.047823	Acc: 36.6% (3663/10000)
[Test]  Epoch: 83	Loss: 0.047889	Acc: 36.5% (3645/10000)
[Test]  Epoch: 84	Loss: 0.047964	Acc: 36.6% (3659/10000)
[Test]  Epoch: 85	Loss: 0.047847	Acc: 36.5% (3650/10000)
[Test]  Epoch: 86	Loss: 0.047917	Acc: 36.3% (3633/10000)
[Test]  Epoch: 87	Loss: 0.047904	Acc: 36.5% (3650/10000)
[Test]  Epoch: 88	Loss: 0.047925	Acc: 36.6% (3659/10000)
[Test]  Epoch: 89	Loss: 0.047924	Acc: 36.5% (3646/10000)
[Test]  Epoch: 90	Loss: 0.047972	Acc: 36.4% (3641/10000)
[Test]  Epoch: 91	Loss: 0.047914	Acc: 36.4% (3642/10000)
[Test]  Epoch: 92	Loss: 0.047823	Acc: 36.6% (3660/10000)
[Test]  Epoch: 93	Loss: 0.047879	Acc: 36.4% (3644/10000)
[Test]  Epoch: 94	Loss: 0.047988	Acc: 36.5% (3649/10000)
[Test]  Epoch: 95	Loss: 0.047941	Acc: 36.6% (3656/10000)
[Test]  Epoch: 96	Loss: 0.047869	Acc: 36.3% (3634/10000)
[Test]  Epoch: 97	Loss: 0.047836	Acc: 36.2% (3622/10000)
[Test]  Epoch: 98	Loss: 0.047862	Acc: 36.5% (3653/10000)
[Test]  Epoch: 99	Loss: 0.047886	Acc: 36.2% (3622/10000)
[Test]  Epoch: 100	Loss: 0.047802	Acc: 36.4% (3639/10000)
===========finish==========
['2024-08-19', '03:43:08.941716', '100', 'test', '0.047801900112628935', '36.39', '36.63']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.0.1.weight', '_features.15.conv.1.1.weight', '_features.14.conv.3.weight', '_features.12.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.18.1.weight', '_features.15.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.16.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.14.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.17.conv.3.weight', '_features.8.conv.1.0.weight', '_features.12.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.4.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.3.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.1.conv.1.weight', '_features.2.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.3.conv.0.0.weight', '_features.5.conv.0.0.weight', '_features.2.conv.0.0.weight', '_features.5.conv.2.weight', '_features.3.conv.2.weight', '_features.0.0.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.8.conv.0.0.weight', '_features.2.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.10.conv.2.weight', '_features.4.conv.2.weight', '_features.7.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.12.conv.2.weight', '_features.7.conv.2.weight', '_features.13.conv.0.0.weight', '_features.13.conv.2.weight', '_features.15.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.178798	Acc: 2.4% (237/10000)
[Test]  Epoch: 2	Loss: 0.059460	Acc: 18.0% (1798/10000)
[Test]  Epoch: 3	Loss: 0.054973	Acc: 22.4% (2240/10000)
[Test]  Epoch: 4	Loss: 0.054429	Acc: 23.7% (2370/10000)
[Test]  Epoch: 5	Loss: 0.053899	Acc: 24.7% (2471/10000)
[Test]  Epoch: 6	Loss: 0.053439	Acc: 25.7% (2572/10000)
[Test]  Epoch: 7	Loss: 0.052701	Acc: 26.0% (2599/10000)
[Test]  Epoch: 8	Loss: 0.052703	Acc: 26.1% (2613/10000)
[Test]  Epoch: 9	Loss: 0.052506	Acc: 26.8% (2678/10000)
[Test]  Epoch: 10	Loss: 0.052198	Acc: 27.0% (2700/10000)
[Test]  Epoch: 11	Loss: 0.052259	Acc: 27.2% (2720/10000)
[Test]  Epoch: 12	Loss: 0.052257	Acc: 27.1% (2711/10000)
[Test]  Epoch: 13	Loss: 0.052290	Acc: 27.4% (2738/10000)
[Test]  Epoch: 14	Loss: 0.052000	Acc: 28.1% (2812/10000)
[Test]  Epoch: 15	Loss: 0.052127	Acc: 28.3% (2826/10000)
[Test]  Epoch: 16	Loss: 0.052170	Acc: 27.4% (2741/10000)
[Test]  Epoch: 17	Loss: 0.052103	Acc: 27.8% (2781/10000)
[Test]  Epoch: 18	Loss: 0.051495	Acc: 28.6% (2857/10000)
[Test]  Epoch: 19	Loss: 0.051645	Acc: 28.5% (2848/10000)
[Test]  Epoch: 20	Loss: 0.051692	Acc: 28.9% (2895/10000)
[Test]  Epoch: 21	Loss: 0.051603	Acc: 28.9% (2892/10000)
[Test]  Epoch: 22	Loss: 0.051872	Acc: 28.6% (2863/10000)
[Test]  Epoch: 23	Loss: 0.051580	Acc: 28.9% (2892/10000)
[Test]  Epoch: 24	Loss: 0.051559	Acc: 29.2% (2917/10000)
[Test]  Epoch: 25	Loss: 0.051369	Acc: 29.2% (2919/10000)
[Test]  Epoch: 26	Loss: 0.051457	Acc: 28.8% (2883/10000)
[Test]  Epoch: 27	Loss: 0.051239	Acc: 29.2% (2924/10000)
[Test]  Epoch: 28	Loss: 0.051397	Acc: 29.2% (2919/10000)
[Test]  Epoch: 29	Loss: 0.051062	Acc: 29.4% (2938/10000)
[Test]  Epoch: 30	Loss: 0.051406	Acc: 29.6% (2961/10000)
[Test]  Epoch: 31	Loss: 0.051460	Acc: 29.3% (2930/10000)
[Test]  Epoch: 32	Loss: 0.051446	Acc: 29.5% (2954/10000)
[Test]  Epoch: 33	Loss: 0.051324	Acc: 29.7% (2970/10000)
[Test]  Epoch: 34	Loss: 0.051322	Acc: 29.7% (2971/10000)
[Test]  Epoch: 35	Loss: 0.051273	Acc: 30.0% (3000/10000)
[Test]  Epoch: 36	Loss: 0.051312	Acc: 29.8% (2984/10000)
[Test]  Epoch: 37	Loss: 0.051105	Acc: 30.4% (3045/10000)
[Test]  Epoch: 38	Loss: 0.051183	Acc: 30.1% (3008/10000)
[Test]  Epoch: 39	Loss: 0.051384	Acc: 29.5% (2948/10000)
[Test]  Epoch: 40	Loss: 0.051127	Acc: 29.9% (2988/10000)
[Test]  Epoch: 41	Loss: 0.051218	Acc: 30.0% (2998/10000)
[Test]  Epoch: 42	Loss: 0.051202	Acc: 29.9% (2995/10000)
[Test]  Epoch: 43	Loss: 0.051001	Acc: 30.3% (3027/10000)
[Test]  Epoch: 44	Loss: 0.051283	Acc: 30.4% (3038/10000)
[Test]  Epoch: 45	Loss: 0.050876	Acc: 30.3% (3033/10000)
[Test]  Epoch: 46	Loss: 0.050910	Acc: 30.3% (3034/10000)
[Test]  Epoch: 47	Loss: 0.051186	Acc: 30.2% (3020/10000)
[Test]  Epoch: 48	Loss: 0.051172	Acc: 30.2% (3017/10000)
[Test]  Epoch: 49	Loss: 0.051277	Acc: 30.5% (3051/10000)
[Test]  Epoch: 50	Loss: 0.051121	Acc: 30.5% (3050/10000)
[Test]  Epoch: 51	Loss: 0.051121	Acc: 30.5% (3053/10000)
[Test]  Epoch: 52	Loss: 0.051080	Acc: 30.6% (3056/10000)
[Test]  Epoch: 53	Loss: 0.051117	Acc: 30.6% (3065/10000)
[Test]  Epoch: 54	Loss: 0.051101	Acc: 30.4% (3040/10000)
[Test]  Epoch: 55	Loss: 0.050964	Acc: 30.6% (3065/10000)
[Test]  Epoch: 56	Loss: 0.051176	Acc: 30.2% (3024/10000)
[Test]  Epoch: 57	Loss: 0.050985	Acc: 30.6% (3065/10000)
[Test]  Epoch: 58	Loss: 0.050948	Acc: 30.9% (3091/10000)
[Test]  Epoch: 59	Loss: 0.051015	Acc: 30.7% (3069/10000)
[Test]  Epoch: 60	Loss: 0.051012	Acc: 30.9% (3091/10000)
[Test]  Epoch: 61	Loss: 0.051013	Acc: 30.7% (3068/10000)
[Test]  Epoch: 62	Loss: 0.051058	Acc: 30.8% (3080/10000)
[Test]  Epoch: 63	Loss: 0.050912	Acc: 31.0% (3100/10000)
[Test]  Epoch: 64	Loss: 0.050885	Acc: 30.9% (3092/10000)
[Test]  Epoch: 65	Loss: 0.051036	Acc: 31.0% (3099/10000)
[Test]  Epoch: 66	Loss: 0.050948	Acc: 30.8% (3078/10000)
[Test]  Epoch: 67	Loss: 0.051011	Acc: 30.9% (3085/10000)
[Test]  Epoch: 68	Loss: 0.051160	Acc: 30.6% (3060/10000)
[Test]  Epoch: 69	Loss: 0.051034	Acc: 30.8% (3081/10000)
[Test]  Epoch: 70	Loss: 0.051062	Acc: 30.9% (3095/10000)
[Test]  Epoch: 71	Loss: 0.050971	Acc: 30.9% (3086/10000)
[Test]  Epoch: 72	Loss: 0.051071	Acc: 30.6% (3065/10000)
[Test]  Epoch: 73	Loss: 0.051038	Acc: 30.9% (3095/10000)
[Test]  Epoch: 74	Loss: 0.051016	Acc: 31.0% (3099/10000)
[Test]  Epoch: 75	Loss: 0.051094	Acc: 31.0% (3097/10000)
[Test]  Epoch: 76	Loss: 0.050996	Acc: 30.9% (3087/10000)
[Test]  Epoch: 77	Loss: 0.051048	Acc: 31.0% (3098/10000)
[Test]  Epoch: 78	Loss: 0.051030	Acc: 30.8% (3077/10000)
[Test]  Epoch: 79	Loss: 0.051110	Acc: 30.8% (3079/10000)
[Test]  Epoch: 80	Loss: 0.050935	Acc: 30.9% (3090/10000)
[Test]  Epoch: 81	Loss: 0.051069	Acc: 31.0% (3099/10000)
[Test]  Epoch: 82	Loss: 0.051039	Acc: 31.2% (3119/10000)
[Test]  Epoch: 83	Loss: 0.051054	Acc: 30.9% (3091/10000)
[Test]  Epoch: 84	Loss: 0.051052	Acc: 30.7% (3073/10000)
[Test]  Epoch: 85	Loss: 0.051117	Acc: 30.9% (3085/10000)
[Test]  Epoch: 86	Loss: 0.051124	Acc: 30.9% (3088/10000)
[Test]  Epoch: 87	Loss: 0.051062	Acc: 30.9% (3091/10000)
[Test]  Epoch: 88	Loss: 0.051016	Acc: 31.0% (3097/10000)
[Test]  Epoch: 89	Loss: 0.050972	Acc: 31.0% (3104/10000)
[Test]  Epoch: 90	Loss: 0.051048	Acc: 30.9% (3086/10000)
[Test]  Epoch: 91	Loss: 0.051078	Acc: 30.9% (3094/10000)
[Test]  Epoch: 92	Loss: 0.050994	Acc: 31.2% (3119/10000)
[Test]  Epoch: 93	Loss: 0.051083	Acc: 30.9% (3087/10000)
[Test]  Epoch: 94	Loss: 0.051093	Acc: 30.9% (3089/10000)
[Test]  Epoch: 95	Loss: 0.051044	Acc: 31.1% (3105/10000)
[Test]  Epoch: 96	Loss: 0.050893	Acc: 31.0% (3103/10000)
[Test]  Epoch: 97	Loss: 0.050940	Acc: 31.1% (3109/10000)
[Test]  Epoch: 98	Loss: 0.051013	Acc: 30.9% (3094/10000)
[Test]  Epoch: 99	Loss: 0.051006	Acc: 30.6% (3065/10000)
[Test]  Epoch: 100	Loss: 0.050978	Acc: 31.0% (3101/10000)
===========finish==========
['2024-08-19', '03:48:36.024817', '100', 'test', '0.05097805950641632', '31.01', '31.19']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight', '_features.9.conv.3.weight', '_features.4.conv.3.weight', '_features.8.conv.3.weight', '_features.7.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.15.conv.3.weight', '_features.12.conv.3.weight', '_features.5.conv.1.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.3.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.3.conv.1.1.weight', '_features.6.conv.0.1.weight', '_features.1.conv.0.1.weight', '_features.8.conv.0.1.weight', '_features.5.conv.0.1.weight', '_features.13.conv.3.weight', '_features.10.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.16.conv.3.weight', '_features.13.conv.1.1.weight', '_features.12.conv.1.1.weight', '_features.0.1.weight', '_features.15.conv.1.1.weight', '_features.14.conv.3.weight', '_features.12.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.18.1.weight', '_features.15.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.4.conv.0.1.weight', '_features.7.conv.1.1.weight', '_features.16.conv.0.1.weight', '_features.11.conv.1.1.weight', '_features.7.conv.0.1.weight', '_features.11.conv.0.1.weight', '_features.14.conv.1.1.weight', '_features.14.conv.0.1.weight', '_features.9.conv.1.0.weight', '_features.10.conv.1.0.weight', '_features.15.conv.1.0.weight', '_features.6.conv.1.0.weight', '_features.13.conv.1.0.weight', '_features.17.conv.3.weight', '_features.8.conv.1.0.weight', '_features.12.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.17.conv.1.1.weight', '_features.4.conv.1.0.weight', '_features.5.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.3.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.1.conv.1.weight', '_features.2.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.3.conv.0.0.weight', '_features.5.conv.0.0.weight', '_features.2.conv.0.0.weight', '_features.5.conv.2.weight', '_features.3.conv.2.weight', '_features.0.0.weight', '_features.9.conv.0.0.weight', '_features.9.conv.2.weight', '_features.10.conv.0.0.weight', '_features.8.conv.0.0.weight', '_features.2.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.10.conv.2.weight', '_features.4.conv.2.weight', '_features.7.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.12.conv.2.weight', '_features.7.conv.2.weight', '_features.13.conv.0.0.weight', '_features.13.conv.2.weight', '_features.15.conv.0.0.weight', '_features.11.conv.0.0.weight', '_features.15.conv.2.weight', '_features.11.conv.2.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.14.conv.0.0.weight', '_features.14.conv.2.weight', '_features.17.conv.0.0.weight', 'last_linear.weight', '_features.17.conv.2.weight', '_features.18.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.105464	Acc: 0.6% (58/10000)
[Test]  Epoch: 2	Loss: 0.082493	Acc: 2.2% (219/10000)
[Test]  Epoch: 3	Loss: 0.079792	Acc: 3.8% (375/10000)
[Test]  Epoch: 4	Loss: 0.077851	Acc: 5.1% (508/10000)
[Test]  Epoch: 5	Loss: 0.075985	Acc: 6.3% (629/10000)
[Test]  Epoch: 6	Loss: 0.074474	Acc: 6.9% (693/10000)
[Test]  Epoch: 7	Loss: 0.073152	Acc: 7.7% (769/10000)
[Test]  Epoch: 8	Loss: 0.071729	Acc: 8.4% (843/10000)
[Test]  Epoch: 9	Loss: 0.070562	Acc: 8.9% (895/10000)
[Test]  Epoch: 10	Loss: 0.069527	Acc: 9.8% (980/10000)
[Test]  Epoch: 11	Loss: 0.068798	Acc: 10.1% (1012/10000)
[Test]  Epoch: 12	Loss: 0.068029	Acc: 10.5% (1046/10000)
[Test]  Epoch: 13	Loss: 0.067386	Acc: 11.2% (1118/10000)
[Test]  Epoch: 14	Loss: 0.066777	Acc: 11.3% (1135/10000)
[Test]  Epoch: 15	Loss: 0.066246	Acc: 12.0% (1200/10000)
[Test]  Epoch: 16	Loss: 0.065680	Acc: 12.4% (1236/10000)
[Test]  Epoch: 17	Loss: 0.065287	Acc: 12.6% (1257/10000)
[Test]  Epoch: 18	Loss: 0.064884	Acc: 12.9% (1287/10000)
[Test]  Epoch: 19	Loss: 0.064518	Acc: 13.0% (1300/10000)
[Test]  Epoch: 20	Loss: 0.064212	Acc: 13.4% (1336/10000)
[Test]  Epoch: 21	Loss: 0.063858	Acc: 13.5% (1349/10000)
[Test]  Epoch: 22	Loss: 0.063631	Acc: 13.6% (1363/10000)
[Test]  Epoch: 23	Loss: 0.063367	Acc: 14.4% (1441/10000)
[Test]  Epoch: 24	Loss: 0.063144	Acc: 14.2% (1415/10000)
[Test]  Epoch: 25	Loss: 0.063016	Acc: 14.3% (1431/10000)
[Test]  Epoch: 26	Loss: 0.062751	Acc: 14.5% (1449/10000)
[Test]  Epoch: 27	Loss: 0.062599	Acc: 14.6% (1463/10000)
[Test]  Epoch: 28	Loss: 0.062546	Acc: 14.8% (1478/10000)
[Test]  Epoch: 29	Loss: 0.062201	Acc: 15.0% (1496/10000)
[Test]  Epoch: 30	Loss: 0.062051	Acc: 15.1% (1513/10000)
[Test]  Epoch: 31	Loss: 0.062046	Acc: 15.4% (1537/10000)
[Test]  Epoch: 32	Loss: 0.061765	Acc: 15.2% (1521/10000)
[Test]  Epoch: 33	Loss: 0.061703	Acc: 15.3% (1531/10000)
[Test]  Epoch: 34	Loss: 0.061555	Acc: 15.6% (1559/10000)
[Test]  Epoch: 35	Loss: 0.061383	Acc: 16.0% (1600/10000)
[Test]  Epoch: 36	Loss: 0.061333	Acc: 15.9% (1591/10000)
[Test]  Epoch: 37	Loss: 0.061222	Acc: 16.1% (1611/10000)
[Test]  Epoch: 38	Loss: 0.061192	Acc: 16.0% (1596/10000)
[Test]  Epoch: 39	Loss: 0.061092	Acc: 16.3% (1627/10000)
[Test]  Epoch: 40	Loss: 0.061005	Acc: 16.4% (1639/10000)
[Test]  Epoch: 41	Loss: 0.060943	Acc: 16.3% (1627/10000)
[Test]  Epoch: 42	Loss: 0.060841	Acc: 16.2% (1623/10000)
[Test]  Epoch: 43	Loss: 0.060756	Acc: 16.1% (1605/10000)
[Test]  Epoch: 44	Loss: 0.060577	Acc: 16.6% (1655/10000)
[Test]  Epoch: 45	Loss: 0.060581	Acc: 16.4% (1640/10000)
[Test]  Epoch: 46	Loss: 0.060518	Acc: 16.6% (1663/10000)
[Test]  Epoch: 47	Loss: 0.060381	Acc: 16.6% (1663/10000)
[Test]  Epoch: 48	Loss: 0.060335	Acc: 16.9% (1691/10000)
[Test]  Epoch: 49	Loss: 0.060377	Acc: 17.1% (1706/10000)
[Test]  Epoch: 50	Loss: 0.060435	Acc: 16.8% (1680/10000)
[Test]  Epoch: 51	Loss: 0.060257	Acc: 17.0% (1704/10000)
[Test]  Epoch: 52	Loss: 0.059995	Acc: 17.3% (1727/10000)
[Test]  Epoch: 53	Loss: 0.060028	Acc: 17.1% (1711/10000)
[Test]  Epoch: 54	Loss: 0.060054	Acc: 17.1% (1710/10000)
[Test]  Epoch: 55	Loss: 0.060330	Acc: 17.3% (1728/10000)
[Test]  Epoch: 56	Loss: 0.060073	Acc: 17.2% (1718/10000)
[Test]  Epoch: 57	Loss: 0.059962	Acc: 17.3% (1731/10000)
[Test]  Epoch: 58	Loss: 0.059718	Acc: 17.3% (1728/10000)
[Test]  Epoch: 59	Loss: 0.059858	Acc: 17.3% (1731/10000)
[Test]  Epoch: 60	Loss: 0.059963	Acc: 17.3% (1729/10000)
[Test]  Epoch: 61	Loss: 0.059891	Acc: 17.4% (1739/10000)
[Test]  Epoch: 62	Loss: 0.059917	Acc: 17.4% (1742/10000)
[Test]  Epoch: 63	Loss: 0.059737	Acc: 17.6% (1759/10000)
[Test]  Epoch: 64	Loss: 0.059749	Acc: 17.6% (1757/10000)
[Test]  Epoch: 65	Loss: 0.059769	Acc: 17.5% (1746/10000)
[Test]  Epoch: 66	Loss: 0.059749	Acc: 17.5% (1751/10000)
[Test]  Epoch: 67	Loss: 0.059872	Acc: 17.4% (1744/10000)
[Test]  Epoch: 68	Loss: 0.059867	Acc: 17.5% (1753/10000)
[Test]  Epoch: 69	Loss: 0.059713	Acc: 17.6% (1757/10000)
[Test]  Epoch: 70	Loss: 0.059727	Acc: 17.5% (1749/10000)
[Test]  Epoch: 71	Loss: 0.059727	Acc: 17.5% (1746/10000)
[Test]  Epoch: 72	Loss: 0.059812	Acc: 17.5% (1754/10000)
[Test]  Epoch: 73	Loss: 0.059706	Acc: 17.5% (1750/10000)
[Test]  Epoch: 74	Loss: 0.059708	Acc: 17.6% (1756/10000)
[Test]  Epoch: 75	Loss: 0.059715	Acc: 17.7% (1772/10000)
[Test]  Epoch: 76	Loss: 0.059679	Acc: 17.7% (1767/10000)
[Test]  Epoch: 77	Loss: 0.059713	Acc: 17.8% (1776/10000)
[Test]  Epoch: 78	Loss: 0.059691	Acc: 17.8% (1781/10000)
[Test]  Epoch: 79	Loss: 0.059752	Acc: 17.5% (1747/10000)
[Test]  Epoch: 80	Loss: 0.059681	Acc: 17.6% (1765/10000)
[Test]  Epoch: 81	Loss: 0.059632	Acc: 17.8% (1783/10000)
[Test]  Epoch: 82	Loss: 0.059710	Acc: 17.5% (1752/10000)
[Test]  Epoch: 83	Loss: 0.059713	Acc: 17.6% (1756/10000)
[Test]  Epoch: 84	Loss: 0.059790	Acc: 17.7% (1766/10000)
[Test]  Epoch: 85	Loss: 0.059719	Acc: 17.8% (1783/10000)
[Test]  Epoch: 86	Loss: 0.059743	Acc: 17.6% (1765/10000)
[Test]  Epoch: 87	Loss: 0.059709	Acc: 17.7% (1766/10000)
[Test]  Epoch: 88	Loss: 0.059699	Acc: 17.8% (1783/10000)
[Test]  Epoch: 89	Loss: 0.059580	Acc: 17.8% (1779/10000)
[Test]  Epoch: 90	Loss: 0.059624	Acc: 17.8% (1780/10000)
[Test]  Epoch: 91	Loss: 0.059697	Acc: 17.7% (1771/10000)
[Test]  Epoch: 92	Loss: 0.059624	Acc: 17.7% (1770/10000)
[Test]  Epoch: 93	Loss: 0.059665	Acc: 17.8% (1784/10000)
[Test]  Epoch: 94	Loss: 0.059722	Acc: 17.5% (1754/10000)
[Test]  Epoch: 95	Loss: 0.059730	Acc: 17.7% (1772/10000)
[Test]  Epoch: 96	Loss: 0.059582	Acc: 17.8% (1782/10000)
[Test]  Epoch: 97	Loss: 0.059633	Acc: 17.7% (1769/10000)
[Test]  Epoch: 98	Loss: 0.059712	Acc: 17.7% (1771/10000)
[Test]  Epoch: 99	Loss: 0.059636	Acc: 17.7% (1772/10000)
[Test]  Epoch: 100	Loss: 0.059616	Acc: 17.8% (1778/10000)
===========finish==========
['2024-08-19', '03:53:26.790329', '100', 'test', '0.05961570513248444', '17.78', '17.84']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-vgg16_bn/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('features.0.weight', nan), ('features.0.bias', 0.0), ('features.1.weight', nan), ('features.1.bias', 0.0), ('features.3.weight', nan), ('features.3.bias', 0.0), ('features.4.weight', nan), ('features.4.bias', 0.0), ('features.7.weight', nan), ('features.7.bias', 0.0), ('features.8.weight', nan), ('features.8.bias', 0.0), ('features.10.weight', nan), ('features.10.bias', 0.0), ('features.11.weight', nan), ('features.11.bias', 0.0), ('features.14.weight', nan), ('features.14.bias', 0.0), ('features.15.weight', nan), ('features.15.bias', 0.0), ('features.17.weight', nan), ('features.17.bias', 0.0), ('features.18.weight', nan), ('features.18.bias', 0.0), ('features.20.weight', nan), ('features.20.bias', 0.0), ('features.21.weight', nan), ('features.21.bias', 0.0), ('features.24.weight', nan), ('features.24.bias', 0.0), ('features.25.weight', nan), ('features.25.bias', 0.0), ('features.27.weight', nan), ('features.27.bias', 0.0), ('features.28.weight', nan), ('features.28.bias', 0.0), ('features.30.weight', nan), ('features.30.bias', 0.0), ('features.31.weight', nan), ('features.31.bias', 0.0), ('features.34.weight', nan), ('features.34.bias', 0.0), ('features.35.weight', nan), ('features.35.bias', 0.0), ('features.37.weight', nan), ('features.37.bias', 0.0), ('features.38.weight', nan), ('features.38.bias', 0.0), ('features.40.weight', nan), ('features.40.bias', 0.0), ('features.41.weight', nan), ('features.41.bias', 0.0), ('classifier.weight', nan), ('classifier.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('features.0.weight', nan), ('features.0.bias', 0.0), ('features.1.weight', nan), ('features.1.bias', 0.0), ('features.3.weight', nan), ('features.3.bias', 0.0), ('features.4.weight', nan), ('features.4.bias', 0.0), ('features.7.weight', nan), ('features.7.bias', 0.0), ('features.8.weight', nan), ('features.8.bias', 0.0), ('features.10.weight', nan), ('features.10.bias', 0.0), ('features.11.weight', nan), ('features.11.bias', 0.0), ('features.14.weight', nan), ('features.14.bias', 0.0), ('features.15.weight', nan), ('features.15.bias', 0.0), ('features.17.weight', nan), ('features.17.bias', 0.0), ('features.18.weight', nan), ('features.18.bias', 0.0), ('features.20.weight', nan), ('features.20.bias', 0.0), ('features.21.weight', nan), ('features.21.bias', 0.0), ('features.24.weight', nan), ('features.24.bias', 0.0), ('features.25.weight', nan), ('features.25.bias', 0.0), ('features.27.weight', nan), ('features.27.bias', 0.0), ('features.28.weight', nan), ('features.28.bias', 0.0), ('features.30.weight', nan), ('features.30.bias', 0.0), ('features.31.weight', nan), ('features.31.bias', 0.0), ('features.34.weight', nan), ('features.34.bias', 0.0), ('features.35.weight', nan), ('features.35.bias', 0.0), ('features.37.weight', nan), ('features.37.bias', 0.0), ('features.38.weight', nan), ('features.38.bias', 0.0), ('features.40.weight', nan), ('features.40.bias', 0.0), ('features.41.weight', nan), ('features.41.bias', 0.0), ('classifier.weight', nan), ('classifier.bias', 0.0)]
--------------------------------------------
get_sample_layers n=27  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042393	Acc: 38.2% (3825/10000)
[Test]  Epoch: 2	Loss: 0.043102	Acc: 36.7% (3668/10000)
[Test]  Epoch: 3	Loss: 0.043239	Acc: 36.4% (3638/10000)
[Test]  Epoch: 4	Loss: 0.043355	Acc: 36.4% (3644/10000)
[Test]  Epoch: 5	Loss: 0.043828	Acc: 36.2% (3624/10000)
[Test]  Epoch: 6	Loss: 0.044445	Acc: 35.1% (3513/10000)
[Test]  Epoch: 7	Loss: 0.044491	Acc: 35.6% (3556/10000)
[Test]  Epoch: 8	Loss: 0.044581	Acc: 35.3% (3529/10000)
[Test]  Epoch: 9	Loss: 0.044393	Acc: 35.8% (3580/10000)
[Test]  Epoch: 10	Loss: 0.044456	Acc: 35.6% (3560/10000)
[Test]  Epoch: 11	Loss: 0.044695	Acc: 35.2% (3516/10000)
[Test]  Epoch: 12	Loss: 0.044528	Acc: 35.4% (3544/10000)
[Test]  Epoch: 13	Loss: 0.045180	Acc: 34.8% (3478/10000)
[Test]  Epoch: 14	Loss: 0.044870	Acc: 34.8% (3478/10000)
[Test]  Epoch: 15	Loss: 0.044925	Acc: 35.1% (3510/10000)
[Test]  Epoch: 16	Loss: 0.045326	Acc: 33.9% (3391/10000)
[Test]  Epoch: 17	Loss: 0.045169	Acc: 34.7% (3469/10000)
[Test]  Epoch: 18	Loss: 0.045258	Acc: 34.6% (3463/10000)
[Test]  Epoch: 19	Loss: 0.045302	Acc: 33.7% (3369/10000)
[Test]  Epoch: 20	Loss: 0.045401	Acc: 34.6% (3457/10000)
[Test]  Epoch: 21	Loss: 0.045285	Acc: 34.7% (3466/10000)
[Test]  Epoch: 22	Loss: 0.045366	Acc: 34.2% (3419/10000)
[Test]  Epoch: 23	Loss: 0.045621	Acc: 34.1% (3412/10000)
[Test]  Epoch: 24	Loss: 0.045457	Acc: 34.4% (3442/10000)
[Test]  Epoch: 25	Loss: 0.045168	Acc: 34.0% (3403/10000)
[Test]  Epoch: 26	Loss: 0.045627	Acc: 34.1% (3408/10000)
[Test]  Epoch: 27	Loss: 0.045354	Acc: 33.9% (3388/10000)
[Test]  Epoch: 28	Loss: 0.045485	Acc: 34.6% (3456/10000)
[Test]  Epoch: 29	Loss: 0.045636	Acc: 34.3% (3433/10000)
[Test]  Epoch: 30	Loss: 0.046133	Acc: 33.9% (3385/10000)
[Test]  Epoch: 31	Loss: 0.046253	Acc: 33.1% (3313/10000)
[Test]  Epoch: 32	Loss: 0.045769	Acc: 34.3% (3429/10000)
[Test]  Epoch: 33	Loss: 0.046024	Acc: 33.9% (3390/10000)
[Test]  Epoch: 34	Loss: 0.045873	Acc: 33.6% (3362/10000)
[Test]  Epoch: 35	Loss: 0.046180	Acc: 34.0% (3405/10000)
[Test]  Epoch: 36	Loss: 0.046394	Acc: 32.8% (3282/10000)
[Test]  Epoch: 37	Loss: 0.046389	Acc: 33.1% (3311/10000)
[Test]  Epoch: 38	Loss: 0.046291	Acc: 33.2% (3320/10000)
[Test]  Epoch: 39	Loss: 0.046268	Acc: 32.9% (3293/10000)
[Test]  Epoch: 40	Loss: 0.046518	Acc: 33.0% (3302/10000)
[Test]  Epoch: 41	Loss: 0.046489	Acc: 32.7% (3273/10000)
[Test]  Epoch: 42	Loss: 0.046107	Acc: 33.0% (3297/10000)
[Test]  Epoch: 43	Loss: 0.046392	Acc: 32.6% (3263/10000)
[Test]  Epoch: 44	Loss: 0.046147	Acc: 33.4% (3343/10000)
[Test]  Epoch: 45	Loss: 0.046204	Acc: 33.2% (3320/10000)
[Test]  Epoch: 46	Loss: 0.046636	Acc: 33.4% (3343/10000)
[Test]  Epoch: 47	Loss: 0.046717	Acc: 32.2% (3221/10000)
[Test]  Epoch: 48	Loss: 0.046817	Acc: 32.5% (3253/10000)
[Test]  Epoch: 49	Loss: 0.046744	Acc: 32.9% (3287/10000)
[Test]  Epoch: 50	Loss: 0.046269	Acc: 33.0% (3305/10000)
[Test]  Epoch: 51	Loss: 0.046640	Acc: 32.7% (3268/10000)
[Test]  Epoch: 52	Loss: 0.046908	Acc: 32.5% (3251/10000)
[Test]  Epoch: 53	Loss: 0.046838	Acc: 32.2% (3217/10000)
[Test]  Epoch: 54	Loss: 0.046620	Acc: 32.5% (3252/10000)
[Test]  Epoch: 55	Loss: 0.046584	Acc: 33.2% (3322/10000)
[Test]  Epoch: 56	Loss: 0.047365	Acc: 31.9% (3188/10000)
[Test]  Epoch: 57	Loss: 0.046890	Acc: 31.9% (3195/10000)
[Test]  Epoch: 58	Loss: 0.047031	Acc: 31.8% (3183/10000)
[Test]  Epoch: 59	Loss: 0.047044	Acc: 31.6% (3159/10000)
[Test]  Epoch: 60	Loss: 0.047369	Acc: 32.5% (3255/10000)
[Test]  Epoch: 61	Loss: 0.047229	Acc: 32.2% (3225/10000)
[Test]  Epoch: 62	Loss: 0.046975	Acc: 32.1% (3213/10000)
[Test]  Epoch: 63	Loss: 0.046979	Acc: 32.1% (3209/10000)
[Test]  Epoch: 64	Loss: 0.046857	Acc: 32.3% (3230/10000)
[Test]  Epoch: 65	Loss: 0.046764	Acc: 33.1% (3312/10000)
[Test]  Epoch: 66	Loss: 0.047053	Acc: 32.5% (3250/10000)
[Test]  Epoch: 67	Loss: 0.047062	Acc: 31.9% (3192/10000)
[Test]  Epoch: 68	Loss: 0.047002	Acc: 32.1% (3215/10000)
[Test]  Epoch: 69	Loss: 0.047265	Acc: 32.1% (3213/10000)
[Test]  Epoch: 70	Loss: 0.046614	Acc: 32.3% (3234/10000)
[Test]  Epoch: 71	Loss: 0.046933	Acc: 32.2% (3225/10000)
[Test]  Epoch: 72	Loss: 0.046553	Acc: 33.1% (3311/10000)
[Test]  Epoch: 73	Loss: 0.046831	Acc: 32.7% (3271/10000)
[Test]  Epoch: 74	Loss: 0.047062	Acc: 32.6% (3265/10000)
[Test]  Epoch: 75	Loss: 0.046865	Acc: 32.6% (3258/10000)
[Test]  Epoch: 76	Loss: 0.047126	Acc: 32.8% (3284/10000)
[Test]  Epoch: 77	Loss: 0.046838	Acc: 32.8% (3279/10000)
[Test]  Epoch: 78	Loss: 0.046966	Acc: 32.4% (3243/10000)
[Test]  Epoch: 79	Loss: 0.046965	Acc: 32.2% (3225/10000)
[Test]  Epoch: 80	Loss: 0.046408	Acc: 33.1% (3307/10000)
[Test]  Epoch: 81	Loss: 0.047090	Acc: 32.2% (3216/10000)
[Test]  Epoch: 82	Loss: 0.046880	Acc: 32.6% (3262/10000)
[Test]  Epoch: 83	Loss: 0.046860	Acc: 32.7% (3273/10000)
[Test]  Epoch: 84	Loss: 0.046788	Acc: 32.7% (3267/10000)
[Test]  Epoch: 85	Loss: 0.046801	Acc: 32.5% (3250/10000)
[Test]  Epoch: 86	Loss: 0.046929	Acc: 32.0% (3200/10000)
[Test]  Epoch: 87	Loss: 0.046774	Acc: 32.6% (3261/10000)
[Test]  Epoch: 88	Loss: 0.046745	Acc: 32.4% (3243/10000)
[Test]  Epoch: 89	Loss: 0.046805	Acc: 32.7% (3272/10000)
[Test]  Epoch: 90	Loss: 0.046761	Acc: 32.5% (3246/10000)
[Test]  Epoch: 91	Loss: 0.046943	Acc: 32.7% (3269/10000)
[Test]  Epoch: 92	Loss: 0.046721	Acc: 32.6% (3260/10000)
[Test]  Epoch: 93	Loss: 0.046969	Acc: 32.3% (3230/10000)
[Test]  Epoch: 94	Loss: 0.046615	Acc: 33.0% (3298/10000)
[Test]  Epoch: 95	Loss: 0.046761	Acc: 32.5% (3251/10000)
[Test]  Epoch: 96	Loss: 0.046842	Acc: 32.4% (3237/10000)
[Test]  Epoch: 97	Loss: 0.046412	Acc: 33.4% (3341/10000)
[Test]  Epoch: 98	Loss: 0.047148	Acc: 31.9% (3185/10000)
[Test]  Epoch: 99	Loss: 0.046919	Acc: 32.9% (3285/10000)
[Test]  Epoch: 100	Loss: 0.046846	Acc: 31.9% (3192/10000)
===========finish==========
['2024-08-19', '04:03:05.011866', '100', 'test', '0.04684568165540695', '31.92', '38.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.0.weight', 'features.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.075954	Acc: 10.1% (1014/10000)
[Test]  Epoch: 2	Loss: 0.058376	Acc: 22.1% (2211/10000)
[Test]  Epoch: 3	Loss: 0.056064	Acc: 22.8% (2281/10000)
[Test]  Epoch: 4	Loss: 0.052831	Acc: 26.3% (2633/10000)
[Test]  Epoch: 5	Loss: 0.052569	Acc: 26.1% (2607/10000)
[Test]  Epoch: 6	Loss: 0.051237	Acc: 27.6% (2765/10000)
[Test]  Epoch: 7	Loss: 0.050565	Acc: 28.4% (2839/10000)
[Test]  Epoch: 8	Loss: 0.050991	Acc: 27.7% (2773/10000)
[Test]  Epoch: 9	Loss: 0.050168	Acc: 28.6% (2857/10000)
[Test]  Epoch: 10	Loss: 0.049981	Acc: 28.9% (2889/10000)
[Test]  Epoch: 11	Loss: 0.049312	Acc: 29.6% (2956/10000)
[Test]  Epoch: 12	Loss: 0.049253	Acc: 29.8% (2981/10000)
[Test]  Epoch: 13	Loss: 0.050084	Acc: 28.7% (2866/10000)
[Test]  Epoch: 14	Loss: 0.049398	Acc: 29.7% (2966/10000)
[Test]  Epoch: 15	Loss: 0.049340	Acc: 29.9% (2985/10000)
[Test]  Epoch: 16	Loss: 0.049218	Acc: 29.4% (2945/10000)
[Test]  Epoch: 17	Loss: 0.049244	Acc: 29.9% (2993/10000)
[Test]  Epoch: 18	Loss: 0.049180	Acc: 29.9% (2991/10000)
[Test]  Epoch: 19	Loss: 0.049332	Acc: 29.4% (2935/10000)
[Test]  Epoch: 20	Loss: 0.049053	Acc: 30.4% (3037/10000)
[Test]  Epoch: 21	Loss: 0.048675	Acc: 29.9% (2994/10000)
[Test]  Epoch: 22	Loss: 0.049118	Acc: 30.1% (3014/10000)
[Test]  Epoch: 23	Loss: 0.048808	Acc: 30.1% (3009/10000)
[Test]  Epoch: 24	Loss: 0.049243	Acc: 29.4% (2937/10000)
[Test]  Epoch: 25	Loss: 0.048799	Acc: 30.1% (3015/10000)
[Test]  Epoch: 26	Loss: 0.048852	Acc: 30.1% (3011/10000)
[Test]  Epoch: 27	Loss: 0.048556	Acc: 29.9% (2988/10000)
[Test]  Epoch: 28	Loss: 0.048872	Acc: 29.9% (2994/10000)
[Test]  Epoch: 29	Loss: 0.048977	Acc: 30.1% (3006/10000)
[Test]  Epoch: 30	Loss: 0.049050	Acc: 30.0% (3000/10000)
[Test]  Epoch: 31	Loss: 0.049217	Acc: 29.7% (2970/10000)
[Test]  Epoch: 32	Loss: 0.049202	Acc: 29.9% (2988/10000)
[Test]  Epoch: 33	Loss: 0.049100	Acc: 29.9% (2986/10000)
[Test]  Epoch: 34	Loss: 0.049159	Acc: 29.4% (2938/10000)
[Test]  Epoch: 35	Loss: 0.049504	Acc: 29.2% (2924/10000)
[Test]  Epoch: 36	Loss: 0.049506	Acc: 28.7% (2874/10000)
[Test]  Epoch: 37	Loss: 0.049458	Acc: 29.3% (2927/10000)
[Test]  Epoch: 38	Loss: 0.049478	Acc: 29.4% (2939/10000)
[Test]  Epoch: 39	Loss: 0.049561	Acc: 29.1% (2912/10000)
[Test]  Epoch: 40	Loss: 0.049237	Acc: 29.3% (2927/10000)
[Test]  Epoch: 41	Loss: 0.048979	Acc: 29.7% (2974/10000)
[Test]  Epoch: 42	Loss: 0.048851	Acc: 30.5% (3048/10000)
[Test]  Epoch: 43	Loss: 0.049312	Acc: 29.2% (2916/10000)
[Test]  Epoch: 44	Loss: 0.048833	Acc: 29.9% (2992/10000)
[Test]  Epoch: 45	Loss: 0.049184	Acc: 29.8% (2980/10000)
[Test]  Epoch: 46	Loss: 0.050011	Acc: 28.6% (2865/10000)
[Test]  Epoch: 47	Loss: 0.049339	Acc: 29.3% (2934/10000)
[Test]  Epoch: 48	Loss: 0.049699	Acc: 28.8% (2884/10000)
[Test]  Epoch: 49	Loss: 0.050210	Acc: 29.0% (2898/10000)
[Test]  Epoch: 50	Loss: 0.048704	Acc: 30.3% (3027/10000)
[Test]  Epoch: 51	Loss: 0.049485	Acc: 29.2% (2925/10000)
[Test]  Epoch: 52	Loss: 0.049588	Acc: 29.2% (2920/10000)
[Test]  Epoch: 53	Loss: 0.049164	Acc: 29.5% (2952/10000)
[Test]  Epoch: 54	Loss: 0.049120	Acc: 29.4% (2937/10000)
[Test]  Epoch: 55	Loss: 0.049317	Acc: 29.8% (2978/10000)
[Test]  Epoch: 56	Loss: 0.049869	Acc: 28.8% (2876/10000)
[Test]  Epoch: 57	Loss: 0.049446	Acc: 29.4% (2938/10000)
[Test]  Epoch: 58	Loss: 0.049765	Acc: 29.4% (2938/10000)
[Test]  Epoch: 59	Loss: 0.049458	Acc: 28.7% (2872/10000)
[Test]  Epoch: 60	Loss: 0.049949	Acc: 29.0% (2898/10000)
[Test]  Epoch: 61	Loss: 0.049569	Acc: 29.2% (2919/10000)
[Test]  Epoch: 62	Loss: 0.049308	Acc: 29.6% (2962/10000)
[Test]  Epoch: 63	Loss: 0.049359	Acc: 29.8% (2976/10000)
[Test]  Epoch: 64	Loss: 0.049039	Acc: 29.6% (2963/10000)
[Test]  Epoch: 65	Loss: 0.049026	Acc: 29.8% (2982/10000)
[Test]  Epoch: 66	Loss: 0.049251	Acc: 29.7% (2967/10000)
[Test]  Epoch: 67	Loss: 0.049334	Acc: 29.2% (2925/10000)
[Test]  Epoch: 68	Loss: 0.049248	Acc: 29.2% (2916/10000)
[Test]  Epoch: 69	Loss: 0.049428	Acc: 29.0% (2901/10000)
[Test]  Epoch: 70	Loss: 0.049007	Acc: 29.7% (2973/10000)
[Test]  Epoch: 71	Loss: 0.049205	Acc: 29.9% (2994/10000)
[Test]  Epoch: 72	Loss: 0.048924	Acc: 30.7% (3069/10000)
[Test]  Epoch: 73	Loss: 0.049127	Acc: 29.7% (2971/10000)
[Test]  Epoch: 74	Loss: 0.049147	Acc: 30.0% (3004/10000)
[Test]  Epoch: 75	Loss: 0.049086	Acc: 30.1% (3008/10000)
[Test]  Epoch: 76	Loss: 0.049289	Acc: 30.1% (3013/10000)
[Test]  Epoch: 77	Loss: 0.048929	Acc: 30.3% (3034/10000)
[Test]  Epoch: 78	Loss: 0.049260	Acc: 29.3% (2928/10000)
[Test]  Epoch: 79	Loss: 0.049167	Acc: 29.5% (2954/10000)
[Test]  Epoch: 80	Loss: 0.048745	Acc: 30.3% (3029/10000)
[Test]  Epoch: 81	Loss: 0.049338	Acc: 29.6% (2955/10000)
[Test]  Epoch: 82	Loss: 0.049004	Acc: 30.5% (3049/10000)
[Test]  Epoch: 83	Loss: 0.049021	Acc: 30.2% (3018/10000)
[Test]  Epoch: 84	Loss: 0.049105	Acc: 29.8% (2979/10000)
[Test]  Epoch: 85	Loss: 0.049073	Acc: 29.5% (2952/10000)
[Test]  Epoch: 86	Loss: 0.049123	Acc: 29.1% (2908/10000)
[Test]  Epoch: 87	Loss: 0.048841	Acc: 30.5% (3052/10000)
[Test]  Epoch: 88	Loss: 0.048803	Acc: 30.1% (3010/10000)
[Test]  Epoch: 89	Loss: 0.048976	Acc: 29.8% (2982/10000)
[Test]  Epoch: 90	Loss: 0.048935	Acc: 30.2% (3022/10000)
[Test]  Epoch: 91	Loss: 0.049058	Acc: 30.1% (3014/10000)
[Test]  Epoch: 92	Loss: 0.049002	Acc: 29.8% (2984/10000)
[Test]  Epoch: 93	Loss: 0.049266	Acc: 29.5% (2950/10000)
[Test]  Epoch: 94	Loss: 0.048843	Acc: 30.1% (3011/10000)
[Test]  Epoch: 95	Loss: 0.048991	Acc: 30.2% (3025/10000)
[Test]  Epoch: 96	Loss: 0.049058	Acc: 29.8% (2981/10000)
[Test]  Epoch: 97	Loss: 0.048682	Acc: 30.1% (3008/10000)
[Test]  Epoch: 98	Loss: 0.049264	Acc: 29.8% (2978/10000)
[Test]  Epoch: 99	Loss: 0.049201	Acc: 29.5% (2952/10000)
[Test]  Epoch: 100	Loss: 0.049102	Acc: 29.0% (2902/10000)
===========finish==========
['2024-08-19', '04:08:02.243612', '100', 'test', '0.04910189076662064', '29.02', '30.69']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.104904	Acc: 1.5% (153/10000)
[Test]  Epoch: 2	Loss: 0.084471	Acc: 3.7% (368/10000)
[Test]  Epoch: 3	Loss: 0.076414	Acc: 5.0% (497/10000)
[Test]  Epoch: 4	Loss: 0.076183	Acc: 6.1% (612/10000)
[Test]  Epoch: 5	Loss: 0.076153	Acc: 6.3% (632/10000)
[Test]  Epoch: 6	Loss: 0.073768	Acc: 6.8% (681/10000)
[Test]  Epoch: 7	Loss: 0.073543	Acc: 6.8% (682/10000)
[Test]  Epoch: 8	Loss: 0.071619	Acc: 8.2% (821/10000)
[Test]  Epoch: 9	Loss: 0.071440	Acc: 8.3% (830/10000)
[Test]  Epoch: 10	Loss: 0.070834	Acc: 8.4% (843/10000)
[Test]  Epoch: 11	Loss: 0.069996	Acc: 9.6% (957/10000)
[Test]  Epoch: 12	Loss: 0.068949	Acc: 10.5% (1053/10000)
[Test]  Epoch: 13	Loss: 0.069785	Acc: 9.6% (960/10000)
[Test]  Epoch: 14	Loss: 0.069175	Acc: 10.2% (1021/10000)
[Test]  Epoch: 15	Loss: 0.068221	Acc: 11.1% (1106/10000)
[Test]  Epoch: 16	Loss: 0.068721	Acc: 11.1% (1114/10000)
[Test]  Epoch: 17	Loss: 0.067071	Acc: 12.2% (1224/10000)
[Test]  Epoch: 18	Loss: 0.066578	Acc: 12.4% (1242/10000)
[Test]  Epoch: 19	Loss: 0.067118	Acc: 12.5% (1247/10000)
[Test]  Epoch: 20	Loss: 0.065111	Acc: 13.7% (1370/10000)
[Test]  Epoch: 21	Loss: 0.065522	Acc: 13.7% (1371/10000)
[Test]  Epoch: 22	Loss: 0.064716	Acc: 14.4% (1445/10000)
[Test]  Epoch: 23	Loss: 0.063544	Acc: 14.8% (1475/10000)
[Test]  Epoch: 24	Loss: 0.063794	Acc: 15.3% (1528/10000)
[Test]  Epoch: 25	Loss: 0.063448	Acc: 15.2% (1522/10000)
[Test]  Epoch: 26	Loss: 0.063429	Acc: 15.2% (1521/10000)
[Test]  Epoch: 27	Loss: 0.063665	Acc: 15.3% (1534/10000)
[Test]  Epoch: 28	Loss: 0.062672	Acc: 15.6% (1563/10000)
[Test]  Epoch: 29	Loss: 0.062746	Acc: 15.7% (1574/10000)
[Test]  Epoch: 30	Loss: 0.062360	Acc: 16.2% (1624/10000)
[Test]  Epoch: 31	Loss: 0.062157	Acc: 16.1% (1609/10000)
[Test]  Epoch: 32	Loss: 0.062887	Acc: 15.4% (1544/10000)
[Test]  Epoch: 33	Loss: 0.062570	Acc: 16.1% (1611/10000)
[Test]  Epoch: 34	Loss: 0.061748	Acc: 16.8% (1677/10000)
[Test]  Epoch: 35	Loss: 0.062698	Acc: 15.7% (1565/10000)
[Test]  Epoch: 36	Loss: 0.061840	Acc: 16.5% (1652/10000)
[Test]  Epoch: 37	Loss: 0.061199	Acc: 17.3% (1728/10000)
[Test]  Epoch: 38	Loss: 0.062463	Acc: 16.1% (1613/10000)
[Test]  Epoch: 39	Loss: 0.061108	Acc: 17.6% (1757/10000)
[Test]  Epoch: 40	Loss: 0.061641	Acc: 16.8% (1675/10000)
[Test]  Epoch: 41	Loss: 0.060908	Acc: 17.2% (1718/10000)
[Test]  Epoch: 42	Loss: 0.060416	Acc: 18.1% (1811/10000)
[Test]  Epoch: 43	Loss: 0.060738	Acc: 18.1% (1808/10000)
[Test]  Epoch: 44	Loss: 0.059985	Acc: 18.1% (1814/10000)
[Test]  Epoch: 45	Loss: 0.060855	Acc: 17.6% (1764/10000)
[Test]  Epoch: 46	Loss: 0.061573	Acc: 17.2% (1725/10000)
[Test]  Epoch: 47	Loss: 0.060397	Acc: 17.9% (1787/10000)
[Test]  Epoch: 48	Loss: 0.060550	Acc: 18.3% (1828/10000)
[Test]  Epoch: 49	Loss: 0.060048	Acc: 18.2% (1822/10000)
[Test]  Epoch: 50	Loss: 0.059509	Acc: 18.8% (1875/10000)
[Test]  Epoch: 51	Loss: 0.060207	Acc: 17.9% (1791/10000)
[Test]  Epoch: 52	Loss: 0.060493	Acc: 17.7% (1766/10000)
[Test]  Epoch: 53	Loss: 0.060522	Acc: 17.5% (1746/10000)
[Test]  Epoch: 54	Loss: 0.059904	Acc: 17.8% (1776/10000)
[Test]  Epoch: 55	Loss: 0.059601	Acc: 18.1% (1815/10000)
[Test]  Epoch: 56	Loss: 0.060641	Acc: 18.1% (1806/10000)
[Test]  Epoch: 57	Loss: 0.059916	Acc: 18.3% (1832/10000)
[Test]  Epoch: 58	Loss: 0.060205	Acc: 18.3% (1833/10000)
[Test]  Epoch: 59	Loss: 0.060089	Acc: 18.2% (1816/10000)
[Test]  Epoch: 60	Loss: 0.060563	Acc: 18.0% (1800/10000)
[Test]  Epoch: 61	Loss: 0.059149	Acc: 19.2% (1922/10000)
[Test]  Epoch: 62	Loss: 0.058745	Acc: 19.1% (1915/10000)
[Test]  Epoch: 63	Loss: 0.059117	Acc: 18.8% (1881/10000)
[Test]  Epoch: 64	Loss: 0.058834	Acc: 19.3% (1926/10000)
[Test]  Epoch: 65	Loss: 0.058865	Acc: 18.8% (1877/10000)
[Test]  Epoch: 66	Loss: 0.058928	Acc: 19.1% (1913/10000)
[Test]  Epoch: 67	Loss: 0.059034	Acc: 19.3% (1934/10000)
[Test]  Epoch: 68	Loss: 0.058860	Acc: 19.0% (1902/10000)
[Test]  Epoch: 69	Loss: 0.058950	Acc: 19.5% (1953/10000)
[Test]  Epoch: 70	Loss: 0.058806	Acc: 19.1% (1905/10000)
[Test]  Epoch: 71	Loss: 0.058537	Acc: 19.2% (1919/10000)
[Test]  Epoch: 72	Loss: 0.058414	Acc: 20.1% (2010/10000)
[Test]  Epoch: 73	Loss: 0.058574	Acc: 19.4% (1945/10000)
[Test]  Epoch: 74	Loss: 0.058869	Acc: 19.3% (1931/10000)
[Test]  Epoch: 75	Loss: 0.058596	Acc: 19.4% (1939/10000)
[Test]  Epoch: 76	Loss: 0.058676	Acc: 19.5% (1950/10000)
[Test]  Epoch: 77	Loss: 0.058583	Acc: 20.1% (2009/10000)
[Test]  Epoch: 78	Loss: 0.058837	Acc: 19.4% (1935/10000)
[Test]  Epoch: 79	Loss: 0.058650	Acc: 19.6% (1964/10000)
[Test]  Epoch: 80	Loss: 0.057995	Acc: 19.9% (1990/10000)
[Test]  Epoch: 81	Loss: 0.058850	Acc: 19.1% (1910/10000)
[Test]  Epoch: 82	Loss: 0.058437	Acc: 19.4% (1944/10000)
[Test]  Epoch: 83	Loss: 0.058783	Acc: 19.6% (1960/10000)
[Test]  Epoch: 84	Loss: 0.058773	Acc: 19.7% (1966/10000)
[Test]  Epoch: 85	Loss: 0.058855	Acc: 19.2% (1918/10000)
[Test]  Epoch: 86	Loss: 0.058563	Acc: 19.3% (1930/10000)
[Test]  Epoch: 87	Loss: 0.058508	Acc: 19.9% (1993/10000)
[Test]  Epoch: 88	Loss: 0.058392	Acc: 19.4% (1944/10000)
[Test]  Epoch: 89	Loss: 0.058199	Acc: 19.8% (1978/10000)
[Test]  Epoch: 90	Loss: 0.058261	Acc: 20.1% (2008/10000)
[Test]  Epoch: 91	Loss: 0.058455	Acc: 19.9% (1990/10000)
[Test]  Epoch: 92	Loss: 0.058472	Acc: 19.7% (1972/10000)
[Test]  Epoch: 93	Loss: 0.058495	Acc: 19.5% (1952/10000)
[Test]  Epoch: 94	Loss: 0.058211	Acc: 19.9% (1991/10000)
[Test]  Epoch: 95	Loss: 0.058791	Acc: 19.2% (1923/10000)
[Test]  Epoch: 96	Loss: 0.058602	Acc: 18.8% (1882/10000)
[Test]  Epoch: 97	Loss: 0.058216	Acc: 19.4% (1942/10000)
[Test]  Epoch: 98	Loss: 0.058515	Acc: 19.9% (1986/10000)
[Test]  Epoch: 99	Loss: 0.058562	Acc: 19.6% (1958/10000)
[Test]  Epoch: 100	Loss: 0.058252	Acc: 19.9% (1991/10000)
===========finish==========
['2024-08-19', '04:13:02.863577', '100', 'test', '0.05825176589488983', '19.91', '20.1']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.108519	Acc: 1.8% (176/10000)
[Test]  Epoch: 2	Loss: 0.084773	Acc: 3.2% (323/10000)
[Test]  Epoch: 3	Loss: 0.077959	Acc: 4.9% (487/10000)
[Test]  Epoch: 4	Loss: 0.076409	Acc: 5.2% (520/10000)
[Test]  Epoch: 5	Loss: 0.075541	Acc: 5.9% (588/10000)
[Test]  Epoch: 6	Loss: 0.074664	Acc: 6.6% (664/10000)
[Test]  Epoch: 7	Loss: 0.073410	Acc: 6.9% (688/10000)
[Test]  Epoch: 8	Loss: 0.073536	Acc: 7.4% (736/10000)
[Test]  Epoch: 9	Loss: 0.072376	Acc: 7.8% (776/10000)
[Test]  Epoch: 10	Loss: 0.072036	Acc: 7.8% (784/10000)
[Test]  Epoch: 11	Loss: 0.072597	Acc: 7.7% (774/10000)
[Test]  Epoch: 12	Loss: 0.071156	Acc: 8.8% (879/10000)
[Test]  Epoch: 13	Loss: 0.071910	Acc: 8.0% (804/10000)
[Test]  Epoch: 14	Loss: 0.071004	Acc: 8.7% (868/10000)
[Test]  Epoch: 15	Loss: 0.071463	Acc: 8.5% (848/10000)
[Test]  Epoch: 16	Loss: 0.071460	Acc: 8.8% (883/10000)
[Test]  Epoch: 17	Loss: 0.069800	Acc: 9.9% (992/10000)
[Test]  Epoch: 18	Loss: 0.070535	Acc: 9.6% (963/10000)
[Test]  Epoch: 19	Loss: 0.069061	Acc: 10.1% (1008/10000)
[Test]  Epoch: 20	Loss: 0.068642	Acc: 10.8% (1078/10000)
[Test]  Epoch: 21	Loss: 0.068259	Acc: 11.4% (1138/10000)
[Test]  Epoch: 22	Loss: 0.069173	Acc: 10.9% (1086/10000)
[Test]  Epoch: 23	Loss: 0.069054	Acc: 10.3% (1034/10000)
[Test]  Epoch: 24	Loss: 0.068044	Acc: 10.7% (1074/10000)
[Test]  Epoch: 25	Loss: 0.067889	Acc: 11.2% (1123/10000)
[Test]  Epoch: 26	Loss: 0.068120	Acc: 11.5% (1148/10000)
[Test]  Epoch: 27	Loss: 0.069403	Acc: 10.6% (1055/10000)
[Test]  Epoch: 28	Loss: 0.067229	Acc: 11.9% (1186/10000)
[Test]  Epoch: 29	Loss: 0.068146	Acc: 11.0% (1098/10000)
[Test]  Epoch: 30	Loss: 0.067387	Acc: 12.1% (1206/10000)
[Test]  Epoch: 31	Loss: 0.067467	Acc: 11.7% (1166/10000)
[Test]  Epoch: 32	Loss: 0.066973	Acc: 12.1% (1211/10000)
[Test]  Epoch: 33	Loss: 0.067333	Acc: 11.5% (1147/10000)
[Test]  Epoch: 34	Loss: 0.066498	Acc: 12.0% (1204/10000)
[Test]  Epoch: 35	Loss: 0.068769	Acc: 11.3% (1133/10000)
[Test]  Epoch: 36	Loss: 0.066808	Acc: 11.8% (1180/10000)
[Test]  Epoch: 37	Loss: 0.066374	Acc: 12.4% (1244/10000)
[Test]  Epoch: 38	Loss: 0.067074	Acc: 11.6% (1163/10000)
[Test]  Epoch: 39	Loss: 0.066476	Acc: 12.2% (1219/10000)
[Test]  Epoch: 40	Loss: 0.066757	Acc: 12.2% (1224/10000)
[Test]  Epoch: 41	Loss: 0.066270	Acc: 12.3% (1234/10000)
[Test]  Epoch: 42	Loss: 0.065703	Acc: 12.9% (1295/10000)
[Test]  Epoch: 43	Loss: 0.066285	Acc: 12.5% (1249/10000)
[Test]  Epoch: 44	Loss: 0.065154	Acc: 13.7% (1370/10000)
[Test]  Epoch: 45	Loss: 0.065825	Acc: 13.5% (1350/10000)
[Test]  Epoch: 46	Loss: 0.066328	Acc: 12.9% (1290/10000)
[Test]  Epoch: 47	Loss: 0.065049	Acc: 13.1% (1313/10000)
[Test]  Epoch: 48	Loss: 0.065092	Acc: 13.8% (1384/10000)
[Test]  Epoch: 49	Loss: 0.065195	Acc: 13.5% (1347/10000)
[Test]  Epoch: 50	Loss: 0.065418	Acc: 13.5% (1351/10000)
[Test]  Epoch: 51	Loss: 0.065472	Acc: 13.6% (1361/10000)
[Test]  Epoch: 52	Loss: 0.065592	Acc: 13.2% (1318/10000)
[Test]  Epoch: 53	Loss: 0.065318	Acc: 13.2% (1321/10000)
[Test]  Epoch: 54	Loss: 0.064950	Acc: 13.4% (1338/10000)
[Test]  Epoch: 55	Loss: 0.065274	Acc: 13.5% (1346/10000)
[Test]  Epoch: 56	Loss: 0.066448	Acc: 12.6% (1264/10000)
[Test]  Epoch: 57	Loss: 0.064534	Acc: 14.1% (1414/10000)
[Test]  Epoch: 58	Loss: 0.064823	Acc: 13.8% (1383/10000)
[Test]  Epoch: 59	Loss: 0.065046	Acc: 12.9% (1294/10000)
[Test]  Epoch: 60	Loss: 0.065827	Acc: 13.3% (1335/10000)
[Test]  Epoch: 61	Loss: 0.064173	Acc: 14.1% (1414/10000)
[Test]  Epoch: 62	Loss: 0.064107	Acc: 14.2% (1419/10000)
[Test]  Epoch: 63	Loss: 0.063942	Acc: 14.2% (1419/10000)
[Test]  Epoch: 64	Loss: 0.063825	Acc: 14.7% (1468/10000)
[Test]  Epoch: 65	Loss: 0.063828	Acc: 13.9% (1388/10000)
[Test]  Epoch: 66	Loss: 0.063680	Acc: 14.4% (1439/10000)
[Test]  Epoch: 67	Loss: 0.063835	Acc: 14.7% (1467/10000)
[Test]  Epoch: 68	Loss: 0.063571	Acc: 14.3% (1431/10000)
[Test]  Epoch: 69	Loss: 0.063758	Acc: 14.4% (1442/10000)
[Test]  Epoch: 70	Loss: 0.063711	Acc: 14.4% (1441/10000)
[Test]  Epoch: 71	Loss: 0.063825	Acc: 14.8% (1478/10000)
[Test]  Epoch: 72	Loss: 0.063304	Acc: 14.9% (1487/10000)
[Test]  Epoch: 73	Loss: 0.063608	Acc: 14.2% (1422/10000)
[Test]  Epoch: 74	Loss: 0.063721	Acc: 15.1% (1512/10000)
[Test]  Epoch: 75	Loss: 0.063447	Acc: 14.6% (1461/10000)
[Test]  Epoch: 76	Loss: 0.063413	Acc: 14.6% (1458/10000)
[Test]  Epoch: 77	Loss: 0.063320	Acc: 14.9% (1489/10000)
[Test]  Epoch: 78	Loss: 0.063557	Acc: 14.5% (1451/10000)
[Test]  Epoch: 79	Loss: 0.063605	Acc: 14.6% (1455/10000)
[Test]  Epoch: 80	Loss: 0.062973	Acc: 15.7% (1570/10000)
[Test]  Epoch: 81	Loss: 0.063600	Acc: 14.7% (1467/10000)
[Test]  Epoch: 82	Loss: 0.063392	Acc: 15.0% (1500/10000)
[Test]  Epoch: 83	Loss: 0.063597	Acc: 14.9% (1486/10000)
[Test]  Epoch: 84	Loss: 0.063620	Acc: 14.7% (1473/10000)
[Test]  Epoch: 85	Loss: 0.063940	Acc: 14.7% (1466/10000)
[Test]  Epoch: 86	Loss: 0.063594	Acc: 14.7% (1467/10000)
[Test]  Epoch: 87	Loss: 0.063371	Acc: 15.2% (1518/10000)
[Test]  Epoch: 88	Loss: 0.063328	Acc: 14.7% (1472/10000)
[Test]  Epoch: 89	Loss: 0.063267	Acc: 15.0% (1503/10000)
[Test]  Epoch: 90	Loss: 0.063298	Acc: 14.7% (1467/10000)
[Test]  Epoch: 91	Loss: 0.063609	Acc: 14.5% (1447/10000)
[Test]  Epoch: 92	Loss: 0.063188	Acc: 15.6% (1562/10000)
[Test]  Epoch: 93	Loss: 0.063424	Acc: 15.3% (1528/10000)
[Test]  Epoch: 94	Loss: 0.063074	Acc: 15.1% (1513/10000)
[Test]  Epoch: 95	Loss: 0.063598	Acc: 14.7% (1469/10000)
[Test]  Epoch: 96	Loss: 0.063312	Acc: 15.2% (1518/10000)
[Test]  Epoch: 97	Loss: 0.063151	Acc: 14.8% (1484/10000)
[Test]  Epoch: 98	Loss: 0.063223	Acc: 14.7% (1474/10000)
[Test]  Epoch: 99	Loss: 0.063432	Acc: 14.8% (1485/10000)
[Test]  Epoch: 100	Loss: 0.063128	Acc: 15.5% (1547/10000)
===========finish==========
['2024-08-19', '04:17:59.469968', '100', 'test', '0.06312788710594178', '15.47', '15.7']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.162799	Acc: 1.1% (110/10000)
[Test]  Epoch: 2	Loss: 0.084553	Acc: 2.3% (230/10000)
[Test]  Epoch: 3	Loss: 0.079517	Acc: 2.9% (291/10000)
[Test]  Epoch: 4	Loss: 0.079657	Acc: 3.1% (312/10000)
[Test]  Epoch: 5	Loss: 0.077562	Acc: 3.7% (370/10000)
[Test]  Epoch: 6	Loss: 0.078417	Acc: 4.0% (395/10000)
[Test]  Epoch: 7	Loss: 0.077061	Acc: 4.3% (431/10000)
[Test]  Epoch: 8	Loss: 0.076859	Acc: 4.6% (459/10000)
[Test]  Epoch: 9	Loss: 0.075753	Acc: 4.8% (478/10000)
[Test]  Epoch: 10	Loss: 0.076205	Acc: 5.0% (503/10000)
[Test]  Epoch: 11	Loss: 0.075423	Acc: 4.7% (471/10000)
[Test]  Epoch: 12	Loss: 0.074970	Acc: 5.7% (570/10000)
[Test]  Epoch: 13	Loss: 0.076053	Acc: 5.2% (515/10000)
[Test]  Epoch: 14	Loss: 0.077319	Acc: 4.5% (455/10000)
[Test]  Epoch: 15	Loss: 0.073823	Acc: 6.5% (655/10000)
[Test]  Epoch: 16	Loss: 0.075821	Acc: 6.0% (598/10000)
[Test]  Epoch: 17	Loss: 0.073959	Acc: 6.4% (637/10000)
[Test]  Epoch: 18	Loss: 0.074754	Acc: 6.3% (630/10000)
[Test]  Epoch: 19	Loss: 0.075894	Acc: 6.0% (599/10000)
[Test]  Epoch: 20	Loss: 0.072833	Acc: 7.5% (750/10000)
[Test]  Epoch: 21	Loss: 0.073880	Acc: 7.1% (712/10000)
[Test]  Epoch: 22	Loss: 0.072976	Acc: 7.5% (751/10000)
[Test]  Epoch: 23	Loss: 0.073425	Acc: 6.7% (667/10000)
[Test]  Epoch: 24	Loss: 0.072821	Acc: 7.2% (720/10000)
[Test]  Epoch: 25	Loss: 0.072986	Acc: 7.5% (753/10000)
[Test]  Epoch: 26	Loss: 0.074526	Acc: 7.1% (706/10000)
[Test]  Epoch: 27	Loss: 0.073132	Acc: 7.6% (763/10000)
[Test]  Epoch: 28	Loss: 0.072596	Acc: 7.7% (771/10000)
[Test]  Epoch: 29	Loss: 0.073458	Acc: 7.6% (756/10000)
[Test]  Epoch: 30	Loss: 0.072778	Acc: 7.8% (781/10000)
[Test]  Epoch: 31	Loss: 0.072189	Acc: 8.1% (813/10000)
[Test]  Epoch: 32	Loss: 0.072868	Acc: 8.2% (817/10000)
[Test]  Epoch: 33	Loss: 0.074641	Acc: 7.0% (696/10000)
[Test]  Epoch: 34	Loss: 0.071899	Acc: 8.5% (850/10000)
[Test]  Epoch: 35	Loss: 0.072777	Acc: 7.6% (764/10000)
[Test]  Epoch: 36	Loss: 0.072148	Acc: 8.8% (880/10000)
[Test]  Epoch: 37	Loss: 0.072753	Acc: 8.2% (819/10000)
[Test]  Epoch: 38	Loss: 0.070935	Acc: 8.4% (844/10000)
[Test]  Epoch: 39	Loss: 0.070777	Acc: 9.5% (951/10000)
[Test]  Epoch: 40	Loss: 0.072260	Acc: 8.1% (806/10000)
[Test]  Epoch: 41	Loss: 0.070881	Acc: 8.8% (885/10000)
[Test]  Epoch: 42	Loss: 0.070892	Acc: 8.9% (891/10000)
[Test]  Epoch: 43	Loss: 0.071144	Acc: 9.2% (917/10000)
[Test]  Epoch: 44	Loss: 0.072553	Acc: 8.7% (865/10000)
[Test]  Epoch: 45	Loss: 0.071807	Acc: 8.7% (873/10000)
[Test]  Epoch: 46	Loss: 0.071718	Acc: 8.7% (874/10000)
[Test]  Epoch: 47	Loss: 0.070628	Acc: 9.1% (909/10000)
[Test]  Epoch: 48	Loss: 0.071055	Acc: 9.4% (943/10000)
[Test]  Epoch: 49	Loss: 0.071875	Acc: 9.2% (922/10000)
[Test]  Epoch: 50	Loss: 0.071016	Acc: 9.7% (968/10000)
[Test]  Epoch: 51	Loss: 0.070296	Acc: 9.5% (950/10000)
[Test]  Epoch: 52	Loss: 0.070248	Acc: 9.3% (930/10000)
[Test]  Epoch: 53	Loss: 0.071065	Acc: 9.2% (923/10000)
[Test]  Epoch: 54	Loss: 0.070602	Acc: 9.4% (937/10000)
[Test]  Epoch: 55	Loss: 0.072196	Acc: 8.8% (882/10000)
[Test]  Epoch: 56	Loss: 0.070977	Acc: 9.4% (936/10000)
[Test]  Epoch: 57	Loss: 0.071041	Acc: 9.2% (916/10000)
[Test]  Epoch: 58	Loss: 0.070879	Acc: 9.6% (964/10000)
[Test]  Epoch: 59	Loss: 0.070443	Acc: 9.2% (920/10000)
[Test]  Epoch: 60	Loss: 0.071002	Acc: 9.2% (918/10000)
[Test]  Epoch: 61	Loss: 0.069461	Acc: 10.2% (1016/10000)
[Test]  Epoch: 62	Loss: 0.068923	Acc: 10.6% (1055/10000)
[Test]  Epoch: 63	Loss: 0.068827	Acc: 10.6% (1060/10000)
[Test]  Epoch: 64	Loss: 0.068730	Acc: 10.6% (1062/10000)
[Test]  Epoch: 65	Loss: 0.068786	Acc: 10.5% (1048/10000)
[Test]  Epoch: 66	Loss: 0.068404	Acc: 10.6% (1062/10000)
[Test]  Epoch: 67	Loss: 0.068667	Acc: 10.8% (1082/10000)
[Test]  Epoch: 68	Loss: 0.068612	Acc: 10.2% (1024/10000)
[Test]  Epoch: 69	Loss: 0.068342	Acc: 10.8% (1082/10000)
[Test]  Epoch: 70	Loss: 0.068153	Acc: 11.0% (1100/10000)
[Test]  Epoch: 71	Loss: 0.068311	Acc: 10.9% (1086/10000)
[Test]  Epoch: 72	Loss: 0.068419	Acc: 10.9% (1092/10000)
[Test]  Epoch: 73	Loss: 0.068495	Acc: 11.0% (1104/10000)
[Test]  Epoch: 74	Loss: 0.068656	Acc: 11.0% (1098/10000)
[Test]  Epoch: 75	Loss: 0.068110	Acc: 11.1% (1110/10000)
[Test]  Epoch: 76	Loss: 0.068234	Acc: 10.4% (1045/10000)
[Test]  Epoch: 77	Loss: 0.068488	Acc: 11.1% (1109/10000)
[Test]  Epoch: 78	Loss: 0.068424	Acc: 10.8% (1077/10000)
[Test]  Epoch: 79	Loss: 0.068328	Acc: 10.9% (1093/10000)
[Test]  Epoch: 80	Loss: 0.067903	Acc: 11.2% (1116/10000)
[Test]  Epoch: 81	Loss: 0.068350	Acc: 10.6% (1063/10000)
[Test]  Epoch: 82	Loss: 0.068177	Acc: 10.9% (1094/10000)
[Test]  Epoch: 83	Loss: 0.068353	Acc: 10.8% (1085/10000)
[Test]  Epoch: 84	Loss: 0.068352	Acc: 10.5% (1054/10000)
[Test]  Epoch: 85	Loss: 0.068205	Acc: 10.8% (1080/10000)
[Test]  Epoch: 86	Loss: 0.068469	Acc: 10.7% (1072/10000)
[Test]  Epoch: 87	Loss: 0.068056	Acc: 10.8% (1081/10000)
[Test]  Epoch: 88	Loss: 0.068035	Acc: 10.9% (1092/10000)
[Test]  Epoch: 89	Loss: 0.067948	Acc: 10.7% (1069/10000)
[Test]  Epoch: 90	Loss: 0.067999	Acc: 11.2% (1124/10000)
[Test]  Epoch: 91	Loss: 0.068428	Acc: 11.2% (1120/10000)
[Test]  Epoch: 92	Loss: 0.067849	Acc: 11.3% (1134/10000)
[Test]  Epoch: 93	Loss: 0.068133	Acc: 11.3% (1128/10000)
[Test]  Epoch: 94	Loss: 0.068169	Acc: 11.2% (1120/10000)
[Test]  Epoch: 95	Loss: 0.068307	Acc: 10.7% (1071/10000)
[Test]  Epoch: 96	Loss: 0.068327	Acc: 10.5% (1048/10000)
[Test]  Epoch: 97	Loss: 0.068017	Acc: 10.8% (1085/10000)
[Test]  Epoch: 98	Loss: 0.068113	Acc: 11.1% (1108/10000)
[Test]  Epoch: 99	Loss: 0.068304	Acc: 10.7% (1069/10000)
[Test]  Epoch: 100	Loss: 0.068100	Acc: 10.9% (1091/10000)
===========finish==========
['2024-08-19', '04:23:03.087820', '100', 'test', '0.0680998556613922', '10.91', '11.34']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.146738	Acc: 0.7% (73/10000)
[Test]  Epoch: 2	Loss: 0.081905	Acc: 2.2% (219/10000)
[Test]  Epoch: 3	Loss: 0.082693	Acc: 2.4% (244/10000)
[Test]  Epoch: 4	Loss: 0.080093	Acc: 3.1% (308/10000)
[Test]  Epoch: 5	Loss: 0.079233	Acc: 3.1% (309/10000)
[Test]  Epoch: 6	Loss: 0.078994	Acc: 3.4% (343/10000)
[Test]  Epoch: 7	Loss: 0.077922	Acc: 4.0% (405/10000)
[Test]  Epoch: 8	Loss: 0.078132	Acc: 4.0% (397/10000)
[Test]  Epoch: 9	Loss: 0.078001	Acc: 3.7% (367/10000)
[Test]  Epoch: 10	Loss: 0.076742	Acc: 4.8% (478/10000)
[Test]  Epoch: 11	Loss: 0.077271	Acc: 4.0% (405/10000)
[Test]  Epoch: 12	Loss: 0.076501	Acc: 4.7% (471/10000)
[Test]  Epoch: 13	Loss: 0.076617	Acc: 4.6% (460/10000)
[Test]  Epoch: 14	Loss: 0.077158	Acc: 5.0% (495/10000)
[Test]  Epoch: 15	Loss: 0.076720	Acc: 4.7% (465/10000)
[Test]  Epoch: 16	Loss: 0.076674	Acc: 4.8% (484/10000)
[Test]  Epoch: 17	Loss: 0.075353	Acc: 5.5% (545/10000)
[Test]  Epoch: 18	Loss: 0.076421	Acc: 4.7% (471/10000)
[Test]  Epoch: 19	Loss: 0.074710	Acc: 5.8% (576/10000)
[Test]  Epoch: 20	Loss: 0.073929	Acc: 6.4% (638/10000)
[Test]  Epoch: 21	Loss: 0.074738	Acc: 6.1% (613/10000)
[Test]  Epoch: 22	Loss: 0.074646	Acc: 6.2% (624/10000)
[Test]  Epoch: 23	Loss: 0.075888	Acc: 5.5% (548/10000)
[Test]  Epoch: 24	Loss: 0.075075	Acc: 6.1% (608/10000)
[Test]  Epoch: 25	Loss: 0.074127	Acc: 6.7% (667/10000)
[Test]  Epoch: 26	Loss: 0.074253	Acc: 6.3% (626/10000)
[Test]  Epoch: 27	Loss: 0.076026	Acc: 5.8% (583/10000)
[Test]  Epoch: 28	Loss: 0.074227	Acc: 6.4% (636/10000)
[Test]  Epoch: 29	Loss: 0.075808	Acc: 5.8% (578/10000)
[Test]  Epoch: 30	Loss: 0.073935	Acc: 6.7% (671/10000)
[Test]  Epoch: 31	Loss: 0.075408	Acc: 6.4% (641/10000)
[Test]  Epoch: 32	Loss: 0.075025	Acc: 6.6% (661/10000)
[Test]  Epoch: 33	Loss: 0.074803	Acc: 6.5% (650/10000)
[Test]  Epoch: 34	Loss: 0.073949	Acc: 6.7% (669/10000)
[Test]  Epoch: 35	Loss: 0.073921	Acc: 6.8% (679/10000)
[Test]  Epoch: 36	Loss: 0.075150	Acc: 6.4% (641/10000)
[Test]  Epoch: 37	Loss: 0.073891	Acc: 6.7% (667/10000)
[Test]  Epoch: 38	Loss: 0.073743	Acc: 7.0% (697/10000)
[Test]  Epoch: 39	Loss: 0.074270	Acc: 7.0% (697/10000)
[Test]  Epoch: 40	Loss: 0.074265	Acc: 6.4% (644/10000)
[Test]  Epoch: 41	Loss: 0.073945	Acc: 7.2% (722/10000)
[Test]  Epoch: 42	Loss: 0.072917	Acc: 7.6% (757/10000)
[Test]  Epoch: 43	Loss: 0.073349	Acc: 6.9% (692/10000)
[Test]  Epoch: 44	Loss: 0.073554	Acc: 7.4% (739/10000)
[Test]  Epoch: 45	Loss: 0.073562	Acc: 7.0% (705/10000)
[Test]  Epoch: 46	Loss: 0.073718	Acc: 7.3% (734/10000)
[Test]  Epoch: 47	Loss: 0.073662	Acc: 7.3% (735/10000)
[Test]  Epoch: 48	Loss: 0.073084	Acc: 7.5% (749/10000)
[Test]  Epoch: 49	Loss: 0.073910	Acc: 7.3% (735/10000)
[Test]  Epoch: 50	Loss: 0.073590	Acc: 7.5% (746/10000)
[Test]  Epoch: 51	Loss: 0.074393	Acc: 7.3% (735/10000)
[Test]  Epoch: 52	Loss: 0.073086	Acc: 7.6% (757/10000)
[Test]  Epoch: 53	Loss: 0.072621	Acc: 7.3% (729/10000)
[Test]  Epoch: 54	Loss: 0.072950	Acc: 7.8% (777/10000)
[Test]  Epoch: 55	Loss: 0.074090	Acc: 7.0% (701/10000)
[Test]  Epoch: 56	Loss: 0.073802	Acc: 7.3% (731/10000)
[Test]  Epoch: 57	Loss: 0.072945	Acc: 7.9% (790/10000)
[Test]  Epoch: 58	Loss: 0.072352	Acc: 8.1% (805/10000)
[Test]  Epoch: 59	Loss: 0.073429	Acc: 7.8% (775/10000)
[Test]  Epoch: 60	Loss: 0.073306	Acc: 7.8% (783/10000)
[Test]  Epoch: 61	Loss: 0.071588	Acc: 8.5% (848/10000)
[Test]  Epoch: 62	Loss: 0.071395	Acc: 8.7% (865/10000)
[Test]  Epoch: 63	Loss: 0.071453	Acc: 8.8% (875/10000)
[Test]  Epoch: 64	Loss: 0.071028	Acc: 9.2% (918/10000)
[Test]  Epoch: 65	Loss: 0.071313	Acc: 8.4% (844/10000)
[Test]  Epoch: 66	Loss: 0.070939	Acc: 9.2% (924/10000)
[Test]  Epoch: 67	Loss: 0.071192	Acc: 9.0% (901/10000)
[Test]  Epoch: 68	Loss: 0.070989	Acc: 9.0% (899/10000)
[Test]  Epoch: 69	Loss: 0.070986	Acc: 8.4% (844/10000)
[Test]  Epoch: 70	Loss: 0.070684	Acc: 9.1% (909/10000)
[Test]  Epoch: 71	Loss: 0.070801	Acc: 8.8% (882/10000)
[Test]  Epoch: 72	Loss: 0.070621	Acc: 9.3% (927/10000)
[Test]  Epoch: 73	Loss: 0.070629	Acc: 8.8% (878/10000)
[Test]  Epoch: 74	Loss: 0.071115	Acc: 8.7% (874/10000)
[Test]  Epoch: 75	Loss: 0.070831	Acc: 8.8% (885/10000)
[Test]  Epoch: 76	Loss: 0.070881	Acc: 8.6% (858/10000)
[Test]  Epoch: 77	Loss: 0.070768	Acc: 9.5% (949/10000)
[Test]  Epoch: 78	Loss: 0.070855	Acc: 8.9% (890/10000)
[Test]  Epoch: 79	Loss: 0.070732	Acc: 9.5% (952/10000)
[Test]  Epoch: 80	Loss: 0.070379	Acc: 9.2% (923/10000)
[Test]  Epoch: 81	Loss: 0.070725	Acc: 9.2% (919/10000)
[Test]  Epoch: 82	Loss: 0.070638	Acc: 9.2% (916/10000)
[Test]  Epoch: 83	Loss: 0.070879	Acc: 9.5% (949/10000)
[Test]  Epoch: 84	Loss: 0.070634	Acc: 9.0% (902/10000)
[Test]  Epoch: 85	Loss: 0.070690	Acc: 9.2% (916/10000)
[Test]  Epoch: 86	Loss: 0.070869	Acc: 9.1% (907/10000)
[Test]  Epoch: 87	Loss: 0.070830	Acc: 9.0% (899/10000)
[Test]  Epoch: 88	Loss: 0.070525	Acc: 9.4% (939/10000)
[Test]  Epoch: 89	Loss: 0.070690	Acc: 9.0% (897/10000)
[Test]  Epoch: 90	Loss: 0.070509	Acc: 9.3% (931/10000)
[Test]  Epoch: 91	Loss: 0.070719	Acc: 9.2% (920/10000)
[Test]  Epoch: 92	Loss: 0.070594	Acc: 9.1% (911/10000)
[Test]  Epoch: 93	Loss: 0.070751	Acc: 9.3% (930/10000)
[Test]  Epoch: 94	Loss: 0.070692	Acc: 8.9% (894/10000)
[Test]  Epoch: 95	Loss: 0.070686	Acc: 9.2% (918/10000)
[Test]  Epoch: 96	Loss: 0.070758	Acc: 8.9% (891/10000)
[Test]  Epoch: 97	Loss: 0.070412	Acc: 9.2% (918/10000)
[Test]  Epoch: 98	Loss: 0.070791	Acc: 9.4% (937/10000)
[Test]  Epoch: 99	Loss: 0.070465	Acc: 9.4% (936/10000)
[Test]  Epoch: 100	Loss: 0.070737	Acc: 9.2% (924/10000)
===========finish==========
['2024-08-19', '04:28:08.117299', '100', 'test', '0.0707373920917511', '9.24', '9.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.225262	Acc: 0.7% (65/10000)
[Test]  Epoch: 2	Loss: 0.082772	Acc: 2.4% (238/10000)
[Test]  Epoch: 3	Loss: 0.079319	Acc: 2.9% (285/10000)
[Test]  Epoch: 4	Loss: 0.078933	Acc: 3.4% (336/10000)
[Test]  Epoch: 5	Loss: 0.079025	Acc: 3.6% (359/10000)
[Test]  Epoch: 6	Loss: 0.078194	Acc: 3.5% (351/10000)
[Test]  Epoch: 7	Loss: 0.077438	Acc: 3.7% (373/10000)
[Test]  Epoch: 8	Loss: 0.077834	Acc: 3.9% (386/10000)
[Test]  Epoch: 9	Loss: 0.077194	Acc: 4.1% (408/10000)
[Test]  Epoch: 10	Loss: 0.077661	Acc: 3.9% (388/10000)
[Test]  Epoch: 11	Loss: 0.077473	Acc: 4.0% (399/10000)
[Test]  Epoch: 12	Loss: 0.076655	Acc: 4.3% (434/10000)
[Test]  Epoch: 13	Loss: 0.076827	Acc: 4.2% (424/10000)
[Test]  Epoch: 14	Loss: 0.077086	Acc: 4.5% (450/10000)
[Test]  Epoch: 15	Loss: 0.077063	Acc: 4.8% (477/10000)
[Test]  Epoch: 16	Loss: 0.078887	Acc: 4.5% (446/10000)
[Test]  Epoch: 17	Loss: 0.076181	Acc: 5.2% (523/10000)
[Test]  Epoch: 18	Loss: 0.077308	Acc: 4.1% (409/10000)
[Test]  Epoch: 19	Loss: 0.076509	Acc: 5.1% (509/10000)
[Test]  Epoch: 20	Loss: 0.076241	Acc: 5.3% (529/10000)
[Test]  Epoch: 21	Loss: 0.077914	Acc: 4.5% (453/10000)
[Test]  Epoch: 22	Loss: 0.075781	Acc: 5.2% (517/10000)
[Test]  Epoch: 23	Loss: 0.076016	Acc: 5.3% (533/10000)
[Test]  Epoch: 24	Loss: 0.075599	Acc: 5.8% (581/10000)
[Test]  Epoch: 25	Loss: 0.075293	Acc: 5.6% (560/10000)
[Test]  Epoch: 26	Loss: 0.076227	Acc: 5.4% (539/10000)
[Test]  Epoch: 27	Loss: 0.076016	Acc: 5.2% (522/10000)
[Test]  Epoch: 28	Loss: 0.074761	Acc: 6.0% (600/10000)
[Test]  Epoch: 29	Loss: 0.075121	Acc: 5.4% (544/10000)
[Test]  Epoch: 30	Loss: 0.075514	Acc: 5.6% (561/10000)
[Test]  Epoch: 31	Loss: 0.075615	Acc: 5.8% (575/10000)
[Test]  Epoch: 32	Loss: 0.075187	Acc: 6.1% (608/10000)
[Test]  Epoch: 33	Loss: 0.075631	Acc: 5.6% (561/10000)
[Test]  Epoch: 34	Loss: 0.074804	Acc: 6.3% (632/10000)
[Test]  Epoch: 35	Loss: 0.074965	Acc: 5.7% (573/10000)
[Test]  Epoch: 36	Loss: 0.075541	Acc: 6.2% (618/10000)
[Test]  Epoch: 37	Loss: 0.076563	Acc: 5.2% (523/10000)
[Test]  Epoch: 38	Loss: 0.074531	Acc: 6.2% (623/10000)
[Test]  Epoch: 39	Loss: 0.075271	Acc: 6.1% (606/10000)
[Test]  Epoch: 40	Loss: 0.074956	Acc: 5.5% (555/10000)
[Test]  Epoch: 41	Loss: 0.074809	Acc: 6.4% (636/10000)
[Test]  Epoch: 42	Loss: 0.075098	Acc: 6.1% (612/10000)
[Test]  Epoch: 43	Loss: 0.074581	Acc: 6.4% (644/10000)
[Test]  Epoch: 44	Loss: 0.075165	Acc: 6.2% (619/10000)
[Test]  Epoch: 45	Loss: 0.075206	Acc: 6.0% (598/10000)
[Test]  Epoch: 46	Loss: 0.075204	Acc: 5.9% (591/10000)
[Test]  Epoch: 47	Loss: 0.076075	Acc: 6.0% (595/10000)
[Test]  Epoch: 48	Loss: 0.074910	Acc: 6.7% (665/10000)
[Test]  Epoch: 49	Loss: 0.076814	Acc: 5.8% (579/10000)
[Test]  Epoch: 50	Loss: 0.075412	Acc: 6.5% (648/10000)
[Test]  Epoch: 51	Loss: 0.076061	Acc: 6.1% (612/10000)
[Test]  Epoch: 52	Loss: 0.074065	Acc: 6.5% (655/10000)
[Test]  Epoch: 53	Loss: 0.074353	Acc: 6.9% (692/10000)
[Test]  Epoch: 54	Loss: 0.073300	Acc: 7.2% (720/10000)
[Test]  Epoch: 55	Loss: 0.074529	Acc: 6.6% (658/10000)
[Test]  Epoch: 56	Loss: 0.075199	Acc: 6.7% (666/10000)
[Test]  Epoch: 57	Loss: 0.075134	Acc: 6.3% (628/10000)
[Test]  Epoch: 58	Loss: 0.074705	Acc: 6.4% (636/10000)
[Test]  Epoch: 59	Loss: 0.074748	Acc: 7.0% (702/10000)
[Test]  Epoch: 60	Loss: 0.074792	Acc: 6.6% (658/10000)
[Test]  Epoch: 61	Loss: 0.072933	Acc: 7.6% (757/10000)
[Test]  Epoch: 62	Loss: 0.072523	Acc: 7.8% (782/10000)
[Test]  Epoch: 63	Loss: 0.072589	Acc: 7.5% (751/10000)
[Test]  Epoch: 64	Loss: 0.072318	Acc: 8.0% (799/10000)
[Test]  Epoch: 65	Loss: 0.072316	Acc: 7.4% (742/10000)
[Test]  Epoch: 66	Loss: 0.072341	Acc: 8.0% (795/10000)
[Test]  Epoch: 67	Loss: 0.072499	Acc: 7.6% (756/10000)
[Test]  Epoch: 68	Loss: 0.072239	Acc: 7.8% (780/10000)
[Test]  Epoch: 69	Loss: 0.072268	Acc: 8.0% (800/10000)
[Test]  Epoch: 70	Loss: 0.072232	Acc: 8.0% (796/10000)
[Test]  Epoch: 71	Loss: 0.072078	Acc: 7.6% (764/10000)
[Test]  Epoch: 72	Loss: 0.072044	Acc: 7.7% (772/10000)
[Test]  Epoch: 73	Loss: 0.072256	Acc: 7.8% (779/10000)
[Test]  Epoch: 74	Loss: 0.072306	Acc: 7.8% (784/10000)
[Test]  Epoch: 75	Loss: 0.072205	Acc: 7.8% (784/10000)
[Test]  Epoch: 76	Loss: 0.072006	Acc: 7.8% (783/10000)
[Test]  Epoch: 77	Loss: 0.072331	Acc: 7.6% (761/10000)
[Test]  Epoch: 78	Loss: 0.072393	Acc: 7.8% (779/10000)
[Test]  Epoch: 79	Loss: 0.072367	Acc: 7.7% (768/10000)
[Test]  Epoch: 80	Loss: 0.071649	Acc: 8.3% (829/10000)
[Test]  Epoch: 81	Loss: 0.072191	Acc: 8.2% (817/10000)
[Test]  Epoch: 82	Loss: 0.072076	Acc: 8.1% (805/10000)
[Test]  Epoch: 83	Loss: 0.072208	Acc: 8.0% (804/10000)
[Test]  Epoch: 84	Loss: 0.072076	Acc: 8.0% (803/10000)
[Test]  Epoch: 85	Loss: 0.071919	Acc: 8.2% (823/10000)
[Test]  Epoch: 86	Loss: 0.072181	Acc: 8.1% (807/10000)
[Test]  Epoch: 87	Loss: 0.072057	Acc: 7.6% (763/10000)
[Test]  Epoch: 88	Loss: 0.071796	Acc: 8.3% (829/10000)
[Test]  Epoch: 89	Loss: 0.072135	Acc: 7.9% (794/10000)
[Test]  Epoch: 90	Loss: 0.071995	Acc: 8.0% (803/10000)
[Test]  Epoch: 91	Loss: 0.071944	Acc: 8.4% (838/10000)
[Test]  Epoch: 92	Loss: 0.071951	Acc: 8.3% (827/10000)
[Test]  Epoch: 93	Loss: 0.072175	Acc: 8.1% (810/10000)
[Test]  Epoch: 94	Loss: 0.072098	Acc: 8.0% (796/10000)
[Test]  Epoch: 95	Loss: 0.072070	Acc: 8.1% (812/10000)
[Test]  Epoch: 96	Loss: 0.072329	Acc: 8.0% (795/10000)
[Test]  Epoch: 97	Loss: 0.071647	Acc: 8.4% (844/10000)
[Test]  Epoch: 98	Loss: 0.072120	Acc: 8.3% (830/10000)
[Test]  Epoch: 99	Loss: 0.072161	Acc: 7.7% (772/10000)
[Test]  Epoch: 100	Loss: 0.072262	Acc: 7.8% (785/10000)
===========finish==========
['2024-08-19', '04:32:59.721824', '100', 'test', '0.07226221673488617', '7.85', '8.44']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.215650	Acc: 0.6% (64/10000)
[Test]  Epoch: 2	Loss: 0.081611	Acc: 2.5% (247/10000)
[Test]  Epoch: 3	Loss: 0.079630	Acc: 2.7% (274/10000)
[Test]  Epoch: 4	Loss: 0.078773	Acc: 3.1% (309/10000)
[Test]  Epoch: 5	Loss: 0.078519	Acc: 3.2% (319/10000)
[Test]  Epoch: 6	Loss: 0.077805	Acc: 3.5% (351/10000)
[Test]  Epoch: 7	Loss: 0.077226	Acc: 3.9% (391/10000)
[Test]  Epoch: 8	Loss: 0.077769	Acc: 3.9% (391/10000)
[Test]  Epoch: 9	Loss: 0.077563	Acc: 3.8% (375/10000)
[Test]  Epoch: 10	Loss: 0.076882	Acc: 3.9% (392/10000)
[Test]  Epoch: 11	Loss: 0.077448	Acc: 3.6% (359/10000)
[Test]  Epoch: 12	Loss: 0.076281	Acc: 4.4% (440/10000)
[Test]  Epoch: 13	Loss: 0.077524	Acc: 4.2% (416/10000)
[Test]  Epoch: 14	Loss: 0.076686	Acc: 4.4% (436/10000)
[Test]  Epoch: 15	Loss: 0.079791	Acc: 3.9% (386/10000)
[Test]  Epoch: 16	Loss: 0.078019	Acc: 4.2% (422/10000)
[Test]  Epoch: 17	Loss: 0.077063	Acc: 4.7% (467/10000)
[Test]  Epoch: 18	Loss: 0.076819	Acc: 4.4% (437/10000)
[Test]  Epoch: 19	Loss: 0.076423	Acc: 4.9% (489/10000)
[Test]  Epoch: 20	Loss: 0.077071	Acc: 5.0% (502/10000)
[Test]  Epoch: 21	Loss: 0.075916	Acc: 4.7% (471/10000)
[Test]  Epoch: 22	Loss: 0.075461	Acc: 5.3% (529/10000)
[Test]  Epoch: 23	Loss: 0.076243	Acc: 5.1% (507/10000)
[Test]  Epoch: 24	Loss: 0.076069	Acc: 5.0% (499/10000)
[Test]  Epoch: 25	Loss: 0.075063	Acc: 5.4% (536/10000)
[Test]  Epoch: 26	Loss: 0.075390	Acc: 5.5% (551/10000)
[Test]  Epoch: 27	Loss: 0.076140	Acc: 5.3% (533/10000)
[Test]  Epoch: 28	Loss: 0.074986	Acc: 5.5% (554/10000)
[Test]  Epoch: 29	Loss: 0.076523	Acc: 5.1% (513/10000)
[Test]  Epoch: 30	Loss: 0.074353	Acc: 6.1% (612/10000)
[Test]  Epoch: 31	Loss: 0.076607	Acc: 5.1% (507/10000)
[Test]  Epoch: 32	Loss: 0.074810	Acc: 6.1% (608/10000)
[Test]  Epoch: 33	Loss: 0.075957	Acc: 5.3% (529/10000)
[Test]  Epoch: 34	Loss: 0.075419	Acc: 5.9% (589/10000)
[Test]  Epoch: 35	Loss: 0.075523	Acc: 5.5% (553/10000)
[Test]  Epoch: 36	Loss: 0.075410	Acc: 6.0% (605/10000)
[Test]  Epoch: 37	Loss: 0.075528	Acc: 5.8% (578/10000)
[Test]  Epoch: 38	Loss: 0.075115	Acc: 5.7% (573/10000)
[Test]  Epoch: 39	Loss: 0.075775	Acc: 5.7% (567/10000)
[Test]  Epoch: 40	Loss: 0.074332	Acc: 6.1% (611/10000)
[Test]  Epoch: 41	Loss: 0.074408	Acc: 6.0% (601/10000)
[Test]  Epoch: 42	Loss: 0.074513	Acc: 5.8% (579/10000)
[Test]  Epoch: 43	Loss: 0.075101	Acc: 5.9% (587/10000)
[Test]  Epoch: 44	Loss: 0.074672	Acc: 6.4% (638/10000)
[Test]  Epoch: 45	Loss: 0.074968	Acc: 5.9% (591/10000)
[Test]  Epoch: 46	Loss: 0.074911	Acc: 6.1% (609/10000)
[Test]  Epoch: 47	Loss: 0.074534	Acc: 6.5% (655/10000)
[Test]  Epoch: 48	Loss: 0.074654	Acc: 6.3% (632/10000)
[Test]  Epoch: 49	Loss: 0.077097	Acc: 5.8% (576/10000)
[Test]  Epoch: 50	Loss: 0.075092	Acc: 6.3% (630/10000)
[Test]  Epoch: 51	Loss: 0.074834	Acc: 6.4% (640/10000)
[Test]  Epoch: 52	Loss: 0.074587	Acc: 6.2% (615/10000)
[Test]  Epoch: 53	Loss: 0.074829	Acc: 6.4% (638/10000)
[Test]  Epoch: 54	Loss: 0.074534	Acc: 6.2% (622/10000)
[Test]  Epoch: 55	Loss: 0.077016	Acc: 5.8% (578/10000)
[Test]  Epoch: 56	Loss: 0.074687	Acc: 6.5% (649/10000)
[Test]  Epoch: 57	Loss: 0.074983	Acc: 6.0% (595/10000)
[Test]  Epoch: 58	Loss: 0.075131	Acc: 6.3% (628/10000)
[Test]  Epoch: 59	Loss: 0.075809	Acc: 6.2% (616/10000)
[Test]  Epoch: 60	Loss: 0.074299	Acc: 6.7% (665/10000)
[Test]  Epoch: 61	Loss: 0.072883	Acc: 7.2% (725/10000)
[Test]  Epoch: 62	Loss: 0.072607	Acc: 7.4% (744/10000)
[Test]  Epoch: 63	Loss: 0.072809	Acc: 7.5% (750/10000)
[Test]  Epoch: 64	Loss: 0.072502	Acc: 7.7% (769/10000)
[Test]  Epoch: 65	Loss: 0.072448	Acc: 7.5% (748/10000)
[Test]  Epoch: 66	Loss: 0.072376	Acc: 7.9% (787/10000)
[Test]  Epoch: 67	Loss: 0.072684	Acc: 7.4% (741/10000)
[Test]  Epoch: 68	Loss: 0.072455	Acc: 7.2% (725/10000)
[Test]  Epoch: 69	Loss: 0.072622	Acc: 7.2% (723/10000)
[Test]  Epoch: 70	Loss: 0.072205	Acc: 7.9% (787/10000)
[Test]  Epoch: 71	Loss: 0.072412	Acc: 7.5% (747/10000)
[Test]  Epoch: 72	Loss: 0.072530	Acc: 7.6% (758/10000)
[Test]  Epoch: 73	Loss: 0.072121	Acc: 8.0% (801/10000)
[Test]  Epoch: 74	Loss: 0.072684	Acc: 7.4% (738/10000)
[Test]  Epoch: 75	Loss: 0.072365	Acc: 7.4% (744/10000)
[Test]  Epoch: 76	Loss: 0.072400	Acc: 7.5% (750/10000)
[Test]  Epoch: 77	Loss: 0.072399	Acc: 7.7% (769/10000)
[Test]  Epoch: 78	Loss: 0.072479	Acc: 7.7% (765/10000)
[Test]  Epoch: 79	Loss: 0.072517	Acc: 7.6% (760/10000)
[Test]  Epoch: 80	Loss: 0.072023	Acc: 8.4% (839/10000)
[Test]  Epoch: 81	Loss: 0.072494	Acc: 7.7% (766/10000)
[Test]  Epoch: 82	Loss: 0.072444	Acc: 7.6% (762/10000)
[Test]  Epoch: 83	Loss: 0.072507	Acc: 7.5% (752/10000)
[Test]  Epoch: 84	Loss: 0.072394	Acc: 7.8% (782/10000)
[Test]  Epoch: 85	Loss: 0.072146	Acc: 8.2% (815/10000)
[Test]  Epoch: 86	Loss: 0.072511	Acc: 7.6% (761/10000)
[Test]  Epoch: 87	Loss: 0.072636	Acc: 7.9% (790/10000)
[Test]  Epoch: 88	Loss: 0.072259	Acc: 7.7% (773/10000)
[Test]  Epoch: 89	Loss: 0.072376	Acc: 7.3% (727/10000)
[Test]  Epoch: 90	Loss: 0.072146	Acc: 7.6% (759/10000)
[Test]  Epoch: 91	Loss: 0.072424	Acc: 7.8% (785/10000)
[Test]  Epoch: 92	Loss: 0.072131	Acc: 8.2% (816/10000)
[Test]  Epoch: 93	Loss: 0.072513	Acc: 7.9% (791/10000)
[Test]  Epoch: 94	Loss: 0.072535	Acc: 7.6% (760/10000)
[Test]  Epoch: 95	Loss: 0.072705	Acc: 7.5% (754/10000)
[Test]  Epoch: 96	Loss: 0.072719	Acc: 7.8% (778/10000)
[Test]  Epoch: 97	Loss: 0.072087	Acc: 8.4% (836/10000)
[Test]  Epoch: 98	Loss: 0.072529	Acc: 7.7% (774/10000)
[Test]  Epoch: 99	Loss: 0.072226	Acc: 7.9% (792/10000)
[Test]  Epoch: 100	Loss: 0.072510	Acc: 8.0% (802/10000)
===========finish==========
['2024-08-19', '04:38:07.754302', '100', 'test', '0.07250981051921844', '8.02', '8.39']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.222985	Acc: 0.5% (54/10000)
[Test]  Epoch: 2	Loss: 0.082707	Acc: 1.9% (193/10000)
[Test]  Epoch: 3	Loss: 0.080365	Acc: 2.2% (225/10000)
[Test]  Epoch: 4	Loss: 0.081304	Acc: 2.6% (257/10000)
[Test]  Epoch: 5	Loss: 0.079536	Acc: 2.9% (290/10000)
[Test]  Epoch: 6	Loss: 0.079373	Acc: 3.0% (302/10000)
[Test]  Epoch: 7	Loss: 0.078164	Acc: 3.2% (317/10000)
[Test]  Epoch: 8	Loss: 0.079263	Acc: 2.9% (293/10000)
[Test]  Epoch: 9	Loss: 0.078344	Acc: 3.0% (304/10000)
[Test]  Epoch: 10	Loss: 0.077505	Acc: 3.5% (355/10000)
[Test]  Epoch: 11	Loss: 0.077720	Acc: 3.3% (334/10000)
[Test]  Epoch: 12	Loss: 0.077599	Acc: 3.4% (344/10000)
[Test]  Epoch: 13	Loss: 0.078507	Acc: 3.4% (336/10000)
[Test]  Epoch: 14	Loss: 0.078089	Acc: 3.8% (377/10000)
[Test]  Epoch: 15	Loss: 0.077257	Acc: 3.6% (365/10000)
[Test]  Epoch: 16	Loss: 0.076896	Acc: 4.1% (406/10000)
[Test]  Epoch: 17	Loss: 0.078722	Acc: 3.5% (350/10000)
[Test]  Epoch: 18	Loss: 0.078146	Acc: 3.6% (361/10000)
[Test]  Epoch: 19	Loss: 0.076766	Acc: 4.3% (433/10000)
[Test]  Epoch: 20	Loss: 0.077729	Acc: 4.2% (419/10000)
[Test]  Epoch: 21	Loss: 0.076334	Acc: 4.6% (459/10000)
[Test]  Epoch: 22	Loss: 0.076509	Acc: 4.5% (450/10000)
[Test]  Epoch: 23	Loss: 0.077151	Acc: 4.3% (435/10000)
[Test]  Epoch: 24	Loss: 0.076088	Acc: 4.6% (461/10000)
[Test]  Epoch: 25	Loss: 0.076247	Acc: 4.5% (455/10000)
[Test]  Epoch: 26	Loss: 0.076464	Acc: 5.0% (498/10000)
[Test]  Epoch: 27	Loss: 0.077521	Acc: 4.5% (447/10000)
[Test]  Epoch: 28	Loss: 0.076008	Acc: 5.2% (518/10000)
[Test]  Epoch: 29	Loss: 0.075983	Acc: 4.6% (457/10000)
[Test]  Epoch: 30	Loss: 0.075467	Acc: 4.9% (494/10000)
[Test]  Epoch: 31	Loss: 0.077036	Acc: 4.3% (431/10000)
[Test]  Epoch: 32	Loss: 0.075482	Acc: 5.3% (532/10000)
[Test]  Epoch: 33	Loss: 0.077067	Acc: 4.8% (479/10000)
[Test]  Epoch: 34	Loss: 0.076196	Acc: 4.9% (491/10000)
[Test]  Epoch: 35	Loss: 0.077526	Acc: 4.4% (440/10000)
[Test]  Epoch: 36	Loss: 0.076308	Acc: 5.2% (520/10000)
[Test]  Epoch: 37	Loss: 0.076335	Acc: 4.7% (466/10000)
[Test]  Epoch: 38	Loss: 0.076335	Acc: 5.3% (531/10000)
[Test]  Epoch: 39	Loss: 0.076430	Acc: 5.1% (514/10000)
[Test]  Epoch: 40	Loss: 0.076754	Acc: 4.8% (484/10000)
[Test]  Epoch: 41	Loss: 0.075144	Acc: 5.1% (510/10000)
[Test]  Epoch: 42	Loss: 0.075135	Acc: 5.2% (524/10000)
[Test]  Epoch: 43	Loss: 0.076439	Acc: 5.4% (536/10000)
[Test]  Epoch: 44	Loss: 0.074600	Acc: 5.7% (569/10000)
[Test]  Epoch: 45	Loss: 0.075536	Acc: 5.2% (523/10000)
[Test]  Epoch: 46	Loss: 0.075388	Acc: 5.5% (548/10000)
[Test]  Epoch: 47	Loss: 0.076345	Acc: 5.0% (498/10000)
[Test]  Epoch: 48	Loss: 0.074823	Acc: 5.8% (575/10000)
[Test]  Epoch: 49	Loss: 0.076604	Acc: 5.4% (541/10000)
[Test]  Epoch: 50	Loss: 0.077592	Acc: 5.1% (509/10000)
[Test]  Epoch: 51	Loss: 0.076365	Acc: 5.4% (544/10000)
[Test]  Epoch: 52	Loss: 0.074893	Acc: 6.2% (618/10000)
[Test]  Epoch: 53	Loss: 0.075474	Acc: 5.7% (566/10000)
[Test]  Epoch: 54	Loss: 0.074919	Acc: 6.0% (598/10000)
[Test]  Epoch: 55	Loss: 0.075588	Acc: 5.8% (577/10000)
[Test]  Epoch: 56	Loss: 0.075408	Acc: 5.9% (593/10000)
[Test]  Epoch: 57	Loss: 0.077616	Acc: 5.5% (548/10000)
[Test]  Epoch: 58	Loss: 0.076082	Acc: 5.4% (543/10000)
[Test]  Epoch: 59	Loss: 0.075908	Acc: 5.4% (543/10000)
[Test]  Epoch: 60	Loss: 0.075842	Acc: 5.8% (578/10000)
[Test]  Epoch: 61	Loss: 0.074503	Acc: 6.3% (635/10000)
[Test]  Epoch: 62	Loss: 0.074110	Acc: 6.3% (635/10000)
[Test]  Epoch: 63	Loss: 0.074381	Acc: 6.4% (638/10000)
[Test]  Epoch: 64	Loss: 0.074168	Acc: 6.3% (628/10000)
[Test]  Epoch: 65	Loss: 0.073858	Acc: 6.4% (640/10000)
[Test]  Epoch: 66	Loss: 0.073954	Acc: 6.8% (683/10000)
[Test]  Epoch: 67	Loss: 0.074227	Acc: 6.4% (644/10000)
[Test]  Epoch: 68	Loss: 0.073861	Acc: 6.8% (679/10000)
[Test]  Epoch: 69	Loss: 0.073984	Acc: 6.5% (647/10000)
[Test]  Epoch: 70	Loss: 0.073633	Acc: 7.0% (702/10000)
[Test]  Epoch: 71	Loss: 0.073904	Acc: 6.6% (660/10000)
[Test]  Epoch: 72	Loss: 0.073901	Acc: 6.6% (662/10000)
[Test]  Epoch: 73	Loss: 0.073813	Acc: 6.8% (678/10000)
[Test]  Epoch: 74	Loss: 0.074197	Acc: 6.4% (643/10000)
[Test]  Epoch: 75	Loss: 0.073867	Acc: 6.5% (649/10000)
[Test]  Epoch: 76	Loss: 0.073840	Acc: 6.8% (684/10000)
[Test]  Epoch: 77	Loss: 0.074035	Acc: 6.6% (663/10000)
[Test]  Epoch: 78	Loss: 0.073933	Acc: 6.5% (655/10000)
[Test]  Epoch: 79	Loss: 0.073920	Acc: 6.8% (677/10000)
[Test]  Epoch: 80	Loss: 0.073303	Acc: 7.1% (708/10000)
[Test]  Epoch: 81	Loss: 0.073865	Acc: 6.7% (667/10000)
[Test]  Epoch: 82	Loss: 0.073927	Acc: 6.5% (653/10000)
[Test]  Epoch: 83	Loss: 0.073957	Acc: 6.7% (674/10000)
[Test]  Epoch: 84	Loss: 0.073793	Acc: 6.5% (650/10000)
[Test]  Epoch: 85	Loss: 0.073773	Acc: 6.8% (684/10000)
[Test]  Epoch: 86	Loss: 0.073914	Acc: 6.5% (646/10000)
[Test]  Epoch: 87	Loss: 0.074012	Acc: 6.7% (671/10000)
[Test]  Epoch: 88	Loss: 0.073554	Acc: 7.1% (709/10000)
[Test]  Epoch: 89	Loss: 0.073926	Acc: 6.5% (646/10000)
[Test]  Epoch: 90	Loss: 0.073636	Acc: 6.7% (674/10000)
[Test]  Epoch: 91	Loss: 0.073974	Acc: 7.1% (707/10000)
[Test]  Epoch: 92	Loss: 0.073722	Acc: 6.8% (675/10000)
[Test]  Epoch: 93	Loss: 0.073696	Acc: 6.9% (689/10000)
[Test]  Epoch: 94	Loss: 0.073815	Acc: 6.9% (686/10000)
[Test]  Epoch: 95	Loss: 0.074046	Acc: 6.8% (678/10000)
[Test]  Epoch: 96	Loss: 0.074105	Acc: 6.8% (677/10000)
[Test]  Epoch: 97	Loss: 0.073606	Acc: 6.8% (685/10000)
[Test]  Epoch: 98	Loss: 0.074356	Acc: 6.7% (671/10000)
[Test]  Epoch: 99	Loss: 0.073863	Acc: 6.9% (690/10000)
[Test]  Epoch: 100	Loss: 0.074015	Acc: 6.8% (685/10000)
===========finish==========
['2024-08-19', '04:43:11.736240', '100', 'test', '0.07401467165946961', '6.85', '7.09']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.324991	Acc: 0.7% (65/10000)
[Test]  Epoch: 2	Loss: 0.084192	Acc: 1.6% (155/10000)
[Test]  Epoch: 3	Loss: 0.083581	Acc: 1.8% (183/10000)
[Test]  Epoch: 4	Loss: 0.080035	Acc: 2.5% (253/10000)
[Test]  Epoch: 5	Loss: 0.079588	Acc: 2.7% (269/10000)
[Test]  Epoch: 6	Loss: 0.079506	Acc: 2.7% (266/10000)
[Test]  Epoch: 7	Loss: 0.078749	Acc: 3.0% (301/10000)
[Test]  Epoch: 8	Loss: 0.078281	Acc: 3.1% (309/10000)
[Test]  Epoch: 9	Loss: 0.078741	Acc: 3.2% (323/10000)
[Test]  Epoch: 10	Loss: 0.078037	Acc: 3.3% (333/10000)
[Test]  Epoch: 11	Loss: 0.077658	Acc: 3.2% (321/10000)
[Test]  Epoch: 12	Loss: 0.077387	Acc: 3.5% (351/10000)
[Test]  Epoch: 13	Loss: 0.078272	Acc: 3.0% (299/10000)
[Test]  Epoch: 14	Loss: 0.077486	Acc: 3.8% (383/10000)
[Test]  Epoch: 15	Loss: 0.077121	Acc: 4.0% (404/10000)
[Test]  Epoch: 16	Loss: 0.076722	Acc: 4.1% (408/10000)
[Test]  Epoch: 17	Loss: 0.077094	Acc: 4.1% (406/10000)
[Test]  Epoch: 18	Loss: 0.078263	Acc: 3.5% (354/10000)
[Test]  Epoch: 19	Loss: 0.077098	Acc: 3.8% (381/10000)
[Test]  Epoch: 20	Loss: 0.076436	Acc: 4.5% (454/10000)
[Test]  Epoch: 21	Loss: 0.076273	Acc: 4.4% (442/10000)
[Test]  Epoch: 22	Loss: 0.076591	Acc: 4.4% (436/10000)
[Test]  Epoch: 23	Loss: 0.077397	Acc: 4.0% (400/10000)
[Test]  Epoch: 24	Loss: 0.076504	Acc: 4.4% (441/10000)
[Test]  Epoch: 25	Loss: 0.076028	Acc: 4.5% (448/10000)
[Test]  Epoch: 26	Loss: 0.075818	Acc: 4.7% (466/10000)
[Test]  Epoch: 27	Loss: 0.075832	Acc: 4.9% (490/10000)
[Test]  Epoch: 28	Loss: 0.075875	Acc: 4.8% (477/10000)
[Test]  Epoch: 29	Loss: 0.075962	Acc: 4.7% (471/10000)
[Test]  Epoch: 30	Loss: 0.075657	Acc: 5.3% (528/10000)
[Test]  Epoch: 31	Loss: 0.075772	Acc: 4.7% (470/10000)
[Test]  Epoch: 32	Loss: 0.075369	Acc: 5.0% (505/10000)
[Test]  Epoch: 33	Loss: 0.076067	Acc: 4.7% (474/10000)
[Test]  Epoch: 34	Loss: 0.075653	Acc: 5.1% (514/10000)
[Test]  Epoch: 35	Loss: 0.077664	Acc: 4.4% (437/10000)
[Test]  Epoch: 36	Loss: 0.075257	Acc: 5.2% (521/10000)
[Test]  Epoch: 37	Loss: 0.076572	Acc: 4.7% (468/10000)
[Test]  Epoch: 38	Loss: 0.076010	Acc: 5.0% (505/10000)
[Test]  Epoch: 39	Loss: 0.075830	Acc: 5.0% (505/10000)
[Test]  Epoch: 40	Loss: 0.075748	Acc: 4.8% (483/10000)
[Test]  Epoch: 41	Loss: 0.075245	Acc: 5.3% (530/10000)
[Test]  Epoch: 42	Loss: 0.075832	Acc: 5.0% (495/10000)
[Test]  Epoch: 43	Loss: 0.075060	Acc: 5.5% (547/10000)
[Test]  Epoch: 44	Loss: 0.075312	Acc: 5.5% (554/10000)
[Test]  Epoch: 45	Loss: 0.076321	Acc: 4.7% (473/10000)
[Test]  Epoch: 46	Loss: 0.075192	Acc: 5.6% (556/10000)
[Test]  Epoch: 47	Loss: 0.076068	Acc: 5.2% (516/10000)
[Test]  Epoch: 48	Loss: 0.075419	Acc: 5.7% (574/10000)
[Test]  Epoch: 49	Loss: 0.075527	Acc: 5.6% (559/10000)
[Test]  Epoch: 50	Loss: 0.075261	Acc: 5.7% (571/10000)
[Test]  Epoch: 51	Loss: 0.076774	Acc: 5.0% (496/10000)
[Test]  Epoch: 52	Loss: 0.075736	Acc: 5.8% (579/10000)
[Test]  Epoch: 53	Loss: 0.075175	Acc: 5.5% (555/10000)
[Test]  Epoch: 54	Loss: 0.075107	Acc: 5.5% (552/10000)
[Test]  Epoch: 55	Loss: 0.075606	Acc: 5.9% (589/10000)
[Test]  Epoch: 56	Loss: 0.075081	Acc: 5.7% (571/10000)
[Test]  Epoch: 57	Loss: 0.076218	Acc: 5.7% (565/10000)
[Test]  Epoch: 58	Loss: 0.076095	Acc: 5.2% (524/10000)
[Test]  Epoch: 59	Loss: 0.076031	Acc: 5.4% (542/10000)
[Test]  Epoch: 60	Loss: 0.076108	Acc: 6.0% (597/10000)
[Test]  Epoch: 61	Loss: 0.074328	Acc: 6.1% (611/10000)
[Test]  Epoch: 62	Loss: 0.073874	Acc: 6.6% (660/10000)
[Test]  Epoch: 63	Loss: 0.074135	Acc: 6.3% (635/10000)
[Test]  Epoch: 64	Loss: 0.074052	Acc: 6.4% (636/10000)
[Test]  Epoch: 65	Loss: 0.073820	Acc: 6.2% (615/10000)
[Test]  Epoch: 66	Loss: 0.073770	Acc: 6.8% (675/10000)
[Test]  Epoch: 67	Loss: 0.073974	Acc: 6.5% (651/10000)
[Test]  Epoch: 68	Loss: 0.073927	Acc: 6.6% (658/10000)
[Test]  Epoch: 69	Loss: 0.073947	Acc: 6.6% (661/10000)
[Test]  Epoch: 70	Loss: 0.073841	Acc: 6.8% (683/10000)
[Test]  Epoch: 71	Loss: 0.073945	Acc: 6.4% (643/10000)
[Test]  Epoch: 72	Loss: 0.073897	Acc: 6.3% (627/10000)
[Test]  Epoch: 73	Loss: 0.073850	Acc: 6.6% (664/10000)
[Test]  Epoch: 74	Loss: 0.073925	Acc: 6.5% (651/10000)
[Test]  Epoch: 75	Loss: 0.074002	Acc: 6.3% (630/10000)
[Test]  Epoch: 76	Loss: 0.073905	Acc: 6.2% (620/10000)
[Test]  Epoch: 77	Loss: 0.074178	Acc: 6.2% (619/10000)
[Test]  Epoch: 78	Loss: 0.074096	Acc: 6.7% (669/10000)
[Test]  Epoch: 79	Loss: 0.073713	Acc: 6.9% (690/10000)
[Test]  Epoch: 80	Loss: 0.073490	Acc: 6.7% (666/10000)
[Test]  Epoch: 81	Loss: 0.074051	Acc: 6.5% (645/10000)
[Test]  Epoch: 82	Loss: 0.073911	Acc: 6.7% (666/10000)
[Test]  Epoch: 83	Loss: 0.074131	Acc: 6.6% (663/10000)
[Test]  Epoch: 84	Loss: 0.074138	Acc: 6.2% (619/10000)
[Test]  Epoch: 85	Loss: 0.073782	Acc: 6.5% (654/10000)
[Test]  Epoch: 86	Loss: 0.074032	Acc: 6.7% (667/10000)
[Test]  Epoch: 87	Loss: 0.074069	Acc: 6.6% (657/10000)
[Test]  Epoch: 88	Loss: 0.073756	Acc: 6.8% (676/10000)
[Test]  Epoch: 89	Loss: 0.073929	Acc: 6.5% (646/10000)
[Test]  Epoch: 90	Loss: 0.073730	Acc: 6.4% (640/10000)
[Test]  Epoch: 91	Loss: 0.074138	Acc: 6.4% (637/10000)
[Test]  Epoch: 92	Loss: 0.073641	Acc: 6.5% (654/10000)
[Test]  Epoch: 93	Loss: 0.073982	Acc: 6.8% (684/10000)
[Test]  Epoch: 94	Loss: 0.074153	Acc: 6.7% (666/10000)
[Test]  Epoch: 95	Loss: 0.074160	Acc: 6.5% (646/10000)
[Test]  Epoch: 96	Loss: 0.074200	Acc: 6.5% (645/10000)
[Test]  Epoch: 97	Loss: 0.073636	Acc: 6.8% (680/10000)
[Test]  Epoch: 98	Loss: 0.073993	Acc: 6.7% (666/10000)
[Test]  Epoch: 99	Loss: 0.073985	Acc: 6.7% (672/10000)
[Test]  Epoch: 100	Loss: 0.074079	Acc: 6.7% (668/10000)
===========finish==========
['2024-08-19', '04:47:46.254951', '100', 'test', '0.07407923233509063', '6.68', '6.9']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight', 'features.40.weight', 'features.41.weight', 'classifier.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.110965	Acc: 0.5% (52/10000)
[Test]  Epoch: 2	Loss: 0.083657	Acc: 0.8% (75/10000)
[Test]  Epoch: 3	Loss: 0.083327	Acc: 0.9% (87/10000)
[Test]  Epoch: 4	Loss: 0.083224	Acc: 1.0% (104/10000)
[Test]  Epoch: 5	Loss: 0.083019	Acc: 0.9% (92/10000)
[Test]  Epoch: 6	Loss: 0.082839	Acc: 1.1% (105/10000)
[Test]  Epoch: 7	Loss: 0.082421	Acc: 1.8% (175/10000)
[Test]  Epoch: 8	Loss: 0.082377	Acc: 1.7% (171/10000)
[Test]  Epoch: 9	Loss: 0.081912	Acc: 1.9% (187/10000)
[Test]  Epoch: 10	Loss: 0.081419	Acc: 2.3% (230/10000)
[Test]  Epoch: 11	Loss: 0.081179	Acc: 2.3% (234/10000)
[Test]  Epoch: 12	Loss: 0.080910	Acc: 2.3% (228/10000)
[Test]  Epoch: 13	Loss: 0.080524	Acc: 2.4% (237/10000)
[Test]  Epoch: 14	Loss: 0.080160	Acc: 2.8% (283/10000)
[Test]  Epoch: 15	Loss: 0.080086	Acc: 2.7% (267/10000)
[Test]  Epoch: 16	Loss: 0.079701	Acc: 2.6% (264/10000)
[Test]  Epoch: 17	Loss: 0.079640	Acc: 2.9% (287/10000)
[Test]  Epoch: 18	Loss: 0.079196	Acc: 3.2% (319/10000)
[Test]  Epoch: 19	Loss: 0.079021	Acc: 3.0% (299/10000)
[Test]  Epoch: 20	Loss: 0.078811	Acc: 2.9% (290/10000)
[Test]  Epoch: 21	Loss: 0.078563	Acc: 3.2% (318/10000)
[Test]  Epoch: 22	Loss: 0.078628	Acc: 2.9% (292/10000)
[Test]  Epoch: 23	Loss: 0.078475	Acc: 2.9% (294/10000)
[Test]  Epoch: 24	Loss: 0.078202	Acc: 3.0% (303/10000)
[Test]  Epoch: 25	Loss: 0.078044	Acc: 3.0% (301/10000)
[Test]  Epoch: 26	Loss: 0.077907	Acc: 3.3% (333/10000)
[Test]  Epoch: 27	Loss: 0.077624	Acc: 3.5% (347/10000)
[Test]  Epoch: 28	Loss: 0.077616	Acc: 3.5% (349/10000)
[Test]  Epoch: 29	Loss: 0.077635	Acc: 3.4% (342/10000)
[Test]  Epoch: 30	Loss: 0.077615	Acc: 3.5% (353/10000)
[Test]  Epoch: 31	Loss: 0.077532	Acc: 3.4% (335/10000)
[Test]  Epoch: 32	Loss: 0.077143	Acc: 3.7% (373/10000)
[Test]  Epoch: 33	Loss: 0.077115	Acc: 3.7% (371/10000)
[Test]  Epoch: 34	Loss: 0.077499	Acc: 3.7% (373/10000)
[Test]  Epoch: 35	Loss: 0.077115	Acc: 3.6% (364/10000)
[Test]  Epoch: 36	Loss: 0.076971	Acc: 3.5% (355/10000)
[Test]  Epoch: 37	Loss: 0.076786	Acc: 3.6% (361/10000)
[Test]  Epoch: 38	Loss: 0.077157	Acc: 4.0% (400/10000)
[Test]  Epoch: 39	Loss: 0.076766	Acc: 4.2% (418/10000)
[Test]  Epoch: 40	Loss: 0.076520	Acc: 3.8% (380/10000)
[Test]  Epoch: 41	Loss: 0.076357	Acc: 3.8% (380/10000)
[Test]  Epoch: 42	Loss: 0.076464	Acc: 3.8% (380/10000)
[Test]  Epoch: 43	Loss: 0.076707	Acc: 3.8% (381/10000)
[Test]  Epoch: 44	Loss: 0.076031	Acc: 4.2% (419/10000)
[Test]  Epoch: 45	Loss: 0.076297	Acc: 3.6% (365/10000)
[Test]  Epoch: 46	Loss: 0.076153	Acc: 4.2% (419/10000)
[Test]  Epoch: 47	Loss: 0.076295	Acc: 4.1% (411/10000)
[Test]  Epoch: 48	Loss: 0.076412	Acc: 4.1% (407/10000)
[Test]  Epoch: 49	Loss: 0.076443	Acc: 4.0% (395/10000)
[Test]  Epoch: 50	Loss: 0.075979	Acc: 4.2% (422/10000)
[Test]  Epoch: 51	Loss: 0.076251	Acc: 4.3% (429/10000)
[Test]  Epoch: 52	Loss: 0.075923	Acc: 4.9% (493/10000)
[Test]  Epoch: 53	Loss: 0.075818	Acc: 4.4% (443/10000)
[Test]  Epoch: 54	Loss: 0.075804	Acc: 4.2% (424/10000)
[Test]  Epoch: 55	Loss: 0.075858	Acc: 4.3% (431/10000)
[Test]  Epoch: 56	Loss: 0.075640	Acc: 4.6% (459/10000)
[Test]  Epoch: 57	Loss: 0.075763	Acc: 4.5% (446/10000)
[Test]  Epoch: 58	Loss: 0.075757	Acc: 4.2% (419/10000)
[Test]  Epoch: 59	Loss: 0.075757	Acc: 4.5% (453/10000)
[Test]  Epoch: 60	Loss: 0.076475	Acc: 4.2% (425/10000)
[Test]  Epoch: 61	Loss: 0.075342	Acc: 4.8% (484/10000)
[Test]  Epoch: 62	Loss: 0.075058	Acc: 4.8% (478/10000)
[Test]  Epoch: 63	Loss: 0.075244	Acc: 4.7% (469/10000)
[Test]  Epoch: 64	Loss: 0.075017	Acc: 5.0% (495/10000)
[Test]  Epoch: 65	Loss: 0.075077	Acc: 4.8% (483/10000)
[Test]  Epoch: 66	Loss: 0.075121	Acc: 4.9% (489/10000)
[Test]  Epoch: 67	Loss: 0.075150	Acc: 4.5% (449/10000)
[Test]  Epoch: 68	Loss: 0.075023	Acc: 4.9% (489/10000)
[Test]  Epoch: 69	Loss: 0.075132	Acc: 4.8% (482/10000)
[Test]  Epoch: 70	Loss: 0.074785	Acc: 5.2% (516/10000)
[Test]  Epoch: 71	Loss: 0.074979	Acc: 4.6% (464/10000)
[Test]  Epoch: 72	Loss: 0.074902	Acc: 4.9% (486/10000)
[Test]  Epoch: 73	Loss: 0.075071	Acc: 4.9% (491/10000)
[Test]  Epoch: 74	Loss: 0.074924	Acc: 4.8% (484/10000)
[Test]  Epoch: 75	Loss: 0.074982	Acc: 4.8% (480/10000)
[Test]  Epoch: 76	Loss: 0.074953	Acc: 4.6% (464/10000)
[Test]  Epoch: 77	Loss: 0.074929	Acc: 5.2% (517/10000)
[Test]  Epoch: 78	Loss: 0.074936	Acc: 4.8% (484/10000)
[Test]  Epoch: 79	Loss: 0.074922	Acc: 5.1% (511/10000)
[Test]  Epoch: 80	Loss: 0.074627	Acc: 5.1% (512/10000)
[Test]  Epoch: 81	Loss: 0.074969	Acc: 5.0% (497/10000)
[Test]  Epoch: 82	Loss: 0.074849	Acc: 4.9% (486/10000)
[Test]  Epoch: 83	Loss: 0.074921	Acc: 4.8% (480/10000)
[Test]  Epoch: 84	Loss: 0.074873	Acc: 5.1% (512/10000)
[Test]  Epoch: 85	Loss: 0.074700	Acc: 5.0% (497/10000)
[Test]  Epoch: 86	Loss: 0.074803	Acc: 5.1% (507/10000)
[Test]  Epoch: 87	Loss: 0.074934	Acc: 4.9% (493/10000)
[Test]  Epoch: 88	Loss: 0.074661	Acc: 5.1% (507/10000)
[Test]  Epoch: 89	Loss: 0.074658	Acc: 4.9% (486/10000)
[Test]  Epoch: 90	Loss: 0.074792	Acc: 5.0% (500/10000)
[Test]  Epoch: 91	Loss: 0.074802	Acc: 5.0% (497/10000)
[Test]  Epoch: 92	Loss: 0.074753	Acc: 4.9% (486/10000)
[Test]  Epoch: 93	Loss: 0.074698	Acc: 5.2% (515/10000)
[Test]  Epoch: 94	Loss: 0.074784	Acc: 5.2% (515/10000)
[Test]  Epoch: 95	Loss: 0.074738	Acc: 5.3% (531/10000)
[Test]  Epoch: 96	Loss: 0.074950	Acc: 4.9% (489/10000)
[Test]  Epoch: 97	Loss: 0.074515	Acc: 5.3% (530/10000)
[Test]  Epoch: 98	Loss: 0.074857	Acc: 5.0% (501/10000)
[Test]  Epoch: 99	Loss: 0.074767	Acc: 5.0% (503/10000)
[Test]  Epoch: 100	Loss: 0.074765	Acc: 5.3% (535/10000)
===========finish==========
['2024-08-19', '04:52:22.723008', '100', 'test', '0.07476477890014649', '5.35', '5.35']
result path:  /home/gpu2/jbw/other_XAI_mean/knockoffnets/ms_elastictrainer_result_resnet50.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.1 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.2 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.3 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.4 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.5 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.6 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.7 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.8 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.9 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 1 --channel_percent -1
Traceback (most recent call last):
  File "/home/gpu2/jbw/other_XAI_mean/knockoffnets/./knockoff/adversary/train.py", line 19, in <module>
    import knockoff.models.imagenet
ModuleNotFoundError: No module named 'knockoff'
===========finish==========
['2024-08-15', '17:59:57.359867', '100', 'test', '0.08741831188201904', '11.4', '11.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0 --channel_percent -1
result path:  /home/gpu2/jbw/other_XAI_mean/knockoffnets/ms_elastictrainer_result_resnet50.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/cifar100-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.bias', 0.0), ('layer4.2.bn3.weight', -6.51259810524607e-08), ('layer4.2.bn1.weight', -6.189943178469548e-06), ('layer4.2.bn2.weight', -2.6232621166855097e-05), ('layer4.2.conv3.weight', -0.04097927734255791), ('layer4.1.bn2.weight', -0.07906759530305862), ('layer4.2.conv1.weight', -0.082496777176857), ('layer4.2.conv2.weight', -0.2527070939540863), ('layer4.1.bn1.weight', -1.9460922479629517), ('layer4.1.bn3.weight', -35.04563903808594), ('layer1.2.bn1.weight', -53.0912971496582), ('layer1.1.bn1.weight', -57.2817268371582), ('layer1.1.bn2.weight', -67.19623565673828), ('layer1.0.bn1.weight', -74.99185180664062), ('layer1.0.bn2.weight', -76.69172668457031), ('layer1.2.bn2.weight', -78.68086242675781), ('layer2.2.bn1.weight', -134.81333923339844), ('layer2.1.bn1.weight', -139.69558715820312), ('layer2.3.bn1.weight', -144.12376403808594), ('layer2.2.bn2.weight', -188.96249389648438), ('layer2.1.bn2.weight', -197.51991271972656), ('layer2.3.bn2.weight', -242.89801025390625), ('layer3.3.bn1.weight', -284.9638977050781), ('layer1.2.bn3.weight', -289.85528564453125), ('layer3.5.bn2.weight', -290.999755859375), ('layer3.5.bn1.weight', -294.98492431640625), ('layer1.1.bn3.weight', -300.7429504394531), ('layer3.3.bn2.weight', -315.2882385253906), ('layer3.4.bn1.weight', -359.4571533203125), ('layer2.0.bn2.weight', -380.23797607421875), ('layer2.0.bn1.weight', -401.15008544921875), ('layer3.4.bn2.weight', -403.1504211425781), ('layer3.2.bn1.weight', -416.1991882324219), ('layer1.0.bn3.weight', -450.5668640136719), ('layer3.2.bn2.weight', -514.076416015625), ('layer3.1.bn1.weight', -542.324951171875), ('layer4.0.downsample.1.weight', -618.3447265625), ('bn1.weight', -633.3642578125), ('layer3.1.bn2.weight', -655.4685668945312), ('layer4.0.bn3.weight', -749.6220703125), ('layer1.0.downsample.1.weight', -893.9508666992188), ('layer2.1.bn3.weight', -897.5707397460938), ('layer2.3.bn3.weight', -966.005126953125), ('layer3.0.bn2.weight', -1029.81884765625), ('layer2.2.bn3.weight', -1052.3306884765625), ('layer3.0.bn1.weight', -1281.67236328125), ('layer3.3.bn3.weight', -1359.9683837890625), ('layer3.5.bn3.weight', -1467.515380859375), ('layer3.4.bn3.weight', -1666.2435302734375), ('layer4.1.conv3.weight', -1708.160400390625), ('layer2.0.bn3.weight', -1839.81787109375), ('layer3.2.bn3.weight', -2121.152587890625), ('layer2.0.downsample.1.weight', -2301.059814453125), ('layer3.1.bn3.weight', -2661.040771484375), ('layer3.0.bn3.weight', -5688.2216796875), ('layer3.0.downsample.1.weight', -5888.62646484375), ('layer4.0.bn2.weight', -6690.5712890625), ('layer4.0.bn1.weight', -6735.63330078125), ('layer4.1.conv2.weight', -7857.9677734375), ('layer1.0.conv1.weight', -49837.42578125), ('layer4.1.conv1.weight', -51470.34375), ('layer1.2.conv3.weight', -64686.12890625), ('layer1.1.conv1.weight', -69018.0), ('layer1.1.conv3.weight', -74489.2265625), ('layer1.2.conv1.weight', -76773.59375), ('layer1.0.conv3.weight', -122254.609375), ('layer1.1.conv2.weight', -288661.875), ('layer1.2.conv2.weight', -362028.5625), ('layer1.0.conv2.weight', -376845.71875), ('layer2.1.conv1.weight', -391376.6875), ('layer2.2.conv1.weight', -584522.1875), ('layer2.1.conv3.weight', -637813.8125), ('conv1.weight', -664349.0), ('layer2.0.conv1.weight', -700906.375), ('layer2.3.conv1.weight', -749144.6875), ('layer2.2.conv3.weight', -783827.375), ('layer1.0.downsample.0.weight', -796684.25), ('layer2.3.conv3.weight', -836084.5), ('layer2.0.conv3.weight', -1376145.375), ('layer3.5.conv3.weight', -1494173.75), ('layer2.1.conv2.weight', -1929269.25), ('layer3.3.conv3.weight', -1945648.25), ('layer3.3.conv1.weight', -2263681.75), ('layer3.4.conv3.weight', -2293809.0), ('layer3.5.conv1.weight', -2340772.0), ('layer2.2.conv2.weight', -2559930.75), ('layer3.4.conv1.weight', -2901685.5), ('layer3.2.conv1.weight', -3102873.25), ('layer3.2.conv3.weight', -3184492.5), ('layer2.3.conv2.weight', -3297896.5), ('layer3.1.conv1.weight', -3996620.0), ('layer2.0.conv2.weight', -4049906.5), ('last_linear.weight', -4242942.0), ('layer3.1.conv3.weight', -4452423.0), ('layer2.0.downsample.0.weight', -4773552.5), ('layer3.5.conv2.weight', -4949539.0), ('layer3.0.conv1.weight', -5732912.0), ('layer3.3.conv2.weight', -6310019.0), ('layer3.4.conv2.weight', -8114661.0), ('layer3.2.conv2.weight', -10536796.0), ('layer3.0.conv3.weight', -13081917.0), ('layer3.1.conv2.weight', -15175996.0), ('layer3.0.downsample.0.weight', -27106686.0), ('layer3.0.conv2.weight', -37330496.0), ('layer4.0.conv1.weight', -49213492.0), ('layer4.0.conv3.weight', -94581792.0), ('layer4.0.downsample.0.weight', -103260760.0), ('layer4.0.conv2.weight', -337931360.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('last_linear.bias', 0.0), ('layer4.2.conv3.weight', -0.04097927734255791), ('layer4.2.conv1.weight', -0.082496777176857), ('layer4.2.conv2.weight', -0.2527070939540863), ('layer4.1.conv3.weight', -1708.160400390625), ('layer4.1.conv2.weight', -7857.9677734375), ('layer1.0.conv1.weight', -49837.42578125), ('layer4.1.conv1.weight', -51470.34375), ('layer1.2.conv3.weight', -64686.12890625), ('layer1.1.conv1.weight', -69018.0), ('layer1.1.conv3.weight', -74489.2265625), ('layer1.2.conv1.weight', -76773.59375), ('layer1.0.conv3.weight', -122254.609375), ('layer1.1.conv2.weight', -288661.875), ('layer1.2.conv2.weight', -362028.5625), ('layer1.0.conv2.weight', -376845.71875), ('layer2.1.conv1.weight', -391376.6875), ('layer2.2.conv1.weight', -584522.1875), ('layer2.1.conv3.weight', -637813.8125), ('conv1.weight', -664349.0), ('layer2.0.conv1.weight', -700906.375), ('layer2.3.conv1.weight', -749144.6875), ('layer2.2.conv3.weight', -783827.375), ('layer2.3.conv3.weight', -836084.5), ('layer2.0.conv3.weight', -1376145.375), ('layer3.5.conv3.weight', -1494173.75), ('layer2.1.conv2.weight', -1929269.25), ('layer3.3.conv3.weight', -1945648.25), ('layer3.3.conv1.weight', -2263681.75), ('layer3.4.conv3.weight', -2293809.0), ('layer3.5.conv1.weight', -2340772.0), ('layer2.2.conv2.weight', -2559930.75), ('layer3.4.conv1.weight', -2901685.5), ('layer3.2.conv1.weight', -3102873.25), ('layer3.2.conv3.weight', -3184492.5), ('layer2.3.conv2.weight', -3297896.5), ('layer3.1.conv1.weight', -3996620.0), ('layer2.0.conv2.weight', -4049906.5), ('last_linear.weight', -4242942.0), ('layer3.1.conv3.weight', -4452423.0), ('layer3.5.conv2.weight', -4949539.0), ('layer3.0.conv1.weight', -5732912.0), ('layer3.3.conv2.weight', -6310019.0), ('layer3.4.conv2.weight', -8114661.0), ('layer3.2.conv2.weight', -10536796.0), ('layer3.0.conv3.weight', -13081917.0), ('layer3.1.conv2.weight', -15175996.0), ('layer3.0.conv2.weight', -37330496.0), ('layer4.0.conv1.weight', -49213492.0), ('layer4.0.conv3.weight', -94581792.0), ('layer4.0.conv2.weight', -337931360.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.026456	Acc: 63.2% (6316/10000)
[Test]  Epoch: 2	Loss: 0.026503	Acc: 63.0% (6302/10000)
[Test]  Epoch: 3	Loss: 0.026499	Acc: 62.8% (6276/10000)
[Test]  Epoch: 4	Loss: 0.026593	Acc: 62.8% (6276/10000)
[Test]  Epoch: 5	Loss: 0.026561	Acc: 62.8% (6276/10000)
[Test]  Epoch: 6	Loss: 0.026493	Acc: 62.7% (6269/10000)
[Test]  Epoch: 7	Loss: 0.026479	Acc: 62.6% (6259/10000)
[Test]  Epoch: 8	Loss: 0.026441	Acc: 62.8% (6284/10000)
[Test]  Epoch: 9	Loss: 0.026405	Acc: 62.8% (6279/10000)
[Test]  Epoch: 10	Loss: 0.026343	Acc: 62.9% (6288/10000)
[Test]  Epoch: 11	Loss: 0.026256	Acc: 62.9% (6293/10000)
[Test]  Epoch: 12	Loss: 0.026262	Acc: 62.8% (6284/10000)
[Test]  Epoch: 13	Loss: 0.026240	Acc: 63.0% (6305/10000)
[Test]  Epoch: 14	Loss: 0.026265	Acc: 63.0% (6300/10000)
[Test]  Epoch: 15	Loss: 0.026215	Acc: 63.1% (6315/10000)
[Test]  Epoch: 16	Loss: 0.026260	Acc: 63.1% (6310/10000)
[Test]  Epoch: 17	Loss: 0.026108	Acc: 63.2% (6322/10000)
[Test]  Epoch: 18	Loss: 0.026188	Acc: 63.1% (6308/10000)
[Test]  Epoch: 19	Loss: 0.026095	Acc: 63.1% (6315/10000)
[Test]  Epoch: 20	Loss: 0.025983	Acc: 63.5% (6345/10000)
[Test]  Epoch: 21	Loss: 0.026050	Acc: 63.2% (6318/10000)
[Test]  Epoch: 22	Loss: 0.025975	Acc: 63.2% (6323/10000)
[Test]  Epoch: 23	Loss: 0.026053	Acc: 63.1% (6312/10000)
[Test]  Epoch: 24	Loss: 0.026030	Acc: 63.0% (6302/10000)
[Test]  Epoch: 25	Loss: 0.026017	Acc: 63.5% (6345/10000)
[Test]  Epoch: 26	Loss: 0.026041	Acc: 63.2% (6316/10000)
[Test]  Epoch: 27	Loss: 0.026122	Acc: 63.0% (6304/10000)
[Test]  Epoch: 28	Loss: 0.025994	Acc: 63.2% (6323/10000)
[Test]  Epoch: 29	Loss: 0.025997	Acc: 63.3% (6327/10000)
[Test]  Epoch: 30	Loss: 0.025983	Acc: 63.2% (6320/10000)
[Test]  Epoch: 31	Loss: 0.025949	Acc: 63.2% (6323/10000)
[Test]  Epoch: 32	Loss: 0.025949	Acc: 63.4% (6335/10000)
[Test]  Epoch: 33	Loss: 0.025989	Acc: 63.0% (6300/10000)
[Test]  Epoch: 34	Loss: 0.026004	Acc: 63.1% (6312/10000)
[Test]  Epoch: 35	Loss: 0.025905	Acc: 63.3% (6331/10000)
[Test]  Epoch: 36	Loss: 0.025889	Acc: 63.3% (6332/10000)
[Test]  Epoch: 37	Loss: 0.025895	Acc: 63.2% (6322/10000)
[Test]  Epoch: 38	Loss: 0.025967	Acc: 63.1% (6315/10000)
[Test]  Epoch: 39	Loss: 0.025975	Acc: 63.3% (6333/10000)
[Test]  Epoch: 40	Loss: 0.025893	Acc: 63.3% (6334/10000)
[Test]  Epoch: 41	Loss: 0.025871	Acc: 63.2% (6321/10000)
[Test]  Epoch: 42	Loss: 0.025830	Acc: 63.4% (6338/10000)
[Test]  Epoch: 43	Loss: 0.025925	Acc: 63.5% (6348/10000)
[Test]  Epoch: 44	Loss: 0.025913	Acc: 63.4% (6342/10000)
[Test]  Epoch: 45	Loss: 0.025822	Acc: 63.3% (6330/10000)
[Test]  Epoch: 46	Loss: 0.025782	Acc: 63.3% (6327/10000)
[Test]  Epoch: 47	Loss: 0.025884	Acc: 63.2% (6325/10000)
[Test]  Epoch: 48	Loss: 0.025839	Acc: 63.3% (6330/10000)
[Test]  Epoch: 49	Loss: 0.025819	Acc: 63.4% (6342/10000)
[Test]  Epoch: 50	Loss: 0.025806	Acc: 63.4% (6342/10000)
[Test]  Epoch: 51	Loss: 0.025913	Acc: 63.2% (6316/10000)
[Test]  Epoch: 52	Loss: 0.025790	Acc: 63.4% (6337/10000)
[Test]  Epoch: 53	Loss: 0.025763	Acc: 63.4% (6340/10000)
[Test]  Epoch: 54	Loss: 0.025823	Acc: 63.3% (6327/10000)
[Test]  Epoch: 55	Loss: 0.025735	Acc: 63.5% (6348/10000)
[Test]  Epoch: 56	Loss: 0.025834	Acc: 63.1% (6312/10000)
[Test]  Epoch: 57	Loss: 0.025814	Acc: 63.4% (6338/10000)
[Test]  Epoch: 58	Loss: 0.025739	Acc: 63.5% (6351/10000)
[Test]  Epoch: 59	Loss: 0.025809	Acc: 63.3% (6332/10000)
[Test]  Epoch: 60	Loss: 0.025812	Acc: 63.3% (6328/10000)
[Test]  Epoch: 61	Loss: 0.025822	Acc: 63.4% (6340/10000)
[Test]  Epoch: 62	Loss: 0.025828	Acc: 63.2% (6325/10000)
[Test]  Epoch: 63	Loss: 0.025765	Acc: 63.5% (6346/10000)
[Test]  Epoch: 64	Loss: 0.025772	Acc: 63.3% (6332/10000)
[Test]  Epoch: 65	Loss: 0.025699	Acc: 63.3% (6329/10000)
[Test]  Epoch: 66	Loss: 0.025769	Acc: 63.5% (6347/10000)
[Test]  Epoch: 67	Loss: 0.025810	Acc: 63.2% (6316/10000)
[Test]  Epoch: 68	Loss: 0.025742	Acc: 63.4% (6335/10000)
[Test]  Epoch: 69	Loss: 0.025773	Acc: 63.4% (6342/10000)
[Test]  Epoch: 70	Loss: 0.025731	Acc: 63.2% (6323/10000)
[Test]  Epoch: 71	Loss: 0.025747	Acc: 63.5% (6345/10000)
[Test]  Epoch: 72	Loss: 0.025704	Acc: 63.3% (6332/10000)
[Test]  Epoch: 73	Loss: 0.025746	Acc: 63.5% (6351/10000)
[Test]  Epoch: 74	Loss: 0.025740	Acc: 63.5% (6348/10000)
[Test]  Epoch: 75	Loss: 0.025682	Acc: 63.5% (6350/10000)
[Test]  Epoch: 76	Loss: 0.025798	Acc: 63.1% (6312/10000)
[Test]  Epoch: 77	Loss: 0.025829	Acc: 63.2% (6317/10000)
[Test]  Epoch: 78	Loss: 0.025751	Acc: 63.3% (6331/10000)
[Test]  Epoch: 79	Loss: 0.025830	Acc: 63.3% (6330/10000)
[Test]  Epoch: 80	Loss: 0.025778	Acc: 63.4% (6340/10000)
[Test]  Epoch: 81	Loss: 0.025781	Acc: 63.4% (6339/10000)
[Test]  Epoch: 82	Loss: 0.025774	Acc: 63.1% (6313/10000)
[Test]  Epoch: 83	Loss: 0.025740	Acc: 63.2% (6321/10000)
[Test]  Epoch: 84	Loss: 0.025693	Acc: 63.4% (6339/10000)
[Test]  Epoch: 85	Loss: 0.025776	Acc: 63.5% (6345/10000)
[Test]  Epoch: 86	Loss: 0.025783	Acc: 63.4% (6339/10000)
[Test]  Epoch: 87	Loss: 0.025739	Acc: 63.5% (6352/10000)
[Test]  Epoch: 88	Loss: 0.025804	Acc: 63.3% (6333/10000)
[Test]  Epoch: 89	Loss: 0.025759	Acc: 63.3% (6327/10000)
[Test]  Epoch: 90	Loss: 0.025693	Acc: 63.4% (6344/10000)
[Test]  Epoch: 91	Loss: 0.025698	Acc: 63.4% (6344/10000)
[Test]  Epoch: 92	Loss: 0.025808	Acc: 63.4% (6336/10000)
[Test]  Epoch: 93	Loss: 0.025712	Acc: 63.6% (6359/10000)
[Test]  Epoch: 94	Loss: 0.025694	Acc: 63.4% (6339/10000)
[Test]  Epoch: 95	Loss: 0.025789	Acc: 63.4% (6340/10000)
[Test]  Epoch: 96	Loss: 0.025747	Acc: 63.5% (6349/10000)
[Test]  Epoch: 97	Loss: 0.025730	Acc: 63.4% (6335/10000)
[Test]  Epoch: 98	Loss: 0.025711	Acc: 63.5% (6355/10000)
[Test]  Epoch: 99	Loss: 0.025790	Acc: 63.4% (6337/10000)
[Test]  Epoch: 100	Loss: 0.025780	Acc: 63.5% (6345/10000)
===========finish==========
['2024-08-19', '16:19:59.451744', '100', 'test', '0.025779516988992692', '63.45', '63.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.038678	Acc: 54.3% (5432/10000)
[Test]  Epoch: 2	Loss: 0.035249	Acc: 57.7% (5769/10000)
[Test]  Epoch: 3	Loss: 0.034341	Acc: 58.4% (5835/10000)
[Test]  Epoch: 4	Loss: 0.033454	Acc: 59.0% (5898/10000)
[Test]  Epoch: 5	Loss: 0.033170	Acc: 59.2% (5917/10000)
[Test]  Epoch: 6	Loss: 0.032405	Acc: 59.7% (5972/10000)
[Test]  Epoch: 7	Loss: 0.032289	Acc: 59.6% (5960/10000)
[Test]  Epoch: 8	Loss: 0.032152	Acc: 59.8% (5981/10000)
[Test]  Epoch: 9	Loss: 0.032013	Acc: 59.8% (5979/10000)
[Test]  Epoch: 10	Loss: 0.031878	Acc: 59.9% (5991/10000)
[Test]  Epoch: 11	Loss: 0.031433	Acc: 59.8% (5976/10000)
[Test]  Epoch: 12	Loss: 0.031348	Acc: 60.1% (6009/10000)
[Test]  Epoch: 13	Loss: 0.031360	Acc: 60.1% (6015/10000)
[Test]  Epoch: 14	Loss: 0.031183	Acc: 59.7% (5968/10000)
[Test]  Epoch: 15	Loss: 0.031148	Acc: 60.1% (6013/10000)
[Test]  Epoch: 16	Loss: 0.031138	Acc: 60.0% (5997/10000)
[Test]  Epoch: 17	Loss: 0.030715	Acc: 60.1% (6012/10000)
[Test]  Epoch: 18	Loss: 0.030965	Acc: 59.8% (5984/10000)
[Test]  Epoch: 19	Loss: 0.030935	Acc: 59.5% (5955/10000)
[Test]  Epoch: 20	Loss: 0.030589	Acc: 60.0% (5997/10000)
[Test]  Epoch: 21	Loss: 0.030400	Acc: 59.9% (5989/10000)
[Test]  Epoch: 22	Loss: 0.030267	Acc: 60.1% (6015/10000)
[Test]  Epoch: 23	Loss: 0.030270	Acc: 60.1% (6010/10000)
[Test]  Epoch: 24	Loss: 0.030128	Acc: 60.2% (6020/10000)
[Test]  Epoch: 25	Loss: 0.030046	Acc: 60.1% (6012/10000)
[Test]  Epoch: 26	Loss: 0.030031	Acc: 60.0% (6001/10000)
[Test]  Epoch: 27	Loss: 0.029904	Acc: 60.0% (6004/10000)
[Test]  Epoch: 28	Loss: 0.029723	Acc: 60.0% (6002/10000)
[Test]  Epoch: 29	Loss: 0.029697	Acc: 60.2% (6019/10000)
[Test]  Epoch: 30	Loss: 0.029755	Acc: 60.5% (6052/10000)
[Test]  Epoch: 31	Loss: 0.029466	Acc: 60.4% (6039/10000)
[Test]  Epoch: 32	Loss: 0.029416	Acc: 60.7% (6066/10000)
[Test]  Epoch: 33	Loss: 0.029341	Acc: 60.7% (6069/10000)
[Test]  Epoch: 34	Loss: 0.029349	Acc: 60.8% (6077/10000)
[Test]  Epoch: 35	Loss: 0.029163	Acc: 60.8% (6080/10000)
[Test]  Epoch: 36	Loss: 0.029084	Acc: 60.7% (6068/10000)
[Test]  Epoch: 37	Loss: 0.028915	Acc: 60.6% (6059/10000)
[Test]  Epoch: 38	Loss: 0.028922	Acc: 60.9% (6086/10000)
[Test]  Epoch: 39	Loss: 0.028998	Acc: 60.7% (6069/10000)
[Test]  Epoch: 40	Loss: 0.028931	Acc: 60.8% (6075/10000)
[Test]  Epoch: 41	Loss: 0.028851	Acc: 60.8% (6083/10000)
[Test]  Epoch: 42	Loss: 0.028723	Acc: 61.0% (6100/10000)
[Test]  Epoch: 43	Loss: 0.028876	Acc: 60.9% (6088/10000)
[Test]  Epoch: 44	Loss: 0.028567	Acc: 61.1% (6107/10000)
[Test]  Epoch: 45	Loss: 0.028520	Acc: 61.1% (6112/10000)
[Test]  Epoch: 46	Loss: 0.028393	Acc: 61.2% (6119/10000)
[Test]  Epoch: 47	Loss: 0.028531	Acc: 61.2% (6121/10000)
[Test]  Epoch: 48	Loss: 0.028308	Acc: 61.2% (6117/10000)
[Test]  Epoch: 49	Loss: 0.028214	Acc: 61.3% (6129/10000)
[Test]  Epoch: 50	Loss: 0.028195	Acc: 61.1% (6113/10000)
[Test]  Epoch: 51	Loss: 0.028258	Acc: 61.0% (6105/10000)
[Test]  Epoch: 52	Loss: 0.028042	Acc: 61.3% (6131/10000)
[Test]  Epoch: 53	Loss: 0.028060	Acc: 61.4% (6140/10000)
[Test]  Epoch: 54	Loss: 0.028211	Acc: 61.0% (6100/10000)
[Test]  Epoch: 55	Loss: 0.028106	Acc: 61.0% (6103/10000)
[Test]  Epoch: 56	Loss: 0.028074	Acc: 61.0% (6095/10000)
[Test]  Epoch: 57	Loss: 0.027991	Acc: 60.8% (6081/10000)
[Test]  Epoch: 58	Loss: 0.027888	Acc: 61.2% (6124/10000)
[Test]  Epoch: 59	Loss: 0.027851	Acc: 60.9% (6086/10000)
[Test]  Epoch: 60	Loss: 0.027862	Acc: 61.0% (6097/10000)
[Test]  Epoch: 61	Loss: 0.028001	Acc: 61.2% (6119/10000)
[Test]  Epoch: 62	Loss: 0.028019	Acc: 61.1% (6108/10000)
[Test]  Epoch: 63	Loss: 0.027968	Acc: 61.2% (6120/10000)
[Test]  Epoch: 64	Loss: 0.027908	Acc: 61.2% (6120/10000)
[Test]  Epoch: 65	Loss: 0.027879	Acc: 61.1% (6110/10000)
[Test]  Epoch: 66	Loss: 0.027791	Acc: 61.3% (6126/10000)
[Test]  Epoch: 67	Loss: 0.027915	Acc: 61.0% (6096/10000)
[Test]  Epoch: 68	Loss: 0.027959	Acc: 61.1% (6115/10000)
[Test]  Epoch: 69	Loss: 0.027875	Acc: 61.2% (6122/10000)
[Test]  Epoch: 70	Loss: 0.027848	Acc: 61.2% (6123/10000)
[Test]  Epoch: 71	Loss: 0.027907	Acc: 61.3% (6134/10000)
[Test]  Epoch: 72	Loss: 0.027993	Acc: 61.1% (6107/10000)
[Test]  Epoch: 73	Loss: 0.027741	Acc: 61.1% (6115/10000)
[Test]  Epoch: 74	Loss: 0.027859	Acc: 61.2% (6118/10000)
[Test]  Epoch: 75	Loss: 0.027811	Acc: 61.3% (6130/10000)
[Test]  Epoch: 76	Loss: 0.027995	Acc: 61.2% (6122/10000)
[Test]  Epoch: 77	Loss: 0.027929	Acc: 61.2% (6118/10000)
[Test]  Epoch: 78	Loss: 0.027852	Acc: 61.1% (6115/10000)
[Test]  Epoch: 79	Loss: 0.027822	Acc: 61.2% (6119/10000)
[Test]  Epoch: 80	Loss: 0.027847	Acc: 61.1% (6112/10000)
[Test]  Epoch: 81	Loss: 0.027844	Acc: 61.1% (6110/10000)
[Test]  Epoch: 82	Loss: 0.027818	Acc: 61.0% (6104/10000)
[Test]  Epoch: 83	Loss: 0.027775	Acc: 61.3% (6126/10000)
[Test]  Epoch: 84	Loss: 0.027727	Acc: 61.1% (6115/10000)
[Test]  Epoch: 85	Loss: 0.027796	Acc: 61.3% (6128/10000)
[Test]  Epoch: 86	Loss: 0.027875	Acc: 61.4% (6137/10000)
[Test]  Epoch: 87	Loss: 0.027822	Acc: 61.1% (6115/10000)
[Test]  Epoch: 88	Loss: 0.027721	Acc: 61.1% (6106/10000)
[Test]  Epoch: 89	Loss: 0.027826	Acc: 61.3% (6130/10000)
[Test]  Epoch: 90	Loss: 0.027699	Acc: 61.2% (6123/10000)
[Test]  Epoch: 91	Loss: 0.027710	Acc: 61.2% (6122/10000)
[Test]  Epoch: 92	Loss: 0.027850	Acc: 61.4% (6135/10000)
[Test]  Epoch: 93	Loss: 0.027699	Acc: 61.4% (6140/10000)
[Test]  Epoch: 94	Loss: 0.027697	Acc: 61.2% (6117/10000)
[Test]  Epoch: 95	Loss: 0.027775	Acc: 60.9% (6093/10000)
[Test]  Epoch: 96	Loss: 0.027811	Acc: 61.2% (6121/10000)
[Test]  Epoch: 97	Loss: 0.027740	Acc: 61.3% (6129/10000)
[Test]  Epoch: 98	Loss: 0.027662	Acc: 61.4% (6144/10000)
[Test]  Epoch: 99	Loss: 0.027716	Acc: 61.4% (6135/10000)
[Test]  Epoch: 100	Loss: 0.027711	Acc: 61.3% (6129/10000)
===========finish==========
['2024-08-19', '16:24:43.197150', '100', 'test', '0.027710970973968505', '61.29', '61.44']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.044436	Acc: 50.7% (5070/10000)
[Test]  Epoch: 2	Loss: 0.036520	Acc: 56.8% (5676/10000)
[Test]  Epoch: 3	Loss: 0.034889	Acc: 58.0% (5795/10000)
[Test]  Epoch: 4	Loss: 0.033852	Acc: 58.7% (5873/10000)
[Test]  Epoch: 5	Loss: 0.032958	Acc: 59.2% (5917/10000)
[Test]  Epoch: 6	Loss: 0.032702	Acc: 59.4% (5941/10000)
[Test]  Epoch: 7	Loss: 0.032482	Acc: 59.5% (5952/10000)
[Test]  Epoch: 8	Loss: 0.032162	Acc: 59.6% (5963/10000)
[Test]  Epoch: 9	Loss: 0.032227	Acc: 59.6% (5965/10000)
[Test]  Epoch: 10	Loss: 0.032207	Acc: 59.4% (5941/10000)
[Test]  Epoch: 11	Loss: 0.032126	Acc: 59.3% (5932/10000)
[Test]  Epoch: 12	Loss: 0.031935	Acc: 59.8% (5980/10000)
[Test]  Epoch: 13	Loss: 0.031787	Acc: 59.9% (5989/10000)
[Test]  Epoch: 14	Loss: 0.031674	Acc: 59.9% (5988/10000)
[Test]  Epoch: 15	Loss: 0.031184	Acc: 60.1% (6007/10000)
[Test]  Epoch: 16	Loss: 0.031261	Acc: 59.9% (5993/10000)
[Test]  Epoch: 17	Loss: 0.030833	Acc: 60.4% (6044/10000)
[Test]  Epoch: 18	Loss: 0.031011	Acc: 60.0% (5997/10000)
[Test]  Epoch: 19	Loss: 0.030765	Acc: 60.0% (6003/10000)
[Test]  Epoch: 20	Loss: 0.030595	Acc: 60.4% (6035/10000)
[Test]  Epoch: 21	Loss: 0.030330	Acc: 60.2% (6025/10000)
[Test]  Epoch: 22	Loss: 0.030049	Acc: 60.3% (6029/10000)
[Test]  Epoch: 23	Loss: 0.029936	Acc: 60.7% (6069/10000)
[Test]  Epoch: 24	Loss: 0.029945	Acc: 60.2% (6024/10000)
[Test]  Epoch: 25	Loss: 0.029764	Acc: 60.5% (6050/10000)
[Test]  Epoch: 26	Loss: 0.029663	Acc: 60.5% (6055/10000)
[Test]  Epoch: 27	Loss: 0.029747	Acc: 60.4% (6044/10000)
[Test]  Epoch: 28	Loss: 0.029422	Acc: 60.4% (6039/10000)
[Test]  Epoch: 29	Loss: 0.029821	Acc: 60.2% (6019/10000)
[Test]  Epoch: 30	Loss: 0.029488	Acc: 60.6% (6056/10000)
[Test]  Epoch: 31	Loss: 0.029285	Acc: 60.3% (6029/10000)
[Test]  Epoch: 32	Loss: 0.029228	Acc: 60.7% (6074/10000)
[Test]  Epoch: 33	Loss: 0.029322	Acc: 60.6% (6065/10000)
[Test]  Epoch: 34	Loss: 0.029377	Acc: 60.6% (6062/10000)
[Test]  Epoch: 35	Loss: 0.029144	Acc: 60.5% (6053/10000)
[Test]  Epoch: 36	Loss: 0.029238	Acc: 60.8% (6079/10000)
[Test]  Epoch: 37	Loss: 0.029087	Acc: 60.6% (6060/10000)
[Test]  Epoch: 38	Loss: 0.029180	Acc: 60.7% (6073/10000)
[Test]  Epoch: 39	Loss: 0.029177	Acc: 60.8% (6078/10000)
[Test]  Epoch: 40	Loss: 0.028992	Acc: 60.7% (6071/10000)
[Test]  Epoch: 41	Loss: 0.029089	Acc: 60.6% (6056/10000)
[Test]  Epoch: 42	Loss: 0.028881	Acc: 60.8% (6081/10000)
[Test]  Epoch: 43	Loss: 0.028963	Acc: 60.9% (6088/10000)
[Test]  Epoch: 44	Loss: 0.028779	Acc: 60.9% (6093/10000)
[Test]  Epoch: 45	Loss: 0.028824	Acc: 60.7% (6073/10000)
[Test]  Epoch: 46	Loss: 0.028749	Acc: 60.9% (6086/10000)
[Test]  Epoch: 47	Loss: 0.028774	Acc: 60.7% (6073/10000)
[Test]  Epoch: 48	Loss: 0.028570	Acc: 60.7% (6073/10000)
[Test]  Epoch: 49	Loss: 0.028486	Acc: 60.8% (6078/10000)
[Test]  Epoch: 50	Loss: 0.028522	Acc: 60.9% (6088/10000)
[Test]  Epoch: 51	Loss: 0.028510	Acc: 61.0% (6099/10000)
[Test]  Epoch: 52	Loss: 0.028225	Acc: 61.1% (6115/10000)
[Test]  Epoch: 53	Loss: 0.028288	Acc: 60.8% (6084/10000)
[Test]  Epoch: 54	Loss: 0.028376	Acc: 61.0% (6098/10000)
[Test]  Epoch: 55	Loss: 0.028324	Acc: 60.8% (6082/10000)
[Test]  Epoch: 56	Loss: 0.028391	Acc: 60.9% (6085/10000)
[Test]  Epoch: 57	Loss: 0.028290	Acc: 61.1% (6109/10000)
[Test]  Epoch: 58	Loss: 0.028061	Acc: 61.2% (6118/10000)
[Test]  Epoch: 59	Loss: 0.028256	Acc: 61.0% (6105/10000)
[Test]  Epoch: 60	Loss: 0.028163	Acc: 61.1% (6107/10000)
[Test]  Epoch: 61	Loss: 0.028192	Acc: 61.0% (6099/10000)
[Test]  Epoch: 62	Loss: 0.028166	Acc: 61.0% (6101/10000)
[Test]  Epoch: 63	Loss: 0.028154	Acc: 61.0% (6104/10000)
[Test]  Epoch: 64	Loss: 0.028132	Acc: 61.1% (6107/10000)
[Test]  Epoch: 65	Loss: 0.028096	Acc: 61.0% (6097/10000)
[Test]  Epoch: 66	Loss: 0.028132	Acc: 61.0% (6101/10000)
[Test]  Epoch: 67	Loss: 0.028207	Acc: 60.9% (6094/10000)
[Test]  Epoch: 68	Loss: 0.028163	Acc: 61.1% (6115/10000)
[Test]  Epoch: 69	Loss: 0.028099	Acc: 61.2% (6124/10000)
[Test]  Epoch: 70	Loss: 0.028081	Acc: 61.2% (6118/10000)
[Test]  Epoch: 71	Loss: 0.028159	Acc: 61.0% (6102/10000)
[Test]  Epoch: 72	Loss: 0.028120	Acc: 61.0% (6102/10000)
[Test]  Epoch: 73	Loss: 0.028088	Acc: 61.0% (6100/10000)
[Test]  Epoch: 74	Loss: 0.028060	Acc: 61.1% (6115/10000)
[Test]  Epoch: 75	Loss: 0.028055	Acc: 61.1% (6109/10000)
[Test]  Epoch: 76	Loss: 0.028105	Acc: 61.0% (6103/10000)
[Test]  Epoch: 77	Loss: 0.028111	Acc: 61.0% (6100/10000)
[Test]  Epoch: 78	Loss: 0.028030	Acc: 61.0% (6097/10000)
[Test]  Epoch: 79	Loss: 0.028070	Acc: 61.1% (6114/10000)
[Test]  Epoch: 80	Loss: 0.028125	Acc: 61.1% (6113/10000)
[Test]  Epoch: 81	Loss: 0.028073	Acc: 61.0% (6103/10000)
[Test]  Epoch: 82	Loss: 0.028114	Acc: 61.1% (6113/10000)
[Test]  Epoch: 83	Loss: 0.028044	Acc: 61.1% (6106/10000)
[Test]  Epoch: 84	Loss: 0.027895	Acc: 61.1% (6112/10000)
[Test]  Epoch: 85	Loss: 0.027939	Acc: 61.1% (6107/10000)
[Test]  Epoch: 86	Loss: 0.028108	Acc: 61.0% (6101/10000)
[Test]  Epoch: 87	Loss: 0.027954	Acc: 61.1% (6112/10000)
[Test]  Epoch: 88	Loss: 0.028082	Acc: 61.0% (6103/10000)
[Test]  Epoch: 89	Loss: 0.028022	Acc: 61.0% (6103/10000)
[Test]  Epoch: 90	Loss: 0.027996	Acc: 61.0% (6103/10000)
[Test]  Epoch: 91	Loss: 0.027977	Acc: 61.2% (6116/10000)
[Test]  Epoch: 92	Loss: 0.028119	Acc: 60.9% (6089/10000)
[Test]  Epoch: 93	Loss: 0.027941	Acc: 61.0% (6103/10000)
[Test]  Epoch: 94	Loss: 0.027938	Acc: 61.0% (6103/10000)
[Test]  Epoch: 95	Loss: 0.028027	Acc: 61.0% (6102/10000)
[Test]  Epoch: 96	Loss: 0.027998	Acc: 61.1% (6114/10000)
[Test]  Epoch: 97	Loss: 0.028086	Acc: 60.9% (6086/10000)
[Test]  Epoch: 98	Loss: 0.028017	Acc: 61.1% (6107/10000)
[Test]  Epoch: 99	Loss: 0.028027	Acc: 61.1% (6109/10000)
[Test]  Epoch: 100	Loss: 0.027976	Acc: 61.0% (6096/10000)
===========finish==========
['2024-08-19', '16:28:58.942623', '100', 'test', '0.027975876235961913', '60.96', '61.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.048885	Acc: 47.5% (4746/10000)
[Test]  Epoch: 2	Loss: 0.036715	Acc: 56.2% (5624/10000)
[Test]  Epoch: 3	Loss: 0.035172	Acc: 57.3% (5728/10000)
[Test]  Epoch: 4	Loss: 0.034314	Acc: 58.6% (5863/10000)
[Test]  Epoch: 5	Loss: 0.033562	Acc: 58.8% (5875/10000)
[Test]  Epoch: 6	Loss: 0.033097	Acc: 58.7% (5872/10000)
[Test]  Epoch: 7	Loss: 0.032253	Acc: 59.4% (5941/10000)
[Test]  Epoch: 8	Loss: 0.032225	Acc: 59.7% (5970/10000)
[Test]  Epoch: 9	Loss: 0.032825	Acc: 59.1% (5906/10000)
[Test]  Epoch: 10	Loss: 0.031612	Acc: 59.9% (5991/10000)
[Test]  Epoch: 11	Loss: 0.031184	Acc: 59.9% (5987/10000)
[Test]  Epoch: 12	Loss: 0.031261	Acc: 59.8% (5976/10000)
[Test]  Epoch: 13	Loss: 0.030922	Acc: 59.8% (5979/10000)
[Test]  Epoch: 14	Loss: 0.031024	Acc: 59.9% (5992/10000)
[Test]  Epoch: 15	Loss: 0.031194	Acc: 59.6% (5965/10000)
[Test]  Epoch: 16	Loss: 0.031116	Acc: 59.7% (5969/10000)
[Test]  Epoch: 17	Loss: 0.030768	Acc: 59.9% (5994/10000)
[Test]  Epoch: 18	Loss: 0.030824	Acc: 60.1% (6009/10000)
[Test]  Epoch: 19	Loss: 0.031020	Acc: 59.6% (5965/10000)
[Test]  Epoch: 20	Loss: 0.030533	Acc: 60.0% (5998/10000)
[Test]  Epoch: 21	Loss: 0.030397	Acc: 60.1% (6014/10000)
[Test]  Epoch: 22	Loss: 0.030412	Acc: 59.8% (5983/10000)
[Test]  Epoch: 23	Loss: 0.029994	Acc: 60.1% (6006/10000)
[Test]  Epoch: 24	Loss: 0.029914	Acc: 60.0% (6005/10000)
[Test]  Epoch: 25	Loss: 0.029705	Acc: 60.2% (6021/10000)
[Test]  Epoch: 26	Loss: 0.029843	Acc: 60.1% (6015/10000)
[Test]  Epoch: 27	Loss: 0.029818	Acc: 60.2% (6023/10000)
[Test]  Epoch: 28	Loss: 0.029520	Acc: 60.5% (6046/10000)
[Test]  Epoch: 29	Loss: 0.029742	Acc: 60.1% (6012/10000)
[Test]  Epoch: 30	Loss: 0.029622	Acc: 60.0% (6000/10000)
[Test]  Epoch: 31	Loss: 0.029464	Acc: 60.0% (5998/10000)
[Test]  Epoch: 32	Loss: 0.029349	Acc: 60.0% (5999/10000)
[Test]  Epoch: 33	Loss: 0.029331	Acc: 59.9% (5993/10000)
[Test]  Epoch: 34	Loss: 0.029333	Acc: 60.2% (6023/10000)
[Test]  Epoch: 35	Loss: 0.029183	Acc: 60.1% (6015/10000)
[Test]  Epoch: 36	Loss: 0.029202	Acc: 60.2% (6022/10000)
[Test]  Epoch: 37	Loss: 0.029132	Acc: 60.5% (6045/10000)
[Test]  Epoch: 38	Loss: 0.029038	Acc: 60.4% (6038/10000)
[Test]  Epoch: 39	Loss: 0.029204	Acc: 60.5% (6050/10000)
[Test]  Epoch: 40	Loss: 0.028963	Acc: 60.2% (6025/10000)
[Test]  Epoch: 41	Loss: 0.028927	Acc: 60.4% (6036/10000)
[Test]  Epoch: 42	Loss: 0.028718	Acc: 60.6% (6061/10000)
[Test]  Epoch: 43	Loss: 0.028831	Acc: 60.7% (6069/10000)
[Test]  Epoch: 44	Loss: 0.028768	Acc: 60.6% (6060/10000)
[Test]  Epoch: 45	Loss: 0.028834	Acc: 60.7% (6066/10000)
[Test]  Epoch: 46	Loss: 0.028663	Acc: 60.5% (6050/10000)
[Test]  Epoch: 47	Loss: 0.028656	Acc: 60.5% (6046/10000)
[Test]  Epoch: 48	Loss: 0.028424	Acc: 60.6% (6062/10000)
[Test]  Epoch: 49	Loss: 0.028329	Acc: 60.6% (6062/10000)
[Test]  Epoch: 50	Loss: 0.028287	Acc: 60.5% (6055/10000)
[Test]  Epoch: 51	Loss: 0.028434	Acc: 60.7% (6067/10000)
[Test]  Epoch: 52	Loss: 0.028151	Acc: 60.5% (6048/10000)
[Test]  Epoch: 53	Loss: 0.028185	Acc: 60.7% (6071/10000)
[Test]  Epoch: 54	Loss: 0.028305	Acc: 60.7% (6066/10000)
[Test]  Epoch: 55	Loss: 0.028127	Acc: 60.6% (6058/10000)
[Test]  Epoch: 56	Loss: 0.028281	Acc: 60.5% (6045/10000)
[Test]  Epoch: 57	Loss: 0.027993	Acc: 60.5% (6048/10000)
[Test]  Epoch: 58	Loss: 0.027979	Acc: 60.6% (6056/10000)
[Test]  Epoch: 59	Loss: 0.028077	Acc: 60.4% (6040/10000)
[Test]  Epoch: 60	Loss: 0.027899	Acc: 60.6% (6061/10000)
[Test]  Epoch: 61	Loss: 0.027973	Acc: 60.7% (6068/10000)
[Test]  Epoch: 62	Loss: 0.028022	Acc: 60.6% (6058/10000)
[Test]  Epoch: 63	Loss: 0.028055	Acc: 60.7% (6073/10000)
[Test]  Epoch: 64	Loss: 0.028088	Acc: 60.6% (6059/10000)
[Test]  Epoch: 65	Loss: 0.027959	Acc: 60.6% (6057/10000)
[Test]  Epoch: 66	Loss: 0.027883	Acc: 60.6% (6062/10000)
[Test]  Epoch: 67	Loss: 0.028016	Acc: 60.6% (6062/10000)
[Test]  Epoch: 68	Loss: 0.027929	Acc: 60.6% (6057/10000)
[Test]  Epoch: 69	Loss: 0.027853	Acc: 60.5% (6054/10000)
[Test]  Epoch: 70	Loss: 0.027847	Acc: 60.7% (6067/10000)
[Test]  Epoch: 71	Loss: 0.027870	Acc: 60.6% (6061/10000)
[Test]  Epoch: 72	Loss: 0.027888	Acc: 60.4% (6040/10000)
[Test]  Epoch: 73	Loss: 0.027903	Acc: 60.6% (6065/10000)
[Test]  Epoch: 74	Loss: 0.027867	Acc: 60.7% (6068/10000)
[Test]  Epoch: 75	Loss: 0.027854	Acc: 60.8% (6084/10000)
[Test]  Epoch: 76	Loss: 0.028012	Acc: 60.5% (6052/10000)
[Test]  Epoch: 77	Loss: 0.027891	Acc: 60.6% (6064/10000)
[Test]  Epoch: 78	Loss: 0.027838	Acc: 60.7% (6074/10000)
[Test]  Epoch: 79	Loss: 0.027948	Acc: 60.7% (6073/10000)
[Test]  Epoch: 80	Loss: 0.027900	Acc: 60.6% (6064/10000)
[Test]  Epoch: 81	Loss: 0.027803	Acc: 60.5% (6055/10000)
[Test]  Epoch: 82	Loss: 0.027835	Acc: 60.7% (6068/10000)
[Test]  Epoch: 83	Loss: 0.027772	Acc: 60.7% (6066/10000)
[Test]  Epoch: 84	Loss: 0.027770	Acc: 60.7% (6072/10000)
[Test]  Epoch: 85	Loss: 0.027923	Acc: 60.7% (6068/10000)
[Test]  Epoch: 86	Loss: 0.027908	Acc: 60.7% (6071/10000)
[Test]  Epoch: 87	Loss: 0.027807	Acc: 60.6% (6065/10000)
[Test]  Epoch: 88	Loss: 0.027833	Acc: 60.8% (6080/10000)
[Test]  Epoch: 89	Loss: 0.027883	Acc: 60.7% (6067/10000)
[Test]  Epoch: 90	Loss: 0.027751	Acc: 60.8% (6079/10000)
[Test]  Epoch: 91	Loss: 0.027867	Acc: 60.8% (6083/10000)
[Test]  Epoch: 92	Loss: 0.027903	Acc: 60.6% (6058/10000)
[Test]  Epoch: 93	Loss: 0.027668	Acc: 60.6% (6064/10000)
[Test]  Epoch: 94	Loss: 0.027683	Acc: 60.9% (6087/10000)
[Test]  Epoch: 95	Loss: 0.027831	Acc: 60.7% (6074/10000)
[Test]  Epoch: 96	Loss: 0.027908	Acc: 60.9% (6086/10000)
[Test]  Epoch: 97	Loss: 0.027800	Acc: 60.7% (6072/10000)
[Test]  Epoch: 98	Loss: 0.027749	Acc: 60.8% (6082/10000)
[Test]  Epoch: 99	Loss: 0.027811	Acc: 60.7% (6072/10000)
[Test]  Epoch: 100	Loss: 0.027806	Acc: 60.8% (6081/10000)
===========finish==========
['2024-08-19', '16:33:10.502711', '100', 'test', '0.027805747294425965', '60.81', '60.87']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.054504	Acc: 42.5% (4251/10000)
[Test]  Epoch: 2	Loss: 0.040639	Acc: 54.8% (5482/10000)
[Test]  Epoch: 3	Loss: 0.038779	Acc: 56.7% (5674/10000)
[Test]  Epoch: 4	Loss: 0.037697	Acc: 57.9% (5793/10000)
[Test]  Epoch: 5	Loss: 0.037322	Acc: 57.8% (5779/10000)
[Test]  Epoch: 6	Loss: 0.036357	Acc: 58.3% (5831/10000)
[Test]  Epoch: 7	Loss: 0.036096	Acc: 58.2% (5823/10000)
[Test]  Epoch: 8	Loss: 0.035918	Acc: 58.6% (5858/10000)
[Test]  Epoch: 9	Loss: 0.035560	Acc: 58.8% (5876/10000)
[Test]  Epoch: 10	Loss: 0.035338	Acc: 58.3% (5834/10000)
[Test]  Epoch: 11	Loss: 0.035044	Acc: 58.5% (5854/10000)
[Test]  Epoch: 12	Loss: 0.034825	Acc: 58.5% (5847/10000)
[Test]  Epoch: 13	Loss: 0.034684	Acc: 58.7% (5867/10000)
[Test]  Epoch: 14	Loss: 0.034319	Acc: 58.6% (5862/10000)
[Test]  Epoch: 15	Loss: 0.034156	Acc: 58.7% (5872/10000)
[Test]  Epoch: 16	Loss: 0.034139	Acc: 58.6% (5860/10000)
[Test]  Epoch: 17	Loss: 0.033847	Acc: 58.9% (5890/10000)
[Test]  Epoch: 18	Loss: 0.033579	Acc: 58.6% (5859/10000)
[Test]  Epoch: 19	Loss: 0.033823	Acc: 59.0% (5896/10000)
[Test]  Epoch: 20	Loss: 0.033561	Acc: 58.9% (5888/10000)
[Test]  Epoch: 21	Loss: 0.033355	Acc: 58.8% (5879/10000)
[Test]  Epoch: 22	Loss: 0.033131	Acc: 58.9% (5889/10000)
[Test]  Epoch: 23	Loss: 0.033201	Acc: 58.7% (5868/10000)
[Test]  Epoch: 24	Loss: 0.032972	Acc: 58.8% (5881/10000)
[Test]  Epoch: 25	Loss: 0.032841	Acc: 59.1% (5914/10000)
[Test]  Epoch: 26	Loss: 0.032672	Acc: 59.0% (5903/10000)
[Test]  Epoch: 27	Loss: 0.032583	Acc: 59.1% (5906/10000)
[Test]  Epoch: 28	Loss: 0.032363	Acc: 59.0% (5902/10000)
[Test]  Epoch: 29	Loss: 0.032610	Acc: 58.8% (5881/10000)
[Test]  Epoch: 30	Loss: 0.032476	Acc: 58.8% (5877/10000)
[Test]  Epoch: 31	Loss: 0.032274	Acc: 58.9% (5889/10000)
[Test]  Epoch: 32	Loss: 0.032258	Acc: 59.1% (5915/10000)
[Test]  Epoch: 33	Loss: 0.032107	Acc: 59.0% (5903/10000)
[Test]  Epoch: 34	Loss: 0.031933	Acc: 58.9% (5893/10000)
[Test]  Epoch: 35	Loss: 0.031793	Acc: 59.2% (5925/10000)
[Test]  Epoch: 36	Loss: 0.031859	Acc: 59.3% (5931/10000)
[Test]  Epoch: 37	Loss: 0.031719	Acc: 59.1% (5906/10000)
[Test]  Epoch: 38	Loss: 0.031670	Acc: 59.0% (5895/10000)
[Test]  Epoch: 39	Loss: 0.031922	Acc: 58.7% (5873/10000)
[Test]  Epoch: 40	Loss: 0.031859	Acc: 58.8% (5878/10000)
[Test]  Epoch: 41	Loss: 0.031626	Acc: 59.0% (5900/10000)
[Test]  Epoch: 42	Loss: 0.031574	Acc: 58.9% (5890/10000)
[Test]  Epoch: 43	Loss: 0.031608	Acc: 58.9% (5887/10000)
[Test]  Epoch: 44	Loss: 0.031400	Acc: 59.1% (5910/10000)
[Test]  Epoch: 45	Loss: 0.031349	Acc: 59.1% (5908/10000)
[Test]  Epoch: 46	Loss: 0.031229	Acc: 59.2% (5920/10000)
[Test]  Epoch: 47	Loss: 0.031341	Acc: 59.3% (5927/10000)
[Test]  Epoch: 48	Loss: 0.031115	Acc: 59.3% (5927/10000)
[Test]  Epoch: 49	Loss: 0.030955	Acc: 59.3% (5932/10000)
[Test]  Epoch: 50	Loss: 0.031040	Acc: 59.3% (5934/10000)
[Test]  Epoch: 51	Loss: 0.030894	Acc: 59.3% (5927/10000)
[Test]  Epoch: 52	Loss: 0.030820	Acc: 59.3% (5926/10000)
[Test]  Epoch: 53	Loss: 0.030752	Acc: 59.2% (5924/10000)
[Test]  Epoch: 54	Loss: 0.030702	Acc: 59.3% (5926/10000)
[Test]  Epoch: 55	Loss: 0.030637	Acc: 59.3% (5933/10000)
[Test]  Epoch: 56	Loss: 0.030837	Acc: 59.0% (5905/10000)
[Test]  Epoch: 57	Loss: 0.030650	Acc: 59.2% (5924/10000)
[Test]  Epoch: 58	Loss: 0.030551	Acc: 59.5% (5953/10000)
[Test]  Epoch: 59	Loss: 0.030556	Acc: 59.4% (5935/10000)
[Test]  Epoch: 60	Loss: 0.030656	Acc: 59.4% (5944/10000)
[Test]  Epoch: 61	Loss: 0.030587	Acc: 59.5% (5950/10000)
[Test]  Epoch: 62	Loss: 0.030600	Acc: 59.4% (5941/10000)
[Test]  Epoch: 63	Loss: 0.030629	Acc: 59.3% (5929/10000)
[Test]  Epoch: 64	Loss: 0.030710	Acc: 59.5% (5947/10000)
[Test]  Epoch: 65	Loss: 0.030558	Acc: 59.4% (5940/10000)
[Test]  Epoch: 66	Loss: 0.030503	Acc: 59.5% (5945/10000)
[Test]  Epoch: 67	Loss: 0.030502	Acc: 59.4% (5937/10000)
[Test]  Epoch: 68	Loss: 0.030471	Acc: 59.5% (5951/10000)
[Test]  Epoch: 69	Loss: 0.030364	Acc: 59.5% (5950/10000)
[Test]  Epoch: 70	Loss: 0.030482	Acc: 59.4% (5935/10000)
[Test]  Epoch: 71	Loss: 0.030332	Acc: 59.6% (5960/10000)
[Test]  Epoch: 72	Loss: 0.030488	Acc: 59.5% (5945/10000)
[Test]  Epoch: 73	Loss: 0.030452	Acc: 59.3% (5926/10000)
[Test]  Epoch: 74	Loss: 0.030487	Acc: 59.5% (5951/10000)
[Test]  Epoch: 75	Loss: 0.030358	Acc: 59.5% (5954/10000)
[Test]  Epoch: 76	Loss: 0.030520	Acc: 59.3% (5928/10000)
[Test]  Epoch: 77	Loss: 0.030615	Acc: 59.2% (5921/10000)
[Test]  Epoch: 78	Loss: 0.030359	Acc: 59.5% (5953/10000)
[Test]  Epoch: 79	Loss: 0.030489	Acc: 59.4% (5940/10000)
[Test]  Epoch: 80	Loss: 0.030498	Acc: 59.5% (5947/10000)
[Test]  Epoch: 81	Loss: 0.030438	Acc: 59.6% (5963/10000)
[Test]  Epoch: 82	Loss: 0.030236	Acc: 59.5% (5948/10000)
[Test]  Epoch: 83	Loss: 0.030466	Acc: 59.2% (5921/10000)
[Test]  Epoch: 84	Loss: 0.030326	Acc: 59.5% (5954/10000)
[Test]  Epoch: 85	Loss: 0.030411	Acc: 59.4% (5936/10000)
[Test]  Epoch: 86	Loss: 0.030449	Acc: 59.5% (5951/10000)
[Test]  Epoch: 87	Loss: 0.030368	Acc: 59.5% (5950/10000)
[Test]  Epoch: 88	Loss: 0.030283	Acc: 59.5% (5947/10000)
[Test]  Epoch: 89	Loss: 0.030296	Acc: 59.4% (5943/10000)
[Test]  Epoch: 90	Loss: 0.030268	Acc: 59.5% (5953/10000)
[Test]  Epoch: 91	Loss: 0.030458	Acc: 59.5% (5948/10000)
[Test]  Epoch: 92	Loss: 0.030341	Acc: 59.4% (5937/10000)
[Test]  Epoch: 93	Loss: 0.030098	Acc: 59.5% (5948/10000)
[Test]  Epoch: 94	Loss: 0.030078	Acc: 59.4% (5943/10000)
[Test]  Epoch: 95	Loss: 0.030288	Acc: 59.4% (5937/10000)
[Test]  Epoch: 96	Loss: 0.030396	Acc: 59.5% (5955/10000)
[Test]  Epoch: 97	Loss: 0.030320	Acc: 59.5% (5945/10000)
[Test]  Epoch: 98	Loss: 0.030242	Acc: 59.6% (5962/10000)
[Test]  Epoch: 99	Loss: 0.030383	Acc: 59.5% (5955/10000)
[Test]  Epoch: 100	Loss: 0.030250	Acc: 59.5% (5954/10000)
===========finish==========
['2024-08-19', '16:37:29.345843', '100', 'test', '0.030249739277362825', '59.54', '59.63']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.086395	Acc: 21.3% (2127/10000)
[Test]  Epoch: 2	Loss: 0.042203	Acc: 48.7% (4867/10000)
[Test]  Epoch: 3	Loss: 0.038224	Acc: 53.1% (5309/10000)
[Test]  Epoch: 4	Loss: 0.036372	Acc: 54.3% (5430/10000)
[Test]  Epoch: 5	Loss: 0.035137	Acc: 54.9% (5487/10000)
[Test]  Epoch: 6	Loss: 0.035248	Acc: 54.9% (5493/10000)
[Test]  Epoch: 7	Loss: 0.034724	Acc: 55.3% (5528/10000)
[Test]  Epoch: 8	Loss: 0.034454	Acc: 55.2% (5520/10000)
[Test]  Epoch: 9	Loss: 0.034337	Acc: 55.1% (5512/10000)
[Test]  Epoch: 10	Loss: 0.033951	Acc: 55.3% (5527/10000)
[Test]  Epoch: 11	Loss: 0.033641	Acc: 55.3% (5528/10000)
[Test]  Epoch: 12	Loss: 0.033551	Acc: 55.6% (5561/10000)
[Test]  Epoch: 13	Loss: 0.033386	Acc: 55.4% (5543/10000)
[Test]  Epoch: 14	Loss: 0.033166	Acc: 55.5% (5546/10000)
[Test]  Epoch: 15	Loss: 0.032848	Acc: 55.7% (5571/10000)
[Test]  Epoch: 16	Loss: 0.032979	Acc: 55.5% (5547/10000)
[Test]  Epoch: 17	Loss: 0.032874	Acc: 55.6% (5556/10000)
[Test]  Epoch: 18	Loss: 0.032773	Acc: 55.5% (5550/10000)
[Test]  Epoch: 19	Loss: 0.032749	Acc: 55.6% (5563/10000)
[Test]  Epoch: 20	Loss: 0.032548	Acc: 55.6% (5563/10000)
[Test]  Epoch: 21	Loss: 0.032433	Acc: 55.6% (5558/10000)
[Test]  Epoch: 22	Loss: 0.032231	Acc: 55.4% (5542/10000)
[Test]  Epoch: 23	Loss: 0.032125	Acc: 55.9% (5586/10000)
[Test]  Epoch: 24	Loss: 0.032025	Acc: 55.9% (5590/10000)
[Test]  Epoch: 25	Loss: 0.032065	Acc: 56.0% (5598/10000)
[Test]  Epoch: 26	Loss: 0.031975	Acc: 55.9% (5593/10000)
[Test]  Epoch: 27	Loss: 0.031797	Acc: 56.0% (5600/10000)
[Test]  Epoch: 28	Loss: 0.031731	Acc: 56.0% (5595/10000)
[Test]  Epoch: 29	Loss: 0.031837	Acc: 56.0% (5597/10000)
[Test]  Epoch: 30	Loss: 0.031711	Acc: 56.3% (5629/10000)
[Test]  Epoch: 31	Loss: 0.031658	Acc: 56.0% (5603/10000)
[Test]  Epoch: 32	Loss: 0.031596	Acc: 56.4% (5643/10000)
[Test]  Epoch: 33	Loss: 0.031456	Acc: 56.2% (5621/10000)
[Test]  Epoch: 34	Loss: 0.031328	Acc: 56.2% (5624/10000)
[Test]  Epoch: 35	Loss: 0.031208	Acc: 56.2% (5620/10000)
[Test]  Epoch: 36	Loss: 0.031335	Acc: 56.3% (5629/10000)
[Test]  Epoch: 37	Loss: 0.031294	Acc: 56.2% (5619/10000)
[Test]  Epoch: 38	Loss: 0.031037	Acc: 56.3% (5630/10000)
[Test]  Epoch: 39	Loss: 0.031141	Acc: 56.3% (5630/10000)
[Test]  Epoch: 40	Loss: 0.031088	Acc: 56.2% (5624/10000)
[Test]  Epoch: 41	Loss: 0.031017	Acc: 56.5% (5645/10000)
[Test]  Epoch: 42	Loss: 0.030891	Acc: 56.5% (5645/10000)
[Test]  Epoch: 43	Loss: 0.030934	Acc: 56.4% (5643/10000)
[Test]  Epoch: 44	Loss: 0.030867	Acc: 56.5% (5647/10000)
[Test]  Epoch: 45	Loss: 0.030796	Acc: 56.2% (5623/10000)
[Test]  Epoch: 46	Loss: 0.030686	Acc: 56.4% (5635/10000)
[Test]  Epoch: 47	Loss: 0.030744	Acc: 56.2% (5624/10000)
[Test]  Epoch: 48	Loss: 0.030560	Acc: 56.6% (5658/10000)
[Test]  Epoch: 49	Loss: 0.030437	Acc: 56.3% (5631/10000)
[Test]  Epoch: 50	Loss: 0.030525	Acc: 56.5% (5648/10000)
[Test]  Epoch: 51	Loss: 0.030514	Acc: 56.6% (5657/10000)
[Test]  Epoch: 52	Loss: 0.030268	Acc: 56.7% (5669/10000)
[Test]  Epoch: 53	Loss: 0.030277	Acc: 56.6% (5662/10000)
[Test]  Epoch: 54	Loss: 0.030361	Acc: 56.5% (5650/10000)
[Test]  Epoch: 55	Loss: 0.030254	Acc: 56.6% (5659/10000)
[Test]  Epoch: 56	Loss: 0.030282	Acc: 56.6% (5656/10000)
[Test]  Epoch: 57	Loss: 0.030156	Acc: 56.8% (5683/10000)
[Test]  Epoch: 58	Loss: 0.030110	Acc: 56.6% (5660/10000)
[Test]  Epoch: 59	Loss: 0.030214	Acc: 56.7% (5666/10000)
[Test]  Epoch: 60	Loss: 0.030136	Acc: 56.6% (5658/10000)
[Test]  Epoch: 61	Loss: 0.030132	Acc: 56.6% (5663/10000)
[Test]  Epoch: 62	Loss: 0.030144	Acc: 56.7% (5673/10000)
[Test]  Epoch: 63	Loss: 0.030023	Acc: 56.8% (5675/10000)
[Test]  Epoch: 64	Loss: 0.030031	Acc: 56.8% (5675/10000)
[Test]  Epoch: 65	Loss: 0.030134	Acc: 56.6% (5656/10000)
[Test]  Epoch: 66	Loss: 0.030005	Acc: 56.6% (5660/10000)
[Test]  Epoch: 67	Loss: 0.030088	Acc: 56.6% (5664/10000)
[Test]  Epoch: 68	Loss: 0.030034	Acc: 56.8% (5677/10000)
[Test]  Epoch: 69	Loss: 0.029907	Acc: 56.8% (5680/10000)
[Test]  Epoch: 70	Loss: 0.029999	Acc: 56.9% (5687/10000)
[Test]  Epoch: 71	Loss: 0.030049	Acc: 56.6% (5665/10000)
[Test]  Epoch: 72	Loss: 0.030095	Acc: 56.7% (5669/10000)
[Test]  Epoch: 73	Loss: 0.030054	Acc: 56.7% (5671/10000)
[Test]  Epoch: 74	Loss: 0.030017	Acc: 56.7% (5667/10000)
[Test]  Epoch: 75	Loss: 0.030007	Acc: 56.6% (5663/10000)
[Test]  Epoch: 76	Loss: 0.030170	Acc: 56.5% (5653/10000)
[Test]  Epoch: 77	Loss: 0.030081	Acc: 56.5% (5653/10000)
[Test]  Epoch: 78	Loss: 0.029958	Acc: 56.8% (5684/10000)
[Test]  Epoch: 79	Loss: 0.029894	Acc: 56.9% (5692/10000)
[Test]  Epoch: 80	Loss: 0.029971	Acc: 56.8% (5682/10000)
[Test]  Epoch: 81	Loss: 0.030028	Acc: 56.7% (5673/10000)
[Test]  Epoch: 82	Loss: 0.030032	Acc: 56.6% (5664/10000)
[Test]  Epoch: 83	Loss: 0.030107	Acc: 56.7% (5666/10000)
[Test]  Epoch: 84	Loss: 0.029918	Acc: 56.7% (5674/10000)
[Test]  Epoch: 85	Loss: 0.029949	Acc: 56.7% (5669/10000)
[Test]  Epoch: 86	Loss: 0.030045	Acc: 56.7% (5674/10000)
[Test]  Epoch: 87	Loss: 0.029929	Acc: 56.9% (5690/10000)
[Test]  Epoch: 88	Loss: 0.029972	Acc: 56.7% (5672/10000)
[Test]  Epoch: 89	Loss: 0.029992	Acc: 56.8% (5678/10000)
[Test]  Epoch: 90	Loss: 0.029922	Acc: 56.8% (5681/10000)
[Test]  Epoch: 91	Loss: 0.029829	Acc: 56.8% (5680/10000)
[Test]  Epoch: 92	Loss: 0.030012	Acc: 56.7% (5669/10000)
[Test]  Epoch: 93	Loss: 0.029900	Acc: 56.8% (5677/10000)
[Test]  Epoch: 94	Loss: 0.029841	Acc: 56.7% (5674/10000)
[Test]  Epoch: 95	Loss: 0.029942	Acc: 56.5% (5650/10000)
[Test]  Epoch: 96	Loss: 0.029926	Acc: 56.6% (5662/10000)
[Test]  Epoch: 97	Loss: 0.030027	Acc: 56.9% (5685/10000)
[Test]  Epoch: 98	Loss: 0.029836	Acc: 56.8% (5684/10000)
[Test]  Epoch: 99	Loss: 0.029959	Acc: 56.9% (5693/10000)
[Test]  Epoch: 100	Loss: 0.029915	Acc: 56.8% (5675/10000)
===========finish==========
['2024-08-19', '16:41:50.085492', '100', 'test', '0.029915466636419297', '56.75', '56.93']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.085585	Acc: 22.8% (2279/10000)
[Test]  Epoch: 2	Loss: 0.045780	Acc: 46.8% (4678/10000)
[Test]  Epoch: 3	Loss: 0.041266	Acc: 50.5% (5049/10000)
[Test]  Epoch: 4	Loss: 0.040278	Acc: 51.3% (5130/10000)
[Test]  Epoch: 5	Loss: 0.038976	Acc: 52.1% (5209/10000)
[Test]  Epoch: 6	Loss: 0.038544	Acc: 52.5% (5248/10000)
[Test]  Epoch: 7	Loss: 0.038128	Acc: 52.9% (5286/10000)
[Test]  Epoch: 8	Loss: 0.037675	Acc: 52.8% (5283/10000)
[Test]  Epoch: 9	Loss: 0.037807	Acc: 52.6% (5265/10000)
[Test]  Epoch: 10	Loss: 0.037568	Acc: 52.7% (5272/10000)
[Test]  Epoch: 11	Loss: 0.036905	Acc: 53.4% (5339/10000)
[Test]  Epoch: 12	Loss: 0.036811	Acc: 53.4% (5341/10000)
[Test]  Epoch: 13	Loss: 0.036826	Acc: 53.4% (5335/10000)
[Test]  Epoch: 14	Loss: 0.036517	Acc: 53.4% (5341/10000)
[Test]  Epoch: 15	Loss: 0.035903	Acc: 53.8% (5380/10000)
[Test]  Epoch: 16	Loss: 0.035867	Acc: 53.7% (5372/10000)
[Test]  Epoch: 17	Loss: 0.035611	Acc: 53.8% (5377/10000)
[Test]  Epoch: 18	Loss: 0.035575	Acc: 54.0% (5399/10000)
[Test]  Epoch: 19	Loss: 0.035534	Acc: 53.8% (5382/10000)
[Test]  Epoch: 20	Loss: 0.035089	Acc: 53.9% (5394/10000)
[Test]  Epoch: 21	Loss: 0.034832	Acc: 54.0% (5399/10000)
[Test]  Epoch: 22	Loss: 0.034893	Acc: 53.9% (5390/10000)
[Test]  Epoch: 23	Loss: 0.034834	Acc: 54.0% (5401/10000)
[Test]  Epoch: 24	Loss: 0.034867	Acc: 54.2% (5419/10000)
[Test]  Epoch: 25	Loss: 0.034619	Acc: 54.2% (5423/10000)
[Test]  Epoch: 26	Loss: 0.034448	Acc: 54.3% (5427/10000)
[Test]  Epoch: 27	Loss: 0.034487	Acc: 54.0% (5400/10000)
[Test]  Epoch: 28	Loss: 0.034221	Acc: 54.5% (5455/10000)
[Test]  Epoch: 29	Loss: 0.034175	Acc: 54.2% (5421/10000)
[Test]  Epoch: 30	Loss: 0.034030	Acc: 54.3% (5426/10000)
[Test]  Epoch: 31	Loss: 0.034061	Acc: 54.4% (5441/10000)
[Test]  Epoch: 32	Loss: 0.033940	Acc: 54.6% (5459/10000)
[Test]  Epoch: 33	Loss: 0.033809	Acc: 54.5% (5446/10000)
[Test]  Epoch: 34	Loss: 0.033756	Acc: 54.5% (5452/10000)
[Test]  Epoch: 35	Loss: 0.033706	Acc: 54.5% (5454/10000)
[Test]  Epoch: 36	Loss: 0.033711	Acc: 54.6% (5457/10000)
[Test]  Epoch: 37	Loss: 0.033613	Acc: 54.5% (5445/10000)
[Test]  Epoch: 38	Loss: 0.033583	Acc: 54.6% (5462/10000)
[Test]  Epoch: 39	Loss: 0.033505	Acc: 54.9% (5486/10000)
[Test]  Epoch: 40	Loss: 0.033421	Acc: 54.6% (5458/10000)
[Test]  Epoch: 41	Loss: 0.033137	Acc: 55.0% (5497/10000)
[Test]  Epoch: 42	Loss: 0.033031	Acc: 54.8% (5482/10000)
[Test]  Epoch: 43	Loss: 0.033184	Acc: 54.9% (5487/10000)
[Test]  Epoch: 44	Loss: 0.033131	Acc: 54.8% (5475/10000)
[Test]  Epoch: 45	Loss: 0.033104	Acc: 55.0% (5499/10000)
[Test]  Epoch: 46	Loss: 0.033100	Acc: 54.8% (5479/10000)
[Test]  Epoch: 47	Loss: 0.033035	Acc: 54.9% (5490/10000)
[Test]  Epoch: 48	Loss: 0.033113	Acc: 54.7% (5467/10000)
[Test]  Epoch: 49	Loss: 0.032947	Acc: 54.9% (5485/10000)
[Test]  Epoch: 50	Loss: 0.032954	Acc: 54.9% (5489/10000)
[Test]  Epoch: 51	Loss: 0.033051	Acc: 54.6% (5457/10000)
[Test]  Epoch: 52	Loss: 0.032922	Acc: 54.9% (5488/10000)
[Test]  Epoch: 53	Loss: 0.032821	Acc: 55.0% (5498/10000)
[Test]  Epoch: 54	Loss: 0.033049	Acc: 54.7% (5474/10000)
[Test]  Epoch: 55	Loss: 0.032792	Acc: 55.0% (5497/10000)
[Test]  Epoch: 56	Loss: 0.032822	Acc: 54.8% (5481/10000)
[Test]  Epoch: 57	Loss: 0.032728	Acc: 54.9% (5486/10000)
[Test]  Epoch: 58	Loss: 0.032698	Acc: 55.1% (5509/10000)
[Test]  Epoch: 59	Loss: 0.032605	Acc: 55.2% (5519/10000)
[Test]  Epoch: 60	Loss: 0.032522	Acc: 55.2% (5519/10000)
[Test]  Epoch: 61	Loss: 0.032418	Acc: 55.1% (5512/10000)
[Test]  Epoch: 62	Loss: 0.032432	Acc: 55.1% (5513/10000)
[Test]  Epoch: 63	Loss: 0.032607	Acc: 55.0% (5503/10000)
[Test]  Epoch: 64	Loss: 0.032417	Acc: 55.2% (5516/10000)
[Test]  Epoch: 65	Loss: 0.032446	Acc: 55.1% (5510/10000)
[Test]  Epoch: 66	Loss: 0.032366	Acc: 55.2% (5516/10000)
[Test]  Epoch: 67	Loss: 0.032617	Acc: 55.0% (5497/10000)
[Test]  Epoch: 68	Loss: 0.032547	Acc: 54.9% (5493/10000)
[Test]  Epoch: 69	Loss: 0.032521	Acc: 55.1% (5509/10000)
[Test]  Epoch: 70	Loss: 0.032462	Acc: 55.1% (5514/10000)
[Test]  Epoch: 71	Loss: 0.032472	Acc: 55.0% (5498/10000)
[Test]  Epoch: 72	Loss: 0.032431	Acc: 55.1% (5511/10000)
[Test]  Epoch: 73	Loss: 0.032448	Acc: 55.3% (5530/10000)
[Test]  Epoch: 74	Loss: 0.032348	Acc: 55.1% (5515/10000)
[Test]  Epoch: 75	Loss: 0.032335	Acc: 55.0% (5499/10000)
[Test]  Epoch: 76	Loss: 0.032435	Acc: 55.3% (5530/10000)
[Test]  Epoch: 77	Loss: 0.032419	Acc: 55.2% (5521/10000)
[Test]  Epoch: 78	Loss: 0.032439	Acc: 55.2% (5525/10000)
[Test]  Epoch: 79	Loss: 0.032432	Acc: 55.1% (5508/10000)
[Test]  Epoch: 80	Loss: 0.032286	Acc: 55.2% (5517/10000)
[Test]  Epoch: 81	Loss: 0.032386	Acc: 55.0% (5501/10000)
[Test]  Epoch: 82	Loss: 0.032518	Acc: 54.9% (5493/10000)
[Test]  Epoch: 83	Loss: 0.032427	Acc: 55.1% (5512/10000)
[Test]  Epoch: 84	Loss: 0.032429	Acc: 55.1% (5506/10000)
[Test]  Epoch: 85	Loss: 0.032486	Acc: 55.2% (5525/10000)
[Test]  Epoch: 86	Loss: 0.032280	Acc: 55.1% (5509/10000)
[Test]  Epoch: 87	Loss: 0.032240	Acc: 55.1% (5514/10000)
[Test]  Epoch: 88	Loss: 0.032355	Acc: 55.2% (5525/10000)
[Test]  Epoch: 89	Loss: 0.032262	Acc: 55.1% (5510/10000)
[Test]  Epoch: 90	Loss: 0.032182	Acc: 55.2% (5520/10000)
[Test]  Epoch: 91	Loss: 0.032353	Acc: 55.1% (5514/10000)
[Test]  Epoch: 92	Loss: 0.032436	Acc: 55.1% (5510/10000)
[Test]  Epoch: 93	Loss: 0.032347	Acc: 55.1% (5511/10000)
[Test]  Epoch: 94	Loss: 0.032265	Acc: 55.2% (5516/10000)
[Test]  Epoch: 95	Loss: 0.032304	Acc: 55.2% (5519/10000)
[Test]  Epoch: 96	Loss: 0.032299	Acc: 55.2% (5524/10000)
[Test]  Epoch: 97	Loss: 0.032529	Acc: 54.9% (5485/10000)
[Test]  Epoch: 98	Loss: 0.032404	Acc: 55.2% (5525/10000)
[Test]  Epoch: 99	Loss: 0.032269	Acc: 55.2% (5517/10000)
[Test]  Epoch: 100	Loss: 0.032246	Acc: 55.2% (5516/10000)
===========finish==========
['2024-08-19', '16:46:20.984863', '100', 'test', '0.032246278142929075', '55.16', '55.3']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer1.2.conv2.weight', 'layer1.0.conv2.weight', 'layer2.1.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer2.3.conv1.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer1.2.conv2.weight', 'layer1.0.conv2.weight', 'layer2.1.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer2.3.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.083980	Acc: 23.2% (2319/10000)
[Test]  Epoch: 2	Loss: 0.048568	Acc: 41.9% (4194/10000)
[Test]  Epoch: 3	Loss: 0.042275	Acc: 46.8% (4677/10000)
[Test]  Epoch: 4	Loss: 0.040207	Acc: 47.9% (4791/10000)
[Test]  Epoch: 5	Loss: 0.039770	Acc: 48.5% (4845/10000)
[Test]  Epoch: 6	Loss: 0.039448	Acc: 48.7% (4870/10000)
[Test]  Epoch: 7	Loss: 0.039368	Acc: 48.6% (4858/10000)
[Test]  Epoch: 8	Loss: 0.039161	Acc: 48.7% (4870/10000)
[Test]  Epoch: 9	Loss: 0.038893	Acc: 48.4% (4835/10000)
[Test]  Epoch: 10	Loss: 0.038483	Acc: 48.7% (4870/10000)
[Test]  Epoch: 11	Loss: 0.038540	Acc: 48.8% (4884/10000)
[Test]  Epoch: 12	Loss: 0.038643	Acc: 48.7% (4869/10000)
[Test]  Epoch: 13	Loss: 0.038447	Acc: 48.8% (4879/10000)
[Test]  Epoch: 14	Loss: 0.038156	Acc: 49.0% (4896/10000)
[Test]  Epoch: 15	Loss: 0.037890	Acc: 49.1% (4913/10000)
[Test]  Epoch: 16	Loss: 0.037842	Acc: 49.2% (4922/10000)
[Test]  Epoch: 17	Loss: 0.037754	Acc: 49.1% (4908/10000)
[Test]  Epoch: 18	Loss: 0.037700	Acc: 48.6% (4862/10000)
[Test]  Epoch: 19	Loss: 0.038070	Acc: 48.7% (4866/10000)
[Test]  Epoch: 20	Loss: 0.037513	Acc: 49.1% (4907/10000)
[Test]  Epoch: 21	Loss: 0.037363	Acc: 49.1% (4914/10000)
[Test]  Epoch: 22	Loss: 0.037274	Acc: 49.1% (4912/10000)
[Test]  Epoch: 23	Loss: 0.037139	Acc: 49.2% (4921/10000)
[Test]  Epoch: 24	Loss: 0.037219	Acc: 49.1% (4911/10000)
[Test]  Epoch: 25	Loss: 0.037151	Acc: 49.1% (4910/10000)
[Test]  Epoch: 26	Loss: 0.036785	Acc: 49.1% (4915/10000)
[Test]  Epoch: 27	Loss: 0.036814	Acc: 49.4% (4940/10000)
[Test]  Epoch: 28	Loss: 0.036649	Acc: 49.5% (4951/10000)
[Test]  Epoch: 29	Loss: 0.036554	Acc: 49.6% (4957/10000)
[Test]  Epoch: 30	Loss: 0.036480	Acc: 49.7% (4969/10000)
[Test]  Epoch: 31	Loss: 0.036319	Acc: 49.6% (4962/10000)
[Test]  Epoch: 32	Loss: 0.036310	Acc: 49.8% (4982/10000)
[Test]  Epoch: 33	Loss: 0.036205	Acc: 49.8% (4984/10000)
[Test]  Epoch: 34	Loss: 0.036214	Acc: 49.5% (4952/10000)
[Test]  Epoch: 35	Loss: 0.036049	Acc: 50.0% (4998/10000)
[Test]  Epoch: 36	Loss: 0.036103	Acc: 49.9% (4990/10000)
[Test]  Epoch: 37	Loss: 0.036009	Acc: 49.9% (4991/10000)
[Test]  Epoch: 38	Loss: 0.035931	Acc: 49.8% (4983/10000)
[Test]  Epoch: 39	Loss: 0.035873	Acc: 50.1% (5010/10000)
[Test]  Epoch: 40	Loss: 0.035935	Acc: 49.9% (4988/10000)
[Test]  Epoch: 41	Loss: 0.035683	Acc: 49.9% (4987/10000)
[Test]  Epoch: 42	Loss: 0.035569	Acc: 50.2% (5017/10000)
[Test]  Epoch: 43	Loss: 0.035652	Acc: 50.1% (5007/10000)
[Test]  Epoch: 44	Loss: 0.035639	Acc: 49.8% (4976/10000)
[Test]  Epoch: 45	Loss: 0.035637	Acc: 50.3% (5028/10000)
[Test]  Epoch: 46	Loss: 0.035519	Acc: 50.2% (5021/10000)
[Test]  Epoch: 47	Loss: 0.035485	Acc: 50.2% (5020/10000)
[Test]  Epoch: 48	Loss: 0.035315	Acc: 50.3% (5030/10000)
[Test]  Epoch: 49	Loss: 0.035266	Acc: 50.4% (5035/10000)
[Test]  Epoch: 50	Loss: 0.035242	Acc: 50.2% (5019/10000)
[Test]  Epoch: 51	Loss: 0.035227	Acc: 50.3% (5030/10000)
[Test]  Epoch: 52	Loss: 0.035139	Acc: 50.3% (5028/10000)
[Test]  Epoch: 53	Loss: 0.035134	Acc: 50.2% (5019/10000)
[Test]  Epoch: 54	Loss: 0.035145	Acc: 50.4% (5044/10000)
[Test]  Epoch: 55	Loss: 0.035044	Acc: 50.5% (5049/10000)
[Test]  Epoch: 56	Loss: 0.035021	Acc: 50.5% (5048/10000)
[Test]  Epoch: 57	Loss: 0.034948	Acc: 50.5% (5047/10000)
[Test]  Epoch: 58	Loss: 0.035020	Acc: 50.3% (5028/10000)
[Test]  Epoch: 59	Loss: 0.034777	Acc: 50.7% (5067/10000)
[Test]  Epoch: 60	Loss: 0.035068	Acc: 50.4% (5040/10000)
[Test]  Epoch: 61	Loss: 0.035037	Acc: 50.0% (5001/10000)
[Test]  Epoch: 62	Loss: 0.035048	Acc: 50.4% (5039/10000)
[Test]  Epoch: 63	Loss: 0.034960	Acc: 50.4% (5042/10000)
[Test]  Epoch: 64	Loss: 0.034868	Acc: 50.3% (5031/10000)
[Test]  Epoch: 65	Loss: 0.034888	Acc: 50.3% (5026/10000)
[Test]  Epoch: 66	Loss: 0.034812	Acc: 50.2% (5021/10000)
[Test]  Epoch: 67	Loss: 0.034894	Acc: 50.4% (5044/10000)
[Test]  Epoch: 68	Loss: 0.034940	Acc: 50.4% (5043/10000)
[Test]  Epoch: 69	Loss: 0.034840	Acc: 50.5% (5046/10000)
[Test]  Epoch: 70	Loss: 0.034797	Acc: 50.5% (5050/10000)
[Test]  Epoch: 71	Loss: 0.034772	Acc: 50.4% (5042/10000)
[Test]  Epoch: 72	Loss: 0.034775	Acc: 50.5% (5053/10000)
[Test]  Epoch: 73	Loss: 0.034697	Acc: 50.6% (5064/10000)
[Test]  Epoch: 74	Loss: 0.034795	Acc: 50.6% (5063/10000)
[Test]  Epoch: 75	Loss: 0.034754	Acc: 50.6% (5064/10000)
[Test]  Epoch: 76	Loss: 0.034888	Acc: 50.4% (5044/10000)
[Test]  Epoch: 77	Loss: 0.034828	Acc: 50.6% (5057/10000)
[Test]  Epoch: 78	Loss: 0.034782	Acc: 50.4% (5039/10000)
[Test]  Epoch: 79	Loss: 0.034844	Acc: 50.3% (5034/10000)
[Test]  Epoch: 80	Loss: 0.034668	Acc: 50.8% (5076/10000)
[Test]  Epoch: 81	Loss: 0.034835	Acc: 50.5% (5047/10000)
[Test]  Epoch: 82	Loss: 0.034887	Acc: 50.4% (5041/10000)
[Test]  Epoch: 83	Loss: 0.034886	Acc: 50.5% (5046/10000)
[Test]  Epoch: 84	Loss: 0.034836	Acc: 50.5% (5047/10000)
[Test]  Epoch: 85	Loss: 0.034811	Acc: 50.6% (5056/10000)
[Test]  Epoch: 86	Loss: 0.034696	Acc: 50.7% (5066/10000)
[Test]  Epoch: 87	Loss: 0.034732	Acc: 50.5% (5047/10000)
[Test]  Epoch: 88	Loss: 0.034817	Acc: 50.6% (5056/10000)
[Test]  Epoch: 89	Loss: 0.034801	Acc: 50.4% (5044/10000)
[Test]  Epoch: 90	Loss: 0.034713	Acc: 50.3% (5027/10000)
[Test]  Epoch: 91	Loss: 0.034658	Acc: 50.6% (5057/10000)
[Test]  Epoch: 92	Loss: 0.034791	Acc: 50.4% (5040/10000)
[Test]  Epoch: 93	Loss: 0.034745	Acc: 50.3% (5027/10000)
[Test]  Epoch: 94	Loss: 0.034742	Acc: 50.4% (5036/10000)
[Test]  Epoch: 95	Loss: 0.034923	Acc: 50.3% (5031/10000)
[Test]  Epoch: 96	Loss: 0.034818	Acc: 50.3% (5034/10000)
[Test]  Epoch: 97	Loss: 0.034798	Acc: 50.5% (5055/10000)
[Test]  Epoch: 98	Loss: 0.034624	Acc: 50.5% (5055/10000)
[Test]  Epoch: 99	Loss: 0.034611	Acc: 50.7% (5069/10000)
[Test]  Epoch: 100	Loss: 0.034706	Acc: 50.6% (5059/10000)
===========finish==========
['2024-08-19', '16:50:41.931674', '100', 'test', '0.034706023156642916', '50.59', '50.76']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer1.2.conv2.weight', 'layer1.0.conv2.weight', 'layer2.1.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer2.3.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.3.conv3.weight', 'layer2.0.conv3.weight', 'layer3.5.conv3.weight', 'layer2.1.conv2.weight', 'layer3.3.conv3.weight', 'layer3.3.conv1.weight', 'layer3.4.conv3.weight', 'layer3.5.conv1.weight', 'layer2.2.conv2.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer1.2.conv2.weight', 'layer1.0.conv2.weight', 'layer2.1.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer2.3.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.3.conv3.weight', 'layer2.0.conv3.weight', 'layer3.5.conv3.weight', 'layer2.1.conv2.weight', 'layer3.3.conv3.weight', 'layer3.3.conv1.weight', 'layer3.4.conv3.weight', 'layer3.5.conv1.weight', 'layer2.2.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.107465	Acc: 18.8% (1878/10000)
[Test]  Epoch: 2	Loss: 0.051963	Acc: 37.5% (3753/10000)
[Test]  Epoch: 3	Loss: 0.043968	Acc: 43.0% (4304/10000)
[Test]  Epoch: 4	Loss: 0.041708	Acc: 44.9% (4493/10000)
[Test]  Epoch: 5	Loss: 0.041330	Acc: 45.0% (4495/10000)
[Test]  Epoch: 6	Loss: 0.041021	Acc: 45.3% (4526/10000)
[Test]  Epoch: 7	Loss: 0.040915	Acc: 45.2% (4524/10000)
[Test]  Epoch: 8	Loss: 0.041077	Acc: 45.1% (4507/10000)
[Test]  Epoch: 9	Loss: 0.040780	Acc: 44.9% (4490/10000)
[Test]  Epoch: 10	Loss: 0.040259	Acc: 45.3% (4531/10000)
[Test]  Epoch: 11	Loss: 0.040267	Acc: 45.4% (4536/10000)
[Test]  Epoch: 12	Loss: 0.040296	Acc: 45.3% (4531/10000)
[Test]  Epoch: 13	Loss: 0.040239	Acc: 45.2% (4522/10000)
[Test]  Epoch: 14	Loss: 0.039976	Acc: 45.5% (4552/10000)
[Test]  Epoch: 15	Loss: 0.039629	Acc: 45.6% (4562/10000)
[Test]  Epoch: 16	Loss: 0.039638	Acc: 45.6% (4564/10000)
[Test]  Epoch: 17	Loss: 0.039409	Acc: 45.6% (4557/10000)
[Test]  Epoch: 18	Loss: 0.039521	Acc: 45.7% (4573/10000)
[Test]  Epoch: 19	Loss: 0.039361	Acc: 45.9% (4585/10000)
[Test]  Epoch: 20	Loss: 0.038927	Acc: 46.1% (4614/10000)
[Test]  Epoch: 21	Loss: 0.038955	Acc: 46.1% (4614/10000)
[Test]  Epoch: 22	Loss: 0.039085	Acc: 46.0% (4601/10000)
[Test]  Epoch: 23	Loss: 0.039044	Acc: 45.9% (4591/10000)
[Test]  Epoch: 24	Loss: 0.038880	Acc: 46.1% (4606/10000)
[Test]  Epoch: 25	Loss: 0.038921	Acc: 45.9% (4588/10000)
[Test]  Epoch: 26	Loss: 0.038700	Acc: 46.3% (4627/10000)
[Test]  Epoch: 27	Loss: 0.038818	Acc: 46.3% (4629/10000)
[Test]  Epoch: 28	Loss: 0.038677	Acc: 46.3% (4631/10000)
[Test]  Epoch: 29	Loss: 0.038731	Acc: 46.0% (4605/10000)
[Test]  Epoch: 30	Loss: 0.038552	Acc: 46.3% (4631/10000)
[Test]  Epoch: 31	Loss: 0.038494	Acc: 46.3% (4632/10000)
[Test]  Epoch: 32	Loss: 0.038412	Acc: 46.4% (4635/10000)
[Test]  Epoch: 33	Loss: 0.038448	Acc: 46.2% (4621/10000)
[Test]  Epoch: 34	Loss: 0.038432	Acc: 46.2% (4616/10000)
[Test]  Epoch: 35	Loss: 0.038283	Acc: 46.1% (4610/10000)
[Test]  Epoch: 36	Loss: 0.038324	Acc: 46.0% (4602/10000)
[Test]  Epoch: 37	Loss: 0.038227	Acc: 46.1% (4614/10000)
[Test]  Epoch: 38	Loss: 0.038158	Acc: 46.2% (4621/10000)
[Test]  Epoch: 39	Loss: 0.038188	Acc: 46.0% (4603/10000)
[Test]  Epoch: 40	Loss: 0.038096	Acc: 46.3% (4632/10000)
[Test]  Epoch: 41	Loss: 0.038138	Acc: 46.1% (4613/10000)
[Test]  Epoch: 42	Loss: 0.037987	Acc: 46.2% (4623/10000)
[Test]  Epoch: 43	Loss: 0.037993	Acc: 46.4% (4635/10000)
[Test]  Epoch: 44	Loss: 0.037968	Acc: 46.2% (4625/10000)
[Test]  Epoch: 45	Loss: 0.037954	Acc: 46.3% (4632/10000)
[Test]  Epoch: 46	Loss: 0.037873	Acc: 46.2% (4623/10000)
[Test]  Epoch: 47	Loss: 0.037879	Acc: 46.5% (4646/10000)
[Test]  Epoch: 48	Loss: 0.037725	Acc: 46.4% (4641/10000)
[Test]  Epoch: 49	Loss: 0.037819	Acc: 46.3% (4630/10000)
[Test]  Epoch: 50	Loss: 0.037720	Acc: 46.6% (4658/10000)
[Test]  Epoch: 51	Loss: 0.037718	Acc: 46.7% (4668/10000)
[Test]  Epoch: 52	Loss: 0.037594	Acc: 46.8% (4677/10000)
[Test]  Epoch: 53	Loss: 0.037752	Acc: 46.6% (4656/10000)
[Test]  Epoch: 54	Loss: 0.037723	Acc: 46.6% (4661/10000)
[Test]  Epoch: 55	Loss: 0.037500	Acc: 46.6% (4661/10000)
[Test]  Epoch: 56	Loss: 0.037440	Acc: 46.6% (4660/10000)
[Test]  Epoch: 57	Loss: 0.037335	Acc: 46.7% (4668/10000)
[Test]  Epoch: 58	Loss: 0.037412	Acc: 46.7% (4672/10000)
[Test]  Epoch: 59	Loss: 0.037313	Acc: 46.7% (4670/10000)
[Test]  Epoch: 60	Loss: 0.037417	Acc: 46.8% (4683/10000)
[Test]  Epoch: 61	Loss: 0.037434	Acc: 46.9% (4691/10000)
[Test]  Epoch: 62	Loss: 0.037323	Acc: 46.8% (4677/10000)
[Test]  Epoch: 63	Loss: 0.037348	Acc: 47.0% (4696/10000)
[Test]  Epoch: 64	Loss: 0.037358	Acc: 46.8% (4683/10000)
[Test]  Epoch: 65	Loss: 0.037446	Acc: 46.6% (4664/10000)
[Test]  Epoch: 66	Loss: 0.037293	Acc: 46.9% (4692/10000)
[Test]  Epoch: 67	Loss: 0.037256	Acc: 46.8% (4683/10000)
[Test]  Epoch: 68	Loss: 0.037283	Acc: 46.9% (4690/10000)
[Test]  Epoch: 69	Loss: 0.037325	Acc: 46.8% (4681/10000)
[Test]  Epoch: 70	Loss: 0.037261	Acc: 46.9% (4692/10000)
[Test]  Epoch: 71	Loss: 0.037205	Acc: 46.9% (4688/10000)
[Test]  Epoch: 72	Loss: 0.037206	Acc: 46.8% (4682/10000)
[Test]  Epoch: 73	Loss: 0.037141	Acc: 46.9% (4692/10000)
[Test]  Epoch: 74	Loss: 0.037275	Acc: 46.9% (4689/10000)
[Test]  Epoch: 75	Loss: 0.037278	Acc: 46.8% (4680/10000)
[Test]  Epoch: 76	Loss: 0.037313	Acc: 46.7% (4674/10000)
[Test]  Epoch: 77	Loss: 0.037218	Acc: 46.9% (4694/10000)
[Test]  Epoch: 78	Loss: 0.037178	Acc: 46.9% (4692/10000)
[Test]  Epoch: 79	Loss: 0.037311	Acc: 46.9% (4690/10000)
[Test]  Epoch: 80	Loss: 0.037155	Acc: 46.9% (4691/10000)
[Test]  Epoch: 81	Loss: 0.037237	Acc: 46.9% (4688/10000)
[Test]  Epoch: 82	Loss: 0.037163	Acc: 46.6% (4665/10000)
[Test]  Epoch: 83	Loss: 0.037244	Acc: 46.8% (4679/10000)
[Test]  Epoch: 84	Loss: 0.037146	Acc: 47.0% (4703/10000)
[Test]  Epoch: 85	Loss: 0.037207	Acc: 46.9% (4689/10000)
[Test]  Epoch: 86	Loss: 0.037191	Acc: 47.0% (4701/10000)
[Test]  Epoch: 87	Loss: 0.037168	Acc: 47.0% (4696/10000)
[Test]  Epoch: 88	Loss: 0.037143	Acc: 47.0% (4702/10000)
[Test]  Epoch: 89	Loss: 0.037202	Acc: 46.9% (4690/10000)
[Test]  Epoch: 90	Loss: 0.037204	Acc: 46.9% (4689/10000)
[Test]  Epoch: 91	Loss: 0.037178	Acc: 47.1% (4706/10000)
[Test]  Epoch: 92	Loss: 0.037222	Acc: 46.7% (4669/10000)
[Test]  Epoch: 93	Loss: 0.037044	Acc: 46.9% (4689/10000)
[Test]  Epoch: 94	Loss: 0.037045	Acc: 46.9% (4691/10000)
[Test]  Epoch: 95	Loss: 0.037124	Acc: 46.9% (4688/10000)
[Test]  Epoch: 96	Loss: 0.037145	Acc: 46.9% (4685/10000)
[Test]  Epoch: 97	Loss: 0.037207	Acc: 46.8% (4678/10000)
[Test]  Epoch: 98	Loss: 0.037082	Acc: 46.9% (4693/10000)
[Test]  Epoch: 99	Loss: 0.037102	Acc: 46.9% (4693/10000)
[Test]  Epoch: 100	Loss: 0.037139	Acc: 46.9% (4687/10000)
===========finish==========
['2024-08-19', '16:55:09.920336', '100', 'test', '0.037139306914806366', '46.87', '47.06']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer1.2.conv2.weight', 'layer1.0.conv2.weight', 'layer2.1.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer2.3.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.3.conv3.weight', 'layer2.0.conv3.weight', 'layer3.5.conv3.weight', 'layer2.1.conv2.weight', 'layer3.3.conv3.weight', 'layer3.3.conv1.weight', 'layer3.4.conv3.weight', 'layer3.5.conv1.weight', 'layer2.2.conv2.weight', 'layer3.4.conv1.weight', 'layer3.2.conv1.weight', 'layer3.2.conv3.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer2.0.conv2.weight', 'last_linear.weight', 'layer3.1.conv3.weight', 'layer2.0.downsample.0.weight', 'layer3.5.conv2.weight', 'layer3.0.conv1.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer1.2.conv2.weight', 'layer1.0.conv2.weight', 'layer2.1.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer2.3.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.3.conv3.weight', 'layer2.0.conv3.weight', 'layer3.5.conv3.weight', 'layer2.1.conv2.weight', 'layer3.3.conv3.weight', 'layer3.3.conv1.weight', 'layer3.4.conv3.weight', 'layer3.5.conv1.weight', 'layer2.2.conv2.weight', 'layer3.4.conv1.weight', 'layer3.2.conv1.weight', 'layer3.2.conv3.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer2.0.conv2.weight', 'last_linear.weight', 'layer3.1.conv3.weight', 'layer2.0.downsample.0.weight', 'layer3.5.conv2.weight', 'layer3.0.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.065938	Acc: 9.9% (989/10000)
[Test]  Epoch: 2	Loss: 0.056731	Acc: 18.2% (1825/10000)
[Test]  Epoch: 3	Loss: 0.052851	Acc: 22.0% (2198/10000)
[Test]  Epoch: 4	Loss: 0.050019	Acc: 25.1% (2505/10000)
[Test]  Epoch: 5	Loss: 0.048590	Acc: 26.4% (2645/10000)
[Test]  Epoch: 6	Loss: 0.047952	Acc: 27.3% (2726/10000)
[Test]  Epoch: 7	Loss: 0.047598	Acc: 27.5% (2752/10000)
[Test]  Epoch: 8	Loss: 0.047323	Acc: 27.8% (2778/10000)
[Test]  Epoch: 9	Loss: 0.047300	Acc: 27.8% (2783/10000)
[Test]  Epoch: 10	Loss: 0.047264	Acc: 27.9% (2787/10000)
[Test]  Epoch: 11	Loss: 0.047159	Acc: 27.9% (2792/10000)
[Test]  Epoch: 12	Loss: 0.046935	Acc: 28.5% (2846/10000)
[Test]  Epoch: 13	Loss: 0.047013	Acc: 28.5% (2848/10000)
[Test]  Epoch: 14	Loss: 0.047095	Acc: 28.4% (2843/10000)
[Test]  Epoch: 15	Loss: 0.047020	Acc: 28.2% (2822/10000)
[Test]  Epoch: 16	Loss: 0.047111	Acc: 28.1% (2807/10000)
[Test]  Epoch: 17	Loss: 0.047048	Acc: 28.3% (2829/10000)
[Test]  Epoch: 18	Loss: 0.047079	Acc: 28.3% (2831/10000)
[Test]  Epoch: 19	Loss: 0.047116	Acc: 28.3% (2828/10000)
[Test]  Epoch: 20	Loss: 0.046969	Acc: 28.4% (2838/10000)
[Test]  Epoch: 21	Loss: 0.046934	Acc: 28.4% (2837/10000)
[Test]  Epoch: 22	Loss: 0.046954	Acc: 28.5% (2848/10000)
[Test]  Epoch: 23	Loss: 0.046970	Acc: 28.7% (2868/10000)
[Test]  Epoch: 24	Loss: 0.046987	Acc: 28.5% (2854/10000)
[Test]  Epoch: 25	Loss: 0.046891	Acc: 28.6% (2865/10000)
[Test]  Epoch: 26	Loss: 0.046875	Acc: 28.8% (2882/10000)
[Test]  Epoch: 27	Loss: 0.046957	Acc: 28.8% (2883/10000)
[Test]  Epoch: 28	Loss: 0.046923	Acc: 28.6% (2859/10000)
[Test]  Epoch: 29	Loss: 0.046977	Acc: 29.0% (2896/10000)
[Test]  Epoch: 30	Loss: 0.046925	Acc: 28.8% (2875/10000)
[Test]  Epoch: 31	Loss: 0.046838	Acc: 28.9% (2891/10000)
[Test]  Epoch: 32	Loss: 0.046942	Acc: 28.8% (2877/10000)
[Test]  Epoch: 33	Loss: 0.046955	Acc: 28.7% (2870/10000)
[Test]  Epoch: 34	Loss: 0.046943	Acc: 28.9% (2886/10000)
[Test]  Epoch: 35	Loss: 0.046952	Acc: 29.0% (2896/10000)
[Test]  Epoch: 36	Loss: 0.046908	Acc: 29.2% (2918/10000)
[Test]  Epoch: 37	Loss: 0.046979	Acc: 28.7% (2871/10000)
[Test]  Epoch: 38	Loss: 0.046905	Acc: 29.1% (2909/10000)
[Test]  Epoch: 39	Loss: 0.046875	Acc: 29.1% (2906/10000)
[Test]  Epoch: 40	Loss: 0.046904	Acc: 29.1% (2906/10000)
[Test]  Epoch: 41	Loss: 0.046779	Acc: 28.9% (2885/10000)
[Test]  Epoch: 42	Loss: 0.046725	Acc: 29.2% (2917/10000)
[Test]  Epoch: 43	Loss: 0.046801	Acc: 29.1% (2905/10000)
[Test]  Epoch: 44	Loss: 0.046815	Acc: 29.2% (2921/10000)
[Test]  Epoch: 45	Loss: 0.046761	Acc: 29.2% (2922/10000)
[Test]  Epoch: 46	Loss: 0.046813	Acc: 29.4% (2943/10000)
[Test]  Epoch: 47	Loss: 0.046789	Acc: 29.0% (2896/10000)
[Test]  Epoch: 48	Loss: 0.046752	Acc: 29.0% (2899/10000)
[Test]  Epoch: 49	Loss: 0.046839	Acc: 28.9% (2887/10000)
[Test]  Epoch: 50	Loss: 0.046794	Acc: 29.1% (2907/10000)
[Test]  Epoch: 51	Loss: 0.046716	Acc: 29.3% (2926/10000)
[Test]  Epoch: 52	Loss: 0.046819	Acc: 29.0% (2899/10000)
[Test]  Epoch: 53	Loss: 0.046921	Acc: 28.9% (2889/10000)
[Test]  Epoch: 54	Loss: 0.046865	Acc: 28.9% (2894/10000)
[Test]  Epoch: 55	Loss: 0.046779	Acc: 29.1% (2913/10000)
[Test]  Epoch: 56	Loss: 0.046745	Acc: 29.1% (2915/10000)
[Test]  Epoch: 57	Loss: 0.046725	Acc: 29.1% (2913/10000)
[Test]  Epoch: 58	Loss: 0.046766	Acc: 28.9% (2893/10000)
[Test]  Epoch: 59	Loss: 0.046768	Acc: 29.2% (2925/10000)
[Test]  Epoch: 60	Loss: 0.046783	Acc: 29.1% (2912/10000)
[Test]  Epoch: 61	Loss: 0.046809	Acc: 29.2% (2919/10000)
[Test]  Epoch: 62	Loss: 0.046748	Acc: 29.4% (2939/10000)
[Test]  Epoch: 63	Loss: 0.046781	Acc: 29.3% (2927/10000)
[Test]  Epoch: 64	Loss: 0.046752	Acc: 29.1% (2909/10000)
[Test]  Epoch: 65	Loss: 0.046790	Acc: 28.9% (2885/10000)
[Test]  Epoch: 66	Loss: 0.046761	Acc: 29.4% (2935/10000)
[Test]  Epoch: 67	Loss: 0.046737	Acc: 29.2% (2924/10000)
[Test]  Epoch: 68	Loss: 0.046807	Acc: 29.2% (2924/10000)
[Test]  Epoch: 69	Loss: 0.046807	Acc: 29.2% (2917/10000)
[Test]  Epoch: 70	Loss: 0.046811	Acc: 29.1% (2909/10000)
[Test]  Epoch: 71	Loss: 0.046734	Acc: 29.1% (2913/10000)
[Test]  Epoch: 72	Loss: 0.046755	Acc: 29.0% (2901/10000)
[Test]  Epoch: 73	Loss: 0.046778	Acc: 29.0% (2903/10000)
[Test]  Epoch: 74	Loss: 0.046746	Acc: 29.0% (2901/10000)
[Test]  Epoch: 75	Loss: 0.046734	Acc: 29.2% (2916/10000)
[Test]  Epoch: 76	Loss: 0.046711	Acc: 29.3% (2930/10000)
[Test]  Epoch: 77	Loss: 0.046701	Acc: 29.2% (2918/10000)
[Test]  Epoch: 78	Loss: 0.046750	Acc: 29.1% (2915/10000)
[Test]  Epoch: 79	Loss: 0.046815	Acc: 29.1% (2910/10000)
[Test]  Epoch: 80	Loss: 0.046809	Acc: 28.8% (2884/10000)
[Test]  Epoch: 81	Loss: 0.046775	Acc: 29.2% (2923/10000)
[Test]  Epoch: 82	Loss: 0.046795	Acc: 29.1% (2910/10000)
[Test]  Epoch: 83	Loss: 0.046749	Acc: 29.1% (2915/10000)
[Test]  Epoch: 84	Loss: 0.046738	Acc: 29.4% (2944/10000)
[Test]  Epoch: 85	Loss: 0.046662	Acc: 29.3% (2927/10000)
[Test]  Epoch: 86	Loss: 0.046675	Acc: 29.1% (2906/10000)
[Test]  Epoch: 87	Loss: 0.046682	Acc: 29.1% (2912/10000)
[Test]  Epoch: 88	Loss: 0.046723	Acc: 29.2% (2919/10000)
[Test]  Epoch: 89	Loss: 0.046766	Acc: 29.1% (2905/10000)
[Test]  Epoch: 90	Loss: 0.046828	Acc: 28.8% (2884/10000)
[Test]  Epoch: 91	Loss: 0.046755	Acc: 29.1% (2908/10000)
[Test]  Epoch: 92	Loss: 0.046664	Acc: 28.9% (2894/10000)
[Test]  Epoch: 93	Loss: 0.046685	Acc: 29.0% (2899/10000)
[Test]  Epoch: 94	Loss: 0.046697	Acc: 29.1% (2914/10000)
[Test]  Epoch: 95	Loss: 0.046680	Acc: 29.2% (2925/10000)
[Test]  Epoch: 96	Loss: 0.046724	Acc: 29.2% (2917/10000)
[Test]  Epoch: 97	Loss: 0.046801	Acc: 29.1% (2906/10000)
[Test]  Epoch: 98	Loss: 0.046653	Acc: 29.2% (2922/10000)
[Test]  Epoch: 99	Loss: 0.046779	Acc: 29.0% (2902/10000)
[Test]  Epoch: 100	Loss: 0.046808	Acc: 29.0% (2902/10000)
===========finish==========
['2024-08-19', '16:59:45.762089', '100', 'test', '0.04680818147659302', '29.02', '29.44']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-resnet50-channel resnet50 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar100-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer1.2.conv2.weight', 'layer1.0.conv2.weight', 'layer2.1.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer2.3.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.3.conv3.weight', 'layer2.0.conv3.weight', 'layer3.5.conv3.weight', 'layer2.1.conv2.weight', 'layer3.3.conv3.weight', 'layer3.3.conv1.weight', 'layer3.4.conv3.weight', 'layer3.5.conv1.weight', 'layer2.2.conv2.weight', 'layer3.4.conv1.weight', 'layer3.2.conv1.weight', 'layer3.2.conv3.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer2.0.conv2.weight', 'last_linear.weight', 'layer3.1.conv3.weight', 'layer2.0.downsample.0.weight', 'layer3.5.conv2.weight', 'layer3.0.conv1.weight', 'layer3.3.conv2.weight', 'layer3.4.conv2.weight', 'layer3.2.conv2.weight', 'layer3.0.conv3.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight']
['layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer4.2.bn2.weight', 'layer4.2.conv3.weight', 'layer4.1.bn2.weight', 'layer4.2.conv1.weight', 'layer4.2.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn2.weight', 'layer1.2.bn2.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer2.1.bn2.weight', 'layer2.3.bn2.weight', 'layer3.3.bn1.weight', 'layer1.2.bn3.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer1.1.bn3.weight', 'layer3.3.bn2.weight', 'layer3.4.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.4.bn2.weight', 'layer3.2.bn1.weight', 'layer1.0.bn3.weight', 'layer3.2.bn2.weight', 'layer3.1.bn1.weight', 'layer4.0.downsample.1.weight', 'bn1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn3.weight', 'layer1.0.downsample.1.weight', 'layer2.1.bn3.weight', 'layer2.3.bn3.weight', 'layer3.0.bn2.weight', 'layer2.2.bn3.weight', 'layer3.0.bn1.weight', 'layer3.3.bn3.weight', 'layer3.5.bn3.weight', 'layer3.4.bn3.weight', 'layer4.1.conv3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.1.bn3.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer4.1.conv2.weight', 'layer1.0.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv3.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer1.2.conv2.weight', 'layer1.0.conv2.weight', 'layer2.1.conv1.weight', 'layer2.2.conv1.weight', 'layer2.1.conv3.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer2.3.conv1.weight', 'layer2.2.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.3.conv3.weight', 'layer2.0.conv3.weight', 'layer3.5.conv3.weight', 'layer2.1.conv2.weight', 'layer3.3.conv3.weight', 'layer3.3.conv1.weight', 'layer3.4.conv3.weight', 'layer3.5.conv1.weight', 'layer2.2.conv2.weight', 'layer3.4.conv1.weight', 'layer3.2.conv1.weight', 'layer3.2.conv3.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer2.0.conv2.weight', 'last_linear.weight', 'layer3.1.conv3.weight', 'layer2.0.downsample.0.weight', 'layer3.5.conv2.weight', 'layer3.0.conv1.weight', 'layer3.3.conv2.weight', 'layer3.4.conv2.weight', 'layer3.2.conv2.weight', 'layer3.0.conv3.weight', 'layer3.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer3.0.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.074573	Acc: 4.5% (452/10000)
[Test]  Epoch: 2	Loss: 0.064880	Acc: 11.0% (1103/10000)
[Test]  Epoch: 3	Loss: 0.061354	Acc: 13.6% (1360/10000)
[Test]  Epoch: 4	Loss: 0.058884	Acc: 16.1% (1614/10000)
[Test]  Epoch: 5	Loss: 0.057383	Acc: 18.0% (1796/10000)
[Test]  Epoch: 6	Loss: 0.056552	Acc: 18.5% (1849/10000)
[Test]  Epoch: 7	Loss: 0.056000	Acc: 18.8% (1884/10000)
[Test]  Epoch: 8	Loss: 0.055949	Acc: 18.9% (1891/10000)
[Test]  Epoch: 9	Loss: 0.055881	Acc: 19.3% (1929/10000)
[Test]  Epoch: 10	Loss: 0.055581	Acc: 19.6% (1955/10000)
[Test]  Epoch: 11	Loss: 0.055598	Acc: 19.6% (1959/10000)
[Test]  Epoch: 12	Loss: 0.055468	Acc: 19.8% (1981/10000)
[Test]  Epoch: 13	Loss: 0.055560	Acc: 20.0% (2001/10000)
[Test]  Epoch: 14	Loss: 0.055652	Acc: 20.0% (1999/10000)
[Test]  Epoch: 15	Loss: 0.055453	Acc: 20.1% (2008/10000)
[Test]  Epoch: 16	Loss: 0.055674	Acc: 19.8% (1984/10000)
[Test]  Epoch: 17	Loss: 0.055434	Acc: 20.0% (2003/10000)
[Test]  Epoch: 18	Loss: 0.055650	Acc: 20.1% (2005/10000)
[Test]  Epoch: 19	Loss: 0.055637	Acc: 20.3% (2033/10000)
[Test]  Epoch: 20	Loss: 0.055474	Acc: 20.4% (2036/10000)
[Test]  Epoch: 21	Loss: 0.055373	Acc: 20.4% (2037/10000)
[Test]  Epoch: 22	Loss: 0.055430	Acc: 20.3% (2032/10000)
[Test]  Epoch: 23	Loss: 0.055456	Acc: 20.5% (2048/10000)
[Test]  Epoch: 24	Loss: 0.055478	Acc: 20.5% (2051/10000)
[Test]  Epoch: 25	Loss: 0.055432	Acc: 20.5% (2047/10000)
[Test]  Epoch: 26	Loss: 0.055418	Acc: 20.4% (2035/10000)
[Test]  Epoch: 27	Loss: 0.055335	Acc: 20.6% (2055/10000)
[Test]  Epoch: 28	Loss: 0.055363	Acc: 20.5% (2052/10000)
[Test]  Epoch: 29	Loss: 0.055418	Acc: 20.6% (2058/10000)
[Test]  Epoch: 30	Loss: 0.055266	Acc: 20.9% (2093/10000)
[Test]  Epoch: 31	Loss: 0.055365	Acc: 20.6% (2063/10000)
[Test]  Epoch: 32	Loss: 0.055235	Acc: 20.7% (2067/10000)
[Test]  Epoch: 33	Loss: 0.055361	Acc: 20.8% (2075/10000)
[Test]  Epoch: 34	Loss: 0.055281	Acc: 20.7% (2072/10000)
[Test]  Epoch: 35	Loss: 0.055191	Acc: 20.7% (2067/10000)
[Test]  Epoch: 36	Loss: 0.055205	Acc: 20.7% (2071/10000)
[Test]  Epoch: 37	Loss: 0.055161	Acc: 20.6% (2065/10000)
[Test]  Epoch: 38	Loss: 0.055134	Acc: 20.7% (2070/10000)
[Test]  Epoch: 39	Loss: 0.055133	Acc: 21.0% (2098/10000)
[Test]  Epoch: 40	Loss: 0.055101	Acc: 20.9% (2089/10000)
[Test]  Epoch: 41	Loss: 0.055137	Acc: 20.9% (2085/10000)
[Test]  Epoch: 42	Loss: 0.055068	Acc: 21.0% (2100/10000)
[Test]  Epoch: 43	Loss: 0.055062	Acc: 20.9% (2092/10000)
[Test]  Epoch: 44	Loss: 0.055045	Acc: 20.9% (2091/10000)
[Test]  Epoch: 45	Loss: 0.055035	Acc: 21.0% (2101/10000)
[Test]  Epoch: 46	Loss: 0.055123	Acc: 20.8% (2084/10000)
[Test]  Epoch: 47	Loss: 0.055190	Acc: 20.8% (2083/10000)
[Test]  Epoch: 48	Loss: 0.055139	Acc: 20.7% (2067/10000)
[Test]  Epoch: 49	Loss: 0.055153	Acc: 20.9% (2089/10000)
[Test]  Epoch: 50	Loss: 0.055038	Acc: 20.8% (2081/10000)
[Test]  Epoch: 51	Loss: 0.055186	Acc: 21.0% (2100/10000)
[Test]  Epoch: 52	Loss: 0.055065	Acc: 21.0% (2103/10000)
[Test]  Epoch: 53	Loss: 0.055051	Acc: 21.0% (2104/10000)
[Test]  Epoch: 54	Loss: 0.055092	Acc: 20.9% (2091/10000)
[Test]  Epoch: 55	Loss: 0.055035	Acc: 21.1% (2107/10000)
[Test]  Epoch: 56	Loss: 0.055032	Acc: 21.1% (2107/10000)
[Test]  Epoch: 57	Loss: 0.054948	Acc: 21.1% (2106/10000)
[Test]  Epoch: 58	Loss: 0.055041	Acc: 21.1% (2111/10000)
[Test]  Epoch: 59	Loss: 0.054974	Acc: 21.1% (2105/10000)
[Test]  Epoch: 60	Loss: 0.054991	Acc: 21.0% (2101/10000)
[Test]  Epoch: 61	Loss: 0.055012	Acc: 20.9% (2087/10000)
[Test]  Epoch: 62	Loss: 0.054967	Acc: 21.1% (2106/10000)
[Test]  Epoch: 63	Loss: 0.054990	Acc: 21.0% (2097/10000)
[Test]  Epoch: 64	Loss: 0.055016	Acc: 20.9% (2090/10000)
[Test]  Epoch: 65	Loss: 0.055057	Acc: 20.8% (2082/10000)
[Test]  Epoch: 66	Loss: 0.054979	Acc: 20.9% (2094/10000)
[Test]  Epoch: 67	Loss: 0.054996	Acc: 21.0% (2102/10000)
[Test]  Epoch: 68	Loss: 0.055013	Acc: 21.0% (2097/10000)
[Test]  Epoch: 69	Loss: 0.054958	Acc: 20.9% (2088/10000)
[Test]  Epoch: 70	Loss: 0.054966	Acc: 21.0% (2100/10000)
[Test]  Epoch: 71	Loss: 0.054933	Acc: 21.1% (2108/10000)
[Test]  Epoch: 72	Loss: 0.054980	Acc: 20.9% (2092/10000)
[Test]  Epoch: 73	Loss: 0.054985	Acc: 21.1% (2115/10000)
[Test]  Epoch: 74	Loss: 0.054938	Acc: 21.1% (2108/10000)
[Test]  Epoch: 75	Loss: 0.055006	Acc: 20.9% (2091/10000)
[Test]  Epoch: 76	Loss: 0.055047	Acc: 21.1% (2105/10000)
[Test]  Epoch: 77	Loss: 0.054970	Acc: 21.1% (2107/10000)
[Test]  Epoch: 78	Loss: 0.054870	Acc: 21.1% (2114/10000)
[Test]  Epoch: 79	Loss: 0.054857	Acc: 21.1% (2106/10000)
[Test]  Epoch: 80	Loss: 0.055057	Acc: 21.0% (2100/10000)
[Test]  Epoch: 81	Loss: 0.054988	Acc: 21.1% (2111/10000)
[Test]  Epoch: 82	Loss: 0.054972	Acc: 20.9% (2094/10000)
[Test]  Epoch: 83	Loss: 0.054978	Acc: 21.0% (2102/10000)
[Test]  Epoch: 84	Loss: 0.054867	Acc: 21.0% (2102/10000)
[Test]  Epoch: 85	Loss: 0.054912	Acc: 21.0% (2103/10000)
[Test]  Epoch: 86	Loss: 0.055037	Acc: 21.0% (2100/10000)
[Test]  Epoch: 87	Loss: 0.054900	Acc: 21.0% (2100/10000)
[Test]  Epoch: 88	Loss: 0.054887	Acc: 21.1% (2114/10000)
[Test]  Epoch: 89	Loss: 0.055024	Acc: 21.1% (2112/10000)
[Test]  Epoch: 90	Loss: 0.055059	Acc: 20.7% (2073/10000)
[Test]  Epoch: 91	Loss: 0.054982	Acc: 20.9% (2094/10000)
[Test]  Epoch: 92	Loss: 0.054915	Acc: 21.1% (2109/10000)
[Test]  Epoch: 93	Loss: 0.054896	Acc: 21.2% (2116/10000)
[Test]  Epoch: 94	Loss: 0.054970	Acc: 21.1% (2114/10000)
[Test]  Epoch: 95	Loss: 0.054845	Acc: 21.1% (2108/10000)
[Test]  Epoch: 96	Loss: 0.054987	Acc: 21.1% (2108/10000)
[Test]  Epoch: 97	Loss: 0.055002	Acc: 21.1% (2114/10000)
[Test]  Epoch: 98	Loss: 0.054972	Acc: 21.1% (2109/10000)
[Test]  Epoch: 99	Loss: 0.054977	Acc: 21.0% (2103/10000)
[Test]  Epoch: 100	Loss: 0.055043	Acc: 21.0% (2101/10000)
===========finish==========
['2024-08-19', '17:04:16.539783', '100', 'test', '0.05504311807155609', '21.01', '21.16']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.bias', 0.0), ('layer3.4.bn2.weight', -0.3980368375778198), ('layer3.4.bn1.weight', -0.75387042760849), ('layer4.1.bn2.weight', -1.6384824514389038), ('layer3.5.bn2.weight', -3.3912339210510254), ('layer3.5.bn1.weight', -3.6709682941436768), ('layer3.4.bn3.weight', -5.956405162811279), ('layer4.2.bn2.weight', -6.100573539733887), ('layer4.1.bn1.weight', -7.655907154083252), ('layer4.1.bn3.weight', -7.936943531036377), ('layer3.3.bn1.weight', -11.002126693725586), ('layer3.5.bn3.weight', -15.350693702697754), ('layer3.3.bn2.weight', -18.77312660217285), ('layer3.1.bn1.weight', -19.394428253173828), ('layer3.2.bn1.weight', -20.020009994506836), ('layer3.2.bn2.weight', -27.445476531982422), ('layer4.2.bn3.weight', -31.85346031188965), ('layer4.2.bn1.weight', -31.973234176635742), ('layer1.0.bn1.weight', -34.429840087890625), ('layer3.1.bn2.weight', -36.685298919677734), ('layer1.2.bn1.weight', -39.644622802734375), ('layer1.1.bn1.weight', -43.83026123046875), ('layer1.2.bn2.weight', -47.533573150634766), ('layer1.0.bn2.weight', -51.02251434326172), ('layer3.3.bn3.weight', -66.76686096191406), ('layer2.2.bn1.weight', -75.37330627441406), ('layer1.1.bn2.weight', -78.21586608886719), ('layer3.2.bn3.weight', -88.02265930175781), ('layer2.1.bn1.weight', -93.9759292602539), ('layer2.3.bn1.weight', -94.35055541992188), ('layer2.2.bn2.weight', -114.15695190429688), ('layer3.1.bn3.weight', -130.00537109375), ('layer2.3.bn2.weight', -136.09909057617188), ('layer2.1.bn2.weight', -160.7105255126953), ('layer4.0.downsample.1.weight', -177.05673217773438), ('layer4.0.bn3.weight', -177.66354370117188), ('layer2.0.bn2.weight', -202.8376922607422), ('layer3.0.bn2.weight', -224.1938934326172), ('layer1.2.bn3.weight', -232.70814514160156), ('bn1.weight', -246.14865112304688), ('layer2.0.bn1.weight', -260.3870849609375), ('layer1.0.bn3.weight', -326.8310241699219), ('layer3.0.bn1.weight', -346.68768310546875), ('layer1.0.downsample.1.weight', -347.6661071777344), ('layer2.3.bn3.weight', -352.9622802734375), ('layer2.2.bn3.weight', -353.5318603515625), ('layer1.1.bn3.weight', -363.3321838378906), ('layer4.0.bn1.weight', -475.0508728027344), ('layer4.0.bn2.weight', -522.674072265625), ('layer2.1.bn3.weight', -577.5298461914062), ('layer3.0.downsample.1.weight', -610.0713500976562), ('layer3.0.bn3.weight', -712.166748046875), ('layer2.0.downsample.1.weight', -714.9824829101562), ('layer2.0.bn3.weight', -890.4526977539062), ('layer3.4.conv3.weight', -3572.5634765625), ('layer4.1.conv3.weight', -6401.27978515625), ('layer3.4.conv1.weight', -10399.0830078125), ('layer3.5.conv3.weight', -13045.3427734375), ('layer3.4.conv2.weight', -14766.76171875), ('layer1.0.conv1.weight', -21449.048828125), ('layer3.5.conv1.weight', -33241.359375), ('last_linear.weight', -34925.859375), ('layer3.5.conv2.weight', -42753.75390625), ('layer3.3.conv3.weight', -46401.52734375), ('layer1.2.conv3.weight', -48044.37109375), ('layer3.2.conv3.weight', -63270.5546875), ('layer4.2.conv3.weight', -63724.890625), ('layer1.1.conv1.weight', -71284.90625), ('layer4.1.conv1.weight', -71652.921875), ('layer1.2.conv1.weight', -72936.4453125), ('layer1.1.conv3.weight', -83358.5234375), ('layer3.3.conv1.weight', -85955.1796875), ('layer4.1.conv2.weight', -89476.703125), ('layer3.1.conv3.weight', -100353.9296875), ('layer1.0.conv3.weight', -106134.953125), ('layer3.2.conv1.weight', -109839.03125), ('layer3.1.conv1.weight', -110381.1953125), ('layer4.2.conv2.weight', -124180.34375), ('layer3.3.conv2.weight', -128017.796875), ('layer3.2.conv2.weight', -143525.3125), ('layer3.1.conv2.weight', -171137.3125), ('layer1.2.conv2.weight', -200075.421875), ('layer2.2.conv3.weight', -242717.46875), ('layer2.3.conv3.weight', -261252.125), ('conv1.weight', -263560.5625), ('layer2.2.conv1.weight', -329215.3125), ('layer1.0.conv2.weight', -339188.03125), ('layer1.1.conv2.weight', -364724.0625), ('layer1.0.downsample.0.weight', -387144.4375), ('layer2.1.conv1.weight', -399260.3125), ('layer4.2.conv1.weight', -411688.03125), ('layer2.3.conv1.weight', -433089.71875), ('layer2.1.conv3.weight', -462344.03125), ('layer2.0.conv1.weight', -463289.0), ('layer2.0.conv3.weight', -653661.25), ('layer3.0.conv3.weight', -977986.5625), ('layer2.2.conv2.weight', -1299746.625), ('layer3.0.conv1.weight', -1361474.625), ('layer2.3.conv2.weight', -1688138.125), ('layer2.0.downsample.0.weight', -1693457.125), ('layer2.1.conv2.weight', -2114699.0), ('layer4.0.conv1.weight', -2229246.25), ('layer2.0.conv2.weight', -2335571.25), ('layer3.0.downsample.0.weight', -2713129.0), ('layer4.0.conv3.weight', -5262470.0), ('layer4.0.downsample.0.weight', -5888779.5), ('layer3.0.conv2.weight', -6956346.0), ('layer4.0.conv2.weight', -16310258.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('last_linear.bias', 0.0), ('layer3.4.conv3.weight', -3572.5634765625), ('layer4.1.conv3.weight', -6401.27978515625), ('layer3.4.conv1.weight', -10399.0830078125), ('layer3.5.conv3.weight', -13045.3427734375), ('layer3.4.conv2.weight', -14766.76171875), ('layer1.0.conv1.weight', -21449.048828125), ('layer3.5.conv1.weight', -33241.359375), ('last_linear.weight', -34925.859375), ('layer3.5.conv2.weight', -42753.75390625), ('layer3.3.conv3.weight', -46401.52734375), ('layer1.2.conv3.weight', -48044.37109375), ('layer3.2.conv3.weight', -63270.5546875), ('layer4.2.conv3.weight', -63724.890625), ('layer1.1.conv1.weight', -71284.90625), ('layer4.1.conv1.weight', -71652.921875), ('layer1.2.conv1.weight', -72936.4453125), ('layer1.1.conv3.weight', -83358.5234375), ('layer3.3.conv1.weight', -85955.1796875), ('layer4.1.conv2.weight', -89476.703125), ('layer3.1.conv3.weight', -100353.9296875), ('layer1.0.conv3.weight', -106134.953125), ('layer3.2.conv1.weight', -109839.03125), ('layer3.1.conv1.weight', -110381.1953125), ('layer4.2.conv2.weight', -124180.34375), ('layer3.3.conv2.weight', -128017.796875), ('layer3.2.conv2.weight', -143525.3125), ('layer3.1.conv2.weight', -171137.3125), ('layer1.2.conv2.weight', -200075.421875), ('layer2.2.conv3.weight', -242717.46875), ('layer2.3.conv3.weight', -261252.125), ('conv1.weight', -263560.5625), ('layer2.2.conv1.weight', -329215.3125), ('layer1.0.conv2.weight', -339188.03125), ('layer1.1.conv2.weight', -364724.0625), ('layer2.1.conv1.weight', -399260.3125), ('layer4.2.conv1.weight', -411688.03125), ('layer2.3.conv1.weight', -433089.71875), ('layer2.1.conv3.weight', -462344.03125), ('layer2.0.conv1.weight', -463289.0), ('layer2.0.conv3.weight', -653661.25), ('layer3.0.conv3.weight', -977986.5625), ('layer2.2.conv2.weight', -1299746.625), ('layer3.0.conv1.weight', -1361474.625), ('layer2.3.conv2.weight', -1688138.125), ('layer2.1.conv2.weight', -2114699.0), ('layer4.0.conv1.weight', -2229246.25), ('layer2.0.conv2.weight', -2335571.25), ('layer4.0.conv3.weight', -5262470.0), ('layer3.0.conv2.weight', -6956346.0), ('layer4.0.conv2.weight', -16310258.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.007167	Acc: 85.8% (8584/10000)
[Test]  Epoch: 2	Loss: 0.006685	Acc: 86.5% (8653/10000)
[Test]  Epoch: 3	Loss: 0.006650	Acc: 86.5% (8653/10000)
[Test]  Epoch: 4	Loss: 0.006605	Acc: 86.6% (8662/10000)
[Test]  Epoch: 5	Loss: 0.006632	Acc: 86.7% (8668/10000)
[Test]  Epoch: 6	Loss: 0.006567	Acc: 86.7% (8665/10000)
[Test]  Epoch: 7	Loss: 0.006467	Acc: 86.8% (8682/10000)
[Test]  Epoch: 8	Loss: 0.006552	Acc: 86.5% (8648/10000)
[Test]  Epoch: 9	Loss: 0.006453	Acc: 86.7% (8668/10000)
[Test]  Epoch: 10	Loss: 0.006413	Acc: 86.8% (8683/10000)
[Test]  Epoch: 11	Loss: 0.006419	Acc: 86.7% (8670/10000)
[Test]  Epoch: 12	Loss: 0.006379	Acc: 86.8% (8681/10000)
[Test]  Epoch: 13	Loss: 0.006377	Acc: 86.8% (8677/10000)
[Test]  Epoch: 14	Loss: 0.006321	Acc: 87.0% (8700/10000)
[Test]  Epoch: 15	Loss: 0.006383	Acc: 86.8% (8682/10000)
[Test]  Epoch: 16	Loss: 0.006337	Acc: 86.8% (8685/10000)
[Test]  Epoch: 17	Loss: 0.006369	Acc: 87.0% (8698/10000)
[Test]  Epoch: 18	Loss: 0.006300	Acc: 86.8% (8681/10000)
[Test]  Epoch: 19	Loss: 0.006274	Acc: 86.8% (8681/10000)
[Test]  Epoch: 20	Loss: 0.006276	Acc: 86.9% (8694/10000)
[Test]  Epoch: 21	Loss: 0.006309	Acc: 87.0% (8697/10000)
[Test]  Epoch: 22	Loss: 0.006269	Acc: 87.0% (8704/10000)
[Test]  Epoch: 23	Loss: 0.006278	Acc: 87.0% (8700/10000)
[Test]  Epoch: 24	Loss: 0.006244	Acc: 87.0% (8699/10000)
[Test]  Epoch: 25	Loss: 0.006285	Acc: 87.0% (8698/10000)
[Test]  Epoch: 26	Loss: 0.006265	Acc: 86.9% (8690/10000)
[Test]  Epoch: 27	Loss: 0.006243	Acc: 87.0% (8698/10000)
[Test]  Epoch: 28	Loss: 0.006229	Acc: 87.1% (8707/10000)
[Test]  Epoch: 29	Loss: 0.006246	Acc: 87.0% (8705/10000)
[Test]  Epoch: 30	Loss: 0.006228	Acc: 87.0% (8698/10000)
[Test]  Epoch: 31	Loss: 0.006216	Acc: 86.9% (8693/10000)
[Test]  Epoch: 32	Loss: 0.006261	Acc: 87.0% (8704/10000)
[Test]  Epoch: 33	Loss: 0.006233	Acc: 87.0% (8705/10000)
[Test]  Epoch: 34	Loss: 0.006220	Acc: 87.1% (8706/10000)
[Test]  Epoch: 35	Loss: 0.006233	Acc: 87.0% (8699/10000)
[Test]  Epoch: 36	Loss: 0.006251	Acc: 86.9% (8693/10000)
[Test]  Epoch: 37	Loss: 0.006232	Acc: 87.1% (8706/10000)
[Test]  Epoch: 38	Loss: 0.006231	Acc: 86.9% (8690/10000)
[Test]  Epoch: 39	Loss: 0.006210	Acc: 87.1% (8710/10000)
[Test]  Epoch: 40	Loss: 0.006221	Acc: 87.0% (8700/10000)
[Test]  Epoch: 41	Loss: 0.006210	Acc: 87.0% (8705/10000)
[Test]  Epoch: 42	Loss: 0.006184	Acc: 87.1% (8709/10000)
[Test]  Epoch: 43	Loss: 0.006217	Acc: 87.0% (8701/10000)
[Test]  Epoch: 44	Loss: 0.006198	Acc: 87.1% (8710/10000)
[Test]  Epoch: 45	Loss: 0.006187	Acc: 87.1% (8706/10000)
[Test]  Epoch: 46	Loss: 0.006186	Acc: 87.0% (8701/10000)
[Test]  Epoch: 47	Loss: 0.006185	Acc: 87.0% (8696/10000)
[Test]  Epoch: 48	Loss: 0.006177	Acc: 87.1% (8711/10000)
[Test]  Epoch: 49	Loss: 0.006179	Acc: 87.1% (8707/10000)
[Test]  Epoch: 50	Loss: 0.006184	Acc: 87.0% (8699/10000)
[Test]  Epoch: 51	Loss: 0.006146	Acc: 87.1% (8709/10000)
[Test]  Epoch: 52	Loss: 0.006157	Acc: 87.1% (8713/10000)
[Test]  Epoch: 53	Loss: 0.006162	Acc: 87.1% (8707/10000)
[Test]  Epoch: 54	Loss: 0.006166	Acc: 87.2% (8719/10000)
[Test]  Epoch: 55	Loss: 0.006182	Acc: 87.0% (8705/10000)
[Test]  Epoch: 56	Loss: 0.006147	Acc: 87.1% (8706/10000)
[Test]  Epoch: 57	Loss: 0.006158	Acc: 87.0% (8705/10000)
[Test]  Epoch: 58	Loss: 0.006171	Acc: 87.1% (8707/10000)
[Test]  Epoch: 59	Loss: 0.006179	Acc: 87.0% (8705/10000)
[Test]  Epoch: 60	Loss: 0.006177	Acc: 87.2% (8719/10000)
[Test]  Epoch: 61	Loss: 0.006163	Acc: 87.1% (8710/10000)
[Test]  Epoch: 62	Loss: 0.006182	Acc: 87.1% (8712/10000)
[Test]  Epoch: 63	Loss: 0.006190	Acc: 87.0% (8700/10000)
[Test]  Epoch: 64	Loss: 0.006153	Acc: 87.1% (8713/10000)
[Test]  Epoch: 65	Loss: 0.006155	Acc: 87.1% (8709/10000)
[Test]  Epoch: 66	Loss: 0.006157	Acc: 87.0% (8703/10000)
[Test]  Epoch: 67	Loss: 0.006155	Acc: 87.0% (8705/10000)
[Test]  Epoch: 68	Loss: 0.006164	Acc: 87.1% (8711/10000)
[Test]  Epoch: 69	Loss: 0.006187	Acc: 87.0% (8699/10000)
[Test]  Epoch: 70	Loss: 0.006164	Acc: 87.1% (8709/10000)
[Test]  Epoch: 71	Loss: 0.006138	Acc: 87.1% (8713/10000)
[Test]  Epoch: 72	Loss: 0.006134	Acc: 87.2% (8715/10000)
[Test]  Epoch: 73	Loss: 0.006156	Acc: 87.1% (8712/10000)
[Test]  Epoch: 74	Loss: 0.006143	Acc: 87.1% (8708/10000)
[Test]  Epoch: 75	Loss: 0.006154	Acc: 87.1% (8709/10000)
[Test]  Epoch: 76	Loss: 0.006136	Acc: 87.2% (8722/10000)
[Test]  Epoch: 77	Loss: 0.006163	Acc: 87.0% (8702/10000)
[Test]  Epoch: 78	Loss: 0.006145	Acc: 87.2% (8717/10000)
[Test]  Epoch: 79	Loss: 0.006151	Acc: 87.0% (8703/10000)
[Test]  Epoch: 80	Loss: 0.006153	Acc: 87.1% (8709/10000)
[Test]  Epoch: 81	Loss: 0.006166	Acc: 87.0% (8701/10000)
[Test]  Epoch: 82	Loss: 0.006166	Acc: 87.0% (8698/10000)
[Test]  Epoch: 83	Loss: 0.006132	Acc: 87.1% (8710/10000)
[Test]  Epoch: 84	Loss: 0.006142	Acc: 87.2% (8719/10000)
[Test]  Epoch: 85	Loss: 0.006150	Acc: 87.0% (8704/10000)
[Test]  Epoch: 86	Loss: 0.006132	Acc: 87.1% (8713/10000)
[Test]  Epoch: 87	Loss: 0.006146	Acc: 87.1% (8713/10000)
[Test]  Epoch: 88	Loss: 0.006144	Acc: 87.1% (8712/10000)
[Test]  Epoch: 89	Loss: 0.006151	Acc: 87.1% (8712/10000)
[Test]  Epoch: 90	Loss: 0.006153	Acc: 87.1% (8712/10000)
[Test]  Epoch: 91	Loss: 0.006168	Acc: 87.0% (8697/10000)
[Test]  Epoch: 92	Loss: 0.006143	Acc: 87.1% (8707/10000)
[Test]  Epoch: 93	Loss: 0.006135	Acc: 87.1% (8714/10000)
[Test]  Epoch: 94	Loss: 0.006155	Acc: 87.1% (8706/10000)
[Test]  Epoch: 95	Loss: 0.006135	Acc: 87.2% (8715/10000)
[Test]  Epoch: 96	Loss: 0.006134	Acc: 87.1% (8714/10000)
[Test]  Epoch: 97	Loss: 0.006132	Acc: 87.2% (8718/10000)
[Test]  Epoch: 98	Loss: 0.006159	Acc: 87.1% (8710/10000)
[Test]  Epoch: 99	Loss: 0.006150	Acc: 87.1% (8707/10000)
[Test]  Epoch: 100	Loss: 0.006175	Acc: 86.9% (8694/10000)
===========finish==========
['2024-08-19', '17:11:37.164414', '100', 'test', '0.006174696397036314', '86.94', '87.22']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.009651	Acc: 83.6% (8363/10000)
[Test]  Epoch: 2	Loss: 0.007036	Acc: 85.2% (8523/10000)
[Test]  Epoch: 3	Loss: 0.006750	Acc: 85.9% (8591/10000)
[Test]  Epoch: 4	Loss: 0.006629	Acc: 86.0% (8605/10000)
[Test]  Epoch: 5	Loss: 0.006714	Acc: 86.0% (8596/10000)
[Test]  Epoch: 6	Loss: 0.006444	Acc: 86.4% (8641/10000)
[Test]  Epoch: 7	Loss: 0.006361	Acc: 86.5% (8648/10000)
[Test]  Epoch: 8	Loss: 0.006392	Acc: 86.8% (8676/10000)
[Test]  Epoch: 9	Loss: 0.006343	Acc: 86.7% (8670/10000)
[Test]  Epoch: 10	Loss: 0.006318	Acc: 86.8% (8680/10000)
[Test]  Epoch: 11	Loss: 0.006320	Acc: 86.5% (8652/10000)
[Test]  Epoch: 12	Loss: 0.006267	Acc: 86.8% (8682/10000)
[Test]  Epoch: 13	Loss: 0.006301	Acc: 86.8% (8677/10000)
[Test]  Epoch: 14	Loss: 0.006260	Acc: 86.9% (8686/10000)
[Test]  Epoch: 15	Loss: 0.006264	Acc: 86.9% (8686/10000)
[Test]  Epoch: 16	Loss: 0.006246	Acc: 86.8% (8678/10000)
[Test]  Epoch: 17	Loss: 0.006310	Acc: 86.8% (8681/10000)
[Test]  Epoch: 18	Loss: 0.006234	Acc: 86.9% (8691/10000)
[Test]  Epoch: 19	Loss: 0.006182	Acc: 87.0% (8699/10000)
[Test]  Epoch: 20	Loss: 0.006244	Acc: 86.9% (8694/10000)
[Test]  Epoch: 21	Loss: 0.006247	Acc: 86.7% (8672/10000)
[Test]  Epoch: 22	Loss: 0.006178	Acc: 87.0% (8702/10000)
[Test]  Epoch: 23	Loss: 0.006170	Acc: 87.0% (8695/10000)
[Test]  Epoch: 24	Loss: 0.006192	Acc: 87.0% (8704/10000)
[Test]  Epoch: 25	Loss: 0.006228	Acc: 87.1% (8708/10000)
[Test]  Epoch: 26	Loss: 0.006192	Acc: 87.0% (8698/10000)
[Test]  Epoch: 27	Loss: 0.006179	Acc: 87.2% (8722/10000)
[Test]  Epoch: 28	Loss: 0.006177	Acc: 87.0% (8701/10000)
[Test]  Epoch: 29	Loss: 0.006184	Acc: 87.0% (8699/10000)
[Test]  Epoch: 30	Loss: 0.006183	Acc: 87.1% (8713/10000)
[Test]  Epoch: 31	Loss: 0.006181	Acc: 87.2% (8715/10000)
[Test]  Epoch: 32	Loss: 0.006242	Acc: 86.7% (8671/10000)
[Test]  Epoch: 33	Loss: 0.006195	Acc: 87.1% (8709/10000)
[Test]  Epoch: 34	Loss: 0.006188	Acc: 87.1% (8706/10000)
[Test]  Epoch: 35	Loss: 0.006175	Acc: 87.1% (8713/10000)
[Test]  Epoch: 36	Loss: 0.006194	Acc: 87.0% (8699/10000)
[Test]  Epoch: 37	Loss: 0.006183	Acc: 87.1% (8714/10000)
[Test]  Epoch: 38	Loss: 0.006186	Acc: 87.1% (8706/10000)
[Test]  Epoch: 39	Loss: 0.006160	Acc: 87.1% (8710/10000)
[Test]  Epoch: 40	Loss: 0.006181	Acc: 87.0% (8703/10000)
[Test]  Epoch: 41	Loss: 0.006174	Acc: 87.1% (8709/10000)
[Test]  Epoch: 42	Loss: 0.006171	Acc: 87.1% (8711/10000)
[Test]  Epoch: 43	Loss: 0.006243	Acc: 87.2% (8723/10000)
[Test]  Epoch: 44	Loss: 0.006167	Acc: 87.1% (8714/10000)
[Test]  Epoch: 45	Loss: 0.006153	Acc: 87.2% (8725/10000)
[Test]  Epoch: 46	Loss: 0.006150	Acc: 87.2% (8724/10000)
[Test]  Epoch: 47	Loss: 0.006159	Acc: 87.0% (8702/10000)
[Test]  Epoch: 48	Loss: 0.006168	Acc: 87.1% (8707/10000)
[Test]  Epoch: 49	Loss: 0.006195	Acc: 87.1% (8708/10000)
[Test]  Epoch: 50	Loss: 0.006156	Acc: 87.1% (8709/10000)
[Test]  Epoch: 51	Loss: 0.006116	Acc: 87.2% (8717/10000)
[Test]  Epoch: 52	Loss: 0.006137	Acc: 87.2% (8724/10000)
[Test]  Epoch: 53	Loss: 0.006148	Acc: 87.2% (8725/10000)
[Test]  Epoch: 54	Loss: 0.006137	Acc: 87.2% (8719/10000)
[Test]  Epoch: 55	Loss: 0.006149	Acc: 87.1% (8713/10000)
[Test]  Epoch: 56	Loss: 0.006134	Acc: 87.1% (8713/10000)
[Test]  Epoch: 57	Loss: 0.006157	Acc: 87.1% (8706/10000)
[Test]  Epoch: 58	Loss: 0.006161	Acc: 87.0% (8705/10000)
[Test]  Epoch: 59	Loss: 0.006191	Acc: 87.0% (8701/10000)
[Test]  Epoch: 60	Loss: 0.006161	Acc: 87.1% (8711/10000)
[Test]  Epoch: 61	Loss: 0.006134	Acc: 87.1% (8710/10000)
[Test]  Epoch: 62	Loss: 0.006144	Acc: 87.2% (8716/10000)
[Test]  Epoch: 63	Loss: 0.006156	Acc: 87.2% (8717/10000)
[Test]  Epoch: 64	Loss: 0.006131	Acc: 87.2% (8718/10000)
[Test]  Epoch: 65	Loss: 0.006139	Acc: 87.1% (8706/10000)
[Test]  Epoch: 66	Loss: 0.006142	Acc: 87.0% (8705/10000)
[Test]  Epoch: 67	Loss: 0.006134	Acc: 87.2% (8717/10000)
[Test]  Epoch: 68	Loss: 0.006154	Acc: 87.0% (8705/10000)
[Test]  Epoch: 69	Loss: 0.006175	Acc: 87.1% (8709/10000)
[Test]  Epoch: 70	Loss: 0.006154	Acc: 87.1% (8712/10000)
[Test]  Epoch: 71	Loss: 0.006129	Acc: 87.2% (8718/10000)
[Test]  Epoch: 72	Loss: 0.006122	Acc: 87.2% (8718/10000)
[Test]  Epoch: 73	Loss: 0.006139	Acc: 87.0% (8701/10000)
[Test]  Epoch: 74	Loss: 0.006136	Acc: 87.2% (8720/10000)
[Test]  Epoch: 75	Loss: 0.006150	Acc: 87.2% (8716/10000)
[Test]  Epoch: 76	Loss: 0.006124	Acc: 87.2% (8718/10000)
[Test]  Epoch: 77	Loss: 0.006154	Acc: 87.1% (8713/10000)
[Test]  Epoch: 78	Loss: 0.006139	Acc: 87.1% (8714/10000)
[Test]  Epoch: 79	Loss: 0.006143	Acc: 87.0% (8701/10000)
[Test]  Epoch: 80	Loss: 0.006144	Acc: 87.1% (8710/10000)
[Test]  Epoch: 81	Loss: 0.006157	Acc: 87.1% (8711/10000)
[Test]  Epoch: 82	Loss: 0.006154	Acc: 87.2% (8716/10000)
[Test]  Epoch: 83	Loss: 0.006118	Acc: 87.2% (8717/10000)
[Test]  Epoch: 84	Loss: 0.006124	Acc: 87.2% (8717/10000)
[Test]  Epoch: 85	Loss: 0.006140	Acc: 87.2% (8716/10000)
[Test]  Epoch: 86	Loss: 0.006122	Acc: 87.2% (8716/10000)
[Test]  Epoch: 87	Loss: 0.006131	Acc: 87.2% (8720/10000)
[Test]  Epoch: 88	Loss: 0.006130	Acc: 87.2% (8715/10000)
[Test]  Epoch: 89	Loss: 0.006134	Acc: 87.2% (8715/10000)
[Test]  Epoch: 90	Loss: 0.006137	Acc: 87.1% (8707/10000)
[Test]  Epoch: 91	Loss: 0.006150	Acc: 87.1% (8713/10000)
[Test]  Epoch: 92	Loss: 0.006132	Acc: 87.1% (8709/10000)
[Test]  Epoch: 93	Loss: 0.006128	Acc: 87.2% (8716/10000)
[Test]  Epoch: 94	Loss: 0.006143	Acc: 87.2% (8718/10000)
[Test]  Epoch: 95	Loss: 0.006125	Acc: 87.1% (8710/10000)
[Test]  Epoch: 96	Loss: 0.006124	Acc: 87.1% (8706/10000)
[Test]  Epoch: 97	Loss: 0.006121	Acc: 87.1% (8711/10000)
[Test]  Epoch: 98	Loss: 0.006140	Acc: 87.1% (8713/10000)
[Test]  Epoch: 99	Loss: 0.006135	Acc: 87.2% (8717/10000)
[Test]  Epoch: 100	Loss: 0.006156	Acc: 87.1% (8709/10000)
===========finish==========
['2024-08-19', '17:16:30.848308', '100', 'test', '0.006156098587810993', '87.09', '87.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.052192	Acc: 43.3% (4326/10000)
[Test]  Epoch: 2	Loss: 0.012414	Acc: 72.9% (7289/10000)
[Test]  Epoch: 3	Loss: 0.010050	Acc: 79.3% (7932/10000)
[Test]  Epoch: 4	Loss: 0.011647	Acc: 73.8% (7382/10000)
[Test]  Epoch: 5	Loss: 0.009237	Acc: 81.2% (8123/10000)
[Test]  Epoch: 6	Loss: 0.009176	Acc: 81.2% (8121/10000)
[Test]  Epoch: 7	Loss: 0.008924	Acc: 81.9% (8187/10000)
[Test]  Epoch: 8	Loss: 0.009364	Acc: 81.3% (8133/10000)
[Test]  Epoch: 9	Loss: 0.008959	Acc: 81.9% (8186/10000)
[Test]  Epoch: 10	Loss: 0.008819	Acc: 82.1% (8214/10000)
[Test]  Epoch: 11	Loss: 0.008792	Acc: 81.8% (8183/10000)
[Test]  Epoch: 12	Loss: 0.008696	Acc: 82.0% (8204/10000)
[Test]  Epoch: 13	Loss: 0.008620	Acc: 82.2% (8215/10000)
[Test]  Epoch: 14	Loss: 0.008783	Acc: 82.0% (8200/10000)
[Test]  Epoch: 15	Loss: 0.008755	Acc: 81.9% (8187/10000)
[Test]  Epoch: 16	Loss: 0.008648	Acc: 82.1% (8213/10000)
[Test]  Epoch: 17	Loss: 0.008588	Acc: 82.3% (8226/10000)
[Test]  Epoch: 18	Loss: 0.008485	Acc: 82.3% (8229/10000)
[Test]  Epoch: 19	Loss: 0.008372	Acc: 82.6% (8263/10000)
[Test]  Epoch: 20	Loss: 0.008487	Acc: 82.4% (8241/10000)
[Test]  Epoch: 21	Loss: 0.008385	Acc: 82.5% (8252/10000)
[Test]  Epoch: 22	Loss: 0.008381	Acc: 82.6% (8256/10000)
[Test]  Epoch: 23	Loss: 0.008382	Acc: 82.6% (8258/10000)
[Test]  Epoch: 24	Loss: 0.008324	Acc: 82.8% (8275/10000)
[Test]  Epoch: 25	Loss: 0.008301	Acc: 82.7% (8274/10000)
[Test]  Epoch: 26	Loss: 0.008285	Acc: 82.5% (8255/10000)
[Test]  Epoch: 27	Loss: 0.008294	Acc: 82.7% (8269/10000)
[Test]  Epoch: 28	Loss: 0.008321	Acc: 82.7% (8273/10000)
[Test]  Epoch: 29	Loss: 0.008408	Acc: 82.6% (8258/10000)
[Test]  Epoch: 30	Loss: 0.008303	Acc: 82.6% (8261/10000)
[Test]  Epoch: 31	Loss: 0.008301	Acc: 82.7% (8268/10000)
[Test]  Epoch: 32	Loss: 0.008323	Acc: 82.7% (8265/10000)
[Test]  Epoch: 33	Loss: 0.008237	Acc: 82.7% (8274/10000)
[Test]  Epoch: 34	Loss: 0.008240	Acc: 82.7% (8271/10000)
[Test]  Epoch: 35	Loss: 0.008212	Acc: 82.8% (8276/10000)
[Test]  Epoch: 36	Loss: 0.008294	Acc: 82.6% (8262/10000)
[Test]  Epoch: 37	Loss: 0.008165	Acc: 82.8% (8285/10000)
[Test]  Epoch: 38	Loss: 0.008117	Acc: 82.9% (8287/10000)
[Test]  Epoch: 39	Loss: 0.008120	Acc: 82.8% (8283/10000)
[Test]  Epoch: 40	Loss: 0.008120	Acc: 82.7% (8270/10000)
[Test]  Epoch: 41	Loss: 0.008156	Acc: 82.9% (8290/10000)
[Test]  Epoch: 42	Loss: 0.008134	Acc: 82.8% (8285/10000)
[Test]  Epoch: 43	Loss: 0.008209	Acc: 82.7% (8266/10000)
[Test]  Epoch: 44	Loss: 0.008105	Acc: 82.8% (8276/10000)
[Test]  Epoch: 45	Loss: 0.008126	Acc: 82.8% (8284/10000)
[Test]  Epoch: 46	Loss: 0.008116	Acc: 82.8% (8275/10000)
[Test]  Epoch: 47	Loss: 0.008104	Acc: 83.0% (8299/10000)
[Test]  Epoch: 48	Loss: 0.008071	Acc: 83.0% (8296/10000)
[Test]  Epoch: 49	Loss: 0.008121	Acc: 82.9% (8292/10000)
[Test]  Epoch: 50	Loss: 0.008094	Acc: 82.9% (8288/10000)
[Test]  Epoch: 51	Loss: 0.008049	Acc: 82.9% (8290/10000)
[Test]  Epoch: 52	Loss: 0.008073	Acc: 82.9% (8294/10000)
[Test]  Epoch: 53	Loss: 0.008090	Acc: 82.8% (8280/10000)
[Test]  Epoch: 54	Loss: 0.008058	Acc: 82.9% (8290/10000)
[Test]  Epoch: 55	Loss: 0.008031	Acc: 83.0% (8299/10000)
[Test]  Epoch: 56	Loss: 0.008042	Acc: 83.1% (8307/10000)
[Test]  Epoch: 57	Loss: 0.008215	Acc: 83.0% (8305/10000)
[Test]  Epoch: 58	Loss: 0.008077	Acc: 83.0% (8300/10000)
[Test]  Epoch: 59	Loss: 0.008117	Acc: 83.0% (8297/10000)
[Test]  Epoch: 60	Loss: 0.008059	Acc: 83.0% (8303/10000)
[Test]  Epoch: 61	Loss: 0.008053	Acc: 83.0% (8300/10000)
[Test]  Epoch: 62	Loss: 0.008072	Acc: 83.1% (8309/10000)
[Test]  Epoch: 63	Loss: 0.008093	Acc: 82.9% (8291/10000)
[Test]  Epoch: 64	Loss: 0.008053	Acc: 83.0% (8297/10000)
[Test]  Epoch: 65	Loss: 0.008052	Acc: 82.9% (8289/10000)
[Test]  Epoch: 66	Loss: 0.008043	Acc: 83.0% (8301/10000)
[Test]  Epoch: 67	Loss: 0.008022	Acc: 83.1% (8307/10000)
[Test]  Epoch: 68	Loss: 0.008067	Acc: 82.9% (8292/10000)
[Test]  Epoch: 69	Loss: 0.008063	Acc: 83.0% (8296/10000)
[Test]  Epoch: 70	Loss: 0.008032	Acc: 83.0% (8304/10000)
[Test]  Epoch: 71	Loss: 0.007993	Acc: 83.2% (8315/10000)
[Test]  Epoch: 72	Loss: 0.008033	Acc: 83.0% (8298/10000)
[Test]  Epoch: 73	Loss: 0.008019	Acc: 83.1% (8311/10000)
[Test]  Epoch: 74	Loss: 0.008037	Acc: 83.1% (8313/10000)
[Test]  Epoch: 75	Loss: 0.008032	Acc: 83.2% (8315/10000)
[Test]  Epoch: 76	Loss: 0.008018	Acc: 83.0% (8298/10000)
[Test]  Epoch: 77	Loss: 0.008029	Acc: 83.0% (8303/10000)
[Test]  Epoch: 78	Loss: 0.008044	Acc: 83.0% (8303/10000)
[Test]  Epoch: 79	Loss: 0.008040	Acc: 83.0% (8303/10000)
[Test]  Epoch: 80	Loss: 0.008017	Acc: 83.0% (8305/10000)
[Test]  Epoch: 81	Loss: 0.008005	Acc: 83.1% (8311/10000)
[Test]  Epoch: 82	Loss: 0.008013	Acc: 83.1% (8311/10000)
[Test]  Epoch: 83	Loss: 0.008007	Acc: 83.0% (8303/10000)
[Test]  Epoch: 84	Loss: 0.008023	Acc: 83.0% (8305/10000)
[Test]  Epoch: 85	Loss: 0.008018	Acc: 83.0% (8298/10000)
[Test]  Epoch: 86	Loss: 0.008029	Acc: 83.1% (8309/10000)
[Test]  Epoch: 87	Loss: 0.008012	Acc: 83.2% (8316/10000)
[Test]  Epoch: 88	Loss: 0.008028	Acc: 83.1% (8306/10000)
[Test]  Epoch: 89	Loss: 0.008034	Acc: 83.0% (8300/10000)
[Test]  Epoch: 90	Loss: 0.008021	Acc: 83.0% (8302/10000)
[Test]  Epoch: 91	Loss: 0.008027	Acc: 83.0% (8305/10000)
[Test]  Epoch: 92	Loss: 0.008004	Acc: 83.0% (8301/10000)
[Test]  Epoch: 93	Loss: 0.008008	Acc: 83.0% (8301/10000)
[Test]  Epoch: 94	Loss: 0.008018	Acc: 83.1% (8308/10000)
[Test]  Epoch: 95	Loss: 0.008014	Acc: 83.1% (8311/10000)
[Test]  Epoch: 96	Loss: 0.007995	Acc: 83.0% (8304/10000)
[Test]  Epoch: 97	Loss: 0.008018	Acc: 83.1% (8307/10000)
[Test]  Epoch: 98	Loss: 0.008001	Acc: 83.2% (8325/10000)
[Test]  Epoch: 99	Loss: 0.007990	Acc: 83.1% (8313/10000)
[Test]  Epoch: 100	Loss: 0.008023	Acc: 83.1% (8309/10000)
===========finish==========
['2024-08-19', '17:20:45.322754', '100', 'test', '0.008022664314508439', '83.09', '83.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.349240	Acc: 27.9% (2794/10000)
[Test]  Epoch: 2	Loss: 0.027514	Acc: 54.6% (5461/10000)
[Test]  Epoch: 3	Loss: 0.017660	Acc: 62.1% (6209/10000)
[Test]  Epoch: 4	Loss: 0.025208	Acc: 59.3% (5928/10000)
[Test]  Epoch: 5	Loss: 0.016722	Acc: 66.5% (6647/10000)
[Test]  Epoch: 6	Loss: 0.015724	Acc: 67.5% (6745/10000)
[Test]  Epoch: 7	Loss: 0.017662	Acc: 67.3% (6733/10000)
[Test]  Epoch: 8	Loss: 0.015604	Acc: 68.6% (6861/10000)
[Test]  Epoch: 9	Loss: 0.014745	Acc: 70.0% (6996/10000)
[Test]  Epoch: 10	Loss: 0.014865	Acc: 70.1% (7012/10000)
[Test]  Epoch: 11	Loss: 0.014405	Acc: 70.5% (7053/10000)
[Test]  Epoch: 12	Loss: 0.014371	Acc: 71.1% (7112/10000)
[Test]  Epoch: 13	Loss: 0.013546	Acc: 72.2% (7221/10000)
[Test]  Epoch: 14	Loss: 0.013633	Acc: 72.0% (7199/10000)
[Test]  Epoch: 15	Loss: 0.013658	Acc: 72.2% (7217/10000)
[Test]  Epoch: 16	Loss: 0.013697	Acc: 72.0% (7198/10000)
[Test]  Epoch: 17	Loss: 0.013681	Acc: 72.1% (7211/10000)
[Test]  Epoch: 18	Loss: 0.013536	Acc: 72.2% (7216/10000)
[Test]  Epoch: 19	Loss: 0.013139	Acc: 72.8% (7277/10000)
[Test]  Epoch: 20	Loss: 0.013373	Acc: 72.6% (7257/10000)
[Test]  Epoch: 21	Loss: 0.013193	Acc: 72.7% (7266/10000)
[Test]  Epoch: 22	Loss: 0.013110	Acc: 73.0% (7305/10000)
[Test]  Epoch: 23	Loss: 0.013215	Acc: 72.9% (7286/10000)
[Test]  Epoch: 24	Loss: 0.013026	Acc: 73.1% (7314/10000)
[Test]  Epoch: 25	Loss: 0.013220	Acc: 72.9% (7293/10000)
[Test]  Epoch: 26	Loss: 0.013092	Acc: 73.4% (7336/10000)
[Test]  Epoch: 27	Loss: 0.013047	Acc: 72.9% (7288/10000)
[Test]  Epoch: 28	Loss: 0.013355	Acc: 72.5% (7248/10000)
[Test]  Epoch: 29	Loss: 0.012979	Acc: 73.0% (7304/10000)
[Test]  Epoch: 30	Loss: 0.012890	Acc: 73.4% (7337/10000)
[Test]  Epoch: 31	Loss: 0.012901	Acc: 73.3% (7330/10000)
[Test]  Epoch: 32	Loss: 0.012836	Acc: 73.1% (7308/10000)
[Test]  Epoch: 33	Loss: 0.012788	Acc: 73.2% (7323/10000)
[Test]  Epoch: 34	Loss: 0.012848	Acc: 73.3% (7326/10000)
[Test]  Epoch: 35	Loss: 0.012764	Acc: 73.6% (7358/10000)
[Test]  Epoch: 36	Loss: 0.012740	Acc: 73.5% (7347/10000)
[Test]  Epoch: 37	Loss: 0.012708	Acc: 73.3% (7334/10000)
[Test]  Epoch: 38	Loss: 0.012554	Acc: 73.6% (7362/10000)
[Test]  Epoch: 39	Loss: 0.012767	Acc: 73.6% (7357/10000)
[Test]  Epoch: 40	Loss: 0.012680	Acc: 73.6% (7362/10000)
[Test]  Epoch: 41	Loss: 0.012538	Acc: 73.7% (7373/10000)
[Test]  Epoch: 42	Loss: 0.012572	Acc: 73.7% (7368/10000)
[Test]  Epoch: 43	Loss: 0.012753	Acc: 73.2% (7316/10000)
[Test]  Epoch: 44	Loss: 0.012606	Acc: 73.5% (7348/10000)
[Test]  Epoch: 45	Loss: 0.012493	Acc: 73.7% (7370/10000)
[Test]  Epoch: 46	Loss: 0.012516	Acc: 73.8% (7376/10000)
[Test]  Epoch: 47	Loss: 0.012474	Acc: 73.6% (7364/10000)
[Test]  Epoch: 48	Loss: 0.012484	Acc: 73.7% (7366/10000)
[Test]  Epoch: 49	Loss: 0.012561	Acc: 73.5% (7349/10000)
[Test]  Epoch: 50	Loss: 0.012461	Acc: 73.9% (7392/10000)
[Test]  Epoch: 51	Loss: 0.012859	Acc: 73.4% (7340/10000)
[Test]  Epoch: 52	Loss: 0.012513	Acc: 73.8% (7383/10000)
[Test]  Epoch: 53	Loss: 0.012506	Acc: 73.8% (7377/10000)
[Test]  Epoch: 54	Loss: 0.012316	Acc: 74.1% (7407/10000)
[Test]  Epoch: 55	Loss: 0.012496	Acc: 73.5% (7348/10000)
[Test]  Epoch: 56	Loss: 0.012366	Acc: 74.0% (7399/10000)
[Test]  Epoch: 57	Loss: 0.027602	Acc: 68.7% (6868/10000)
[Test]  Epoch: 58	Loss: 0.012427	Acc: 73.8% (7377/10000)
[Test]  Epoch: 59	Loss: 0.012455	Acc: 73.8% (7375/10000)
[Test]  Epoch: 60	Loss: 0.012648	Acc: 74.0% (7401/10000)
[Test]  Epoch: 61	Loss: 0.012571	Acc: 73.9% (7394/10000)
[Test]  Epoch: 62	Loss: 0.012493	Acc: 74.0% (7402/10000)
[Test]  Epoch: 63	Loss: 0.012521	Acc: 73.8% (7385/10000)
[Test]  Epoch: 64	Loss: 0.012482	Acc: 73.8% (7380/10000)
[Test]  Epoch: 65	Loss: 0.012466	Acc: 74.0% (7397/10000)
[Test]  Epoch: 66	Loss: 0.012410	Acc: 74.0% (7398/10000)
[Test]  Epoch: 67	Loss: 0.012364	Acc: 74.2% (7423/10000)
[Test]  Epoch: 68	Loss: 0.012378	Acc: 74.3% (7427/10000)
[Test]  Epoch: 69	Loss: 0.012394	Acc: 74.0% (7405/10000)
[Test]  Epoch: 70	Loss: 0.012354	Acc: 74.1% (7412/10000)
[Test]  Epoch: 71	Loss: 0.012296	Acc: 74.2% (7424/10000)
[Test]  Epoch: 72	Loss: 0.012405	Acc: 74.2% (7415/10000)
[Test]  Epoch: 73	Loss: 0.012364	Acc: 74.1% (7408/10000)
[Test]  Epoch: 74	Loss: 0.012312	Acc: 74.1% (7414/10000)
[Test]  Epoch: 75	Loss: 0.012304	Acc: 74.2% (7418/10000)
[Test]  Epoch: 76	Loss: 0.012310	Acc: 74.3% (7435/10000)
[Test]  Epoch: 77	Loss: 0.012348	Acc: 74.0% (7400/10000)
[Test]  Epoch: 78	Loss: 0.012388	Acc: 74.1% (7407/10000)
[Test]  Epoch: 79	Loss: 0.012346	Acc: 74.2% (7421/10000)
[Test]  Epoch: 80	Loss: 0.012289	Acc: 74.2% (7418/10000)
[Test]  Epoch: 81	Loss: 0.012329	Acc: 74.1% (7414/10000)
[Test]  Epoch: 82	Loss: 0.012310	Acc: 74.2% (7419/10000)
[Test]  Epoch: 83	Loss: 0.012362	Acc: 74.2% (7417/10000)
[Test]  Epoch: 84	Loss: 0.012336	Acc: 74.2% (7422/10000)
[Test]  Epoch: 85	Loss: 0.012325	Acc: 74.2% (7422/10000)
[Test]  Epoch: 86	Loss: 0.012310	Acc: 74.2% (7422/10000)
[Test]  Epoch: 87	Loss: 0.012286	Acc: 74.1% (7412/10000)
[Test]  Epoch: 88	Loss: 0.012276	Acc: 74.2% (7425/10000)
[Test]  Epoch: 89	Loss: 0.012363	Acc: 74.1% (7410/10000)
[Test]  Epoch: 90	Loss: 0.012347	Acc: 74.2% (7415/10000)
[Test]  Epoch: 91	Loss: 0.012327	Acc: 74.2% (7420/10000)
[Test]  Epoch: 92	Loss: 0.012373	Acc: 74.1% (7409/10000)
[Test]  Epoch: 93	Loss: 0.012329	Acc: 74.2% (7425/10000)
[Test]  Epoch: 94	Loss: 0.012350	Acc: 74.2% (7417/10000)
[Test]  Epoch: 95	Loss: 0.012320	Acc: 74.2% (7417/10000)
[Test]  Epoch: 96	Loss: 0.012235	Acc: 74.3% (7428/10000)
[Test]  Epoch: 97	Loss: 0.012298	Acc: 74.2% (7423/10000)
[Test]  Epoch: 98	Loss: 0.012320	Acc: 74.1% (7409/10000)
[Test]  Epoch: 99	Loss: 0.012281	Acc: 74.2% (7415/10000)
[Test]  Epoch: 100	Loss: 0.012292	Acc: 74.2% (7415/10000)
===========finish==========
['2024-08-19', '17:25:10.898325', '100', 'test', '0.012292476144433021', '74.15', '74.35']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 1.296541	Acc: 29.7% (2968/10000)
[Test]  Epoch: 2	Loss: 0.027680	Acc: 55.6% (5559/10000)
[Test]  Epoch: 3	Loss: 0.018973	Acc: 59.7% (5973/10000)
[Test]  Epoch: 4	Loss: 0.020202	Acc: 61.4% (6137/10000)
[Test]  Epoch: 5	Loss: 0.019710	Acc: 61.9% (6194/10000)
[Test]  Epoch: 6	Loss: 0.017525	Acc: 65.1% (6510/10000)
[Test]  Epoch: 7	Loss: 0.017066	Acc: 66.8% (6685/10000)
[Test]  Epoch: 8	Loss: 0.030984	Acc: 57.2% (5716/10000)
[Test]  Epoch: 9	Loss: 0.020420	Acc: 64.0% (6398/10000)
[Test]  Epoch: 10	Loss: 0.018426	Acc: 65.3% (6533/10000)
[Test]  Epoch: 11	Loss: 0.017325	Acc: 66.4% (6642/10000)
[Test]  Epoch: 12	Loss: 0.017507	Acc: 66.0% (6598/10000)
[Test]  Epoch: 13	Loss: 0.016741	Acc: 67.1% (6711/10000)
[Test]  Epoch: 14	Loss: 0.016290	Acc: 67.7% (6772/10000)
[Test]  Epoch: 15	Loss: 0.016049	Acc: 68.4% (6839/10000)
[Test]  Epoch: 16	Loss: 0.015779	Acc: 68.5% (6853/10000)
[Test]  Epoch: 17	Loss: 0.015674	Acc: 68.6% (6864/10000)
[Test]  Epoch: 18	Loss: 0.015488	Acc: 68.7% (6873/10000)
[Test]  Epoch: 19	Loss: 0.015417	Acc: 68.9% (6893/10000)
[Test]  Epoch: 20	Loss: 0.015460	Acc: 68.8% (6884/10000)
[Test]  Epoch: 21	Loss: 0.015377	Acc: 69.0% (6900/10000)
[Test]  Epoch: 22	Loss: 0.016031	Acc: 68.1% (6811/10000)
[Test]  Epoch: 23	Loss: 0.015327	Acc: 68.9% (6891/10000)
[Test]  Epoch: 24	Loss: 0.015418	Acc: 68.9% (6889/10000)
[Test]  Epoch: 25	Loss: 0.015585	Acc: 68.7% (6872/10000)
[Test]  Epoch: 26	Loss: 0.015196	Acc: 69.1% (6909/10000)
[Test]  Epoch: 27	Loss: 0.019239	Acc: 67.2% (6725/10000)
[Test]  Epoch: 28	Loss: 0.015625	Acc: 68.5% (6852/10000)
[Test]  Epoch: 29	Loss: 0.015048	Acc: 69.3% (6930/10000)
[Test]  Epoch: 30	Loss: 0.015397	Acc: 68.7% (6866/10000)
[Test]  Epoch: 31	Loss: 0.015312	Acc: 69.2% (6924/10000)
[Test]  Epoch: 32	Loss: 0.015079	Acc: 69.3% (6926/10000)
[Test]  Epoch: 33	Loss: 0.015005	Acc: 69.0% (6904/10000)
[Test]  Epoch: 34	Loss: 0.014852	Acc: 69.4% (6942/10000)
[Test]  Epoch: 35	Loss: 0.016770	Acc: 68.6% (6864/10000)
[Test]  Epoch: 36	Loss: 0.014877	Acc: 69.6% (6960/10000)
[Test]  Epoch: 37	Loss: 0.014725	Acc: 69.7% (6971/10000)
[Test]  Epoch: 38	Loss: 0.014627	Acc: 70.1% (7009/10000)
[Test]  Epoch: 39	Loss: 0.014533	Acc: 69.8% (6983/10000)
[Test]  Epoch: 40	Loss: 0.014449	Acc: 69.9% (6994/10000)
[Test]  Epoch: 41	Loss: 0.014694	Acc: 69.4% (6940/10000)
[Test]  Epoch: 42	Loss: 0.014459	Acc: 69.9% (6993/10000)
[Test]  Epoch: 43	Loss: 0.014479	Acc: 69.8% (6985/10000)
[Test]  Epoch: 44	Loss: 0.014453	Acc: 69.7% (6974/10000)
[Test]  Epoch: 45	Loss: 0.014450	Acc: 69.9% (6987/10000)
[Test]  Epoch: 46	Loss: 0.014336	Acc: 70.0% (7005/10000)
[Test]  Epoch: 47	Loss: 0.014383	Acc: 70.0% (6998/10000)
[Test]  Epoch: 48	Loss: 0.014320	Acc: 69.9% (6990/10000)
[Test]  Epoch: 49	Loss: 0.014474	Acc: 69.9% (6989/10000)
[Test]  Epoch: 50	Loss: 0.014281	Acc: 70.1% (7010/10000)
[Test]  Epoch: 51	Loss: 0.014206	Acc: 70.1% (7007/10000)
[Test]  Epoch: 52	Loss: 0.014360	Acc: 70.0% (7000/10000)
[Test]  Epoch: 53	Loss: 0.014285	Acc: 70.2% (7015/10000)
[Test]  Epoch: 54	Loss: 0.014139	Acc: 70.2% (7022/10000)
[Test]  Epoch: 55	Loss: 0.014144	Acc: 70.3% (7032/10000)
[Test]  Epoch: 56	Loss: 0.014085	Acc: 70.6% (7059/10000)
[Test]  Epoch: 57	Loss: 0.014078	Acc: 70.3% (7031/10000)
[Test]  Epoch: 58	Loss: 0.014119	Acc: 70.4% (7040/10000)
[Test]  Epoch: 59	Loss: 0.014362	Acc: 69.8% (6984/10000)
[Test]  Epoch: 60	Loss: 0.014227	Acc: 70.2% (7025/10000)
[Test]  Epoch: 61	Loss: 0.014278	Acc: 70.3% (7035/10000)
[Test]  Epoch: 62	Loss: 0.014237	Acc: 70.2% (7024/10000)
[Test]  Epoch: 63	Loss: 0.014202	Acc: 70.3% (7032/10000)
[Test]  Epoch: 64	Loss: 0.014234	Acc: 70.2% (7020/10000)
[Test]  Epoch: 65	Loss: 0.014231	Acc: 70.2% (7023/10000)
[Test]  Epoch: 66	Loss: 0.014227	Acc: 70.3% (7027/10000)
[Test]  Epoch: 67	Loss: 0.014167	Acc: 70.3% (7032/10000)
[Test]  Epoch: 68	Loss: 0.014169	Acc: 70.5% (7045/10000)
[Test]  Epoch: 69	Loss: 0.014168	Acc: 70.4% (7036/10000)
[Test]  Epoch: 70	Loss: 0.014106	Acc: 70.4% (7037/10000)
[Test]  Epoch: 71	Loss: 0.014079	Acc: 70.4% (7039/10000)
[Test]  Epoch: 72	Loss: 0.014112	Acc: 70.3% (7032/10000)
[Test]  Epoch: 73	Loss: 0.014139	Acc: 70.4% (7039/10000)
[Test]  Epoch: 74	Loss: 0.014099	Acc: 70.3% (7027/10000)
[Test]  Epoch: 75	Loss: 0.014080	Acc: 70.3% (7034/10000)
[Test]  Epoch: 76	Loss: 0.014120	Acc: 70.5% (7053/10000)
[Test]  Epoch: 77	Loss: 0.014134	Acc: 70.3% (7028/10000)
[Test]  Epoch: 78	Loss: 0.014141	Acc: 70.4% (7040/10000)
[Test]  Epoch: 79	Loss: 0.014162	Acc: 70.3% (7033/10000)
[Test]  Epoch: 80	Loss: 0.014150	Acc: 70.4% (7040/10000)
[Test]  Epoch: 81	Loss: 0.014125	Acc: 70.3% (7027/10000)
[Test]  Epoch: 82	Loss: 0.014152	Acc: 70.3% (7030/10000)
[Test]  Epoch: 83	Loss: 0.014118	Acc: 70.4% (7043/10000)
[Test]  Epoch: 84	Loss: 0.014101	Acc: 70.4% (7042/10000)
[Test]  Epoch: 85	Loss: 0.014149	Acc: 70.3% (7032/10000)
[Test]  Epoch: 86	Loss: 0.014091	Acc: 70.3% (7029/10000)
[Test]  Epoch: 87	Loss: 0.014132	Acc: 70.4% (7039/10000)
[Test]  Epoch: 88	Loss: 0.014155	Acc: 70.3% (7032/10000)
[Test]  Epoch: 89	Loss: 0.014139	Acc: 70.5% (7045/10000)
[Test]  Epoch: 90	Loss: 0.014187	Acc: 70.2% (7025/10000)
[Test]  Epoch: 91	Loss: 0.014035	Acc: 70.4% (7041/10000)
[Test]  Epoch: 92	Loss: 0.014180	Acc: 70.5% (7045/10000)
[Test]  Epoch: 93	Loss: 0.014075	Acc: 70.4% (7036/10000)
[Test]  Epoch: 94	Loss: 0.014215	Acc: 70.2% (7017/10000)
[Test]  Epoch: 95	Loss: 0.014081	Acc: 70.5% (7053/10000)
[Test]  Epoch: 96	Loss: 0.014139	Acc: 70.5% (7050/10000)
[Test]  Epoch: 97	Loss: 0.014128	Acc: 70.6% (7060/10000)
[Test]  Epoch: 98	Loss: 0.014107	Acc: 70.5% (7054/10000)
[Test]  Epoch: 99	Loss: 0.014076	Acc: 70.5% (7048/10000)
[Test]  Epoch: 100	Loss: 0.014066	Acc: 70.5% (7048/10000)
===========finish==========
['2024-08-19', '17:29:33.387089', '100', 'test', '0.014065825125575065', '70.48', '70.6']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 1.472590	Acc: 17.9% (1785/10000)
[Test]  Epoch: 2	Loss: 0.063926	Acc: 32.4% (3241/10000)
[Test]  Epoch: 3	Loss: 0.025722	Acc: 44.1% (4411/10000)
[Test]  Epoch: 4	Loss: 0.025950	Acc: 46.1% (4614/10000)
[Test]  Epoch: 5	Loss: 0.025797	Acc: 49.1% (4908/10000)
[Test]  Epoch: 6	Loss: 0.028326	Acc: 47.7% (4772/10000)
[Test]  Epoch: 7	Loss: 0.024523	Acc: 51.9% (5193/10000)
[Test]  Epoch: 8	Loss: 0.031511	Acc: 49.1% (4909/10000)
[Test]  Epoch: 9	Loss: 0.029325	Acc: 51.5% (5154/10000)
[Test]  Epoch: 10	Loss: 0.028921	Acc: 52.2% (5218/10000)
[Test]  Epoch: 11	Loss: 0.025782	Acc: 53.3% (5327/10000)
[Test]  Epoch: 12	Loss: 0.029324	Acc: 50.0% (5004/10000)
[Test]  Epoch: 13	Loss: 0.027445	Acc: 52.9% (5291/10000)
[Test]  Epoch: 14	Loss: 0.026403	Acc: 52.1% (5209/10000)
[Test]  Epoch: 15	Loss: 0.025675	Acc: 55.1% (5507/10000)
[Test]  Epoch: 16	Loss: 0.027451	Acc: 53.1% (5312/10000)
[Test]  Epoch: 17	Loss: 0.029071	Acc: 54.2% (5420/10000)
[Test]  Epoch: 18	Loss: 0.027067	Acc: 53.5% (5345/10000)
[Test]  Epoch: 19	Loss: 0.024653	Acc: 55.0% (5505/10000)
[Test]  Epoch: 20	Loss: 0.024470	Acc: 54.5% (5450/10000)
[Test]  Epoch: 21	Loss: 0.025422	Acc: 53.4% (5344/10000)
[Test]  Epoch: 22	Loss: 0.023932	Acc: 54.9% (5492/10000)
[Test]  Epoch: 23	Loss: 0.064644	Acc: 49.6% (4962/10000)
[Test]  Epoch: 24	Loss: 0.025253	Acc: 55.0% (5496/10000)
[Test]  Epoch: 25	Loss: 0.023638	Acc: 55.8% (5576/10000)
[Test]  Epoch: 26	Loss: 0.023086	Acc: 56.8% (5679/10000)
[Test]  Epoch: 27	Loss: 0.023190	Acc: 56.1% (5613/10000)
[Test]  Epoch: 28	Loss: 0.023906	Acc: 55.3% (5534/10000)
[Test]  Epoch: 29	Loss: 0.022777	Acc: 57.0% (5702/10000)
[Test]  Epoch: 30	Loss: 0.022983	Acc: 56.1% (5608/10000)
[Test]  Epoch: 31	Loss: 0.023894	Acc: 55.2% (5522/10000)
[Test]  Epoch: 32	Loss: 0.022391	Acc: 57.2% (5720/10000)
[Test]  Epoch: 33	Loss: 0.022136	Acc: 57.6% (5764/10000)
[Test]  Epoch: 34	Loss: 0.022898	Acc: 56.6% (5665/10000)
[Test]  Epoch: 35	Loss: 0.023396	Acc: 56.3% (5626/10000)
[Test]  Epoch: 36	Loss: 0.022316	Acc: 57.4% (5740/10000)
[Test]  Epoch: 37	Loss: 0.022147	Acc: 57.0% (5698/10000)
[Test]  Epoch: 38	Loss: 0.022393	Acc: 57.0% (5695/10000)
[Test]  Epoch: 39	Loss: 0.022032	Acc: 57.8% (5783/10000)
[Test]  Epoch: 40	Loss: 0.021813	Acc: 57.7% (5766/10000)
[Test]  Epoch: 41	Loss: 0.022030	Acc: 57.7% (5770/10000)
[Test]  Epoch: 42	Loss: 0.021541	Acc: 57.6% (5761/10000)
[Test]  Epoch: 43	Loss: 0.021930	Acc: 57.5% (5752/10000)
[Test]  Epoch: 44	Loss: 0.024724	Acc: 55.7% (5567/10000)
[Test]  Epoch: 45	Loss: 0.021613	Acc: 58.2% (5819/10000)
[Test]  Epoch: 46	Loss: 0.022454	Acc: 56.8% (5675/10000)
[Test]  Epoch: 47	Loss: 0.022937	Acc: 56.3% (5633/10000)
[Test]  Epoch: 48	Loss: 0.022245	Acc: 57.4% (5735/10000)
[Test]  Epoch: 49	Loss: 0.032051	Acc: 54.0% (5401/10000)
[Test]  Epoch: 50	Loss: 0.024840	Acc: 55.9% (5585/10000)
[Test]  Epoch: 51	Loss: 0.021933	Acc: 57.6% (5761/10000)
[Test]  Epoch: 52	Loss: 0.021476	Acc: 58.1% (5807/10000)
[Test]  Epoch: 53	Loss: 0.023255	Acc: 57.0% (5698/10000)
[Test]  Epoch: 54	Loss: 0.021779	Acc: 57.5% (5747/10000)
[Test]  Epoch: 55	Loss: 0.021507	Acc: 57.9% (5789/10000)
[Test]  Epoch: 56	Loss: 0.021346	Acc: 58.5% (5847/10000)
[Test]  Epoch: 57	Loss: 0.024075	Acc: 55.7% (5574/10000)
[Test]  Epoch: 58	Loss: 0.027419	Acc: 55.6% (5562/10000)
[Test]  Epoch: 59	Loss: 0.024104	Acc: 55.9% (5593/10000)
[Test]  Epoch: 60	Loss: 0.022309	Acc: 57.6% (5759/10000)
[Test]  Epoch: 61	Loss: 0.022063	Acc: 57.6% (5765/10000)
[Test]  Epoch: 62	Loss: 0.022051	Acc: 57.8% (5777/10000)
[Test]  Epoch: 63	Loss: 0.022016	Acc: 57.8% (5781/10000)
[Test]  Epoch: 64	Loss: 0.022169	Acc: 57.7% (5770/10000)
[Test]  Epoch: 65	Loss: 0.022042	Acc: 57.8% (5776/10000)
[Test]  Epoch: 66	Loss: 0.021833	Acc: 57.8% (5780/10000)
[Test]  Epoch: 67	Loss: 0.021898	Acc: 57.9% (5790/10000)
[Test]  Epoch: 68	Loss: 0.021967	Acc: 58.0% (5797/10000)
[Test]  Epoch: 69	Loss: 0.021867	Acc: 57.9% (5790/10000)
[Test]  Epoch: 70	Loss: 0.021909	Acc: 57.8% (5776/10000)
[Test]  Epoch: 71	Loss: 0.021772	Acc: 57.9% (5793/10000)
[Test]  Epoch: 72	Loss: 0.021875	Acc: 57.6% (5761/10000)
[Test]  Epoch: 73	Loss: 0.021766	Acc: 57.8% (5782/10000)
[Test]  Epoch: 74	Loss: 0.021666	Acc: 58.0% (5799/10000)
[Test]  Epoch: 75	Loss: 0.021815	Acc: 57.9% (5791/10000)
[Test]  Epoch: 76	Loss: 0.021754	Acc: 58.1% (5815/10000)
[Test]  Epoch: 77	Loss: 0.021826	Acc: 57.9% (5792/10000)
[Test]  Epoch: 78	Loss: 0.021783	Acc: 58.1% (5808/10000)
[Test]  Epoch: 79	Loss: 0.021816	Acc: 58.0% (5805/10000)
[Test]  Epoch: 80	Loss: 0.021733	Acc: 58.0% (5802/10000)
[Test]  Epoch: 81	Loss: 0.021694	Acc: 57.8% (5781/10000)
[Test]  Epoch: 82	Loss: 0.021680	Acc: 57.8% (5777/10000)
[Test]  Epoch: 83	Loss: 0.021548	Acc: 58.0% (5798/10000)
[Test]  Epoch: 84	Loss: 0.021736	Acc: 57.9% (5791/10000)
[Test]  Epoch: 85	Loss: 0.021612	Acc: 57.8% (5779/10000)
[Test]  Epoch: 86	Loss: 0.021491	Acc: 58.2% (5818/10000)
[Test]  Epoch: 87	Loss: 0.021595	Acc: 58.2% (5822/10000)
[Test]  Epoch: 88	Loss: 0.021646	Acc: 58.1% (5808/10000)
[Test]  Epoch: 89	Loss: 0.021728	Acc: 57.9% (5794/10000)
[Test]  Epoch: 90	Loss: 0.021786	Acc: 58.0% (5799/10000)
[Test]  Epoch: 91	Loss: 0.021719	Acc: 58.1% (5807/10000)
[Test]  Epoch: 92	Loss: 0.021794	Acc: 57.9% (5792/10000)
[Test]  Epoch: 93	Loss: 0.021524	Acc: 58.1% (5815/10000)
[Test]  Epoch: 94	Loss: 0.021662	Acc: 58.0% (5797/10000)
[Test]  Epoch: 95	Loss: 0.021602	Acc: 57.9% (5790/10000)
[Test]  Epoch: 96	Loss: 0.021501	Acc: 58.2% (5819/10000)
[Test]  Epoch: 97	Loss: 0.021511	Acc: 58.2% (5819/10000)
[Test]  Epoch: 98	Loss: 0.021658	Acc: 57.9% (5792/10000)
[Test]  Epoch: 99	Loss: 0.021498	Acc: 57.9% (5789/10000)
[Test]  Epoch: 100	Loss: 0.021544	Acc: 57.8% (5778/10000)
===========finish==========
['2024-08-19', '17:33:57.109116', '100', 'test', '0.021544385081529616', '57.78', '58.47']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035156	Acc: 31.1% (3109/10000)
[Test]  Epoch: 2	Loss: 0.025957	Acc: 45.0% (4504/10000)
[Test]  Epoch: 3	Loss: 0.024404	Acc: 50.9% (5085/10000)
[Test]  Epoch: 4	Loss: 0.024989	Acc: 52.8% (5279/10000)
[Test]  Epoch: 5	Loss: 0.022117	Acc: 56.5% (5646/10000)
[Test]  Epoch: 6	Loss: 0.021143	Acc: 59.0% (5901/10000)
[Test]  Epoch: 7	Loss: 0.020761	Acc: 59.3% (5928/10000)
[Test]  Epoch: 8	Loss: 0.021088	Acc: 59.4% (5937/10000)
[Test]  Epoch: 9	Loss: 0.020614	Acc: 60.1% (6011/10000)
[Test]  Epoch: 10	Loss: 0.021510	Acc: 59.3% (5930/10000)
[Test]  Epoch: 11	Loss: 0.019490	Acc: 61.9% (6193/10000)
[Test]  Epoch: 12	Loss: 0.019326	Acc: 61.9% (6186/10000)
[Test]  Epoch: 13	Loss: 0.018150	Acc: 63.7% (6369/10000)
[Test]  Epoch: 14	Loss: 0.018363	Acc: 63.3% (6327/10000)
[Test]  Epoch: 15	Loss: 0.018613	Acc: 62.9% (6294/10000)
[Test]  Epoch: 16	Loss: 0.018497	Acc: 63.2% (6323/10000)
[Test]  Epoch: 17	Loss: 0.018451	Acc: 62.9% (6287/10000)
[Test]  Epoch: 18	Loss: 0.018172	Acc: 64.0% (6396/10000)
[Test]  Epoch: 19	Loss: 0.018168	Acc: 64.0% (6395/10000)
[Test]  Epoch: 20	Loss: 0.018203	Acc: 64.2% (6418/10000)
[Test]  Epoch: 21	Loss: 0.017869	Acc: 64.0% (6402/10000)
[Test]  Epoch: 22	Loss: 0.018089	Acc: 63.5% (6351/10000)
[Test]  Epoch: 23	Loss: 0.017958	Acc: 64.2% (6415/10000)
[Test]  Epoch: 24	Loss: 0.017566	Acc: 64.5% (6449/10000)
[Test]  Epoch: 25	Loss: 0.017493	Acc: 64.9% (6492/10000)
[Test]  Epoch: 26	Loss: 0.017431	Acc: 64.7% (6472/10000)
[Test]  Epoch: 27	Loss: 0.017538	Acc: 64.8% (6478/10000)
[Test]  Epoch: 28	Loss: 0.017391	Acc: 64.7% (6469/10000)
[Test]  Epoch: 29	Loss: 0.017142	Acc: 64.9% (6494/10000)
[Test]  Epoch: 30	Loss: 0.017335	Acc: 64.7% (6471/10000)
[Test]  Epoch: 31	Loss: 0.017190	Acc: 65.1% (6508/10000)
[Test]  Epoch: 32	Loss: 0.017425	Acc: 64.5% (6451/10000)
[Test]  Epoch: 33	Loss: 0.017232	Acc: 64.8% (6485/10000)
[Test]  Epoch: 34	Loss: 0.017333	Acc: 64.9% (6493/10000)
[Test]  Epoch: 35	Loss: 0.017428	Acc: 64.5% (6449/10000)
[Test]  Epoch: 36	Loss: 0.017579	Acc: 64.2% (6424/10000)
[Test]  Epoch: 37	Loss: 0.017132	Acc: 64.9% (6487/10000)
[Test]  Epoch: 38	Loss: 0.017252	Acc: 64.8% (6479/10000)
[Test]  Epoch: 39	Loss: 0.017469	Acc: 64.5% (6450/10000)
[Test]  Epoch: 40	Loss: 0.017164	Acc: 64.8% (6484/10000)
[Test]  Epoch: 41	Loss: 0.017210	Acc: 64.9% (6490/10000)
[Test]  Epoch: 42	Loss: 0.017408	Acc: 64.5% (6448/10000)
[Test]  Epoch: 43	Loss: 0.017403	Acc: 64.5% (6453/10000)
[Test]  Epoch: 44	Loss: 0.017103	Acc: 64.7% (6472/10000)
[Test]  Epoch: 45	Loss: 0.017199	Acc: 65.0% (6495/10000)
[Test]  Epoch: 46	Loss: 0.017162	Acc: 64.6% (6461/10000)
[Test]  Epoch: 47	Loss: 0.017097	Acc: 64.9% (6492/10000)
[Test]  Epoch: 48	Loss: 0.017187	Acc: 64.8% (6485/10000)
[Test]  Epoch: 49	Loss: 0.017210	Acc: 65.0% (6495/10000)
[Test]  Epoch: 50	Loss: 0.017050	Acc: 65.0% (6504/10000)
[Test]  Epoch: 51	Loss: 0.017237	Acc: 64.8% (6484/10000)
[Test]  Epoch: 52	Loss: 0.017162	Acc: 64.8% (6479/10000)
[Test]  Epoch: 53	Loss: 0.017142	Acc: 64.7% (6469/10000)
[Test]  Epoch: 54	Loss: 0.016914	Acc: 65.1% (6507/10000)
[Test]  Epoch: 55	Loss: 0.016993	Acc: 65.0% (6501/10000)
[Test]  Epoch: 56	Loss: 0.017063	Acc: 65.0% (6504/10000)
[Test]  Epoch: 57	Loss: 0.017053	Acc: 64.9% (6494/10000)
[Test]  Epoch: 58	Loss: 0.017046	Acc: 65.2% (6516/10000)
[Test]  Epoch: 59	Loss: 0.017177	Acc: 65.0% (6495/10000)
[Test]  Epoch: 60	Loss: 0.017100	Acc: 65.2% (6519/10000)
[Test]  Epoch: 61	Loss: 0.017027	Acc: 65.2% (6521/10000)
[Test]  Epoch: 62	Loss: 0.017002	Acc: 65.4% (6544/10000)
[Test]  Epoch: 63	Loss: 0.017043	Acc: 65.2% (6519/10000)
[Test]  Epoch: 64	Loss: 0.017036	Acc: 65.3% (6533/10000)
[Test]  Epoch: 65	Loss: 0.017022	Acc: 65.4% (6544/10000)
[Test]  Epoch: 66	Loss: 0.016993	Acc: 65.4% (6542/10000)
[Test]  Epoch: 67	Loss: 0.016986	Acc: 65.0% (6500/10000)
[Test]  Epoch: 68	Loss: 0.016939	Acc: 65.5% (6545/10000)
[Test]  Epoch: 69	Loss: 0.017020	Acc: 65.4% (6541/10000)
[Test]  Epoch: 70	Loss: 0.016953	Acc: 65.3% (6529/10000)
[Test]  Epoch: 71	Loss: 0.017036	Acc: 65.0% (6505/10000)
[Test]  Epoch: 72	Loss: 0.016951	Acc: 65.2% (6524/10000)
[Test]  Epoch: 73	Loss: 0.016898	Acc: 65.2% (6521/10000)
[Test]  Epoch: 74	Loss: 0.016996	Acc: 65.2% (6518/10000)
[Test]  Epoch: 75	Loss: 0.017059	Acc: 65.0% (6497/10000)
[Test]  Epoch: 76	Loss: 0.016949	Acc: 65.3% (6530/10000)
[Test]  Epoch: 77	Loss: 0.016943	Acc: 65.2% (6521/10000)
[Test]  Epoch: 78	Loss: 0.017080	Acc: 65.2% (6524/10000)
[Test]  Epoch: 79	Loss: 0.017007	Acc: 65.4% (6542/10000)
[Test]  Epoch: 80	Loss: 0.016977	Acc: 65.3% (6535/10000)
[Test]  Epoch: 81	Loss: 0.017028	Acc: 65.0% (6503/10000)
[Test]  Epoch: 82	Loss: 0.017009	Acc: 65.2% (6524/10000)
[Test]  Epoch: 83	Loss: 0.016914	Acc: 65.5% (6546/10000)
[Test]  Epoch: 84	Loss: 0.016913	Acc: 65.3% (6527/10000)
[Test]  Epoch: 85	Loss: 0.017003	Acc: 65.1% (6512/10000)
[Test]  Epoch: 86	Loss: 0.016932	Acc: 65.5% (6552/10000)
[Test]  Epoch: 87	Loss: 0.016943	Acc: 65.2% (6518/10000)
[Test]  Epoch: 88	Loss: 0.017019	Acc: 65.2% (6521/10000)
[Test]  Epoch: 89	Loss: 0.017049	Acc: 65.2% (6525/10000)
[Test]  Epoch: 90	Loss: 0.016984	Acc: 65.5% (6546/10000)
[Test]  Epoch: 91	Loss: 0.016919	Acc: 65.4% (6543/10000)
[Test]  Epoch: 92	Loss: 0.016967	Acc: 65.1% (6513/10000)
[Test]  Epoch: 93	Loss: 0.016912	Acc: 65.3% (6535/10000)
[Test]  Epoch: 94	Loss: 0.016973	Acc: 65.2% (6515/10000)
[Test]  Epoch: 95	Loss: 0.017008	Acc: 65.1% (6506/10000)
[Test]  Epoch: 96	Loss: 0.016930	Acc: 65.2% (6517/10000)
[Test]  Epoch: 97	Loss: 0.016932	Acc: 65.2% (6520/10000)
[Test]  Epoch: 98	Loss: 0.016968	Acc: 65.2% (6521/10000)
[Test]  Epoch: 99	Loss: 0.016980	Acc: 65.1% (6508/10000)
[Test]  Epoch: 100	Loss: 0.016964	Acc: 65.1% (6512/10000)
===========finish==========
['2024-08-19', '17:38:12.485078', '100', 'test', '0.016963566333055496', '65.12', '65.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight', 'layer3.2.conv3.weight', 'layer4.2.conv3.weight', 'layer1.1.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv3.weight', 'layer3.3.conv1.weight', 'layer4.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.0.conv3.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight', 'layer3.2.conv3.weight', 'layer4.2.conv3.weight', 'layer1.1.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv3.weight', 'layer3.3.conv1.weight', 'layer4.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.0.conv3.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.040317	Acc: 32.2% (3218/10000)
[Test]  Epoch: 2	Loss: 0.024214	Acc: 49.0% (4904/10000)
[Test]  Epoch: 3	Loss: 0.026572	Acc: 50.4% (5044/10000)
[Test]  Epoch: 4	Loss: 0.023022	Acc: 55.1% (5514/10000)
[Test]  Epoch: 5	Loss: 0.022530	Acc: 55.9% (5589/10000)
[Test]  Epoch: 6	Loss: 0.021472	Acc: 57.1% (5711/10000)
[Test]  Epoch: 7	Loss: 0.024172	Acc: 54.0% (5395/10000)
[Test]  Epoch: 8	Loss: 0.023800	Acc: 56.7% (5671/10000)
[Test]  Epoch: 9	Loss: 0.020883	Acc: 59.1% (5912/10000)
[Test]  Epoch: 10	Loss: 0.020428	Acc: 60.2% (6023/10000)
[Test]  Epoch: 11	Loss: 0.020287	Acc: 61.0% (6098/10000)
[Test]  Epoch: 12	Loss: 0.020086	Acc: 60.6% (6058/10000)
[Test]  Epoch: 13	Loss: 0.019446	Acc: 61.1% (6107/10000)
[Test]  Epoch: 14	Loss: 0.019902	Acc: 60.8% (6083/10000)
[Test]  Epoch: 15	Loss: 0.019912	Acc: 60.9% (6088/10000)
[Test]  Epoch: 16	Loss: 0.019387	Acc: 61.5% (6146/10000)
[Test]  Epoch: 17	Loss: 0.019345	Acc: 61.7% (6168/10000)
[Test]  Epoch: 18	Loss: 0.019353	Acc: 61.9% (6192/10000)
[Test]  Epoch: 19	Loss: 0.018988	Acc: 62.2% (6222/10000)
[Test]  Epoch: 20	Loss: 0.018874	Acc: 62.2% (6216/10000)
[Test]  Epoch: 21	Loss: 0.018698	Acc: 62.2% (6224/10000)
[Test]  Epoch: 22	Loss: 0.018530	Acc: 62.6% (6260/10000)
[Test]  Epoch: 23	Loss: 0.018634	Acc: 62.8% (6280/10000)
[Test]  Epoch: 24	Loss: 0.019028	Acc: 62.3% (6229/10000)
[Test]  Epoch: 25	Loss: 0.018654	Acc: 62.4% (6243/10000)
[Test]  Epoch: 26	Loss: 0.018848	Acc: 62.0% (6205/10000)
[Test]  Epoch: 27	Loss: 0.018760	Acc: 62.6% (6258/10000)
[Test]  Epoch: 28	Loss: 0.018416	Acc: 63.0% (6301/10000)
[Test]  Epoch: 29	Loss: 0.018976	Acc: 61.8% (6178/10000)
[Test]  Epoch: 30	Loss: 0.018923	Acc: 61.8% (6179/10000)
[Test]  Epoch: 31	Loss: 0.018937	Acc: 62.4% (6241/10000)
[Test]  Epoch: 32	Loss: 0.018819	Acc: 62.2% (6225/10000)
[Test]  Epoch: 33	Loss: 0.018635	Acc: 62.4% (6242/10000)
[Test]  Epoch: 34	Loss: 0.018510	Acc: 62.8% (6278/10000)
[Test]  Epoch: 35	Loss: 0.018520	Acc: 62.7% (6274/10000)
[Test]  Epoch: 36	Loss: 0.018757	Acc: 62.4% (6244/10000)
[Test]  Epoch: 37	Loss: 0.018692	Acc: 62.7% (6271/10000)
[Test]  Epoch: 38	Loss: 0.018437	Acc: 62.9% (6285/10000)
[Test]  Epoch: 39	Loss: 0.018587	Acc: 62.7% (6268/10000)
[Test]  Epoch: 40	Loss: 0.018454	Acc: 62.7% (6269/10000)
[Test]  Epoch: 41	Loss: 0.018593	Acc: 62.7% (6266/10000)
[Test]  Epoch: 42	Loss: 0.018498	Acc: 62.6% (6264/10000)
[Test]  Epoch: 43	Loss: 0.018484	Acc: 62.7% (6274/10000)
[Test]  Epoch: 44	Loss: 0.018227	Acc: 63.0% (6302/10000)
[Test]  Epoch: 45	Loss: 0.018259	Acc: 63.1% (6315/10000)
[Test]  Epoch: 46	Loss: 0.018136	Acc: 63.3% (6332/10000)
[Test]  Epoch: 47	Loss: 0.018168	Acc: 63.3% (6328/10000)
[Test]  Epoch: 48	Loss: 0.018105	Acc: 63.3% (6334/10000)
[Test]  Epoch: 49	Loss: 0.018434	Acc: 63.1% (6306/10000)
[Test]  Epoch: 50	Loss: 0.018277	Acc: 63.2% (6320/10000)
[Test]  Epoch: 51	Loss: 0.018282	Acc: 62.9% (6294/10000)
[Test]  Epoch: 52	Loss: 0.018196	Acc: 63.1% (6309/10000)
[Test]  Epoch: 53	Loss: 0.018248	Acc: 63.3% (6328/10000)
[Test]  Epoch: 54	Loss: 0.018179	Acc: 63.3% (6330/10000)
[Test]  Epoch: 55	Loss: 0.018189	Acc: 63.4% (6336/10000)
[Test]  Epoch: 56	Loss: 0.018287	Acc: 63.3% (6332/10000)
[Test]  Epoch: 57	Loss: 0.018139	Acc: 63.3% (6329/10000)
[Test]  Epoch: 58	Loss: 0.018185	Acc: 63.4% (6341/10000)
[Test]  Epoch: 59	Loss: 0.018330	Acc: 63.1% (6314/10000)
[Test]  Epoch: 60	Loss: 0.018315	Acc: 63.2% (6319/10000)
[Test]  Epoch: 61	Loss: 0.018253	Acc: 63.4% (6341/10000)
[Test]  Epoch: 62	Loss: 0.018249	Acc: 63.5% (6348/10000)
[Test]  Epoch: 63	Loss: 0.018151	Acc: 63.4% (6335/10000)
[Test]  Epoch: 64	Loss: 0.018155	Acc: 63.4% (6338/10000)
[Test]  Epoch: 65	Loss: 0.018194	Acc: 63.4% (6342/10000)
[Test]  Epoch: 66	Loss: 0.018228	Acc: 63.3% (6331/10000)
[Test]  Epoch: 67	Loss: 0.018206	Acc: 63.4% (6340/10000)
[Test]  Epoch: 68	Loss: 0.018191	Acc: 63.5% (6355/10000)
[Test]  Epoch: 69	Loss: 0.018172	Acc: 63.5% (6351/10000)
[Test]  Epoch: 70	Loss: 0.018128	Acc: 63.6% (6362/10000)
[Test]  Epoch: 71	Loss: 0.018166	Acc: 63.4% (6340/10000)
[Test]  Epoch: 72	Loss: 0.018087	Acc: 63.4% (6344/10000)
[Test]  Epoch: 73	Loss: 0.018091	Acc: 63.5% (6351/10000)
[Test]  Epoch: 74	Loss: 0.018128	Acc: 63.4% (6339/10000)
[Test]  Epoch: 75	Loss: 0.018140	Acc: 63.4% (6340/10000)
[Test]  Epoch: 76	Loss: 0.018135	Acc: 63.6% (6357/10000)
[Test]  Epoch: 77	Loss: 0.018103	Acc: 63.3% (6332/10000)
[Test]  Epoch: 78	Loss: 0.018161	Acc: 63.5% (6345/10000)
[Test]  Epoch: 79	Loss: 0.018213	Acc: 63.4% (6339/10000)
[Test]  Epoch: 80	Loss: 0.018156	Acc: 63.4% (6340/10000)
[Test]  Epoch: 81	Loss: 0.018172	Acc: 63.3% (6328/10000)
[Test]  Epoch: 82	Loss: 0.018227	Acc: 63.3% (6330/10000)
[Test]  Epoch: 83	Loss: 0.018136	Acc: 63.3% (6332/10000)
[Test]  Epoch: 84	Loss: 0.018071	Acc: 63.5% (6351/10000)
[Test]  Epoch: 85	Loss: 0.018169	Acc: 63.2% (6324/10000)
[Test]  Epoch: 86	Loss: 0.018134	Acc: 63.3% (6332/10000)
[Test]  Epoch: 87	Loss: 0.018156	Acc: 63.5% (6348/10000)
[Test]  Epoch: 88	Loss: 0.018159	Acc: 63.5% (6355/10000)
[Test]  Epoch: 89	Loss: 0.018176	Acc: 63.5% (6348/10000)
[Test]  Epoch: 90	Loss: 0.018188	Acc: 63.4% (6342/10000)
[Test]  Epoch: 91	Loss: 0.018152	Acc: 63.5% (6346/10000)
[Test]  Epoch: 92	Loss: 0.018155	Acc: 63.4% (6336/10000)
[Test]  Epoch: 93	Loss: 0.018122	Acc: 63.5% (6350/10000)
[Test]  Epoch: 94	Loss: 0.018178	Acc: 63.4% (6341/10000)
[Test]  Epoch: 95	Loss: 0.018134	Acc: 63.4% (6342/10000)
[Test]  Epoch: 96	Loss: 0.018106	Acc: 63.2% (6324/10000)
[Test]  Epoch: 97	Loss: 0.018155	Acc: 63.4% (6336/10000)
[Test]  Epoch: 98	Loss: 0.018178	Acc: 63.4% (6342/10000)
[Test]  Epoch: 99	Loss: 0.018204	Acc: 63.4% (6338/10000)
[Test]  Epoch: 100	Loss: 0.018183	Acc: 63.5% (6348/10000)
===========finish==========
['2024-08-19', '17:42:39.483991', '100', 'test', '0.01818251721262932', '63.48', '63.62']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight', 'layer3.2.conv3.weight', 'layer4.2.conv3.weight', 'layer1.1.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv3.weight', 'layer3.3.conv1.weight', 'layer4.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer3.1.conv1.weight', 'layer4.2.conv2.weight', 'layer3.3.conv2.weight', 'layer3.2.conv2.weight', 'layer3.1.conv2.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv3.weight', 'conv1.weight', 'layer2.2.conv1.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight', 'layer3.2.conv3.weight', 'layer4.2.conv3.weight', 'layer1.1.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv3.weight', 'layer3.3.conv1.weight', 'layer4.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer3.1.conv1.weight', 'layer4.2.conv2.weight', 'layer3.3.conv2.weight', 'layer3.2.conv2.weight', 'layer3.1.conv2.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv3.weight', 'conv1.weight', 'layer2.2.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.031747	Acc: 28.4% (2841/10000)
[Test]  Epoch: 2	Loss: 0.033151	Acc: 36.2% (3625/10000)
[Test]  Epoch: 3	Loss: 0.026274	Acc: 44.5% (4447/10000)
[Test]  Epoch: 4	Loss: 0.031534	Acc: 41.9% (4192/10000)
[Test]  Epoch: 5	Loss: 0.029760	Acc: 45.0% (4500/10000)
[Test]  Epoch: 6	Loss: 0.028904	Acc: 46.9% (4694/10000)
[Test]  Epoch: 7	Loss: 0.028619	Acc: 47.1% (4713/10000)
[Test]  Epoch: 8	Loss: 0.030861	Acc: 46.9% (4687/10000)
[Test]  Epoch: 9	Loss: 0.027618	Acc: 49.3% (4930/10000)
[Test]  Epoch: 10	Loss: 0.027258	Acc: 49.5% (4952/10000)
[Test]  Epoch: 11	Loss: 0.027250	Acc: 49.5% (4948/10000)
[Test]  Epoch: 12	Loss: 0.026379	Acc: 50.8% (5076/10000)
[Test]  Epoch: 13	Loss: 0.025412	Acc: 51.6% (5156/10000)
[Test]  Epoch: 14	Loss: 0.025731	Acc: 51.4% (5135/10000)
[Test]  Epoch: 15	Loss: 0.025879	Acc: 50.9% (5093/10000)
[Test]  Epoch: 16	Loss: 0.026626	Acc: 49.9% (4990/10000)
[Test]  Epoch: 17	Loss: 0.025394	Acc: 52.0% (5200/10000)
[Test]  Epoch: 18	Loss: 0.024939	Acc: 52.2% (5225/10000)
[Test]  Epoch: 19	Loss: 0.024839	Acc: 52.2% (5225/10000)
[Test]  Epoch: 20	Loss: 0.024785	Acc: 52.4% (5243/10000)
[Test]  Epoch: 21	Loss: 0.024805	Acc: 52.5% (5247/10000)
[Test]  Epoch: 22	Loss: 0.025077	Acc: 52.3% (5230/10000)
[Test]  Epoch: 23	Loss: 0.025029	Acc: 52.5% (5250/10000)
[Test]  Epoch: 24	Loss: 0.024814	Acc: 52.8% (5278/10000)
[Test]  Epoch: 25	Loss: 0.025014	Acc: 52.7% (5270/10000)
[Test]  Epoch: 26	Loss: 0.024688	Acc: 52.9% (5292/10000)
[Test]  Epoch: 27	Loss: 0.024853	Acc: 53.3% (5332/10000)
[Test]  Epoch: 28	Loss: 0.024852	Acc: 53.5% (5345/10000)
[Test]  Epoch: 29	Loss: 0.024869	Acc: 53.0% (5302/10000)
[Test]  Epoch: 30	Loss: 0.024842	Acc: 53.1% (5314/10000)
[Test]  Epoch: 31	Loss: 0.024808	Acc: 53.4% (5341/10000)
[Test]  Epoch: 32	Loss: 0.025185	Acc: 52.8% (5280/10000)
[Test]  Epoch: 33	Loss: 0.025044	Acc: 53.0% (5303/10000)
[Test]  Epoch: 34	Loss: 0.025012	Acc: 53.1% (5310/10000)
[Test]  Epoch: 35	Loss: 0.025051	Acc: 52.9% (5290/10000)
[Test]  Epoch: 36	Loss: 0.025118	Acc: 53.0% (5304/10000)
[Test]  Epoch: 37	Loss: 0.025141	Acc: 53.4% (5336/10000)
[Test]  Epoch: 38	Loss: 0.025474	Acc: 53.4% (5337/10000)
[Test]  Epoch: 39	Loss: 0.025572	Acc: 52.8% (5277/10000)
[Test]  Epoch: 40	Loss: 0.025100	Acc: 53.2% (5318/10000)
[Test]  Epoch: 41	Loss: 0.025417	Acc: 52.9% (5293/10000)
[Test]  Epoch: 42	Loss: 0.025223	Acc: 52.9% (5286/10000)
[Test]  Epoch: 43	Loss: 0.025235	Acc: 52.9% (5294/10000)
[Test]  Epoch: 44	Loss: 0.025220	Acc: 53.3% (5330/10000)
[Test]  Epoch: 45	Loss: 0.025333	Acc: 53.1% (5308/10000)
[Test]  Epoch: 46	Loss: 0.025265	Acc: 53.2% (5319/10000)
[Test]  Epoch: 47	Loss: 0.025198	Acc: 53.0% (5304/10000)
[Test]  Epoch: 48	Loss: 0.025449	Acc: 53.0% (5297/10000)
[Test]  Epoch: 49	Loss: 0.025420	Acc: 52.9% (5289/10000)
[Test]  Epoch: 50	Loss: 0.025482	Acc: 52.7% (5273/10000)
[Test]  Epoch: 51	Loss: 0.025450	Acc: 53.0% (5302/10000)
[Test]  Epoch: 52	Loss: 0.025461	Acc: 52.8% (5284/10000)
[Test]  Epoch: 53	Loss: 0.025506	Acc: 52.8% (5280/10000)
[Test]  Epoch: 54	Loss: 0.025465	Acc: 52.8% (5283/10000)
[Test]  Epoch: 55	Loss: 0.025978	Acc: 52.3% (5231/10000)
[Test]  Epoch: 56	Loss: 0.025717	Acc: 52.4% (5244/10000)
[Test]  Epoch: 57	Loss: 0.025489	Acc: 53.3% (5326/10000)
[Test]  Epoch: 58	Loss: 0.025447	Acc: 53.5% (5351/10000)
[Test]  Epoch: 59	Loss: 0.025819	Acc: 52.9% (5289/10000)
[Test]  Epoch: 60	Loss: 0.025664	Acc: 52.7% (5272/10000)
[Test]  Epoch: 61	Loss: 0.025599	Acc: 53.4% (5342/10000)
[Test]  Epoch: 62	Loss: 0.025625	Acc: 53.3% (5330/10000)
[Test]  Epoch: 63	Loss: 0.025549	Acc: 53.3% (5332/10000)
[Test]  Epoch: 64	Loss: 0.025597	Acc: 53.3% (5331/10000)
[Test]  Epoch: 65	Loss: 0.025645	Acc: 53.2% (5324/10000)
[Test]  Epoch: 66	Loss: 0.025687	Acc: 53.1% (5313/10000)
[Test]  Epoch: 67	Loss: 0.025542	Acc: 53.2% (5316/10000)
[Test]  Epoch: 68	Loss: 0.025696	Acc: 53.3% (5333/10000)
[Test]  Epoch: 69	Loss: 0.025652	Acc: 53.1% (5315/10000)
[Test]  Epoch: 70	Loss: 0.025490	Acc: 53.4% (5336/10000)
[Test]  Epoch: 71	Loss: 0.025599	Acc: 53.1% (5311/10000)
[Test]  Epoch: 72	Loss: 0.025496	Acc: 53.6% (5357/10000)
[Test]  Epoch: 73	Loss: 0.025566	Acc: 53.2% (5322/10000)
[Test]  Epoch: 74	Loss: 0.025553	Acc: 53.5% (5349/10000)
[Test]  Epoch: 75	Loss: 0.025670	Acc: 53.2% (5318/10000)
[Test]  Epoch: 76	Loss: 0.025603	Acc: 53.2% (5318/10000)
[Test]  Epoch: 77	Loss: 0.025646	Acc: 53.2% (5318/10000)
[Test]  Epoch: 78	Loss: 0.025623	Acc: 53.3% (5333/10000)
[Test]  Epoch: 79	Loss: 0.025731	Acc: 53.5% (5352/10000)
[Test]  Epoch: 80	Loss: 0.025640	Acc: 53.2% (5317/10000)
[Test]  Epoch: 81	Loss: 0.025664	Acc: 53.0% (5295/10000)
[Test]  Epoch: 82	Loss: 0.025620	Acc: 53.1% (5308/10000)
[Test]  Epoch: 83	Loss: 0.025656	Acc: 53.4% (5343/10000)
[Test]  Epoch: 84	Loss: 0.025581	Acc: 53.5% (5346/10000)
[Test]  Epoch: 85	Loss: 0.025620	Acc: 53.2% (5322/10000)
[Test]  Epoch: 86	Loss: 0.025631	Acc: 53.6% (5357/10000)
[Test]  Epoch: 87	Loss: 0.025660	Acc: 53.2% (5323/10000)
[Test]  Epoch: 88	Loss: 0.025739	Acc: 53.1% (5314/10000)
[Test]  Epoch: 89	Loss: 0.025648	Acc: 53.5% (5346/10000)
[Test]  Epoch: 90	Loss: 0.025742	Acc: 53.4% (5335/10000)
[Test]  Epoch: 91	Loss: 0.025560	Acc: 53.3% (5331/10000)
[Test]  Epoch: 92	Loss: 0.025798	Acc: 53.2% (5319/10000)
[Test]  Epoch: 93	Loss: 0.025675	Acc: 53.4% (5340/10000)
[Test]  Epoch: 94	Loss: 0.025659	Acc: 53.4% (5336/10000)
[Test]  Epoch: 95	Loss: 0.025715	Acc: 53.3% (5332/10000)
[Test]  Epoch: 96	Loss: 0.025699	Acc: 53.3% (5327/10000)
[Test]  Epoch: 97	Loss: 0.025726	Acc: 53.3% (5326/10000)
[Test]  Epoch: 98	Loss: 0.025627	Acc: 53.3% (5326/10000)
[Test]  Epoch: 99	Loss: 0.025755	Acc: 53.0% (5296/10000)
[Test]  Epoch: 100	Loss: 0.025691	Acc: 53.1% (5308/10000)
===========finish==========
['2024-08-19', '17:47:02.789230', '100', 'test', '0.025690759587287903', '53.08', '53.57']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight', 'layer3.2.conv3.weight', 'layer4.2.conv3.weight', 'layer1.1.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv3.weight', 'layer3.3.conv1.weight', 'layer4.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer3.1.conv1.weight', 'layer4.2.conv2.weight', 'layer3.3.conv2.weight', 'layer3.2.conv2.weight', 'layer3.1.conv2.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv3.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer2.3.conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv1.weight', 'layer2.0.conv3.weight', 'layer3.0.conv3.weight', 'layer2.2.conv2.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight', 'layer3.2.conv3.weight', 'layer4.2.conv3.weight', 'layer1.1.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv3.weight', 'layer3.3.conv1.weight', 'layer4.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer3.1.conv1.weight', 'layer4.2.conv2.weight', 'layer3.3.conv2.weight', 'layer3.2.conv2.weight', 'layer3.1.conv2.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv3.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer2.3.conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv1.weight', 'layer2.0.conv3.weight', 'layer3.0.conv3.weight', 'layer2.2.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.043256	Acc: 15.2% (1522/10000)
[Test]  Epoch: 2	Loss: 0.028692	Acc: 36.1% (3613/10000)
[Test]  Epoch: 3	Loss: 0.029855	Acc: 39.7% (3969/10000)
[Test]  Epoch: 4	Loss: 0.032712	Acc: 40.5% (4052/10000)
[Test]  Epoch: 5	Loss: 0.029043	Acc: 46.9% (4688/10000)
[Test]  Epoch: 6	Loss: 0.028213	Acc: 48.3% (4826/10000)
[Test]  Epoch: 7	Loss: 0.028974	Acc: 46.7% (4672/10000)
[Test]  Epoch: 8	Loss: 0.028402	Acc: 48.5% (4852/10000)
[Test]  Epoch: 9	Loss: 0.028907	Acc: 48.6% (4862/10000)
[Test]  Epoch: 10	Loss: 0.029111	Acc: 49.0% (4902/10000)
[Test]  Epoch: 11	Loss: 0.028451	Acc: 48.9% (4886/10000)
[Test]  Epoch: 12	Loss: 0.029952	Acc: 47.6% (4765/10000)
[Test]  Epoch: 13	Loss: 0.027436	Acc: 50.0% (5002/10000)
[Test]  Epoch: 14	Loss: 0.027227	Acc: 49.2% (4925/10000)
[Test]  Epoch: 15	Loss: 0.027124	Acc: 50.3% (5027/10000)
[Test]  Epoch: 16	Loss: 0.026679	Acc: 50.4% (5044/10000)
[Test]  Epoch: 17	Loss: 0.026576	Acc: 51.2% (5117/10000)
[Test]  Epoch: 18	Loss: 0.026561	Acc: 50.4% (5041/10000)
[Test]  Epoch: 19	Loss: 0.026870	Acc: 50.7% (5069/10000)
[Test]  Epoch: 20	Loss: 0.026769	Acc: 51.0% (5100/10000)
[Test]  Epoch: 21	Loss: 0.027084	Acc: 51.1% (5107/10000)
[Test]  Epoch: 22	Loss: 0.027324	Acc: 51.0% (5101/10000)
[Test]  Epoch: 23	Loss: 0.026764	Acc: 51.1% (5115/10000)
[Test]  Epoch: 24	Loss: 0.026922	Acc: 51.1% (5107/10000)
[Test]  Epoch: 25	Loss: 0.027015	Acc: 50.9% (5086/10000)
[Test]  Epoch: 26	Loss: 0.026803	Acc: 50.8% (5082/10000)
[Test]  Epoch: 27	Loss: 0.027101	Acc: 51.0% (5096/10000)
[Test]  Epoch: 28	Loss: 0.026979	Acc: 51.0% (5101/10000)
[Test]  Epoch: 29	Loss: 0.026558	Acc: 51.3% (5133/10000)
[Test]  Epoch: 30	Loss: 0.026924	Acc: 51.5% (5150/10000)
[Test]  Epoch: 31	Loss: 0.026700	Acc: 51.5% (5151/10000)
[Test]  Epoch: 32	Loss: 0.026853	Acc: 51.5% (5155/10000)
[Test]  Epoch: 33	Loss: 0.026686	Acc: 51.3% (5126/10000)
[Test]  Epoch: 34	Loss: 0.026706	Acc: 51.3% (5134/10000)
[Test]  Epoch: 35	Loss: 0.026753	Acc: 51.4% (5135/10000)
[Test]  Epoch: 36	Loss: 0.027064	Acc: 51.2% (5116/10000)
[Test]  Epoch: 37	Loss: 0.027214	Acc: 51.1% (5113/10000)
[Test]  Epoch: 38	Loss: 0.027061	Acc: 51.0% (5096/10000)
[Test]  Epoch: 39	Loss: 0.027489	Acc: 50.9% (5092/10000)
[Test]  Epoch: 40	Loss: 0.027243	Acc: 51.1% (5115/10000)
[Test]  Epoch: 41	Loss: 0.027292	Acc: 51.2% (5121/10000)
[Test]  Epoch: 42	Loss: 0.027169	Acc: 51.1% (5109/10000)
[Test]  Epoch: 43	Loss: 0.027371	Acc: 51.3% (5126/10000)
[Test]  Epoch: 44	Loss: 0.027137	Acc: 51.5% (5152/10000)
[Test]  Epoch: 45	Loss: 0.027332	Acc: 51.5% (5149/10000)
[Test]  Epoch: 46	Loss: 0.027285	Acc: 51.6% (5157/10000)
[Test]  Epoch: 47	Loss: 0.027349	Acc: 51.5% (5149/10000)
[Test]  Epoch: 48	Loss: 0.027402	Acc: 51.4% (5144/10000)
[Test]  Epoch: 49	Loss: 0.027359	Acc: 51.3% (5130/10000)
[Test]  Epoch: 50	Loss: 0.027358	Acc: 51.5% (5150/10000)
[Test]  Epoch: 51	Loss: 0.027076	Acc: 51.8% (5175/10000)
[Test]  Epoch: 52	Loss: 0.027197	Acc: 51.6% (5162/10000)
[Test]  Epoch: 53	Loss: 0.027183	Acc: 51.5% (5155/10000)
[Test]  Epoch: 54	Loss: 0.026960	Acc: 51.7% (5166/10000)
[Test]  Epoch: 55	Loss: 0.027070	Acc: 51.8% (5176/10000)
[Test]  Epoch: 56	Loss: 0.026974	Acc: 51.9% (5186/10000)
[Test]  Epoch: 57	Loss: 0.027096	Acc: 51.6% (5158/10000)
[Test]  Epoch: 58	Loss: 0.027284	Acc: 51.5% (5153/10000)
[Test]  Epoch: 59	Loss: 0.027357	Acc: 51.3% (5132/10000)
[Test]  Epoch: 60	Loss: 0.027317	Acc: 51.5% (5147/10000)
[Test]  Epoch: 61	Loss: 0.027346	Acc: 51.6% (5164/10000)
[Test]  Epoch: 62	Loss: 0.027213	Acc: 51.6% (5159/10000)
[Test]  Epoch: 63	Loss: 0.027139	Acc: 51.6% (5160/10000)
[Test]  Epoch: 64	Loss: 0.027246	Acc: 51.6% (5158/10000)
[Test]  Epoch: 65	Loss: 0.027259	Acc: 52.0% (5195/10000)
[Test]  Epoch: 66	Loss: 0.027211	Acc: 51.6% (5160/10000)
[Test]  Epoch: 67	Loss: 0.027182	Acc: 51.6% (5159/10000)
[Test]  Epoch: 68	Loss: 0.027268	Acc: 51.6% (5160/10000)
[Test]  Epoch: 69	Loss: 0.027331	Acc: 51.6% (5159/10000)
[Test]  Epoch: 70	Loss: 0.027066	Acc: 51.8% (5179/10000)
[Test]  Epoch: 71	Loss: 0.027220	Acc: 51.6% (5165/10000)
[Test]  Epoch: 72	Loss: 0.027161	Acc: 51.6% (5163/10000)
[Test]  Epoch: 73	Loss: 0.027202	Acc: 51.6% (5162/10000)
[Test]  Epoch: 74	Loss: 0.027263	Acc: 51.6% (5165/10000)
[Test]  Epoch: 75	Loss: 0.027323	Acc: 51.4% (5144/10000)
[Test]  Epoch: 76	Loss: 0.027183	Acc: 51.6% (5157/10000)
[Test]  Epoch: 77	Loss: 0.027284	Acc: 51.7% (5174/10000)
[Test]  Epoch: 78	Loss: 0.027239	Acc: 51.8% (5181/10000)
[Test]  Epoch: 79	Loss: 0.027247	Acc: 51.7% (5173/10000)
[Test]  Epoch: 80	Loss: 0.027242	Acc: 51.8% (5184/10000)
[Test]  Epoch: 81	Loss: 0.027185	Acc: 51.8% (5184/10000)
[Test]  Epoch: 82	Loss: 0.027180	Acc: 51.7% (5167/10000)
[Test]  Epoch: 83	Loss: 0.027347	Acc: 51.8% (5179/10000)
[Test]  Epoch: 84	Loss: 0.027207	Acc: 51.7% (5167/10000)
[Test]  Epoch: 85	Loss: 0.027241	Acc: 51.6% (5165/10000)
[Test]  Epoch: 86	Loss: 0.027214	Acc: 51.7% (5171/10000)
[Test]  Epoch: 87	Loss: 0.027273	Acc: 51.6% (5157/10000)
[Test]  Epoch: 88	Loss: 0.027239	Acc: 51.8% (5177/10000)
[Test]  Epoch: 89	Loss: 0.027233	Acc: 51.8% (5179/10000)
[Test]  Epoch: 90	Loss: 0.027348	Acc: 51.7% (5173/10000)
[Test]  Epoch: 91	Loss: 0.027175	Acc: 51.9% (5185/10000)
[Test]  Epoch: 92	Loss: 0.027278	Acc: 51.7% (5170/10000)
[Test]  Epoch: 93	Loss: 0.027258	Acc: 51.8% (5180/10000)
[Test]  Epoch: 94	Loss: 0.027340	Acc: 51.7% (5171/10000)
[Test]  Epoch: 95	Loss: 0.027294	Acc: 51.6% (5160/10000)
[Test]  Epoch: 96	Loss: 0.027194	Acc: 51.8% (5177/10000)
[Test]  Epoch: 97	Loss: 0.027243	Acc: 51.8% (5179/10000)
[Test]  Epoch: 98	Loss: 0.027307	Acc: 51.6% (5165/10000)
[Test]  Epoch: 99	Loss: 0.027273	Acc: 51.7% (5166/10000)
[Test]  Epoch: 100	Loss: 0.027309	Acc: 51.8% (5180/10000)
===========finish==========
['2024-08-19', '17:51:25.294133', '100', 'test', '0.027308913803100587', '51.8', '51.95']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet50-channel resnet50 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/cifar10-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight', 'layer3.2.conv3.weight', 'layer4.2.conv3.weight', 'layer1.1.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv3.weight', 'layer3.3.conv1.weight', 'layer4.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer3.1.conv1.weight', 'layer4.2.conv2.weight', 'layer3.3.conv2.weight', 'layer3.2.conv2.weight', 'layer3.1.conv2.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv3.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer2.3.conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv1.weight', 'layer2.0.conv3.weight', 'layer3.0.conv3.weight', 'layer2.2.conv2.weight', 'layer3.0.conv1.weight', 'layer2.3.conv2.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer3.0.downsample.0.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer3.0.conv2.weight', 'layer4.0.conv2.weight']
['layer3.4.bn2.weight', 'layer3.4.bn1.weight', 'layer4.1.bn2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn1.weight', 'layer3.4.bn3.weight', 'layer4.2.bn2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn3.weight', 'layer3.3.bn1.weight', 'layer3.5.bn3.weight', 'layer3.3.bn2.weight', 'layer3.1.bn1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn2.weight', 'layer4.2.bn3.weight', 'layer4.2.bn1.weight', 'layer1.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer3.3.bn3.weight', 'layer2.2.bn1.weight', 'layer1.1.bn2.weight', 'layer3.2.bn3.weight', 'layer2.1.bn1.weight', 'layer2.3.bn1.weight', 'layer2.2.bn2.weight', 'layer3.1.bn3.weight', 'layer2.3.bn2.weight', 'layer2.1.bn2.weight', 'layer4.0.downsample.1.weight', 'layer4.0.bn3.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.2.bn3.weight', 'bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn3.weight', 'layer3.0.bn1.weight', 'layer1.0.downsample.1.weight', 'layer2.3.bn3.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.0.bn1.weight', 'layer4.0.bn2.weight', 'layer2.1.bn3.weight', 'layer3.0.downsample.1.weight', 'layer3.0.bn3.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn3.weight', 'layer3.4.conv3.weight', 'layer4.1.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.4.conv2.weight', 'layer1.0.conv1.weight', 'layer3.5.conv1.weight', 'last_linear.weight', 'layer3.5.conv2.weight', 'layer3.3.conv3.weight', 'layer1.2.conv3.weight', 'layer3.2.conv3.weight', 'layer4.2.conv3.weight', 'layer1.1.conv1.weight', 'layer4.1.conv1.weight', 'layer1.2.conv1.weight', 'layer1.1.conv3.weight', 'layer3.3.conv1.weight', 'layer4.1.conv2.weight', 'layer3.1.conv3.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer3.1.conv1.weight', 'layer4.2.conv2.weight', 'layer3.3.conv2.weight', 'layer3.2.conv2.weight', 'layer3.1.conv2.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv3.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv2.weight', 'layer1.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer4.2.conv1.weight', 'layer2.3.conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv1.weight', 'layer2.0.conv3.weight', 'layer3.0.conv3.weight', 'layer2.2.conv2.weight', 'layer3.0.conv1.weight', 'layer2.3.conv2.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer3.0.downsample.0.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer3.0.conv2.weight', 'layer4.0.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036095	Acc: 22.5% (2246/10000)
[Test]  Epoch: 2	Loss: 0.024123	Acc: 47.2% (4725/10000)
[Test]  Epoch: 3	Loss: 0.021792	Acc: 52.9% (5285/10000)
[Test]  Epoch: 4	Loss: 0.021993	Acc: 54.4% (5438/10000)
[Test]  Epoch: 5	Loss: 0.022013	Acc: 54.8% (5478/10000)
[Test]  Epoch: 6	Loss: 0.021537	Acc: 55.6% (5558/10000)
[Test]  Epoch: 7	Loss: 0.021733	Acc: 55.3% (5533/10000)
[Test]  Epoch: 8	Loss: 0.021496	Acc: 55.7% (5569/10000)
[Test]  Epoch: 9	Loss: 0.021623	Acc: 55.6% (5557/10000)
[Test]  Epoch: 10	Loss: 0.021305	Acc: 56.0% (5598/10000)
[Test]  Epoch: 11	Loss: 0.021175	Acc: 56.2% (5624/10000)
[Test]  Epoch: 12	Loss: 0.021811	Acc: 55.3% (5532/10000)
[Test]  Epoch: 13	Loss: 0.021365	Acc: 56.2% (5616/10000)
[Test]  Epoch: 14	Loss: 0.021311	Acc: 56.3% (5631/10000)
[Test]  Epoch: 15	Loss: 0.022172	Acc: 55.9% (5592/10000)
[Test]  Epoch: 16	Loss: 0.021506	Acc: 55.9% (5592/10000)
[Test]  Epoch: 17	Loss: 0.021520	Acc: 56.2% (5623/10000)
[Test]  Epoch: 18	Loss: 0.021301	Acc: 56.5% (5645/10000)
[Test]  Epoch: 19	Loss: 0.021380	Acc: 56.4% (5640/10000)
[Test]  Epoch: 20	Loss: 0.021290	Acc: 56.5% (5653/10000)
[Test]  Epoch: 21	Loss: 0.021098	Acc: 56.9% (5693/10000)
[Test]  Epoch: 22	Loss: 0.021060	Acc: 57.0% (5702/10000)
[Test]  Epoch: 23	Loss: 0.021069	Acc: 57.0% (5703/10000)
[Test]  Epoch: 24	Loss: 0.021151	Acc: 56.8% (5678/10000)
[Test]  Epoch: 25	Loss: 0.021361	Acc: 56.8% (5678/10000)
[Test]  Epoch: 26	Loss: 0.021201	Acc: 56.7% (5669/10000)
[Test]  Epoch: 27	Loss: 0.020976	Acc: 57.0% (5700/10000)
[Test]  Epoch: 28	Loss: 0.021164	Acc: 56.8% (5684/10000)
[Test]  Epoch: 29	Loss: 0.021232	Acc: 56.8% (5677/10000)
[Test]  Epoch: 30	Loss: 0.021105	Acc: 57.1% (5708/10000)
[Test]  Epoch: 31	Loss: 0.021121	Acc: 56.8% (5683/10000)
[Test]  Epoch: 32	Loss: 0.021057	Acc: 57.2% (5723/10000)
[Test]  Epoch: 33	Loss: 0.020884	Acc: 57.2% (5725/10000)
[Test]  Epoch: 34	Loss: 0.020778	Acc: 57.2% (5723/10000)
[Test]  Epoch: 35	Loss: 0.020789	Acc: 57.4% (5739/10000)
[Test]  Epoch: 36	Loss: 0.020991	Acc: 57.3% (5726/10000)
[Test]  Epoch: 37	Loss: 0.020896	Acc: 57.3% (5732/10000)
[Test]  Epoch: 38	Loss: 0.020887	Acc: 57.5% (5749/10000)
[Test]  Epoch: 39	Loss: 0.021032	Acc: 57.2% (5719/10000)
[Test]  Epoch: 40	Loss: 0.020862	Acc: 57.4% (5735/10000)
[Test]  Epoch: 41	Loss: 0.020769	Acc: 57.3% (5730/10000)
[Test]  Epoch: 42	Loss: 0.020865	Acc: 57.2% (5723/10000)
[Test]  Epoch: 43	Loss: 0.020915	Acc: 57.5% (5745/10000)
[Test]  Epoch: 44	Loss: 0.020657	Acc: 57.6% (5759/10000)
[Test]  Epoch: 45	Loss: 0.020775	Acc: 57.4% (5742/10000)
[Test]  Epoch: 46	Loss: 0.020667	Acc: 57.4% (5739/10000)
[Test]  Epoch: 47	Loss: 0.020633	Acc: 57.4% (5743/10000)
[Test]  Epoch: 48	Loss: 0.020607	Acc: 57.3% (5734/10000)
[Test]  Epoch: 49	Loss: 0.020865	Acc: 57.3% (5732/10000)
[Test]  Epoch: 50	Loss: 0.020702	Acc: 57.1% (5714/10000)
[Test]  Epoch: 51	Loss: 0.020835	Acc: 57.2% (5717/10000)
[Test]  Epoch: 52	Loss: 0.020722	Acc: 57.4% (5739/10000)
[Test]  Epoch: 53	Loss: 0.020742	Acc: 57.4% (5744/10000)
[Test]  Epoch: 54	Loss: 0.020677	Acc: 57.5% (5748/10000)
[Test]  Epoch: 55	Loss: 0.020619	Acc: 57.3% (5728/10000)
[Test]  Epoch: 56	Loss: 0.020653	Acc: 57.5% (5747/10000)
[Test]  Epoch: 57	Loss: 0.020902	Acc: 57.1% (5715/10000)
[Test]  Epoch: 58	Loss: 0.021116	Acc: 56.9% (5692/10000)
[Test]  Epoch: 59	Loss: 0.021156	Acc: 56.9% (5685/10000)
[Test]  Epoch: 60	Loss: 0.021188	Acc: 56.7% (5670/10000)
[Test]  Epoch: 61	Loss: 0.021057	Acc: 56.9% (5694/10000)
[Test]  Epoch: 62	Loss: 0.021027	Acc: 57.2% (5719/10000)
[Test]  Epoch: 63	Loss: 0.020913	Acc: 56.9% (5690/10000)
[Test]  Epoch: 64	Loss: 0.020867	Acc: 57.2% (5721/10000)
[Test]  Epoch: 65	Loss: 0.021011	Acc: 57.1% (5712/10000)
[Test]  Epoch: 66	Loss: 0.020968	Acc: 57.0% (5704/10000)
[Test]  Epoch: 67	Loss: 0.021100	Acc: 56.7% (5673/10000)
[Test]  Epoch: 68	Loss: 0.020999	Acc: 57.0% (5699/10000)
[Test]  Epoch: 69	Loss: 0.020986	Acc: 57.2% (5716/10000)
[Test]  Epoch: 70	Loss: 0.020868	Acc: 57.1% (5711/10000)
[Test]  Epoch: 71	Loss: 0.020908	Acc: 57.1% (5712/10000)
[Test]  Epoch: 72	Loss: 0.020865	Acc: 57.2% (5716/10000)
[Test]  Epoch: 73	Loss: 0.020970	Acc: 57.2% (5717/10000)
[Test]  Epoch: 74	Loss: 0.020927	Acc: 57.1% (5713/10000)
[Test]  Epoch: 75	Loss: 0.020919	Acc: 57.0% (5701/10000)
[Test]  Epoch: 76	Loss: 0.020947	Acc: 57.2% (5720/10000)
[Test]  Epoch: 77	Loss: 0.020840	Acc: 57.0% (5699/10000)
[Test]  Epoch: 78	Loss: 0.020953	Acc: 57.4% (5739/10000)
[Test]  Epoch: 79	Loss: 0.020996	Acc: 57.4% (5738/10000)
[Test]  Epoch: 80	Loss: 0.020985	Acc: 57.1% (5708/10000)
[Test]  Epoch: 81	Loss: 0.020917	Acc: 57.2% (5724/10000)
[Test]  Epoch: 82	Loss: 0.020936	Acc: 57.0% (5700/10000)
[Test]  Epoch: 83	Loss: 0.020907	Acc: 57.3% (5728/10000)
[Test]  Epoch: 84	Loss: 0.020901	Acc: 57.3% (5732/10000)
[Test]  Epoch: 85	Loss: 0.020915	Acc: 57.1% (5713/10000)
[Test]  Epoch: 86	Loss: 0.020869	Acc: 57.4% (5736/10000)
[Test]  Epoch: 87	Loss: 0.020927	Acc: 57.2% (5720/10000)
[Test]  Epoch: 88	Loss: 0.021009	Acc: 57.0% (5695/10000)
[Test]  Epoch: 89	Loss: 0.021008	Acc: 57.0% (5697/10000)
[Test]  Epoch: 90	Loss: 0.020977	Acc: 57.0% (5703/10000)
[Test]  Epoch: 91	Loss: 0.020893	Acc: 57.2% (5720/10000)
[Test]  Epoch: 92	Loss: 0.020938	Acc: 57.0% (5704/10000)
[Test]  Epoch: 93	Loss: 0.020843	Acc: 57.2% (5722/10000)
[Test]  Epoch: 94	Loss: 0.020872	Acc: 57.2% (5721/10000)
[Test]  Epoch: 95	Loss: 0.020873	Acc: 57.2% (5725/10000)
[Test]  Epoch: 96	Loss: 0.020915	Acc: 57.2% (5725/10000)
[Test]  Epoch: 97	Loss: 0.020879	Acc: 57.1% (5712/10000)
[Test]  Epoch: 98	Loss: 0.020948	Acc: 57.1% (5712/10000)
[Test]  Epoch: 99	Loss: 0.020986	Acc: 57.0% (5703/10000)
[Test]  Epoch: 100	Loss: 0.020865	Acc: 57.2% (5717/10000)
===========finish==========
['2024-08-19', '17:55:46.151442', '100', 'test', '0.020864622813463213', '57.17', '57.59']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.bias', 0.0), ('layer1.2.bn1.weight', -125.31305694580078), ('layer1.1.bn1.weight', -127.07915496826172), ('layer1.1.bn2.weight', -134.53329467773438), ('layer1.2.bn2.weight', -169.27345275878906), ('layer1.0.bn2.weight', -169.29544067382812), ('layer1.0.bn1.weight', -178.04136657714844), ('layer4.1.bn2.weight', -225.75633239746094), ('layer2.3.bn1.weight', -240.0723876953125), ('layer4.2.bn3.weight', -248.3144073486328), ('layer2.2.bn1.weight', -278.0777587890625), ('layer2.1.bn1.weight', -327.2108154296875), ('layer2.3.bn2.weight', -331.2226257324219), ('layer2.2.bn2.weight', -331.50494384765625), ('layer3.4.bn1.weight', -335.24456787109375), ('layer3.4.bn2.weight', -355.18316650390625), ('layer3.3.bn1.weight', -373.4551696777344), ('layer2.1.bn2.weight', -374.0879211425781), ('layer4.2.bn2.weight', -393.2042541503906), ('layer3.3.bn2.weight', -440.45513916015625), ('layer4.1.bn3.weight', -453.94091796875), ('layer3.2.bn1.weight', -464.92340087890625), ('layer4.1.bn1.weight', -466.51495361328125), ('layer3.5.bn1.weight', -478.6558837890625), ('layer3.5.bn2.weight', -500.0171813964844), ('layer3.2.bn2.weight', -596.2176513671875), ('layer4.2.bn1.weight', -633.6458740234375), ('layer1.1.bn3.weight', -638.7655639648438), ('layer1.2.bn3.weight', -640.8837890625), ('layer3.1.bn1.weight', -681.2804565429688), ('layer2.0.bn2.weight', -702.5533447265625), ('layer2.0.bn1.weight', -745.2955322265625), ('layer3.1.bn2.weight', -829.150634765625), ('layer1.0.bn3.weight', -900.1953125), ('bn1.weight', -1114.389404296875), ('layer2.3.bn3.weight', -1416.1766357421875), ('layer4.0.bn3.weight', -1433.3736572265625), ('layer2.2.bn3.weight', -1597.8956298828125), ('layer3.4.bn3.weight', -1684.715087890625), ('layer2.1.bn3.weight', -1776.26806640625), ('layer3.0.bn2.weight', -1917.516845703125), ('layer3.3.bn3.weight', -2211.27001953125), ('layer3.0.bn1.weight', -2249.62451171875), ('layer4.0.downsample.1.weight', -2272.242431640625), ('layer1.0.downsample.1.weight', -2370.304443359375), ('layer3.5.bn3.weight', -2377.2626953125), ('layer2.0.bn3.weight', -2771.11767578125), ('layer3.2.bn3.weight', -3215.7744140625), ('layer3.1.bn3.weight', -3760.02685546875), ('layer2.0.downsample.1.weight', -4929.58740234375), ('layer4.0.bn2.weight', -6489.912109375), ('layer4.0.bn1.weight', -6645.166015625), ('layer3.0.bn3.weight', -8536.3603515625), ('layer3.0.downsample.1.weight', -10074.42578125), ('layer1.0.conv1.weight', -52729.15625), ('layer1.1.conv1.weight', -113281.484375), ('layer1.1.conv3.weight', -127852.1171875), ('layer1.2.conv1.weight', -150146.125), ('layer1.2.conv3.weight', -160174.265625), ('layer1.0.conv3.weight', -193347.75), ('layer1.1.conv2.weight', -411514.84375), ('layer2.1.conv1.weight', -541307.9375), ('layer1.0.conv2.weight', -572568.25), ('conv1.weight', -761338.1875), ('layer2.2.conv1.weight', -789682.5), ('layer1.2.conv2.weight', -792729.125), ('layer2.3.conv1.weight', -830845.375), ('layer2.3.conv3.weight', -884106.0), ('layer2.2.conv3.weight', -1067409.625), ('layer2.0.conv1.weight', -1077539.0), ('layer2.1.conv3.weight', -1121654.5), ('layer1.0.downsample.0.weight', -1288798.875), ('layer2.0.conv3.weight', -1981604.125), ('layer3.4.conv3.weight', -2139301.0), ('layer3.4.conv1.weight', -2541560.5), ('layer3.5.conv3.weight', -2640074.25), ('layer3.3.conv1.weight', -2667853.25), ('layer3.3.conv3.weight', -2797789.5), ('layer2.1.conv2.weight', -2875951.0), ('layer3.2.conv1.weight', -2958465.5), ('layer2.2.conv2.weight', -3213509.5), ('layer3.5.conv1.weight', -3377089.75), ('layer2.3.conv2.weight', -3448353.5), ('layer3.1.conv1.weight', -3989938.5), ('layer3.2.conv3.weight', -4112360.0), ('layer4.1.conv3.weight', -4582063.5), ('layer3.1.conv3.weight', -5178741.0), ('layer4.2.conv3.weight', -5727319.0), ('layer2.0.conv2.weight', -5811652.0), ('layer4.1.conv1.weight', -6481339.5), ('layer3.4.conv2.weight', -6880740.0), ('layer2.0.downsample.0.weight', -7059234.0), ('layer3.0.conv1.weight', -7844286.0), ('layer3.5.conv2.weight', -8131084.0), ('layer3.3.conv2.weight', -8326159.5), ('layer4.1.conv2.weight', -8842110.0), ('layer4.2.conv1.weight', -9177486.0), ('layer3.2.conv2.weight', -10935930.0), ('layer4.2.conv2.weight', -13822194.0), ('layer3.1.conv2.weight', -14512398.0), ('layer3.0.conv3.weight', -16267724.0), ('last_linear.weight', -25863768.0), ('layer3.0.downsample.0.weight', -35741820.0), ('layer3.0.conv2.weight', -42171656.0), ('layer4.0.conv1.weight', -46753212.0), ('layer4.0.conv3.weight', -99361816.0), ('layer4.0.downsample.0.weight', -194650368.0), ('layer4.0.conv2.weight', -260357248.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('last_linear.bias', 0.0), ('layer1.0.conv1.weight', -52729.15625), ('layer1.1.conv1.weight', -113281.484375), ('layer1.1.conv3.weight', -127852.1171875), ('layer1.2.conv1.weight', -150146.125), ('layer1.2.conv3.weight', -160174.265625), ('layer1.0.conv3.weight', -193347.75), ('layer1.1.conv2.weight', -411514.84375), ('layer2.1.conv1.weight', -541307.9375), ('layer1.0.conv2.weight', -572568.25), ('conv1.weight', -761338.1875), ('layer2.2.conv1.weight', -789682.5), ('layer1.2.conv2.weight', -792729.125), ('layer2.3.conv1.weight', -830845.375), ('layer2.3.conv3.weight', -884106.0), ('layer2.2.conv3.weight', -1067409.625), ('layer2.0.conv1.weight', -1077539.0), ('layer2.1.conv3.weight', -1121654.5), ('layer2.0.conv3.weight', -1981604.125), ('layer3.4.conv3.weight', -2139301.0), ('layer3.4.conv1.weight', -2541560.5), ('layer3.5.conv3.weight', -2640074.25), ('layer3.3.conv1.weight', -2667853.25), ('layer3.3.conv3.weight', -2797789.5), ('layer2.1.conv2.weight', -2875951.0), ('layer3.2.conv1.weight', -2958465.5), ('layer2.2.conv2.weight', -3213509.5), ('layer3.5.conv1.weight', -3377089.75), ('layer2.3.conv2.weight', -3448353.5), ('layer3.1.conv1.weight', -3989938.5), ('layer3.2.conv3.weight', -4112360.0), ('layer4.1.conv3.weight', -4582063.5), ('layer3.1.conv3.weight', -5178741.0), ('layer4.2.conv3.weight', -5727319.0), ('layer2.0.conv2.weight', -5811652.0), ('layer4.1.conv1.weight', -6481339.5), ('layer3.4.conv2.weight', -6880740.0), ('layer3.0.conv1.weight', -7844286.0), ('layer3.5.conv2.weight', -8131084.0), ('layer3.3.conv2.weight', -8326159.5), ('layer4.1.conv2.weight', -8842110.0), ('layer4.2.conv1.weight', -9177486.0), ('layer3.2.conv2.weight', -10935930.0), ('layer4.2.conv2.weight', -13822194.0), ('layer3.1.conv2.weight', -14512398.0), ('layer3.0.conv3.weight', -16267724.0), ('last_linear.weight', -25863768.0), ('layer3.0.conv2.weight', -42171656.0), ('layer4.0.conv1.weight', -46753212.0), ('layer4.0.conv3.weight', -99361816.0), ('layer4.0.conv2.weight', -260357248.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.032275	Acc: 54.8% (5480/10000)
[Test]  Epoch: 2	Loss: 0.032158	Acc: 54.9% (5489/10000)
[Test]  Epoch: 3	Loss: 0.032260	Acc: 55.0% (5505/10000)
[Test]  Epoch: 4	Loss: 0.032231	Acc: 55.3% (5526/10000)
[Test]  Epoch: 5	Loss: 0.032360	Acc: 55.2% (5525/10000)
[Test]  Epoch: 6	Loss: 0.032439	Acc: 55.0% (5504/10000)
[Test]  Epoch: 7	Loss: 0.032411	Acc: 55.2% (5522/10000)
[Test]  Epoch: 8	Loss: 0.032482	Acc: 55.2% (5524/10000)
[Test]  Epoch: 9	Loss: 0.032505	Acc: 55.1% (5514/10000)
[Test]  Epoch: 10	Loss: 0.032562	Acc: 55.2% (5522/10000)
[Test]  Epoch: 11	Loss: 0.032618	Acc: 55.2% (5517/10000)
[Test]  Epoch: 12	Loss: 0.032545	Acc: 55.1% (5513/10000)
[Test]  Epoch: 13	Loss: 0.032652	Acc: 55.2% (5519/10000)
[Test]  Epoch: 14	Loss: 0.032691	Acc: 55.4% (5535/10000)
[Test]  Epoch: 15	Loss: 0.032761	Acc: 55.2% (5524/10000)
[Test]  Epoch: 16	Loss: 0.032713	Acc: 55.1% (5514/10000)
[Test]  Epoch: 17	Loss: 0.032699	Acc: 55.3% (5527/10000)
[Test]  Epoch: 18	Loss: 0.032736	Acc: 55.4% (5544/10000)
[Test]  Epoch: 19	Loss: 0.032780	Acc: 55.2% (5518/10000)
[Test]  Epoch: 20	Loss: 0.032808	Acc: 55.3% (5534/10000)
[Test]  Epoch: 21	Loss: 0.032766	Acc: 55.3% (5528/10000)
[Test]  Epoch: 22	Loss: 0.032837	Acc: 55.2% (5525/10000)
[Test]  Epoch: 23	Loss: 0.032844	Acc: 55.3% (5533/10000)
[Test]  Epoch: 24	Loss: 0.032933	Acc: 55.2% (5523/10000)
[Test]  Epoch: 25	Loss: 0.032846	Acc: 55.3% (5533/10000)
[Test]  Epoch: 26	Loss: 0.032821	Acc: 55.3% (5532/10000)
[Test]  Epoch: 27	Loss: 0.032872	Acc: 55.3% (5530/10000)
[Test]  Epoch: 28	Loss: 0.032970	Acc: 55.2% (5522/10000)
[Test]  Epoch: 29	Loss: 0.032954	Acc: 55.3% (5530/10000)
[Test]  Epoch: 30	Loss: 0.032922	Acc: 55.4% (5536/10000)
[Test]  Epoch: 31	Loss: 0.033063	Acc: 55.2% (5519/10000)
[Test]  Epoch: 32	Loss: 0.033065	Acc: 55.1% (5509/10000)
[Test]  Epoch: 33	Loss: 0.033039	Acc: 55.3% (5534/10000)
[Test]  Epoch: 34	Loss: 0.032959	Acc: 55.4% (5537/10000)
[Test]  Epoch: 35	Loss: 0.033025	Acc: 55.4% (5542/10000)
[Test]  Epoch: 36	Loss: 0.033062	Acc: 55.3% (5532/10000)
[Test]  Epoch: 37	Loss: 0.033093	Acc: 55.3% (5528/10000)
[Test]  Epoch: 38	Loss: 0.033153	Acc: 55.2% (5525/10000)
[Test]  Epoch: 39	Loss: 0.033153	Acc: 55.3% (5527/10000)
[Test]  Epoch: 40	Loss: 0.033177	Acc: 55.3% (5527/10000)
[Test]  Epoch: 41	Loss: 0.033128	Acc: 55.4% (5535/10000)
[Test]  Epoch: 42	Loss: 0.033212	Acc: 55.3% (5528/10000)
[Test]  Epoch: 43	Loss: 0.033197	Acc: 55.3% (5528/10000)
[Test]  Epoch: 44	Loss: 0.033202	Acc: 55.5% (5550/10000)
[Test]  Epoch: 45	Loss: 0.033208	Acc: 55.3% (5534/10000)
[Test]  Epoch: 46	Loss: 0.033271	Acc: 55.2% (5525/10000)
[Test]  Epoch: 47	Loss: 0.033201	Acc: 55.4% (5543/10000)
[Test]  Epoch: 48	Loss: 0.033199	Acc: 55.3% (5534/10000)
[Test]  Epoch: 49	Loss: 0.033259	Acc: 55.2% (5524/10000)
[Test]  Epoch: 50	Loss: 0.033347	Acc: 55.4% (5543/10000)
[Test]  Epoch: 51	Loss: 0.033331	Acc: 55.7% (5567/10000)
[Test]  Epoch: 52	Loss: 0.033294	Acc: 55.4% (5536/10000)
[Test]  Epoch: 53	Loss: 0.033356	Acc: 55.4% (5541/10000)
[Test]  Epoch: 54	Loss: 0.033354	Acc: 55.4% (5542/10000)
[Test]  Epoch: 55	Loss: 0.033344	Acc: 55.5% (5546/10000)
[Test]  Epoch: 56	Loss: 0.033338	Acc: 55.6% (5556/10000)
[Test]  Epoch: 57	Loss: 0.033348	Acc: 55.4% (5543/10000)
[Test]  Epoch: 58	Loss: 0.033341	Acc: 55.5% (5547/10000)
[Test]  Epoch: 59	Loss: 0.033417	Acc: 55.5% (5552/10000)
[Test]  Epoch: 60	Loss: 0.033485	Acc: 55.4% (5542/10000)
[Test]  Epoch: 61	Loss: 0.033494	Acc: 55.5% (5548/10000)
[Test]  Epoch: 62	Loss: 0.033439	Acc: 55.5% (5549/10000)
[Test]  Epoch: 63	Loss: 0.033412	Acc: 55.4% (5544/10000)
[Test]  Epoch: 64	Loss: 0.033434	Acc: 55.4% (5538/10000)
[Test]  Epoch: 65	Loss: 0.033423	Acc: 55.4% (5539/10000)
[Test]  Epoch: 66	Loss: 0.033452	Acc: 55.6% (5560/10000)
[Test]  Epoch: 67	Loss: 0.033413	Acc: 55.6% (5558/10000)
[Test]  Epoch: 68	Loss: 0.033470	Acc: 55.5% (5549/10000)
[Test]  Epoch: 69	Loss: 0.033452	Acc: 55.6% (5558/10000)
[Test]  Epoch: 70	Loss: 0.033389	Acc: 55.5% (5555/10000)
[Test]  Epoch: 71	Loss: 0.033480	Acc: 55.4% (5537/10000)
[Test]  Epoch: 72	Loss: 0.033446	Acc: 55.4% (5541/10000)
[Test]  Epoch: 73	Loss: 0.033375	Acc: 55.4% (5544/10000)
[Test]  Epoch: 74	Loss: 0.033357	Acc: 55.5% (5552/10000)
[Test]  Epoch: 75	Loss: 0.033386	Acc: 55.5% (5555/10000)
[Test]  Epoch: 76	Loss: 0.033343	Acc: 55.5% (5554/10000)
[Test]  Epoch: 77	Loss: 0.033389	Acc: 55.4% (5544/10000)
[Test]  Epoch: 78	Loss: 0.033437	Acc: 55.3% (5529/10000)
[Test]  Epoch: 79	Loss: 0.033428	Acc: 55.4% (5540/10000)
[Test]  Epoch: 80	Loss: 0.033442	Acc: 55.3% (5534/10000)
[Test]  Epoch: 81	Loss: 0.033479	Acc: 55.4% (5537/10000)
[Test]  Epoch: 82	Loss: 0.033497	Acc: 55.3% (5534/10000)
[Test]  Epoch: 83	Loss: 0.033442	Acc: 55.4% (5537/10000)
[Test]  Epoch: 84	Loss: 0.033481	Acc: 55.5% (5548/10000)
[Test]  Epoch: 85	Loss: 0.033443	Acc: 55.5% (5549/10000)
[Test]  Epoch: 86	Loss: 0.033499	Acc: 55.3% (5531/10000)
[Test]  Epoch: 87	Loss: 0.033444	Acc: 55.5% (5545/10000)
[Test]  Epoch: 88	Loss: 0.033443	Acc: 55.4% (5542/10000)
[Test]  Epoch: 89	Loss: 0.033474	Acc: 55.5% (5549/10000)
[Test]  Epoch: 90	Loss: 0.033480	Acc: 55.5% (5549/10000)
[Test]  Epoch: 91	Loss: 0.033469	Acc: 55.5% (5550/10000)
[Test]  Epoch: 92	Loss: 0.033379	Acc: 55.4% (5538/10000)
[Test]  Epoch: 93	Loss: 0.033460	Acc: 55.5% (5549/10000)
[Test]  Epoch: 94	Loss: 0.033443	Acc: 55.5% (5545/10000)
[Test]  Epoch: 95	Loss: 0.033442	Acc: 55.4% (5544/10000)
[Test]  Epoch: 96	Loss: 0.033425	Acc: 55.6% (5556/10000)
[Test]  Epoch: 97	Loss: 0.033463	Acc: 55.4% (5536/10000)
[Test]  Epoch: 98	Loss: 0.033452	Acc: 55.5% (5551/10000)
[Test]  Epoch: 99	Loss: 0.033439	Acc: 55.5% (5545/10000)
[Test]  Epoch: 100	Loss: 0.033430	Acc: 55.5% (5548/10000)
===========finish==========
['2024-08-19', '18:06:47.503540', '100', 'test', '0.03343019089102745', '55.48', '55.67']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.042903	Acc: 37.8% (3775/10000)
[Test]  Epoch: 2	Loss: 0.036856	Acc: 45.7% (4572/10000)
[Test]  Epoch: 3	Loss: 0.034912	Acc: 50.0% (5001/10000)
[Test]  Epoch: 4	Loss: 0.034213	Acc: 51.6% (5156/10000)
[Test]  Epoch: 5	Loss: 0.034244	Acc: 52.3% (5228/10000)
[Test]  Epoch: 6	Loss: 0.034158	Acc: 52.8% (5277/10000)
[Test]  Epoch: 7	Loss: 0.034038	Acc: 53.0% (5295/10000)
[Test]  Epoch: 8	Loss: 0.034040	Acc: 52.8% (5283/10000)
[Test]  Epoch: 9	Loss: 0.034084	Acc: 53.0% (5305/10000)
[Test]  Epoch: 10	Loss: 0.034033	Acc: 53.1% (5315/10000)
[Test]  Epoch: 11	Loss: 0.034098	Acc: 53.3% (5333/10000)
[Test]  Epoch: 12	Loss: 0.033969	Acc: 53.2% (5318/10000)
[Test]  Epoch: 13	Loss: 0.034086	Acc: 53.3% (5332/10000)
[Test]  Epoch: 14	Loss: 0.034063	Acc: 53.4% (5337/10000)
[Test]  Epoch: 15	Loss: 0.034131	Acc: 53.2% (5322/10000)
[Test]  Epoch: 16	Loss: 0.034088	Acc: 53.4% (5335/10000)
[Test]  Epoch: 17	Loss: 0.034074	Acc: 53.8% (5380/10000)
[Test]  Epoch: 18	Loss: 0.034039	Acc: 53.8% (5376/10000)
[Test]  Epoch: 19	Loss: 0.034093	Acc: 53.8% (5379/10000)
[Test]  Epoch: 20	Loss: 0.034156	Acc: 53.6% (5365/10000)
[Test]  Epoch: 21	Loss: 0.034036	Acc: 54.0% (5405/10000)
[Test]  Epoch: 22	Loss: 0.034152	Acc: 53.9% (5392/10000)
[Test]  Epoch: 23	Loss: 0.034174	Acc: 53.6% (5361/10000)
[Test]  Epoch: 24	Loss: 0.034215	Acc: 53.8% (5376/10000)
[Test]  Epoch: 25	Loss: 0.034140	Acc: 53.9% (5389/10000)
[Test]  Epoch: 26	Loss: 0.034098	Acc: 54.1% (5410/10000)
[Test]  Epoch: 27	Loss: 0.034124	Acc: 54.1% (5406/10000)
[Test]  Epoch: 28	Loss: 0.034262	Acc: 53.8% (5384/10000)
[Test]  Epoch: 29	Loss: 0.034173	Acc: 54.0% (5404/10000)
[Test]  Epoch: 30	Loss: 0.034131	Acc: 54.2% (5424/10000)
[Test]  Epoch: 31	Loss: 0.034287	Acc: 53.8% (5381/10000)
[Test]  Epoch: 32	Loss: 0.034269	Acc: 53.8% (5380/10000)
[Test]  Epoch: 33	Loss: 0.034267	Acc: 54.1% (5412/10000)
[Test]  Epoch: 34	Loss: 0.034226	Acc: 54.0% (5401/10000)
[Test]  Epoch: 35	Loss: 0.034223	Acc: 54.3% (5431/10000)
[Test]  Epoch: 36	Loss: 0.034300	Acc: 54.1% (5415/10000)
[Test]  Epoch: 37	Loss: 0.034326	Acc: 54.0% (5404/10000)
[Test]  Epoch: 38	Loss: 0.034374	Acc: 54.1% (5409/10000)
[Test]  Epoch: 39	Loss: 0.034358	Acc: 54.1% (5406/10000)
[Test]  Epoch: 40	Loss: 0.034443	Acc: 53.8% (5383/10000)
[Test]  Epoch: 41	Loss: 0.034300	Acc: 53.9% (5394/10000)
[Test]  Epoch: 42	Loss: 0.034407	Acc: 54.0% (5402/10000)
[Test]  Epoch: 43	Loss: 0.034348	Acc: 54.2% (5418/10000)
[Test]  Epoch: 44	Loss: 0.034391	Acc: 54.0% (5403/10000)
[Test]  Epoch: 45	Loss: 0.034400	Acc: 54.0% (5396/10000)
[Test]  Epoch: 46	Loss: 0.034443	Acc: 54.0% (5400/10000)
[Test]  Epoch: 47	Loss: 0.034410	Acc: 54.2% (5422/10000)
[Test]  Epoch: 48	Loss: 0.034386	Acc: 54.4% (5435/10000)
[Test]  Epoch: 49	Loss: 0.034441	Acc: 54.1% (5414/10000)
[Test]  Epoch: 50	Loss: 0.034538	Acc: 54.1% (5407/10000)
[Test]  Epoch: 51	Loss: 0.034489	Acc: 54.2% (5422/10000)
[Test]  Epoch: 52	Loss: 0.034436	Acc: 54.3% (5430/10000)
[Test]  Epoch: 53	Loss: 0.034521	Acc: 54.2% (5419/10000)
[Test]  Epoch: 54	Loss: 0.034525	Acc: 54.4% (5439/10000)
[Test]  Epoch: 55	Loss: 0.034493	Acc: 54.3% (5432/10000)
[Test]  Epoch: 56	Loss: 0.034522	Acc: 54.3% (5431/10000)
[Test]  Epoch: 57	Loss: 0.034535	Acc: 54.3% (5426/10000)
[Test]  Epoch: 58	Loss: 0.034508	Acc: 54.4% (5439/10000)
[Test]  Epoch: 59	Loss: 0.034637	Acc: 54.1% (5408/10000)
[Test]  Epoch: 60	Loss: 0.034629	Acc: 54.2% (5423/10000)
[Test]  Epoch: 61	Loss: 0.034648	Acc: 54.0% (5400/10000)
[Test]  Epoch: 62	Loss: 0.034602	Acc: 54.1% (5410/10000)
[Test]  Epoch: 63	Loss: 0.034576	Acc: 54.2% (5419/10000)
[Test]  Epoch: 64	Loss: 0.034555	Acc: 54.2% (5425/10000)
[Test]  Epoch: 65	Loss: 0.034597	Acc: 54.1% (5415/10000)
[Test]  Epoch: 66	Loss: 0.034622	Acc: 54.1% (5414/10000)
[Test]  Epoch: 67	Loss: 0.034570	Acc: 54.3% (5426/10000)
[Test]  Epoch: 68	Loss: 0.034609	Acc: 54.0% (5399/10000)
[Test]  Epoch: 69	Loss: 0.034604	Acc: 54.1% (5413/10000)
[Test]  Epoch: 70	Loss: 0.034514	Acc: 54.2% (5417/10000)
[Test]  Epoch: 71	Loss: 0.034649	Acc: 54.2% (5416/10000)
[Test]  Epoch: 72	Loss: 0.034624	Acc: 54.2% (5419/10000)
[Test]  Epoch: 73	Loss: 0.034561	Acc: 54.4% (5438/10000)
[Test]  Epoch: 74	Loss: 0.034522	Acc: 54.4% (5444/10000)
[Test]  Epoch: 75	Loss: 0.034575	Acc: 54.4% (5435/10000)
[Test]  Epoch: 76	Loss: 0.034529	Acc: 54.4% (5441/10000)
[Test]  Epoch: 77	Loss: 0.034552	Acc: 54.3% (5431/10000)
[Test]  Epoch: 78	Loss: 0.034565	Acc: 54.1% (5410/10000)
[Test]  Epoch: 79	Loss: 0.034587	Acc: 54.2% (5421/10000)
[Test]  Epoch: 80	Loss: 0.034618	Acc: 54.3% (5430/10000)
[Test]  Epoch: 81	Loss: 0.034634	Acc: 54.2% (5423/10000)
[Test]  Epoch: 82	Loss: 0.034641	Acc: 54.1% (5411/10000)
[Test]  Epoch: 83	Loss: 0.034573	Acc: 54.1% (5410/10000)
[Test]  Epoch: 84	Loss: 0.034644	Acc: 54.1% (5414/10000)
[Test]  Epoch: 85	Loss: 0.034605	Acc: 54.1% (5415/10000)
[Test]  Epoch: 86	Loss: 0.034671	Acc: 54.1% (5410/10000)
[Test]  Epoch: 87	Loss: 0.034606	Acc: 54.2% (5419/10000)
[Test]  Epoch: 88	Loss: 0.034577	Acc: 54.1% (5411/10000)
[Test]  Epoch: 89	Loss: 0.034623	Acc: 54.2% (5421/10000)
[Test]  Epoch: 90	Loss: 0.034644	Acc: 54.3% (5429/10000)
[Test]  Epoch: 91	Loss: 0.034611	Acc: 54.5% (5445/10000)
[Test]  Epoch: 92	Loss: 0.034512	Acc: 54.5% (5449/10000)
[Test]  Epoch: 93	Loss: 0.034592	Acc: 54.3% (5430/10000)
[Test]  Epoch: 94	Loss: 0.034602	Acc: 54.3% (5433/10000)
[Test]  Epoch: 95	Loss: 0.034580	Acc: 54.2% (5420/10000)
[Test]  Epoch: 96	Loss: 0.034581	Acc: 54.2% (5423/10000)
[Test]  Epoch: 97	Loss: 0.034617	Acc: 54.2% (5419/10000)
[Test]  Epoch: 98	Loss: 0.034609	Acc: 54.2% (5417/10000)
[Test]  Epoch: 99	Loss: 0.034566	Acc: 54.2% (5419/10000)
[Test]  Epoch: 100	Loss: 0.034581	Acc: 54.3% (5427/10000)
===========finish==========
['2024-08-19', '18:14:18.842060', '100', 'test', '0.03458103557229042', '54.27', '54.49']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039489	Acc: 44.1% (4415/10000)
[Test]  Epoch: 2	Loss: 0.034959	Acc: 50.4% (5039/10000)
[Test]  Epoch: 3	Loss: 0.033881	Acc: 52.8% (5278/10000)
[Test]  Epoch: 4	Loss: 0.033721	Acc: 53.1% (5314/10000)
[Test]  Epoch: 5	Loss: 0.033838	Acc: 53.4% (5338/10000)
[Test]  Epoch: 6	Loss: 0.033844	Acc: 53.6% (5363/10000)
[Test]  Epoch: 7	Loss: 0.033777	Acc: 53.8% (5377/10000)
[Test]  Epoch: 8	Loss: 0.033796	Acc: 53.8% (5378/10000)
[Test]  Epoch: 9	Loss: 0.033852	Acc: 53.8% (5377/10000)
[Test]  Epoch: 10	Loss: 0.033852	Acc: 53.9% (5385/10000)
[Test]  Epoch: 11	Loss: 0.033895	Acc: 54.1% (5411/10000)
[Test]  Epoch: 12	Loss: 0.033774	Acc: 54.2% (5416/10000)
[Test]  Epoch: 13	Loss: 0.033894	Acc: 53.9% (5387/10000)
[Test]  Epoch: 14	Loss: 0.033875	Acc: 53.9% (5394/10000)
[Test]  Epoch: 15	Loss: 0.033927	Acc: 54.1% (5413/10000)
[Test]  Epoch: 16	Loss: 0.033902	Acc: 53.8% (5381/10000)
[Test]  Epoch: 17	Loss: 0.033879	Acc: 54.1% (5406/10000)
[Test]  Epoch: 18	Loss: 0.033889	Acc: 54.1% (5410/10000)
[Test]  Epoch: 19	Loss: 0.033956	Acc: 54.1% (5409/10000)
[Test]  Epoch: 20	Loss: 0.033973	Acc: 54.3% (5430/10000)
[Test]  Epoch: 21	Loss: 0.033871	Acc: 54.5% (5453/10000)
[Test]  Epoch: 22	Loss: 0.033998	Acc: 54.4% (5440/10000)
[Test]  Epoch: 23	Loss: 0.033951	Acc: 54.1% (5415/10000)
[Test]  Epoch: 24	Loss: 0.034055	Acc: 54.2% (5420/10000)
[Test]  Epoch: 25	Loss: 0.033964	Acc: 54.3% (5433/10000)
[Test]  Epoch: 26	Loss: 0.033973	Acc: 54.3% (5426/10000)
[Test]  Epoch: 27	Loss: 0.033951	Acc: 54.1% (5415/10000)
[Test]  Epoch: 28	Loss: 0.034081	Acc: 54.3% (5428/10000)
[Test]  Epoch: 29	Loss: 0.034029	Acc: 54.3% (5432/10000)
[Test]  Epoch: 30	Loss: 0.034040	Acc: 54.5% (5452/10000)
[Test]  Epoch: 31	Loss: 0.034148	Acc: 54.2% (5420/10000)
[Test]  Epoch: 32	Loss: 0.034148	Acc: 54.3% (5434/10000)
[Test]  Epoch: 33	Loss: 0.034174	Acc: 54.3% (5430/10000)
[Test]  Epoch: 34	Loss: 0.034088	Acc: 54.3% (5433/10000)
[Test]  Epoch: 35	Loss: 0.034102	Acc: 54.6% (5459/10000)
[Test]  Epoch: 36	Loss: 0.034132	Acc: 54.6% (5465/10000)
[Test]  Epoch: 37	Loss: 0.034204	Acc: 54.5% (5452/10000)
[Test]  Epoch: 38	Loss: 0.034220	Acc: 54.2% (5422/10000)
[Test]  Epoch: 39	Loss: 0.034233	Acc: 54.3% (5432/10000)
[Test]  Epoch: 40	Loss: 0.034241	Acc: 54.3% (5433/10000)
[Test]  Epoch: 41	Loss: 0.034162	Acc: 54.5% (5449/10000)
[Test]  Epoch: 42	Loss: 0.034281	Acc: 54.5% (5450/10000)
[Test]  Epoch: 43	Loss: 0.034230	Acc: 54.5% (5454/10000)
[Test]  Epoch: 44	Loss: 0.034241	Acc: 54.5% (5450/10000)
[Test]  Epoch: 45	Loss: 0.034246	Acc: 54.6% (5459/10000)
[Test]  Epoch: 46	Loss: 0.034384	Acc: 54.4% (5440/10000)
[Test]  Epoch: 47	Loss: 0.034288	Acc: 54.8% (5476/10000)
[Test]  Epoch: 48	Loss: 0.034233	Acc: 54.7% (5468/10000)
[Test]  Epoch: 49	Loss: 0.034327	Acc: 54.4% (5444/10000)
[Test]  Epoch: 50	Loss: 0.034407	Acc: 54.5% (5451/10000)
[Test]  Epoch: 51	Loss: 0.034373	Acc: 54.5% (5452/10000)
[Test]  Epoch: 52	Loss: 0.034339	Acc: 54.5% (5452/10000)
[Test]  Epoch: 53	Loss: 0.034394	Acc: 54.7% (5472/10000)
[Test]  Epoch: 54	Loss: 0.034374	Acc: 54.6% (5462/10000)
[Test]  Epoch: 55	Loss: 0.034378	Acc: 54.6% (5456/10000)
[Test]  Epoch: 56	Loss: 0.034396	Acc: 54.6% (5460/10000)
[Test]  Epoch: 57	Loss: 0.034400	Acc: 54.6% (5464/10000)
[Test]  Epoch: 58	Loss: 0.034370	Acc: 54.6% (5465/10000)
[Test]  Epoch: 59	Loss: 0.034446	Acc: 54.4% (5443/10000)
[Test]  Epoch: 60	Loss: 0.034544	Acc: 54.2% (5423/10000)
[Test]  Epoch: 61	Loss: 0.034558	Acc: 54.4% (5435/10000)
[Test]  Epoch: 62	Loss: 0.034524	Acc: 54.6% (5461/10000)
[Test]  Epoch: 63	Loss: 0.034486	Acc: 54.6% (5457/10000)
[Test]  Epoch: 64	Loss: 0.034505	Acc: 54.6% (5460/10000)
[Test]  Epoch: 65	Loss: 0.034481	Acc: 54.5% (5449/10000)
[Test]  Epoch: 66	Loss: 0.034499	Acc: 54.6% (5456/10000)
[Test]  Epoch: 67	Loss: 0.034466	Acc: 54.7% (5466/10000)
[Test]  Epoch: 68	Loss: 0.034533	Acc: 54.4% (5443/10000)
[Test]  Epoch: 69	Loss: 0.034510	Acc: 54.5% (5453/10000)
[Test]  Epoch: 70	Loss: 0.034432	Acc: 54.7% (5471/10000)
[Test]  Epoch: 71	Loss: 0.034550	Acc: 54.6% (5456/10000)
[Test]  Epoch: 72	Loss: 0.034496	Acc: 54.4% (5444/10000)
[Test]  Epoch: 73	Loss: 0.034424	Acc: 54.6% (5456/10000)
[Test]  Epoch: 74	Loss: 0.034399	Acc: 54.7% (5473/10000)
[Test]  Epoch: 75	Loss: 0.034443	Acc: 54.8% (5477/10000)
[Test]  Epoch: 76	Loss: 0.034393	Acc: 54.7% (5469/10000)
[Test]  Epoch: 77	Loss: 0.034448	Acc: 54.5% (5455/10000)
[Test]  Epoch: 78	Loss: 0.034467	Acc: 54.5% (5454/10000)
[Test]  Epoch: 79	Loss: 0.034459	Acc: 54.6% (5462/10000)
[Test]  Epoch: 80	Loss: 0.034480	Acc: 54.5% (5451/10000)
[Test]  Epoch: 81	Loss: 0.034502	Acc: 54.6% (5464/10000)
[Test]  Epoch: 82	Loss: 0.034508	Acc: 54.4% (5435/10000)
[Test]  Epoch: 83	Loss: 0.034447	Acc: 54.4% (5436/10000)
[Test]  Epoch: 84	Loss: 0.034514	Acc: 54.6% (5461/10000)
[Test]  Epoch: 85	Loss: 0.034500	Acc: 54.5% (5453/10000)
[Test]  Epoch: 86	Loss: 0.034542	Acc: 54.4% (5438/10000)
[Test]  Epoch: 87	Loss: 0.034469	Acc: 54.5% (5451/10000)
[Test]  Epoch: 88	Loss: 0.034444	Acc: 54.6% (5460/10000)
[Test]  Epoch: 89	Loss: 0.034479	Acc: 54.4% (5444/10000)
[Test]  Epoch: 90	Loss: 0.034524	Acc: 54.6% (5458/10000)
[Test]  Epoch: 91	Loss: 0.034490	Acc: 54.5% (5455/10000)
[Test]  Epoch: 92	Loss: 0.034375	Acc: 54.7% (5473/10000)
[Test]  Epoch: 93	Loss: 0.034473	Acc: 54.7% (5473/10000)
[Test]  Epoch: 94	Loss: 0.034485	Acc: 54.5% (5452/10000)
[Test]  Epoch: 95	Loss: 0.034451	Acc: 54.5% (5449/10000)
[Test]  Epoch: 96	Loss: 0.034427	Acc: 54.6% (5464/10000)
[Test]  Epoch: 97	Loss: 0.034476	Acc: 54.5% (5451/10000)
[Test]  Epoch: 98	Loss: 0.034469	Acc: 54.5% (5455/10000)
[Test]  Epoch: 99	Loss: 0.034446	Acc: 54.6% (5457/10000)
[Test]  Epoch: 100	Loss: 0.034437	Acc: 54.5% (5454/10000)
===========finish==========
['2024-08-19', '18:21:30.522654', '100', 'test', '0.034436953127384186', '54.54', '54.77']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.043405	Acc: 39.9% (3987/10000)
[Test]  Epoch: 2	Loss: 0.036241	Acc: 48.8% (4880/10000)
[Test]  Epoch: 3	Loss: 0.034368	Acc: 51.7% (5174/10000)
[Test]  Epoch: 4	Loss: 0.033974	Acc: 52.3% (5233/10000)
[Test]  Epoch: 5	Loss: 0.034066	Acc: 53.2% (5317/10000)
[Test]  Epoch: 6	Loss: 0.034030	Acc: 53.3% (5328/10000)
[Test]  Epoch: 7	Loss: 0.034014	Acc: 53.5% (5355/10000)
[Test]  Epoch: 8	Loss: 0.034019	Acc: 53.3% (5333/10000)
[Test]  Epoch: 9	Loss: 0.034036	Acc: 53.4% (5341/10000)
[Test]  Epoch: 10	Loss: 0.034004	Acc: 53.6% (5360/10000)
[Test]  Epoch: 11	Loss: 0.034076	Acc: 53.8% (5383/10000)
[Test]  Epoch: 12	Loss: 0.033936	Acc: 53.9% (5391/10000)
[Test]  Epoch: 13	Loss: 0.034047	Acc: 53.8% (5376/10000)
[Test]  Epoch: 14	Loss: 0.034051	Acc: 53.7% (5373/10000)
[Test]  Epoch: 15	Loss: 0.034094	Acc: 54.0% (5395/10000)
[Test]  Epoch: 16	Loss: 0.034157	Acc: 53.6% (5359/10000)
[Test]  Epoch: 17	Loss: 0.034062	Acc: 54.2% (5420/10000)
[Test]  Epoch: 18	Loss: 0.034172	Acc: 53.9% (5390/10000)
[Test]  Epoch: 19	Loss: 0.034182	Acc: 54.0% (5397/10000)
[Test]  Epoch: 20	Loss: 0.034159	Acc: 54.1% (5410/10000)
[Test]  Epoch: 21	Loss: 0.034084	Acc: 54.2% (5425/10000)
[Test]  Epoch: 22	Loss: 0.034210	Acc: 54.0% (5402/10000)
[Test]  Epoch: 23	Loss: 0.034154	Acc: 54.0% (5402/10000)
[Test]  Epoch: 24	Loss: 0.034284	Acc: 54.1% (5408/10000)
[Test]  Epoch: 25	Loss: 0.034186	Acc: 54.1% (5408/10000)
[Test]  Epoch: 26	Loss: 0.034173	Acc: 54.4% (5436/10000)
[Test]  Epoch: 27	Loss: 0.034186	Acc: 54.1% (5412/10000)
[Test]  Epoch: 28	Loss: 0.034275	Acc: 54.1% (5406/10000)
[Test]  Epoch: 29	Loss: 0.034204	Acc: 54.2% (5416/10000)
[Test]  Epoch: 30	Loss: 0.034231	Acc: 54.6% (5456/10000)
[Test]  Epoch: 31	Loss: 0.034414	Acc: 54.0% (5396/10000)
[Test]  Epoch: 32	Loss: 0.034356	Acc: 54.0% (5403/10000)
[Test]  Epoch: 33	Loss: 0.034377	Acc: 54.1% (5407/10000)
[Test]  Epoch: 34	Loss: 0.034283	Acc: 54.1% (5410/10000)
[Test]  Epoch: 35	Loss: 0.034299	Acc: 54.3% (5430/10000)
[Test]  Epoch: 36	Loss: 0.034387	Acc: 54.2% (5421/10000)
[Test]  Epoch: 37	Loss: 0.034413	Acc: 54.2% (5418/10000)
[Test]  Epoch: 38	Loss: 0.034448	Acc: 53.9% (5394/10000)
[Test]  Epoch: 39	Loss: 0.034465	Acc: 54.2% (5418/10000)
[Test]  Epoch: 40	Loss: 0.034459	Acc: 54.0% (5403/10000)
[Test]  Epoch: 41	Loss: 0.034384	Acc: 54.3% (5426/10000)
[Test]  Epoch: 42	Loss: 0.034494	Acc: 54.2% (5419/10000)
[Test]  Epoch: 43	Loss: 0.034476	Acc: 54.2% (5425/10000)
[Test]  Epoch: 44	Loss: 0.034497	Acc: 54.2% (5422/10000)
[Test]  Epoch: 45	Loss: 0.034473	Acc: 54.3% (5432/10000)
[Test]  Epoch: 46	Loss: 0.034543	Acc: 54.0% (5401/10000)
[Test]  Epoch: 47	Loss: 0.034506	Acc: 54.2% (5424/10000)
[Test]  Epoch: 48	Loss: 0.034451	Acc: 54.4% (5439/10000)
[Test]  Epoch: 49	Loss: 0.034547	Acc: 54.2% (5419/10000)
[Test]  Epoch: 50	Loss: 0.034631	Acc: 54.2% (5424/10000)
[Test]  Epoch: 51	Loss: 0.034593	Acc: 54.5% (5445/10000)
[Test]  Epoch: 52	Loss: 0.034555	Acc: 54.2% (5417/10000)
[Test]  Epoch: 53	Loss: 0.034655	Acc: 54.1% (5411/10000)
[Test]  Epoch: 54	Loss: 0.034642	Acc: 54.3% (5433/10000)
[Test]  Epoch: 55	Loss: 0.034643	Acc: 54.1% (5414/10000)
[Test]  Epoch: 56	Loss: 0.034655	Acc: 54.1% (5413/10000)
[Test]  Epoch: 57	Loss: 0.034643	Acc: 54.3% (5430/10000)
[Test]  Epoch: 58	Loss: 0.034615	Acc: 54.4% (5441/10000)
[Test]  Epoch: 59	Loss: 0.034684	Acc: 54.3% (5431/10000)
[Test]  Epoch: 60	Loss: 0.034749	Acc: 54.1% (5415/10000)
[Test]  Epoch: 61	Loss: 0.034762	Acc: 54.4% (5440/10000)
[Test]  Epoch: 62	Loss: 0.034719	Acc: 54.4% (5438/10000)
[Test]  Epoch: 63	Loss: 0.034683	Acc: 54.2% (5422/10000)
[Test]  Epoch: 64	Loss: 0.034696	Acc: 54.5% (5446/10000)
[Test]  Epoch: 65	Loss: 0.034688	Acc: 54.1% (5414/10000)
[Test]  Epoch: 66	Loss: 0.034712	Acc: 54.4% (5439/10000)
[Test]  Epoch: 67	Loss: 0.034671	Acc: 54.4% (5438/10000)
[Test]  Epoch: 68	Loss: 0.034753	Acc: 54.3% (5434/10000)
[Test]  Epoch: 69	Loss: 0.034713	Acc: 54.5% (5448/10000)
[Test]  Epoch: 70	Loss: 0.034658	Acc: 54.3% (5433/10000)
[Test]  Epoch: 71	Loss: 0.034755	Acc: 54.3% (5428/10000)
[Test]  Epoch: 72	Loss: 0.034715	Acc: 54.5% (5445/10000)
[Test]  Epoch: 73	Loss: 0.034642	Acc: 54.3% (5434/10000)
[Test]  Epoch: 74	Loss: 0.034628	Acc: 54.3% (5429/10000)
[Test]  Epoch: 75	Loss: 0.034653	Acc: 54.3% (5427/10000)
[Test]  Epoch: 76	Loss: 0.034618	Acc: 54.5% (5445/10000)
[Test]  Epoch: 77	Loss: 0.034665	Acc: 54.4% (5440/10000)
[Test]  Epoch: 78	Loss: 0.034682	Acc: 54.4% (5436/10000)
[Test]  Epoch: 79	Loss: 0.034682	Acc: 54.3% (5431/10000)
[Test]  Epoch: 80	Loss: 0.034712	Acc: 54.2% (5417/10000)
[Test]  Epoch: 81	Loss: 0.034741	Acc: 54.2% (5421/10000)
[Test]  Epoch: 82	Loss: 0.034743	Acc: 54.1% (5413/10000)
[Test]  Epoch: 83	Loss: 0.034691	Acc: 54.3% (5426/10000)
[Test]  Epoch: 84	Loss: 0.034725	Acc: 54.3% (5430/10000)
[Test]  Epoch: 85	Loss: 0.034714	Acc: 54.4% (5435/10000)
[Test]  Epoch: 86	Loss: 0.034741	Acc: 54.2% (5425/10000)
[Test]  Epoch: 87	Loss: 0.034704	Acc: 54.3% (5433/10000)
[Test]  Epoch: 88	Loss: 0.034694	Acc: 54.3% (5430/10000)
[Test]  Epoch: 89	Loss: 0.034718	Acc: 54.3% (5433/10000)
[Test]  Epoch: 90	Loss: 0.034744	Acc: 54.3% (5430/10000)
[Test]  Epoch: 91	Loss: 0.034710	Acc: 54.4% (5436/10000)
[Test]  Epoch: 92	Loss: 0.034636	Acc: 54.4% (5439/10000)
[Test]  Epoch: 93	Loss: 0.034713	Acc: 54.3% (5429/10000)
[Test]  Epoch: 94	Loss: 0.034688	Acc: 54.4% (5441/10000)
[Test]  Epoch: 95	Loss: 0.034689	Acc: 54.2% (5423/10000)
[Test]  Epoch: 96	Loss: 0.034643	Acc: 54.3% (5434/10000)
[Test]  Epoch: 97	Loss: 0.034698	Acc: 54.2% (5421/10000)
[Test]  Epoch: 98	Loss: 0.034692	Acc: 54.3% (5426/10000)
[Test]  Epoch: 99	Loss: 0.034690	Acc: 54.2% (5419/10000)
[Test]  Epoch: 100	Loss: 0.034676	Acc: 54.2% (5425/10000)
===========finish==========
['2024-08-19', '18:28:47.211261', '100', 'test', '0.03467572048902512', '54.25', '54.56']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.054623	Acc: 27.8% (2779/10000)
[Test]  Epoch: 2	Loss: 0.039846	Acc: 43.9% (4394/10000)
[Test]  Epoch: 3	Loss: 0.036430	Acc: 48.1% (4812/10000)
[Test]  Epoch: 4	Loss: 0.035778	Acc: 49.5% (4951/10000)
[Test]  Epoch: 5	Loss: 0.036148	Acc: 49.7% (4970/10000)
[Test]  Epoch: 6	Loss: 0.035689	Acc: 50.1% (5015/10000)
[Test]  Epoch: 7	Loss: 0.035568	Acc: 50.7% (5069/10000)
[Test]  Epoch: 8	Loss: 0.035578	Acc: 50.8% (5079/10000)
[Test]  Epoch: 9	Loss: 0.035622	Acc: 51.1% (5106/10000)
[Test]  Epoch: 10	Loss: 0.035449	Acc: 51.4% (5138/10000)
[Test]  Epoch: 11	Loss: 0.035618	Acc: 51.1% (5114/10000)
[Test]  Epoch: 12	Loss: 0.035520	Acc: 51.3% (5126/10000)
[Test]  Epoch: 13	Loss: 0.035597	Acc: 51.1% (5115/10000)
[Test]  Epoch: 14	Loss: 0.035436	Acc: 51.7% (5173/10000)
[Test]  Epoch: 15	Loss: 0.035604	Acc: 51.5% (5145/10000)
[Test]  Epoch: 16	Loss: 0.035462	Acc: 51.6% (5156/10000)
[Test]  Epoch: 17	Loss: 0.035414	Acc: 51.6% (5163/10000)
[Test]  Epoch: 18	Loss: 0.035642	Acc: 51.5% (5149/10000)
[Test]  Epoch: 19	Loss: 0.035497	Acc: 52.2% (5216/10000)
[Test]  Epoch: 20	Loss: 0.035544	Acc: 52.1% (5209/10000)
[Test]  Epoch: 21	Loss: 0.035522	Acc: 52.0% (5199/10000)
[Test]  Epoch: 22	Loss: 0.035729	Acc: 52.0% (5195/10000)
[Test]  Epoch: 23	Loss: 0.035580	Acc: 52.2% (5223/10000)
[Test]  Epoch: 24	Loss: 0.035740	Acc: 52.0% (5202/10000)
[Test]  Epoch: 25	Loss: 0.035565	Acc: 52.0% (5195/10000)
[Test]  Epoch: 26	Loss: 0.035641	Acc: 52.1% (5210/10000)
[Test]  Epoch: 27	Loss: 0.035556	Acc: 52.0% (5199/10000)
[Test]  Epoch: 28	Loss: 0.035634	Acc: 52.1% (5214/10000)
[Test]  Epoch: 29	Loss: 0.035612	Acc: 52.5% (5251/10000)
[Test]  Epoch: 30	Loss: 0.035703	Acc: 52.3% (5231/10000)
[Test]  Epoch: 31	Loss: 0.035777	Acc: 52.2% (5221/10000)
[Test]  Epoch: 32	Loss: 0.035768	Acc: 52.2% (5218/10000)
[Test]  Epoch: 33	Loss: 0.035797	Acc: 52.3% (5226/10000)
[Test]  Epoch: 34	Loss: 0.035656	Acc: 52.3% (5228/10000)
[Test]  Epoch: 35	Loss: 0.035647	Acc: 52.5% (5250/10000)
[Test]  Epoch: 36	Loss: 0.035795	Acc: 52.3% (5230/10000)
[Test]  Epoch: 37	Loss: 0.035772	Acc: 52.5% (5251/10000)
[Test]  Epoch: 38	Loss: 0.035892	Acc: 52.4% (5235/10000)
[Test]  Epoch: 39	Loss: 0.035771	Acc: 52.4% (5237/10000)
[Test]  Epoch: 40	Loss: 0.035857	Acc: 52.2% (5219/10000)
[Test]  Epoch: 41	Loss: 0.035827	Acc: 52.3% (5231/10000)
[Test]  Epoch: 42	Loss: 0.035916	Acc: 52.3% (5228/10000)
[Test]  Epoch: 43	Loss: 0.035905	Acc: 52.5% (5252/10000)
[Test]  Epoch: 44	Loss: 0.035930	Acc: 52.5% (5251/10000)
[Test]  Epoch: 45	Loss: 0.035866	Acc: 52.3% (5234/10000)
[Test]  Epoch: 46	Loss: 0.035965	Acc: 52.2% (5222/10000)
[Test]  Epoch: 47	Loss: 0.035869	Acc: 52.4% (5238/10000)
[Test]  Epoch: 48	Loss: 0.035864	Acc: 52.5% (5250/10000)
[Test]  Epoch: 49	Loss: 0.035945	Acc: 52.4% (5244/10000)
[Test]  Epoch: 50	Loss: 0.036026	Acc: 52.4% (5242/10000)
[Test]  Epoch: 51	Loss: 0.035969	Acc: 52.3% (5226/10000)
[Test]  Epoch: 52	Loss: 0.035926	Acc: 52.7% (5269/10000)
[Test]  Epoch: 53	Loss: 0.036091	Acc: 52.3% (5231/10000)
[Test]  Epoch: 54	Loss: 0.036017	Acc: 52.5% (5249/10000)
[Test]  Epoch: 55	Loss: 0.035966	Acc: 52.5% (5252/10000)
[Test]  Epoch: 56	Loss: 0.036017	Acc: 52.3% (5230/10000)
[Test]  Epoch: 57	Loss: 0.036007	Acc: 52.6% (5256/10000)
[Test]  Epoch: 58	Loss: 0.035982	Acc: 52.8% (5276/10000)
[Test]  Epoch: 59	Loss: 0.036089	Acc: 52.6% (5256/10000)
[Test]  Epoch: 60	Loss: 0.036170	Acc: 52.5% (5255/10000)
[Test]  Epoch: 61	Loss: 0.036178	Acc: 52.4% (5238/10000)
[Test]  Epoch: 62	Loss: 0.036136	Acc: 52.5% (5248/10000)
[Test]  Epoch: 63	Loss: 0.036106	Acc: 52.6% (5258/10000)
[Test]  Epoch: 64	Loss: 0.036095	Acc: 52.5% (5253/10000)
[Test]  Epoch: 65	Loss: 0.036065	Acc: 52.4% (5241/10000)
[Test]  Epoch: 66	Loss: 0.036079	Acc: 52.5% (5250/10000)
[Test]  Epoch: 67	Loss: 0.036065	Acc: 52.7% (5266/10000)
[Test]  Epoch: 68	Loss: 0.036111	Acc: 52.5% (5251/10000)
[Test]  Epoch: 69	Loss: 0.036128	Acc: 52.5% (5250/10000)
[Test]  Epoch: 70	Loss: 0.036064	Acc: 52.7% (5270/10000)
[Test]  Epoch: 71	Loss: 0.036164	Acc: 52.5% (5251/10000)
[Test]  Epoch: 72	Loss: 0.036131	Acc: 52.5% (5255/10000)
[Test]  Epoch: 73	Loss: 0.036061	Acc: 52.6% (5263/10000)
[Test]  Epoch: 74	Loss: 0.036026	Acc: 52.6% (5265/10000)
[Test]  Epoch: 75	Loss: 0.036040	Acc: 52.6% (5259/10000)
[Test]  Epoch: 76	Loss: 0.036000	Acc: 52.6% (5264/10000)
[Test]  Epoch: 77	Loss: 0.036080	Acc: 52.7% (5269/10000)
[Test]  Epoch: 78	Loss: 0.036096	Acc: 52.6% (5265/10000)
[Test]  Epoch: 79	Loss: 0.036128	Acc: 52.7% (5272/10000)
[Test]  Epoch: 80	Loss: 0.036115	Acc: 52.5% (5251/10000)
[Test]  Epoch: 81	Loss: 0.036106	Acc: 52.6% (5258/10000)
[Test]  Epoch: 82	Loss: 0.036099	Acc: 52.5% (5253/10000)
[Test]  Epoch: 83	Loss: 0.036089	Acc: 52.5% (5255/10000)
[Test]  Epoch: 84	Loss: 0.036130	Acc: 52.5% (5250/10000)
[Test]  Epoch: 85	Loss: 0.036143	Acc: 52.5% (5254/10000)
[Test]  Epoch: 86	Loss: 0.036174	Acc: 52.5% (5252/10000)
[Test]  Epoch: 87	Loss: 0.036130	Acc: 52.7% (5268/10000)
[Test]  Epoch: 88	Loss: 0.036147	Acc: 52.6% (5257/10000)
[Test]  Epoch: 89	Loss: 0.036168	Acc: 52.5% (5253/10000)
[Test]  Epoch: 90	Loss: 0.036145	Acc: 52.5% (5251/10000)
[Test]  Epoch: 91	Loss: 0.036123	Acc: 52.7% (5273/10000)
[Test]  Epoch: 92	Loss: 0.036056	Acc: 52.8% (5275/10000)
[Test]  Epoch: 93	Loss: 0.036149	Acc: 52.6% (5256/10000)
[Test]  Epoch: 94	Loss: 0.036158	Acc: 52.5% (5247/10000)
[Test]  Epoch: 95	Loss: 0.036143	Acc: 52.6% (5262/10000)
[Test]  Epoch: 96	Loss: 0.036059	Acc: 52.6% (5261/10000)
[Test]  Epoch: 97	Loss: 0.036126	Acc: 52.7% (5266/10000)
[Test]  Epoch: 98	Loss: 0.036106	Acc: 52.6% (5259/10000)
[Test]  Epoch: 99	Loss: 0.036116	Acc: 52.5% (5253/10000)
[Test]  Epoch: 100	Loss: 0.036081	Acc: 52.6% (5259/10000)
===========finish==========
['2024-08-19', '18:35:56.469475', '100', 'test', '0.03608112466335297', '52.59', '52.76']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.061752	Acc: 26.0% (2598/10000)
[Test]  Epoch: 2	Loss: 0.039647	Acc: 43.0% (4295/10000)
[Test]  Epoch: 3	Loss: 0.036874	Acc: 46.7% (4674/10000)
[Test]  Epoch: 4	Loss: 0.036415	Acc: 48.1% (4811/10000)
[Test]  Epoch: 5	Loss: 0.036401	Acc: 48.5% (4853/10000)
[Test]  Epoch: 6	Loss: 0.036175	Acc: 49.1% (4912/10000)
[Test]  Epoch: 7	Loss: 0.036040	Acc: 49.1% (4911/10000)
[Test]  Epoch: 8	Loss: 0.036062	Acc: 49.5% (4946/10000)
[Test]  Epoch: 9	Loss: 0.036083	Acc: 49.7% (4970/10000)
[Test]  Epoch: 10	Loss: 0.035966	Acc: 49.9% (4994/10000)
[Test]  Epoch: 11	Loss: 0.036119	Acc: 49.9% (4994/10000)
[Test]  Epoch: 12	Loss: 0.035958	Acc: 50.2% (5025/10000)
[Test]  Epoch: 13	Loss: 0.036049	Acc: 50.3% (5031/10000)
[Test]  Epoch: 14	Loss: 0.035940	Acc: 50.5% (5049/10000)
[Test]  Epoch: 15	Loss: 0.036129	Acc: 50.3% (5033/10000)
[Test]  Epoch: 16	Loss: 0.035961	Acc: 50.8% (5082/10000)
[Test]  Epoch: 17	Loss: 0.035970	Acc: 50.6% (5058/10000)
[Test]  Epoch: 18	Loss: 0.036187	Acc: 50.7% (5068/10000)
[Test]  Epoch: 19	Loss: 0.036014	Acc: 50.9% (5094/10000)
[Test]  Epoch: 20	Loss: 0.036127	Acc: 50.8% (5083/10000)
[Test]  Epoch: 21	Loss: 0.036067	Acc: 51.1% (5109/10000)
[Test]  Epoch: 22	Loss: 0.036220	Acc: 51.1% (5114/10000)
[Test]  Epoch: 23	Loss: 0.036126	Acc: 51.0% (5098/10000)
[Test]  Epoch: 24	Loss: 0.036288	Acc: 50.9% (5092/10000)
[Test]  Epoch: 25	Loss: 0.036145	Acc: 51.2% (5117/10000)
[Test]  Epoch: 26	Loss: 0.036161	Acc: 51.5% (5147/10000)
[Test]  Epoch: 27	Loss: 0.036064	Acc: 51.4% (5137/10000)
[Test]  Epoch: 28	Loss: 0.036279	Acc: 51.0% (5104/10000)
[Test]  Epoch: 29	Loss: 0.036179	Acc: 51.2% (5118/10000)
[Test]  Epoch: 30	Loss: 0.036314	Acc: 51.6% (5165/10000)
[Test]  Epoch: 31	Loss: 0.036400	Acc: 51.3% (5128/10000)
[Test]  Epoch: 32	Loss: 0.036342	Acc: 51.3% (5130/10000)
[Test]  Epoch: 33	Loss: 0.036340	Acc: 51.4% (5136/10000)
[Test]  Epoch: 34	Loss: 0.036253	Acc: 51.4% (5142/10000)
[Test]  Epoch: 35	Loss: 0.036311	Acc: 51.4% (5142/10000)
[Test]  Epoch: 36	Loss: 0.036340	Acc: 51.5% (5151/10000)
[Test]  Epoch: 37	Loss: 0.036433	Acc: 51.4% (5140/10000)
[Test]  Epoch: 38	Loss: 0.036491	Acc: 51.2% (5124/10000)
[Test]  Epoch: 39	Loss: 0.036447	Acc: 51.4% (5139/10000)
[Test]  Epoch: 40	Loss: 0.036564	Acc: 51.3% (5130/10000)
[Test]  Epoch: 41	Loss: 0.036398	Acc: 51.4% (5144/10000)
[Test]  Epoch: 42	Loss: 0.036495	Acc: 51.4% (5135/10000)
[Test]  Epoch: 43	Loss: 0.036522	Acc: 51.5% (5153/10000)
[Test]  Epoch: 44	Loss: 0.036495	Acc: 51.7% (5167/10000)
[Test]  Epoch: 45	Loss: 0.036479	Acc: 51.6% (5157/10000)
[Test]  Epoch: 46	Loss: 0.036602	Acc: 51.4% (5143/10000)
[Test]  Epoch: 47	Loss: 0.036534	Acc: 51.4% (5144/10000)
[Test]  Epoch: 48	Loss: 0.036546	Acc: 51.6% (5156/10000)
[Test]  Epoch: 49	Loss: 0.036606	Acc: 51.3% (5128/10000)
[Test]  Epoch: 50	Loss: 0.036665	Acc: 51.6% (5165/10000)
[Test]  Epoch: 51	Loss: 0.036651	Acc: 51.5% (5147/10000)
[Test]  Epoch: 52	Loss: 0.036579	Acc: 51.5% (5146/10000)
[Test]  Epoch: 53	Loss: 0.036734	Acc: 51.5% (5149/10000)
[Test]  Epoch: 54	Loss: 0.036704	Acc: 51.6% (5159/10000)
[Test]  Epoch: 55	Loss: 0.036641	Acc: 51.6% (5162/10000)
[Test]  Epoch: 56	Loss: 0.036691	Acc: 51.5% (5145/10000)
[Test]  Epoch: 57	Loss: 0.036688	Acc: 51.5% (5148/10000)
[Test]  Epoch: 58	Loss: 0.036668	Acc: 51.6% (5156/10000)
[Test]  Epoch: 59	Loss: 0.036769	Acc: 51.6% (5158/10000)
[Test]  Epoch: 60	Loss: 0.036781	Acc: 51.6% (5161/10000)
[Test]  Epoch: 61	Loss: 0.036808	Acc: 51.6% (5157/10000)
[Test]  Epoch: 62	Loss: 0.036772	Acc: 51.5% (5150/10000)
[Test]  Epoch: 63	Loss: 0.036756	Acc: 51.6% (5158/10000)
[Test]  Epoch: 64	Loss: 0.036754	Acc: 51.7% (5172/10000)
[Test]  Epoch: 65	Loss: 0.036743	Acc: 51.6% (5159/10000)
[Test]  Epoch: 66	Loss: 0.036774	Acc: 51.5% (5154/10000)
[Test]  Epoch: 67	Loss: 0.036752	Acc: 51.7% (5172/10000)
[Test]  Epoch: 68	Loss: 0.036824	Acc: 51.7% (5172/10000)
[Test]  Epoch: 69	Loss: 0.036775	Acc: 51.8% (5175/10000)
[Test]  Epoch: 70	Loss: 0.036726	Acc: 51.6% (5161/10000)
[Test]  Epoch: 71	Loss: 0.036827	Acc: 51.5% (5148/10000)
[Test]  Epoch: 72	Loss: 0.036794	Acc: 51.5% (5153/10000)
[Test]  Epoch: 73	Loss: 0.036692	Acc: 51.5% (5148/10000)
[Test]  Epoch: 74	Loss: 0.036699	Acc: 51.7% (5167/10000)
[Test]  Epoch: 75	Loss: 0.036743	Acc: 51.8% (5179/10000)
[Test]  Epoch: 76	Loss: 0.036680	Acc: 51.8% (5175/10000)
[Test]  Epoch: 77	Loss: 0.036757	Acc: 51.6% (5164/10000)
[Test]  Epoch: 78	Loss: 0.036806	Acc: 51.5% (5152/10000)
[Test]  Epoch: 79	Loss: 0.036767	Acc: 51.6% (5162/10000)
[Test]  Epoch: 80	Loss: 0.036825	Acc: 51.5% (5154/10000)
[Test]  Epoch: 81	Loss: 0.036820	Acc: 51.6% (5161/10000)
[Test]  Epoch: 82	Loss: 0.036838	Acc: 51.5% (5148/10000)
[Test]  Epoch: 83	Loss: 0.036778	Acc: 51.5% (5150/10000)
[Test]  Epoch: 84	Loss: 0.036815	Acc: 51.6% (5160/10000)
[Test]  Epoch: 85	Loss: 0.036795	Acc: 51.5% (5152/10000)
[Test]  Epoch: 86	Loss: 0.036852	Acc: 51.4% (5137/10000)
[Test]  Epoch: 87	Loss: 0.036797	Acc: 51.5% (5154/10000)
[Test]  Epoch: 88	Loss: 0.036786	Acc: 51.6% (5161/10000)
[Test]  Epoch: 89	Loss: 0.036831	Acc: 51.6% (5163/10000)
[Test]  Epoch: 90	Loss: 0.036846	Acc: 51.6% (5161/10000)
[Test]  Epoch: 91	Loss: 0.036809	Acc: 51.6% (5165/10000)
[Test]  Epoch: 92	Loss: 0.036737	Acc: 51.8% (5175/10000)
[Test]  Epoch: 93	Loss: 0.036812	Acc: 51.7% (5166/10000)
[Test]  Epoch: 94	Loss: 0.036837	Acc: 51.6% (5158/10000)
[Test]  Epoch: 95	Loss: 0.036814	Acc: 51.7% (5170/10000)
[Test]  Epoch: 96	Loss: 0.036744	Acc: 51.8% (5177/10000)
[Test]  Epoch: 97	Loss: 0.036781	Acc: 51.6% (5163/10000)
[Test]  Epoch: 98	Loss: 0.036798	Acc: 51.6% (5160/10000)
[Test]  Epoch: 99	Loss: 0.036784	Acc: 51.6% (5161/10000)
[Test]  Epoch: 100	Loss: 0.036769	Acc: 51.6% (5157/10000)
===========finish==========
['2024-08-19', '18:43:12.439705', '100', 'test', '0.036768953084945676', '51.57', '51.79']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.064691	Acc: 19.8% (1982/10000)
[Test]  Epoch: 2	Loss: 0.041664	Acc: 40.3% (4033/10000)
[Test]  Epoch: 3	Loss: 0.039173	Acc: 44.2% (4419/10000)
[Test]  Epoch: 4	Loss: 0.038187	Acc: 45.7% (4573/10000)
[Test]  Epoch: 5	Loss: 0.038513	Acc: 46.0% (4601/10000)
[Test]  Epoch: 6	Loss: 0.038167	Acc: 46.7% (4670/10000)
[Test]  Epoch: 7	Loss: 0.038060	Acc: 46.5% (4652/10000)
[Test]  Epoch: 8	Loss: 0.037986	Acc: 46.9% (4694/10000)
[Test]  Epoch: 9	Loss: 0.037989	Acc: 47.1% (4713/10000)
[Test]  Epoch: 10	Loss: 0.037806	Acc: 47.3% (4734/10000)
[Test]  Epoch: 11	Loss: 0.038007	Acc: 47.6% (4757/10000)
[Test]  Epoch: 12	Loss: 0.037819	Acc: 47.8% (4780/10000)
[Test]  Epoch: 13	Loss: 0.038020	Acc: 47.8% (4780/10000)
[Test]  Epoch: 14	Loss: 0.037853	Acc: 47.8% (4782/10000)
[Test]  Epoch: 15	Loss: 0.037971	Acc: 47.7% (4773/10000)
[Test]  Epoch: 16	Loss: 0.037899	Acc: 48.1% (4812/10000)
[Test]  Epoch: 17	Loss: 0.037783	Acc: 48.2% (4825/10000)
[Test]  Epoch: 18	Loss: 0.037991	Acc: 48.0% (4803/10000)
[Test]  Epoch: 19	Loss: 0.037854	Acc: 48.2% (4816/10000)
[Test]  Epoch: 20	Loss: 0.037940	Acc: 48.6% (4859/10000)
[Test]  Epoch: 21	Loss: 0.037835	Acc: 48.7% (4874/10000)
[Test]  Epoch: 22	Loss: 0.038078	Acc: 48.6% (4856/10000)
[Test]  Epoch: 23	Loss: 0.037942	Acc: 48.4% (4842/10000)
[Test]  Epoch: 24	Loss: 0.038020	Acc: 48.6% (4863/10000)
[Test]  Epoch: 25	Loss: 0.037965	Acc: 48.5% (4852/10000)
[Test]  Epoch: 26	Loss: 0.037892	Acc: 48.8% (4875/10000)
[Test]  Epoch: 27	Loss: 0.037921	Acc: 48.8% (4877/10000)
[Test]  Epoch: 28	Loss: 0.038016	Acc: 48.7% (4867/10000)
[Test]  Epoch: 29	Loss: 0.037896	Acc: 48.8% (4882/10000)
[Test]  Epoch: 30	Loss: 0.038026	Acc: 48.7% (4870/10000)
[Test]  Epoch: 31	Loss: 0.038170	Acc: 48.6% (4858/10000)
[Test]  Epoch: 32	Loss: 0.038125	Acc: 48.8% (4881/10000)
[Test]  Epoch: 33	Loss: 0.038111	Acc: 48.5% (4854/10000)
[Test]  Epoch: 34	Loss: 0.038014	Acc: 49.0% (4903/10000)
[Test]  Epoch: 35	Loss: 0.037916	Acc: 49.1% (4906/10000)
[Test]  Epoch: 36	Loss: 0.038090	Acc: 49.1% (4906/10000)
[Test]  Epoch: 37	Loss: 0.038152	Acc: 49.0% (4902/10000)
[Test]  Epoch: 38	Loss: 0.038273	Acc: 48.8% (4883/10000)
[Test]  Epoch: 39	Loss: 0.038199	Acc: 48.9% (4890/10000)
[Test]  Epoch: 40	Loss: 0.038277	Acc: 48.6% (4861/10000)
[Test]  Epoch: 41	Loss: 0.038144	Acc: 48.9% (4887/10000)
[Test]  Epoch: 42	Loss: 0.038157	Acc: 49.1% (4911/10000)
[Test]  Epoch: 43	Loss: 0.038210	Acc: 48.8% (4883/10000)
[Test]  Epoch: 44	Loss: 0.038228	Acc: 48.9% (4894/10000)
[Test]  Epoch: 45	Loss: 0.038112	Acc: 49.0% (4899/10000)
[Test]  Epoch: 46	Loss: 0.038268	Acc: 48.7% (4874/10000)
[Test]  Epoch: 47	Loss: 0.038266	Acc: 49.1% (4906/10000)
[Test]  Epoch: 48	Loss: 0.038209	Acc: 49.1% (4913/10000)
[Test]  Epoch: 49	Loss: 0.038283	Acc: 49.1% (4915/10000)
[Test]  Epoch: 50	Loss: 0.038320	Acc: 49.1% (4907/10000)
[Test]  Epoch: 51	Loss: 0.038335	Acc: 49.0% (4899/10000)
[Test]  Epoch: 52	Loss: 0.038272	Acc: 49.1% (4907/10000)
[Test]  Epoch: 53	Loss: 0.038377	Acc: 49.1% (4909/10000)
[Test]  Epoch: 54	Loss: 0.038389	Acc: 49.1% (4912/10000)
[Test]  Epoch: 55	Loss: 0.038304	Acc: 49.0% (4902/10000)
[Test]  Epoch: 56	Loss: 0.038379	Acc: 49.0% (4895/10000)
[Test]  Epoch: 57	Loss: 0.038328	Acc: 49.2% (4925/10000)
[Test]  Epoch: 58	Loss: 0.038313	Acc: 49.1% (4908/10000)
[Test]  Epoch: 59	Loss: 0.038470	Acc: 49.1% (4909/10000)
[Test]  Epoch: 60	Loss: 0.038471	Acc: 49.1% (4915/10000)
[Test]  Epoch: 61	Loss: 0.038514	Acc: 48.9% (4893/10000)
[Test]  Epoch: 62	Loss: 0.038456	Acc: 49.0% (4903/10000)
[Test]  Epoch: 63	Loss: 0.038420	Acc: 49.1% (4911/10000)
[Test]  Epoch: 64	Loss: 0.038411	Acc: 49.2% (4921/10000)
[Test]  Epoch: 65	Loss: 0.038409	Acc: 49.1% (4906/10000)
[Test]  Epoch: 66	Loss: 0.038453	Acc: 49.1% (4913/10000)
[Test]  Epoch: 67	Loss: 0.038496	Acc: 49.2% (4920/10000)
[Test]  Epoch: 68	Loss: 0.038528	Acc: 49.2% (4920/10000)
[Test]  Epoch: 69	Loss: 0.038473	Acc: 49.3% (4928/10000)
[Test]  Epoch: 70	Loss: 0.038446	Acc: 49.2% (4922/10000)
[Test]  Epoch: 71	Loss: 0.038507	Acc: 49.1% (4910/10000)
[Test]  Epoch: 72	Loss: 0.038516	Acc: 49.1% (4910/10000)
[Test]  Epoch: 73	Loss: 0.038406	Acc: 49.2% (4920/10000)
[Test]  Epoch: 74	Loss: 0.038423	Acc: 49.2% (4921/10000)
[Test]  Epoch: 75	Loss: 0.038473	Acc: 49.2% (4921/10000)
[Test]  Epoch: 76	Loss: 0.038403	Acc: 49.1% (4906/10000)
[Test]  Epoch: 77	Loss: 0.038444	Acc: 49.2% (4916/10000)
[Test]  Epoch: 78	Loss: 0.038509	Acc: 49.2% (4916/10000)
[Test]  Epoch: 79	Loss: 0.038473	Acc: 49.1% (4907/10000)
[Test]  Epoch: 80	Loss: 0.038529	Acc: 49.1% (4906/10000)
[Test]  Epoch: 81	Loss: 0.038503	Acc: 49.1% (4908/10000)
[Test]  Epoch: 82	Loss: 0.038536	Acc: 49.1% (4913/10000)
[Test]  Epoch: 83	Loss: 0.038468	Acc: 49.2% (4917/10000)
[Test]  Epoch: 84	Loss: 0.038515	Acc: 49.0% (4904/10000)
[Test]  Epoch: 85	Loss: 0.038497	Acc: 49.2% (4918/10000)
[Test]  Epoch: 86	Loss: 0.038527	Acc: 48.9% (4894/10000)
[Test]  Epoch: 87	Loss: 0.038491	Acc: 49.1% (4913/10000)
[Test]  Epoch: 88	Loss: 0.038483	Acc: 49.3% (4930/10000)
[Test]  Epoch: 89	Loss: 0.038485	Acc: 49.2% (4923/10000)
[Test]  Epoch: 90	Loss: 0.038499	Acc: 49.1% (4912/10000)
[Test]  Epoch: 91	Loss: 0.038464	Acc: 49.1% (4914/10000)
[Test]  Epoch: 92	Loss: 0.038426	Acc: 49.4% (4939/10000)
[Test]  Epoch: 93	Loss: 0.038509	Acc: 49.2% (4916/10000)
[Test]  Epoch: 94	Loss: 0.038527	Acc: 49.1% (4906/10000)
[Test]  Epoch: 95	Loss: 0.038506	Acc: 49.1% (4915/10000)
[Test]  Epoch: 96	Loss: 0.038426	Acc: 49.2% (4919/10000)
[Test]  Epoch: 97	Loss: 0.038450	Acc: 49.3% (4927/10000)
[Test]  Epoch: 98	Loss: 0.038494	Acc: 49.1% (4915/10000)
[Test]  Epoch: 99	Loss: 0.038479	Acc: 49.0% (4904/10000)
[Test]  Epoch: 100	Loss: 0.038490	Acc: 49.1% (4910/10000)
===========finish==========
['2024-08-19', '18:50:21.459264', '100', 'test', '0.0384895105600357', '49.1', '49.39']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.2.conv2.weight', 'layer2.3.conv1.weight', 'layer2.3.conv3.weight', 'layer2.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.1.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.0.conv3.weight', 'layer3.4.conv3.weight', 'layer3.4.conv1.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.2.conv2.weight', 'layer2.3.conv1.weight', 'layer2.3.conv3.weight', 'layer2.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.1.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.0.conv3.weight', 'layer3.4.conv3.weight', 'layer3.4.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.060357	Acc: 21.2% (2124/10000)
[Test]  Epoch: 2	Loss: 0.043394	Acc: 39.0% (3895/10000)
[Test]  Epoch: 3	Loss: 0.039648	Acc: 43.1% (4307/10000)
[Test]  Epoch: 4	Loss: 0.038414	Acc: 44.8% (4481/10000)
[Test]  Epoch: 5	Loss: 0.038797	Acc: 44.8% (4478/10000)
[Test]  Epoch: 6	Loss: 0.038473	Acc: 45.4% (4537/10000)
[Test]  Epoch: 7	Loss: 0.038366	Acc: 45.9% (4585/10000)
[Test]  Epoch: 8	Loss: 0.038299	Acc: 45.8% (4576/10000)
[Test]  Epoch: 9	Loss: 0.038391	Acc: 45.9% (4591/10000)
[Test]  Epoch: 10	Loss: 0.038343	Acc: 46.2% (4620/10000)
[Test]  Epoch: 11	Loss: 0.038356	Acc: 46.2% (4616/10000)
[Test]  Epoch: 12	Loss: 0.038271	Acc: 46.5% (4653/10000)
[Test]  Epoch: 13	Loss: 0.038387	Acc: 46.5% (4654/10000)
[Test]  Epoch: 14	Loss: 0.038333	Acc: 46.5% (4649/10000)
[Test]  Epoch: 15	Loss: 0.038466	Acc: 46.7% (4672/10000)
[Test]  Epoch: 16	Loss: 0.038409	Acc: 46.5% (4655/10000)
[Test]  Epoch: 17	Loss: 0.038381	Acc: 46.9% (4688/10000)
[Test]  Epoch: 18	Loss: 0.038448	Acc: 46.8% (4676/10000)
[Test]  Epoch: 19	Loss: 0.038357	Acc: 46.9% (4689/10000)
[Test]  Epoch: 20	Loss: 0.038515	Acc: 46.8% (4679/10000)
[Test]  Epoch: 21	Loss: 0.038433	Acc: 47.1% (4709/10000)
[Test]  Epoch: 22	Loss: 0.038583	Acc: 46.9% (4694/10000)
[Test]  Epoch: 23	Loss: 0.038487	Acc: 47.0% (4704/10000)
[Test]  Epoch: 24	Loss: 0.038651	Acc: 46.9% (4691/10000)
[Test]  Epoch: 25	Loss: 0.038567	Acc: 47.1% (4708/10000)
[Test]  Epoch: 26	Loss: 0.038518	Acc: 47.2% (4717/10000)
[Test]  Epoch: 27	Loss: 0.038553	Acc: 47.2% (4721/10000)
[Test]  Epoch: 28	Loss: 0.038564	Acc: 47.4% (4735/10000)
[Test]  Epoch: 29	Loss: 0.038594	Acc: 47.3% (4734/10000)
[Test]  Epoch: 30	Loss: 0.038618	Acc: 47.4% (4743/10000)
[Test]  Epoch: 31	Loss: 0.038803	Acc: 47.4% (4735/10000)
[Test]  Epoch: 32	Loss: 0.038787	Acc: 47.2% (4721/10000)
[Test]  Epoch: 33	Loss: 0.038767	Acc: 47.5% (4751/10000)
[Test]  Epoch: 34	Loss: 0.038661	Acc: 47.3% (4731/10000)
[Test]  Epoch: 35	Loss: 0.038623	Acc: 47.6% (4757/10000)
[Test]  Epoch: 36	Loss: 0.038865	Acc: 47.2% (4717/10000)
[Test]  Epoch: 37	Loss: 0.038823	Acc: 47.4% (4738/10000)
[Test]  Epoch: 38	Loss: 0.038727	Acc: 47.5% (4748/10000)
[Test]  Epoch: 39	Loss: 0.038802	Acc: 47.5% (4752/10000)
[Test]  Epoch: 40	Loss: 0.038826	Acc: 47.3% (4729/10000)
[Test]  Epoch: 41	Loss: 0.038799	Acc: 47.5% (4749/10000)
[Test]  Epoch: 42	Loss: 0.038889	Acc: 47.5% (4749/10000)
[Test]  Epoch: 43	Loss: 0.038851	Acc: 47.5% (4748/10000)
[Test]  Epoch: 44	Loss: 0.038929	Acc: 47.6% (4756/10000)
[Test]  Epoch: 45	Loss: 0.038848	Acc: 47.3% (4734/10000)
[Test]  Epoch: 46	Loss: 0.039035	Acc: 47.4% (4742/10000)
[Test]  Epoch: 47	Loss: 0.038964	Acc: 47.7% (4767/10000)
[Test]  Epoch: 48	Loss: 0.039036	Acc: 47.5% (4751/10000)
[Test]  Epoch: 49	Loss: 0.038955	Acc: 47.5% (4754/10000)
[Test]  Epoch: 50	Loss: 0.039123	Acc: 47.4% (4741/10000)
[Test]  Epoch: 51	Loss: 0.039039	Acc: 47.7% (4771/10000)
[Test]  Epoch: 52	Loss: 0.039029	Acc: 47.5% (4746/10000)
[Test]  Epoch: 53	Loss: 0.039113	Acc: 47.6% (4756/10000)
[Test]  Epoch: 54	Loss: 0.039091	Acc: 47.6% (4757/10000)
[Test]  Epoch: 55	Loss: 0.039079	Acc: 47.5% (4746/10000)
[Test]  Epoch: 56	Loss: 0.039142	Acc: 47.6% (4758/10000)
[Test]  Epoch: 57	Loss: 0.039060	Acc: 47.5% (4749/10000)
[Test]  Epoch: 58	Loss: 0.039047	Acc: 47.7% (4768/10000)
[Test]  Epoch: 59	Loss: 0.039122	Acc: 47.5% (4751/10000)
[Test]  Epoch: 60	Loss: 0.039197	Acc: 47.7% (4773/10000)
[Test]  Epoch: 61	Loss: 0.039189	Acc: 47.6% (4760/10000)
[Test]  Epoch: 62	Loss: 0.039137	Acc: 47.8% (4779/10000)
[Test]  Epoch: 63	Loss: 0.039101	Acc: 47.8% (4776/10000)
[Test]  Epoch: 64	Loss: 0.039114	Acc: 47.8% (4783/10000)
[Test]  Epoch: 65	Loss: 0.039181	Acc: 47.7% (4768/10000)
[Test]  Epoch: 66	Loss: 0.039170	Acc: 47.6% (4765/10000)
[Test]  Epoch: 67	Loss: 0.039198	Acc: 47.7% (4766/10000)
[Test]  Epoch: 68	Loss: 0.039255	Acc: 47.3% (4734/10000)
[Test]  Epoch: 69	Loss: 0.039166	Acc: 47.6% (4765/10000)
[Test]  Epoch: 70	Loss: 0.039143	Acc: 47.6% (4765/10000)
[Test]  Epoch: 71	Loss: 0.039239	Acc: 47.6% (4756/10000)
[Test]  Epoch: 72	Loss: 0.039280	Acc: 47.4% (4739/10000)
[Test]  Epoch: 73	Loss: 0.039171	Acc: 47.8% (4776/10000)
[Test]  Epoch: 74	Loss: 0.039111	Acc: 47.8% (4783/10000)
[Test]  Epoch: 75	Loss: 0.039165	Acc: 47.8% (4775/10000)
[Test]  Epoch: 76	Loss: 0.039132	Acc: 47.9% (4785/10000)
[Test]  Epoch: 77	Loss: 0.039140	Acc: 47.8% (4780/10000)
[Test]  Epoch: 78	Loss: 0.039226	Acc: 47.4% (4735/10000)
[Test]  Epoch: 79	Loss: 0.039123	Acc: 47.6% (4760/10000)
[Test]  Epoch: 80	Loss: 0.039263	Acc: 47.6% (4760/10000)
[Test]  Epoch: 81	Loss: 0.039237	Acc: 47.5% (4753/10000)
[Test]  Epoch: 82	Loss: 0.039210	Acc: 47.4% (4743/10000)
[Test]  Epoch: 83	Loss: 0.039190	Acc: 47.5% (4753/10000)
[Test]  Epoch: 84	Loss: 0.039216	Acc: 47.7% (4774/10000)
[Test]  Epoch: 85	Loss: 0.039201	Acc: 47.6% (4756/10000)
[Test]  Epoch: 86	Loss: 0.039299	Acc: 47.4% (4744/10000)
[Test]  Epoch: 87	Loss: 0.039252	Acc: 47.5% (4755/10000)
[Test]  Epoch: 88	Loss: 0.039218	Acc: 47.7% (4766/10000)
[Test]  Epoch: 89	Loss: 0.039217	Acc: 47.6% (4761/10000)
[Test]  Epoch: 90	Loss: 0.039243	Acc: 47.7% (4771/10000)
[Test]  Epoch: 91	Loss: 0.039191	Acc: 47.6% (4763/10000)
[Test]  Epoch: 92	Loss: 0.039120	Acc: 47.8% (4780/10000)
[Test]  Epoch: 93	Loss: 0.039231	Acc: 47.6% (4760/10000)
[Test]  Epoch: 94	Loss: 0.039232	Acc: 47.6% (4761/10000)
[Test]  Epoch: 95	Loss: 0.039190	Acc: 47.6% (4763/10000)
[Test]  Epoch: 96	Loss: 0.039148	Acc: 47.7% (4774/10000)
[Test]  Epoch: 97	Loss: 0.039238	Acc: 47.6% (4756/10000)
[Test]  Epoch: 98	Loss: 0.039217	Acc: 47.7% (4768/10000)
[Test]  Epoch: 99	Loss: 0.039228	Acc: 47.5% (4755/10000)
[Test]  Epoch: 100	Loss: 0.039224	Acc: 47.5% (4754/10000)
===========finish==========
['2024-08-19', '18:57:33.109326', '100', 'test', '0.03922412530183792', '47.54', '47.85']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.2.conv2.weight', 'layer2.3.conv1.weight', 'layer2.3.conv3.weight', 'layer2.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.1.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.0.conv3.weight', 'layer3.4.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv1.weight', 'layer3.3.conv3.weight', 'layer2.1.conv2.weight', 'layer3.2.conv1.weight', 'layer2.2.conv2.weight', 'layer3.5.conv1.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer3.2.conv3.weight', 'layer4.1.conv3.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.2.conv2.weight', 'layer2.3.conv1.weight', 'layer2.3.conv3.weight', 'layer2.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.1.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.0.conv3.weight', 'layer3.4.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv1.weight', 'layer3.3.conv3.weight', 'layer2.1.conv2.weight', 'layer3.2.conv1.weight', 'layer2.2.conv2.weight', 'layer3.5.conv1.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer3.2.conv3.weight', 'layer4.1.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.088481	Acc: 19.2% (1922/10000)
[Test]  Epoch: 2	Loss: 0.043604	Acc: 38.7% (3868/10000)
[Test]  Epoch: 3	Loss: 0.040506	Acc: 42.6% (4259/10000)
[Test]  Epoch: 4	Loss: 0.039609	Acc: 44.0% (4405/10000)
[Test]  Epoch: 5	Loss: 0.039832	Acc: 44.3% (4433/10000)
[Test]  Epoch: 6	Loss: 0.039460	Acc: 45.1% (4511/10000)
[Test]  Epoch: 7	Loss: 0.039546	Acc: 45.1% (4509/10000)
[Test]  Epoch: 8	Loss: 0.039387	Acc: 45.3% (4533/10000)
[Test]  Epoch: 9	Loss: 0.039451	Acc: 45.6% (4557/10000)
[Test]  Epoch: 10	Loss: 0.039463	Acc: 45.4% (4539/10000)
[Test]  Epoch: 11	Loss: 0.039573	Acc: 45.6% (4563/10000)
[Test]  Epoch: 12	Loss: 0.039452	Acc: 45.7% (4573/10000)
[Test]  Epoch: 13	Loss: 0.039517	Acc: 45.8% (4578/10000)
[Test]  Epoch: 14	Loss: 0.039562	Acc: 45.8% (4583/10000)
[Test]  Epoch: 15	Loss: 0.039645	Acc: 45.7% (4574/10000)
[Test]  Epoch: 16	Loss: 0.039454	Acc: 45.8% (4580/10000)
[Test]  Epoch: 17	Loss: 0.039424	Acc: 45.7% (4574/10000)
[Test]  Epoch: 18	Loss: 0.039630	Acc: 45.9% (4589/10000)
[Test]  Epoch: 19	Loss: 0.039558	Acc: 46.2% (4619/10000)
[Test]  Epoch: 20	Loss: 0.039664	Acc: 46.1% (4610/10000)
[Test]  Epoch: 21	Loss: 0.039565	Acc: 45.9% (4593/10000)
[Test]  Epoch: 22	Loss: 0.039866	Acc: 46.1% (4608/10000)
[Test]  Epoch: 23	Loss: 0.039597	Acc: 46.5% (4650/10000)
[Test]  Epoch: 24	Loss: 0.039805	Acc: 46.3% (4633/10000)
[Test]  Epoch: 25	Loss: 0.039749	Acc: 46.0% (4600/10000)
[Test]  Epoch: 26	Loss: 0.039599	Acc: 46.3% (4626/10000)
[Test]  Epoch: 27	Loss: 0.039910	Acc: 45.8% (4582/10000)
[Test]  Epoch: 28	Loss: 0.039772	Acc: 46.3% (4632/10000)
[Test]  Epoch: 29	Loss: 0.039649	Acc: 46.2% (4620/10000)
[Test]  Epoch: 30	Loss: 0.039799	Acc: 46.1% (4613/10000)
[Test]  Epoch: 31	Loss: 0.039933	Acc: 46.1% (4615/10000)
[Test]  Epoch: 32	Loss: 0.039938	Acc: 46.2% (4624/10000)
[Test]  Epoch: 33	Loss: 0.039940	Acc: 46.2% (4625/10000)
[Test]  Epoch: 34	Loss: 0.039880	Acc: 46.2% (4623/10000)
[Test]  Epoch: 35	Loss: 0.039724	Acc: 46.7% (4673/10000)
[Test]  Epoch: 36	Loss: 0.039873	Acc: 46.4% (4638/10000)
[Test]  Epoch: 37	Loss: 0.039966	Acc: 46.5% (4651/10000)
[Test]  Epoch: 38	Loss: 0.039889	Acc: 46.5% (4648/10000)
[Test]  Epoch: 39	Loss: 0.039940	Acc: 46.1% (4613/10000)
[Test]  Epoch: 40	Loss: 0.040035	Acc: 46.4% (4640/10000)
[Test]  Epoch: 41	Loss: 0.039972	Acc: 46.3% (4633/10000)
[Test]  Epoch: 42	Loss: 0.040061	Acc: 46.2% (4620/10000)
[Test]  Epoch: 43	Loss: 0.040002	Acc: 46.6% (4656/10000)
[Test]  Epoch: 44	Loss: 0.040084	Acc: 46.2% (4619/10000)
[Test]  Epoch: 45	Loss: 0.040112	Acc: 46.2% (4621/10000)
[Test]  Epoch: 46	Loss: 0.040127	Acc: 46.1% (4612/10000)
[Test]  Epoch: 47	Loss: 0.040079	Acc: 46.6% (4664/10000)
[Test]  Epoch: 48	Loss: 0.040084	Acc: 46.5% (4653/10000)
[Test]  Epoch: 49	Loss: 0.040225	Acc: 46.5% (4646/10000)
[Test]  Epoch: 50	Loss: 0.040197	Acc: 46.4% (4642/10000)
[Test]  Epoch: 51	Loss: 0.040234	Acc: 46.3% (4626/10000)
[Test]  Epoch: 52	Loss: 0.040260	Acc: 46.3% (4628/10000)
[Test]  Epoch: 53	Loss: 0.040267	Acc: 46.3% (4632/10000)
[Test]  Epoch: 54	Loss: 0.040286	Acc: 46.3% (4634/10000)
[Test]  Epoch: 55	Loss: 0.040244	Acc: 46.2% (4624/10000)
[Test]  Epoch: 56	Loss: 0.040310	Acc: 46.2% (4617/10000)
[Test]  Epoch: 57	Loss: 0.040255	Acc: 46.4% (4639/10000)
[Test]  Epoch: 58	Loss: 0.040199	Acc: 46.2% (4622/10000)
[Test]  Epoch: 59	Loss: 0.040419	Acc: 46.1% (4606/10000)
[Test]  Epoch: 60	Loss: 0.040335	Acc: 46.5% (4651/10000)
[Test]  Epoch: 61	Loss: 0.040328	Acc: 46.5% (4652/10000)
[Test]  Epoch: 62	Loss: 0.040341	Acc: 46.6% (4657/10000)
[Test]  Epoch: 63	Loss: 0.040327	Acc: 46.6% (4660/10000)
[Test]  Epoch: 64	Loss: 0.040327	Acc: 46.5% (4648/10000)
[Test]  Epoch: 65	Loss: 0.040320	Acc: 46.5% (4651/10000)
[Test]  Epoch: 66	Loss: 0.040320	Acc: 46.4% (4643/10000)
[Test]  Epoch: 67	Loss: 0.040339	Acc: 46.6% (4661/10000)
[Test]  Epoch: 68	Loss: 0.040374	Acc: 46.4% (4638/10000)
[Test]  Epoch: 69	Loss: 0.040329	Acc: 46.5% (4648/10000)
[Test]  Epoch: 70	Loss: 0.040322	Acc: 46.7% (4666/10000)
[Test]  Epoch: 71	Loss: 0.040426	Acc: 46.4% (4635/10000)
[Test]  Epoch: 72	Loss: 0.040424	Acc: 46.3% (4626/10000)
[Test]  Epoch: 73	Loss: 0.040362	Acc: 46.5% (4645/10000)
[Test]  Epoch: 74	Loss: 0.040339	Acc: 46.5% (4649/10000)
[Test]  Epoch: 75	Loss: 0.040401	Acc: 46.6% (4660/10000)
[Test]  Epoch: 76	Loss: 0.040365	Acc: 46.5% (4652/10000)
[Test]  Epoch: 77	Loss: 0.040377	Acc: 46.5% (4650/10000)
[Test]  Epoch: 78	Loss: 0.040430	Acc: 46.3% (4626/10000)
[Test]  Epoch: 79	Loss: 0.040351	Acc: 46.4% (4636/10000)
[Test]  Epoch: 80	Loss: 0.040442	Acc: 46.4% (4643/10000)
[Test]  Epoch: 81	Loss: 0.040444	Acc: 46.3% (4631/10000)
[Test]  Epoch: 82	Loss: 0.040425	Acc: 46.4% (4636/10000)
[Test]  Epoch: 83	Loss: 0.040390	Acc: 46.4% (4640/10000)
[Test]  Epoch: 84	Loss: 0.040426	Acc: 46.6% (4660/10000)
[Test]  Epoch: 85	Loss: 0.040432	Acc: 46.4% (4640/10000)
[Test]  Epoch: 86	Loss: 0.040492	Acc: 46.4% (4641/10000)
[Test]  Epoch: 87	Loss: 0.040466	Acc: 46.4% (4644/10000)
[Test]  Epoch: 88	Loss: 0.040425	Acc: 46.4% (4641/10000)
[Test]  Epoch: 89	Loss: 0.040454	Acc: 46.4% (4639/10000)
[Test]  Epoch: 90	Loss: 0.040461	Acc: 46.4% (4638/10000)
[Test]  Epoch: 91	Loss: 0.040410	Acc: 46.6% (4657/10000)
[Test]  Epoch: 92	Loss: 0.040352	Acc: 46.5% (4652/10000)
[Test]  Epoch: 93	Loss: 0.040456	Acc: 46.5% (4651/10000)
[Test]  Epoch: 94	Loss: 0.040451	Acc: 46.4% (4640/10000)
[Test]  Epoch: 95	Loss: 0.040382	Acc: 46.4% (4639/10000)
[Test]  Epoch: 96	Loss: 0.040332	Acc: 46.4% (4643/10000)
[Test]  Epoch: 97	Loss: 0.040462	Acc: 46.5% (4647/10000)
[Test]  Epoch: 98	Loss: 0.040470	Acc: 46.5% (4650/10000)
[Test]  Epoch: 99	Loss: 0.040469	Acc: 46.6% (4659/10000)
[Test]  Epoch: 100	Loss: 0.040429	Acc: 46.5% (4651/10000)
===========finish==========
['2024-08-19', '19:04:38.895938', '100', 'test', '0.04042927486896515', '46.51', '46.73']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.2.conv2.weight', 'layer2.3.conv1.weight', 'layer2.3.conv3.weight', 'layer2.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.1.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.0.conv3.weight', 'layer3.4.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv1.weight', 'layer3.3.conv3.weight', 'layer2.1.conv2.weight', 'layer3.2.conv1.weight', 'layer2.2.conv2.weight', 'layer3.5.conv1.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer3.2.conv3.weight', 'layer4.1.conv3.weight', 'layer3.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.conv2.weight', 'layer4.1.conv1.weight', 'layer3.4.conv2.weight', 'layer2.0.downsample.0.weight', 'layer3.0.conv1.weight', 'layer3.5.conv2.weight', 'layer3.3.conv2.weight', 'layer4.1.conv2.weight', 'layer4.2.conv1.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.2.conv2.weight', 'layer2.3.conv1.weight', 'layer2.3.conv3.weight', 'layer2.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.1.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.0.conv3.weight', 'layer3.4.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv1.weight', 'layer3.3.conv3.weight', 'layer2.1.conv2.weight', 'layer3.2.conv1.weight', 'layer2.2.conv2.weight', 'layer3.5.conv1.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer3.2.conv3.weight', 'layer4.1.conv3.weight', 'layer3.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.conv2.weight', 'layer4.1.conv1.weight', 'layer3.4.conv2.weight', 'layer2.0.downsample.0.weight', 'layer3.0.conv1.weight', 'layer3.5.conv2.weight', 'layer3.3.conv2.weight', 'layer4.1.conv2.weight', 'layer4.2.conv1.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.054302	Acc: 24.9% (2489/10000)
[Test]  Epoch: 2	Loss: 0.043177	Acc: 37.8% (3782/10000)
[Test]  Epoch: 3	Loss: 0.040109	Acc: 41.4% (4139/10000)
[Test]  Epoch: 4	Loss: 0.040101	Acc: 42.2% (4216/10000)
[Test]  Epoch: 5	Loss: 0.040432	Acc: 42.3% (4228/10000)
[Test]  Epoch: 6	Loss: 0.040080	Acc: 42.6% (4256/10000)
[Test]  Epoch: 7	Loss: 0.040016	Acc: 42.9% (4286/10000)
[Test]  Epoch: 8	Loss: 0.040009	Acc: 43.1% (4315/10000)
[Test]  Epoch: 9	Loss: 0.040179	Acc: 43.0% (4303/10000)
[Test]  Epoch: 10	Loss: 0.039927	Acc: 43.5% (4345/10000)
[Test]  Epoch: 11	Loss: 0.040196	Acc: 43.4% (4343/10000)
[Test]  Epoch: 12	Loss: 0.040139	Acc: 43.5% (4353/10000)
[Test]  Epoch: 13	Loss: 0.040231	Acc: 43.4% (4335/10000)
[Test]  Epoch: 14	Loss: 0.040329	Acc: 43.6% (4357/10000)
[Test]  Epoch: 15	Loss: 0.040336	Acc: 43.6% (4357/10000)
[Test]  Epoch: 16	Loss: 0.040271	Acc: 43.5% (4354/10000)
[Test]  Epoch: 17	Loss: 0.040103	Acc: 44.0% (4395/10000)
[Test]  Epoch: 18	Loss: 0.040345	Acc: 43.8% (4378/10000)
[Test]  Epoch: 19	Loss: 0.040330	Acc: 43.6% (4359/10000)
[Test]  Epoch: 20	Loss: 0.040399	Acc: 44.0% (4402/10000)
[Test]  Epoch: 21	Loss: 0.040211	Acc: 43.8% (4379/10000)
[Test]  Epoch: 22	Loss: 0.040510	Acc: 43.8% (4383/10000)
[Test]  Epoch: 23	Loss: 0.040255	Acc: 43.9% (4388/10000)
[Test]  Epoch: 24	Loss: 0.040445	Acc: 44.1% (4410/10000)
[Test]  Epoch: 25	Loss: 0.040395	Acc: 44.2% (4419/10000)
[Test]  Epoch: 26	Loss: 0.040449	Acc: 44.1% (4410/10000)
[Test]  Epoch: 27	Loss: 0.040457	Acc: 43.8% (4379/10000)
[Test]  Epoch: 28	Loss: 0.040376	Acc: 44.0% (4405/10000)
[Test]  Epoch: 29	Loss: 0.040433	Acc: 44.1% (4414/10000)
[Test]  Epoch: 30	Loss: 0.040455	Acc: 44.3% (4432/10000)
[Test]  Epoch: 31	Loss: 0.040682	Acc: 43.9% (4394/10000)
[Test]  Epoch: 32	Loss: 0.040668	Acc: 44.2% (4420/10000)
[Test]  Epoch: 33	Loss: 0.040635	Acc: 44.2% (4417/10000)
[Test]  Epoch: 34	Loss: 0.040639	Acc: 44.1% (4406/10000)
[Test]  Epoch: 35	Loss: 0.040569	Acc: 44.3% (4426/10000)
[Test]  Epoch: 36	Loss: 0.040678	Acc: 44.4% (4438/10000)
[Test]  Epoch: 37	Loss: 0.040724	Acc: 44.3% (4426/10000)
[Test]  Epoch: 38	Loss: 0.040725	Acc: 44.3% (4430/10000)
[Test]  Epoch: 39	Loss: 0.040730	Acc: 44.1% (4413/10000)
[Test]  Epoch: 40	Loss: 0.040800	Acc: 44.0% (4400/10000)
[Test]  Epoch: 41	Loss: 0.040860	Acc: 44.1% (4414/10000)
[Test]  Epoch: 42	Loss: 0.040799	Acc: 44.2% (4417/10000)
[Test]  Epoch: 43	Loss: 0.040852	Acc: 44.1% (4415/10000)
[Test]  Epoch: 44	Loss: 0.041006	Acc: 44.2% (4423/10000)
[Test]  Epoch: 45	Loss: 0.040839	Acc: 44.2% (4420/10000)
[Test]  Epoch: 46	Loss: 0.041002	Acc: 44.2% (4421/10000)
[Test]  Epoch: 47	Loss: 0.040999	Acc: 44.2% (4421/10000)
[Test]  Epoch: 48	Loss: 0.040958	Acc: 44.4% (4441/10000)
[Test]  Epoch: 49	Loss: 0.040984	Acc: 44.2% (4421/10000)
[Test]  Epoch: 50	Loss: 0.041063	Acc: 44.2% (4423/10000)
[Test]  Epoch: 51	Loss: 0.041023	Acc: 44.2% (4424/10000)
[Test]  Epoch: 52	Loss: 0.041032	Acc: 44.1% (4412/10000)
[Test]  Epoch: 53	Loss: 0.041173	Acc: 44.0% (4395/10000)
[Test]  Epoch: 54	Loss: 0.041162	Acc: 44.2% (4423/10000)
[Test]  Epoch: 55	Loss: 0.041075	Acc: 44.2% (4424/10000)
[Test]  Epoch: 56	Loss: 0.041205	Acc: 44.2% (4417/10000)
[Test]  Epoch: 57	Loss: 0.041120	Acc: 44.4% (4443/10000)
[Test]  Epoch: 58	Loss: 0.041117	Acc: 44.4% (4437/10000)
[Test]  Epoch: 59	Loss: 0.041293	Acc: 44.1% (4412/10000)
[Test]  Epoch: 60	Loss: 0.041188	Acc: 44.3% (4433/10000)
[Test]  Epoch: 61	Loss: 0.041296	Acc: 44.1% (4415/10000)
[Test]  Epoch: 62	Loss: 0.041245	Acc: 44.4% (4442/10000)
[Test]  Epoch: 63	Loss: 0.041230	Acc: 44.1% (4406/10000)
[Test]  Epoch: 64	Loss: 0.041256	Acc: 44.2% (4420/10000)
[Test]  Epoch: 65	Loss: 0.041256	Acc: 44.3% (4427/10000)
[Test]  Epoch: 66	Loss: 0.041224	Acc: 44.4% (4437/10000)
[Test]  Epoch: 67	Loss: 0.041243	Acc: 44.4% (4438/10000)
[Test]  Epoch: 68	Loss: 0.041315	Acc: 44.2% (4423/10000)
[Test]  Epoch: 69	Loss: 0.041272	Acc: 44.3% (4427/10000)
[Test]  Epoch: 70	Loss: 0.041258	Acc: 44.3% (4426/10000)
[Test]  Epoch: 71	Loss: 0.041307	Acc: 44.0% (4402/10000)
[Test]  Epoch: 72	Loss: 0.041354	Acc: 44.1% (4414/10000)
[Test]  Epoch: 73	Loss: 0.041274	Acc: 44.1% (4414/10000)
[Test]  Epoch: 74	Loss: 0.041199	Acc: 44.4% (4435/10000)
[Test]  Epoch: 75	Loss: 0.041243	Acc: 44.4% (4439/10000)
[Test]  Epoch: 76	Loss: 0.041218	Acc: 44.4% (4435/10000)
[Test]  Epoch: 77	Loss: 0.041245	Acc: 44.2% (4420/10000)
[Test]  Epoch: 78	Loss: 0.041302	Acc: 44.2% (4416/10000)
[Test]  Epoch: 79	Loss: 0.041242	Acc: 44.4% (4436/10000)
[Test]  Epoch: 80	Loss: 0.041285	Acc: 44.1% (4413/10000)
[Test]  Epoch: 81	Loss: 0.041322	Acc: 44.1% (4412/10000)
[Test]  Epoch: 82	Loss: 0.041343	Acc: 44.1% (4411/10000)
[Test]  Epoch: 83	Loss: 0.041237	Acc: 44.2% (4424/10000)
[Test]  Epoch: 84	Loss: 0.041301	Acc: 44.4% (4435/10000)
[Test]  Epoch: 85	Loss: 0.041275	Acc: 44.3% (4427/10000)
[Test]  Epoch: 86	Loss: 0.041357	Acc: 44.0% (4399/10000)
[Test]  Epoch: 87	Loss: 0.041315	Acc: 43.9% (4393/10000)
[Test]  Epoch: 88	Loss: 0.041331	Acc: 44.2% (4424/10000)
[Test]  Epoch: 89	Loss: 0.041333	Acc: 44.1% (4409/10000)
[Test]  Epoch: 90	Loss: 0.041334	Acc: 44.1% (4411/10000)
[Test]  Epoch: 91	Loss: 0.041348	Acc: 44.2% (4418/10000)
[Test]  Epoch: 92	Loss: 0.041222	Acc: 44.4% (4435/10000)
[Test]  Epoch: 93	Loss: 0.041350	Acc: 44.1% (4412/10000)
[Test]  Epoch: 94	Loss: 0.041331	Acc: 44.3% (4433/10000)
[Test]  Epoch: 95	Loss: 0.041312	Acc: 44.1% (4414/10000)
[Test]  Epoch: 96	Loss: 0.041306	Acc: 44.1% (4413/10000)
[Test]  Epoch: 97	Loss: 0.041376	Acc: 44.0% (4399/10000)
[Test]  Epoch: 98	Loss: 0.041357	Acc: 44.1% (4407/10000)
[Test]  Epoch: 99	Loss: 0.041298	Acc: 44.3% (4427/10000)
[Test]  Epoch: 100	Loss: 0.041300	Acc: 44.3% (4429/10000)
===========finish==========
['2024-08-19', '19:11:42.303269', '100', 'test', '0.04130048453807831', '44.29', '44.43']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet50-channel resnet50 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/tinyimagenet200-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.2.conv2.weight', 'layer2.3.conv1.weight', 'layer2.3.conv3.weight', 'layer2.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.1.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.0.conv3.weight', 'layer3.4.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv1.weight', 'layer3.3.conv3.weight', 'layer2.1.conv2.weight', 'layer3.2.conv1.weight', 'layer2.2.conv2.weight', 'layer3.5.conv1.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer3.2.conv3.weight', 'layer4.1.conv3.weight', 'layer3.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.conv2.weight', 'layer4.1.conv1.weight', 'layer3.4.conv2.weight', 'layer2.0.downsample.0.weight', 'layer3.0.conv1.weight', 'layer3.5.conv2.weight', 'layer3.3.conv2.weight', 'layer4.1.conv2.weight', 'layer4.2.conv1.weight', 'layer3.2.conv2.weight', 'layer4.2.conv2.weight', 'layer3.1.conv2.weight', 'layer3.0.conv3.weight', 'last_linear.weight', 'layer3.0.downsample.0.weight', 'layer3.0.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight']
['layer1.2.bn1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn2.weight', 'layer1.2.bn2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn1.weight', 'layer4.1.bn2.weight', 'layer2.3.bn1.weight', 'layer4.2.bn3.weight', 'layer2.2.bn1.weight', 'layer2.1.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.4.bn1.weight', 'layer3.4.bn2.weight', 'layer3.3.bn1.weight', 'layer2.1.bn2.weight', 'layer4.2.bn2.weight', 'layer3.3.bn2.weight', 'layer4.1.bn3.weight', 'layer3.2.bn1.weight', 'layer4.1.bn1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn2.weight', 'layer3.2.bn2.weight', 'layer4.2.bn1.weight', 'layer1.1.bn3.weight', 'layer1.2.bn3.weight', 'layer3.1.bn1.weight', 'layer2.0.bn2.weight', 'layer2.0.bn1.weight', 'layer3.1.bn2.weight', 'layer1.0.bn3.weight', 'bn1.weight', 'layer2.3.bn3.weight', 'layer4.0.bn3.weight', 'layer2.2.bn3.weight', 'layer3.4.bn3.weight', 'layer2.1.bn3.weight', 'layer3.0.bn2.weight', 'layer3.3.bn3.weight', 'layer3.0.bn1.weight', 'layer4.0.downsample.1.weight', 'layer1.0.downsample.1.weight', 'layer3.5.bn3.weight', 'layer2.0.bn3.weight', 'layer3.2.bn3.weight', 'layer3.1.bn3.weight', 'layer2.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer4.0.bn1.weight', 'layer3.0.bn3.weight', 'layer3.0.downsample.1.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.1.conv3.weight', 'layer1.2.conv1.weight', 'layer1.2.conv3.weight', 'layer1.0.conv3.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer1.0.conv2.weight', 'conv1.weight', 'layer2.2.conv1.weight', 'layer1.2.conv2.weight', 'layer2.3.conv1.weight', 'layer2.3.conv3.weight', 'layer2.2.conv3.weight', 'layer2.0.conv1.weight', 'layer2.1.conv3.weight', 'layer1.0.downsample.0.weight', 'layer2.0.conv3.weight', 'layer3.4.conv3.weight', 'layer3.4.conv1.weight', 'layer3.5.conv3.weight', 'layer3.3.conv1.weight', 'layer3.3.conv3.weight', 'layer2.1.conv2.weight', 'layer3.2.conv1.weight', 'layer2.2.conv2.weight', 'layer3.5.conv1.weight', 'layer2.3.conv2.weight', 'layer3.1.conv1.weight', 'layer3.2.conv3.weight', 'layer4.1.conv3.weight', 'layer3.1.conv3.weight', 'layer4.2.conv3.weight', 'layer2.0.conv2.weight', 'layer4.1.conv1.weight', 'layer3.4.conv2.weight', 'layer2.0.downsample.0.weight', 'layer3.0.conv1.weight', 'layer3.5.conv2.weight', 'layer3.3.conv2.weight', 'layer4.1.conv2.weight', 'layer4.2.conv1.weight', 'layer3.2.conv2.weight', 'layer4.2.conv2.weight', 'layer3.1.conv2.weight', 'layer3.0.conv3.weight', 'last_linear.weight', 'layer3.0.downsample.0.weight', 'layer3.0.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.083442	Acc: 1.1% (114/10000)
[Test]  Epoch: 2	Loss: 0.078987	Acc: 4.9% (494/10000)
[Test]  Epoch: 3	Loss: 0.075755	Acc: 8.3% (830/10000)
[Test]  Epoch: 4	Loss: 0.072663	Acc: 11.3% (1127/10000)
[Test]  Epoch: 5	Loss: 0.070008	Acc: 13.4% (1345/10000)
[Test]  Epoch: 6	Loss: 0.068025	Acc: 15.2% (1515/10000)
[Test]  Epoch: 7	Loss: 0.066430	Acc: 16.2% (1616/10000)
[Test]  Epoch: 8	Loss: 0.064831	Acc: 16.9% (1692/10000)
[Test]  Epoch: 9	Loss: 0.063550	Acc: 17.9% (1792/10000)
[Test]  Epoch: 10	Loss: 0.062375	Acc: 18.6% (1855/10000)
[Test]  Epoch: 11	Loss: 0.061541	Acc: 18.8% (1878/10000)
[Test]  Epoch: 12	Loss: 0.060599	Acc: 19.4% (1937/10000)
[Test]  Epoch: 13	Loss: 0.060119	Acc: 19.9% (1988/10000)
[Test]  Epoch: 14	Loss: 0.059665	Acc: 19.8% (1983/10000)
[Test]  Epoch: 15	Loss: 0.059051	Acc: 20.6% (2059/10000)
[Test]  Epoch: 16	Loss: 0.058605	Acc: 20.6% (2065/10000)
[Test]  Epoch: 17	Loss: 0.058160	Acc: 21.2% (2122/10000)
[Test]  Epoch: 18	Loss: 0.057999	Acc: 21.0% (2100/10000)
[Test]  Epoch: 19	Loss: 0.057660	Acc: 21.3% (2132/10000)
[Test]  Epoch: 20	Loss: 0.057271	Acc: 21.6% (2164/10000)
[Test]  Epoch: 21	Loss: 0.057005	Acc: 22.0% (2196/10000)
[Test]  Epoch: 22	Loss: 0.056722	Acc: 22.4% (2244/10000)
[Test]  Epoch: 23	Loss: 0.056530	Acc: 22.5% (2247/10000)
[Test]  Epoch: 24	Loss: 0.056372	Acc: 22.3% (2232/10000)
[Test]  Epoch: 25	Loss: 0.056301	Acc: 22.6% (2258/10000)
[Test]  Epoch: 26	Loss: 0.056054	Acc: 22.9% (2289/10000)
[Test]  Epoch: 27	Loss: 0.055858	Acc: 22.8% (2282/10000)
[Test]  Epoch: 28	Loss: 0.055706	Acc: 23.2% (2318/10000)
[Test]  Epoch: 29	Loss: 0.055529	Acc: 23.3% (2327/10000)
[Test]  Epoch: 30	Loss: 0.055326	Acc: 23.7% (2366/10000)
[Test]  Epoch: 31	Loss: 0.055378	Acc: 23.6% (2365/10000)
[Test]  Epoch: 32	Loss: 0.055295	Acc: 23.6% (2362/10000)
[Test]  Epoch: 33	Loss: 0.055200	Acc: 23.8% (2379/10000)
[Test]  Epoch: 34	Loss: 0.054956	Acc: 24.2% (2420/10000)
[Test]  Epoch: 35	Loss: 0.054837	Acc: 24.4% (2437/10000)
[Test]  Epoch: 36	Loss: 0.054911	Acc: 24.2% (2416/10000)
[Test]  Epoch: 37	Loss: 0.054825	Acc: 24.4% (2438/10000)
[Test]  Epoch: 38	Loss: 0.054562	Acc: 24.6% (2457/10000)
[Test]  Epoch: 39	Loss: 0.054590	Acc: 24.7% (2466/10000)
[Test]  Epoch: 40	Loss: 0.054511	Acc: 24.8% (2484/10000)
[Test]  Epoch: 41	Loss: 0.054351	Acc: 25.0% (2498/10000)
[Test]  Epoch: 42	Loss: 0.054345	Acc: 25.0% (2501/10000)
[Test]  Epoch: 43	Loss: 0.054199	Acc: 25.1% (2514/10000)
[Test]  Epoch: 44	Loss: 0.054313	Acc: 25.1% (2509/10000)
[Test]  Epoch: 45	Loss: 0.054096	Acc: 25.3% (2527/10000)
[Test]  Epoch: 46	Loss: 0.054161	Acc: 25.4% (2539/10000)
[Test]  Epoch: 47	Loss: 0.053993	Acc: 25.5% (2551/10000)
[Test]  Epoch: 48	Loss: 0.053918	Acc: 25.6% (2561/10000)
[Test]  Epoch: 49	Loss: 0.053896	Acc: 25.7% (2571/10000)
[Test]  Epoch: 50	Loss: 0.053868	Acc: 25.7% (2572/10000)
[Test]  Epoch: 51	Loss: 0.053800	Acc: 25.9% (2586/10000)
[Test]  Epoch: 52	Loss: 0.053769	Acc: 25.7% (2567/10000)
[Test]  Epoch: 53	Loss: 0.053718	Acc: 25.7% (2569/10000)
[Test]  Epoch: 54	Loss: 0.053768	Acc: 25.9% (2586/10000)
[Test]  Epoch: 55	Loss: 0.053650	Acc: 26.0% (2601/10000)
[Test]  Epoch: 56	Loss: 0.053627	Acc: 26.0% (2602/10000)
[Test]  Epoch: 57	Loss: 0.053595	Acc: 26.3% (2627/10000)
[Test]  Epoch: 58	Loss: 0.053362	Acc: 26.3% (2634/10000)
[Test]  Epoch: 59	Loss: 0.053463	Acc: 26.5% (2648/10000)
[Test]  Epoch: 60	Loss: 0.053458	Acc: 26.2% (2622/10000)
[Test]  Epoch: 61	Loss: 0.053486	Acc: 26.1% (2612/10000)
[Test]  Epoch: 62	Loss: 0.053452	Acc: 26.4% (2635/10000)
[Test]  Epoch: 63	Loss: 0.053434	Acc: 26.4% (2636/10000)
[Test]  Epoch: 64	Loss: 0.053375	Acc: 26.4% (2636/10000)
[Test]  Epoch: 65	Loss: 0.053466	Acc: 26.4% (2639/10000)
[Test]  Epoch: 66	Loss: 0.053424	Acc: 26.4% (2640/10000)
[Test]  Epoch: 67	Loss: 0.053431	Acc: 26.4% (2641/10000)
[Test]  Epoch: 68	Loss: 0.053454	Acc: 26.4% (2641/10000)
[Test]  Epoch: 69	Loss: 0.053328	Acc: 26.7% (2667/10000)
[Test]  Epoch: 70	Loss: 0.053361	Acc: 26.7% (2672/10000)
[Test]  Epoch: 71	Loss: 0.053476	Acc: 26.4% (2643/10000)
[Test]  Epoch: 72	Loss: 0.053476	Acc: 26.4% (2640/10000)
[Test]  Epoch: 73	Loss: 0.053359	Acc: 26.5% (2646/10000)
[Test]  Epoch: 74	Loss: 0.053302	Acc: 26.6% (2655/10000)
[Test]  Epoch: 75	Loss: 0.053334	Acc: 26.7% (2670/10000)
[Test]  Epoch: 76	Loss: 0.053376	Acc: 26.6% (2659/10000)
[Test]  Epoch: 77	Loss: 0.053311	Acc: 26.5% (2646/10000)
[Test]  Epoch: 78	Loss: 0.053367	Acc: 26.6% (2662/10000)
[Test]  Epoch: 79	Loss: 0.053272	Acc: 26.8% (2677/10000)
[Test]  Epoch: 80	Loss: 0.053407	Acc: 26.3% (2633/10000)
[Test]  Epoch: 81	Loss: 0.053369	Acc: 26.5% (2649/10000)
[Test]  Epoch: 82	Loss: 0.053383	Acc: 26.7% (2674/10000)
[Test]  Epoch: 83	Loss: 0.053309	Acc: 26.7% (2674/10000)
[Test]  Epoch: 84	Loss: 0.053336	Acc: 26.6% (2662/10000)
[Test]  Epoch: 85	Loss: 0.053334	Acc: 26.6% (2662/10000)
[Test]  Epoch: 86	Loss: 0.053334	Acc: 26.5% (2652/10000)
[Test]  Epoch: 87	Loss: 0.053365	Acc: 26.6% (2664/10000)
[Test]  Epoch: 88	Loss: 0.053356	Acc: 26.6% (2663/10000)
[Test]  Epoch: 89	Loss: 0.053360	Acc: 26.6% (2655/10000)
[Test]  Epoch: 90	Loss: 0.053319	Acc: 26.6% (2662/10000)
[Test]  Epoch: 91	Loss: 0.053271	Acc: 26.8% (2676/10000)
[Test]  Epoch: 92	Loss: 0.053283	Acc: 26.8% (2679/10000)
[Test]  Epoch: 93	Loss: 0.053300	Acc: 26.5% (2653/10000)
[Test]  Epoch: 94	Loss: 0.053306	Acc: 26.8% (2678/10000)
[Test]  Epoch: 95	Loss: 0.053335	Acc: 26.8% (2676/10000)
[Test]  Epoch: 96	Loss: 0.053240	Acc: 26.7% (2671/10000)
[Test]  Epoch: 97	Loss: 0.053344	Acc: 26.6% (2665/10000)
[Test]  Epoch: 98	Loss: 0.053386	Acc: 26.6% (2661/10000)
[Test]  Epoch: 99	Loss: 0.053214	Acc: 26.8% (2683/10000)
[Test]  Epoch: 100	Loss: 0.053243	Acc: 26.8% (2679/10000)
===========finish==========
['2024-08-19', '19:18:51.195023', '100', 'test', '0.05324288363456726', '26.79', '26.83']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/stl10-resnet50/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.0.bn3.bias', 0.0), ('layer1.0.downsample.1.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer1.1.bn3.bias', 0.0), ('layer1.2.bn1.bias', 0.0), ('layer1.2.bn2.bias', 0.0), ('layer1.2.bn3.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.bn3.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer2.1.bn3.bias', 0.0), ('layer2.2.bn1.bias', 0.0), ('layer2.2.bn2.bias', 0.0), ('layer2.2.bn3.bias', 0.0), ('layer2.3.bn1.bias', 0.0), ('layer2.3.bn2.bias', 0.0), ('layer2.3.bn3.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.bn3.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer3.1.bn3.bias', 0.0), ('layer3.2.bn1.bias', 0.0), ('layer3.2.bn2.bias', 0.0), ('layer3.2.bn3.bias', 0.0), ('layer3.3.bn1.bias', 0.0), ('layer3.3.bn2.bias', 0.0), ('layer3.3.bn3.bias', 0.0), ('layer3.4.bn1.bias', 0.0), ('layer3.4.bn2.bias', 0.0), ('layer3.4.bn3.bias', 0.0), ('layer3.5.bn1.bias', 0.0), ('layer3.5.bn2.bias', 0.0), ('layer3.5.bn3.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.bn3.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('layer4.1.bn3.bias', 0.0), ('layer4.2.bn1.bias', 0.0), ('layer4.2.bn2.bias', 0.0), ('layer4.2.bn3.bias', 0.0), ('last_linear.bias', 0.0), ('layer2.2.bn1.weight', -0.7237173318862915), ('layer3.2.bn1.weight', -1.0988800525665283), ('layer2.3.bn2.weight', -1.1693089008331299), ('layer2.2.bn2.weight', -1.2498680353164673), ('layer3.2.bn2.weight', -1.4569480419158936), ('layer3.3.bn1.weight', -2.380563259124756), ('layer3.4.bn1.weight', -2.4526541233062744), ('layer1.1.bn1.weight', -2.788163185119629), ('layer2.3.bn1.weight', -2.829146146774292), ('layer1.2.bn2.weight', -3.0546302795410156), ('layer1.2.bn1.weight', -3.070540428161621), ('layer3.1.bn1.weight', -3.1703062057495117), ('layer3.3.bn2.weight', -3.999899387359619), ('layer1.1.bn2.weight', -4.174070358276367), ('layer3.1.bn2.weight', -4.6949028968811035), ('layer3.4.bn2.weight', -5.105581283569336), ('layer2.1.bn1.weight', -5.822661399841309), ('layer2.0.bn1.weight', -6.9797868728637695), ('layer1.0.bn1.weight', -7.367038726806641), ('layer3.0.bn1.weight', -7.729199409484863), ('layer2.0.bn2.weight', -8.31962776184082), ('layer3.0.bn2.weight', -9.004953384399414), ('layer1.0.bn2.weight', -9.057568550109863), ('layer3.5.bn1.weight', -12.433037757873535), ('layer2.1.bn2.weight', -13.792065620422363), ('layer1.2.bn3.weight', -20.300914764404297), ('layer2.3.bn3.weight', -25.200756072998047), ('layer3.5.bn2.weight', -25.54277801513672), ('layer2.2.bn3.weight', -25.847808837890625), ('layer1.1.bn3.weight', -26.145925521850586), ('layer4.2.bn3.weight', -33.948299407958984), ('layer4.1.bn3.weight', -35.671363830566406), ('bn1.weight', -37.420108795166016), ('layer1.0.downsample.1.weight', -43.97019577026367), ('layer1.0.bn3.weight', -45.22014236450195), ('layer4.2.bn2.weight', -46.0751953125), ('layer2.0.bn3.weight', -48.822784423828125), ('layer2.1.bn3.weight', -49.05659103393555), ('layer4.2.bn1.weight', -52.09326171875), ('layer3.2.bn3.weight', -54.302120208740234), ('layer2.0.downsample.1.weight', -57.42535400390625), ('layer3.3.bn3.weight', -65.17198944091797), ('layer3.1.bn3.weight', -74.48711395263672), ('layer4.1.bn1.weight', -79.37942504882812), ('layer4.0.downsample.1.weight', -104.35282897949219), ('layer3.4.bn3.weight', -106.99478912353516), ('layer3.0.bn3.weight', -111.55204010009766), ('layer4.1.bn2.weight', -128.38597106933594), ('layer4.0.bn3.weight', -131.26792907714844), ('layer3.5.bn3.weight', -160.02496337890625), ('layer3.0.downsample.1.weight', -227.84445190429688), ('layer4.0.bn2.weight', -296.3879699707031), ('layer2.2.conv1.weight', -2276.932861328125), ('layer4.0.bn1.weight', -3277.57080078125), ('layer1.2.conv3.weight', -5786.7861328125), ('layer2.2.conv2.weight', -6608.0703125), ('layer1.1.conv3.weight', -6878.0751953125), ('layer1.0.conv1.weight', -7185.91015625), ('layer1.1.conv1.weight', -7224.2392578125), ('layer1.2.conv1.weight', -10160.3349609375), ('layer2.3.conv3.weight', -10566.572265625), ('layer1.2.conv2.weight', -12138.326171875), ('layer2.2.conv3.weight', -12922.041015625), ('layer2.3.conv2.weight', -13926.005859375), ('layer1.0.conv3.weight', -16247.701171875), ('layer3.2.conv1.weight', -16262.333984375), ('layer1.1.conv2.weight', -17021.650390625), ('layer2.1.conv1.weight', -17163.0234375), ('layer2.3.conv1.weight', -21690.6640625), ('layer2.0.conv1.weight', -21707.63671875), ('layer1.0.conv2.weight', -24716.744140625), ('layer3.1.conv1.weight', -26659.1875), ('conv1.weight', -27123.625), ('layer2.1.conv3.weight', -27854.580078125), ('layer2.0.conv3.weight', -28777.607421875), ('layer3.2.conv2.weight', -29073.59375), ('layer3.3.conv1.weight', -30495.26171875), ('layer3.4.conv1.weight', -36438.51953125), ('layer2.0.conv2.weight', -38693.17578125), ('layer2.1.conv2.weight', -41922.15234375), ('layer1.0.downsample.0.weight', -55448.65234375), ('last_linear.weight', -67714.40625), ('layer3.2.conv3.weight', -68444.6875), ('layer3.0.conv1.weight', -71080.75), ('layer3.3.conv2.weight', -73002.0859375), ('layer3.3.conv3.weight', -81342.296875), ('layer3.1.conv2.weight', -98464.953125), ('layer3.4.conv2.weight', -103731.921875), ('layer3.0.conv2.weight', -112268.125), ('layer3.1.conv3.weight', -132506.09375), ('layer3.4.conv3.weight', -157957.046875), ('layer3.0.conv3.weight', -213080.84375), ('layer3.5.conv1.weight', -235876.234375), ('layer2.0.downsample.0.weight', -439199.9375), ('layer3.5.conv3.weight', -472382.25), ('layer3.5.conv2.weight', -718073.875), ('layer4.2.conv1.weight', -1610462.875), ('layer3.0.downsample.0.weight', -2117912.75), ('layer4.1.conv1.weight', -3048237.25), ('layer4.1.conv3.weight', -3333360.25), ('layer4.2.conv2.weight', -4121852.25), ('layer4.2.conv3.weight', -5311876.0), ('layer4.0.conv1.weight', -5726023.0), ('layer4.1.conv2.weight', -6028050.5), ('layer4.0.conv3.weight', -6986964.5), ('layer4.0.downsample.0.weight', -9078660.0), ('layer4.0.conv2.weight', -16936980.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('last_linear.bias', 0.0), ('layer2.2.conv1.weight', -2276.932861328125), ('layer1.2.conv3.weight', -5786.7861328125), ('layer2.2.conv2.weight', -6608.0703125), ('layer1.1.conv3.weight', -6878.0751953125), ('layer1.0.conv1.weight', -7185.91015625), ('layer1.1.conv1.weight', -7224.2392578125), ('layer1.2.conv1.weight', -10160.3349609375), ('layer2.3.conv3.weight', -10566.572265625), ('layer1.2.conv2.weight', -12138.326171875), ('layer2.2.conv3.weight', -12922.041015625), ('layer2.3.conv2.weight', -13926.005859375), ('layer1.0.conv3.weight', -16247.701171875), ('layer3.2.conv1.weight', -16262.333984375), ('layer1.1.conv2.weight', -17021.650390625), ('layer2.1.conv1.weight', -17163.0234375), ('layer2.3.conv1.weight', -21690.6640625), ('layer2.0.conv1.weight', -21707.63671875), ('layer1.0.conv2.weight', -24716.744140625), ('layer3.1.conv1.weight', -26659.1875), ('conv1.weight', -27123.625), ('layer2.1.conv3.weight', -27854.580078125), ('layer2.0.conv3.weight', -28777.607421875), ('layer3.2.conv2.weight', -29073.59375), ('layer3.3.conv1.weight', -30495.26171875), ('layer3.4.conv1.weight', -36438.51953125), ('layer2.0.conv2.weight', -38693.17578125), ('layer2.1.conv2.weight', -41922.15234375), ('last_linear.weight', -67714.40625), ('layer3.2.conv3.weight', -68444.6875), ('layer3.0.conv1.weight', -71080.75), ('layer3.3.conv2.weight', -73002.0859375), ('layer3.3.conv3.weight', -81342.296875), ('layer3.1.conv2.weight', -98464.953125), ('layer3.4.conv2.weight', -103731.921875), ('layer3.0.conv2.weight', -112268.125), ('layer3.1.conv3.weight', -132506.09375), ('layer3.4.conv3.weight', -157957.046875), ('layer3.0.conv3.weight', -213080.84375), ('layer3.5.conv1.weight', -235876.234375), ('layer3.5.conv3.weight', -472382.25), ('layer3.5.conv2.weight', -718073.875), ('layer4.2.conv1.weight', -1610462.875), ('layer4.1.conv1.weight', -3048237.25), ('layer4.1.conv3.weight', -3333360.25), ('layer4.2.conv2.weight', -4121852.25), ('layer4.2.conv3.weight', -5311876.0), ('layer4.0.conv1.weight', -5726023.0), ('layer4.1.conv2.weight', -6028050.5), ('layer4.0.conv3.weight', -6986964.5), ('layer4.0.conv2.weight', -16936980.0)]
--------------------------------------------
get_sample_layers n=107  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []
[]
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.014417	Acc: 67.0% (5357/8000)
[Test]  Epoch: 2	Loss: 0.014409	Acc: 67.2% (5373/8000)
[Test]  Epoch: 3	Loss: 0.014490	Acc: 67.2% (5376/8000)
[Test]  Epoch: 4	Loss: 0.014469	Acc: 66.8% (5341/8000)
[Test]  Epoch: 5	Loss: 0.014454	Acc: 67.0% (5359/8000)
[Test]  Epoch: 6	Loss: 0.014465	Acc: 67.1% (5369/8000)
[Test]  Epoch: 7	Loss: 0.014400	Acc: 67.0% (5357/8000)
[Test]  Epoch: 8	Loss: 0.014355	Acc: 67.3% (5381/8000)
[Test]  Epoch: 9	Loss: 0.014382	Acc: 66.8% (5345/8000)
[Test]  Epoch: 10	Loss: 0.014339	Acc: 67.2% (5380/8000)
[Test]  Epoch: 11	Loss: 0.014438	Acc: 66.9% (5352/8000)
[Test]  Epoch: 12	Loss: 0.014292	Acc: 67.4% (5395/8000)
[Test]  Epoch: 13	Loss: 0.014323	Acc: 67.4% (5392/8000)
[Test]  Epoch: 14	Loss: 0.014308	Acc: 67.2% (5378/8000)
[Test]  Epoch: 15	Loss: 0.014384	Acc: 67.1% (5371/8000)
[Test]  Epoch: 16	Loss: 0.014338	Acc: 67.1% (5366/8000)
[Test]  Epoch: 17	Loss: 0.014299	Acc: 67.2% (5378/8000)
[Test]  Epoch: 18	Loss: 0.014374	Acc: 66.9% (5355/8000)
[Test]  Epoch: 19	Loss: 0.014367	Acc: 67.2% (5375/8000)
[Test]  Epoch: 20	Loss: 0.014452	Acc: 66.8% (5346/8000)
[Test]  Epoch: 21	Loss: 0.014348	Acc: 67.3% (5387/8000)
[Test]  Epoch: 22	Loss: 0.014377	Acc: 67.5% (5400/8000)
[Test]  Epoch: 23	Loss: 0.014273	Acc: 67.6% (5405/8000)
[Test]  Epoch: 24	Loss: 0.014321	Acc: 67.1% (5366/8000)
[Test]  Epoch: 25	Loss: 0.014419	Acc: 67.0% (5356/8000)
[Test]  Epoch: 26	Loss: 0.014324	Acc: 67.3% (5387/8000)
[Test]  Epoch: 27	Loss: 0.014315	Acc: 67.5% (5399/8000)
[Test]  Epoch: 28	Loss: 0.014321	Acc: 67.2% (5376/8000)
[Test]  Epoch: 29	Loss: 0.014320	Acc: 67.3% (5388/8000)
[Test]  Epoch: 30	Loss: 0.014340	Acc: 67.0% (5360/8000)
[Test]  Epoch: 31	Loss: 0.014312	Acc: 67.3% (5381/8000)
[Test]  Epoch: 32	Loss: 0.014280	Acc: 67.4% (5390/8000)
[Test]  Epoch: 33	Loss: 0.014321	Acc: 67.2% (5374/8000)
[Test]  Epoch: 34	Loss: 0.014253	Acc: 67.5% (5400/8000)
[Test]  Epoch: 35	Loss: 0.014270	Acc: 67.3% (5385/8000)
[Test]  Epoch: 36	Loss: 0.014243	Acc: 67.2% (5373/8000)
[Test]  Epoch: 37	Loss: 0.014269	Acc: 67.4% (5394/8000)
[Test]  Epoch: 38	Loss: 0.014340	Acc: 67.3% (5383/8000)
[Test]  Epoch: 39	Loss: 0.014278	Acc: 67.3% (5382/8000)
[Test]  Epoch: 40	Loss: 0.014296	Acc: 67.0% (5361/8000)
[Test]  Epoch: 41	Loss: 0.014245	Acc: 67.0% (5361/8000)
[Test]  Epoch: 42	Loss: 0.014339	Acc: 67.5% (5396/8000)
[Test]  Epoch: 43	Loss: 0.014297	Acc: 67.3% (5383/8000)
[Test]  Epoch: 44	Loss: 0.014275	Acc: 67.7% (5417/8000)
[Test]  Epoch: 45	Loss: 0.014281	Acc: 67.3% (5384/8000)
[Test]  Epoch: 46	Loss: 0.014303	Acc: 67.5% (5396/8000)
[Test]  Epoch: 47	Loss: 0.014293	Acc: 67.5% (5402/8000)
[Test]  Epoch: 48	Loss: 0.014324	Acc: 67.4% (5390/8000)
[Test]  Epoch: 49	Loss: 0.014282	Acc: 67.5% (5399/8000)
[Test]  Epoch: 50	Loss: 0.014269	Acc: 67.5% (5399/8000)
[Test]  Epoch: 51	Loss: 0.014295	Acc: 67.3% (5387/8000)
[Test]  Epoch: 52	Loss: 0.014303	Acc: 67.2% (5372/8000)
[Test]  Epoch: 53	Loss: 0.014275	Acc: 67.2% (5377/8000)
[Test]  Epoch: 54	Loss: 0.014315	Acc: 67.5% (5396/8000)
[Test]  Epoch: 55	Loss: 0.014305	Acc: 67.4% (5389/8000)
[Test]  Epoch: 56	Loss: 0.014291	Acc: 67.3% (5388/8000)
[Test]  Epoch: 57	Loss: 0.014244	Acc: 67.2% (5372/8000)
[Test]  Epoch: 58	Loss: 0.014280	Acc: 67.5% (5396/8000)
[Test]  Epoch: 59	Loss: 0.014313	Acc: 67.2% (5378/8000)
[Test]  Epoch: 60	Loss: 0.014324	Acc: 67.5% (5400/8000)
[Test]  Epoch: 61	Loss: 0.014311	Acc: 67.6% (5407/8000)
[Test]  Epoch: 62	Loss: 0.014310	Acc: 67.3% (5386/8000)
[Test]  Epoch: 63	Loss: 0.014288	Acc: 67.5% (5399/8000)
[Test]  Epoch: 64	Loss: 0.014290	Acc: 67.5% (5402/8000)
[Test]  Epoch: 65	Loss: 0.014280	Acc: 67.5% (5397/8000)
[Test]  Epoch: 66	Loss: 0.014290	Acc: 67.4% (5395/8000)
[Test]  Epoch: 67	Loss: 0.014292	Acc: 67.3% (5388/8000)
[Test]  Epoch: 68	Loss: 0.014291	Acc: 67.5% (5397/8000)
[Test]  Epoch: 69	Loss: 0.014271	Acc: 67.6% (5405/8000)
[Test]  Epoch: 70	Loss: 0.014298	Acc: 67.4% (5393/8000)
[Test]  Epoch: 71	Loss: 0.014280	Acc: 67.5% (5402/8000)
[Test]  Epoch: 72	Loss: 0.014278	Acc: 67.5% (5396/8000)
[Test]  Epoch: 73	Loss: 0.014275	Acc: 67.3% (5387/8000)
[Test]  Epoch: 74	Loss: 0.014279	Acc: 67.5% (5397/8000)
[Test]  Epoch: 75	Loss: 0.014278	Acc: 67.5% (5396/8000)
[Test]  Epoch: 76	Loss: 0.014293	Acc: 67.1% (5368/8000)
[Test]  Epoch: 77	Loss: 0.014275	Acc: 67.3% (5385/8000)
[Test]  Epoch: 78	Loss: 0.014289	Acc: 67.2% (5378/8000)
[Test]  Epoch: 79	Loss: 0.014281	Acc: 67.5% (5400/8000)
[Test]  Epoch: 80	Loss: 0.014274	Acc: 67.4% (5393/8000)
[Test]  Epoch: 81	Loss: 0.014276	Acc: 67.4% (5390/8000)
[Test]  Epoch: 82	Loss: 0.014348	Acc: 67.2% (5377/8000)
[Test]  Epoch: 83	Loss: 0.014273	Acc: 67.2% (5380/8000)
[Test]  Epoch: 84	Loss: 0.014283	Acc: 67.2% (5376/8000)
[Test]  Epoch: 85	Loss: 0.014281	Acc: 67.3% (5388/8000)
[Test]  Epoch: 86	Loss: 0.014290	Acc: 67.5% (5396/8000)
[Test]  Epoch: 87	Loss: 0.014289	Acc: 67.3% (5385/8000)
[Test]  Epoch: 88	Loss: 0.014284	Acc: 67.3% (5385/8000)
[Test]  Epoch: 89	Loss: 0.014281	Acc: 67.5% (5401/8000)
[Test]  Epoch: 90	Loss: 0.014283	Acc: 67.2% (5377/8000)
[Test]  Epoch: 91	Loss: 0.014299	Acc: 67.2% (5380/8000)
[Test]  Epoch: 92	Loss: 0.014287	Acc: 67.2% (5375/8000)
[Test]  Epoch: 93	Loss: 0.014267	Acc: 67.4% (5389/8000)
[Test]  Epoch: 94	Loss: 0.014275	Acc: 67.4% (5391/8000)
[Test]  Epoch: 95	Loss: 0.014272	Acc: 67.3% (5385/8000)
[Test]  Epoch: 96	Loss: 0.014257	Acc: 67.3% (5384/8000)
[Test]  Epoch: 97	Loss: 0.014287	Acc: 67.4% (5395/8000)
[Test]  Epoch: 98	Loss: 0.014280	Acc: 67.5% (5402/8000)
[Test]  Epoch: 99	Loss: 0.014283	Acc: 67.3% (5388/8000)
[Test]  Epoch: 100	Loss: 0.014281	Acc: 67.6% (5411/8000)
===========finish==========
['2024-08-19', '19:27:32.554767', '100', 'test', '0.014281196713447571', '67.6375', '67.7125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=10
get_sample_layers not_random
protect_percent = 0.1 10 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.015807	Acc: 64.4% (5151/8000)
[Test]  Epoch: 2	Loss: 0.014945	Acc: 66.0% (5276/8000)
[Test]  Epoch: 3	Loss: 0.014946	Acc: 65.9% (5274/8000)
[Test]  Epoch: 4	Loss: 0.014970	Acc: 65.6% (5245/8000)
[Test]  Epoch: 5	Loss: 0.014808	Acc: 66.0% (5282/8000)
[Test]  Epoch: 6	Loss: 0.014788	Acc: 66.0% (5278/8000)
[Test]  Epoch: 7	Loss: 0.014703	Acc: 66.0% (5284/8000)
[Test]  Epoch: 8	Loss: 0.014657	Acc: 66.3% (5308/8000)
[Test]  Epoch: 9	Loss: 0.014624	Acc: 66.2% (5294/8000)
[Test]  Epoch: 10	Loss: 0.014634	Acc: 66.5% (5320/8000)
[Test]  Epoch: 11	Loss: 0.014728	Acc: 66.3% (5302/8000)
[Test]  Epoch: 12	Loss: 0.014545	Acc: 66.7% (5332/8000)
[Test]  Epoch: 13	Loss: 0.014589	Acc: 66.8% (5342/8000)
[Test]  Epoch: 14	Loss: 0.014565	Acc: 66.4% (5313/8000)
[Test]  Epoch: 15	Loss: 0.014586	Acc: 66.5% (5321/8000)
[Test]  Epoch: 16	Loss: 0.014596	Acc: 66.6% (5327/8000)
[Test]  Epoch: 17	Loss: 0.014532	Acc: 66.7% (5332/8000)
[Test]  Epoch: 18	Loss: 0.014635	Acc: 66.6% (5329/8000)
[Test]  Epoch: 19	Loss: 0.014595	Acc: 66.8% (5347/8000)
[Test]  Epoch: 20	Loss: 0.014662	Acc: 66.3% (5306/8000)
[Test]  Epoch: 21	Loss: 0.014570	Acc: 66.8% (5340/8000)
[Test]  Epoch: 22	Loss: 0.014596	Acc: 66.6% (5327/8000)
[Test]  Epoch: 23	Loss: 0.014473	Acc: 66.7% (5338/8000)
[Test]  Epoch: 24	Loss: 0.014517	Acc: 66.8% (5344/8000)
[Test]  Epoch: 25	Loss: 0.014662	Acc: 66.3% (5306/8000)
[Test]  Epoch: 26	Loss: 0.014524	Acc: 66.6% (5328/8000)
[Test]  Epoch: 27	Loss: 0.014485	Acc: 66.9% (5354/8000)
[Test]  Epoch: 28	Loss: 0.014536	Acc: 66.7% (5339/8000)
[Test]  Epoch: 29	Loss: 0.014518	Acc: 66.7% (5336/8000)
[Test]  Epoch: 30	Loss: 0.014568	Acc: 66.5% (5317/8000)
[Test]  Epoch: 31	Loss: 0.014532	Acc: 66.7% (5335/8000)
[Test]  Epoch: 32	Loss: 0.014488	Acc: 66.6% (5328/8000)
[Test]  Epoch: 33	Loss: 0.014503	Acc: 66.6% (5331/8000)
[Test]  Epoch: 34	Loss: 0.014437	Acc: 66.8% (5344/8000)
[Test]  Epoch: 35	Loss: 0.014486	Acc: 66.7% (5339/8000)
[Test]  Epoch: 36	Loss: 0.014424	Acc: 66.8% (5344/8000)
[Test]  Epoch: 37	Loss: 0.014472	Acc: 66.8% (5340/8000)
[Test]  Epoch: 38	Loss: 0.014508	Acc: 66.5% (5323/8000)
[Test]  Epoch: 39	Loss: 0.014450	Acc: 66.7% (5337/8000)
[Test]  Epoch: 40	Loss: 0.014465	Acc: 66.7% (5339/8000)
[Test]  Epoch: 41	Loss: 0.014447	Acc: 66.6% (5331/8000)
[Test]  Epoch: 42	Loss: 0.014515	Acc: 66.7% (5339/8000)
[Test]  Epoch: 43	Loss: 0.014471	Acc: 66.7% (5339/8000)
[Test]  Epoch: 44	Loss: 0.014452	Acc: 66.8% (5347/8000)
[Test]  Epoch: 45	Loss: 0.014474	Acc: 66.7% (5333/8000)
[Test]  Epoch: 46	Loss: 0.014498	Acc: 66.8% (5341/8000)
[Test]  Epoch: 47	Loss: 0.014471	Acc: 66.8% (5340/8000)
[Test]  Epoch: 48	Loss: 0.014505	Acc: 66.6% (5328/8000)
[Test]  Epoch: 49	Loss: 0.014475	Acc: 66.8% (5345/8000)
[Test]  Epoch: 50	Loss: 0.014453	Acc: 66.8% (5346/8000)
[Test]  Epoch: 51	Loss: 0.014474	Acc: 66.5% (5323/8000)
[Test]  Epoch: 52	Loss: 0.014484	Acc: 66.6% (5330/8000)
[Test]  Epoch: 53	Loss: 0.014459	Acc: 66.8% (5347/8000)
[Test]  Epoch: 54	Loss: 0.014491	Acc: 66.8% (5341/8000)
[Test]  Epoch: 55	Loss: 0.014469	Acc: 66.8% (5346/8000)
[Test]  Epoch: 56	Loss: 0.014464	Acc: 66.9% (5349/8000)
[Test]  Epoch: 57	Loss: 0.014423	Acc: 66.7% (5336/8000)
[Test]  Epoch: 58	Loss: 0.014476	Acc: 66.7% (5337/8000)
[Test]  Epoch: 59	Loss: 0.014491	Acc: 66.7% (5335/8000)
[Test]  Epoch: 60	Loss: 0.014495	Acc: 66.9% (5350/8000)
[Test]  Epoch: 61	Loss: 0.014486	Acc: 66.7% (5332/8000)
[Test]  Epoch: 62	Loss: 0.014485	Acc: 66.7% (5332/8000)
[Test]  Epoch: 63	Loss: 0.014469	Acc: 66.7% (5338/8000)
[Test]  Epoch: 64	Loss: 0.014467	Acc: 66.8% (5340/8000)
[Test]  Epoch: 65	Loss: 0.014455	Acc: 66.9% (5354/8000)
[Test]  Epoch: 66	Loss: 0.014473	Acc: 66.9% (5355/8000)
[Test]  Epoch: 67	Loss: 0.014473	Acc: 66.8% (5345/8000)
[Test]  Epoch: 68	Loss: 0.014474	Acc: 66.7% (5333/8000)
[Test]  Epoch: 69	Loss: 0.014449	Acc: 66.8% (5344/8000)
[Test]  Epoch: 70	Loss: 0.014470	Acc: 66.8% (5345/8000)
[Test]  Epoch: 71	Loss: 0.014458	Acc: 66.9% (5355/8000)
[Test]  Epoch: 72	Loss: 0.014452	Acc: 66.8% (5342/8000)
[Test]  Epoch: 73	Loss: 0.014451	Acc: 66.8% (5343/8000)
[Test]  Epoch: 74	Loss: 0.014463	Acc: 66.8% (5347/8000)
[Test]  Epoch: 75	Loss: 0.014458	Acc: 67.0% (5357/8000)
[Test]  Epoch: 76	Loss: 0.014475	Acc: 66.8% (5348/8000)
[Test]  Epoch: 77	Loss: 0.014457	Acc: 66.8% (5345/8000)
[Test]  Epoch: 78	Loss: 0.014467	Acc: 66.8% (5347/8000)
[Test]  Epoch: 79	Loss: 0.014455	Acc: 67.0% (5361/8000)
[Test]  Epoch: 80	Loss: 0.014449	Acc: 66.9% (5350/8000)
[Test]  Epoch: 81	Loss: 0.014456	Acc: 66.7% (5333/8000)
[Test]  Epoch: 82	Loss: 0.014523	Acc: 66.8% (5341/8000)
[Test]  Epoch: 83	Loss: 0.014455	Acc: 66.8% (5347/8000)
[Test]  Epoch: 84	Loss: 0.014468	Acc: 66.7% (5339/8000)
[Test]  Epoch: 85	Loss: 0.014459	Acc: 66.8% (5346/8000)
[Test]  Epoch: 86	Loss: 0.014467	Acc: 66.8% (5345/8000)
[Test]  Epoch: 87	Loss: 0.014468	Acc: 66.7% (5336/8000)
[Test]  Epoch: 88	Loss: 0.014459	Acc: 66.9% (5350/8000)
[Test]  Epoch: 89	Loss: 0.014455	Acc: 66.8% (5347/8000)
[Test]  Epoch: 90	Loss: 0.014464	Acc: 66.8% (5341/8000)
[Test]  Epoch: 91	Loss: 0.014475	Acc: 66.7% (5334/8000)
[Test]  Epoch: 92	Loss: 0.014471	Acc: 66.8% (5346/8000)
[Test]  Epoch: 93	Loss: 0.014447	Acc: 67.1% (5370/8000)
[Test]  Epoch: 94	Loss: 0.014453	Acc: 66.9% (5353/8000)
[Test]  Epoch: 95	Loss: 0.014448	Acc: 66.8% (5342/8000)
[Test]  Epoch: 96	Loss: 0.014431	Acc: 66.8% (5348/8000)
[Test]  Epoch: 97	Loss: 0.014462	Acc: 67.0% (5362/8000)
[Test]  Epoch: 98	Loss: 0.014456	Acc: 66.9% (5353/8000)
[Test]  Epoch: 99	Loss: 0.014447	Acc: 67.0% (5358/8000)
[Test]  Epoch: 100	Loss: 0.014453	Acc: 66.9% (5353/8000)
===========finish==========
['2024-08-19', '19:36:58.837235', '100', 'test', '0.014452829875051976', '66.9125', '67.125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=21
get_sample_layers not_random
protect_percent = 0.2 21 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.017819	Acc: 59.9% (4794/8000)
[Test]  Epoch: 2	Loss: 0.015401	Acc: 65.1% (5210/8000)
[Test]  Epoch: 3	Loss: 0.015097	Acc: 65.6% (5246/8000)
[Test]  Epoch: 4	Loss: 0.015128	Acc: 65.3% (5223/8000)
[Test]  Epoch: 5	Loss: 0.014966	Acc: 65.7% (5255/8000)
[Test]  Epoch: 6	Loss: 0.015016	Acc: 65.2% (5218/8000)
[Test]  Epoch: 7	Loss: 0.014868	Acc: 65.7% (5257/8000)
[Test]  Epoch: 8	Loss: 0.014778	Acc: 65.9% (5273/8000)
[Test]  Epoch: 9	Loss: 0.014858	Acc: 65.7% (5252/8000)
[Test]  Epoch: 10	Loss: 0.014728	Acc: 65.9% (5273/8000)
[Test]  Epoch: 11	Loss: 0.014828	Acc: 65.7% (5259/8000)
[Test]  Epoch: 12	Loss: 0.014689	Acc: 66.4% (5310/8000)
[Test]  Epoch: 13	Loss: 0.014735	Acc: 66.2% (5294/8000)
[Test]  Epoch: 14	Loss: 0.014672	Acc: 66.3% (5304/8000)
[Test]  Epoch: 15	Loss: 0.014728	Acc: 66.1% (5289/8000)
[Test]  Epoch: 16	Loss: 0.014697	Acc: 66.1% (5291/8000)
[Test]  Epoch: 17	Loss: 0.014657	Acc: 66.5% (5323/8000)
[Test]  Epoch: 18	Loss: 0.014755	Acc: 66.2% (5300/8000)
[Test]  Epoch: 19	Loss: 0.014682	Acc: 66.5% (5319/8000)
[Test]  Epoch: 20	Loss: 0.014747	Acc: 66.0% (5277/8000)
[Test]  Epoch: 21	Loss: 0.014667	Acc: 66.5% (5323/8000)
[Test]  Epoch: 22	Loss: 0.014680	Acc: 66.2% (5292/8000)
[Test]  Epoch: 23	Loss: 0.014583	Acc: 66.6% (5327/8000)
[Test]  Epoch: 24	Loss: 0.014619	Acc: 66.5% (5319/8000)
[Test]  Epoch: 25	Loss: 0.014791	Acc: 66.4% (5312/8000)
[Test]  Epoch: 26	Loss: 0.014630	Acc: 66.5% (5322/8000)
[Test]  Epoch: 27	Loss: 0.014618	Acc: 66.5% (5319/8000)
[Test]  Epoch: 28	Loss: 0.014626	Acc: 66.5% (5317/8000)
[Test]  Epoch: 29	Loss: 0.014630	Acc: 66.4% (5313/8000)
[Test]  Epoch: 30	Loss: 0.014625	Acc: 66.6% (5328/8000)
[Test]  Epoch: 31	Loss: 0.014634	Acc: 66.2% (5295/8000)
[Test]  Epoch: 32	Loss: 0.014605	Acc: 66.3% (5307/8000)
[Test]  Epoch: 33	Loss: 0.014571	Acc: 66.6% (5329/8000)
[Test]  Epoch: 34	Loss: 0.014553	Acc: 66.8% (5346/8000)
[Test]  Epoch: 35	Loss: 0.014588	Acc: 66.6% (5327/8000)
[Test]  Epoch: 36	Loss: 0.014516	Acc: 66.8% (5348/8000)
[Test]  Epoch: 37	Loss: 0.014570	Acc: 66.4% (5314/8000)
[Test]  Epoch: 38	Loss: 0.014626	Acc: 66.5% (5320/8000)
[Test]  Epoch: 39	Loss: 0.014535	Acc: 66.8% (5345/8000)
[Test]  Epoch: 40	Loss: 0.014546	Acc: 66.8% (5340/8000)
[Test]  Epoch: 41	Loss: 0.014532	Acc: 66.7% (5335/8000)
[Test]  Epoch: 42	Loss: 0.014622	Acc: 66.5% (5324/8000)
[Test]  Epoch: 43	Loss: 0.014558	Acc: 66.8% (5345/8000)
[Test]  Epoch: 44	Loss: 0.014558	Acc: 66.8% (5341/8000)
[Test]  Epoch: 45	Loss: 0.014558	Acc: 66.7% (5337/8000)
[Test]  Epoch: 46	Loss: 0.014603	Acc: 66.3% (5305/8000)
[Test]  Epoch: 47	Loss: 0.014573	Acc: 66.7% (5336/8000)
[Test]  Epoch: 48	Loss: 0.014603	Acc: 66.3% (5306/8000)
[Test]  Epoch: 49	Loss: 0.014563	Acc: 66.4% (5315/8000)
[Test]  Epoch: 50	Loss: 0.014531	Acc: 66.8% (5346/8000)
[Test]  Epoch: 51	Loss: 0.014550	Acc: 66.8% (5341/8000)
[Test]  Epoch: 52	Loss: 0.014573	Acc: 66.6% (5330/8000)
[Test]  Epoch: 53	Loss: 0.014547	Acc: 66.8% (5345/8000)
[Test]  Epoch: 54	Loss: 0.014583	Acc: 66.5% (5316/8000)
[Test]  Epoch: 55	Loss: 0.014558	Acc: 66.7% (5335/8000)
[Test]  Epoch: 56	Loss: 0.014564	Acc: 66.7% (5339/8000)
[Test]  Epoch: 57	Loss: 0.014506	Acc: 66.7% (5333/8000)
[Test]  Epoch: 58	Loss: 0.014558	Acc: 66.8% (5345/8000)
[Test]  Epoch: 59	Loss: 0.014574	Acc: 66.6% (5331/8000)
[Test]  Epoch: 60	Loss: 0.014577	Acc: 66.8% (5341/8000)
[Test]  Epoch: 61	Loss: 0.014575	Acc: 66.5% (5323/8000)
[Test]  Epoch: 62	Loss: 0.014569	Acc: 66.8% (5341/8000)
[Test]  Epoch: 63	Loss: 0.014550	Acc: 66.8% (5346/8000)
[Test]  Epoch: 64	Loss: 0.014557	Acc: 66.8% (5344/8000)
[Test]  Epoch: 65	Loss: 0.014548	Acc: 66.8% (5345/8000)
[Test]  Epoch: 66	Loss: 0.014558	Acc: 66.9% (5349/8000)
[Test]  Epoch: 67	Loss: 0.014558	Acc: 66.9% (5349/8000)
[Test]  Epoch: 68	Loss: 0.014567	Acc: 66.7% (5336/8000)
[Test]  Epoch: 69	Loss: 0.014538	Acc: 67.0% (5359/8000)
[Test]  Epoch: 70	Loss: 0.014558	Acc: 67.0% (5356/8000)
[Test]  Epoch: 71	Loss: 0.014542	Acc: 67.0% (5359/8000)
[Test]  Epoch: 72	Loss: 0.014533	Acc: 67.0% (5361/8000)
[Test]  Epoch: 73	Loss: 0.014536	Acc: 67.0% (5359/8000)
[Test]  Epoch: 74	Loss: 0.014550	Acc: 66.8% (5348/8000)
[Test]  Epoch: 75	Loss: 0.014542	Acc: 67.0% (5359/8000)
[Test]  Epoch: 76	Loss: 0.014552	Acc: 66.8% (5341/8000)
[Test]  Epoch: 77	Loss: 0.014538	Acc: 66.9% (5350/8000)
[Test]  Epoch: 78	Loss: 0.014548	Acc: 66.8% (5346/8000)
[Test]  Epoch: 79	Loss: 0.014546	Acc: 66.8% (5348/8000)
[Test]  Epoch: 80	Loss: 0.014538	Acc: 66.9% (5354/8000)
[Test]  Epoch: 81	Loss: 0.014539	Acc: 67.1% (5365/8000)
[Test]  Epoch: 82	Loss: 0.014613	Acc: 66.5% (5316/8000)
[Test]  Epoch: 83	Loss: 0.014535	Acc: 67.0% (5357/8000)
[Test]  Epoch: 84	Loss: 0.014550	Acc: 66.7% (5336/8000)
[Test]  Epoch: 85	Loss: 0.014549	Acc: 66.8% (5345/8000)
[Test]  Epoch: 86	Loss: 0.014557	Acc: 66.8% (5343/8000)
[Test]  Epoch: 87	Loss: 0.014557	Acc: 66.8% (5343/8000)
[Test]  Epoch: 88	Loss: 0.014537	Acc: 66.9% (5354/8000)
[Test]  Epoch: 89	Loss: 0.014544	Acc: 66.8% (5347/8000)
[Test]  Epoch: 90	Loss: 0.014545	Acc: 66.9% (5351/8000)
[Test]  Epoch: 91	Loss: 0.014560	Acc: 66.9% (5350/8000)
[Test]  Epoch: 92	Loss: 0.014552	Acc: 66.9% (5352/8000)
[Test]  Epoch: 93	Loss: 0.014532	Acc: 67.0% (5358/8000)
[Test]  Epoch: 94	Loss: 0.014540	Acc: 66.9% (5353/8000)
[Test]  Epoch: 95	Loss: 0.014530	Acc: 66.8% (5347/8000)
[Test]  Epoch: 96	Loss: 0.014510	Acc: 66.8% (5348/8000)
[Test]  Epoch: 97	Loss: 0.014551	Acc: 66.9% (5352/8000)
[Test]  Epoch: 98	Loss: 0.014540	Acc: 67.2% (5373/8000)
[Test]  Epoch: 99	Loss: 0.014534	Acc: 67.0% (5356/8000)
[Test]  Epoch: 100	Loss: 0.014535	Acc: 67.0% (5356/8000)
===========finish==========
['2024-08-19', '19:46:21.647476', '100', 'test', '0.014535130999982358', '66.95', '67.1625']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=32
get_sample_layers not_random
protect_percent = 0.3 32 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight']
conv1.weight 1111111111
bn1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.020980	Acc: 52.5% (4199/8000)
[Test]  Epoch: 2	Loss: 0.016810	Acc: 61.8% (4943/8000)
[Test]  Epoch: 3	Loss: 0.016233	Acc: 63.1% (5046/8000)
[Test]  Epoch: 4	Loss: 0.015972	Acc: 63.4% (5072/8000)
[Test]  Epoch: 5	Loss: 0.015887	Acc: 64.0% (5118/8000)
[Test]  Epoch: 6	Loss: 0.015829	Acc: 63.6% (5085/8000)
[Test]  Epoch: 7	Loss: 0.015609	Acc: 64.4% (5154/8000)
[Test]  Epoch: 8	Loss: 0.015589	Acc: 64.7% (5174/8000)
[Test]  Epoch: 9	Loss: 0.015658	Acc: 64.0% (5123/8000)
[Test]  Epoch: 10	Loss: 0.015477	Acc: 64.6% (5168/8000)
[Test]  Epoch: 11	Loss: 0.015577	Acc: 64.3% (5144/8000)
[Test]  Epoch: 12	Loss: 0.015418	Acc: 64.7% (5175/8000)
[Test]  Epoch: 13	Loss: 0.015442	Acc: 64.6% (5169/8000)
[Test]  Epoch: 14	Loss: 0.015438	Acc: 64.6% (5170/8000)
[Test]  Epoch: 15	Loss: 0.015431	Acc: 64.4% (5151/8000)
[Test]  Epoch: 16	Loss: 0.015359	Acc: 65.0% (5201/8000)
[Test]  Epoch: 17	Loss: 0.015324	Acc: 64.9% (5193/8000)
[Test]  Epoch: 18	Loss: 0.015419	Acc: 64.6% (5169/8000)
[Test]  Epoch: 19	Loss: 0.015302	Acc: 65.0% (5202/8000)
[Test]  Epoch: 20	Loss: 0.015459	Acc: 64.5% (5163/8000)
[Test]  Epoch: 21	Loss: 0.015278	Acc: 65.0% (5197/8000)
[Test]  Epoch: 22	Loss: 0.015348	Acc: 64.5% (5158/8000)
[Test]  Epoch: 23	Loss: 0.015227	Acc: 65.0% (5204/8000)
[Test]  Epoch: 24	Loss: 0.015246	Acc: 65.1% (5211/8000)
[Test]  Epoch: 25	Loss: 0.015495	Acc: 64.5% (5158/8000)
[Test]  Epoch: 26	Loss: 0.015293	Acc: 64.5% (5163/8000)
[Test]  Epoch: 27	Loss: 0.015284	Acc: 64.8% (5183/8000)
[Test]  Epoch: 28	Loss: 0.015257	Acc: 65.0% (5196/8000)
[Test]  Epoch: 29	Loss: 0.015264	Acc: 64.8% (5180/8000)
[Test]  Epoch: 30	Loss: 0.015299	Acc: 64.9% (5189/8000)
[Test]  Epoch: 31	Loss: 0.015268	Acc: 65.1% (5209/8000)
[Test]  Epoch: 32	Loss: 0.015241	Acc: 65.0% (5199/8000)
[Test]  Epoch: 33	Loss: 0.015250	Acc: 64.9% (5191/8000)
[Test]  Epoch: 34	Loss: 0.015171	Acc: 65.1% (5206/8000)
[Test]  Epoch: 35	Loss: 0.015216	Acc: 65.1% (5210/8000)
[Test]  Epoch: 36	Loss: 0.015132	Acc: 65.4% (5232/8000)
[Test]  Epoch: 37	Loss: 0.015180	Acc: 65.2% (5214/8000)
[Test]  Epoch: 38	Loss: 0.015281	Acc: 64.9% (5191/8000)
[Test]  Epoch: 39	Loss: 0.015140	Acc: 65.1% (5206/8000)
[Test]  Epoch: 40	Loss: 0.015118	Acc: 65.3% (5222/8000)
[Test]  Epoch: 41	Loss: 0.015174	Acc: 65.2% (5215/8000)
[Test]  Epoch: 42	Loss: 0.015255	Acc: 65.0% (5196/8000)
[Test]  Epoch: 43	Loss: 0.015167	Acc: 65.3% (5224/8000)
[Test]  Epoch: 44	Loss: 0.015156	Acc: 65.5% (5237/8000)
[Test]  Epoch: 45	Loss: 0.015105	Acc: 65.3% (5227/8000)
[Test]  Epoch: 46	Loss: 0.015156	Acc: 65.2% (5219/8000)
[Test]  Epoch: 47	Loss: 0.015149	Acc: 65.4% (5231/8000)
[Test]  Epoch: 48	Loss: 0.015208	Acc: 65.2% (5219/8000)
[Test]  Epoch: 49	Loss: 0.015191	Acc: 65.0% (5203/8000)
[Test]  Epoch: 50	Loss: 0.015122	Acc: 65.4% (5233/8000)
[Test]  Epoch: 51	Loss: 0.015122	Acc: 65.5% (5236/8000)
[Test]  Epoch: 52	Loss: 0.015151	Acc: 65.4% (5231/8000)
[Test]  Epoch: 53	Loss: 0.015171	Acc: 65.2% (5213/8000)
[Test]  Epoch: 54	Loss: 0.015166	Acc: 65.3% (5226/8000)
[Test]  Epoch: 55	Loss: 0.015133	Acc: 65.5% (5240/8000)
[Test]  Epoch: 56	Loss: 0.015140	Acc: 65.4% (5234/8000)
[Test]  Epoch: 57	Loss: 0.015062	Acc: 65.4% (5233/8000)
[Test]  Epoch: 58	Loss: 0.015111	Acc: 65.5% (5240/8000)
[Test]  Epoch: 59	Loss: 0.015146	Acc: 65.2% (5217/8000)
[Test]  Epoch: 60	Loss: 0.015175	Acc: 65.2% (5217/8000)
[Test]  Epoch: 61	Loss: 0.015139	Acc: 65.4% (5229/8000)
[Test]  Epoch: 62	Loss: 0.015134	Acc: 65.2% (5214/8000)
[Test]  Epoch: 63	Loss: 0.015119	Acc: 65.5% (5238/8000)
[Test]  Epoch: 64	Loss: 0.015120	Acc: 65.4% (5235/8000)
[Test]  Epoch: 65	Loss: 0.015109	Acc: 65.4% (5231/8000)
[Test]  Epoch: 66	Loss: 0.015131	Acc: 65.3% (5227/8000)
[Test]  Epoch: 67	Loss: 0.015125	Acc: 65.4% (5232/8000)
[Test]  Epoch: 68	Loss: 0.015131	Acc: 65.5% (5239/8000)
[Test]  Epoch: 69	Loss: 0.015102	Acc: 65.5% (5243/8000)
[Test]  Epoch: 70	Loss: 0.015123	Acc: 65.4% (5232/8000)
[Test]  Epoch: 71	Loss: 0.015102	Acc: 65.5% (5240/8000)
[Test]  Epoch: 72	Loss: 0.015104	Acc: 65.3% (5227/8000)
[Test]  Epoch: 73	Loss: 0.015100	Acc: 65.4% (5234/8000)
[Test]  Epoch: 74	Loss: 0.015115	Acc: 65.3% (5228/8000)
[Test]  Epoch: 75	Loss: 0.015107	Acc: 65.4% (5231/8000)
[Test]  Epoch: 76	Loss: 0.015112	Acc: 65.5% (5239/8000)
[Test]  Epoch: 77	Loss: 0.015100	Acc: 65.3% (5224/8000)
[Test]  Epoch: 78	Loss: 0.015106	Acc: 65.4% (5231/8000)
[Test]  Epoch: 79	Loss: 0.015111	Acc: 65.4% (5231/8000)
[Test]  Epoch: 80	Loss: 0.015108	Acc: 65.5% (5238/8000)
[Test]  Epoch: 81	Loss: 0.015120	Acc: 65.2% (5218/8000)
[Test]  Epoch: 82	Loss: 0.015172	Acc: 65.3% (5222/8000)
[Test]  Epoch: 83	Loss: 0.015096	Acc: 65.5% (5242/8000)
[Test]  Epoch: 84	Loss: 0.015107	Acc: 65.4% (5235/8000)
[Test]  Epoch: 85	Loss: 0.015111	Acc: 65.4% (5235/8000)
[Test]  Epoch: 86	Loss: 0.015117	Acc: 65.4% (5229/8000)
[Test]  Epoch: 87	Loss: 0.015123	Acc: 65.5% (5237/8000)
[Test]  Epoch: 88	Loss: 0.015100	Acc: 65.4% (5233/8000)
[Test]  Epoch: 89	Loss: 0.015107	Acc: 65.5% (5236/8000)
[Test]  Epoch: 90	Loss: 0.015112	Acc: 65.4% (5232/8000)
[Test]  Epoch: 91	Loss: 0.015126	Acc: 65.5% (5243/8000)
[Test]  Epoch: 92	Loss: 0.015103	Acc: 65.3% (5224/8000)
[Test]  Epoch: 93	Loss: 0.015095	Acc: 65.5% (5239/8000)
[Test]  Epoch: 94	Loss: 0.015106	Acc: 65.6% (5246/8000)
[Test]  Epoch: 95	Loss: 0.015086	Acc: 65.5% (5239/8000)
[Test]  Epoch: 96	Loss: 0.015069	Acc: 65.6% (5248/8000)
[Test]  Epoch: 97	Loss: 0.015109	Acc: 65.5% (5236/8000)
[Test]  Epoch: 98	Loss: 0.015115	Acc: 65.5% (5244/8000)
[Test]  Epoch: 99	Loss: 0.015094	Acc: 65.5% (5240/8000)
[Test]  Epoch: 100	Loss: 0.015101	Acc: 65.5% (5240/8000)
===========finish==========
['2024-08-19', '19:55:57.716224', '100', 'test', '0.015101331882178784', '65.5', '65.6']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=42
get_sample_layers not_random
protect_percent = 0.4 42 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.conv1.weight 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.030879	Acc: 28.8% (2305/8000)
[Test]  Epoch: 2	Loss: 0.020702	Acc: 51.2% (4095/8000)
[Test]  Epoch: 3	Loss: 0.019455	Acc: 55.0% (4397/8000)
[Test]  Epoch: 4	Loss: 0.018519	Acc: 57.5% (4597/8000)
[Test]  Epoch: 5	Loss: 0.018866	Acc: 57.0% (4560/8000)
[Test]  Epoch: 6	Loss: 0.018549	Acc: 57.0% (4561/8000)
[Test]  Epoch: 7	Loss: 0.018033	Acc: 58.7% (4699/8000)
[Test]  Epoch: 8	Loss: 0.017922	Acc: 59.0% (4721/8000)
[Test]  Epoch: 9	Loss: 0.017805	Acc: 59.1% (4727/8000)
[Test]  Epoch: 10	Loss: 0.017829	Acc: 59.1% (4730/8000)
[Test]  Epoch: 11	Loss: 0.017877	Acc: 58.8% (4701/8000)
[Test]  Epoch: 12	Loss: 0.017674	Acc: 59.6% (4770/8000)
[Test]  Epoch: 13	Loss: 0.017718	Acc: 59.2% (4740/8000)
[Test]  Epoch: 14	Loss: 0.017913	Acc: 58.9% (4712/8000)
[Test]  Epoch: 15	Loss: 0.017559	Acc: 59.8% (4786/8000)
[Test]  Epoch: 16	Loss: 0.017523	Acc: 59.9% (4788/8000)
[Test]  Epoch: 17	Loss: 0.017651	Acc: 59.5% (4761/8000)
[Test]  Epoch: 18	Loss: 0.017636	Acc: 59.8% (4782/8000)
[Test]  Epoch: 19	Loss: 0.017491	Acc: 60.0% (4803/8000)
[Test]  Epoch: 20	Loss: 0.017470	Acc: 59.7% (4779/8000)
[Test]  Epoch: 21	Loss: 0.017452	Acc: 59.9% (4788/8000)
[Test]  Epoch: 22	Loss: 0.017407	Acc: 60.1% (4807/8000)
[Test]  Epoch: 23	Loss: 0.017320	Acc: 60.3% (4824/8000)
[Test]  Epoch: 24	Loss: 0.017349	Acc: 60.2% (4818/8000)
[Test]  Epoch: 25	Loss: 0.017618	Acc: 59.6% (4765/8000)
[Test]  Epoch: 26	Loss: 0.017350	Acc: 60.1% (4810/8000)
[Test]  Epoch: 27	Loss: 0.017337	Acc: 60.4% (4829/8000)
[Test]  Epoch: 28	Loss: 0.017318	Acc: 60.2% (4815/8000)
[Test]  Epoch: 29	Loss: 0.017232	Acc: 60.6% (4850/8000)
[Test]  Epoch: 30	Loss: 0.017241	Acc: 60.3% (4821/8000)
[Test]  Epoch: 31	Loss: 0.017198	Acc: 60.8% (4865/8000)
[Test]  Epoch: 32	Loss: 0.017293	Acc: 59.9% (4794/8000)
[Test]  Epoch: 33	Loss: 0.017162	Acc: 60.3% (4822/8000)
[Test]  Epoch: 34	Loss: 0.017097	Acc: 60.8% (4862/8000)
[Test]  Epoch: 35	Loss: 0.017282	Acc: 60.3% (4827/8000)
[Test]  Epoch: 36	Loss: 0.017052	Acc: 60.9% (4873/8000)
[Test]  Epoch: 37	Loss: 0.017084	Acc: 60.6% (4845/8000)
[Test]  Epoch: 38	Loss: 0.017085	Acc: 60.5% (4844/8000)
[Test]  Epoch: 39	Loss: 0.017035	Acc: 60.9% (4871/8000)
[Test]  Epoch: 40	Loss: 0.016977	Acc: 61.2% (4900/8000)
[Test]  Epoch: 41	Loss: 0.017061	Acc: 61.1% (4889/8000)
[Test]  Epoch: 42	Loss: 0.017099	Acc: 60.6% (4850/8000)
[Test]  Epoch: 43	Loss: 0.017009	Acc: 61.1% (4885/8000)
[Test]  Epoch: 44	Loss: 0.017197	Acc: 60.6% (4847/8000)
[Test]  Epoch: 45	Loss: 0.017048	Acc: 60.8% (4867/8000)
[Test]  Epoch: 46	Loss: 0.016991	Acc: 61.0% (4883/8000)
[Test]  Epoch: 47	Loss: 0.017112	Acc: 61.0% (4879/8000)
[Test]  Epoch: 48	Loss: 0.017153	Acc: 60.3% (4822/8000)
[Test]  Epoch: 49	Loss: 0.017017	Acc: 60.7% (4853/8000)
[Test]  Epoch: 50	Loss: 0.017016	Acc: 61.1% (4887/8000)
[Test]  Epoch: 51	Loss: 0.017001	Acc: 61.2% (4900/8000)
[Test]  Epoch: 52	Loss: 0.017033	Acc: 61.0% (4882/8000)
[Test]  Epoch: 53	Loss: 0.017065	Acc: 60.6% (4849/8000)
[Test]  Epoch: 54	Loss: 0.017027	Acc: 60.9% (4870/8000)
[Test]  Epoch: 55	Loss: 0.017069	Acc: 61.0% (4877/8000)
[Test]  Epoch: 56	Loss: 0.016996	Acc: 61.3% (4902/8000)
[Test]  Epoch: 57	Loss: 0.016905	Acc: 61.1% (4885/8000)
[Test]  Epoch: 58	Loss: 0.016962	Acc: 61.0% (4877/8000)
[Test]  Epoch: 59	Loss: 0.017003	Acc: 61.2% (4896/8000)
[Test]  Epoch: 60	Loss: 0.016907	Acc: 61.5% (4916/8000)
[Test]  Epoch: 61	Loss: 0.016885	Acc: 61.4% (4910/8000)
[Test]  Epoch: 62	Loss: 0.016877	Acc: 61.5% (4919/8000)
[Test]  Epoch: 63	Loss: 0.016868	Acc: 61.3% (4906/8000)
[Test]  Epoch: 64	Loss: 0.016883	Acc: 61.3% (4906/8000)
[Test]  Epoch: 65	Loss: 0.016869	Acc: 61.4% (4913/8000)
[Test]  Epoch: 66	Loss: 0.016885	Acc: 61.5% (4917/8000)
[Test]  Epoch: 67	Loss: 0.016857	Acc: 61.2% (4900/8000)
[Test]  Epoch: 68	Loss: 0.016887	Acc: 61.2% (4894/8000)
[Test]  Epoch: 69	Loss: 0.016879	Acc: 61.2% (4894/8000)
[Test]  Epoch: 70	Loss: 0.016900	Acc: 61.3% (4907/8000)
[Test]  Epoch: 71	Loss: 0.016868	Acc: 61.5% (4917/8000)
[Test]  Epoch: 72	Loss: 0.016894	Acc: 61.4% (4908/8000)
[Test]  Epoch: 73	Loss: 0.016900	Acc: 61.4% (4912/8000)
[Test]  Epoch: 74	Loss: 0.016884	Acc: 61.3% (4905/8000)
[Test]  Epoch: 75	Loss: 0.016877	Acc: 61.3% (4901/8000)
[Test]  Epoch: 76	Loss: 0.016921	Acc: 61.4% (4909/8000)
[Test]  Epoch: 77	Loss: 0.016895	Acc: 61.4% (4913/8000)
[Test]  Epoch: 78	Loss: 0.016889	Acc: 61.4% (4909/8000)
[Test]  Epoch: 79	Loss: 0.016898	Acc: 61.4% (4911/8000)
[Test]  Epoch: 80	Loss: 0.016894	Acc: 61.3% (4904/8000)
[Test]  Epoch: 81	Loss: 0.016898	Acc: 61.2% (4900/8000)
[Test]  Epoch: 82	Loss: 0.016938	Acc: 61.2% (4893/8000)
[Test]  Epoch: 83	Loss: 0.016878	Acc: 61.3% (4905/8000)
[Test]  Epoch: 84	Loss: 0.016894	Acc: 61.2% (4896/8000)
[Test]  Epoch: 85	Loss: 0.016896	Acc: 61.3% (4902/8000)
[Test]  Epoch: 86	Loss: 0.016915	Acc: 61.2% (4894/8000)
[Test]  Epoch: 87	Loss: 0.016924	Acc: 61.1% (4885/8000)
[Test]  Epoch: 88	Loss: 0.016887	Acc: 61.2% (4899/8000)
[Test]  Epoch: 89	Loss: 0.016906	Acc: 61.3% (4906/8000)
[Test]  Epoch: 90	Loss: 0.016907	Acc: 61.4% (4913/8000)
[Test]  Epoch: 91	Loss: 0.016906	Acc: 61.5% (4917/8000)
[Test]  Epoch: 92	Loss: 0.016897	Acc: 61.3% (4904/8000)
[Test]  Epoch: 93	Loss: 0.016900	Acc: 61.4% (4909/8000)
[Test]  Epoch: 94	Loss: 0.016898	Acc: 61.3% (4906/8000)
[Test]  Epoch: 95	Loss: 0.016874	Acc: 61.4% (4914/8000)
[Test]  Epoch: 96	Loss: 0.016857	Acc: 61.5% (4919/8000)
[Test]  Epoch: 97	Loss: 0.016914	Acc: 61.2% (4893/8000)
[Test]  Epoch: 98	Loss: 0.016907	Acc: 61.4% (4909/8000)
[Test]  Epoch: 99	Loss: 0.016897	Acc: 61.4% (4910/8000)
[Test]  Epoch: 100	Loss: 0.016878	Acc: 61.5% (4918/8000)
===========finish==========
['2024-08-19', '20:05:23.140178', '100', 'test', '0.016878305755555628', '61.475', '61.4875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=53
get_sample_layers not_random
protect_percent = 0.5 53 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.conv1.weight 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.conv1.weight 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.conv3.weight 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.conv1.weight 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.conv2.weight 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.conv3.weight 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.conv2.weight 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.conv3.weight 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.conv2.weight 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.conv3.weight 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.034553	Acc: 22.5% (1802/8000)
[Test]  Epoch: 2	Loss: 0.022876	Acc: 46.3% (3702/8000)
[Test]  Epoch: 3	Loss: 0.020279	Acc: 52.2% (4173/8000)
[Test]  Epoch: 4	Loss: 0.020257	Acc: 52.8% (4223/8000)
[Test]  Epoch: 5	Loss: 0.020070	Acc: 53.0% (4242/8000)
[Test]  Epoch: 6	Loss: 0.019483	Acc: 55.7% (4453/8000)
[Test]  Epoch: 7	Loss: 0.019293	Acc: 55.6% (4450/8000)
[Test]  Epoch: 8	Loss: 0.019113	Acc: 56.6% (4528/8000)
[Test]  Epoch: 9	Loss: 0.019261	Acc: 56.2% (4497/8000)
[Test]  Epoch: 10	Loss: 0.019099	Acc: 56.3% (4501/8000)
[Test]  Epoch: 11	Loss: 0.019220	Acc: 56.2% (4499/8000)
[Test]  Epoch: 12	Loss: 0.018696	Acc: 57.0% (4558/8000)
[Test]  Epoch: 13	Loss: 0.018926	Acc: 56.6% (4527/8000)
[Test]  Epoch: 14	Loss: 0.019110	Acc: 56.2% (4493/8000)
[Test]  Epoch: 15	Loss: 0.018723	Acc: 57.3% (4582/8000)
[Test]  Epoch: 16	Loss: 0.018684	Acc: 57.1% (4566/8000)
[Test]  Epoch: 17	Loss: 0.018777	Acc: 57.0% (4562/8000)
[Test]  Epoch: 18	Loss: 0.018561	Acc: 57.2% (4580/8000)
[Test]  Epoch: 19	Loss: 0.018638	Acc: 57.1% (4567/8000)
[Test]  Epoch: 20	Loss: 0.018608	Acc: 57.3% (4582/8000)
[Test]  Epoch: 21	Loss: 0.018549	Acc: 57.3% (4582/8000)
[Test]  Epoch: 22	Loss: 0.018542	Acc: 57.3% (4586/8000)
[Test]  Epoch: 23	Loss: 0.018404	Acc: 57.9% (4630/8000)
[Test]  Epoch: 24	Loss: 0.018262	Acc: 58.0% (4643/8000)
[Test]  Epoch: 25	Loss: 0.018628	Acc: 57.4% (4592/8000)
[Test]  Epoch: 26	Loss: 0.018344	Acc: 57.8% (4625/8000)
[Test]  Epoch: 27	Loss: 0.018259	Acc: 58.3% (4664/8000)
[Test]  Epoch: 28	Loss: 0.018430	Acc: 57.6% (4606/8000)
[Test]  Epoch: 29	Loss: 0.018298	Acc: 58.2% (4655/8000)
[Test]  Epoch: 30	Loss: 0.018284	Acc: 57.9% (4630/8000)
[Test]  Epoch: 31	Loss: 0.018197	Acc: 58.4% (4673/8000)
[Test]  Epoch: 32	Loss: 0.018224	Acc: 57.8% (4624/8000)
[Test]  Epoch: 33	Loss: 0.018172	Acc: 58.0% (4644/8000)
[Test]  Epoch: 34	Loss: 0.018110	Acc: 58.5% (4684/8000)
[Test]  Epoch: 35	Loss: 0.018197	Acc: 57.7% (4615/8000)
[Test]  Epoch: 36	Loss: 0.018038	Acc: 58.2% (4659/8000)
[Test]  Epoch: 37	Loss: 0.018120	Acc: 58.5% (4677/8000)
[Test]  Epoch: 38	Loss: 0.018051	Acc: 58.2% (4660/8000)
[Test]  Epoch: 39	Loss: 0.018141	Acc: 58.4% (4673/8000)
[Test]  Epoch: 40	Loss: 0.018050	Acc: 58.4% (4675/8000)
[Test]  Epoch: 41	Loss: 0.018079	Acc: 58.2% (4656/8000)
[Test]  Epoch: 42	Loss: 0.018074	Acc: 58.4% (4668/8000)
[Test]  Epoch: 43	Loss: 0.017980	Acc: 58.8% (4705/8000)
[Test]  Epoch: 44	Loss: 0.018238	Acc: 57.9% (4631/8000)
[Test]  Epoch: 45	Loss: 0.018036	Acc: 58.2% (4660/8000)
[Test]  Epoch: 46	Loss: 0.017919	Acc: 58.5% (4681/8000)
[Test]  Epoch: 47	Loss: 0.017957	Acc: 58.9% (4709/8000)
[Test]  Epoch: 48	Loss: 0.018065	Acc: 58.3% (4665/8000)
[Test]  Epoch: 49	Loss: 0.018005	Acc: 58.2% (4654/8000)
[Test]  Epoch: 50	Loss: 0.017982	Acc: 58.1% (4647/8000)
[Test]  Epoch: 51	Loss: 0.017960	Acc: 58.1% (4652/8000)
[Test]  Epoch: 52	Loss: 0.017898	Acc: 58.7% (4693/8000)
[Test]  Epoch: 53	Loss: 0.017916	Acc: 58.3% (4666/8000)
[Test]  Epoch: 54	Loss: 0.017848	Acc: 58.4% (4672/8000)
[Test]  Epoch: 55	Loss: 0.017950	Acc: 58.4% (4671/8000)
[Test]  Epoch: 56	Loss: 0.017960	Acc: 58.7% (4694/8000)
[Test]  Epoch: 57	Loss: 0.017831	Acc: 58.5% (4679/8000)
[Test]  Epoch: 58	Loss: 0.017899	Acc: 58.5% (4683/8000)
[Test]  Epoch: 59	Loss: 0.017895	Acc: 58.6% (4686/8000)
[Test]  Epoch: 60	Loss: 0.017873	Acc: 58.4% (4669/8000)
[Test]  Epoch: 61	Loss: 0.017808	Acc: 59.0% (4717/8000)
[Test]  Epoch: 62	Loss: 0.017786	Acc: 58.9% (4713/8000)
[Test]  Epoch: 63	Loss: 0.017781	Acc: 58.9% (4712/8000)
[Test]  Epoch: 64	Loss: 0.017799	Acc: 58.9% (4709/8000)
[Test]  Epoch: 65	Loss: 0.017795	Acc: 58.8% (4707/8000)
[Test]  Epoch: 66	Loss: 0.017781	Acc: 59.0% (4719/8000)
[Test]  Epoch: 67	Loss: 0.017795	Acc: 58.8% (4700/8000)
[Test]  Epoch: 68	Loss: 0.017790	Acc: 59.0% (4716/8000)
[Test]  Epoch: 69	Loss: 0.017786	Acc: 58.8% (4700/8000)
[Test]  Epoch: 70	Loss: 0.017776	Acc: 59.0% (4720/8000)
[Test]  Epoch: 71	Loss: 0.017787	Acc: 59.0% (4719/8000)
[Test]  Epoch: 72	Loss: 0.017794	Acc: 58.8% (4701/8000)
[Test]  Epoch: 73	Loss: 0.017793	Acc: 59.0% (4717/8000)
[Test]  Epoch: 74	Loss: 0.017782	Acc: 58.9% (4713/8000)
[Test]  Epoch: 75	Loss: 0.017775	Acc: 59.0% (4721/8000)
[Test]  Epoch: 76	Loss: 0.017796	Acc: 58.9% (4710/8000)
[Test]  Epoch: 77	Loss: 0.017801	Acc: 58.7% (4694/8000)
[Test]  Epoch: 78	Loss: 0.017820	Acc: 58.8% (4706/8000)
[Test]  Epoch: 79	Loss: 0.017798	Acc: 58.9% (4708/8000)
[Test]  Epoch: 80	Loss: 0.017794	Acc: 59.0% (4720/8000)
[Test]  Epoch: 81	Loss: 0.017796	Acc: 58.9% (4709/8000)
[Test]  Epoch: 82	Loss: 0.017824	Acc: 58.8% (4702/8000)
[Test]  Epoch: 83	Loss: 0.017783	Acc: 58.9% (4710/8000)
[Test]  Epoch: 84	Loss: 0.017803	Acc: 58.7% (4699/8000)
[Test]  Epoch: 85	Loss: 0.017775	Acc: 58.9% (4711/8000)
[Test]  Epoch: 86	Loss: 0.017809	Acc: 58.9% (4709/8000)
[Test]  Epoch: 87	Loss: 0.017824	Acc: 58.8% (4705/8000)
[Test]  Epoch: 88	Loss: 0.017799	Acc: 58.9% (4708/8000)
[Test]  Epoch: 89	Loss: 0.017791	Acc: 58.9% (4710/8000)
[Test]  Epoch: 90	Loss: 0.017795	Acc: 58.8% (4706/8000)
[Test]  Epoch: 91	Loss: 0.017787	Acc: 58.8% (4706/8000)
[Test]  Epoch: 92	Loss: 0.017826	Acc: 58.9% (4710/8000)
[Test]  Epoch: 93	Loss: 0.017829	Acc: 58.7% (4697/8000)
[Test]  Epoch: 94	Loss: 0.017797	Acc: 58.8% (4705/8000)
[Test]  Epoch: 95	Loss: 0.017761	Acc: 58.7% (4698/8000)
[Test]  Epoch: 96	Loss: 0.017763	Acc: 59.0% (4716/8000)
[Test]  Epoch: 97	Loss: 0.017825	Acc: 59.0% (4717/8000)
[Test]  Epoch: 98	Loss: 0.017794	Acc: 59.0% (4717/8000)
[Test]  Epoch: 99	Loss: 0.017791	Acc: 58.8% (4703/8000)
[Test]  Epoch: 100	Loss: 0.017796	Acc: 58.8% (4702/8000)
===========finish==========
['2024-08-19', '20:14:45.219218', '100', 'test', '0.0177963165640831', '58.775', '59.0125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=64
get_sample_layers not_random
protect_percent = 0.6 64 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight']
conv1.weight 1111111111
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.conv2.weight 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.conv3.weight 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.conv2.weight 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.conv1.weight 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.conv1.weight 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.conv3.weight 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.conv1.weight 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.conv1.weight 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.conv1.weight 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.039224	Acc: 19.1% (1526/8000)
[Test]  Epoch: 2	Loss: 0.024261	Acc: 43.4% (3474/8000)
[Test]  Epoch: 3	Loss: 0.020974	Acc: 51.4% (4113/8000)
[Test]  Epoch: 4	Loss: 0.019680	Acc: 54.0% (4322/8000)
[Test]  Epoch: 5	Loss: 0.019418	Acc: 55.5% (4444/8000)
[Test]  Epoch: 6	Loss: 0.019253	Acc: 55.5% (4437/8000)
[Test]  Epoch: 7	Loss: 0.019153	Acc: 56.3% (4506/8000)
[Test]  Epoch: 8	Loss: 0.019342	Acc: 56.1% (4487/8000)
[Test]  Epoch: 9	Loss: 0.018982	Acc: 57.0% (4560/8000)
[Test]  Epoch: 10	Loss: 0.018791	Acc: 57.0% (4563/8000)
[Test]  Epoch: 11	Loss: 0.019026	Acc: 56.6% (4532/8000)
[Test]  Epoch: 12	Loss: 0.018542	Acc: 57.2% (4579/8000)
[Test]  Epoch: 13	Loss: 0.018730	Acc: 56.9% (4555/8000)
[Test]  Epoch: 14	Loss: 0.018648	Acc: 57.3% (4581/8000)
[Test]  Epoch: 15	Loss: 0.018506	Acc: 57.1% (4568/8000)
[Test]  Epoch: 16	Loss: 0.018364	Acc: 57.8% (4622/8000)
[Test]  Epoch: 17	Loss: 0.018469	Acc: 57.6% (4606/8000)
[Test]  Epoch: 18	Loss: 0.018398	Acc: 57.8% (4621/8000)
[Test]  Epoch: 19	Loss: 0.018364	Acc: 58.0% (4636/8000)
[Test]  Epoch: 20	Loss: 0.018487	Acc: 57.6% (4609/8000)
[Test]  Epoch: 21	Loss: 0.018296	Acc: 57.9% (4632/8000)
[Test]  Epoch: 22	Loss: 0.018361	Acc: 57.8% (4627/8000)
[Test]  Epoch: 23	Loss: 0.018060	Acc: 58.4% (4670/8000)
[Test]  Epoch: 24	Loss: 0.018426	Acc: 57.6% (4611/8000)
[Test]  Epoch: 25	Loss: 0.019033	Acc: 57.3% (4581/8000)
[Test]  Epoch: 26	Loss: 0.018064	Acc: 58.2% (4660/8000)
[Test]  Epoch: 27	Loss: 0.018150	Acc: 57.8% (4626/8000)
[Test]  Epoch: 28	Loss: 0.018143	Acc: 58.6% (4689/8000)
[Test]  Epoch: 29	Loss: 0.018098	Acc: 58.5% (4684/8000)
[Test]  Epoch: 30	Loss: 0.018253	Acc: 57.9% (4631/8000)
[Test]  Epoch: 31	Loss: 0.018035	Acc: 58.4% (4671/8000)
[Test]  Epoch: 32	Loss: 0.017993	Acc: 58.5% (4682/8000)
[Test]  Epoch: 33	Loss: 0.018005	Acc: 58.3% (4662/8000)
[Test]  Epoch: 34	Loss: 0.017910	Acc: 58.7% (4694/8000)
[Test]  Epoch: 35	Loss: 0.018085	Acc: 58.5% (4681/8000)
[Test]  Epoch: 36	Loss: 0.017973	Acc: 58.8% (4700/8000)
[Test]  Epoch: 37	Loss: 0.017993	Acc: 58.5% (4676/8000)
[Test]  Epoch: 38	Loss: 0.017838	Acc: 59.0% (4716/8000)
[Test]  Epoch: 39	Loss: 0.017920	Acc: 58.7% (4693/8000)
[Test]  Epoch: 40	Loss: 0.017898	Acc: 58.5% (4683/8000)
[Test]  Epoch: 41	Loss: 0.017831	Acc: 58.8% (4702/8000)
[Test]  Epoch: 42	Loss: 0.017857	Acc: 58.6% (4686/8000)
[Test]  Epoch: 43	Loss: 0.017801	Acc: 58.9% (4710/8000)
[Test]  Epoch: 44	Loss: 0.017865	Acc: 58.8% (4703/8000)
[Test]  Epoch: 45	Loss: 0.017804	Acc: 58.9% (4712/8000)
[Test]  Epoch: 46	Loss: 0.017856	Acc: 58.8% (4707/8000)
[Test]  Epoch: 47	Loss: 0.017868	Acc: 59.1% (4725/8000)
[Test]  Epoch: 48	Loss: 0.017898	Acc: 58.7% (4696/8000)
[Test]  Epoch: 49	Loss: 0.017810	Acc: 58.6% (4687/8000)
[Test]  Epoch: 50	Loss: 0.017704	Acc: 59.2% (4735/8000)
[Test]  Epoch: 51	Loss: 0.017694	Acc: 59.0% (4717/8000)
[Test]  Epoch: 52	Loss: 0.017700	Acc: 59.0% (4718/8000)
[Test]  Epoch: 53	Loss: 0.017787	Acc: 58.9% (4712/8000)
[Test]  Epoch: 54	Loss: 0.017674	Acc: 58.8% (4704/8000)
[Test]  Epoch: 55	Loss: 0.017742	Acc: 59.5% (4759/8000)
[Test]  Epoch: 56	Loss: 0.017725	Acc: 59.1% (4730/8000)
[Test]  Epoch: 57	Loss: 0.017624	Acc: 59.1% (4729/8000)
[Test]  Epoch: 58	Loss: 0.017716	Acc: 58.8% (4704/8000)
[Test]  Epoch: 59	Loss: 0.017877	Acc: 59.0% (4721/8000)
[Test]  Epoch: 60	Loss: 0.017651	Acc: 59.2% (4739/8000)
[Test]  Epoch: 61	Loss: 0.017615	Acc: 59.4% (4749/8000)
[Test]  Epoch: 62	Loss: 0.017586	Acc: 59.6% (4766/8000)
[Test]  Epoch: 63	Loss: 0.017585	Acc: 59.4% (4753/8000)
[Test]  Epoch: 64	Loss: 0.017574	Acc: 59.2% (4738/8000)
[Test]  Epoch: 65	Loss: 0.017579	Acc: 59.0% (4724/8000)
[Test]  Epoch: 66	Loss: 0.017596	Acc: 59.2% (4738/8000)
[Test]  Epoch: 67	Loss: 0.017622	Acc: 59.0% (4720/8000)
[Test]  Epoch: 68	Loss: 0.017606	Acc: 59.4% (4748/8000)
[Test]  Epoch: 69	Loss: 0.017600	Acc: 59.4% (4749/8000)
[Test]  Epoch: 70	Loss: 0.017595	Acc: 59.3% (4743/8000)
[Test]  Epoch: 71	Loss: 0.017581	Acc: 59.3% (4743/8000)
[Test]  Epoch: 72	Loss: 0.017584	Acc: 59.4% (4753/8000)
[Test]  Epoch: 73	Loss: 0.017583	Acc: 59.4% (4751/8000)
[Test]  Epoch: 74	Loss: 0.017596	Acc: 59.4% (4754/8000)
[Test]  Epoch: 75	Loss: 0.017603	Acc: 59.4% (4754/8000)
[Test]  Epoch: 76	Loss: 0.017617	Acc: 59.6% (4767/8000)
[Test]  Epoch: 77	Loss: 0.017584	Acc: 59.3% (4747/8000)
[Test]  Epoch: 78	Loss: 0.017634	Acc: 59.2% (4734/8000)
[Test]  Epoch: 79	Loss: 0.017599	Acc: 59.2% (4736/8000)
[Test]  Epoch: 80	Loss: 0.017576	Acc: 59.2% (4739/8000)
[Test]  Epoch: 81	Loss: 0.017585	Acc: 59.4% (4751/8000)
[Test]  Epoch: 82	Loss: 0.017674	Acc: 59.1% (4726/8000)
[Test]  Epoch: 83	Loss: 0.017592	Acc: 59.5% (4760/8000)
[Test]  Epoch: 84	Loss: 0.017608	Acc: 59.3% (4745/8000)
[Test]  Epoch: 85	Loss: 0.017581	Acc: 59.4% (4751/8000)
[Test]  Epoch: 86	Loss: 0.017601	Acc: 59.3% (4743/8000)
[Test]  Epoch: 87	Loss: 0.017583	Acc: 59.4% (4750/8000)
[Test]  Epoch: 88	Loss: 0.017581	Acc: 59.2% (4740/8000)
[Test]  Epoch: 89	Loss: 0.017580	Acc: 59.3% (4741/8000)
[Test]  Epoch: 90	Loss: 0.017589	Acc: 59.1% (4732/8000)
[Test]  Epoch: 91	Loss: 0.017582	Acc: 59.2% (4734/8000)
[Test]  Epoch: 92	Loss: 0.017585	Acc: 59.4% (4748/8000)
[Test]  Epoch: 93	Loss: 0.017580	Acc: 59.2% (4734/8000)
[Test]  Epoch: 94	Loss: 0.017615	Acc: 59.3% (4747/8000)
[Test]  Epoch: 95	Loss: 0.017600	Acc: 59.2% (4740/8000)
[Test]  Epoch: 96	Loss: 0.017527	Acc: 59.4% (4750/8000)
[Test]  Epoch: 97	Loss: 0.017611	Acc: 59.1% (4728/8000)
[Test]  Epoch: 98	Loss: 0.017578	Acc: 59.2% (4740/8000)
[Test]  Epoch: 99	Loss: 0.017587	Acc: 59.4% (4748/8000)
[Test]  Epoch: 100	Loss: 0.017599	Acc: 59.0% (4719/8000)
===========finish==========
['2024-08-19', '20:24:00.904309', '100', 'test', '0.017599497236311434', '58.9875', '59.5875']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=74
get_sample_layers not_random
protect_percent = 0.7 74 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.3.conv1.weight', 'layer2.0.conv1.weight', 'layer1.0.conv2.weight', 'layer3.1.conv1.weight', 'conv1.weight', 'layer2.1.conv3.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.3.conv1.weight', 'layer2.0.conv1.weight', 'layer1.0.conv2.weight', 'layer3.1.conv1.weight', 'conv1.weight', 'layer2.1.conv3.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.0.weight 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.conv2.weight 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.conv3.weight 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.conv2.weight 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.conv1.weight 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.conv2.weight 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.conv3.weight 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.conv1.weight 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.conv2.weight 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.conv1.weight 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.weight 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.037576	Acc: 17.8% (1427/8000)
[Test]  Epoch: 2	Loss: 0.026790	Acc: 36.7% (2933/8000)
[Test]  Epoch: 3	Loss: 0.021578	Acc: 49.2% (3935/8000)
[Test]  Epoch: 4	Loss: 0.020982	Acc: 52.1% (4166/8000)
[Test]  Epoch: 5	Loss: 0.020629	Acc: 52.7% (4216/8000)
[Test]  Epoch: 6	Loss: 0.020061	Acc: 54.4% (4351/8000)
[Test]  Epoch: 7	Loss: 0.020291	Acc: 54.1% (4327/8000)
[Test]  Epoch: 8	Loss: 0.019853	Acc: 55.0% (4404/8000)
[Test]  Epoch: 9	Loss: 0.020123	Acc: 55.0% (4398/8000)
[Test]  Epoch: 10	Loss: 0.019559	Acc: 55.4% (4435/8000)
[Test]  Epoch: 11	Loss: 0.019723	Acc: 55.6% (4446/8000)
[Test]  Epoch: 12	Loss: 0.019809	Acc: 54.4% (4352/8000)
[Test]  Epoch: 13	Loss: 0.019336	Acc: 56.0% (4482/8000)
[Test]  Epoch: 14	Loss: 0.019687	Acc: 55.3% (4421/8000)
[Test]  Epoch: 15	Loss: 0.019295	Acc: 56.2% (4494/8000)
[Test]  Epoch: 16	Loss: 0.019323	Acc: 56.1% (4492/8000)
[Test]  Epoch: 17	Loss: 0.019118	Acc: 56.4% (4510/8000)
[Test]  Epoch: 18	Loss: 0.019068	Acc: 56.8% (4545/8000)
[Test]  Epoch: 19	Loss: 0.019050	Acc: 56.7% (4537/8000)
[Test]  Epoch: 20	Loss: 0.019008	Acc: 56.6% (4530/8000)
[Test]  Epoch: 21	Loss: 0.019019	Acc: 56.5% (4523/8000)
[Test]  Epoch: 22	Loss: 0.019097	Acc: 56.4% (4514/8000)
[Test]  Epoch: 23	Loss: 0.018872	Acc: 57.3% (4581/8000)
[Test]  Epoch: 24	Loss: 0.019239	Acc: 56.4% (4509/8000)
[Test]  Epoch: 25	Loss: 0.019190	Acc: 55.9% (4475/8000)
[Test]  Epoch: 26	Loss: 0.018747	Acc: 56.9% (4548/8000)
[Test]  Epoch: 27	Loss: 0.018577	Acc: 57.4% (4593/8000)
[Test]  Epoch: 28	Loss: 0.018822	Acc: 56.8% (4544/8000)
[Test]  Epoch: 29	Loss: 0.018616	Acc: 57.2% (4575/8000)
[Test]  Epoch: 30	Loss: 0.018697	Acc: 57.4% (4594/8000)
[Test]  Epoch: 31	Loss: 0.018737	Acc: 56.7% (4534/8000)
[Test]  Epoch: 32	Loss: 0.018569	Acc: 57.6% (4612/8000)
[Test]  Epoch: 33	Loss: 0.018738	Acc: 57.1% (4567/8000)
[Test]  Epoch: 34	Loss: 0.018618	Acc: 57.3% (4582/8000)
[Test]  Epoch: 35	Loss: 0.018769	Acc: 57.4% (4592/8000)
[Test]  Epoch: 36	Loss: 0.018723	Acc: 57.2% (4580/8000)
[Test]  Epoch: 37	Loss: 0.018580	Acc: 57.5% (4599/8000)
[Test]  Epoch: 38	Loss: 0.018525	Acc: 57.4% (4589/8000)
[Test]  Epoch: 39	Loss: 0.018580	Acc: 57.5% (4604/8000)
[Test]  Epoch: 40	Loss: 0.018493	Acc: 57.3% (4583/8000)
[Test]  Epoch: 41	Loss: 0.018456	Acc: 57.9% (4634/8000)
[Test]  Epoch: 42	Loss: 0.018446	Acc: 57.4% (4592/8000)
[Test]  Epoch: 43	Loss: 0.018418	Acc: 57.8% (4621/8000)
[Test]  Epoch: 44	Loss: 0.018477	Acc: 57.6% (4606/8000)
[Test]  Epoch: 45	Loss: 0.018338	Acc: 57.8% (4626/8000)
[Test]  Epoch: 46	Loss: 0.018449	Acc: 57.7% (4619/8000)
[Test]  Epoch: 47	Loss: 0.018427	Acc: 57.9% (4630/8000)
[Test]  Epoch: 48	Loss: 0.018458	Acc: 57.7% (4614/8000)
[Test]  Epoch: 49	Loss: 0.018420	Acc: 57.8% (4624/8000)
[Test]  Epoch: 50	Loss: 0.018367	Acc: 57.9% (4628/8000)
[Test]  Epoch: 51	Loss: 0.018375	Acc: 57.7% (4618/8000)
[Test]  Epoch: 52	Loss: 0.018345	Acc: 58.0% (4637/8000)
[Test]  Epoch: 53	Loss: 0.018484	Acc: 57.5% (4598/8000)
[Test]  Epoch: 54	Loss: 0.018327	Acc: 58.0% (4640/8000)
[Test]  Epoch: 55	Loss: 0.018290	Acc: 58.1% (4646/8000)
[Test]  Epoch: 56	Loss: 0.018246	Acc: 58.2% (4654/8000)
[Test]  Epoch: 57	Loss: 0.018276	Acc: 58.0% (4639/8000)
[Test]  Epoch: 58	Loss: 0.018250	Acc: 58.5% (4681/8000)
[Test]  Epoch: 59	Loss: 0.018507	Acc: 57.7% (4618/8000)
[Test]  Epoch: 60	Loss: 0.018289	Acc: 57.7% (4617/8000)
[Test]  Epoch: 61	Loss: 0.018252	Acc: 58.0% (4643/8000)
[Test]  Epoch: 62	Loss: 0.018255	Acc: 58.1% (4647/8000)
[Test]  Epoch: 63	Loss: 0.018236	Acc: 58.2% (4655/8000)
[Test]  Epoch: 64	Loss: 0.018236	Acc: 58.1% (4645/8000)
[Test]  Epoch: 65	Loss: 0.018242	Acc: 58.1% (4645/8000)
[Test]  Epoch: 66	Loss: 0.018234	Acc: 58.1% (4645/8000)
[Test]  Epoch: 67	Loss: 0.018262	Acc: 58.0% (4643/8000)
[Test]  Epoch: 68	Loss: 0.018246	Acc: 58.4% (4671/8000)
[Test]  Epoch: 69	Loss: 0.018226	Acc: 58.3% (4662/8000)
[Test]  Epoch: 70	Loss: 0.018213	Acc: 58.4% (4668/8000)
[Test]  Epoch: 71	Loss: 0.018232	Acc: 58.2% (4653/8000)
[Test]  Epoch: 72	Loss: 0.018203	Acc: 58.5% (4676/8000)
[Test]  Epoch: 73	Loss: 0.018212	Acc: 58.4% (4668/8000)
[Test]  Epoch: 74	Loss: 0.018210	Acc: 58.1% (4648/8000)
[Test]  Epoch: 75	Loss: 0.018216	Acc: 58.2% (4653/8000)
[Test]  Epoch: 76	Loss: 0.018251	Acc: 58.1% (4650/8000)
[Test]  Epoch: 77	Loss: 0.018209	Acc: 58.2% (4659/8000)
[Test]  Epoch: 78	Loss: 0.018253	Acc: 58.0% (4640/8000)
[Test]  Epoch: 79	Loss: 0.018221	Acc: 58.1% (4651/8000)
[Test]  Epoch: 80	Loss: 0.018248	Acc: 58.3% (4661/8000)
[Test]  Epoch: 81	Loss: 0.018219	Acc: 58.2% (4655/8000)
[Test]  Epoch: 82	Loss: 0.018251	Acc: 58.0% (4641/8000)
[Test]  Epoch: 83	Loss: 0.018210	Acc: 58.2% (4657/8000)
[Test]  Epoch: 84	Loss: 0.018234	Acc: 58.5% (4676/8000)
[Test]  Epoch: 85	Loss: 0.018208	Acc: 58.3% (4662/8000)
[Test]  Epoch: 86	Loss: 0.018213	Acc: 58.4% (4672/8000)
[Test]  Epoch: 87	Loss: 0.018229	Acc: 58.4% (4668/8000)
[Test]  Epoch: 88	Loss: 0.018216	Acc: 58.4% (4674/8000)
[Test]  Epoch: 89	Loss: 0.018206	Acc: 58.3% (4664/8000)
[Test]  Epoch: 90	Loss: 0.018215	Acc: 58.3% (4665/8000)
[Test]  Epoch: 91	Loss: 0.018183	Acc: 58.5% (4679/8000)
[Test]  Epoch: 92	Loss: 0.018221	Acc: 58.3% (4665/8000)
[Test]  Epoch: 93	Loss: 0.018210	Acc: 58.3% (4661/8000)
[Test]  Epoch: 94	Loss: 0.018240	Acc: 58.3% (4667/8000)
[Test]  Epoch: 95	Loss: 0.018209	Acc: 58.4% (4671/8000)
[Test]  Epoch: 96	Loss: 0.018173	Acc: 58.4% (4673/8000)
[Test]  Epoch: 97	Loss: 0.018223	Acc: 58.3% (4666/8000)
[Test]  Epoch: 98	Loss: 0.018200	Acc: 58.3% (4663/8000)
[Test]  Epoch: 99	Loss: 0.018214	Acc: 58.1% (4645/8000)
[Test]  Epoch: 100	Loss: 0.018224	Acc: 58.1% (4645/8000)
===========finish==========
['2024-08-19', '20:33:20.755948', '100', 'test', '0.018224181272089482', '58.0625', '58.5125']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=85
get_sample_layers not_random
protect_percent = 0.8 85 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.3.conv1.weight', 'layer2.0.conv1.weight', 'layer1.0.conv2.weight', 'layer3.1.conv1.weight', 'conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv3.weight', 'layer3.2.conv2.weight', 'layer3.3.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.conv2.weight', 'layer2.1.conv2.weight', 'layer1.0.downsample.0.weight', 'last_linear.weight', 'layer3.2.conv3.weight', 'layer3.0.conv1.weight', 'layer3.3.conv2.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.3.conv1.weight', 'layer2.0.conv1.weight', 'layer1.0.conv2.weight', 'layer3.1.conv1.weight', 'conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv3.weight', 'layer3.2.conv2.weight', 'layer3.3.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.conv2.weight', 'layer2.1.conv2.weight', 'layer1.0.downsample.0.weight', 'last_linear.weight', 'layer3.2.conv3.weight', 'layer3.0.conv1.weight', 'layer3.3.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.0.weight 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.conv2.weight 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.conv3.weight 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.conv2.weight 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.conv3.weight 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.conv3.weight 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.conv2.weight 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.conv3.weight 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.conv1.weight 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.conv2.weight 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.conv3.weight 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035964	Acc: 10.1% (809/8000)
[Test]  Epoch: 2	Loss: 0.033248	Acc: 20.8% (1665/8000)
[Test]  Epoch: 3	Loss: 0.026297	Acc: 39.2% (3138/8000)
[Test]  Epoch: 4	Loss: 0.023060	Acc: 44.9% (3591/8000)
[Test]  Epoch: 5	Loss: 0.021002	Acc: 50.0% (4002/8000)
[Test]  Epoch: 6	Loss: 0.020069	Acc: 52.4% (4188/8000)
[Test]  Epoch: 7	Loss: 0.018674	Acc: 56.1% (4492/8000)
[Test]  Epoch: 8	Loss: 0.018540	Acc: 55.8% (4461/8000)
[Test]  Epoch: 9	Loss: 0.018304	Acc: 56.5% (4522/8000)
[Test]  Epoch: 10	Loss: 0.018130	Acc: 57.4% (4591/8000)
[Test]  Epoch: 11	Loss: 0.017867	Acc: 57.9% (4628/8000)
[Test]  Epoch: 12	Loss: 0.017930	Acc: 58.0% (4639/8000)
[Test]  Epoch: 13	Loss: 0.017836	Acc: 57.8% (4625/8000)
[Test]  Epoch: 14	Loss: 0.017360	Acc: 59.0% (4717/8000)
[Test]  Epoch: 15	Loss: 0.017583	Acc: 58.6% (4692/8000)
[Test]  Epoch: 16	Loss: 0.017731	Acc: 58.2% (4656/8000)
[Test]  Epoch: 17	Loss: 0.017379	Acc: 59.3% (4741/8000)
[Test]  Epoch: 18	Loss: 0.017593	Acc: 58.7% (4695/8000)
[Test]  Epoch: 19	Loss: 0.017456	Acc: 58.6% (4691/8000)
[Test]  Epoch: 20	Loss: 0.017349	Acc: 58.7% (4697/8000)
[Test]  Epoch: 21	Loss: 0.017327	Acc: 59.4% (4754/8000)
[Test]  Epoch: 22	Loss: 0.017279	Acc: 59.2% (4739/8000)
[Test]  Epoch: 23	Loss: 0.017123	Acc: 59.7% (4775/8000)
[Test]  Epoch: 24	Loss: 0.017178	Acc: 59.3% (4746/8000)
[Test]  Epoch: 25	Loss: 0.017617	Acc: 58.6% (4688/8000)
[Test]  Epoch: 26	Loss: 0.017309	Acc: 59.2% (4736/8000)
[Test]  Epoch: 27	Loss: 0.017098	Acc: 59.6% (4766/8000)
[Test]  Epoch: 28	Loss: 0.017049	Acc: 59.9% (4792/8000)
[Test]  Epoch: 29	Loss: 0.017022	Acc: 60.0% (4802/8000)
[Test]  Epoch: 30	Loss: 0.017308	Acc: 59.4% (4753/8000)
[Test]  Epoch: 31	Loss: 0.016941	Acc: 60.2% (4819/8000)
[Test]  Epoch: 32	Loss: 0.017021	Acc: 59.7% (4775/8000)
[Test]  Epoch: 33	Loss: 0.017043	Acc: 59.8% (4784/8000)
[Test]  Epoch: 34	Loss: 0.016916	Acc: 59.9% (4795/8000)
[Test]  Epoch: 35	Loss: 0.016971	Acc: 60.1% (4806/8000)
[Test]  Epoch: 36	Loss: 0.016886	Acc: 60.3% (4824/8000)
[Test]  Epoch: 37	Loss: 0.016913	Acc: 60.2% (4816/8000)
[Test]  Epoch: 38	Loss: 0.016984	Acc: 60.0% (4801/8000)
[Test]  Epoch: 39	Loss: 0.016900	Acc: 60.3% (4824/8000)
[Test]  Epoch: 40	Loss: 0.016994	Acc: 60.0% (4797/8000)
[Test]  Epoch: 41	Loss: 0.016788	Acc: 60.6% (4852/8000)
[Test]  Epoch: 42	Loss: 0.016911	Acc: 60.0% (4801/8000)
[Test]  Epoch: 43	Loss: 0.016794	Acc: 60.3% (4824/8000)
[Test]  Epoch: 44	Loss: 0.016933	Acc: 60.5% (4838/8000)
[Test]  Epoch: 45	Loss: 0.016761	Acc: 60.6% (4848/8000)
[Test]  Epoch: 46	Loss: 0.016900	Acc: 60.1% (4810/8000)
[Test]  Epoch: 47	Loss: 0.016869	Acc: 60.1% (4805/8000)
[Test]  Epoch: 48	Loss: 0.016864	Acc: 60.0% (4802/8000)
[Test]  Epoch: 49	Loss: 0.016780	Acc: 60.7% (4855/8000)
[Test]  Epoch: 50	Loss: 0.016864	Acc: 60.1% (4810/8000)
[Test]  Epoch: 51	Loss: 0.016889	Acc: 60.1% (4807/8000)
[Test]  Epoch: 52	Loss: 0.016754	Acc: 60.5% (4836/8000)
[Test]  Epoch: 53	Loss: 0.016913	Acc: 60.4% (4832/8000)
[Test]  Epoch: 54	Loss: 0.016770	Acc: 60.6% (4846/8000)
[Test]  Epoch: 55	Loss: 0.016912	Acc: 60.2% (4816/8000)
[Test]  Epoch: 56	Loss: 0.016729	Acc: 60.9% (4873/8000)
[Test]  Epoch: 57	Loss: 0.016719	Acc: 60.8% (4861/8000)
[Test]  Epoch: 58	Loss: 0.016789	Acc: 60.5% (4844/8000)
[Test]  Epoch: 59	Loss: 0.016865	Acc: 60.2% (4813/8000)
[Test]  Epoch: 60	Loss: 0.016753	Acc: 60.8% (4867/8000)
[Test]  Epoch: 61	Loss: 0.016712	Acc: 60.9% (4868/8000)
[Test]  Epoch: 62	Loss: 0.016740	Acc: 60.8% (4864/8000)
[Test]  Epoch: 63	Loss: 0.016707	Acc: 60.9% (4870/8000)
[Test]  Epoch: 64	Loss: 0.016678	Acc: 61.0% (4877/8000)
[Test]  Epoch: 65	Loss: 0.016701	Acc: 60.9% (4873/8000)
[Test]  Epoch: 66	Loss: 0.016707	Acc: 60.9% (4873/8000)
[Test]  Epoch: 67	Loss: 0.016730	Acc: 60.5% (4837/8000)
[Test]  Epoch: 68	Loss: 0.016696	Acc: 60.8% (4866/8000)
[Test]  Epoch: 69	Loss: 0.016686	Acc: 60.7% (4859/8000)
[Test]  Epoch: 70	Loss: 0.016675	Acc: 60.9% (4875/8000)
[Test]  Epoch: 71	Loss: 0.016694	Acc: 60.8% (4863/8000)
[Test]  Epoch: 72	Loss: 0.016656	Acc: 60.8% (4861/8000)
[Test]  Epoch: 73	Loss: 0.016675	Acc: 60.7% (4858/8000)
[Test]  Epoch: 74	Loss: 0.016697	Acc: 60.8% (4864/8000)
[Test]  Epoch: 75	Loss: 0.016680	Acc: 60.7% (4858/8000)
[Test]  Epoch: 76	Loss: 0.016663	Acc: 60.8% (4866/8000)
[Test]  Epoch: 77	Loss: 0.016684	Acc: 60.8% (4861/8000)
[Test]  Epoch: 78	Loss: 0.016715	Acc: 60.6% (4847/8000)
[Test]  Epoch: 79	Loss: 0.016703	Acc: 60.6% (4851/8000)
[Test]  Epoch: 80	Loss: 0.016700	Acc: 60.7% (4853/8000)
[Test]  Epoch: 81	Loss: 0.016701	Acc: 60.6% (4850/8000)
[Test]  Epoch: 82	Loss: 0.016764	Acc: 60.8% (4863/8000)
[Test]  Epoch: 83	Loss: 0.016662	Acc: 60.7% (4858/8000)
[Test]  Epoch: 84	Loss: 0.016691	Acc: 60.8% (4862/8000)
[Test]  Epoch: 85	Loss: 0.016682	Acc: 60.9% (4869/8000)
[Test]  Epoch: 86	Loss: 0.016661	Acc: 60.7% (4859/8000)
[Test]  Epoch: 87	Loss: 0.016676	Acc: 60.7% (4859/8000)
[Test]  Epoch: 88	Loss: 0.016701	Acc: 60.8% (4863/8000)
[Test]  Epoch: 89	Loss: 0.016692	Acc: 60.6% (4849/8000)
[Test]  Epoch: 90	Loss: 0.016682	Acc: 60.6% (4852/8000)
[Test]  Epoch: 91	Loss: 0.016672	Acc: 60.8% (4862/8000)
[Test]  Epoch: 92	Loss: 0.016680	Acc: 60.7% (4856/8000)
[Test]  Epoch: 93	Loss: 0.016678	Acc: 60.9% (4870/8000)
[Test]  Epoch: 94	Loss: 0.016690	Acc: 60.7% (4858/8000)
[Test]  Epoch: 95	Loss: 0.016696	Acc: 60.8% (4867/8000)
[Test]  Epoch: 96	Loss: 0.016662	Acc: 60.8% (4863/8000)
[Test]  Epoch: 97	Loss: 0.016694	Acc: 60.9% (4875/8000)
[Test]  Epoch: 98	Loss: 0.016684	Acc: 60.8% (4866/8000)
[Test]  Epoch: 99	Loss: 0.016701	Acc: 60.7% (4856/8000)
[Test]  Epoch: 100	Loss: 0.016689	Acc: 60.9% (4868/8000)
===========finish==========
['2024-08-19', '20:42:47.000921', '100', 'test', '0.016689346492290497', '60.85', '60.9625']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=96
get_sample_layers not_random
protect_percent = 0.9 96 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.3.conv1.weight', 'layer2.0.conv1.weight', 'layer1.0.conv2.weight', 'layer3.1.conv1.weight', 'conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv3.weight', 'layer3.2.conv2.weight', 'layer3.3.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.conv2.weight', 'layer2.1.conv2.weight', 'layer1.0.downsample.0.weight', 'last_linear.weight', 'layer3.2.conv3.weight', 'layer3.0.conv1.weight', 'layer3.3.conv2.weight', 'layer3.3.conv3.weight', 'layer3.1.conv2.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer3.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.conv3.weight', 'layer3.5.conv1.weight', 'layer2.0.downsample.0.weight', 'layer3.5.conv3.weight', 'layer3.5.conv2.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.3.conv1.weight', 'layer2.0.conv1.weight', 'layer1.0.conv2.weight', 'layer3.1.conv1.weight', 'conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv3.weight', 'layer3.2.conv2.weight', 'layer3.3.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.conv2.weight', 'layer2.1.conv2.weight', 'layer1.0.downsample.0.weight', 'last_linear.weight', 'layer3.2.conv3.weight', 'layer3.0.conv1.weight', 'layer3.3.conv2.weight', 'layer3.3.conv3.weight', 'layer3.1.conv2.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer3.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.conv3.weight', 'layer3.5.conv1.weight', 'layer2.0.downsample.0.weight', 'layer3.5.conv3.weight', 'layer3.5.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.0.weight 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.conv1.weight 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.conv2.weight 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.conv3.weight 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.0.weight 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.conv1.weight 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.conv2.weight 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.conv3.weight 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.conv1.weight 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.conv2.weight 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.conv3.weight 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035881	Acc: 12.5% (1003/8000)
[Test]  Epoch: 2	Loss: 0.026946	Acc: 39.9% (3194/8000)
[Test]  Epoch: 3	Loss: 0.017276	Acc: 64.2% (5135/8000)
[Test]  Epoch: 4	Loss: 0.014362	Acc: 70.6% (5645/8000)
[Test]  Epoch: 5	Loss: 0.013170	Acc: 72.6% (5807/8000)
[Test]  Epoch: 6	Loss: 0.012716	Acc: 73.4% (5869/8000)
[Test]  Epoch: 7	Loss: 0.012399	Acc: 73.8% (5906/8000)
[Test]  Epoch: 8	Loss: 0.012298	Acc: 73.7% (5892/8000)
[Test]  Epoch: 9	Loss: 0.012353	Acc: 73.7% (5896/8000)
[Test]  Epoch: 10	Loss: 0.012245	Acc: 73.8% (5900/8000)
[Test]  Epoch: 11	Loss: 0.012054	Acc: 74.2% (5932/8000)
[Test]  Epoch: 12	Loss: 0.012220	Acc: 73.4% (5869/8000)
[Test]  Epoch: 13	Loss: 0.011953	Acc: 74.3% (5945/8000)
[Test]  Epoch: 14	Loss: 0.011901	Acc: 74.6% (5971/8000)
[Test]  Epoch: 15	Loss: 0.012109	Acc: 73.8% (5900/8000)
[Test]  Epoch: 16	Loss: 0.011921	Acc: 74.2% (5939/8000)
[Test]  Epoch: 17	Loss: 0.011942	Acc: 74.8% (5980/8000)
[Test]  Epoch: 18	Loss: 0.012066	Acc: 73.8% (5907/8000)
[Test]  Epoch: 19	Loss: 0.011922	Acc: 74.3% (5947/8000)
[Test]  Epoch: 20	Loss: 0.011976	Acc: 74.3% (5942/8000)
[Test]  Epoch: 21	Loss: 0.011797	Acc: 74.9% (5991/8000)
[Test]  Epoch: 22	Loss: 0.011922	Acc: 74.5% (5963/8000)
[Test]  Epoch: 23	Loss: 0.011858	Acc: 74.4% (5954/8000)
[Test]  Epoch: 24	Loss: 0.011807	Acc: 74.5% (5957/8000)
[Test]  Epoch: 25	Loss: 0.012000	Acc: 73.8% (5903/8000)
[Test]  Epoch: 26	Loss: 0.011780	Acc: 75.1% (6010/8000)
[Test]  Epoch: 27	Loss: 0.011924	Acc: 74.2% (5935/8000)
[Test]  Epoch: 28	Loss: 0.011827	Acc: 74.6% (5968/8000)
[Test]  Epoch: 29	Loss: 0.011748	Acc: 74.7% (5975/8000)
[Test]  Epoch: 30	Loss: 0.011777	Acc: 74.7% (5976/8000)
[Test]  Epoch: 31	Loss: 0.011729	Acc: 75.2% (6014/8000)
[Test]  Epoch: 32	Loss: 0.011771	Acc: 74.7% (5974/8000)
[Test]  Epoch: 33	Loss: 0.011811	Acc: 74.6% (5971/8000)
[Test]  Epoch: 34	Loss: 0.011726	Acc: 75.0% (5999/8000)
[Test]  Epoch: 35	Loss: 0.011811	Acc: 74.7% (5978/8000)
[Test]  Epoch: 36	Loss: 0.011743	Acc: 75.2% (6017/8000)
[Test]  Epoch: 37	Loss: 0.011732	Acc: 74.7% (5976/8000)
[Test]  Epoch: 38	Loss: 0.011747	Acc: 74.7% (5974/8000)
[Test]  Epoch: 39	Loss: 0.011768	Acc: 74.7% (5974/8000)
[Test]  Epoch: 40	Loss: 0.011786	Acc: 74.8% (5984/8000)
[Test]  Epoch: 41	Loss: 0.011802	Acc: 74.6% (5965/8000)
[Test]  Epoch: 42	Loss: 0.011772	Acc: 74.5% (5964/8000)
[Test]  Epoch: 43	Loss: 0.011744	Acc: 75.0% (6000/8000)
[Test]  Epoch: 44	Loss: 0.011798	Acc: 74.7% (5976/8000)
[Test]  Epoch: 45	Loss: 0.011672	Acc: 75.1% (6010/8000)
[Test]  Epoch: 46	Loss: 0.011744	Acc: 74.9% (5989/8000)
[Test]  Epoch: 47	Loss: 0.011834	Acc: 74.4% (5949/8000)
[Test]  Epoch: 48	Loss: 0.011724	Acc: 75.0% (6000/8000)
[Test]  Epoch: 49	Loss: 0.011704	Acc: 75.1% (6005/8000)
[Test]  Epoch: 50	Loss: 0.011709	Acc: 75.2% (6014/8000)
[Test]  Epoch: 51	Loss: 0.011797	Acc: 74.9% (5994/8000)
[Test]  Epoch: 52	Loss: 0.011727	Acc: 75.2% (6012/8000)
[Test]  Epoch: 53	Loss: 0.011755	Acc: 75.0% (5999/8000)
[Test]  Epoch: 54	Loss: 0.011824	Acc: 74.5% (5964/8000)
[Test]  Epoch: 55	Loss: 0.011906	Acc: 74.2% (5933/8000)
[Test]  Epoch: 56	Loss: 0.011847	Acc: 74.7% (5976/8000)
[Test]  Epoch: 57	Loss: 0.011753	Acc: 74.9% (5993/8000)
[Test]  Epoch: 58	Loss: 0.011839	Acc: 74.5% (5958/8000)
[Test]  Epoch: 59	Loss: 0.011698	Acc: 75.0% (6002/8000)
[Test]  Epoch: 60	Loss: 0.011781	Acc: 74.9% (5991/8000)
[Test]  Epoch: 61	Loss: 0.011774	Acc: 74.9% (5994/8000)
[Test]  Epoch: 62	Loss: 0.011769	Acc: 75.0% (6004/8000)
[Test]  Epoch: 63	Loss: 0.011746	Acc: 75.2% (6014/8000)
[Test]  Epoch: 64	Loss: 0.011779	Acc: 75.0% (5997/8000)
[Test]  Epoch: 65	Loss: 0.011794	Acc: 75.0% (6003/8000)
[Test]  Epoch: 66	Loss: 0.011775	Acc: 75.0% (5996/8000)
[Test]  Epoch: 67	Loss: 0.011763	Acc: 75.0% (6001/8000)
[Test]  Epoch: 68	Loss: 0.011754	Acc: 75.0% (5998/8000)
[Test]  Epoch: 69	Loss: 0.011750	Acc: 75.0% (6004/8000)
[Test]  Epoch: 70	Loss: 0.011756	Acc: 75.1% (6005/8000)
[Test]  Epoch: 71	Loss: 0.011751	Acc: 75.0% (5999/8000)
[Test]  Epoch: 72	Loss: 0.011751	Acc: 75.0% (5997/8000)
[Test]  Epoch: 73	Loss: 0.011757	Acc: 74.9% (5993/8000)
[Test]  Epoch: 74	Loss: 0.011741	Acc: 75.0% (6001/8000)
[Test]  Epoch: 75	Loss: 0.011737	Acc: 75.1% (6006/8000)
[Test]  Epoch: 76	Loss: 0.011702	Acc: 75.1% (6005/8000)
[Test]  Epoch: 77	Loss: 0.011741	Acc: 75.1% (6009/8000)
[Test]  Epoch: 78	Loss: 0.011743	Acc: 75.1% (6007/8000)
[Test]  Epoch: 79	Loss: 0.011766	Acc: 75.0% (6000/8000)
[Test]  Epoch: 80	Loss: 0.011751	Acc: 75.0% (5998/8000)
[Test]  Epoch: 81	Loss: 0.011742	Acc: 75.0% (6002/8000)
[Test]  Epoch: 82	Loss: 0.011793	Acc: 75.0% (6000/8000)
[Test]  Epoch: 83	Loss: 0.011724	Acc: 75.1% (6010/8000)
[Test]  Epoch: 84	Loss: 0.011725	Acc: 75.0% (6001/8000)
[Test]  Epoch: 85	Loss: 0.011762	Acc: 75.1% (6010/8000)
[Test]  Epoch: 86	Loss: 0.011749	Acc: 75.1% (6005/8000)
[Test]  Epoch: 87	Loss: 0.011721	Acc: 75.0% (5999/8000)
[Test]  Epoch: 88	Loss: 0.011755	Acc: 75.2% (6020/8000)
[Test]  Epoch: 89	Loss: 0.011769	Acc: 74.9% (5990/8000)
[Test]  Epoch: 90	Loss: 0.011755	Acc: 75.1% (6009/8000)
[Test]  Epoch: 91	Loss: 0.011769	Acc: 75.0% (5998/8000)
[Test]  Epoch: 92	Loss: 0.011736	Acc: 75.0% (6003/8000)
[Test]  Epoch: 93	Loss: 0.011754	Acc: 75.0% (6003/8000)
[Test]  Epoch: 94	Loss: 0.011750	Acc: 75.2% (6012/8000)
[Test]  Epoch: 95	Loss: 0.011738	Acc: 75.0% (6001/8000)
[Test]  Epoch: 96	Loss: 0.011781	Acc: 74.9% (5990/8000)
[Test]  Epoch: 97	Loss: 0.011725	Acc: 75.0% (6003/8000)
[Test]  Epoch: 98	Loss: 0.011750	Acc: 74.9% (5991/8000)
[Test]  Epoch: 99	Loss: 0.011754	Acc: 75.0% (6001/8000)
[Test]  Epoch: 100	Loss: 0.011751	Acc: 75.0% (6002/8000)
===========finish==========
['2024-08-19', '20:52:10.020773', '100', 'test', '0.011751026842743158', '75.025', '75.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/stl10-resnet50-channel resnet50 STL10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.01 --victim_dir models/victim/stl10-resnet50 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 1000 samples, 10 classes
Files already downloaded and verified
8000
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=107  num_select=107
get_sample_layers not_random
protect_percent = 1.0 107 ['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.3.conv1.weight', 'layer2.0.conv1.weight', 'layer1.0.conv2.weight', 'layer3.1.conv1.weight', 'conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv3.weight', 'layer3.2.conv2.weight', 'layer3.3.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.conv2.weight', 'layer2.1.conv2.weight', 'layer1.0.downsample.0.weight', 'last_linear.weight', 'layer3.2.conv3.weight', 'layer3.0.conv1.weight', 'layer3.3.conv2.weight', 'layer3.3.conv3.weight', 'layer3.1.conv2.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer3.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.conv3.weight', 'layer3.5.conv1.weight', 'layer2.0.downsample.0.weight', 'layer3.5.conv3.weight', 'layer3.5.conv2.weight', 'layer4.2.conv1.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv3.weight', 'layer4.2.conv2.weight', 'layer4.2.conv3.weight', 'layer4.0.conv1.weight', 'layer4.1.conv2.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight']
['layer2.2.bn1.weight', 'layer3.2.bn1.weight', 'layer2.3.bn2.weight', 'layer2.2.bn2.weight', 'layer3.2.bn2.weight', 'layer3.3.bn1.weight', 'layer3.4.bn1.weight', 'layer1.1.bn1.weight', 'layer2.3.bn1.weight', 'layer1.2.bn2.weight', 'layer1.2.bn1.weight', 'layer3.1.bn1.weight', 'layer3.3.bn2.weight', 'layer1.1.bn2.weight', 'layer3.1.bn2.weight', 'layer3.4.bn2.weight', 'layer2.1.bn1.weight', 'layer2.0.bn1.weight', 'layer1.0.bn1.weight', 'layer3.0.bn1.weight', 'layer2.0.bn2.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.5.bn1.weight', 'layer2.1.bn2.weight', 'layer1.2.bn3.weight', 'layer2.3.bn3.weight', 'layer3.5.bn2.weight', 'layer2.2.bn3.weight', 'layer1.1.bn3.weight', 'layer4.2.bn3.weight', 'layer4.1.bn3.weight', 'bn1.weight', 'layer1.0.downsample.1.weight', 'layer1.0.bn3.weight', 'layer4.2.bn2.weight', 'layer2.0.bn3.weight', 'layer2.1.bn3.weight', 'layer4.2.bn1.weight', 'layer3.2.bn3.weight', 'layer2.0.downsample.1.weight', 'layer3.3.bn3.weight', 'layer3.1.bn3.weight', 'layer4.1.bn1.weight', 'layer4.0.downsample.1.weight', 'layer3.4.bn3.weight', 'layer3.0.bn3.weight', 'layer4.1.bn2.weight', 'layer4.0.bn3.weight', 'layer3.5.bn3.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn2.weight', 'layer2.2.conv1.weight', 'layer4.0.bn1.weight', 'layer1.2.conv3.weight', 'layer2.2.conv2.weight', 'layer1.1.conv3.weight', 'layer1.0.conv1.weight', 'layer1.1.conv1.weight', 'layer1.2.conv1.weight', 'layer2.3.conv3.weight', 'layer1.2.conv2.weight', 'layer2.2.conv3.weight', 'layer2.3.conv2.weight', 'layer1.0.conv3.weight', 'layer3.2.conv1.weight', 'layer1.1.conv2.weight', 'layer2.1.conv1.weight', 'layer2.3.conv1.weight', 'layer2.0.conv1.weight', 'layer1.0.conv2.weight', 'layer3.1.conv1.weight', 'conv1.weight', 'layer2.1.conv3.weight', 'layer2.0.conv3.weight', 'layer3.2.conv2.weight', 'layer3.3.conv1.weight', 'layer3.4.conv1.weight', 'layer2.0.conv2.weight', 'layer2.1.conv2.weight', 'layer1.0.downsample.0.weight', 'last_linear.weight', 'layer3.2.conv3.weight', 'layer3.0.conv1.weight', 'layer3.3.conv2.weight', 'layer3.3.conv3.weight', 'layer3.1.conv2.weight', 'layer3.4.conv2.weight', 'layer3.0.conv2.weight', 'layer3.1.conv3.weight', 'layer3.4.conv3.weight', 'layer3.0.conv3.weight', 'layer3.5.conv1.weight', 'layer2.0.downsample.0.weight', 'layer3.5.conv3.weight', 'layer3.5.conv2.weight', 'layer4.2.conv1.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv3.weight', 'layer4.2.conv2.weight', 'layer4.2.conv3.weight', 'layer4.0.conv1.weight', 'layer4.1.conv2.weight', 'layer4.0.conv3.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight']
bn1.bias 1111111111
bn1.running_mean 1111111111
bn1.running_var 1111111111
bn1.num_batches_tracked 1111111111
layer1.0.bn1.bias 1111111111
layer1.0.bn1.running_mean 1111111111
layer1.0.bn1.running_var 1111111111
layer1.0.bn1.num_batches_tracked 1111111111
layer1.0.bn2.bias 1111111111
layer1.0.bn2.running_mean 1111111111
layer1.0.bn2.running_var 1111111111
layer1.0.bn2.num_batches_tracked 1111111111
layer1.0.bn3.bias 1111111111
layer1.0.bn3.running_mean 1111111111
layer1.0.bn3.running_var 1111111111
layer1.0.bn3.num_batches_tracked 1111111111
layer1.0.downsample.1.bias 1111111111
layer1.0.downsample.1.running_mean 1111111111
layer1.0.downsample.1.running_var 1111111111
layer1.0.downsample.1.num_batches_tracked 1111111111
layer1.1.bn1.bias 1111111111
layer1.1.bn1.running_mean 1111111111
layer1.1.bn1.running_var 1111111111
layer1.1.bn1.num_batches_tracked 1111111111
layer1.1.bn2.bias 1111111111
layer1.1.bn2.running_mean 1111111111
layer1.1.bn2.running_var 1111111111
layer1.1.bn2.num_batches_tracked 1111111111
layer1.1.bn3.bias 1111111111
layer1.1.bn3.running_mean 1111111111
layer1.1.bn3.running_var 1111111111
layer1.1.bn3.num_batches_tracked 1111111111
layer1.2.bn1.bias 1111111111
layer1.2.bn1.running_mean 1111111111
layer1.2.bn1.running_var 1111111111
layer1.2.bn1.num_batches_tracked 1111111111
layer1.2.bn2.bias 1111111111
layer1.2.bn2.running_mean 1111111111
layer1.2.bn2.running_var 1111111111
layer1.2.bn2.num_batches_tracked 1111111111
layer1.2.bn3.bias 1111111111
layer1.2.bn3.running_mean 1111111111
layer1.2.bn3.running_var 1111111111
layer1.2.bn3.num_batches_tracked 1111111111
layer2.0.bn1.bias 1111111111
layer2.0.bn1.running_mean 1111111111
layer2.0.bn1.running_var 1111111111
layer2.0.bn1.num_batches_tracked 1111111111
layer2.0.bn2.bias 1111111111
layer2.0.bn2.running_mean 1111111111
layer2.0.bn2.running_var 1111111111
layer2.0.bn2.num_batches_tracked 1111111111
layer2.0.bn3.bias 1111111111
layer2.0.bn3.running_mean 1111111111
layer2.0.bn3.running_var 1111111111
layer2.0.bn3.num_batches_tracked 1111111111
layer2.0.downsample.1.bias 1111111111
layer2.0.downsample.1.running_mean 1111111111
layer2.0.downsample.1.running_var 1111111111
layer2.0.downsample.1.num_batches_tracked 1111111111
layer2.1.bn1.bias 1111111111
layer2.1.bn1.running_mean 1111111111
layer2.1.bn1.running_var 1111111111
layer2.1.bn1.num_batches_tracked 1111111111
layer2.1.bn2.bias 1111111111
layer2.1.bn2.running_mean 1111111111
layer2.1.bn2.running_var 1111111111
layer2.1.bn2.num_batches_tracked 1111111111
layer2.1.bn3.bias 1111111111
layer2.1.bn3.running_mean 1111111111
layer2.1.bn3.running_var 1111111111
layer2.1.bn3.num_batches_tracked 1111111111
layer2.2.bn1.bias 1111111111
layer2.2.bn1.running_mean 1111111111
layer2.2.bn1.running_var 1111111111
layer2.2.bn1.num_batches_tracked 1111111111
layer2.2.bn2.bias 1111111111
layer2.2.bn2.running_mean 1111111111
layer2.2.bn2.running_var 1111111111
layer2.2.bn2.num_batches_tracked 1111111111
layer2.2.bn3.bias 1111111111
layer2.2.bn3.running_mean 1111111111
layer2.2.bn3.running_var 1111111111
layer2.2.bn3.num_batches_tracked 1111111111
layer2.3.bn1.bias 1111111111
layer2.3.bn1.running_mean 1111111111
layer2.3.bn1.running_var 1111111111
layer2.3.bn1.num_batches_tracked 1111111111
layer2.3.bn2.bias 1111111111
layer2.3.bn2.running_mean 1111111111
layer2.3.bn2.running_var 1111111111
layer2.3.bn2.num_batches_tracked 1111111111
layer2.3.bn3.bias 1111111111
layer2.3.bn3.running_mean 1111111111
layer2.3.bn3.running_var 1111111111
layer2.3.bn3.num_batches_tracked 1111111111
layer3.0.bn1.bias 1111111111
layer3.0.bn1.running_mean 1111111111
layer3.0.bn1.running_var 1111111111
layer3.0.bn1.num_batches_tracked 1111111111
layer3.0.bn2.bias 1111111111
layer3.0.bn2.running_mean 1111111111
layer3.0.bn2.running_var 1111111111
layer3.0.bn2.num_batches_tracked 1111111111
layer3.0.bn3.bias 1111111111
layer3.0.bn3.running_mean 1111111111
layer3.0.bn3.running_var 1111111111
layer3.0.bn3.num_batches_tracked 1111111111
layer3.0.downsample.1.bias 1111111111
layer3.0.downsample.1.running_mean 1111111111
layer3.0.downsample.1.running_var 1111111111
layer3.0.downsample.1.num_batches_tracked 1111111111
layer3.1.bn1.bias 1111111111
layer3.1.bn1.running_mean 1111111111
layer3.1.bn1.running_var 1111111111
layer3.1.bn1.num_batches_tracked 1111111111
layer3.1.bn2.bias 1111111111
layer3.1.bn2.running_mean 1111111111
layer3.1.bn2.running_var 1111111111
layer3.1.bn2.num_batches_tracked 1111111111
layer3.1.bn3.bias 1111111111
layer3.1.bn3.running_mean 1111111111
layer3.1.bn3.running_var 1111111111
layer3.1.bn3.num_batches_tracked 1111111111
layer3.2.bn1.bias 1111111111
layer3.2.bn1.running_mean 1111111111
layer3.2.bn1.running_var 1111111111
layer3.2.bn1.num_batches_tracked 1111111111
layer3.2.bn2.bias 1111111111
layer3.2.bn2.running_mean 1111111111
layer3.2.bn2.running_var 1111111111
layer3.2.bn2.num_batches_tracked 1111111111
layer3.2.bn3.bias 1111111111
layer3.2.bn3.running_mean 1111111111
layer3.2.bn3.running_var 1111111111
layer3.2.bn3.num_batches_tracked 1111111111
layer3.3.bn1.bias 1111111111
layer3.3.bn1.running_mean 1111111111
layer3.3.bn1.running_var 1111111111
layer3.3.bn1.num_batches_tracked 1111111111
layer3.3.bn2.bias 1111111111
layer3.3.bn2.running_mean 1111111111
layer3.3.bn2.running_var 1111111111
layer3.3.bn2.num_batches_tracked 1111111111
layer3.3.bn3.bias 1111111111
layer3.3.bn3.running_mean 1111111111
layer3.3.bn3.running_var 1111111111
layer3.3.bn3.num_batches_tracked 1111111111
layer3.4.bn1.bias 1111111111
layer3.4.bn1.running_mean 1111111111
layer3.4.bn1.running_var 1111111111
layer3.4.bn1.num_batches_tracked 1111111111
layer3.4.bn2.bias 1111111111
layer3.4.bn2.running_mean 1111111111
layer3.4.bn2.running_var 1111111111
layer3.4.bn2.num_batches_tracked 1111111111
layer3.4.bn3.bias 1111111111
layer3.4.bn3.running_mean 1111111111
layer3.4.bn3.running_var 1111111111
layer3.4.bn3.num_batches_tracked 1111111111
layer3.5.bn1.bias 1111111111
layer3.5.bn1.running_mean 1111111111
layer3.5.bn1.running_var 1111111111
layer3.5.bn1.num_batches_tracked 1111111111
layer3.5.bn2.bias 1111111111
layer3.5.bn2.running_mean 1111111111
layer3.5.bn2.running_var 1111111111
layer3.5.bn2.num_batches_tracked 1111111111
layer3.5.bn3.bias 1111111111
layer3.5.bn3.running_mean 1111111111
layer3.5.bn3.running_var 1111111111
layer3.5.bn3.num_batches_tracked 1111111111
layer4.0.bn1.bias 1111111111
layer4.0.bn1.running_mean 1111111111
layer4.0.bn1.running_var 1111111111
layer4.0.bn1.num_batches_tracked 1111111111
layer4.0.bn2.bias 1111111111
layer4.0.bn2.running_mean 1111111111
layer4.0.bn2.running_var 1111111111
layer4.0.bn2.num_batches_tracked 1111111111
layer4.0.bn3.bias 1111111111
layer4.0.bn3.running_mean 1111111111
layer4.0.bn3.running_var 1111111111
layer4.0.bn3.num_batches_tracked 1111111111
layer4.0.downsample.1.bias 1111111111
layer4.0.downsample.1.running_mean 1111111111
layer4.0.downsample.1.running_var 1111111111
layer4.0.downsample.1.num_batches_tracked 1111111111
layer4.1.bn1.bias 1111111111
layer4.1.bn1.running_mean 1111111111
layer4.1.bn1.running_var 1111111111
layer4.1.bn1.num_batches_tracked 1111111111
layer4.1.bn2.bias 1111111111
layer4.1.bn2.running_mean 1111111111
layer4.1.bn2.running_var 1111111111
layer4.1.bn2.num_batches_tracked 1111111111
layer4.1.bn3.bias 1111111111
layer4.1.bn3.running_mean 1111111111
layer4.1.bn3.running_var 1111111111
layer4.1.bn3.num_batches_tracked 1111111111
layer4.2.bn1.bias 1111111111
layer4.2.bn1.running_mean 1111111111
layer4.2.bn1.running_var 1111111111
layer4.2.bn1.num_batches_tracked 1111111111
layer4.2.bn2.bias 1111111111
layer4.2.bn2.running_mean 1111111111
layer4.2.bn2.running_var 1111111111
layer4.2.bn2.num_batches_tracked 1111111111
layer4.2.bn3.bias 1111111111
layer4.2.bn3.running_mean 1111111111
layer4.2.bn3.running_var 1111111111
layer4.2.bn3.num_batches_tracked 1111111111
last_linear.bias 1111111111

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036713	Acc: 11.6% (930/8000)
[Test]  Epoch: 2	Loss: 0.024555	Acc: 48.3% (3861/8000)
[Test]  Epoch: 3	Loss: 0.014295	Acc: 71.4% (5709/8000)
[Test]  Epoch: 4	Loss: 0.012561	Acc: 75.6% (6051/8000)
[Test]  Epoch: 5	Loss: 0.011938	Acc: 76.3% (6106/8000)
[Test]  Epoch: 6	Loss: 0.011723	Acc: 76.6% (6126/8000)
[Test]  Epoch: 7	Loss: 0.011439	Acc: 77.4% (6194/8000)
[Test]  Epoch: 8	Loss: 0.011458	Acc: 76.7% (6136/8000)
[Test]  Epoch: 9	Loss: 0.011433	Acc: 76.9% (6154/8000)
[Test]  Epoch: 10	Loss: 0.011423	Acc: 77.0% (6163/8000)
[Test]  Epoch: 11	Loss: 0.011318	Acc: 77.5% (6200/8000)
[Test]  Epoch: 12	Loss: 0.011356	Acc: 77.0% (6157/8000)
[Test]  Epoch: 13	Loss: 0.011262	Acc: 77.2% (6177/8000)
[Test]  Epoch: 14	Loss: 0.011273	Acc: 77.0% (6156/8000)
[Test]  Epoch: 15	Loss: 0.011262	Acc: 77.1% (6169/8000)
[Test]  Epoch: 16	Loss: 0.011158	Acc: 77.1% (6165/8000)
[Test]  Epoch: 17	Loss: 0.011226	Acc: 77.6% (6206/8000)
[Test]  Epoch: 18	Loss: 0.011224	Acc: 77.2% (6179/8000)
[Test]  Epoch: 19	Loss: 0.011164	Acc: 77.5% (6197/8000)
[Test]  Epoch: 20	Loss: 0.011252	Acc: 77.3% (6187/8000)
[Test]  Epoch: 21	Loss: 0.011221	Acc: 77.3% (6183/8000)
[Test]  Epoch: 22	Loss: 0.011181	Acc: 77.4% (6194/8000)
[Test]  Epoch: 23	Loss: 0.011052	Acc: 77.7% (6217/8000)
[Test]  Epoch: 24	Loss: 0.011127	Acc: 77.5% (6204/8000)
[Test]  Epoch: 25	Loss: 0.011194	Acc: 77.1% (6166/8000)
[Test]  Epoch: 26	Loss: 0.011190	Acc: 77.2% (6174/8000)
[Test]  Epoch: 27	Loss: 0.011134	Acc: 77.2% (6172/8000)
[Test]  Epoch: 28	Loss: 0.011169	Acc: 77.3% (6183/8000)
[Test]  Epoch: 29	Loss: 0.011097	Acc: 77.3% (6185/8000)
[Test]  Epoch: 30	Loss: 0.011100	Acc: 77.3% (6183/8000)
[Test]  Epoch: 31	Loss: 0.011179	Acc: 77.1% (6168/8000)
[Test]  Epoch: 32	Loss: 0.011061	Acc: 77.6% (6206/8000)
[Test]  Epoch: 33	Loss: 0.011206	Acc: 77.2% (6173/8000)
[Test]  Epoch: 34	Loss: 0.011052	Acc: 77.7% (6217/8000)
[Test]  Epoch: 35	Loss: 0.011101	Acc: 77.6% (6207/8000)
[Test]  Epoch: 36	Loss: 0.011139	Acc: 77.6% (6209/8000)
[Test]  Epoch: 37	Loss: 0.011138	Acc: 77.5% (6202/8000)
[Test]  Epoch: 38	Loss: 0.011101	Acc: 77.2% (6178/8000)
[Test]  Epoch: 39	Loss: 0.011094	Acc: 77.6% (6209/8000)
[Test]  Epoch: 40	Loss: 0.011107	Acc: 77.4% (6195/8000)
[Test]  Epoch: 41	Loss: 0.011167	Acc: 77.3% (6184/8000)
[Test]  Epoch: 42	Loss: 0.011144	Acc: 77.0% (6164/8000)
[Test]  Epoch: 43	Loss: 0.011066	Acc: 77.3% (6184/8000)
[Test]  Epoch: 44	Loss: 0.011172	Acc: 76.8% (6148/8000)
[Test]  Epoch: 45	Loss: 0.011047	Acc: 77.3% (6188/8000)
[Test]  Epoch: 46	Loss: 0.011105	Acc: 77.7% (6216/8000)
[Test]  Epoch: 47	Loss: 0.011122	Acc: 77.2% (6172/8000)
[Test]  Epoch: 48	Loss: 0.011053	Acc: 77.5% (6204/8000)
[Test]  Epoch: 49	Loss: 0.011048	Acc: 77.5% (6198/8000)
[Test]  Epoch: 50	Loss: 0.011065	Acc: 77.7% (6212/8000)
[Test]  Epoch: 51	Loss: 0.011209	Acc: 77.1% (6166/8000)
[Test]  Epoch: 52	Loss: 0.011160	Acc: 77.4% (6191/8000)
[Test]  Epoch: 53	Loss: 0.011059	Acc: 77.5% (6196/8000)
[Test]  Epoch: 54	Loss: 0.011239	Acc: 77.0% (6160/8000)
[Test]  Epoch: 55	Loss: 0.011190	Acc: 76.8% (6148/8000)
[Test]  Epoch: 56	Loss: 0.011151	Acc: 77.3% (6183/8000)
[Test]  Epoch: 57	Loss: 0.011159	Acc: 77.5% (6203/8000)
[Test]  Epoch: 58	Loss: 0.011196	Acc: 77.4% (6193/8000)
[Test]  Epoch: 59	Loss: 0.011112	Acc: 77.6% (6209/8000)
[Test]  Epoch: 60	Loss: 0.011125	Acc: 77.3% (6187/8000)
[Test]  Epoch: 61	Loss: 0.011157	Acc: 77.3% (6182/8000)
[Test]  Epoch: 62	Loss: 0.011164	Acc: 77.2% (6175/8000)
[Test]  Epoch: 63	Loss: 0.011105	Acc: 77.4% (6191/8000)
[Test]  Epoch: 64	Loss: 0.011137	Acc: 77.3% (6188/8000)
[Test]  Epoch: 65	Loss: 0.011105	Acc: 77.4% (6195/8000)
[Test]  Epoch: 66	Loss: 0.011134	Acc: 77.3% (6185/8000)
[Test]  Epoch: 67	Loss: 0.011102	Acc: 77.4% (6195/8000)
[Test]  Epoch: 68	Loss: 0.011132	Acc: 77.4% (6194/8000)
[Test]  Epoch: 69	Loss: 0.011121	Acc: 77.4% (6192/8000)
[Test]  Epoch: 70	Loss: 0.011132	Acc: 77.4% (6191/8000)
[Test]  Epoch: 71	Loss: 0.011120	Acc: 77.3% (6186/8000)
[Test]  Epoch: 72	Loss: 0.011117	Acc: 77.5% (6199/8000)
[Test]  Epoch: 73	Loss: 0.011122	Acc: 77.3% (6188/8000)
[Test]  Epoch: 74	Loss: 0.011143	Acc: 77.4% (6193/8000)
[Test]  Epoch: 75	Loss: 0.011135	Acc: 77.5% (6198/8000)
[Test]  Epoch: 76	Loss: 0.011112	Acc: 77.4% (6193/8000)
[Test]  Epoch: 77	Loss: 0.011123	Acc: 77.5% (6202/8000)
[Test]  Epoch: 78	Loss: 0.011131	Acc: 77.7% (6212/8000)
[Test]  Epoch: 79	Loss: 0.011139	Acc: 77.5% (6201/8000)
[Test]  Epoch: 80	Loss: 0.011136	Acc: 77.5% (6198/8000)
[Test]  Epoch: 81	Loss: 0.011127	Acc: 77.4% (6190/8000)
[Test]  Epoch: 82	Loss: 0.011170	Acc: 77.4% (6190/8000)
[Test]  Epoch: 83	Loss: 0.011128	Acc: 77.3% (6185/8000)
[Test]  Epoch: 84	Loss: 0.011118	Acc: 77.3% (6187/8000)
[Test]  Epoch: 85	Loss: 0.011136	Acc: 77.3% (6188/8000)
[Test]  Epoch: 86	Loss: 0.011110	Acc: 77.6% (6206/8000)
[Test]  Epoch: 87	Loss: 0.011060	Acc: 77.7% (6212/8000)
[Test]  Epoch: 88	Loss: 0.011144	Acc: 77.4% (6192/8000)
[Test]  Epoch: 89	Loss: 0.011160	Acc: 77.4% (6191/8000)
[Test]  Epoch: 90	Loss: 0.011145	Acc: 77.3% (6187/8000)
[Test]  Epoch: 91	Loss: 0.011144	Acc: 77.3% (6188/8000)
[Test]  Epoch: 92	Loss: 0.011122	Acc: 77.4% (6190/8000)
[Test]  Epoch: 93	Loss: 0.011106	Acc: 77.5% (6196/8000)
[Test]  Epoch: 94	Loss: 0.011111	Acc: 77.3% (6183/8000)
[Test]  Epoch: 95	Loss: 0.011141	Acc: 77.3% (6183/8000)
[Test]  Epoch: 96	Loss: 0.011192	Acc: 77.3% (6182/8000)
[Test]  Epoch: 97	Loss: 0.011123	Acc: 77.6% (6209/8000)
[Test]  Epoch: 98	Loss: 0.011124	Acc: 77.6% (6206/8000)
[Test]  Epoch: 99	Loss: 0.011172	Acc: 77.5% (6196/8000)
[Test]  Epoch: 100	Loss: 0.011105	Acc: 77.4% (6194/8000)
===========finish==========
['2024-08-19', '21:01:23.563739', '100', 'test', '0.011104811321943998', '77.425', '77.7125']
