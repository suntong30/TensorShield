result path:  /home/gpu2/jbw/other_XAI_positive/knockoffnets/ms_elastictrainer_result_resnet18_vgg16_mobilenetv2.csv
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info does not exist. Calculating...
Load model from models/victim/cifar100-vgg16_bn/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('features.24.weight', 249.52957153320312), ('features.27.weight', 245.88970947265625), ('features.30.weight', 216.82960510253906), ('features.20.weight', 190.637451171875), ('features.17.weight', 187.53468322753906), ('features.14.weight', 177.380126953125), ('features.34.weight', 171.71876525878906), ('features.37.weight', 164.35182189941406), ('features.10.weight', 137.75526428222656), ('features.40.weight', 124.7841796875), ('features.7.weight', 107.66992950439453), ('features.3.weight', 89.57909393310547), ('features.0.weight', 35.2643928527832), ('classifier.weight', 25.907163619995117), ('features.38.weight', 21.660518646240234), ('features.21.weight', 16.214157104492188), ('features.31.weight', 15.70991325378418), ('features.11.weight', 15.226625442504883), ('features.25.weight', 13.497426986694336), ('features.28.weight', 13.332740783691406), ('features.35.weight', 13.111217498779297), ('features.15.weight', 12.557247161865234), ('features.18.weight', 12.334051132202148), ('features.4.weight', 10.412137031555176), ('features.8.weight', 8.327202796936035), ('features.1.weight', 3.9792394638061523), ('features.41.weight', 0.8090578317642212), ('features.0.bias', 0.0), ('features.1.bias', 0.0), ('features.3.bias', 0.0), ('features.4.bias', 0.0), ('features.7.bias', 0.0), ('features.8.bias', 0.0), ('features.10.bias', 0.0), ('features.11.bias', 0.0), ('features.14.bias', 0.0), ('features.15.bias', 0.0), ('features.17.bias', 0.0), ('features.18.bias', 0.0), ('features.20.bias', 0.0), ('features.21.bias', 0.0), ('features.24.bias', 0.0), ('features.25.bias', 0.0), ('features.27.bias', 0.0), ('features.28.bias', 0.0), ('features.30.bias', 0.0), ('features.31.bias', 0.0), ('features.34.bias', 0.0), ('features.35.bias', 0.0), ('features.37.bias', 0.0), ('features.38.bias', 0.0), ('features.40.bias', 0.0), ('features.41.bias', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('features.24.weight', 249.52957153320312), ('features.27.weight', 245.88970947265625), ('features.30.weight', 216.82960510253906), ('features.20.weight', 190.637451171875), ('features.17.weight', 187.53468322753906), ('features.14.weight', 177.380126953125), ('features.34.weight', 171.71876525878906), ('features.37.weight', 164.35182189941406), ('features.10.weight', 137.75526428222656), ('features.40.weight', 124.7841796875), ('features.7.weight', 107.66992950439453), ('features.3.weight', 89.57909393310547), ('features.0.weight', 35.2643928527832), ('classifier.weight', 25.907163619995117), ('features.38.weight', 21.660518646240234), ('features.21.weight', 16.214157104492188), ('features.31.weight', 15.70991325378418), ('features.11.weight', 15.226625442504883), ('features.25.weight', 13.497426986694336), ('features.28.weight', 13.332740783691406), ('features.35.weight', 13.111217498779297), ('features.15.weight', 12.557247161865234), ('features.18.weight', 12.334051132202148), ('features.4.weight', 10.412137031555176), ('features.8.weight', 8.327202796936035), ('features.1.weight', 3.9792394638061523), ('features.41.weight', 0.8090578317642212), ('features.0.bias', 0.0), ('features.1.bias', 0.0), ('features.3.bias', 0.0), ('features.4.bias', 0.0), ('features.7.bias', 0.0), ('features.8.bias', 0.0), ('features.10.bias', 0.0), ('features.11.bias', 0.0), ('features.14.bias', 0.0), ('features.15.bias', 0.0), ('features.17.bias', 0.0), ('features.18.bias', 0.0), ('features.20.bias', 0.0), ('features.21.bias', 0.0), ('features.24.bias', 0.0), ('features.25.bias', 0.0), ('features.27.bias', 0.0), ('features.28.bias', 0.0), ('features.30.bias', 0.0), ('features.31.bias', 0.0), ('features.34.bias', 0.0), ('features.35.bias', 0.0), ('features.37.bias', 0.0), ('features.38.bias', 0.0), ('features.40.bias', 0.0), ('features.41.bias', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
get_sample_layers n=27  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.177954	Acc: 7.7% (768/10000)
[Test]  Epoch: 2	Loss: 0.063473	Acc: 31.2% (3116/10000)
[Test]  Epoch: 3	Loss: 0.052295	Acc: 35.3% (3532/10000)
[Test]  Epoch: 4	Loss: 0.047417	Acc: 38.4% (3841/10000)
[Test]  Epoch: 5	Loss: 0.045318	Acc: 40.2% (4020/10000)
[Test]  Epoch: 6	Loss: 0.040130	Acc: 45.1% (4513/10000)
[Test]  Epoch: 7	Loss: 0.037755	Acc: 46.5% (4653/10000)
[Test]  Epoch: 8	Loss: 0.035608	Acc: 47.5% (4753/10000)
[Test]  Epoch: 9	Loss: 0.034374	Acc: 50.1% (5015/10000)
[Test]  Epoch: 10	Loss: 0.034389	Acc: 51.2% (5121/10000)
[Test]  Epoch: 11	Loss: 0.034783	Acc: 50.5% (5046/10000)
[Test]  Epoch: 12	Loss: 0.034691	Acc: 50.3% (5031/10000)
[Test]  Epoch: 13	Loss: 0.034183	Acc: 51.5% (5151/10000)
[Test]  Epoch: 14	Loss: 0.034928	Acc: 50.5% (5049/10000)
[Test]  Epoch: 15	Loss: 0.032544	Acc: 52.7% (5271/10000)
[Test]  Epoch: 16	Loss: 0.033034	Acc: 52.2% (5221/10000)
[Test]  Epoch: 17	Loss: 0.032924	Acc: 52.7% (5273/10000)
[Test]  Epoch: 18	Loss: 0.031203	Acc: 54.1% (5408/10000)
[Test]  Epoch: 19	Loss: 0.032800	Acc: 52.8% (5280/10000)
[Test]  Epoch: 20	Loss: 0.032921	Acc: 51.8% (5176/10000)
[Test]  Epoch: 21	Loss: 0.032082	Acc: 53.3% (5326/10000)
[Test]  Epoch: 22	Loss: 0.031001	Acc: 54.2% (5422/10000)
[Test]  Epoch: 23	Loss: 0.030905	Acc: 55.0% (5496/10000)
[Test]  Epoch: 24	Loss: 0.031036	Acc: 53.8% (5378/10000)
[Test]  Epoch: 25	Loss: 0.030608	Acc: 54.6% (5462/10000)
[Test]  Epoch: 26	Loss: 0.030158	Acc: 55.3% (5527/10000)
[Test]  Epoch: 27	Loss: 0.030561	Acc: 54.7% (5474/10000)
[Test]  Epoch: 28	Loss: 0.029812	Acc: 55.6% (5563/10000)
[Test]  Epoch: 29	Loss: 0.029685	Acc: 55.2% (5522/10000)
[Test]  Epoch: 30	Loss: 0.030084	Acc: 55.5% (5545/10000)
[Test]  Epoch: 31	Loss: 0.029815	Acc: 55.9% (5586/10000)
[Test]  Epoch: 32	Loss: 0.029536	Acc: 55.9% (5586/10000)
[Test]  Epoch: 33	Loss: 0.029574	Acc: 56.1% (5607/10000)
[Test]  Epoch: 34	Loss: 0.029054	Acc: 56.8% (5683/10000)
[Test]  Epoch: 35	Loss: 0.029433	Acc: 55.9% (5590/10000)
[Test]  Epoch: 36	Loss: 0.029008	Acc: 56.6% (5664/10000)
[Test]  Epoch: 37	Loss: 0.029408	Acc: 55.6% (5557/10000)
[Test]  Epoch: 38	Loss: 0.029233	Acc: 56.1% (5614/10000)
[Test]  Epoch: 39	Loss: 0.029826	Acc: 55.3% (5527/10000)
[Test]  Epoch: 40	Loss: 0.029105	Acc: 56.1% (5611/10000)
[Test]  Epoch: 41	Loss: 0.028790	Acc: 56.6% (5657/10000)
[Test]  Epoch: 42	Loss: 0.028816	Acc: 56.9% (5686/10000)
[Test]  Epoch: 43	Loss: 0.028533	Acc: 56.3% (5632/10000)
[Test]  Epoch: 44	Loss: 0.028231	Acc: 57.3% (5732/10000)
[Test]  Epoch: 45	Loss: 0.028481	Acc: 57.4% (5742/10000)
[Test]  Epoch: 46	Loss: 0.028466	Acc: 56.8% (5684/10000)
[Test]  Epoch: 47	Loss: 0.028334	Acc: 56.9% (5688/10000)
[Test]  Epoch: 48	Loss: 0.028162	Acc: 57.2% (5722/10000)
[Test]  Epoch: 49	Loss: 0.028175	Acc: 57.0% (5703/10000)
[Test]  Epoch: 50	Loss: 0.028202	Acc: 57.2% (5722/10000)
[Test]  Epoch: 51	Loss: 0.028046	Acc: 57.6% (5761/10000)
[Test]  Epoch: 52	Loss: 0.028448	Acc: 56.9% (5693/10000)
[Test]  Epoch: 53	Loss: 0.028175	Acc: 57.0% (5705/10000)
[Test]  Epoch: 54	Loss: 0.028309	Acc: 57.0% (5695/10000)
[Test]  Epoch: 55	Loss: 0.028025	Acc: 57.6% (5760/10000)
[Test]  Epoch: 56	Loss: 0.028126	Acc: 57.2% (5723/10000)
[Test]  Epoch: 57	Loss: 0.027859	Acc: 57.4% (5741/10000)
[Test]  Epoch: 58	Loss: 0.027917	Acc: 57.5% (5752/10000)
[Test]  Epoch: 59	Loss: 0.028174	Acc: 57.2% (5725/10000)
[Test]  Epoch: 60	Loss: 0.028119	Acc: 57.1% (5706/10000)
[Test]  Epoch: 61	Loss: 0.027910	Acc: 57.4% (5736/10000)
[Test]  Epoch: 62	Loss: 0.028011	Acc: 57.5% (5745/10000)
[Test]  Epoch: 63	Loss: 0.028082	Acc: 57.6% (5758/10000)
[Test]  Epoch: 64	Loss: 0.028127	Acc: 57.1% (5715/10000)
[Test]  Epoch: 65	Loss: 0.028229	Acc: 57.5% (5753/10000)
[Test]  Epoch: 66	Loss: 0.028064	Acc: 57.5% (5745/10000)
[Test]  Epoch: 67	Loss: 0.028121	Acc: 57.4% (5740/10000)
[Test]  Epoch: 68	Loss: 0.028044	Acc: 56.9% (5686/10000)
[Test]  Epoch: 69	Loss: 0.027854	Acc: 57.3% (5731/10000)
[Test]  Epoch: 70	Loss: 0.028047	Acc: 57.1% (5709/10000)
[Test]  Epoch: 71	Loss: 0.027884	Acc: 57.2% (5721/10000)
[Test]  Epoch: 72	Loss: 0.027870	Acc: 57.7% (5772/10000)
[Test]  Epoch: 73	Loss: 0.027967	Acc: 57.4% (5736/10000)
[Test]  Epoch: 74	Loss: 0.027803	Acc: 57.9% (5794/10000)
[Test]  Epoch: 75	Loss: 0.027810	Acc: 57.6% (5765/10000)
[Test]  Epoch: 76	Loss: 0.027754	Acc: 57.0% (5702/10000)
[Test]  Epoch: 77	Loss: 0.027947	Acc: 57.3% (5733/10000)
[Test]  Epoch: 78	Loss: 0.027861	Acc: 57.4% (5742/10000)
[Test]  Epoch: 79	Loss: 0.027607	Acc: 58.1% (5809/10000)
[Test]  Epoch: 80	Loss: 0.027798	Acc: 57.3% (5726/10000)
[Test]  Epoch: 81	Loss: 0.027823	Acc: 57.6% (5762/10000)
[Test]  Epoch: 82	Loss: 0.027910	Acc: 57.4% (5742/10000)
[Test]  Epoch: 83	Loss: 0.027965	Acc: 57.4% (5743/10000)
[Test]  Epoch: 84	Loss: 0.027990	Acc: 57.5% (5754/10000)
[Test]  Epoch: 85	Loss: 0.027948	Acc: 57.6% (5760/10000)
[Test]  Epoch: 86	Loss: 0.027860	Acc: 57.5% (5745/10000)
[Test]  Epoch: 87	Loss: 0.027794	Acc: 57.4% (5741/10000)
[Test]  Epoch: 88	Loss: 0.027789	Acc: 57.5% (5751/10000)
[Test]  Epoch: 89	Loss: 0.027468	Acc: 57.7% (5771/10000)
[Test]  Epoch: 90	Loss: 0.027756	Acc: 57.4% (5740/10000)
[Test]  Epoch: 91	Loss: 0.027961	Acc: 57.6% (5765/10000)
[Test]  Epoch: 92	Loss: 0.027822	Acc: 57.7% (5768/10000)
[Test]  Epoch: 93	Loss: 0.027808	Acc: 57.3% (5732/10000)
[Test]  Epoch: 94	Loss: 0.027851	Acc: 57.8% (5775/10000)
[Test]  Epoch: 95	Loss: 0.027835	Acc: 57.7% (5771/10000)
[Test]  Epoch: 96	Loss: 0.027718	Acc: 57.8% (5776/10000)
[Test]  Epoch: 97	Loss: 0.027791	Acc: 57.7% (5771/10000)
[Test]  Epoch: 98	Loss: 0.027773	Acc: 57.2% (5719/10000)
[Test]  Epoch: 99	Loss: 0.027739	Acc: 57.9% (5786/10000)
[Test]  Epoch: 100	Loss: 0.027737	Acc: 57.6% (5760/10000)
===========finish==========
['2024-08-18', '16:10:47.684287', '100', 'test', '0.027737176549434663', '57.6', '58.09']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.24.weight', 'features.27.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.195543	Acc: 1.1% (109/10000)
[Test]  Epoch: 2	Loss: 0.259233	Acc: 1.2% (118/10000)
[Test]  Epoch: 3	Loss: 0.227370	Acc: 0.9% (95/10000)
[Test]  Epoch: 4	Loss: 0.077989	Acc: 1.1% (111/10000)
[Test]  Epoch: 5	Loss: 0.076529	Acc: 1.0% (101/10000)
[Test]  Epoch: 6	Loss: 0.079971	Acc: 1.1% (106/10000)
[Test]  Epoch: 7	Loss: 0.073454	Acc: 1.0% (100/10000)
[Test]  Epoch: 8	Loss: 0.087125	Acc: 1.2% (123/10000)
[Test]  Epoch: 9	Loss: 0.073694	Acc: 1.1% (105/10000)
[Test]  Epoch: 10	Loss: 0.073504	Acc: 1.1% (107/10000)
[Test]  Epoch: 11	Loss: 0.075374	Acc: 1.1% (109/10000)
[Test]  Epoch: 12	Loss: 0.073935	Acc: 1.1% (113/10000)
[Test]  Epoch: 13	Loss: 0.074384	Acc: 1.0% (104/10000)
[Test]  Epoch: 14	Loss: 0.073592	Acc: 1.1% (111/10000)
[Test]  Epoch: 15	Loss: 0.073790	Acc: 1.0% (104/10000)
[Test]  Epoch: 16	Loss: 0.073227	Acc: 1.3% (132/10000)
[Test]  Epoch: 17	Loss: 0.072580	Acc: 1.7% (169/10000)
[Test]  Epoch: 18	Loss: 0.070657	Acc: 2.5% (248/10000)
[Test]  Epoch: 19	Loss: 0.069977	Acc: 2.6% (256/10000)
[Test]  Epoch: 20	Loss: 0.070328	Acc: 3.0% (299/10000)
[Test]  Epoch: 21	Loss: 0.069816	Acc: 4.1% (409/10000)
[Test]  Epoch: 22	Loss: 0.068211	Acc: 4.5% (448/10000)
[Test]  Epoch: 23	Loss: 0.068298	Acc: 5.4% (536/10000)
[Test]  Epoch: 24	Loss: 0.065967	Acc: 5.7% (572/10000)
[Test]  Epoch: 25	Loss: 0.064798	Acc: 7.1% (711/10000)
[Test]  Epoch: 26	Loss: 0.067758	Acc: 6.3% (628/10000)
[Test]  Epoch: 27	Loss: 0.062304	Acc: 10.4% (1043/10000)
[Test]  Epoch: 28	Loss: 0.070143	Acc: 9.1% (911/10000)
[Test]  Epoch: 29	Loss: 0.064731	Acc: 10.0% (998/10000)
[Test]  Epoch: 30	Loss: 0.062011	Acc: 11.9% (1193/10000)
[Test]  Epoch: 31	Loss: 0.066136	Acc: 10.4% (1043/10000)
[Test]  Epoch: 32	Loss: 0.067399	Acc: 12.6% (1263/10000)
[Test]  Epoch: 33	Loss: 0.066119	Acc: 13.8% (1382/10000)
[Test]  Epoch: 34	Loss: 0.071938	Acc: 12.9% (1289/10000)
[Test]  Epoch: 35	Loss: 0.062325	Acc: 17.2% (1717/10000)
[Test]  Epoch: 36	Loss: 0.072081	Acc: 14.6% (1455/10000)
[Test]  Epoch: 37	Loss: 0.062704	Acc: 19.4% (1937/10000)
[Test]  Epoch: 38	Loss: 0.062477	Acc: 20.5% (2048/10000)
[Test]  Epoch: 39	Loss: 0.066093	Acc: 19.2% (1916/10000)
[Test]  Epoch: 40	Loss: 0.063471	Acc: 20.0% (2004/10000)
[Test]  Epoch: 41	Loss: 0.063823	Acc: 21.7% (2170/10000)
[Test]  Epoch: 42	Loss: 0.078549	Acc: 18.2% (1823/10000)
[Test]  Epoch: 43	Loss: 0.074315	Acc: 18.4% (1838/10000)
[Test]  Epoch: 44	Loss: 0.063114	Acc: 24.4% (2444/10000)
[Test]  Epoch: 45	Loss: 0.065192	Acc: 21.6% (2158/10000)
[Test]  Epoch: 46	Loss: 0.061521	Acc: 25.9% (2595/10000)
[Test]  Epoch: 47	Loss: 0.060610	Acc: 26.1% (2613/10000)
[Test]  Epoch: 48	Loss: 0.058747	Acc: 27.7% (2772/10000)
[Test]  Epoch: 49	Loss: 0.059571	Acc: 27.1% (2710/10000)
[Test]  Epoch: 50	Loss: 0.060284	Acc: 28.0% (2804/10000)
[Test]  Epoch: 51	Loss: 0.062877	Acc: 24.5% (2454/10000)
[Test]  Epoch: 52	Loss: 0.066294	Acc: 23.9% (2390/10000)
[Test]  Epoch: 53	Loss: 0.063883	Acc: 25.8% (2576/10000)
[Test]  Epoch: 54	Loss: 0.066379	Acc: 25.5% (2553/10000)
[Test]  Epoch: 55	Loss: 0.060673	Acc: 27.4% (2737/10000)
[Test]  Epoch: 56	Loss: 0.061427	Acc: 28.2% (2820/10000)
[Test]  Epoch: 57	Loss: 0.059443	Acc: 29.8% (2981/10000)
[Test]  Epoch: 58	Loss: 0.061865	Acc: 27.7% (2768/10000)
[Test]  Epoch: 59	Loss: 0.063253	Acc: 27.3% (2734/10000)
[Test]  Epoch: 60	Loss: 0.063882	Acc: 27.5% (2750/10000)
[Test]  Epoch: 61	Loss: 0.056887	Acc: 31.1% (3106/10000)
[Test]  Epoch: 62	Loss: 0.056385	Acc: 31.1% (3112/10000)
[Test]  Epoch: 63	Loss: 0.055094	Acc: 32.0% (3204/10000)
[Test]  Epoch: 64	Loss: 0.055177	Acc: 31.6% (3163/10000)
[Test]  Epoch: 65	Loss: 0.055464	Acc: 31.8% (3178/10000)
[Test]  Epoch: 66	Loss: 0.055169	Acc: 32.6% (3264/10000)
[Test]  Epoch: 67	Loss: 0.055091	Acc: 32.2% (3218/10000)
[Test]  Epoch: 68	Loss: 0.055096	Acc: 32.5% (3247/10000)
[Test]  Epoch: 69	Loss: 0.054940	Acc: 32.1% (3213/10000)
[Test]  Epoch: 70	Loss: 0.055289	Acc: 31.8% (3183/10000)
[Test]  Epoch: 71	Loss: 0.054457	Acc: 31.9% (3190/10000)
[Test]  Epoch: 72	Loss: 0.054558	Acc: 32.3% (3231/10000)
[Test]  Epoch: 73	Loss: 0.054547	Acc: 32.2% (3218/10000)
[Test]  Epoch: 74	Loss: 0.054454	Acc: 32.9% (3288/10000)
[Test]  Epoch: 75	Loss: 0.054501	Acc: 32.4% (3243/10000)
[Test]  Epoch: 76	Loss: 0.054337	Acc: 32.2% (3224/10000)
[Test]  Epoch: 77	Loss: 0.054115	Acc: 33.0% (3296/10000)
[Test]  Epoch: 78	Loss: 0.054227	Acc: 32.4% (3237/10000)
[Test]  Epoch: 79	Loss: 0.054106	Acc: 32.8% (3284/10000)
[Test]  Epoch: 80	Loss: 0.054015	Acc: 32.4% (3235/10000)
[Test]  Epoch: 81	Loss: 0.053991	Acc: 33.0% (3296/10000)
[Test]  Epoch: 82	Loss: 0.054036	Acc: 32.9% (3292/10000)
[Test]  Epoch: 83	Loss: 0.053865	Acc: 33.0% (3300/10000)
[Test]  Epoch: 84	Loss: 0.054133	Acc: 32.7% (3266/10000)
[Test]  Epoch: 85	Loss: 0.054306	Acc: 32.6% (3260/10000)
[Test]  Epoch: 86	Loss: 0.054132	Acc: 32.9% (3286/10000)
[Test]  Epoch: 87	Loss: 0.053782	Acc: 33.1% (3315/10000)
[Test]  Epoch: 88	Loss: 0.054115	Acc: 32.9% (3286/10000)
[Test]  Epoch: 89	Loss: 0.053711	Acc: 32.9% (3294/10000)
[Test]  Epoch: 90	Loss: 0.053629	Acc: 33.4% (3336/10000)
[Test]  Epoch: 91	Loss: 0.053802	Acc: 32.8% (3276/10000)
[Test]  Epoch: 92	Loss: 0.053860	Acc: 33.0% (3298/10000)
[Test]  Epoch: 93	Loss: 0.053549	Acc: 33.1% (3310/10000)
[Test]  Epoch: 94	Loss: 0.053634	Acc: 32.9% (3292/10000)
[Test]  Epoch: 95	Loss: 0.053414	Acc: 33.0% (3303/10000)
[Test]  Epoch: 96	Loss: 0.053745	Acc: 33.0% (3295/10000)
[Test]  Epoch: 97	Loss: 0.053542	Acc: 33.2% (3320/10000)
[Test]  Epoch: 98	Loss: 0.053512	Acc: 33.2% (3318/10000)
[Test]  Epoch: 99	Loss: 0.053434	Acc: 33.0% (3301/10000)
[Test]  Epoch: 100	Loss: 0.053320	Acc: 33.3% (3329/10000)
===========finish==========
['2024-08-18', '16:14:16.681589', '100', 'test', '0.053319952988624575', '33.29', '33.36']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.494185	Acc: 1.7% (174/10000)
[Test]  Epoch: 2	Loss: 0.084300	Acc: 1.7% (166/10000)
[Test]  Epoch: 3	Loss: 0.075654	Acc: 2.0% (198/10000)
[Test]  Epoch: 4	Loss: 0.073311	Acc: 1.6% (159/10000)
[Test]  Epoch: 5	Loss: 0.072666	Acc: 1.6% (156/10000)
[Test]  Epoch: 6	Loss: 0.071215	Acc: 1.7% (166/10000)
[Test]  Epoch: 7	Loss: 0.070023	Acc: 2.6% (260/10000)
[Test]  Epoch: 8	Loss: 0.074525	Acc: 2.0% (205/10000)
[Test]  Epoch: 9	Loss: 0.070707	Acc: 2.8% (283/10000)
[Test]  Epoch: 10	Loss: 0.069013	Acc: 3.0% (302/10000)
[Test]  Epoch: 11	Loss: 0.068454	Acc: 3.2% (325/10000)
[Test]  Epoch: 12	Loss: 0.069426	Acc: 2.7% (274/10000)
[Test]  Epoch: 13	Loss: 0.068802	Acc: 3.1% (313/10000)
[Test]  Epoch: 14	Loss: 0.068045	Acc: 3.2% (319/10000)
[Test]  Epoch: 15	Loss: 0.067377	Acc: 3.3% (332/10000)
[Test]  Epoch: 16	Loss: 0.067261	Acc: 3.6% (360/10000)
[Test]  Epoch: 17	Loss: 0.067012	Acc: 4.0% (402/10000)
[Test]  Epoch: 18	Loss: 0.067086	Acc: 3.4% (342/10000)
[Test]  Epoch: 19	Loss: 0.067431	Acc: 3.6% (359/10000)
[Test]  Epoch: 20	Loss: 0.067089	Acc: 4.0% (402/10000)
[Test]  Epoch: 21	Loss: 0.067733	Acc: 3.5% (345/10000)
[Test]  Epoch: 22	Loss: 0.065518	Acc: 4.7% (468/10000)
[Test]  Epoch: 23	Loss: 0.076130	Acc: 2.7% (270/10000)
[Test]  Epoch: 24	Loss: 0.068559	Acc: 4.4% (439/10000)
[Test]  Epoch: 25	Loss: 0.064996	Acc: 5.9% (588/10000)
[Test]  Epoch: 26	Loss: 0.067348	Acc: 5.9% (587/10000)
[Test]  Epoch: 27	Loss: 0.070178	Acc: 5.6% (557/10000)
[Test]  Epoch: 28	Loss: 0.066600	Acc: 7.3% (731/10000)
[Test]  Epoch: 29	Loss: 0.067807	Acc: 7.6% (756/10000)
[Test]  Epoch: 30	Loss: 0.064168	Acc: 8.6% (857/10000)
[Test]  Epoch: 31	Loss: 0.066085	Acc: 8.6% (856/10000)
[Test]  Epoch: 32	Loss: 0.064769	Acc: 8.5% (851/10000)
[Test]  Epoch: 33	Loss: 0.069994	Acc: 8.2% (820/10000)
[Test]  Epoch: 34	Loss: 0.069017	Acc: 9.5% (954/10000)
[Test]  Epoch: 35	Loss: 0.066065	Acc: 9.4% (945/10000)
[Test]  Epoch: 36	Loss: 0.072271	Acc: 9.3% (930/10000)
[Test]  Epoch: 37	Loss: 0.076194	Acc: 9.1% (914/10000)
[Test]  Epoch: 38	Loss: 0.069307	Acc: 10.5% (1050/10000)
[Test]  Epoch: 39	Loss: 0.071547	Acc: 10.6% (1059/10000)
[Test]  Epoch: 40	Loss: 0.072901	Acc: 11.8% (1175/10000)
[Test]  Epoch: 41	Loss: 0.074535	Acc: 11.2% (1123/10000)
[Test]  Epoch: 42	Loss: 0.075417	Acc: 11.1% (1114/10000)
[Test]  Epoch: 43	Loss: 0.075869	Acc: 10.6% (1063/10000)
[Test]  Epoch: 44	Loss: 0.078734	Acc: 10.6% (1058/10000)
[Test]  Epoch: 45	Loss: 0.071208	Acc: 12.0% (1196/10000)
[Test]  Epoch: 46	Loss: 0.073712	Acc: 10.9% (1095/10000)
[Test]  Epoch: 47	Loss: 0.082209	Acc: 10.7% (1073/10000)
[Test]  Epoch: 48	Loss: 0.081518	Acc: 12.2% (1215/10000)
[Test]  Epoch: 49	Loss: 0.071376	Acc: 13.7% (1367/10000)
[Test]  Epoch: 50	Loss: 0.073970	Acc: 13.5% (1348/10000)
[Test]  Epoch: 51	Loss: 0.079530	Acc: 14.3% (1435/10000)
[Test]  Epoch: 52	Loss: 0.086852	Acc: 12.6% (1261/10000)
[Test]  Epoch: 53	Loss: 0.087105	Acc: 12.1% (1212/10000)
[Test]  Epoch: 54	Loss: 0.098289	Acc: 11.6% (1157/10000)
[Test]  Epoch: 55	Loss: 0.080723	Acc: 12.6% (1257/10000)
[Test]  Epoch: 56	Loss: 0.080694	Acc: 13.8% (1383/10000)
[Test]  Epoch: 57	Loss: 0.080644	Acc: 16.0% (1604/10000)
[Test]  Epoch: 58	Loss: 0.076689	Acc: 16.1% (1609/10000)
[Test]  Epoch: 59	Loss: 0.087880	Acc: 13.5% (1348/10000)
[Test]  Epoch: 60	Loss: 0.077895	Acc: 16.2% (1619/10000)
[Test]  Epoch: 61	Loss: 0.063874	Acc: 22.4% (2240/10000)
[Test]  Epoch: 62	Loss: 0.062683	Acc: 23.1% (2305/10000)
[Test]  Epoch: 63	Loss: 0.062793	Acc: 23.1% (2310/10000)
[Test]  Epoch: 64	Loss: 0.062722	Acc: 23.1% (2310/10000)
[Test]  Epoch: 65	Loss: 0.063129	Acc: 22.7% (2273/10000)
[Test]  Epoch: 66	Loss: 0.062467	Acc: 23.8% (2380/10000)
[Test]  Epoch: 67	Loss: 0.062654	Acc: 23.5% (2354/10000)
[Test]  Epoch: 68	Loss: 0.062878	Acc: 23.7% (2368/10000)
[Test]  Epoch: 69	Loss: 0.062614	Acc: 23.4% (2338/10000)
[Test]  Epoch: 70	Loss: 0.062657	Acc: 23.6% (2359/10000)
[Test]  Epoch: 71	Loss: 0.062995	Acc: 23.4% (2337/10000)
[Test]  Epoch: 72	Loss: 0.063332	Acc: 23.6% (2364/10000)
[Test]  Epoch: 73	Loss: 0.063587	Acc: 23.6% (2362/10000)
[Test]  Epoch: 74	Loss: 0.063681	Acc: 23.3% (2326/10000)
[Test]  Epoch: 75	Loss: 0.063184	Acc: 23.7% (2373/10000)
[Test]  Epoch: 76	Loss: 0.063853	Acc: 23.5% (2352/10000)
[Test]  Epoch: 77	Loss: 0.063519	Acc: 24.2% (2418/10000)
[Test]  Epoch: 78	Loss: 0.063661	Acc: 23.8% (2384/10000)
[Test]  Epoch: 79	Loss: 0.063805	Acc: 24.1% (2405/10000)
[Test]  Epoch: 80	Loss: 0.064207	Acc: 23.4% (2342/10000)
[Test]  Epoch: 81	Loss: 0.063777	Acc: 23.8% (2380/10000)
[Test]  Epoch: 82	Loss: 0.064045	Acc: 24.1% (2413/10000)
[Test]  Epoch: 83	Loss: 0.064518	Acc: 23.5% (2352/10000)
[Test]  Epoch: 84	Loss: 0.064096	Acc: 24.1% (2412/10000)
[Test]  Epoch: 85	Loss: 0.064798	Acc: 23.8% (2382/10000)
[Test]  Epoch: 86	Loss: 0.064605	Acc: 23.8% (2382/10000)
[Test]  Epoch: 87	Loss: 0.064908	Acc: 23.4% (2337/10000)
[Test]  Epoch: 88	Loss: 0.064795	Acc: 24.0% (2401/10000)
[Test]  Epoch: 89	Loss: 0.064976	Acc: 24.1% (2406/10000)
[Test]  Epoch: 90	Loss: 0.064669	Acc: 24.0% (2398/10000)
[Test]  Epoch: 91	Loss: 0.065388	Acc: 23.4% (2339/10000)
[Test]  Epoch: 92	Loss: 0.065476	Acc: 23.3% (2333/10000)
[Test]  Epoch: 93	Loss: 0.065256	Acc: 23.3% (2326/10000)
[Test]  Epoch: 94	Loss: 0.065113	Acc: 23.9% (2393/10000)
[Test]  Epoch: 95	Loss: 0.065444	Acc: 23.8% (2381/10000)
[Test]  Epoch: 96	Loss: 0.065536	Acc: 23.6% (2361/10000)
[Test]  Epoch: 97	Loss: 0.065528	Acc: 24.1% (2410/10000)
[Test]  Epoch: 98	Loss: 0.065534	Acc: 23.6% (2361/10000)
[Test]  Epoch: 99	Loss: 0.065350	Acc: 23.9% (2389/10000)
[Test]  Epoch: 100	Loss: 0.065799	Acc: 24.1% (2405/10000)
===========finish==========
['2024-08-18', '16:17:40.336613', '100', 'test', '0.06579949016571045', '24.05', '24.18']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight', 'features.14.weight', 'features.34.weight', 'features.37.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.200606	Acc: 1.3% (133/10000)
[Test]  Epoch: 2	Loss: 0.091951	Acc: 1.2% (124/10000)
[Test]  Epoch: 3	Loss: 0.075921	Acc: 1.1% (110/10000)
[Test]  Epoch: 4	Loss: 0.074065	Acc: 1.3% (126/10000)
[Test]  Epoch: 5	Loss: 0.075733	Acc: 1.5% (149/10000)
[Test]  Epoch: 6	Loss: 0.079394	Acc: 1.1% (111/10000)
[Test]  Epoch: 7	Loss: 0.072422	Acc: 1.9% (189/10000)
[Test]  Epoch: 8	Loss: 0.073237	Acc: 1.6% (158/10000)
[Test]  Epoch: 9	Loss: 0.072351	Acc: 1.6% (156/10000)
[Test]  Epoch: 10	Loss: 0.071664	Acc: 2.0% (201/10000)
[Test]  Epoch: 11	Loss: 0.071503	Acc: 1.9% (194/10000)
[Test]  Epoch: 12	Loss: 0.072349	Acc: 1.9% (193/10000)
[Test]  Epoch: 13	Loss: 0.071590	Acc: 1.9% (192/10000)
[Test]  Epoch: 14	Loss: 0.071381	Acc: 2.2% (218/10000)
[Test]  Epoch: 15	Loss: 0.074233	Acc: 2.3% (234/10000)
[Test]  Epoch: 16	Loss: 0.071045	Acc: 2.4% (241/10000)
[Test]  Epoch: 17	Loss: 0.073280	Acc: 2.4% (241/10000)
[Test]  Epoch: 18	Loss: 0.071243	Acc: 1.9% (191/10000)
[Test]  Epoch: 19	Loss: 0.070316	Acc: 2.8% (275/10000)
[Test]  Epoch: 20	Loss: 0.072345	Acc: 3.0% (301/10000)
[Test]  Epoch: 21	Loss: 0.070244	Acc: 2.1% (215/10000)
[Test]  Epoch: 22	Loss: 0.071371	Acc: 3.3% (328/10000)
[Test]  Epoch: 23	Loss: 0.068922	Acc: 2.9% (289/10000)
[Test]  Epoch: 24	Loss: 0.069132	Acc: 3.8% (382/10000)
[Test]  Epoch: 25	Loss: 0.067904	Acc: 4.4% (436/10000)
[Test]  Epoch: 26	Loss: 0.070073	Acc: 3.2% (317/10000)
[Test]  Epoch: 27	Loss: 0.068304	Acc: 4.3% (426/10000)
[Test]  Epoch: 28	Loss: 0.076599	Acc: 3.8% (380/10000)
[Test]  Epoch: 29	Loss: 0.067855	Acc: 4.7% (473/10000)
[Test]  Epoch: 30	Loss: 0.076840	Acc: 4.4% (444/10000)
[Test]  Epoch: 31	Loss: 0.070193	Acc: 4.0% (405/10000)
[Test]  Epoch: 32	Loss: 0.080439	Acc: 4.3% (435/10000)
[Test]  Epoch: 33	Loss: 0.066911	Acc: 6.2% (615/10000)
[Test]  Epoch: 34	Loss: 0.069784	Acc: 5.3% (534/10000)
[Test]  Epoch: 35	Loss: 0.069138	Acc: 7.4% (742/10000)
[Test]  Epoch: 36	Loss: 0.065810	Acc: 7.6% (761/10000)
[Test]  Epoch: 37	Loss: 0.077288	Acc: 7.1% (709/10000)
[Test]  Epoch: 38	Loss: 0.072976	Acc: 8.0% (800/10000)
[Test]  Epoch: 39	Loss: 0.064298	Acc: 11.4% (1142/10000)
[Test]  Epoch: 40	Loss: 0.071725	Acc: 9.4% (944/10000)
[Test]  Epoch: 41	Loss: 0.068617	Acc: 11.4% (1141/10000)
[Test]  Epoch: 42	Loss: 0.077022	Acc: 10.5% (1050/10000)
[Test]  Epoch: 43	Loss: 0.096080	Acc: 6.9% (688/10000)
[Test]  Epoch: 44	Loss: 0.074935	Acc: 11.3% (1130/10000)
[Test]  Epoch: 45	Loss: 0.071076	Acc: 10.7% (1074/10000)
[Test]  Epoch: 46	Loss: 0.087414	Acc: 8.6% (861/10000)
[Test]  Epoch: 47	Loss: 0.069945	Acc: 13.0% (1300/10000)
[Test]  Epoch: 48	Loss: 0.077756	Acc: 11.8% (1181/10000)
[Test]  Epoch: 49	Loss: 0.074448	Acc: 12.2% (1217/10000)
[Test]  Epoch: 50	Loss: 0.088974	Acc: 12.4% (1238/10000)
[Test]  Epoch: 51	Loss: 0.077010	Acc: 12.2% (1224/10000)
[Test]  Epoch: 52	Loss: 0.087141	Acc: 11.2% (1120/10000)
[Test]  Epoch: 53	Loss: 0.086515	Acc: 12.4% (1237/10000)
[Test]  Epoch: 54	Loss: 0.078754	Acc: 14.1% (1414/10000)
[Test]  Epoch: 55	Loss: 0.073388	Acc: 15.8% (1578/10000)
[Test]  Epoch: 56	Loss: 0.071337	Acc: 16.5% (1654/10000)
[Test]  Epoch: 57	Loss: 0.076783	Acc: 16.9% (1691/10000)
[Test]  Epoch: 58	Loss: 0.081312	Acc: 15.0% (1497/10000)
[Test]  Epoch: 59	Loss: 0.087676	Acc: 12.8% (1283/10000)
[Test]  Epoch: 60	Loss: 0.083054	Acc: 14.0% (1401/10000)
[Test]  Epoch: 61	Loss: 0.064173	Acc: 21.7% (2170/10000)
[Test]  Epoch: 62	Loss: 0.062819	Acc: 22.8% (2279/10000)
[Test]  Epoch: 63	Loss: 0.062559	Acc: 22.4% (2243/10000)
[Test]  Epoch: 64	Loss: 0.062495	Acc: 22.9% (2286/10000)
[Test]  Epoch: 65	Loss: 0.062649	Acc: 22.7% (2274/10000)
[Test]  Epoch: 66	Loss: 0.062565	Acc: 22.8% (2282/10000)
[Test]  Epoch: 67	Loss: 0.062242	Acc: 23.7% (2373/10000)
[Test]  Epoch: 68	Loss: 0.062498	Acc: 23.6% (2356/10000)
[Test]  Epoch: 69	Loss: 0.062860	Acc: 23.4% (2341/10000)
[Test]  Epoch: 70	Loss: 0.062513	Acc: 23.8% (2378/10000)
[Test]  Epoch: 71	Loss: 0.062765	Acc: 23.6% (2365/10000)
[Test]  Epoch: 72	Loss: 0.063192	Acc: 23.5% (2349/10000)
[Test]  Epoch: 73	Loss: 0.063129	Acc: 23.6% (2364/10000)
[Test]  Epoch: 74	Loss: 0.062839	Acc: 23.8% (2384/10000)
[Test]  Epoch: 75	Loss: 0.063491	Acc: 23.7% (2370/10000)
[Test]  Epoch: 76	Loss: 0.063207	Acc: 24.0% (2403/10000)
[Test]  Epoch: 77	Loss: 0.063139	Acc: 23.6% (2362/10000)
[Test]  Epoch: 78	Loss: 0.063209	Acc: 23.8% (2380/10000)
[Test]  Epoch: 79	Loss: 0.063261	Acc: 23.9% (2393/10000)
[Test]  Epoch: 80	Loss: 0.063571	Acc: 23.7% (2370/10000)
[Test]  Epoch: 81	Loss: 0.063587	Acc: 23.8% (2375/10000)
[Test]  Epoch: 82	Loss: 0.063435	Acc: 23.6% (2363/10000)
[Test]  Epoch: 83	Loss: 0.063691	Acc: 23.9% (2391/10000)
[Test]  Epoch: 84	Loss: 0.063671	Acc: 24.1% (2407/10000)
[Test]  Epoch: 85	Loss: 0.064063	Acc: 23.9% (2387/10000)
[Test]  Epoch: 86	Loss: 0.064052	Acc: 23.8% (2382/10000)
[Test]  Epoch: 87	Loss: 0.063931	Acc: 24.6% (2458/10000)
[Test]  Epoch: 88	Loss: 0.064064	Acc: 24.0% (2401/10000)
[Test]  Epoch: 89	Loss: 0.064181	Acc: 23.7% (2366/10000)
[Test]  Epoch: 90	Loss: 0.064363	Acc: 23.9% (2390/10000)
[Test]  Epoch: 91	Loss: 0.064471	Acc: 24.0% (2397/10000)
[Test]  Epoch: 92	Loss: 0.064258	Acc: 24.4% (2443/10000)
[Test]  Epoch: 93	Loss: 0.064559	Acc: 23.9% (2386/10000)
[Test]  Epoch: 94	Loss: 0.064321	Acc: 24.4% (2435/10000)
[Test]  Epoch: 95	Loss: 0.064738	Acc: 23.9% (2394/10000)
[Test]  Epoch: 96	Loss: 0.065002	Acc: 23.5% (2354/10000)
[Test]  Epoch: 97	Loss: 0.064603	Acc: 24.3% (2433/10000)
[Test]  Epoch: 98	Loss: 0.064518	Acc: 24.2% (2422/10000)
[Test]  Epoch: 99	Loss: 0.064718	Acc: 24.5% (2453/10000)
[Test]  Epoch: 100	Loss: 0.065079	Acc: 23.6% (2359/10000)
===========finish==========
['2024-08-18', '16:21:01.041078', '100', 'test', '0.06507881388664245', '23.59', '24.58']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight', 'features.14.weight', 'features.34.weight', 'features.37.weight', 'features.10.weight', 'features.40.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.442878	Acc: 0.9% (95/10000)
[Test]  Epoch: 2	Loss: 0.087456	Acc: 1.3% (132/10000)
[Test]  Epoch: 3	Loss: 0.082739	Acc: 1.0% (100/10000)
[Test]  Epoch: 4	Loss: 0.075437	Acc: 1.2% (122/10000)
[Test]  Epoch: 5	Loss: 0.073054	Acc: 1.3% (132/10000)
[Test]  Epoch: 6	Loss: 0.078008	Acc: 1.0% (103/10000)
[Test]  Epoch: 7	Loss: 0.073793	Acc: 1.5% (147/10000)
[Test]  Epoch: 8	Loss: 0.072755	Acc: 1.1% (108/10000)
[Test]  Epoch: 9	Loss: 0.073463	Acc: 1.2% (117/10000)
[Test]  Epoch: 10	Loss: 0.074268	Acc: 1.5% (148/10000)
[Test]  Epoch: 11	Loss: 0.072350	Acc: 1.4% (141/10000)
[Test]  Epoch: 12	Loss: 0.074135	Acc: 1.3% (130/10000)
[Test]  Epoch: 13	Loss: 0.073786	Acc: 1.4% (139/10000)
[Test]  Epoch: 14	Loss: 0.073011	Acc: 1.5% (154/10000)
[Test]  Epoch: 15	Loss: 0.072558	Acc: 1.3% (128/10000)
[Test]  Epoch: 16	Loss: 0.072304	Acc: 1.5% (150/10000)
[Test]  Epoch: 17	Loss: 0.075415	Acc: 1.4% (142/10000)
[Test]  Epoch: 18	Loss: 0.074175	Acc: 1.6% (155/10000)
[Test]  Epoch: 19	Loss: 0.076874	Acc: 1.7% (166/10000)
[Test]  Epoch: 20	Loss: 0.071975	Acc: 1.7% (171/10000)
[Test]  Epoch: 21	Loss: 0.072870	Acc: 1.5% (154/10000)
[Test]  Epoch: 22	Loss: 0.072863	Acc: 1.9% (186/10000)
[Test]  Epoch: 23	Loss: 0.071481	Acc: 2.1% (212/10000)
[Test]  Epoch: 24	Loss: 0.072254	Acc: 1.4% (139/10000)
[Test]  Epoch: 25	Loss: 0.072750	Acc: 1.9% (189/10000)
[Test]  Epoch: 26	Loss: 0.074031	Acc: 1.9% (189/10000)
[Test]  Epoch: 27	Loss: 0.072525	Acc: 1.2% (120/10000)
[Test]  Epoch: 28	Loss: 0.072650	Acc: 1.5% (150/10000)
[Test]  Epoch: 29	Loss: 0.073053	Acc: 2.2% (222/10000)
[Test]  Epoch: 30	Loss: 0.071943	Acc: 1.7% (173/10000)
[Test]  Epoch: 31	Loss: 0.072824	Acc: 1.3% (126/10000)
[Test]  Epoch: 32	Loss: 0.070825	Acc: 2.5% (253/10000)
[Test]  Epoch: 33	Loss: 0.072018	Acc: 2.7% (274/10000)
[Test]  Epoch: 34	Loss: 0.070867	Acc: 2.3% (231/10000)
[Test]  Epoch: 35	Loss: 0.072098	Acc: 2.8% (280/10000)
[Test]  Epoch: 36	Loss: 0.070680	Acc: 2.5% (248/10000)
[Test]  Epoch: 37	Loss: 0.070394	Acc: 3.1% (306/10000)
[Test]  Epoch: 38	Loss: 0.075752	Acc: 2.8% (277/10000)
[Test]  Epoch: 39	Loss: 0.069344	Acc: 3.6% (357/10000)
[Test]  Epoch: 40	Loss: 0.069548	Acc: 2.9% (289/10000)
[Test]  Epoch: 41	Loss: 0.069888	Acc: 4.5% (446/10000)
[Test]  Epoch: 42	Loss: 0.078922	Acc: 3.0% (300/10000)
[Test]  Epoch: 43	Loss: 0.069084	Acc: 4.7% (466/10000)
[Test]  Epoch: 44	Loss: 0.069560	Acc: 5.0% (496/10000)
[Test]  Epoch: 45	Loss: 0.065923	Acc: 5.5% (547/10000)
[Test]  Epoch: 46	Loss: 0.069547	Acc: 5.0% (497/10000)
[Test]  Epoch: 47	Loss: 0.065613	Acc: 7.4% (737/10000)
[Test]  Epoch: 48	Loss: 0.066033	Acc: 7.8% (776/10000)
[Test]  Epoch: 49	Loss: 0.066280	Acc: 7.5% (746/10000)
[Test]  Epoch: 50	Loss: 0.068947	Acc: 8.4% (840/10000)
[Test]  Epoch: 51	Loss: 0.072529	Acc: 7.1% (707/10000)
[Test]  Epoch: 52	Loss: 0.072971	Acc: 7.3% (731/10000)
[Test]  Epoch: 53	Loss: 0.074658	Acc: 6.5% (651/10000)
[Test]  Epoch: 54	Loss: 0.079645	Acc: 6.3% (633/10000)
[Test]  Epoch: 55	Loss: 0.091309	Acc: 7.4% (736/10000)
[Test]  Epoch: 56	Loss: 0.077776	Acc: 9.1% (912/10000)
[Test]  Epoch: 57	Loss: 0.064885	Acc: 11.4% (1138/10000)
[Test]  Epoch: 58	Loss: 0.067112	Acc: 12.6% (1262/10000)
[Test]  Epoch: 59	Loss: 0.072530	Acc: 10.2% (1016/10000)
[Test]  Epoch: 60	Loss: 0.070011	Acc: 12.1% (1206/10000)
[Test]  Epoch: 61	Loss: 0.059190	Acc: 17.8% (1779/10000)
[Test]  Epoch: 62	Loss: 0.057538	Acc: 18.6% (1860/10000)
[Test]  Epoch: 63	Loss: 0.057784	Acc: 19.0% (1899/10000)
[Test]  Epoch: 64	Loss: 0.058108	Acc: 19.2% (1918/10000)
[Test]  Epoch: 65	Loss: 0.057547	Acc: 19.3% (1932/10000)
[Test]  Epoch: 66	Loss: 0.057879	Acc: 19.2% (1923/10000)
[Test]  Epoch: 67	Loss: 0.057841	Acc: 19.5% (1950/10000)
[Test]  Epoch: 68	Loss: 0.058479	Acc: 19.1% (1907/10000)
[Test]  Epoch: 69	Loss: 0.058791	Acc: 19.7% (1974/10000)
[Test]  Epoch: 70	Loss: 0.058847	Acc: 19.5% (1950/10000)
[Test]  Epoch: 71	Loss: 0.059122	Acc: 19.2% (1925/10000)
[Test]  Epoch: 72	Loss: 0.059304	Acc: 19.3% (1932/10000)
[Test]  Epoch: 73	Loss: 0.060061	Acc: 19.4% (1936/10000)
[Test]  Epoch: 74	Loss: 0.058912	Acc: 20.1% (2008/10000)
[Test]  Epoch: 75	Loss: 0.059696	Acc: 20.0% (2004/10000)
[Test]  Epoch: 76	Loss: 0.059676	Acc: 19.7% (1972/10000)
[Test]  Epoch: 77	Loss: 0.059934	Acc: 19.4% (1943/10000)
[Test]  Epoch: 78	Loss: 0.060725	Acc: 20.0% (1999/10000)
[Test]  Epoch: 79	Loss: 0.060759	Acc: 19.6% (1963/10000)
[Test]  Epoch: 80	Loss: 0.060829	Acc: 19.5% (1954/10000)
[Test]  Epoch: 81	Loss: 0.061333	Acc: 20.0% (2001/10000)
[Test]  Epoch: 82	Loss: 0.061000	Acc: 19.3% (1931/10000)
[Test]  Epoch: 83	Loss: 0.061435	Acc: 19.4% (1940/10000)
[Test]  Epoch: 84	Loss: 0.061541	Acc: 20.0% (1999/10000)
[Test]  Epoch: 85	Loss: 0.061722	Acc: 20.1% (2012/10000)
[Test]  Epoch: 86	Loss: 0.061431	Acc: 20.1% (2005/10000)
[Test]  Epoch: 87	Loss: 0.062478	Acc: 19.9% (1987/10000)
[Test]  Epoch: 88	Loss: 0.062352	Acc: 20.1% (2012/10000)
[Test]  Epoch: 89	Loss: 0.062341	Acc: 20.0% (1996/10000)
[Test]  Epoch: 90	Loss: 0.062811	Acc: 19.9% (1991/10000)
[Test]  Epoch: 91	Loss: 0.062853	Acc: 20.0% (2002/10000)
[Test]  Epoch: 92	Loss: 0.062637	Acc: 19.8% (1979/10000)
[Test]  Epoch: 93	Loss: 0.063268	Acc: 19.8% (1984/10000)
[Test]  Epoch: 94	Loss: 0.062883	Acc: 20.4% (2038/10000)
[Test]  Epoch: 95	Loss: 0.063476	Acc: 20.0% (1996/10000)
[Test]  Epoch: 96	Loss: 0.063256	Acc: 20.5% (2050/10000)
[Test]  Epoch: 97	Loss: 0.062759	Acc: 20.3% (2027/10000)
[Test]  Epoch: 98	Loss: 0.063250	Acc: 20.5% (2050/10000)
[Test]  Epoch: 99	Loss: 0.063575	Acc: 20.3% (2029/10000)
[Test]  Epoch: 100	Loss: 0.063806	Acc: 20.4% (2040/10000)
===========finish==========
['2024-08-18', '16:24:24.801041', '100', 'test', '0.0638063337802887', '20.4', '20.5']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight', 'features.14.weight', 'features.34.weight', 'features.37.weight', 'features.10.weight', 'features.40.weight', 'features.7.weight', 'features.3.weight', 'features.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.862161	Acc: 1.2% (123/10000)
[Test]  Epoch: 2	Loss: 0.101369	Acc: 1.2% (119/10000)
[Test]  Epoch: 3	Loss: 0.074205	Acc: 1.2% (123/10000)
[Test]  Epoch: 4	Loss: 0.073795	Acc: 1.0% (100/10000)
[Test]  Epoch: 5	Loss: 0.074373	Acc: 1.7% (171/10000)
[Test]  Epoch: 6	Loss: 0.072729	Acc: 1.8% (180/10000)
[Test]  Epoch: 7	Loss: 0.072636	Acc: 1.6% (157/10000)
[Test]  Epoch: 8	Loss: 0.073034	Acc: 1.0% (99/10000)
[Test]  Epoch: 9	Loss: 0.072771	Acc: 1.8% (175/10000)
[Test]  Epoch: 10	Loss: 0.071942	Acc: 1.6% (163/10000)
[Test]  Epoch: 11	Loss: 0.072265	Acc: 1.7% (173/10000)
[Test]  Epoch: 12	Loss: 0.072443	Acc: 1.6% (156/10000)
[Test]  Epoch: 13	Loss: 0.072519	Acc: 1.5% (147/10000)
[Test]  Epoch: 14	Loss: 0.072020	Acc: 1.3% (127/10000)
[Test]  Epoch: 15	Loss: 0.072023	Acc: 1.5% (154/10000)
[Test]  Epoch: 16	Loss: 0.072731	Acc: 1.9% (189/10000)
[Test]  Epoch: 17	Loss: 0.071596	Acc: 1.9% (192/10000)
[Test]  Epoch: 18	Loss: 0.074531	Acc: 1.9% (190/10000)
[Test]  Epoch: 19	Loss: 0.074080	Acc: 1.7% (169/10000)
[Test]  Epoch: 20	Loss: 0.077741	Acc: 2.0% (198/10000)
[Test]  Epoch: 21	Loss: 0.071382	Acc: 1.4% (142/10000)
[Test]  Epoch: 22	Loss: 0.071758	Acc: 2.0% (196/10000)
[Test]  Epoch: 23	Loss: 0.071372	Acc: 1.8% (182/10000)
[Test]  Epoch: 24	Loss: 0.071847	Acc: 1.7% (171/10000)
[Test]  Epoch: 25	Loss: 0.074425	Acc: 1.8% (177/10000)
[Test]  Epoch: 26	Loss: 0.071334	Acc: 2.3% (230/10000)
[Test]  Epoch: 27	Loss: 0.072148	Acc: 2.2% (218/10000)
[Test]  Epoch: 28	Loss: 0.071419	Acc: 1.7% (171/10000)
[Test]  Epoch: 29	Loss: 0.070909	Acc: 2.4% (238/10000)
[Test]  Epoch: 30	Loss: 0.075017	Acc: 2.5% (248/10000)
[Test]  Epoch: 31	Loss: 0.073714	Acc: 1.1% (111/10000)
[Test]  Epoch: 32	Loss: 0.070469	Acc: 2.2% (224/10000)
[Test]  Epoch: 33	Loss: 0.070068	Acc: 2.7% (272/10000)
[Test]  Epoch: 34	Loss: 0.070337	Acc: 2.7% (272/10000)
[Test]  Epoch: 35	Loss: 0.070963	Acc: 2.3% (231/10000)
[Test]  Epoch: 36	Loss: 0.077357	Acc: 2.7% (270/10000)
[Test]  Epoch: 37	Loss: 0.072037	Acc: 1.8% (179/10000)
[Test]  Epoch: 38	Loss: 0.072853	Acc: 2.0% (199/10000)
[Test]  Epoch: 39	Loss: 0.070074	Acc: 3.1% (307/10000)
[Test]  Epoch: 40	Loss: 0.070428	Acc: 3.4% (335/10000)
[Test]  Epoch: 41	Loss: 0.075677	Acc: 3.8% (382/10000)
[Test]  Epoch: 42	Loss: 0.070223	Acc: 3.9% (394/10000)
[Test]  Epoch: 43	Loss: 0.070631	Acc: 3.4% (335/10000)
[Test]  Epoch: 44	Loss: 0.068598	Acc: 5.6% (560/10000)
[Test]  Epoch: 45	Loss: 0.068434	Acc: 4.9% (487/10000)
[Test]  Epoch: 46	Loss: 0.069189	Acc: 4.8% (475/10000)
[Test]  Epoch: 47	Loss: 0.069079	Acc: 6.1% (609/10000)
[Test]  Epoch: 48	Loss: 0.072682	Acc: 3.9% (389/10000)
[Test]  Epoch: 49	Loss: 0.067651	Acc: 5.5% (545/10000)
[Test]  Epoch: 50	Loss: 0.074099	Acc: 5.5% (554/10000)
[Test]  Epoch: 51	Loss: 0.069448	Acc: 7.1% (710/10000)
[Test]  Epoch: 52	Loss: 0.070846	Acc: 5.5% (550/10000)
[Test]  Epoch: 53	Loss: 0.073479	Acc: 5.4% (539/10000)
[Test]  Epoch: 54	Loss: 0.096833	Acc: 5.3% (528/10000)
[Test]  Epoch: 55	Loss: 0.070485	Acc: 6.2% (624/10000)
[Test]  Epoch: 56	Loss: 0.078903	Acc: 7.3% (734/10000)
[Test]  Epoch: 57	Loss: 0.070342	Acc: 7.8% (778/10000)
[Test]  Epoch: 58	Loss: 0.071286	Acc: 8.1% (813/10000)
[Test]  Epoch: 59	Loss: 0.072251	Acc: 7.3% (734/10000)
[Test]  Epoch: 60	Loss: 0.074713	Acc: 6.9% (688/10000)
[Test]  Epoch: 61	Loss: 0.067491	Acc: 10.3% (1031/10000)
[Test]  Epoch: 62	Loss: 0.067112	Acc: 10.7% (1073/10000)
[Test]  Epoch: 63	Loss: 0.067555	Acc: 10.8% (1079/10000)
[Test]  Epoch: 64	Loss: 0.067792	Acc: 10.8% (1079/10000)
[Test]  Epoch: 65	Loss: 0.067750	Acc: 10.8% (1077/10000)
[Test]  Epoch: 66	Loss: 0.067932	Acc: 11.1% (1106/10000)
[Test]  Epoch: 67	Loss: 0.068060	Acc: 11.4% (1143/10000)
[Test]  Epoch: 68	Loss: 0.068338	Acc: 11.0% (1104/10000)
[Test]  Epoch: 69	Loss: 0.068777	Acc: 11.0% (1102/10000)
[Test]  Epoch: 70	Loss: 0.069111	Acc: 11.3% (1132/10000)
[Test]  Epoch: 71	Loss: 0.068947	Acc: 11.6% (1163/10000)
[Test]  Epoch: 72	Loss: 0.068699	Acc: 11.5% (1152/10000)
[Test]  Epoch: 73	Loss: 0.069716	Acc: 11.6% (1164/10000)
[Test]  Epoch: 74	Loss: 0.069383	Acc: 11.3% (1127/10000)
[Test]  Epoch: 75	Loss: 0.069692	Acc: 11.3% (1126/10000)
[Test]  Epoch: 76	Loss: 0.070403	Acc: 11.2% (1123/10000)
[Test]  Epoch: 77	Loss: 0.069643	Acc: 11.2% (1122/10000)
[Test]  Epoch: 78	Loss: 0.070089	Acc: 11.0% (1096/10000)
[Test]  Epoch: 79	Loss: 0.070815	Acc: 11.0% (1101/10000)
[Test]  Epoch: 80	Loss: 0.070450	Acc: 11.2% (1125/10000)
[Test]  Epoch: 81	Loss: 0.070859	Acc: 11.4% (1136/10000)
[Test]  Epoch: 82	Loss: 0.070946	Acc: 11.5% (1146/10000)
[Test]  Epoch: 83	Loss: 0.071741	Acc: 11.7% (1166/10000)
[Test]  Epoch: 84	Loss: 0.071617	Acc: 11.6% (1161/10000)
[Test]  Epoch: 85	Loss: 0.071558	Acc: 11.6% (1160/10000)
[Test]  Epoch: 86	Loss: 0.071734	Acc: 11.6% (1156/10000)
[Test]  Epoch: 87	Loss: 0.072689	Acc: 11.4% (1138/10000)
[Test]  Epoch: 88	Loss: 0.072190	Acc: 11.4% (1140/10000)
[Test]  Epoch: 89	Loss: 0.072524	Acc: 11.2% (1125/10000)
[Test]  Epoch: 90	Loss: 0.072538	Acc: 11.2% (1117/10000)
[Test]  Epoch: 91	Loss: 0.072843	Acc: 11.7% (1173/10000)
[Test]  Epoch: 92	Loss: 0.073172	Acc: 11.7% (1167/10000)
[Test]  Epoch: 93	Loss: 0.072850	Acc: 11.6% (1156/10000)
[Test]  Epoch: 94	Loss: 0.073096	Acc: 11.8% (1175/10000)
[Test]  Epoch: 95	Loss: 0.073646	Acc: 11.9% (1191/10000)
[Test]  Epoch: 96	Loss: 0.073829	Acc: 11.2% (1116/10000)
[Test]  Epoch: 97	Loss: 0.073599	Acc: 11.3% (1131/10000)
[Test]  Epoch: 98	Loss: 0.074295	Acc: 12.0% (1198/10000)
[Test]  Epoch: 99	Loss: 0.074264	Acc: 11.2% (1115/10000)
[Test]  Epoch: 100	Loss: 0.074713	Acc: 11.8% (1177/10000)
===========finish==========
['2024-08-18', '16:27:02.621994', '100', 'test', '0.07471261162757874', '11.77', '11.98']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight', 'features.14.weight', 'features.34.weight', 'features.37.weight', 'features.10.weight', 'features.40.weight', 'features.7.weight', 'features.3.weight', 'features.0.weight', 'classifier.weight', 'features.38.weight', 'features.21.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.072580	Acc: 0.3% (32/10000)
[Test]  Epoch: 2	Loss: 0.071963	Acc: 1.5% (147/10000)
[Test]  Epoch: 3	Loss: 0.071921	Acc: 1.8% (176/10000)
[Test]  Epoch: 4	Loss: 0.071274	Acc: 2.1% (213/10000)
[Test]  Epoch: 5	Loss: 0.072245	Acc: 1.9% (194/10000)
[Test]  Epoch: 6	Loss: 0.071864	Acc: 2.1% (212/10000)
[Test]  Epoch: 7	Loss: 0.071988	Acc: 2.5% (252/10000)
[Test]  Epoch: 8	Loss: 0.074202	Acc: 2.6% (263/10000)
[Test]  Epoch: 9	Loss: 0.073709	Acc: 2.2% (222/10000)
[Test]  Epoch: 10	Loss: 0.072057	Acc: 2.5% (254/10000)
[Test]  Epoch: 11	Loss: 0.070937	Acc: 2.8% (277/10000)
[Test]  Epoch: 12	Loss: 0.074764	Acc: 2.4% (244/10000)
[Test]  Epoch: 13	Loss: 0.071012	Acc: 2.4% (244/10000)
[Test]  Epoch: 14	Loss: 0.071310	Acc: 2.6% (264/10000)
[Test]  Epoch: 15	Loss: 0.071022	Acc: 2.1% (211/10000)
[Test]  Epoch: 16	Loss: 0.071612	Acc: 3.5% (347/10000)
[Test]  Epoch: 17	Loss: 0.071880	Acc: 3.0% (299/10000)
[Test]  Epoch: 18	Loss: 0.074875	Acc: 2.6% (263/10000)
[Test]  Epoch: 19	Loss: 0.069979	Acc: 3.6% (357/10000)
[Test]  Epoch: 20	Loss: 0.074645	Acc: 2.4% (240/10000)
[Test]  Epoch: 21	Loss: 0.072305	Acc: 3.8% (382/10000)
[Test]  Epoch: 22	Loss: 0.073637	Acc: 3.3% (329/10000)
[Test]  Epoch: 23	Loss: 0.073795	Acc: 4.8% (485/10000)
[Test]  Epoch: 24	Loss: 0.068856	Acc: 4.6% (463/10000)
[Test]  Epoch: 25	Loss: 0.074831	Acc: 5.2% (518/10000)
[Test]  Epoch: 26	Loss: 0.073138	Acc: 5.3% (526/10000)
[Test]  Epoch: 27	Loss: 0.073774	Acc: 5.7% (569/10000)
[Test]  Epoch: 28	Loss: 0.070460	Acc: 5.4% (538/10000)
[Test]  Epoch: 29	Loss: 0.070688	Acc: 5.8% (584/10000)
[Test]  Epoch: 30	Loss: 0.068536	Acc: 6.3% (634/10000)
[Test]  Epoch: 31	Loss: 0.072167	Acc: 5.5% (548/10000)
[Test]  Epoch: 32	Loss: 0.090606	Acc: 5.4% (540/10000)
[Test]  Epoch: 33	Loss: 0.072542	Acc: 6.2% (616/10000)
[Test]  Epoch: 34	Loss: 0.078625	Acc: 7.0% (703/10000)
[Test]  Epoch: 35	Loss: 0.076402	Acc: 7.2% (721/10000)
[Test]  Epoch: 36	Loss: 0.070349	Acc: 7.3% (729/10000)
[Test]  Epoch: 37	Loss: 0.073649	Acc: 7.2% (720/10000)
[Test]  Epoch: 38	Loss: 0.080993	Acc: 6.4% (636/10000)
[Test]  Epoch: 39	Loss: 0.071347	Acc: 8.1% (808/10000)
[Test]  Epoch: 40	Loss: 0.073923	Acc: 8.3% (831/10000)
[Test]  Epoch: 41	Loss: 0.078248	Acc: 8.0% (796/10000)
[Test]  Epoch: 42	Loss: 0.076287	Acc: 7.5% (751/10000)
[Test]  Epoch: 43	Loss: 0.083123	Acc: 7.6% (759/10000)
[Test]  Epoch: 44	Loss: 0.077132	Acc: 7.7% (769/10000)
[Test]  Epoch: 45	Loss: 0.080664	Acc: 8.2% (819/10000)
[Test]  Epoch: 46	Loss: 0.076043	Acc: 9.1% (910/10000)
[Test]  Epoch: 47	Loss: 0.080095	Acc: 8.9% (890/10000)
[Test]  Epoch: 48	Loss: 0.084115	Acc: 7.8% (782/10000)
[Test]  Epoch: 49	Loss: 0.079312	Acc: 9.5% (950/10000)
[Test]  Epoch: 50	Loss: 0.084725	Acc: 9.5% (946/10000)
[Test]  Epoch: 51	Loss: 0.083196	Acc: 8.7% (873/10000)
[Test]  Epoch: 52	Loss: 0.086874	Acc: 8.8% (878/10000)
[Test]  Epoch: 53	Loss: 0.084942	Acc: 8.9% (892/10000)
[Test]  Epoch: 54	Loss: 0.082961	Acc: 9.7% (965/10000)
[Test]  Epoch: 55	Loss: 0.087083	Acc: 8.4% (842/10000)
[Test]  Epoch: 56	Loss: 0.089305	Acc: 9.2% (917/10000)
[Test]  Epoch: 57	Loss: 0.085990	Acc: 9.2% (924/10000)
[Test]  Epoch: 58	Loss: 0.091125	Acc: 9.6% (963/10000)
[Test]  Epoch: 59	Loss: 0.096611	Acc: 9.6% (963/10000)
[Test]  Epoch: 60	Loss: 0.089784	Acc: 9.8% (980/10000)
[Test]  Epoch: 61	Loss: 0.080722	Acc: 12.3% (1231/10000)
[Test]  Epoch: 62	Loss: 0.079876	Acc: 12.7% (1271/10000)
[Test]  Epoch: 63	Loss: 0.079345	Acc: 12.9% (1286/10000)
[Test]  Epoch: 64	Loss: 0.080012	Acc: 12.8% (1284/10000)
[Test]  Epoch: 65	Loss: 0.079857	Acc: 12.8% (1278/10000)
[Test]  Epoch: 66	Loss: 0.079493	Acc: 13.3% (1333/10000)
[Test]  Epoch: 67	Loss: 0.080091	Acc: 12.8% (1284/10000)
[Test]  Epoch: 68	Loss: 0.080863	Acc: 13.2% (1316/10000)
[Test]  Epoch: 69	Loss: 0.080844	Acc: 13.2% (1323/10000)
[Test]  Epoch: 70	Loss: 0.080772	Acc: 12.7% (1274/10000)
[Test]  Epoch: 71	Loss: 0.080730	Acc: 13.2% (1325/10000)
[Test]  Epoch: 72	Loss: 0.081160	Acc: 13.1% (1310/10000)
[Test]  Epoch: 73	Loss: 0.081202	Acc: 13.4% (1340/10000)
[Test]  Epoch: 74	Loss: 0.081314	Acc: 13.2% (1321/10000)
[Test]  Epoch: 75	Loss: 0.081570	Acc: 13.4% (1337/10000)
[Test]  Epoch: 76	Loss: 0.081344	Acc: 13.1% (1314/10000)
[Test]  Epoch: 77	Loss: 0.081613	Acc: 13.3% (1326/10000)
[Test]  Epoch: 78	Loss: 0.081813	Acc: 13.0% (1302/10000)
[Test]  Epoch: 79	Loss: 0.081777	Acc: 12.9% (1291/10000)
[Test]  Epoch: 80	Loss: 0.082378	Acc: 13.1% (1313/10000)
[Test]  Epoch: 81	Loss: 0.082070	Acc: 13.2% (1317/10000)
[Test]  Epoch: 82	Loss: 0.082260	Acc: 13.2% (1315/10000)
[Test]  Epoch: 83	Loss: 0.082643	Acc: 12.9% (1295/10000)
[Test]  Epoch: 84	Loss: 0.082179	Acc: 13.3% (1332/10000)
[Test]  Epoch: 85	Loss: 0.082296	Acc: 13.1% (1313/10000)
[Test]  Epoch: 86	Loss: 0.082681	Acc: 13.0% (1300/10000)
[Test]  Epoch: 87	Loss: 0.083006	Acc: 13.4% (1341/10000)
[Test]  Epoch: 88	Loss: 0.082938	Acc: 13.5% (1347/10000)
[Test]  Epoch: 89	Loss: 0.082772	Acc: 13.1% (1306/10000)
[Test]  Epoch: 90	Loss: 0.083309	Acc: 13.2% (1319/10000)
[Test]  Epoch: 91	Loss: 0.083173	Acc: 13.4% (1339/10000)
[Test]  Epoch: 92	Loss: 0.083248	Acc: 12.7% (1273/10000)
[Test]  Epoch: 93	Loss: 0.083100	Acc: 13.3% (1333/10000)
[Test]  Epoch: 94	Loss: 0.083172	Acc: 13.4% (1336/10000)
[Test]  Epoch: 95	Loss: 0.083884	Acc: 13.1% (1309/10000)
[Test]  Epoch: 96	Loss: 0.083763	Acc: 13.0% (1302/10000)
[Test]  Epoch: 97	Loss: 0.084090	Acc: 12.9% (1294/10000)
[Test]  Epoch: 98	Loss: 0.083510	Acc: 13.2% (1324/10000)
[Test]  Epoch: 99	Loss: 0.083563	Acc: 13.2% (1322/10000)
[Test]  Epoch: 100	Loss: 0.083879	Acc: 13.2% (1317/10000)
===========finish==========
['2024-08-18', '16:29:32.907078', '100', 'test', '0.08387858591079712', '13.17', '13.47']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight', 'features.14.weight', 'features.34.weight', 'features.37.weight', 'features.10.weight', 'features.40.weight', 'features.7.weight', 'features.3.weight', 'features.0.weight', 'classifier.weight', 'features.38.weight', 'features.21.weight', 'features.31.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.090255	Acc: 1.0% (100/10000)
[Test]  Epoch: 2	Loss: 0.077622	Acc: 1.3% (128/10000)
[Test]  Epoch: 3	Loss: 0.071484	Acc: 1.9% (195/10000)
[Test]  Epoch: 4	Loss: 0.071497	Acc: 2.1% (215/10000)
[Test]  Epoch: 5	Loss: 0.073255	Acc: 2.3% (233/10000)
[Test]  Epoch: 6	Loss: 0.076884	Acc: 2.2% (219/10000)
[Test]  Epoch: 7	Loss: 0.070353	Acc: 2.8% (282/10000)
[Test]  Epoch: 8	Loss: 0.071911	Acc: 2.5% (248/10000)
[Test]  Epoch: 9	Loss: 0.079535	Acc: 2.6% (257/10000)
[Test]  Epoch: 10	Loss: 0.069577	Acc: 3.7% (373/10000)
[Test]  Epoch: 11	Loss: 0.072542	Acc: 3.8% (378/10000)
[Test]  Epoch: 12	Loss: 0.071190	Acc: 3.6% (363/10000)
[Test]  Epoch: 13	Loss: 0.068972	Acc: 4.0% (400/10000)
[Test]  Epoch: 14	Loss: 0.071988	Acc: 4.7% (474/10000)
[Test]  Epoch: 15	Loss: 0.069740	Acc: 4.2% (416/10000)
[Test]  Epoch: 16	Loss: 0.067548	Acc: 5.2% (525/10000)
[Test]  Epoch: 17	Loss: 0.077858	Acc: 4.6% (462/10000)
[Test]  Epoch: 18	Loss: 0.069147	Acc: 5.2% (524/10000)
[Test]  Epoch: 19	Loss: 0.072003	Acc: 4.9% (487/10000)
[Test]  Epoch: 20	Loss: 0.078464	Acc: 4.2% (425/10000)
[Test]  Epoch: 21	Loss: 0.070248	Acc: 5.5% (548/10000)
[Test]  Epoch: 22	Loss: 0.069819	Acc: 6.7% (669/10000)
[Test]  Epoch: 23	Loss: 0.073586	Acc: 6.1% (607/10000)
[Test]  Epoch: 24	Loss: 0.079308	Acc: 6.2% (621/10000)
[Test]  Epoch: 25	Loss: 0.072625	Acc: 5.8% (576/10000)
[Test]  Epoch: 26	Loss: 0.069365	Acc: 7.0% (695/10000)
[Test]  Epoch: 27	Loss: 0.071881	Acc: 7.5% (746/10000)
[Test]  Epoch: 28	Loss: 0.073961	Acc: 6.7% (671/10000)
[Test]  Epoch: 29	Loss: 0.077827	Acc: 6.2% (619/10000)
[Test]  Epoch: 30	Loss: 0.072531	Acc: 8.2% (821/10000)
[Test]  Epoch: 31	Loss: 0.076444	Acc: 7.6% (757/10000)
[Test]  Epoch: 32	Loss: 0.081593	Acc: 6.0% (603/10000)
[Test]  Epoch: 33	Loss: 0.079080	Acc: 7.1% (708/10000)
[Test]  Epoch: 34	Loss: 0.077243	Acc: 9.1% (909/10000)
[Test]  Epoch: 35	Loss: 0.084311	Acc: 7.6% (759/10000)
[Test]  Epoch: 36	Loss: 0.081889	Acc: 8.1% (809/10000)
[Test]  Epoch: 37	Loss: 0.081794	Acc: 8.4% (841/10000)
[Test]  Epoch: 38	Loss: 0.080394	Acc: 8.6% (858/10000)
[Test]  Epoch: 39	Loss: 0.077943	Acc: 10.3% (1035/10000)
[Test]  Epoch: 40	Loss: 0.087945	Acc: 8.2% (817/10000)
[Test]  Epoch: 41	Loss: 0.086855	Acc: 8.3% (835/10000)
[Test]  Epoch: 42	Loss: 0.084085	Acc: 8.5% (846/10000)
[Test]  Epoch: 43	Loss: 0.090626	Acc: 8.6% (862/10000)
[Test]  Epoch: 44	Loss: 0.092073	Acc: 7.8% (778/10000)
[Test]  Epoch: 45	Loss: 0.097361	Acc: 7.8% (782/10000)
[Test]  Epoch: 46	Loss: 0.086574	Acc: 8.4% (843/10000)
[Test]  Epoch: 47	Loss: 0.086261	Acc: 8.4% (845/10000)
[Test]  Epoch: 48	Loss: 0.083318	Acc: 10.0% (998/10000)
[Test]  Epoch: 49	Loss: 0.098342	Acc: 7.3% (732/10000)
[Test]  Epoch: 50	Loss: 0.091267	Acc: 10.2% (1023/10000)
[Test]  Epoch: 51	Loss: 0.090981	Acc: 9.3% (932/10000)
[Test]  Epoch: 52	Loss: 0.093713	Acc: 10.1% (1011/10000)
[Test]  Epoch: 53	Loss: 0.094599	Acc: 9.6% (964/10000)
[Test]  Epoch: 54	Loss: 0.099972	Acc: 9.0% (901/10000)
[Test]  Epoch: 55	Loss: 0.096239	Acc: 10.0% (1002/10000)
[Test]  Epoch: 56	Loss: 0.094122	Acc: 10.5% (1053/10000)
[Test]  Epoch: 57	Loss: 0.095463	Acc: 11.1% (1110/10000)
[Test]  Epoch: 58	Loss: 0.094435	Acc: 11.2% (1119/10000)
[Test]  Epoch: 59	Loss: 0.096000	Acc: 10.7% (1073/10000)
[Test]  Epoch: 60	Loss: 0.093922	Acc: 11.0% (1104/10000)
[Test]  Epoch: 61	Loss: 0.090264	Acc: 12.0% (1203/10000)
[Test]  Epoch: 62	Loss: 0.089218	Acc: 12.5% (1248/10000)
[Test]  Epoch: 63	Loss: 0.089138	Acc: 12.8% (1282/10000)
[Test]  Epoch: 64	Loss: 0.089144	Acc: 12.5% (1247/10000)
[Test]  Epoch: 65	Loss: 0.089378	Acc: 12.2% (1215/10000)
[Test]  Epoch: 66	Loss: 0.089269	Acc: 13.0% (1296/10000)
[Test]  Epoch: 67	Loss: 0.088604	Acc: 12.8% (1285/10000)
[Test]  Epoch: 68	Loss: 0.089148	Acc: 12.9% (1287/10000)
[Test]  Epoch: 69	Loss: 0.088968	Acc: 12.7% (1270/10000)
[Test]  Epoch: 70	Loss: 0.088916	Acc: 12.6% (1256/10000)
[Test]  Epoch: 71	Loss: 0.088994	Acc: 12.7% (1273/10000)
[Test]  Epoch: 72	Loss: 0.089067	Acc: 12.7% (1268/10000)
[Test]  Epoch: 73	Loss: 0.089156	Acc: 12.8% (1279/10000)
[Test]  Epoch: 74	Loss: 0.089420	Acc: 12.8% (1280/10000)
[Test]  Epoch: 75	Loss: 0.089084	Acc: 13.0% (1298/10000)
[Test]  Epoch: 76	Loss: 0.088748	Acc: 13.2% (1321/10000)
[Test]  Epoch: 77	Loss: 0.088633	Acc: 13.1% (1313/10000)
[Test]  Epoch: 78	Loss: 0.088938	Acc: 13.1% (1307/10000)
[Test]  Epoch: 79	Loss: 0.089075	Acc: 12.9% (1293/10000)
[Test]  Epoch: 80	Loss: 0.089043	Acc: 13.1% (1309/10000)
[Test]  Epoch: 81	Loss: 0.088685	Acc: 13.5% (1348/10000)
[Test]  Epoch: 82	Loss: 0.089148	Acc: 12.9% (1291/10000)
[Test]  Epoch: 83	Loss: 0.089102	Acc: 12.9% (1293/10000)
[Test]  Epoch: 84	Loss: 0.089048	Acc: 13.2% (1323/10000)
[Test]  Epoch: 85	Loss: 0.089277	Acc: 13.1% (1306/10000)
[Test]  Epoch: 86	Loss: 0.089306	Acc: 13.0% (1304/10000)
[Test]  Epoch: 87	Loss: 0.089415	Acc: 13.0% (1301/10000)
[Test]  Epoch: 88	Loss: 0.089174	Acc: 13.1% (1313/10000)
[Test]  Epoch: 89	Loss: 0.089082	Acc: 13.1% (1311/10000)
[Test]  Epoch: 90	Loss: 0.089358	Acc: 13.1% (1311/10000)
[Test]  Epoch: 91	Loss: 0.089491	Acc: 12.9% (1289/10000)
[Test]  Epoch: 92	Loss: 0.089046	Acc: 13.4% (1336/10000)
[Test]  Epoch: 93	Loss: 0.089020	Acc: 13.3% (1328/10000)
[Test]  Epoch: 94	Loss: 0.089079	Acc: 13.4% (1345/10000)
[Test]  Epoch: 95	Loss: 0.089216	Acc: 13.2% (1317/10000)
[Test]  Epoch: 96	Loss: 0.089575	Acc: 13.0% (1297/10000)
[Test]  Epoch: 97	Loss: 0.089132	Acc: 13.7% (1365/10000)
[Test]  Epoch: 98	Loss: 0.089070	Acc: 13.8% (1375/10000)
[Test]  Epoch: 99	Loss: 0.089851	Acc: 13.1% (1310/10000)
[Test]  Epoch: 100	Loss: 0.089720	Acc: 13.3% (1333/10000)
===========finish==========
['2024-08-18', '16:32:06.886651', '100', 'test', '0.08971975336074829', '13.33', '13.75']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight', 'features.14.weight', 'features.34.weight', 'features.37.weight', 'features.10.weight', 'features.40.weight', 'features.7.weight', 'features.3.weight', 'features.0.weight', 'classifier.weight', 'features.38.weight', 'features.21.weight', 'features.31.weight', 'features.11.weight', 'features.25.weight', 'features.28.weight', 'features.35.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.084794	Acc: 1.1% (113/10000)
[Test]  Epoch: 2	Loss: 0.071226	Acc: 2.2% (223/10000)
[Test]  Epoch: 3	Loss: 0.074405	Acc: 3.0% (295/10000)
[Test]  Epoch: 4	Loss: 0.070319	Acc: 3.4% (343/10000)
[Test]  Epoch: 5	Loss: 0.073638	Acc: 3.3% (330/10000)
[Test]  Epoch: 6	Loss: 0.070463	Acc: 3.6% (360/10000)
[Test]  Epoch: 7	Loss: 0.068127	Acc: 4.4% (436/10000)
[Test]  Epoch: 8	Loss: 0.068971	Acc: 3.8% (381/10000)
[Test]  Epoch: 9	Loss: 0.067650	Acc: 5.3% (531/10000)
[Test]  Epoch: 10	Loss: 0.067236	Acc: 5.5% (553/10000)
[Test]  Epoch: 11	Loss: 0.069367	Acc: 5.9% (586/10000)
[Test]  Epoch: 12	Loss: 0.067717	Acc: 6.0% (597/10000)
[Test]  Epoch: 13	Loss: 0.074475	Acc: 4.8% (481/10000)
[Test]  Epoch: 14	Loss: 0.067283	Acc: 6.6% (661/10000)
[Test]  Epoch: 15	Loss: 0.069182	Acc: 6.1% (608/10000)
[Test]  Epoch: 16	Loss: 0.070544	Acc: 6.1% (611/10000)
[Test]  Epoch: 17	Loss: 0.073317	Acc: 7.1% (711/10000)
[Test]  Epoch: 18	Loss: 0.073792	Acc: 5.5% (547/10000)
[Test]  Epoch: 19	Loss: 0.071539	Acc: 6.2% (619/10000)
[Test]  Epoch: 20	Loss: 0.069986	Acc: 7.8% (777/10000)
[Test]  Epoch: 21	Loss: 0.072141	Acc: 7.4% (739/10000)
[Test]  Epoch: 22	Loss: 0.071439	Acc: 7.8% (780/10000)
[Test]  Epoch: 23	Loss: 0.076184	Acc: 8.5% (852/10000)
[Test]  Epoch: 24	Loss: 0.074870	Acc: 8.0% (802/10000)
[Test]  Epoch: 25	Loss: 0.080207	Acc: 7.8% (785/10000)
[Test]  Epoch: 26	Loss: 0.075115	Acc: 9.1% (906/10000)
[Test]  Epoch: 27	Loss: 0.075255	Acc: 8.6% (862/10000)
[Test]  Epoch: 28	Loss: 0.083798	Acc: 7.7% (773/10000)
[Test]  Epoch: 29	Loss: 0.077303	Acc: 8.6% (863/10000)
[Test]  Epoch: 30	Loss: 0.076600	Acc: 8.2% (821/10000)
[Test]  Epoch: 31	Loss: 0.080567	Acc: 8.4% (843/10000)
[Test]  Epoch: 32	Loss: 0.082780	Acc: 9.2% (920/10000)
[Test]  Epoch: 33	Loss: 0.081767	Acc: 8.9% (889/10000)
[Test]  Epoch: 34	Loss: 0.083927	Acc: 9.6% (960/10000)
[Test]  Epoch: 35	Loss: 0.096708	Acc: 8.7% (872/10000)
[Test]  Epoch: 36	Loss: 0.083358	Acc: 10.1% (1009/10000)
[Test]  Epoch: 37	Loss: 0.083729	Acc: 10.2% (1016/10000)
[Test]  Epoch: 38	Loss: 0.082482	Acc: 9.7% (970/10000)
[Test]  Epoch: 39	Loss: 0.086981	Acc: 10.0% (1000/10000)
[Test]  Epoch: 40	Loss: 0.087167	Acc: 10.1% (1007/10000)
[Test]  Epoch: 41	Loss: 0.088613	Acc: 10.2% (1023/10000)
[Test]  Epoch: 42	Loss: 0.093586	Acc: 10.1% (1009/10000)
[Test]  Epoch: 43	Loss: 0.092862	Acc: 10.0% (999/10000)
[Test]  Epoch: 44	Loss: 0.091711	Acc: 10.4% (1044/10000)
[Test]  Epoch: 45	Loss: 0.094029	Acc: 10.3% (1034/10000)
[Test]  Epoch: 46	Loss: 0.089753	Acc: 10.9% (1088/10000)
[Test]  Epoch: 47	Loss: 0.093796	Acc: 10.3% (1032/10000)
[Test]  Epoch: 48	Loss: 0.092282	Acc: 10.7% (1066/10000)
[Test]  Epoch: 49	Loss: 0.090225	Acc: 11.6% (1161/10000)
[Test]  Epoch: 50	Loss: 0.095333	Acc: 11.6% (1156/10000)
[Test]  Epoch: 51	Loss: 0.096252	Acc: 11.0% (1104/10000)
[Test]  Epoch: 52	Loss: 0.093697	Acc: 11.5% (1149/10000)
[Test]  Epoch: 53	Loss: 0.093359	Acc: 11.3% (1135/10000)
[Test]  Epoch: 54	Loss: 0.093984	Acc: 11.6% (1164/10000)
[Test]  Epoch: 55	Loss: 0.097785	Acc: 10.7% (1070/10000)
[Test]  Epoch: 56	Loss: 0.094307	Acc: 11.2% (1125/10000)
[Test]  Epoch: 57	Loss: 0.094649	Acc: 12.8% (1284/10000)
[Test]  Epoch: 58	Loss: 0.093184	Acc: 11.8% (1184/10000)
[Test]  Epoch: 59	Loss: 0.097418	Acc: 11.9% (1194/10000)
[Test]  Epoch: 60	Loss: 0.093554	Acc: 12.3% (1226/10000)
[Test]  Epoch: 61	Loss: 0.092231	Acc: 13.4% (1336/10000)
[Test]  Epoch: 62	Loss: 0.091003	Acc: 13.3% (1333/10000)
[Test]  Epoch: 63	Loss: 0.091245	Acc: 13.4% (1341/10000)
[Test]  Epoch: 64	Loss: 0.091448	Acc: 13.5% (1346/10000)
[Test]  Epoch: 65	Loss: 0.090767	Acc: 13.7% (1369/10000)
[Test]  Epoch: 66	Loss: 0.090387	Acc: 13.9% (1388/10000)
[Test]  Epoch: 67	Loss: 0.090033	Acc: 13.7% (1365/10000)
[Test]  Epoch: 68	Loss: 0.090293	Acc: 13.7% (1373/10000)
[Test]  Epoch: 69	Loss: 0.090331	Acc: 13.7% (1369/10000)
[Test]  Epoch: 70	Loss: 0.090352	Acc: 13.6% (1356/10000)
[Test]  Epoch: 71	Loss: 0.090220	Acc: 13.7% (1368/10000)
[Test]  Epoch: 72	Loss: 0.089839	Acc: 13.9% (1392/10000)
[Test]  Epoch: 73	Loss: 0.090330	Acc: 14.0% (1397/10000)
[Test]  Epoch: 74	Loss: 0.089836	Acc: 14.1% (1411/10000)
[Test]  Epoch: 75	Loss: 0.090257	Acc: 14.0% (1402/10000)
[Test]  Epoch: 76	Loss: 0.090106	Acc: 13.7% (1367/10000)
[Test]  Epoch: 77	Loss: 0.090081	Acc: 13.7% (1371/10000)
[Test]  Epoch: 78	Loss: 0.089715	Acc: 13.7% (1370/10000)
[Test]  Epoch: 79	Loss: 0.089675	Acc: 13.8% (1385/10000)
[Test]  Epoch: 80	Loss: 0.089895	Acc: 13.8% (1382/10000)
[Test]  Epoch: 81	Loss: 0.089501	Acc: 13.8% (1385/10000)
[Test]  Epoch: 82	Loss: 0.089811	Acc: 13.9% (1387/10000)
[Test]  Epoch: 83	Loss: 0.089727	Acc: 13.8% (1382/10000)
[Test]  Epoch: 84	Loss: 0.089138	Acc: 13.7% (1374/10000)
[Test]  Epoch: 85	Loss: 0.089781	Acc: 13.9% (1389/10000)
[Test]  Epoch: 86	Loss: 0.089290	Acc: 14.2% (1425/10000)
[Test]  Epoch: 87	Loss: 0.089772	Acc: 13.7% (1366/10000)
[Test]  Epoch: 88	Loss: 0.089459	Acc: 13.7% (1371/10000)
[Test]  Epoch: 89	Loss: 0.089279	Acc: 14.0% (1396/10000)
[Test]  Epoch: 90	Loss: 0.089284	Acc: 14.2% (1423/10000)
[Test]  Epoch: 91	Loss: 0.089698	Acc: 13.9% (1389/10000)
[Test]  Epoch: 92	Loss: 0.089527	Acc: 13.8% (1381/10000)
[Test]  Epoch: 93	Loss: 0.089129	Acc: 13.9% (1387/10000)
[Test]  Epoch: 94	Loss: 0.089247	Acc: 14.1% (1409/10000)
[Test]  Epoch: 95	Loss: 0.089258	Acc: 14.2% (1419/10000)
[Test]  Epoch: 96	Loss: 0.089559	Acc: 13.9% (1395/10000)
[Test]  Epoch: 97	Loss: 0.088973	Acc: 14.1% (1411/10000)
[Test]  Epoch: 98	Loss: 0.088719	Acc: 14.2% (1425/10000)
[Test]  Epoch: 99	Loss: 0.089294	Acc: 14.3% (1431/10000)
[Test]  Epoch: 100	Loss: 0.089333	Acc: 13.9% (1386/10000)
===========finish==========
['2024-08-18', '16:34:38.793000', '100', 'test', '0.08933271651268006', '13.86', '14.31']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight', 'features.14.weight', 'features.34.weight', 'features.37.weight', 'features.10.weight', 'features.40.weight', 'features.7.weight', 'features.3.weight', 'features.0.weight', 'classifier.weight', 'features.38.weight', 'features.21.weight', 'features.31.weight', 'features.11.weight', 'features.25.weight', 'features.28.weight', 'features.35.weight', 'features.15.weight', 'features.18.weight', 'features.4.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.154516	Acc: 1.6% (158/10000)
[Test]  Epoch: 2	Loss: 0.074727	Acc: 1.8% (177/10000)
[Test]  Epoch: 3	Loss: 0.076935	Acc: 3.0% (303/10000)
[Test]  Epoch: 4	Loss: 0.070540	Acc: 3.6% (364/10000)
[Test]  Epoch: 5	Loss: 0.069346	Acc: 3.9% (393/10000)
[Test]  Epoch: 6	Loss: 0.072239	Acc: 3.5% (351/10000)
[Test]  Epoch: 7	Loss: 0.066359	Acc: 5.6% (559/10000)
[Test]  Epoch: 8	Loss: 0.067088	Acc: 4.8% (480/10000)
[Test]  Epoch: 9	Loss: 0.066528	Acc: 5.7% (565/10000)
[Test]  Epoch: 10	Loss: 0.067340	Acc: 5.8% (581/10000)
[Test]  Epoch: 11	Loss: 0.067242	Acc: 7.0% (702/10000)
[Test]  Epoch: 12	Loss: 0.066722	Acc: 6.2% (621/10000)
[Test]  Epoch: 13	Loss: 0.070645	Acc: 6.1% (613/10000)
[Test]  Epoch: 14	Loss: 0.068909	Acc: 6.6% (660/10000)
[Test]  Epoch: 15	Loss: 0.068268	Acc: 6.2% (615/10000)
[Test]  Epoch: 16	Loss: 0.070053	Acc: 7.8% (785/10000)
[Test]  Epoch: 17	Loss: 0.070232	Acc: 7.9% (791/10000)
[Test]  Epoch: 18	Loss: 0.071701	Acc: 7.7% (765/10000)
[Test]  Epoch: 19	Loss: 0.080677	Acc: 6.5% (647/10000)
[Test]  Epoch: 20	Loss: 0.077610	Acc: 7.9% (787/10000)
[Test]  Epoch: 21	Loss: 0.070365	Acc: 8.5% (846/10000)
[Test]  Epoch: 22	Loss: 0.070728	Acc: 8.9% (888/10000)
[Test]  Epoch: 23	Loss: 0.079418	Acc: 9.0% (904/10000)
[Test]  Epoch: 24	Loss: 0.074644	Acc: 9.0% (898/10000)
[Test]  Epoch: 25	Loss: 0.075686	Acc: 9.1% (905/10000)
[Test]  Epoch: 26	Loss: 0.073802	Acc: 9.3% (928/10000)
[Test]  Epoch: 27	Loss: 0.077985	Acc: 9.6% (957/10000)
[Test]  Epoch: 28	Loss: 0.081737	Acc: 9.0% (901/10000)
[Test]  Epoch: 29	Loss: 0.081182	Acc: 8.8% (881/10000)
[Test]  Epoch: 30	Loss: 0.077241	Acc: 10.2% (1017/10000)
[Test]  Epoch: 31	Loss: 0.082850	Acc: 9.5% (950/10000)
[Test]  Epoch: 32	Loss: 0.085223	Acc: 7.9% (788/10000)
[Test]  Epoch: 33	Loss: 0.088960	Acc: 8.7% (873/10000)
[Test]  Epoch: 34	Loss: 0.084456	Acc: 11.0% (1100/10000)
[Test]  Epoch: 35	Loss: 0.088313	Acc: 10.2% (1019/10000)
[Test]  Epoch: 36	Loss: 0.084666	Acc: 10.8% (1081/10000)
[Test]  Epoch: 37	Loss: 0.083654	Acc: 11.6% (1158/10000)
[Test]  Epoch: 38	Loss: 0.087060	Acc: 10.1% (1011/10000)
[Test]  Epoch: 39	Loss: 0.083905	Acc: 11.7% (1170/10000)
[Test]  Epoch: 40	Loss: 0.088486	Acc: 12.1% (1209/10000)
[Test]  Epoch: 41	Loss: 0.087903	Acc: 11.2% (1115/10000)
[Test]  Epoch: 42	Loss: 0.089508	Acc: 11.0% (1098/10000)
[Test]  Epoch: 43	Loss: 0.092050	Acc: 11.9% (1191/10000)
[Test]  Epoch: 44	Loss: 0.090123	Acc: 11.6% (1159/10000)
[Test]  Epoch: 45	Loss: 0.091804	Acc: 10.4% (1043/10000)
[Test]  Epoch: 46	Loss: 0.086752	Acc: 12.0% (1202/10000)
[Test]  Epoch: 47	Loss: 0.086956	Acc: 13.0% (1299/10000)
[Test]  Epoch: 48	Loss: 0.090051	Acc: 11.2% (1125/10000)
[Test]  Epoch: 49	Loss: 0.097270	Acc: 11.8% (1178/10000)
[Test]  Epoch: 50	Loss: 0.093250	Acc: 12.7% (1267/10000)
[Test]  Epoch: 51	Loss: 0.093902	Acc: 11.4% (1137/10000)
[Test]  Epoch: 52	Loss: 0.094431	Acc: 11.8% (1185/10000)
[Test]  Epoch: 53	Loss: 0.090984	Acc: 12.3% (1231/10000)
[Test]  Epoch: 54	Loss: 0.090617	Acc: 13.1% (1307/10000)
[Test]  Epoch: 55	Loss: 0.092639	Acc: 12.2% (1222/10000)
[Test]  Epoch: 56	Loss: 0.090643	Acc: 13.3% (1333/10000)
[Test]  Epoch: 57	Loss: 0.090437	Acc: 13.9% (1387/10000)
[Test]  Epoch: 58	Loss: 0.090544	Acc: 13.8% (1381/10000)
[Test]  Epoch: 59	Loss: 0.093112	Acc: 12.1% (1210/10000)
[Test]  Epoch: 60	Loss: 0.092344	Acc: 12.8% (1284/10000)
[Test]  Epoch: 61	Loss: 0.089228	Acc: 13.6% (1364/10000)
[Test]  Epoch: 62	Loss: 0.088554	Acc: 14.1% (1409/10000)
[Test]  Epoch: 63	Loss: 0.088784	Acc: 14.1% (1409/10000)
[Test]  Epoch: 64	Loss: 0.088914	Acc: 14.2% (1425/10000)
[Test]  Epoch: 65	Loss: 0.088715	Acc: 14.2% (1423/10000)
[Test]  Epoch: 66	Loss: 0.088528	Acc: 14.6% (1456/10000)
[Test]  Epoch: 67	Loss: 0.088187	Acc: 14.3% (1433/10000)
[Test]  Epoch: 68	Loss: 0.088329	Acc: 14.3% (1427/10000)
[Test]  Epoch: 69	Loss: 0.087862	Acc: 14.5% (1447/10000)
[Test]  Epoch: 70	Loss: 0.088016	Acc: 14.3% (1435/10000)
[Test]  Epoch: 71	Loss: 0.087994	Acc: 14.3% (1430/10000)
[Test]  Epoch: 72	Loss: 0.087867	Acc: 14.3% (1432/10000)
[Test]  Epoch: 73	Loss: 0.088331	Acc: 14.6% (1459/10000)
[Test]  Epoch: 74	Loss: 0.087935	Acc: 14.1% (1409/10000)
[Test]  Epoch: 75	Loss: 0.088163	Acc: 14.5% (1449/10000)
[Test]  Epoch: 76	Loss: 0.087708	Acc: 14.2% (1424/10000)
[Test]  Epoch: 77	Loss: 0.088156	Acc: 14.3% (1435/10000)
[Test]  Epoch: 78	Loss: 0.087761	Acc: 14.2% (1423/10000)
[Test]  Epoch: 79	Loss: 0.088306	Acc: 14.2% (1424/10000)
[Test]  Epoch: 80	Loss: 0.088213	Acc: 13.8% (1382/10000)
[Test]  Epoch: 81	Loss: 0.087671	Acc: 14.3% (1432/10000)
[Test]  Epoch: 82	Loss: 0.087728	Acc: 14.3% (1427/10000)
[Test]  Epoch: 83	Loss: 0.087850	Acc: 14.4% (1445/10000)
[Test]  Epoch: 84	Loss: 0.087242	Acc: 14.6% (1457/10000)
[Test]  Epoch: 85	Loss: 0.087608	Acc: 14.3% (1434/10000)
[Test]  Epoch: 86	Loss: 0.087261	Acc: 14.5% (1447/10000)
[Test]  Epoch: 87	Loss: 0.087911	Acc: 14.5% (1454/10000)
[Test]  Epoch: 88	Loss: 0.087521	Acc: 14.5% (1446/10000)
[Test]  Epoch: 89	Loss: 0.087092	Acc: 14.4% (1442/10000)
[Test]  Epoch: 90	Loss: 0.087374	Acc: 14.4% (1441/10000)
[Test]  Epoch: 91	Loss: 0.087165	Acc: 14.5% (1454/10000)
[Test]  Epoch: 92	Loss: 0.087346	Acc: 14.2% (1418/10000)
[Test]  Epoch: 93	Loss: 0.087753	Acc: 14.3% (1431/10000)
[Test]  Epoch: 94	Loss: 0.087620	Acc: 14.7% (1466/10000)
[Test]  Epoch: 95	Loss: 0.087453	Acc: 14.1% (1414/10000)
[Test]  Epoch: 96	Loss: 0.087987	Acc: 14.3% (1426/10000)
[Test]  Epoch: 97	Loss: 0.087611	Acc: 14.4% (1440/10000)
[Test]  Epoch: 98	Loss: 0.087193	Acc: 14.5% (1452/10000)
[Test]  Epoch: 99	Loss: 0.087437	Acc: 14.4% (1439/10000)
[Test]  Epoch: 100	Loss: 0.087274	Acc: 14.6% (1460/10000)
===========finish==========
['2024-08-18', '16:37:12.051444', '100', 'test', '0.08727400712966919', '14.6', '14.66']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-vgg16_bn-channel vgg16_bn CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.24.weight', 'features.27.weight', 'features.30.weight', 'features.20.weight', 'features.17.weight', 'features.14.weight', 'features.34.weight', 'features.37.weight', 'features.10.weight', 'features.40.weight', 'features.7.weight', 'features.3.weight', 'features.0.weight', 'classifier.weight', 'features.38.weight', 'features.21.weight', 'features.31.weight', 'features.11.weight', 'features.25.weight', 'features.28.weight', 'features.35.weight', 'features.15.weight', 'features.18.weight', 'features.4.weight', 'features.8.weight', 'features.1.weight', 'features.41.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.360500	Acc: 1.0% (103/10000)
[Test]  Epoch: 2	Loss: 0.076927	Acc: 1.3% (134/10000)
[Test]  Epoch: 3	Loss: 0.073235	Acc: 2.1% (209/10000)
[Test]  Epoch: 4	Loss: 0.072464	Acc: 2.0% (200/10000)
[Test]  Epoch: 5	Loss: 0.072395	Acc: 2.3% (228/10000)
[Test]  Epoch: 6	Loss: 0.071304	Acc: 2.5% (255/10000)
[Test]  Epoch: 7	Loss: 0.073263	Acc: 2.6% (258/10000)
[Test]  Epoch: 8	Loss: 0.069497	Acc: 4.0% (404/10000)
[Test]  Epoch: 9	Loss: 0.069433	Acc: 3.8% (383/10000)
[Test]  Epoch: 10	Loss: 0.068691	Acc: 4.5% (455/10000)
[Test]  Epoch: 11	Loss: 0.070788	Acc: 4.9% (487/10000)
[Test]  Epoch: 12	Loss: 0.068567	Acc: 5.5% (551/10000)
[Test]  Epoch: 13	Loss: 0.076092	Acc: 4.3% (431/10000)
[Test]  Epoch: 14	Loss: 0.074644	Acc: 5.4% (541/10000)
[Test]  Epoch: 15	Loss: 0.070652	Acc: 4.9% (487/10000)
[Test]  Epoch: 16	Loss: 0.076903	Acc: 4.1% (410/10000)
[Test]  Epoch: 17	Loss: 0.074014	Acc: 6.1% (609/10000)
[Test]  Epoch: 18	Loss: 0.073557	Acc: 5.8% (579/10000)
[Test]  Epoch: 19	Loss: 0.072574	Acc: 6.3% (630/10000)
[Test]  Epoch: 20	Loss: 0.089473	Acc: 5.8% (580/10000)
[Test]  Epoch: 21	Loss: 0.071192	Acc: 6.8% (681/10000)
[Test]  Epoch: 22	Loss: 0.071189	Acc: 7.1% (712/10000)
[Test]  Epoch: 23	Loss: 0.073293	Acc: 6.3% (629/10000)
[Test]  Epoch: 24	Loss: 0.075521	Acc: 7.1% (713/10000)
[Test]  Epoch: 25	Loss: 0.076758	Acc: 6.1% (612/10000)
[Test]  Epoch: 26	Loss: 0.075039	Acc: 7.7% (772/10000)
[Test]  Epoch: 27	Loss: 0.091146	Acc: 6.8% (685/10000)
[Test]  Epoch: 28	Loss: 0.086316	Acc: 5.7% (573/10000)
[Test]  Epoch: 29	Loss: 0.084538	Acc: 6.9% (690/10000)
[Test]  Epoch: 30	Loss: 0.077079	Acc: 7.3% (735/10000)
[Test]  Epoch: 31	Loss: 0.085499	Acc: 8.1% (805/10000)
[Test]  Epoch: 32	Loss: 0.080691	Acc: 8.0% (795/10000)
[Test]  Epoch: 33	Loss: 0.088111	Acc: 7.7% (768/10000)
[Test]  Epoch: 34	Loss: 0.087239	Acc: 8.3% (832/10000)
[Test]  Epoch: 35	Loss: 0.096966	Acc: 7.7% (769/10000)
[Test]  Epoch: 36	Loss: 0.091797	Acc: 8.9% (889/10000)
[Test]  Epoch: 37	Loss: 0.099219	Acc: 7.3% (728/10000)
[Test]  Epoch: 38	Loss: 0.087266	Acc: 8.9% (888/10000)
[Test]  Epoch: 39	Loss: 0.091654	Acc: 9.5% (954/10000)
[Test]  Epoch: 40	Loss: 0.099744	Acc: 8.2% (815/10000)
[Test]  Epoch: 41	Loss: 0.098166	Acc: 8.8% (882/10000)
[Test]  Epoch: 42	Loss: 0.102228	Acc: 9.1% (911/10000)
[Test]  Epoch: 43	Loss: 0.104179	Acc: 9.6% (962/10000)
[Test]  Epoch: 44	Loss: 0.098804	Acc: 9.1% (913/10000)
[Test]  Epoch: 45	Loss: 0.103533	Acc: 10.1% (1014/10000)
[Test]  Epoch: 46	Loss: 0.099368	Acc: 10.6% (1061/10000)
[Test]  Epoch: 47	Loss: 0.110669	Acc: 9.3% (934/10000)
[Test]  Epoch: 48	Loss: 0.101444	Acc: 9.8% (983/10000)
[Test]  Epoch: 49	Loss: 0.103877	Acc: 9.9% (995/10000)
[Test]  Epoch: 50	Loss: 0.100854	Acc: 10.9% (1093/10000)
[Test]  Epoch: 51	Loss: 0.097668	Acc: 11.1% (1112/10000)
[Test]  Epoch: 52	Loss: 0.104862	Acc: 10.3% (1031/10000)
[Test]  Epoch: 53	Loss: 0.109772	Acc: 9.9% (994/10000)
[Test]  Epoch: 54	Loss: 0.108150	Acc: 10.5% (1051/10000)
[Test]  Epoch: 55	Loss: 0.115119	Acc: 8.9% (893/10000)
[Test]  Epoch: 56	Loss: 0.100297	Acc: 10.2% (1019/10000)
[Test]  Epoch: 57	Loss: 0.102485	Acc: 11.0% (1097/10000)
[Test]  Epoch: 58	Loss: 0.106045	Acc: 11.0% (1099/10000)
[Test]  Epoch: 59	Loss: 0.099976	Acc: 11.7% (1167/10000)
[Test]  Epoch: 60	Loss: 0.100384	Acc: 12.2% (1219/10000)
[Test]  Epoch: 61	Loss: 0.097807	Acc: 12.7% (1273/10000)
[Test]  Epoch: 62	Loss: 0.096849	Acc: 13.0% (1299/10000)
[Test]  Epoch: 63	Loss: 0.096662	Acc: 13.2% (1319/10000)
[Test]  Epoch: 64	Loss: 0.096433	Acc: 13.2% (1315/10000)
[Test]  Epoch: 65	Loss: 0.096104	Acc: 13.2% (1317/10000)
[Test]  Epoch: 66	Loss: 0.096134	Acc: 13.4% (1345/10000)
[Test]  Epoch: 67	Loss: 0.095272	Acc: 13.5% (1347/10000)
[Test]  Epoch: 68	Loss: 0.096420	Acc: 13.5% (1350/10000)
[Test]  Epoch: 69	Loss: 0.095834	Acc: 13.4% (1341/10000)
[Test]  Epoch: 70	Loss: 0.096019	Acc: 13.3% (1327/10000)
[Test]  Epoch: 71	Loss: 0.095401	Acc: 13.6% (1356/10000)
[Test]  Epoch: 72	Loss: 0.096020	Acc: 13.6% (1356/10000)
[Test]  Epoch: 73	Loss: 0.095869	Acc: 13.3% (1326/10000)
[Test]  Epoch: 74	Loss: 0.095473	Acc: 13.3% (1333/10000)
[Test]  Epoch: 75	Loss: 0.095458	Acc: 13.5% (1351/10000)
[Test]  Epoch: 76	Loss: 0.095386	Acc: 13.8% (1380/10000)
[Test]  Epoch: 77	Loss: 0.094697	Acc: 13.7% (1373/10000)
[Test]  Epoch: 78	Loss: 0.094951	Acc: 13.5% (1353/10000)
[Test]  Epoch: 79	Loss: 0.094892	Acc: 13.4% (1337/10000)
[Test]  Epoch: 80	Loss: 0.094942	Acc: 13.5% (1350/10000)
[Test]  Epoch: 81	Loss: 0.095233	Acc: 13.6% (1356/10000)
[Test]  Epoch: 82	Loss: 0.094936	Acc: 13.6% (1355/10000)
[Test]  Epoch: 83	Loss: 0.094731	Acc: 13.5% (1354/10000)
[Test]  Epoch: 84	Loss: 0.095129	Acc: 13.4% (1343/10000)
[Test]  Epoch: 85	Loss: 0.095066	Acc: 13.5% (1346/10000)
[Test]  Epoch: 86	Loss: 0.094717	Acc: 13.8% (1377/10000)
[Test]  Epoch: 87	Loss: 0.095263	Acc: 13.6% (1360/10000)
[Test]  Epoch: 88	Loss: 0.094829	Acc: 13.5% (1354/10000)
[Test]  Epoch: 89	Loss: 0.094414	Acc: 13.6% (1357/10000)
[Test]  Epoch: 90	Loss: 0.094577	Acc: 13.8% (1382/10000)
[Test]  Epoch: 91	Loss: 0.094780	Acc: 13.5% (1351/10000)
[Test]  Epoch: 92	Loss: 0.094147	Acc: 13.6% (1363/10000)
[Test]  Epoch: 93	Loss: 0.094890	Acc: 13.5% (1348/10000)
[Test]  Epoch: 94	Loss: 0.094273	Acc: 13.6% (1361/10000)
[Test]  Epoch: 95	Loss: 0.094420	Acc: 13.7% (1373/10000)
[Test]  Epoch: 96	Loss: 0.094668	Acc: 13.6% (1358/10000)
[Test]  Epoch: 97	Loss: 0.094342	Acc: 13.6% (1361/10000)
[Test]  Epoch: 98	Loss: 0.094358	Acc: 13.5% (1346/10000)
[Test]  Epoch: 99	Loss: 0.094258	Acc: 13.7% (1368/10000)
[Test]  Epoch: 100	Loss: 0.094337	Acc: 13.6% (1361/10000)
===========finish==========
['2024-08-18', '16:39:46.643338', '100', 'test', '0.09433749995231629', '13.61', '13.82']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info does not exist. Calculating...
Load model from models/victim/cifar100-mobilenetv2/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('_features.18.0.weight', 54997901312.0), ('last_linear.weight', 627054.375), ('_features.17.conv.0.0.weight', 22559.212890625), ('_features.17.conv.1.0.weight', 20770.4609375), ('_features.17.conv.3.weight', 18282.470703125), ('_features.17.conv.2.weight', 15789.4892578125), ('_features.14.conv.1.1.weight', 339.8189697265625), ('_features.14.conv.3.weight', 270.2342529296875), ('_features.17.conv.1.1.weight', 189.4914093017578), ('_features.14.conv.2.weight', 155.06369018554688), ('_features.14.conv.0.1.weight', 71.5553970336914), ('_features.17.conv.0.1.weight', 56.96590042114258), ('_features.14.conv.0.0.weight', 49.598854064941406), ('_features.16.conv.3.weight', 45.588191986083984), ('_features.14.conv.1.0.weight', 39.111202239990234), ('_features.16.conv.2.weight', 37.09537124633789), ('_features.12.conv.0.0.weight', 20.271484375), ('_features.11.conv.2.weight', 17.161720275878906), ('_features.9.conv.2.weight', 16.58329200744629), ('_features.11.conv.0.0.weight', 16.409557342529297), ('_features.10.conv.2.weight', 15.13720703125), ('_features.10.conv.0.0.weight', 9.695609092712402), ('_features.11.conv.1.0.weight', 9.417478561401367), ('_features.15.conv.0.0.weight', 9.358665466308594), ('_features.8.conv.2.weight', 6.763263702392578), ('_features.11.conv.0.1.weight', 6.120171070098877), ('_features.15.conv.0.1.weight', 5.499028205871582), ('_features.7.conv.2.weight', 5.459715843200684), ('_features.9.conv.0.0.weight', 5.338939666748047), ('_features.11.conv.1.1.weight', 4.029754638671875), ('_features.9.conv.1.0.weight', 3.597262144088745), ('_features.12.conv.2.weight', 3.519803524017334), ('_features.10.conv.1.0.weight', 3.303988456726074), ('_features.8.conv.0.0.weight', 2.8887243270874023), ('_features.11.conv.3.weight', 2.605687141418457), ('_features.4.conv.2.weight', 2.4691970348358154), ('_features.10.conv.0.1.weight', 2.288865327835083), ('_features.8.conv.1.0.weight', 2.0069730281829834), ('_features.2.conv.2.weight', 2.0058817863464355), ('_features.13.conv.2.weight', 1.9758379459381104), ('_features.2.conv.1.0.weight', 1.6689603328704834), ('_features.7.conv.0.0.weight', 1.6562814712524414), ('_features.18.1.weight', 1.617580771446228), ('_features.15.conv.2.weight', 1.5608609914779663), ('_features.5.conv.2.weight', 1.4546637535095215), ('_features.12.conv.0.1.weight', 1.4331209659576416), ('_features.10.conv.1.1.weight', 1.407439112663269), ('_features.1.conv.1.weight', 1.3527047634124756), ('_features.6.conv.2.weight', 1.3285086154937744), ('_features.3.conv.1.0.weight', 1.303019642829895), ('_features.16.conv.0.0.weight', 1.2919747829437256), ('_features.3.conv.2.weight', 1.2533862590789795), ('_features.1.conv.0.0.weight', 1.2189669609069824), ('_features.9.conv.0.1.weight', 1.1627848148345947), ('_features.9.conv.3.weight', 0.9507407546043396), ('_features.9.conv.1.1.weight', 0.9217407703399658), ('_features.8.conv.0.1.weight', 0.8682878017425537), ('_features.10.conv.3.weight', 0.8368711471557617), ('_features.13.conv.1.0.weight', 0.8011476993560791), ('_features.13.conv.0.0.weight', 0.7655912637710571), ('_features.7.conv.0.1.weight', 0.7256959676742554), ('_features.12.conv.3.weight', 0.7220079898834229), ('_features.5.conv.1.0.weight', 0.7090407609939575), ('_features.15.conv.1.1.weight', 0.6762012839317322), ('_features.4.conv.0.1.weight', 0.6224404573440552), ('_features.12.conv.1.0.weight', 0.6124007701873779), ('_features.7.conv.1.0.weight', 0.5909829139709473), ('_features.13.conv.3.weight', 0.5765300989151001), ('_features.6.conv.1.0.weight', 0.5733263492584229), ('_features.15.conv.3.weight', 0.5715693235397339), ('_features.8.conv.1.1.weight', 0.5697464346885681), ('_features.16.conv.1.1.weight', 0.562282919883728), ('_features.2.conv.0.1.weight', 0.5307806134223938), ('_features.12.conv.1.1.weight', 0.5121191740036011), ('_features.15.conv.1.0.weight', 0.5087440609931946), ('_features.4.conv.0.0.weight', 0.48437508940696716), ('_features.16.conv.1.0.weight', 0.47436752915382385), ('_features.8.conv.3.weight', 0.4686321020126343), ('_features.16.conv.0.1.weight', 0.45475244522094727), ('_features.5.conv.0.0.weight', 0.4519726037979126), ('_features.6.conv.0.0.weight', 0.4201316237449646), ('_features.4.conv.1.0.weight', 0.40551891922950745), ('_features.7.conv.3.weight', 0.39043280482292175), ('_features.7.conv.1.1.weight', 0.3687276840209961), ('_features.5.conv.0.1.weight', 0.3139670193195343), ('_features.6.conv.0.1.weight', 0.28779661655426025), ('_features.3.conv.0.1.weight', 0.269756555557251), ('_features.13.conv.1.1.weight', 0.26471808552742004), ('_features.13.conv.0.1.weight', 0.2230631411075592), ('_features.4.conv.1.1.weight', 0.20922625064849854), ('_features.2.conv.0.0.weight', 0.20749929547309875), ('_features.1.conv.0.1.weight', 0.17795774340629578), ('_features.0.0.weight', 0.17682214081287384), ('_features.5.conv.1.1.weight', 0.1716790795326233), ('_features.6.conv.1.1.weight', 0.16162820160388947), ('_features.3.conv.0.0.weight', 0.16000370681285858), ('_features.0.1.weight', 0.15697601437568665), ('_features.3.conv.1.1.weight', 0.14887651801109314), ('_features.4.conv.3.weight', 0.14214152097702026), ('_features.2.conv.1.1.weight', 0.12211170792579651), ('_features.6.conv.3.weight', 0.11621688306331635), ('_features.5.conv.3.weight', 0.07944722473621368), ('_features.2.conv.3.weight', 0.07051116973161697), ('_features.1.conv.2.weight', 0.0576472170650959), ('_features.3.conv.3.weight', 0.04752304404973984), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('_features.18.0.weight', 54997901312.0), ('last_linear.weight', 627054.375), ('_features.17.conv.0.0.weight', 22559.212890625), ('_features.17.conv.1.0.weight', 20770.4609375), ('_features.17.conv.3.weight', 18282.470703125), ('_features.17.conv.2.weight', 15789.4892578125), ('_features.14.conv.1.1.weight', 339.8189697265625), ('_features.14.conv.3.weight', 270.2342529296875), ('_features.17.conv.1.1.weight', 189.4914093017578), ('_features.14.conv.2.weight', 155.06369018554688), ('_features.14.conv.0.1.weight', 71.5553970336914), ('_features.17.conv.0.1.weight', 56.96590042114258), ('_features.14.conv.0.0.weight', 49.598854064941406), ('_features.16.conv.3.weight', 45.588191986083984), ('_features.14.conv.1.0.weight', 39.111202239990234), ('_features.16.conv.2.weight', 37.09537124633789), ('_features.12.conv.0.0.weight', 20.271484375), ('_features.11.conv.2.weight', 17.161720275878906), ('_features.9.conv.2.weight', 16.58329200744629), ('_features.11.conv.0.0.weight', 16.409557342529297), ('_features.10.conv.2.weight', 15.13720703125), ('_features.10.conv.0.0.weight', 9.695609092712402), ('_features.11.conv.1.0.weight', 9.417478561401367), ('_features.15.conv.0.0.weight', 9.358665466308594), ('_features.8.conv.2.weight', 6.763263702392578), ('_features.11.conv.0.1.weight', 6.120171070098877), ('_features.15.conv.0.1.weight', 5.499028205871582), ('_features.7.conv.2.weight', 5.459715843200684), ('_features.9.conv.0.0.weight', 5.338939666748047), ('_features.11.conv.1.1.weight', 4.029754638671875), ('_features.9.conv.1.0.weight', 3.597262144088745), ('_features.12.conv.2.weight', 3.519803524017334), ('_features.10.conv.1.0.weight', 3.303988456726074), ('_features.8.conv.0.0.weight', 2.8887243270874023), ('_features.11.conv.3.weight', 2.605687141418457), ('_features.4.conv.2.weight', 2.4691970348358154), ('_features.10.conv.0.1.weight', 2.288865327835083), ('_features.8.conv.1.0.weight', 2.0069730281829834), ('_features.2.conv.2.weight', 2.0058817863464355), ('_features.13.conv.2.weight', 1.9758379459381104), ('_features.2.conv.1.0.weight', 1.6689603328704834), ('_features.7.conv.0.0.weight', 1.6562814712524414), ('_features.18.1.weight', 1.617580771446228), ('_features.15.conv.2.weight', 1.5608609914779663), ('_features.5.conv.2.weight', 1.4546637535095215), ('_features.12.conv.0.1.weight', 1.4331209659576416), ('_features.10.conv.1.1.weight', 1.407439112663269), ('_features.1.conv.1.weight', 1.3527047634124756), ('_features.6.conv.2.weight', 1.3285086154937744), ('_features.3.conv.1.0.weight', 1.303019642829895), ('_features.16.conv.0.0.weight', 1.2919747829437256), ('_features.3.conv.2.weight', 1.2533862590789795), ('_features.1.conv.0.0.weight', 1.2189669609069824), ('_features.9.conv.0.1.weight', 1.1627848148345947), ('_features.9.conv.3.weight', 0.9507407546043396), ('_features.9.conv.1.1.weight', 0.9217407703399658), ('_features.8.conv.0.1.weight', 0.8682878017425537), ('_features.10.conv.3.weight', 0.8368711471557617), ('_features.13.conv.1.0.weight', 0.8011476993560791), ('_features.13.conv.0.0.weight', 0.7655912637710571), ('_features.7.conv.0.1.weight', 0.7256959676742554), ('_features.12.conv.3.weight', 0.7220079898834229), ('_features.5.conv.1.0.weight', 0.7090407609939575), ('_features.15.conv.1.1.weight', 0.6762012839317322), ('_features.4.conv.0.1.weight', 0.6224404573440552), ('_features.12.conv.1.0.weight', 0.6124007701873779), ('_features.7.conv.1.0.weight', 0.5909829139709473), ('_features.13.conv.3.weight', 0.5765300989151001), ('_features.6.conv.1.0.weight', 0.5733263492584229), ('_features.15.conv.3.weight', 0.5715693235397339), ('_features.8.conv.1.1.weight', 0.5697464346885681), ('_features.16.conv.1.1.weight', 0.562282919883728), ('_features.2.conv.0.1.weight', 0.5307806134223938), ('_features.12.conv.1.1.weight', 0.5121191740036011), ('_features.15.conv.1.0.weight', 0.5087440609931946), ('_features.4.conv.0.0.weight', 0.48437508940696716), ('_features.16.conv.1.0.weight', 0.47436752915382385), ('_features.8.conv.3.weight', 0.4686321020126343), ('_features.16.conv.0.1.weight', 0.45475244522094727), ('_features.5.conv.0.0.weight', 0.4519726037979126), ('_features.6.conv.0.0.weight', 0.4201316237449646), ('_features.4.conv.1.0.weight', 0.40551891922950745), ('_features.7.conv.3.weight', 0.39043280482292175), ('_features.7.conv.1.1.weight', 0.3687276840209961), ('_features.5.conv.0.1.weight', 0.3139670193195343), ('_features.6.conv.0.1.weight', 0.28779661655426025), ('_features.3.conv.0.1.weight', 0.269756555557251), ('_features.13.conv.1.1.weight', 0.26471808552742004), ('_features.13.conv.0.1.weight', 0.2230631411075592), ('_features.4.conv.1.1.weight', 0.20922625064849854), ('_features.2.conv.0.0.weight', 0.20749929547309875), ('_features.1.conv.0.1.weight', 0.17795774340629578), ('_features.0.0.weight', 0.17682214081287384), ('_features.5.conv.1.1.weight', 0.1716790795326233), ('_features.6.conv.1.1.weight', 0.16162820160388947), ('_features.3.conv.0.0.weight', 0.16000370681285858), ('_features.0.1.weight', 0.15697601437568665), ('_features.3.conv.1.1.weight', 0.14887651801109314), ('_features.4.conv.3.weight', 0.14214152097702026), ('_features.2.conv.1.1.weight', 0.12211170792579651), ('_features.6.conv.3.weight', 0.11621688306331635), ('_features.5.conv.3.weight', 0.07944722473621368), ('_features.2.conv.3.weight', 0.07051116973161697), ('_features.1.conv.2.weight', 0.0576472170650959), ('_features.3.conv.3.weight', 0.04752304404973984), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=105  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.061022	Acc: 22.3% (2234/10000)
[Test]  Epoch: 2	Loss: 0.041620	Acc: 33.2% (3317/10000)
[Test]  Epoch: 3	Loss: 0.040666	Acc: 33.3% (3328/10000)
[Test]  Epoch: 4	Loss: 0.035342	Acc: 39.5% (3950/10000)
[Test]  Epoch: 5	Loss: 0.035467	Acc: 39.9% (3988/10000)
[Test]  Epoch: 6	Loss: 0.034540	Acc: 41.2% (4120/10000)
[Test]  Epoch: 7	Loss: 0.033758	Acc: 41.9% (4192/10000)
[Test]  Epoch: 8	Loss: 0.034575	Acc: 41.4% (4138/10000)
[Test]  Epoch: 9	Loss: 0.033687	Acc: 42.1% (4213/10000)
[Test]  Epoch: 10	Loss: 0.033638	Acc: 42.5% (4254/10000)
[Test]  Epoch: 11	Loss: 0.033306	Acc: 43.4% (4341/10000)
[Test]  Epoch: 12	Loss: 0.033696	Acc: 42.3% (4234/10000)
[Test]  Epoch: 13	Loss: 0.033570	Acc: 42.9% (4289/10000)
[Test]  Epoch: 14	Loss: 0.033024	Acc: 43.1% (4312/10000)
[Test]  Epoch: 15	Loss: 0.033551	Acc: 42.6% (4257/10000)
[Test]  Epoch: 16	Loss: 0.033009	Acc: 43.0% (4297/10000)
[Test]  Epoch: 17	Loss: 0.033355	Acc: 43.0% (4303/10000)
[Test]  Epoch: 18	Loss: 0.033476	Acc: 43.2% (4320/10000)
[Test]  Epoch: 19	Loss: 0.033232	Acc: 43.0% (4302/10000)
[Test]  Epoch: 20	Loss: 0.033111	Acc: 43.4% (4342/10000)
[Test]  Epoch: 21	Loss: 0.033131	Acc: 43.3% (4333/10000)
[Test]  Epoch: 22	Loss: 0.032717	Acc: 43.8% (4379/10000)
[Test]  Epoch: 23	Loss: 0.032919	Acc: 43.3% (4334/10000)
[Test]  Epoch: 24	Loss: 0.033201	Acc: 43.5% (4349/10000)
[Test]  Epoch: 25	Loss: 0.032805	Acc: 44.3% (4426/10000)
[Test]  Epoch: 26	Loss: 0.033075	Acc: 43.5% (4346/10000)
[Test]  Epoch: 27	Loss: 0.032889	Acc: 44.0% (4400/10000)
[Test]  Epoch: 28	Loss: 0.032985	Acc: 43.5% (4347/10000)
[Test]  Epoch: 29	Loss: 0.033380	Acc: 43.3% (4330/10000)
[Test]  Epoch: 30	Loss: 0.032934	Acc: 44.1% (4414/10000)
[Test]  Epoch: 31	Loss: 0.032950	Acc: 43.9% (4388/10000)
[Test]  Epoch: 32	Loss: 0.032968	Acc: 43.6% (4359/10000)
[Test]  Epoch: 33	Loss: 0.033254	Acc: 43.4% (4340/10000)
[Test]  Epoch: 34	Loss: 0.032898	Acc: 43.9% (4385/10000)
[Test]  Epoch: 35	Loss: 0.033079	Acc: 43.4% (4343/10000)
[Test]  Epoch: 36	Loss: 0.033265	Acc: 43.8% (4382/10000)
[Test]  Epoch: 37	Loss: 0.032918	Acc: 43.9% (4389/10000)
[Test]  Epoch: 38	Loss: 0.032828	Acc: 43.9% (4388/10000)
[Test]  Epoch: 39	Loss: 0.033128	Acc: 43.7% (4368/10000)
[Test]  Epoch: 40	Loss: 0.032918	Acc: 44.2% (4420/10000)
[Test]  Epoch: 41	Loss: 0.032933	Acc: 43.7% (4369/10000)
[Test]  Epoch: 42	Loss: 0.033229	Acc: 43.7% (4373/10000)
[Test]  Epoch: 43	Loss: 0.032718	Acc: 44.0% (4398/10000)
[Test]  Epoch: 44	Loss: 0.032942	Acc: 43.8% (4376/10000)
[Test]  Epoch: 45	Loss: 0.033317	Acc: 43.4% (4343/10000)
[Test]  Epoch: 46	Loss: 0.033067	Acc: 43.5% (4345/10000)
[Test]  Epoch: 47	Loss: 0.032850	Acc: 44.1% (4410/10000)
[Test]  Epoch: 48	Loss: 0.032906	Acc: 43.8% (4380/10000)
[Test]  Epoch: 49	Loss: 0.033117	Acc: 43.6% (4364/10000)
[Test]  Epoch: 50	Loss: 0.033039	Acc: 44.0% (4401/10000)
[Test]  Epoch: 51	Loss: 0.033047	Acc: 44.0% (4396/10000)
[Test]  Epoch: 52	Loss: 0.033003	Acc: 43.8% (4384/10000)
[Test]  Epoch: 53	Loss: 0.033310	Acc: 43.2% (4325/10000)
[Test]  Epoch: 54	Loss: 0.033163	Acc: 43.9% (4388/10000)
[Test]  Epoch: 55	Loss: 0.032938	Acc: 43.9% (4389/10000)
[Test]  Epoch: 56	Loss: 0.033083	Acc: 43.9% (4385/10000)
[Test]  Epoch: 57	Loss: 0.032985	Acc: 43.9% (4389/10000)
[Test]  Epoch: 58	Loss: 0.032909	Acc: 44.0% (4404/10000)
[Test]  Epoch: 59	Loss: 0.032918	Acc: 43.8% (4381/10000)
[Test]  Epoch: 60	Loss: 0.033381	Acc: 43.3% (4329/10000)
[Test]  Epoch: 61	Loss: 0.032804	Acc: 44.1% (4406/10000)
[Test]  Epoch: 62	Loss: 0.032617	Acc: 44.3% (4434/10000)
[Test]  Epoch: 63	Loss: 0.032557	Acc: 44.3% (4429/10000)
[Test]  Epoch: 64	Loss: 0.032587	Acc: 44.2% (4425/10000)
[Test]  Epoch: 65	Loss: 0.032513	Acc: 44.6% (4456/10000)
[Test]  Epoch: 66	Loss: 0.032566	Acc: 44.8% (4478/10000)
[Test]  Epoch: 67	Loss: 0.032590	Acc: 44.5% (4454/10000)
[Test]  Epoch: 68	Loss: 0.032504	Acc: 44.7% (4469/10000)
[Test]  Epoch: 69	Loss: 0.032543	Acc: 44.6% (4457/10000)
[Test]  Epoch: 70	Loss: 0.032514	Acc: 44.4% (4443/10000)
[Test]  Epoch: 71	Loss: 0.032495	Acc: 44.6% (4464/10000)
[Test]  Epoch: 72	Loss: 0.032544	Acc: 44.5% (4454/10000)
[Test]  Epoch: 73	Loss: 0.032456	Acc: 44.7% (4474/10000)
[Test]  Epoch: 74	Loss: 0.032494	Acc: 44.8% (4477/10000)
[Test]  Epoch: 75	Loss: 0.032524	Acc: 44.5% (4453/10000)
[Test]  Epoch: 76	Loss: 0.032551	Acc: 44.5% (4450/10000)
[Test]  Epoch: 77	Loss: 0.032519	Acc: 44.7% (4473/10000)
[Test]  Epoch: 78	Loss: 0.032572	Acc: 44.5% (4446/10000)
[Test]  Epoch: 79	Loss: 0.032572	Acc: 44.5% (4446/10000)
[Test]  Epoch: 80	Loss: 0.032591	Acc: 44.5% (4450/10000)
[Test]  Epoch: 81	Loss: 0.032488	Acc: 44.4% (4443/10000)
[Test]  Epoch: 82	Loss: 0.032595	Acc: 44.3% (4434/10000)
[Test]  Epoch: 83	Loss: 0.032528	Acc: 44.6% (4461/10000)
[Test]  Epoch: 84	Loss: 0.032537	Acc: 44.5% (4454/10000)
[Test]  Epoch: 85	Loss: 0.032505	Acc: 44.8% (4476/10000)
[Test]  Epoch: 86	Loss: 0.032577	Acc: 44.8% (4475/10000)
[Test]  Epoch: 87	Loss: 0.032484	Acc: 44.7% (4468/10000)
[Test]  Epoch: 88	Loss: 0.032579	Acc: 44.4% (4437/10000)
[Test]  Epoch: 89	Loss: 0.032575	Acc: 44.6% (4460/10000)
[Test]  Epoch: 90	Loss: 0.032530	Acc: 44.6% (4461/10000)
[Test]  Epoch: 91	Loss: 0.032542	Acc: 44.5% (4452/10000)
[Test]  Epoch: 92	Loss: 0.032609	Acc: 44.4% (4439/10000)
[Test]  Epoch: 93	Loss: 0.032578	Acc: 44.4% (4439/10000)
[Test]  Epoch: 94	Loss: 0.032511	Acc: 44.5% (4448/10000)
[Test]  Epoch: 95	Loss: 0.032620	Acc: 44.2% (4425/10000)
[Test]  Epoch: 96	Loss: 0.032509	Acc: 44.5% (4451/10000)
[Test]  Epoch: 97	Loss: 0.032574	Acc: 44.4% (4438/10000)
[Test]  Epoch: 98	Loss: 0.032555	Acc: 44.5% (4454/10000)
[Test]  Epoch: 99	Loss: 0.032624	Acc: 44.4% (4440/10000)
[Test]  Epoch: 100	Loss: 0.032588	Acc: 44.5% (4454/10000)
===========finish==========
['2024-08-18', '16:43:01.709992', '100', 'test', '0.032588405644893646', '44.54', '44.78']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.062552	Acc: 10.1% (1007/10000)
[Test]  Epoch: 2	Loss: 0.054069	Acc: 19.8% (1977/10000)
[Test]  Epoch: 3	Loss: 0.048776	Acc: 24.8% (2475/10000)
[Test]  Epoch: 4	Loss: 0.046809	Acc: 30.1% (3005/10000)
[Test]  Epoch: 5	Loss: 0.043920	Acc: 30.8% (3083/10000)
[Test]  Epoch: 6	Loss: 0.041895	Acc: 33.2% (3318/10000)
[Test]  Epoch: 7	Loss: 0.040961	Acc: 35.2% (3525/10000)
[Test]  Epoch: 8	Loss: 0.041164	Acc: 33.9% (3393/10000)
[Test]  Epoch: 9	Loss: 0.038677	Acc: 35.8% (3583/10000)
[Test]  Epoch: 10	Loss: 0.039905	Acc: 35.0% (3498/10000)
[Test]  Epoch: 11	Loss: 0.038639	Acc: 36.5% (3650/10000)
[Test]  Epoch: 12	Loss: 0.038005	Acc: 37.0% (3702/10000)
[Test]  Epoch: 13	Loss: 0.038172	Acc: 37.0% (3699/10000)
[Test]  Epoch: 14	Loss: 0.037512	Acc: 37.6% (3761/10000)
[Test]  Epoch: 15	Loss: 0.037228	Acc: 38.0% (3804/10000)
[Test]  Epoch: 16	Loss: 0.037106	Acc: 38.5% (3847/10000)
[Test]  Epoch: 17	Loss: 0.036751	Acc: 38.6% (3860/10000)
[Test]  Epoch: 18	Loss: 0.037689	Acc: 38.0% (3796/10000)
[Test]  Epoch: 19	Loss: 0.037098	Acc: 37.8% (3782/10000)
[Test]  Epoch: 20	Loss: 0.036688	Acc: 38.6% (3865/10000)
[Test]  Epoch: 21	Loss: 0.036422	Acc: 38.6% (3865/10000)
[Test]  Epoch: 22	Loss: 0.035787	Acc: 39.8% (3979/10000)
[Test]  Epoch: 23	Loss: 0.035811	Acc: 39.7% (3970/10000)
[Test]  Epoch: 24	Loss: 0.036253	Acc: 39.6% (3960/10000)
[Test]  Epoch: 25	Loss: 0.036068	Acc: 39.2% (3923/10000)
[Test]  Epoch: 26	Loss: 0.035974	Acc: 39.6% (3960/10000)
[Test]  Epoch: 27	Loss: 0.036068	Acc: 39.4% (3944/10000)
[Test]  Epoch: 28	Loss: 0.036498	Acc: 38.8% (3882/10000)
[Test]  Epoch: 29	Loss: 0.035971	Acc: 39.3% (3927/10000)
[Test]  Epoch: 30	Loss: 0.036079	Acc: 39.3% (3930/10000)
[Test]  Epoch: 31	Loss: 0.035747	Acc: 39.1% (3913/10000)
[Test]  Epoch: 32	Loss: 0.036010	Acc: 39.4% (3943/10000)
[Test]  Epoch: 33	Loss: 0.035934	Acc: 39.8% (3977/10000)
[Test]  Epoch: 34	Loss: 0.035556	Acc: 40.0% (4003/10000)
[Test]  Epoch: 35	Loss: 0.035466	Acc: 39.7% (3972/10000)
[Test]  Epoch: 36	Loss: 0.035941	Acc: 39.1% (3914/10000)
[Test]  Epoch: 37	Loss: 0.035387	Acc: 40.4% (4043/10000)
[Test]  Epoch: 38	Loss: 0.035431	Acc: 40.1% (4013/10000)
[Test]  Epoch: 39	Loss: 0.035293	Acc: 40.5% (4045/10000)
[Test]  Epoch: 40	Loss: 0.035595	Acc: 40.2% (4024/10000)
[Test]  Epoch: 41	Loss: 0.035332	Acc: 40.3% (4027/10000)
[Test]  Epoch: 42	Loss: 0.035486	Acc: 40.4% (4042/10000)
[Test]  Epoch: 43	Loss: 0.035044	Acc: 40.8% (4082/10000)
[Test]  Epoch: 44	Loss: 0.035144	Acc: 41.0% (4104/10000)
[Test]  Epoch: 45	Loss: 0.035427	Acc: 40.8% (4079/10000)
[Test]  Epoch: 46	Loss: 0.035585	Acc: 40.1% (4009/10000)
[Test]  Epoch: 47	Loss: 0.034933	Acc: 40.9% (4091/10000)
[Test]  Epoch: 48	Loss: 0.034852	Acc: 40.8% (4080/10000)
[Test]  Epoch: 49	Loss: 0.035348	Acc: 39.8% (3975/10000)
[Test]  Epoch: 50	Loss: 0.034926	Acc: 40.8% (4081/10000)
[Test]  Epoch: 51	Loss: 0.035075	Acc: 40.7% (4070/10000)
[Test]  Epoch: 52	Loss: 0.034937	Acc: 40.5% (4055/10000)
[Test]  Epoch: 53	Loss: 0.035583	Acc: 40.1% (4014/10000)
[Test]  Epoch: 54	Loss: 0.035682	Acc: 40.0% (4001/10000)
[Test]  Epoch: 55	Loss: 0.035253	Acc: 40.4% (4044/10000)
[Test]  Epoch: 56	Loss: 0.034879	Acc: 41.1% (4108/10000)
[Test]  Epoch: 57	Loss: 0.034882	Acc: 41.0% (4103/10000)
[Test]  Epoch: 58	Loss: 0.035503	Acc: 39.6% (3965/10000)
[Test]  Epoch: 59	Loss: 0.035106	Acc: 40.7% (4072/10000)
[Test]  Epoch: 60	Loss: 0.035237	Acc: 40.1% (4014/10000)
[Test]  Epoch: 61	Loss: 0.034563	Acc: 41.3% (4132/10000)
[Test]  Epoch: 62	Loss: 0.034416	Acc: 41.7% (4169/10000)
[Test]  Epoch: 63	Loss: 0.034329	Acc: 41.9% (4193/10000)
[Test]  Epoch: 64	Loss: 0.034354	Acc: 41.8% (4182/10000)
[Test]  Epoch: 65	Loss: 0.034267	Acc: 42.0% (4198/10000)
[Test]  Epoch: 66	Loss: 0.034343	Acc: 41.9% (4192/10000)
[Test]  Epoch: 67	Loss: 0.034362	Acc: 41.7% (4169/10000)
[Test]  Epoch: 68	Loss: 0.034273	Acc: 41.9% (4187/10000)
[Test]  Epoch: 69	Loss: 0.034283	Acc: 41.8% (4175/10000)
[Test]  Epoch: 70	Loss: 0.034282	Acc: 41.9% (4186/10000)
[Test]  Epoch: 71	Loss: 0.034255	Acc: 42.0% (4202/10000)
[Test]  Epoch: 72	Loss: 0.034304	Acc: 41.7% (4170/10000)
[Test]  Epoch: 73	Loss: 0.034259	Acc: 42.0% (4197/10000)
[Test]  Epoch: 74	Loss: 0.034248	Acc: 41.9% (4194/10000)
[Test]  Epoch: 75	Loss: 0.034245	Acc: 42.0% (4205/10000)
[Test]  Epoch: 76	Loss: 0.034302	Acc: 42.1% (4209/10000)
[Test]  Epoch: 77	Loss: 0.034264	Acc: 42.1% (4206/10000)
[Test]  Epoch: 78	Loss: 0.034306	Acc: 41.7% (4173/10000)
[Test]  Epoch: 79	Loss: 0.034258	Acc: 41.8% (4180/10000)
[Test]  Epoch: 80	Loss: 0.034291	Acc: 41.8% (4181/10000)
[Test]  Epoch: 81	Loss: 0.034189	Acc: 42.1% (4209/10000)
[Test]  Epoch: 82	Loss: 0.034273	Acc: 41.9% (4190/10000)
[Test]  Epoch: 83	Loss: 0.034294	Acc: 41.8% (4183/10000)
[Test]  Epoch: 84	Loss: 0.034229	Acc: 42.0% (4197/10000)
[Test]  Epoch: 85	Loss: 0.034223	Acc: 42.1% (4215/10000)
[Test]  Epoch: 86	Loss: 0.034289	Acc: 42.0% (4205/10000)
[Test]  Epoch: 87	Loss: 0.034232	Acc: 41.9% (4190/10000)
[Test]  Epoch: 88	Loss: 0.034290	Acc: 41.8% (4182/10000)
[Test]  Epoch: 89	Loss: 0.034310	Acc: 41.9% (4192/10000)
[Test]  Epoch: 90	Loss: 0.034234	Acc: 42.0% (4197/10000)
[Test]  Epoch: 91	Loss: 0.034283	Acc: 42.0% (4202/10000)
[Test]  Epoch: 92	Loss: 0.034299	Acc: 42.1% (4206/10000)
[Test]  Epoch: 93	Loss: 0.034294	Acc: 42.0% (4199/10000)
[Test]  Epoch: 94	Loss: 0.034251	Acc: 42.2% (4216/10000)
[Test]  Epoch: 95	Loss: 0.034327	Acc: 42.0% (4203/10000)
[Test]  Epoch: 96	Loss: 0.034234	Acc: 42.2% (4220/10000)
[Test]  Epoch: 97	Loss: 0.034309	Acc: 42.0% (4197/10000)
[Test]  Epoch: 98	Loss: 0.034276	Acc: 42.2% (4218/10000)
[Test]  Epoch: 99	Loss: 0.034284	Acc: 42.0% (4201/10000)
[Test]  Epoch: 100	Loss: 0.034269	Acc: 42.2% (4216/10000)
===========finish==========
['2024-08-18', '16:45:13.068322', '100', 'test', '0.03426933594942093', '42.16', '42.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.071605	Acc: 5.8% (583/10000)
[Test]  Epoch: 2	Loss: 0.062447	Acc: 9.8% (981/10000)
[Test]  Epoch: 3	Loss: 0.054578	Acc: 15.6% (1557/10000)
[Test]  Epoch: 4	Loss: 0.053673	Acc: 16.3% (1627/10000)
[Test]  Epoch: 5	Loss: 0.049484	Acc: 21.8% (2184/10000)
[Test]  Epoch: 6	Loss: 0.048181	Acc: 22.3% (2228/10000)
[Test]  Epoch: 7	Loss: 0.048311	Acc: 26.5% (2650/10000)
[Test]  Epoch: 8	Loss: 0.074847	Acc: 15.4% (1540/10000)
[Test]  Epoch: 9	Loss: 0.044317	Acc: 28.6% (2862/10000)
[Test]  Epoch: 10	Loss: 0.042613	Acc: 29.7% (2971/10000)
[Test]  Epoch: 11	Loss: 0.042102	Acc: 30.8% (3081/10000)
[Test]  Epoch: 12	Loss: 0.041396	Acc: 32.0% (3203/10000)
[Test]  Epoch: 13	Loss: 0.042974	Acc: 30.0% (2997/10000)
[Test]  Epoch: 14	Loss: 0.040625	Acc: 33.5% (3345/10000)
[Test]  Epoch: 15	Loss: 0.040273	Acc: 33.6% (3359/10000)
[Test]  Epoch: 16	Loss: 0.039766	Acc: 33.8% (3378/10000)
[Test]  Epoch: 17	Loss: 0.040198	Acc: 33.7% (3367/10000)
[Test]  Epoch: 18	Loss: 0.039710	Acc: 33.7% (3370/10000)
[Test]  Epoch: 19	Loss: 0.039847	Acc: 33.2% (3323/10000)
[Test]  Epoch: 20	Loss: 0.039286	Acc: 34.9% (3492/10000)
[Test]  Epoch: 21	Loss: 0.038539	Acc: 35.7% (3571/10000)
[Test]  Epoch: 22	Loss: 0.039350	Acc: 34.9% (3489/10000)
[Test]  Epoch: 23	Loss: 0.038653	Acc: 35.0% (3496/10000)
[Test]  Epoch: 24	Loss: 0.038987	Acc: 35.0% (3497/10000)
[Test]  Epoch: 25	Loss: 0.038319	Acc: 35.6% (3556/10000)
[Test]  Epoch: 26	Loss: 0.038898	Acc: 34.3% (3433/10000)
[Test]  Epoch: 27	Loss: 0.037906	Acc: 36.0% (3598/10000)
[Test]  Epoch: 28	Loss: 0.037920	Acc: 35.9% (3589/10000)
[Test]  Epoch: 29	Loss: 0.038626	Acc: 35.0% (3502/10000)
[Test]  Epoch: 30	Loss: 0.038524	Acc: 35.7% (3571/10000)
[Test]  Epoch: 31	Loss: 0.038305	Acc: 35.5% (3555/10000)
[Test]  Epoch: 32	Loss: 0.038263	Acc: 35.8% (3584/10000)
[Test]  Epoch: 33	Loss: 0.038023	Acc: 36.5% (3652/10000)
[Test]  Epoch: 34	Loss: 0.038070	Acc: 35.8% (3579/10000)
[Test]  Epoch: 35	Loss: 0.037468	Acc: 37.0% (3695/10000)
[Test]  Epoch: 36	Loss: 0.038020	Acc: 35.8% (3578/10000)
[Test]  Epoch: 37	Loss: 0.037547	Acc: 37.4% (3739/10000)
[Test]  Epoch: 38	Loss: 0.037404	Acc: 37.0% (3702/10000)
[Test]  Epoch: 39	Loss: 0.037368	Acc: 37.5% (3745/10000)
[Test]  Epoch: 40	Loss: 0.037732	Acc: 37.0% (3695/10000)
[Test]  Epoch: 41	Loss: 0.037203	Acc: 37.4% (3740/10000)
[Test]  Epoch: 42	Loss: 0.037779	Acc: 36.2% (3621/10000)
[Test]  Epoch: 43	Loss: 0.037514	Acc: 36.7% (3668/10000)
[Test]  Epoch: 44	Loss: 0.036972	Acc: 37.4% (3738/10000)
[Test]  Epoch: 45	Loss: 0.037213	Acc: 37.5% (3750/10000)
[Test]  Epoch: 46	Loss: 0.037269	Acc: 37.2% (3718/10000)
[Test]  Epoch: 47	Loss: 0.037065	Acc: 37.5% (3747/10000)
[Test]  Epoch: 48	Loss: 0.037027	Acc: 37.9% (3792/10000)
[Test]  Epoch: 49	Loss: 0.037136	Acc: 37.0% (3701/10000)
[Test]  Epoch: 50	Loss: 0.036956	Acc: 37.5% (3755/10000)
[Test]  Epoch: 51	Loss: 0.037076	Acc: 36.9% (3691/10000)
[Test]  Epoch: 52	Loss: 0.037151	Acc: 36.9% (3689/10000)
[Test]  Epoch: 53	Loss: 0.037241	Acc: 37.1% (3707/10000)
[Test]  Epoch: 54	Loss: 0.036891	Acc: 37.1% (3707/10000)
[Test]  Epoch: 55	Loss: 0.036922	Acc: 37.7% (3769/10000)
[Test]  Epoch: 56	Loss: 0.037211	Acc: 37.4% (3744/10000)
[Test]  Epoch: 57	Loss: 0.036750	Acc: 38.1% (3814/10000)
[Test]  Epoch: 58	Loss: 0.037149	Acc: 37.6% (3756/10000)
[Test]  Epoch: 59	Loss: 0.036840	Acc: 37.7% (3771/10000)
[Test]  Epoch: 60	Loss: 0.036697	Acc: 38.0% (3798/10000)
[Test]  Epoch: 61	Loss: 0.036382	Acc: 38.2% (3825/10000)
[Test]  Epoch: 62	Loss: 0.036267	Acc: 38.6% (3862/10000)
[Test]  Epoch: 63	Loss: 0.036234	Acc: 38.6% (3863/10000)
[Test]  Epoch: 64	Loss: 0.036245	Acc: 38.7% (3873/10000)
[Test]  Epoch: 65	Loss: 0.036163	Acc: 38.9% (3893/10000)
[Test]  Epoch: 66	Loss: 0.036201	Acc: 38.9% (3887/10000)
[Test]  Epoch: 67	Loss: 0.036209	Acc: 38.9% (3890/10000)
[Test]  Epoch: 68	Loss: 0.036121	Acc: 38.9% (3893/10000)
[Test]  Epoch: 69	Loss: 0.036103	Acc: 39.2% (3919/10000)
[Test]  Epoch: 70	Loss: 0.036170	Acc: 38.9% (3886/10000)
[Test]  Epoch: 71	Loss: 0.036102	Acc: 39.0% (3895/10000)
[Test]  Epoch: 72	Loss: 0.036154	Acc: 39.1% (3911/10000)
[Test]  Epoch: 73	Loss: 0.036099	Acc: 39.2% (3925/10000)
[Test]  Epoch: 74	Loss: 0.036120	Acc: 39.2% (3920/10000)
[Test]  Epoch: 75	Loss: 0.036105	Acc: 39.0% (3899/10000)
[Test]  Epoch: 76	Loss: 0.036154	Acc: 39.1% (3908/10000)
[Test]  Epoch: 77	Loss: 0.036109	Acc: 39.4% (3939/10000)
[Test]  Epoch: 78	Loss: 0.036219	Acc: 39.0% (3899/10000)
[Test]  Epoch: 79	Loss: 0.036150	Acc: 39.3% (3933/10000)
[Test]  Epoch: 80	Loss: 0.036203	Acc: 39.1% (3912/10000)
[Test]  Epoch: 81	Loss: 0.036060	Acc: 39.3% (3928/10000)
[Test]  Epoch: 82	Loss: 0.036113	Acc: 39.1% (3914/10000)
[Test]  Epoch: 83	Loss: 0.036120	Acc: 39.0% (3899/10000)
[Test]  Epoch: 84	Loss: 0.036100	Acc: 39.1% (3908/10000)
[Test]  Epoch: 85	Loss: 0.036112	Acc: 39.4% (3939/10000)
[Test]  Epoch: 86	Loss: 0.036182	Acc: 39.1% (3914/10000)
[Test]  Epoch: 87	Loss: 0.036130	Acc: 39.2% (3917/10000)
[Test]  Epoch: 88	Loss: 0.036160	Acc: 39.1% (3912/10000)
[Test]  Epoch: 89	Loss: 0.036151	Acc: 39.0% (3905/10000)
[Test]  Epoch: 90	Loss: 0.036116	Acc: 39.0% (3902/10000)
[Test]  Epoch: 91	Loss: 0.036153	Acc: 39.1% (3915/10000)
[Test]  Epoch: 92	Loss: 0.036147	Acc: 39.1% (3906/10000)
[Test]  Epoch: 93	Loss: 0.036181	Acc: 39.1% (3908/10000)
[Test]  Epoch: 94	Loss: 0.036122	Acc: 39.0% (3903/10000)
[Test]  Epoch: 95	Loss: 0.036233	Acc: 39.0% (3905/10000)
[Test]  Epoch: 96	Loss: 0.036151	Acc: 39.2% (3918/10000)
[Test]  Epoch: 97	Loss: 0.036152	Acc: 39.3% (3929/10000)
[Test]  Epoch: 98	Loss: 0.036117	Acc: 39.3% (3926/10000)
[Test]  Epoch: 99	Loss: 0.036191	Acc: 39.2% (3922/10000)
[Test]  Epoch: 100	Loss: 0.036143	Acc: 39.2% (3918/10000)
===========finish==========
['2024-08-18', '16:47:24.553518', '100', 'test', '0.036142789149284364', '39.18', '39.39']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.8.conv.2.weight', '_features.11.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.7.conv.2.weight', '_features.9.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.099983	Acc: 5.3% (530/10000)
[Test]  Epoch: 2	Loss: 0.060425	Acc: 11.7% (1166/10000)
[Test]  Epoch: 3	Loss: 0.057091	Acc: 15.1% (1510/10000)
[Test]  Epoch: 4	Loss: 0.054249	Acc: 18.6% (1858/10000)
[Test]  Epoch: 5	Loss: 0.051181	Acc: 21.2% (2124/10000)
[Test]  Epoch: 6	Loss: 0.051360	Acc: 19.8% (1984/10000)
[Test]  Epoch: 7	Loss: 0.047720	Acc: 24.1% (2406/10000)
[Test]  Epoch: 8	Loss: 0.048470	Acc: 23.9% (2394/10000)
[Test]  Epoch: 9	Loss: 0.044726	Acc: 27.0% (2704/10000)
[Test]  Epoch: 10	Loss: 0.045200	Acc: 28.4% (2840/10000)
[Test]  Epoch: 11	Loss: 0.042916	Acc: 29.4% (2942/10000)
[Test]  Epoch: 12	Loss: 0.043056	Acc: 29.5% (2946/10000)
[Test]  Epoch: 13	Loss: 0.043178	Acc: 29.3% (2927/10000)
[Test]  Epoch: 14	Loss: 0.042620	Acc: 29.8% (2983/10000)
[Test]  Epoch: 15	Loss: 0.042244	Acc: 29.8% (2978/10000)
[Test]  Epoch: 16	Loss: 0.041875	Acc: 30.2% (3020/10000)
[Test]  Epoch: 17	Loss: 0.041966	Acc: 30.9% (3092/10000)
[Test]  Epoch: 18	Loss: 0.041355	Acc: 31.8% (3182/10000)
[Test]  Epoch: 19	Loss: 0.040943	Acc: 31.8% (3181/10000)
[Test]  Epoch: 20	Loss: 0.040620	Acc: 32.2% (3218/10000)
[Test]  Epoch: 21	Loss: 0.040893	Acc: 31.7% (3171/10000)
[Test]  Epoch: 22	Loss: 0.040373	Acc: 32.9% (3294/10000)
[Test]  Epoch: 23	Loss: 0.039988	Acc: 33.2% (3320/10000)
[Test]  Epoch: 24	Loss: 0.040914	Acc: 32.2% (3223/10000)
[Test]  Epoch: 25	Loss: 0.040147	Acc: 32.7% (3268/10000)
[Test]  Epoch: 26	Loss: 0.039721	Acc: 33.4% (3343/10000)
[Test]  Epoch: 27	Loss: 0.040163	Acc: 32.4% (3240/10000)
[Test]  Epoch: 28	Loss: 0.040457	Acc: 32.8% (3280/10000)
[Test]  Epoch: 29	Loss: 0.040061	Acc: 32.8% (3281/10000)
[Test]  Epoch: 30	Loss: 0.039422	Acc: 34.3% (3434/10000)
[Test]  Epoch: 31	Loss: 0.039752	Acc: 33.0% (3297/10000)
[Test]  Epoch: 32	Loss: 0.039912	Acc: 33.0% (3303/10000)
[Test]  Epoch: 33	Loss: 0.039343	Acc: 34.3% (3427/10000)
[Test]  Epoch: 34	Loss: 0.039123	Acc: 34.3% (3434/10000)
[Test]  Epoch: 35	Loss: 0.039558	Acc: 33.2% (3317/10000)
[Test]  Epoch: 36	Loss: 0.039294	Acc: 34.0% (3403/10000)
[Test]  Epoch: 37	Loss: 0.038753	Acc: 34.6% (3460/10000)
[Test]  Epoch: 38	Loss: 0.038732	Acc: 34.6% (3456/10000)
[Test]  Epoch: 39	Loss: 0.038647	Acc: 35.1% (3511/10000)
[Test]  Epoch: 40	Loss: 0.038980	Acc: 34.6% (3458/10000)
[Test]  Epoch: 41	Loss: 0.038667	Acc: 35.0% (3496/10000)
[Test]  Epoch: 42	Loss: 0.038742	Acc: 35.2% (3517/10000)
[Test]  Epoch: 43	Loss: 0.039035	Acc: 34.3% (3430/10000)
[Test]  Epoch: 44	Loss: 0.038242	Acc: 35.8% (3582/10000)
[Test]  Epoch: 45	Loss: 0.038578	Acc: 35.2% (3522/10000)
[Test]  Epoch: 46	Loss: 0.038516	Acc: 35.2% (3524/10000)
[Test]  Epoch: 47	Loss: 0.038768	Acc: 35.2% (3523/10000)
[Test]  Epoch: 48	Loss: 0.038425	Acc: 35.2% (3525/10000)
[Test]  Epoch: 49	Loss: 0.038325	Acc: 35.4% (3538/10000)
[Test]  Epoch: 50	Loss: 0.038630	Acc: 35.0% (3495/10000)
[Test]  Epoch: 51	Loss: 0.038587	Acc: 34.6% (3464/10000)
[Test]  Epoch: 52	Loss: 0.038234	Acc: 35.4% (3542/10000)
[Test]  Epoch: 53	Loss: 0.038817	Acc: 35.1% (3510/10000)
[Test]  Epoch: 54	Loss: 0.038308	Acc: 35.5% (3549/10000)
[Test]  Epoch: 55	Loss: 0.038172	Acc: 35.6% (3564/10000)
[Test]  Epoch: 56	Loss: 0.038278	Acc: 35.4% (3537/10000)
[Test]  Epoch: 57	Loss: 0.038301	Acc: 35.7% (3572/10000)
[Test]  Epoch: 58	Loss: 0.038608	Acc: 35.2% (3521/10000)
[Test]  Epoch: 59	Loss: 0.038593	Acc: 35.5% (3549/10000)
[Test]  Epoch: 60	Loss: 0.038361	Acc: 35.1% (3513/10000)
[Test]  Epoch: 61	Loss: 0.037882	Acc: 36.3% (3633/10000)
[Test]  Epoch: 62	Loss: 0.037790	Acc: 36.6% (3661/10000)
[Test]  Epoch: 63	Loss: 0.037722	Acc: 36.7% (3671/10000)
[Test]  Epoch: 64	Loss: 0.037750	Acc: 36.6% (3662/10000)
[Test]  Epoch: 65	Loss: 0.037679	Acc: 36.6% (3665/10000)
[Test]  Epoch: 66	Loss: 0.037705	Acc: 36.7% (3668/10000)
[Test]  Epoch: 67	Loss: 0.037691	Acc: 36.7% (3669/10000)
[Test]  Epoch: 68	Loss: 0.037658	Acc: 36.7% (3669/10000)
[Test]  Epoch: 69	Loss: 0.037674	Acc: 36.8% (3678/10000)
[Test]  Epoch: 70	Loss: 0.037657	Acc: 36.9% (3690/10000)
[Test]  Epoch: 71	Loss: 0.037614	Acc: 37.0% (3700/10000)
[Test]  Epoch: 72	Loss: 0.037662	Acc: 37.0% (3699/10000)
[Test]  Epoch: 73	Loss: 0.037585	Acc: 36.8% (3676/10000)
[Test]  Epoch: 74	Loss: 0.037598	Acc: 37.0% (3698/10000)
[Test]  Epoch: 75	Loss: 0.037610	Acc: 37.2% (3725/10000)
[Test]  Epoch: 76	Loss: 0.037613	Acc: 36.9% (3687/10000)
[Test]  Epoch: 77	Loss: 0.037613	Acc: 37.0% (3696/10000)
[Test]  Epoch: 78	Loss: 0.037633	Acc: 37.0% (3702/10000)
[Test]  Epoch: 79	Loss: 0.037604	Acc: 36.8% (3677/10000)
[Test]  Epoch: 80	Loss: 0.037651	Acc: 36.8% (3679/10000)
[Test]  Epoch: 81	Loss: 0.037567	Acc: 37.0% (3701/10000)
[Test]  Epoch: 82	Loss: 0.037586	Acc: 36.8% (3676/10000)
[Test]  Epoch: 83	Loss: 0.037563	Acc: 37.1% (3709/10000)
[Test]  Epoch: 84	Loss: 0.037537	Acc: 37.0% (3696/10000)
[Test]  Epoch: 85	Loss: 0.037511	Acc: 36.8% (3684/10000)
[Test]  Epoch: 86	Loss: 0.037623	Acc: 36.9% (3685/10000)
[Test]  Epoch: 87	Loss: 0.037534	Acc: 36.9% (3686/10000)
[Test]  Epoch: 88	Loss: 0.037581	Acc: 36.9% (3685/10000)
[Test]  Epoch: 89	Loss: 0.037606	Acc: 36.9% (3689/10000)
[Test]  Epoch: 90	Loss: 0.037546	Acc: 37.1% (3710/10000)
[Test]  Epoch: 91	Loss: 0.037527	Acc: 36.9% (3694/10000)
[Test]  Epoch: 92	Loss: 0.037588	Acc: 36.8% (3684/10000)
[Test]  Epoch: 93	Loss: 0.037585	Acc: 37.0% (3702/10000)
[Test]  Epoch: 94	Loss: 0.037523	Acc: 36.9% (3691/10000)
[Test]  Epoch: 95	Loss: 0.037593	Acc: 36.9% (3692/10000)
[Test]  Epoch: 96	Loss: 0.037536	Acc: 36.9% (3691/10000)
[Test]  Epoch: 97	Loss: 0.037572	Acc: 36.9% (3691/10000)
[Test]  Epoch: 98	Loss: 0.037541	Acc: 37.2% (3719/10000)
[Test]  Epoch: 99	Loss: 0.037618	Acc: 36.9% (3690/10000)
[Test]  Epoch: 100	Loss: 0.037587	Acc: 36.8% (3684/10000)
===========finish==========
['2024-08-18', '16:49:36.444212', '100', 'test', '0.03758734518289566', '36.84', '37.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.8.conv.2.weight', '_features.11.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.7.conv.2.weight', '_features.9.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.10.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.13.conv.2.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.080842	Acc: 2.3% (234/10000)
[Test]  Epoch: 2	Loss: 0.074226	Acc: 2.6% (263/10000)
[Test]  Epoch: 3	Loss: 0.068524	Acc: 4.7% (466/10000)
[Test]  Epoch: 4	Loss: 0.065295	Acc: 7.3% (726/10000)
[Test]  Epoch: 5	Loss: 0.063495	Acc: 8.2% (818/10000)
[Test]  Epoch: 6	Loss: 0.062337	Acc: 8.5% (848/10000)
[Test]  Epoch: 7	Loss: 0.060958	Acc: 9.6% (961/10000)
[Test]  Epoch: 8	Loss: 0.063108	Acc: 8.5% (850/10000)
[Test]  Epoch: 9	Loss: 0.060077	Acc: 11.0% (1097/10000)
[Test]  Epoch: 10	Loss: 0.059945	Acc: 11.0% (1097/10000)
[Test]  Epoch: 11	Loss: 0.059379	Acc: 11.4% (1145/10000)
[Test]  Epoch: 12	Loss: 0.061076	Acc: 10.8% (1084/10000)
[Test]  Epoch: 13	Loss: 0.060438	Acc: 11.1% (1114/10000)
[Test]  Epoch: 14	Loss: 0.059001	Acc: 12.1% (1214/10000)
[Test]  Epoch: 15	Loss: 0.058464	Acc: 12.9% (1287/10000)
[Test]  Epoch: 16	Loss: 0.058544	Acc: 13.7% (1369/10000)
[Test]  Epoch: 17	Loss: 0.057914	Acc: 13.8% (1382/10000)
[Test]  Epoch: 18	Loss: 0.057495	Acc: 14.0% (1404/10000)
[Test]  Epoch: 19	Loss: 0.060706	Acc: 11.6% (1158/10000)
[Test]  Epoch: 20	Loss: 0.057130	Acc: 14.8% (1479/10000)
[Test]  Epoch: 21	Loss: 0.057062	Acc: 14.4% (1439/10000)
[Test]  Epoch: 22	Loss: 0.056782	Acc: 15.1% (1511/10000)
[Test]  Epoch: 23	Loss: 0.056727	Acc: 16.0% (1601/10000)
[Test]  Epoch: 24	Loss: 0.057379	Acc: 14.7% (1465/10000)
[Test]  Epoch: 25	Loss: 0.056435	Acc: 15.2% (1518/10000)
[Test]  Epoch: 26	Loss: 0.055784	Acc: 16.8% (1677/10000)
[Test]  Epoch: 27	Loss: 0.054750	Acc: 16.8% (1680/10000)
[Test]  Epoch: 28	Loss: 0.054954	Acc: 17.1% (1707/10000)
[Test]  Epoch: 29	Loss: 0.055144	Acc: 16.8% (1684/10000)
[Test]  Epoch: 30	Loss: 0.055417	Acc: 15.9% (1590/10000)
[Test]  Epoch: 31	Loss: 0.054779	Acc: 16.9% (1693/10000)
[Test]  Epoch: 32	Loss: 0.056005	Acc: 16.3% (1630/10000)
[Test]  Epoch: 33	Loss: 0.056020	Acc: 16.1% (1613/10000)
[Test]  Epoch: 34	Loss: 0.054705	Acc: 17.3% (1726/10000)
[Test]  Epoch: 35	Loss: 0.054125	Acc: 17.1% (1708/10000)
[Test]  Epoch: 36	Loss: 0.054169	Acc: 17.5% (1752/10000)
[Test]  Epoch: 37	Loss: 0.054287	Acc: 16.9% (1695/10000)
[Test]  Epoch: 38	Loss: 0.053826	Acc: 17.7% (1767/10000)
[Test]  Epoch: 39	Loss: 0.054145	Acc: 17.1% (1705/10000)
[Test]  Epoch: 40	Loss: 0.053883	Acc: 17.7% (1766/10000)
[Test]  Epoch: 41	Loss: 0.053954	Acc: 17.7% (1767/10000)
[Test]  Epoch: 42	Loss: 0.053859	Acc: 17.3% (1734/10000)
[Test]  Epoch: 43	Loss: 0.053391	Acc: 18.1% (1815/10000)
[Test]  Epoch: 44	Loss: 0.053291	Acc: 18.8% (1875/10000)
[Test]  Epoch: 45	Loss: 0.053343	Acc: 18.4% (1843/10000)
[Test]  Epoch: 46	Loss: 0.053977	Acc: 18.3% (1826/10000)
[Test]  Epoch: 47	Loss: 0.053093	Acc: 18.7% (1866/10000)
[Test]  Epoch: 48	Loss: 0.053280	Acc: 17.7% (1772/10000)
[Test]  Epoch: 49	Loss: 0.053516	Acc: 17.8% (1777/10000)
[Test]  Epoch: 50	Loss: 0.053553	Acc: 17.8% (1775/10000)
[Test]  Epoch: 51	Loss: 0.054297	Acc: 17.3% (1730/10000)
[Test]  Epoch: 52	Loss: 0.053584	Acc: 17.7% (1772/10000)
[Test]  Epoch: 53	Loss: 0.053130	Acc: 18.1% (1810/10000)
[Test]  Epoch: 54	Loss: 0.053395	Acc: 18.1% (1809/10000)
[Test]  Epoch: 55	Loss: 0.053136	Acc: 18.2% (1823/10000)
[Test]  Epoch: 56	Loss: 0.053134	Acc: 18.5% (1849/10000)
[Test]  Epoch: 57	Loss: 0.052688	Acc: 18.7% (1872/10000)
[Test]  Epoch: 58	Loss: 0.053164	Acc: 18.2% (1822/10000)
[Test]  Epoch: 59	Loss: 0.053239	Acc: 18.4% (1838/10000)
[Test]  Epoch: 60	Loss: 0.053158	Acc: 18.6% (1855/10000)
[Test]  Epoch: 61	Loss: 0.052439	Acc: 19.0% (1903/10000)
[Test]  Epoch: 62	Loss: 0.052241	Acc: 19.3% (1927/10000)
[Test]  Epoch: 63	Loss: 0.052249	Acc: 19.3% (1931/10000)
[Test]  Epoch: 64	Loss: 0.052199	Acc: 19.2% (1922/10000)
[Test]  Epoch: 65	Loss: 0.052099	Acc: 19.2% (1920/10000)
[Test]  Epoch: 66	Loss: 0.052099	Acc: 19.2% (1919/10000)
[Test]  Epoch: 67	Loss: 0.052075	Acc: 19.2% (1921/10000)
[Test]  Epoch: 68	Loss: 0.052080	Acc: 19.5% (1950/10000)
[Test]  Epoch: 69	Loss: 0.052042	Acc: 19.5% (1950/10000)
[Test]  Epoch: 70	Loss: 0.052152	Acc: 19.5% (1950/10000)
[Test]  Epoch: 71	Loss: 0.052069	Acc: 19.6% (1964/10000)
[Test]  Epoch: 72	Loss: 0.052111	Acc: 19.4% (1937/10000)
[Test]  Epoch: 73	Loss: 0.052098	Acc: 19.4% (1938/10000)
[Test]  Epoch: 74	Loss: 0.051990	Acc: 19.4% (1945/10000)
[Test]  Epoch: 75	Loss: 0.052001	Acc: 19.4% (1944/10000)
[Test]  Epoch: 76	Loss: 0.052024	Acc: 19.5% (1950/10000)
[Test]  Epoch: 77	Loss: 0.051985	Acc: 19.6% (1956/10000)
[Test]  Epoch: 78	Loss: 0.052028	Acc: 19.6% (1956/10000)
[Test]  Epoch: 79	Loss: 0.052080	Acc: 19.3% (1934/10000)
[Test]  Epoch: 80	Loss: 0.052079	Acc: 19.4% (1945/10000)
[Test]  Epoch: 81	Loss: 0.051944	Acc: 19.6% (1962/10000)
[Test]  Epoch: 82	Loss: 0.051965	Acc: 19.5% (1949/10000)
[Test]  Epoch: 83	Loss: 0.052006	Acc: 19.5% (1954/10000)
[Test]  Epoch: 84	Loss: 0.052029	Acc: 19.4% (1941/10000)
[Test]  Epoch: 85	Loss: 0.052044	Acc: 19.4% (1942/10000)
[Test]  Epoch: 86	Loss: 0.052108	Acc: 19.5% (1951/10000)
[Test]  Epoch: 87	Loss: 0.052077	Acc: 19.6% (1955/10000)
[Test]  Epoch: 88	Loss: 0.051997	Acc: 19.5% (1954/10000)
[Test]  Epoch: 89	Loss: 0.052029	Acc: 19.4% (1944/10000)
[Test]  Epoch: 90	Loss: 0.052011	Acc: 19.5% (1954/10000)
[Test]  Epoch: 91	Loss: 0.052062	Acc: 19.4% (1942/10000)
[Test]  Epoch: 92	Loss: 0.051950	Acc: 19.5% (1947/10000)
[Test]  Epoch: 93	Loss: 0.052002	Acc: 19.4% (1939/10000)
[Test]  Epoch: 94	Loss: 0.051984	Acc: 19.5% (1949/10000)
[Test]  Epoch: 95	Loss: 0.052082	Acc: 19.5% (1953/10000)
[Test]  Epoch: 96	Loss: 0.052061	Acc: 19.4% (1943/10000)
[Test]  Epoch: 97	Loss: 0.051947	Acc: 19.6% (1960/10000)
[Test]  Epoch: 98	Loss: 0.051997	Acc: 19.4% (1940/10000)
[Test]  Epoch: 99	Loss: 0.052046	Acc: 19.5% (1948/10000)
[Test]  Epoch: 100	Loss: 0.052033	Acc: 19.2% (1925/10000)
===========finish==========
['2024-08-18', '16:51:49.104253', '100', 'test', '0.05203303740024567', '19.25', '19.64']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.8.conv.2.weight', '_features.11.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.7.conv.2.weight', '_features.9.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.10.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.13.conv.2.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight', '_features.18.1.weight', '_features.15.conv.2.weight', '_features.5.conv.2.weight', '_features.12.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.6.conv.2.weight', '_features.3.conv.1.0.weight', '_features.16.conv.0.0.weight', '_features.3.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.146361	Acc: 1.1% (112/10000)
[Test]  Epoch: 2	Loss: 0.147334	Acc: 0.9% (95/10000)
[Test]  Epoch: 3	Loss: 0.104539	Acc: 1.1% (111/10000)
[Test]  Epoch: 4	Loss: 0.069008	Acc: 2.9% (294/10000)
[Test]  Epoch: 5	Loss: 0.085383	Acc: 3.2% (323/10000)
[Test]  Epoch: 6	Loss: 0.067132	Acc: 4.3% (435/10000)
[Test]  Epoch: 7	Loss: 0.066473	Acc: 4.9% (489/10000)
[Test]  Epoch: 8	Loss: 0.065752	Acc: 5.5% (547/10000)
[Test]  Epoch: 9	Loss: 0.066385	Acc: 5.1% (511/10000)
[Test]  Epoch: 10	Loss: 0.064583	Acc: 6.0% (599/10000)
[Test]  Epoch: 11	Loss: 0.065447	Acc: 6.1% (609/10000)
[Test]  Epoch: 12	Loss: 0.067619	Acc: 5.3% (535/10000)
[Test]  Epoch: 13	Loss: 0.104398	Acc: 1.5% (146/10000)
[Test]  Epoch: 14	Loss: 0.075629	Acc: 3.5% (355/10000)
[Test]  Epoch: 15	Loss: 0.075868	Acc: 6.4% (638/10000)
[Test]  Epoch: 16	Loss: 0.072879	Acc: 3.1% (309/10000)
[Test]  Epoch: 17	Loss: 0.101785	Acc: 1.9% (192/10000)
[Test]  Epoch: 18	Loss: 0.095992	Acc: 1.7% (167/10000)
[Test]  Epoch: 19	Loss: 0.100707	Acc: 1.0% (101/10000)
[Test]  Epoch: 20	Loss: 0.113106	Acc: 1.0% (102/10000)
[Test]  Epoch: 21	Loss: 0.078986	Acc: 2.5% (248/10000)
[Test]  Epoch: 22	Loss: 0.099227	Acc: 1.4% (140/10000)
[Test]  Epoch: 23	Loss: 0.070185	Acc: 3.7% (370/10000)
[Test]  Epoch: 24	Loss: 0.067368	Acc: 4.4% (437/10000)
[Test]  Epoch: 25	Loss: 0.070007	Acc: 3.7% (367/10000)
[Test]  Epoch: 26	Loss: 0.071900	Acc: 3.3% (329/10000)
[Test]  Epoch: 27	Loss: 0.091112	Acc: 3.3% (329/10000)
[Test]  Epoch: 28	Loss: 0.125767	Acc: 1.1% (109/10000)
[Test]  Epoch: 29	Loss: 0.076276	Acc: 4.0% (405/10000)
[Test]  Epoch: 30	Loss: 0.067075	Acc: 5.3% (533/10000)
[Test]  Epoch: 31	Loss: 0.064951	Acc: 5.8% (582/10000)
[Test]  Epoch: 32	Loss: 0.065699	Acc: 6.0% (603/10000)
[Test]  Epoch: 33	Loss: 0.066189	Acc: 6.3% (635/10000)
[Test]  Epoch: 34	Loss: 0.064177	Acc: 6.5% (645/10000)
[Test]  Epoch: 35	Loss: 0.065042	Acc: 7.0% (703/10000)
[Test]  Epoch: 36	Loss: 0.069745	Acc: 5.6% (560/10000)
[Test]  Epoch: 37	Loss: 0.067667	Acc: 7.7% (769/10000)
[Test]  Epoch: 38	Loss: 0.064510	Acc: 7.9% (791/10000)
[Test]  Epoch: 39	Loss: 0.065362	Acc: 7.5% (747/10000)
[Test]  Epoch: 40	Loss: 0.066046	Acc: 7.1% (710/10000)
[Test]  Epoch: 41	Loss: 0.065862	Acc: 7.3% (726/10000)
[Test]  Epoch: 42	Loss: 0.066062	Acc: 7.3% (727/10000)
[Test]  Epoch: 43	Loss: 0.092730	Acc: 1.5% (148/10000)
[Test]  Epoch: 44	Loss: 0.066079	Acc: 7.4% (741/10000)
[Test]  Epoch: 45	Loss: 0.065986	Acc: 6.8% (684/10000)
[Test]  Epoch: 46	Loss: 0.105476	Acc: 3.5% (350/10000)
[Test]  Epoch: 47	Loss: 0.067646	Acc: 5.1% (506/10000)
[Test]  Epoch: 48	Loss: 0.076699	Acc: 3.4% (341/10000)
[Test]  Epoch: 49	Loss: 0.066076	Acc: 6.2% (620/10000)
[Test]  Epoch: 50	Loss: 0.068995	Acc: 5.6% (557/10000)
[Test]  Epoch: 51	Loss: 0.067638	Acc: 7.3% (733/10000)
[Test]  Epoch: 52	Loss: 0.073028	Acc: 6.4% (643/10000)
[Test]  Epoch: 53	Loss: 0.069242	Acc: 6.4% (642/10000)
[Test]  Epoch: 54	Loss: 0.063888	Acc: 8.6% (857/10000)
[Test]  Epoch: 55	Loss: 0.064755	Acc: 8.0% (798/10000)
[Test]  Epoch: 56	Loss: 0.067463	Acc: 7.3% (730/10000)
[Test]  Epoch: 57	Loss: 0.067978	Acc: 7.9% (790/10000)
[Test]  Epoch: 58	Loss: 0.066096	Acc: 8.1% (809/10000)
[Test]  Epoch: 59	Loss: 0.066251	Acc: 8.3% (831/10000)
[Test]  Epoch: 60	Loss: 0.067109	Acc: 8.4% (838/10000)
[Test]  Epoch: 61	Loss: 0.065096	Acc: 9.6% (957/10000)
[Test]  Epoch: 62	Loss: 0.065247	Acc: 9.6% (964/10000)
[Test]  Epoch: 63	Loss: 0.064961	Acc: 9.8% (976/10000)
[Test]  Epoch: 64	Loss: 0.065180	Acc: 9.9% (986/10000)
[Test]  Epoch: 65	Loss: 0.065169	Acc: 9.9% (994/10000)
[Test]  Epoch: 66	Loss: 0.065309	Acc: 10.0% (1001/10000)
[Test]  Epoch: 67	Loss: 0.065499	Acc: 10.2% (1015/10000)
[Test]  Epoch: 68	Loss: 0.065771	Acc: 9.9% (988/10000)
[Test]  Epoch: 69	Loss: 0.065872	Acc: 9.7% (970/10000)
[Test]  Epoch: 70	Loss: 0.065952	Acc: 9.9% (986/10000)
[Test]  Epoch: 71	Loss: 0.066271	Acc: 9.7% (972/10000)
[Test]  Epoch: 72	Loss: 0.066958	Acc: 9.7% (970/10000)
[Test]  Epoch: 73	Loss: 0.066892	Acc: 9.5% (953/10000)
[Test]  Epoch: 74	Loss: 0.066681	Acc: 9.5% (948/10000)
[Test]  Epoch: 75	Loss: 0.066855	Acc: 9.5% (954/10000)
[Test]  Epoch: 76	Loss: 0.066870	Acc: 9.6% (960/10000)
[Test]  Epoch: 77	Loss: 0.067378	Acc: 9.7% (966/10000)
[Test]  Epoch: 78	Loss: 0.067392	Acc: 9.5% (948/10000)
[Test]  Epoch: 79	Loss: 0.067320	Acc: 9.6% (964/10000)
[Test]  Epoch: 80	Loss: 0.067380	Acc: 9.3% (930/10000)
[Test]  Epoch: 81	Loss: 0.067642	Acc: 9.9% (991/10000)
[Test]  Epoch: 82	Loss: 0.067609	Acc: 9.6% (963/10000)
[Test]  Epoch: 83	Loss: 0.067545	Acc: 9.7% (967/10000)
[Test]  Epoch: 84	Loss: 0.067997	Acc: 9.4% (944/10000)
[Test]  Epoch: 85	Loss: 0.067705	Acc: 9.7% (966/10000)
[Test]  Epoch: 86	Loss: 0.068001	Acc: 9.6% (955/10000)
[Test]  Epoch: 87	Loss: 0.068063	Acc: 9.8% (981/10000)
[Test]  Epoch: 88	Loss: 0.067909	Acc: 9.7% (967/10000)
[Test]  Epoch: 89	Loss: 0.067916	Acc: 9.5% (949/10000)
[Test]  Epoch: 90	Loss: 0.068160	Acc: 9.8% (980/10000)
[Test]  Epoch: 91	Loss: 0.068597	Acc: 9.8% (977/10000)
[Test]  Epoch: 92	Loss: 0.068060	Acc: 9.9% (988/10000)
[Test]  Epoch: 93	Loss: 0.068783	Acc: 9.4% (944/10000)
[Test]  Epoch: 94	Loss: 0.068466	Acc: 9.7% (972/10000)
[Test]  Epoch: 95	Loss: 0.068926	Acc: 9.8% (981/10000)
[Test]  Epoch: 96	Loss: 0.068692	Acc: 9.6% (959/10000)
[Test]  Epoch: 97	Loss: 0.068812	Acc: 9.4% (941/10000)
[Test]  Epoch: 98	Loss: 0.068870	Acc: 9.5% (946/10000)
[Test]  Epoch: 99	Loss: 0.068978	Acc: 9.7% (973/10000)
[Test]  Epoch: 100	Loss: 0.069299	Acc: 9.5% (953/10000)
===========finish==========
['2024-08-18', '16:54:02.320296', '100', 'test', '0.06929935359954834', '9.53', '10.15']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.8.conv.2.weight', '_features.11.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.7.conv.2.weight', '_features.9.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.10.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.13.conv.2.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight', '_features.18.1.weight', '_features.15.conv.2.weight', '_features.5.conv.2.weight', '_features.12.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.6.conv.2.weight', '_features.3.conv.1.0.weight', '_features.16.conv.0.0.weight', '_features.3.conv.2.weight', '_features.1.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.3.weight', '_features.9.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.12.conv.3.weight', '_features.5.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.133513	Acc: 0.5% (46/10000)
[Test]  Epoch: 2	Loss: 0.130698	Acc: 1.3% (128/10000)
[Test]  Epoch: 3	Loss: 0.081113	Acc: 3.8% (380/10000)
[Test]  Epoch: 4	Loss: 0.066750	Acc: 4.8% (475/10000)
[Test]  Epoch: 5	Loss: 0.078551	Acc: 2.1% (213/10000)
[Test]  Epoch: 6	Loss: 0.066727	Acc: 4.5% (454/10000)
[Test]  Epoch: 7	Loss: 0.117900	Acc: 1.8% (182/10000)
[Test]  Epoch: 8	Loss: 0.074124	Acc: 4.1% (408/10000)
[Test]  Epoch: 9	Loss: 0.069337	Acc: 4.6% (464/10000)
[Test]  Epoch: 10	Loss: 0.072443	Acc: 3.0% (298/10000)
[Test]  Epoch: 11	Loss: 0.065903	Acc: 5.6% (558/10000)
[Test]  Epoch: 12	Loss: 0.086506	Acc: 4.2% (422/10000)
[Test]  Epoch: 13	Loss: 0.080434	Acc: 2.5% (247/10000)
[Test]  Epoch: 14	Loss: 0.066908	Acc: 5.9% (593/10000)
[Test]  Epoch: 15	Loss: 0.065428	Acc: 5.7% (574/10000)
[Test]  Epoch: 16	Loss: 0.065134	Acc: 6.5% (646/10000)
[Test]  Epoch: 17	Loss: 0.079932	Acc: 1.8% (180/10000)
[Test]  Epoch: 18	Loss: 0.065655	Acc: 6.4% (642/10000)
[Test]  Epoch: 19	Loss: 0.085142	Acc: 1.4% (137/10000)
[Test]  Epoch: 20	Loss: 0.073640	Acc: 4.0% (398/10000)
[Test]  Epoch: 21	Loss: 0.067314	Acc: 5.4% (544/10000)
[Test]  Epoch: 22	Loss: 0.066777	Acc: 5.8% (582/10000)
[Test]  Epoch: 23	Loss: 0.066386	Acc: 6.3% (631/10000)
[Test]  Epoch: 24	Loss: 0.068390	Acc: 5.9% (592/10000)
[Test]  Epoch: 25	Loss: 0.065087	Acc: 7.3% (731/10000)
[Test]  Epoch: 26	Loss: 0.168323	Acc: 1.6% (158/10000)
[Test]  Epoch: 27	Loss: 0.065523	Acc: 7.0% (697/10000)
[Test]  Epoch: 28	Loss: 0.074645	Acc: 4.0% (396/10000)
[Test]  Epoch: 29	Loss: 0.069171	Acc: 6.2% (617/10000)
[Test]  Epoch: 30	Loss: 0.065084	Acc: 7.5% (752/10000)
[Test]  Epoch: 31	Loss: 0.067083	Acc: 6.3% (635/10000)
[Test]  Epoch: 32	Loss: 0.073882	Acc: 7.0% (700/10000)
[Test]  Epoch: 33	Loss: 0.066490	Acc: 7.2% (721/10000)
[Test]  Epoch: 34	Loss: 0.063231	Acc: 8.5% (850/10000)
[Test]  Epoch: 35	Loss: 0.100837	Acc: 2.1% (210/10000)
[Test]  Epoch: 36	Loss: 0.114991	Acc: 2.5% (251/10000)
[Test]  Epoch: 37	Loss: 0.072451	Acc: 4.2% (416/10000)
[Test]  Epoch: 38	Loss: 0.067147	Acc: 5.9% (586/10000)
[Test]  Epoch: 39	Loss: 0.086367	Acc: 1.2% (125/10000)
[Test]  Epoch: 40	Loss: 0.077275	Acc: 2.0% (203/10000)
[Test]  Epoch: 41	Loss: 0.072722	Acc: 2.4% (241/10000)
[Test]  Epoch: 42	Loss: 0.092141	Acc: 1.3% (128/10000)
[Test]  Epoch: 43	Loss: 0.073918	Acc: 3.3% (326/10000)
[Test]  Epoch: 44	Loss: 0.072545	Acc: 2.5% (247/10000)
[Test]  Epoch: 45	Loss: 0.063837	Acc: 7.5% (748/10000)
[Test]  Epoch: 46	Loss: 0.065731	Acc: 6.0% (599/10000)
[Test]  Epoch: 47	Loss: 0.065013	Acc: 7.3% (732/10000)
[Test]  Epoch: 48	Loss: 0.068123	Acc: 6.4% (637/10000)
[Test]  Epoch: 49	Loss: 0.065179	Acc: 7.8% (779/10000)
[Test]  Epoch: 50	Loss: 0.063873	Acc: 8.9% (890/10000)
[Test]  Epoch: 51	Loss: 0.066719	Acc: 7.6% (762/10000)
[Test]  Epoch: 52	Loss: 0.067597	Acc: 5.7% (568/10000)
[Test]  Epoch: 53	Loss: 0.065859	Acc: 6.8% (682/10000)
[Test]  Epoch: 54	Loss: 0.067703	Acc: 6.8% (681/10000)
[Test]  Epoch: 55	Loss: 0.067675	Acc: 7.7% (770/10000)
[Test]  Epoch: 56	Loss: 0.068983	Acc: 5.6% (563/10000)
[Test]  Epoch: 57	Loss: 0.067066	Acc: 7.7% (773/10000)
[Test]  Epoch: 58	Loss: 0.064886	Acc: 8.7% (870/10000)
[Test]  Epoch: 59	Loss: 0.064774	Acc: 9.6% (956/10000)
[Test]  Epoch: 60	Loss: 0.065870	Acc: 8.7% (865/10000)
[Test]  Epoch: 61	Loss: 0.062980	Acc: 11.0% (1100/10000)
[Test]  Epoch: 62	Loss: 0.063076	Acc: 11.0% (1101/10000)
[Test]  Epoch: 63	Loss: 0.063215	Acc: 11.2% (1117/10000)
[Test]  Epoch: 64	Loss: 0.063472	Acc: 11.2% (1124/10000)
[Test]  Epoch: 65	Loss: 0.063575	Acc: 10.9% (1094/10000)
[Test]  Epoch: 66	Loss: 0.063752	Acc: 10.9% (1095/10000)
[Test]  Epoch: 67	Loss: 0.063889	Acc: 10.9% (1087/10000)
[Test]  Epoch: 68	Loss: 0.064248	Acc: 10.9% (1090/10000)
[Test]  Epoch: 69	Loss: 0.064128	Acc: 10.8% (1081/10000)
[Test]  Epoch: 70	Loss: 0.064407	Acc: 10.8% (1075/10000)
[Test]  Epoch: 71	Loss: 0.064415	Acc: 10.8% (1084/10000)
[Test]  Epoch: 72	Loss: 0.064746	Acc: 10.8% (1079/10000)
[Test]  Epoch: 73	Loss: 0.064697	Acc: 10.7% (1069/10000)
[Test]  Epoch: 74	Loss: 0.079283	Acc: 4.6% (457/10000)
[Test]  Epoch: 75	Loss: 0.066243	Acc: 8.7% (870/10000)
[Test]  Epoch: 76	Loss: 0.064626	Acc: 10.2% (1023/10000)
[Test]  Epoch: 77	Loss: 0.064238	Acc: 10.8% (1078/10000)
[Test]  Epoch: 78	Loss: 0.063963	Acc: 10.9% (1089/10000)
[Test]  Epoch: 79	Loss: 0.063915	Acc: 11.2% (1115/10000)
[Test]  Epoch: 80	Loss: 0.064203	Acc: 11.0% (1098/10000)
[Test]  Epoch: 81	Loss: 0.064398	Acc: 11.2% (1115/10000)
[Test]  Epoch: 82	Loss: 0.064582	Acc: 11.1% (1114/10000)
[Test]  Epoch: 83	Loss: 0.064496	Acc: 11.1% (1113/10000)
[Test]  Epoch: 84	Loss: 0.064678	Acc: 11.2% (1121/10000)
[Test]  Epoch: 85	Loss: 0.064841	Acc: 11.2% (1124/10000)
[Test]  Epoch: 86	Loss: 0.065214	Acc: 11.1% (1107/10000)
[Test]  Epoch: 87	Loss: 0.065276	Acc: 11.1% (1107/10000)
[Test]  Epoch: 88	Loss: 0.065147	Acc: 10.9% (1089/10000)
[Test]  Epoch: 89	Loss: 0.065162	Acc: 11.0% (1101/10000)
[Test]  Epoch: 90	Loss: 0.065388	Acc: 11.1% (1108/10000)
[Test]  Epoch: 91	Loss: 0.065334	Acc: 11.1% (1107/10000)
[Test]  Epoch: 92	Loss: 0.065610	Acc: 11.0% (1099/10000)
[Test]  Epoch: 93	Loss: 0.065687	Acc: 11.0% (1099/10000)
[Test]  Epoch: 94	Loss: 0.065809	Acc: 11.1% (1108/10000)
[Test]  Epoch: 95	Loss: 0.065748	Acc: 10.9% (1086/10000)
[Test]  Epoch: 96	Loss: 0.065909	Acc: 10.8% (1084/10000)
[Test]  Epoch: 97	Loss: 0.065882	Acc: 11.0% (1100/10000)
[Test]  Epoch: 98	Loss: 0.065922	Acc: 10.9% (1086/10000)
[Test]  Epoch: 99	Loss: 0.066148	Acc: 11.0% (1101/10000)
[Test]  Epoch: 100	Loss: 0.066266	Acc: 10.8% (1076/10000)
===========finish==========
['2024-08-18', '16:56:15.371199', '100', 'test', '0.06626582674980164', '10.76', '11.24']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.8.conv.2.weight', '_features.11.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.7.conv.2.weight', '_features.9.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.10.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.13.conv.2.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight', '_features.18.1.weight', '_features.15.conv.2.weight', '_features.5.conv.2.weight', '_features.12.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.6.conv.2.weight', '_features.3.conv.1.0.weight', '_features.16.conv.0.0.weight', '_features.3.conv.2.weight', '_features.1.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.3.weight', '_features.9.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.12.conv.3.weight', '_features.5.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.1.0.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.138625	Acc: 1.1% (106/10000)
[Test]  Epoch: 2	Loss: 0.106868	Acc: 1.6% (158/10000)
[Test]  Epoch: 3	Loss: 0.070219	Acc: 4.0% (405/10000)
[Test]  Epoch: 4	Loss: 0.067478	Acc: 5.1% (507/10000)
[Test]  Epoch: 5	Loss: 0.064913	Acc: 6.1% (613/10000)
[Test]  Epoch: 6	Loss: 0.068303	Acc: 5.3% (531/10000)
[Test]  Epoch: 7	Loss: 0.063469	Acc: 7.6% (758/10000)
[Test]  Epoch: 8	Loss: 0.065497	Acc: 6.4% (636/10000)
[Test]  Epoch: 9	Loss: 0.068883	Acc: 5.8% (585/10000)
[Test]  Epoch: 10	Loss: 0.065414	Acc: 7.1% (714/10000)
[Test]  Epoch: 11	Loss: 0.070400	Acc: 6.6% (662/10000)
[Test]  Epoch: 12	Loss: 0.064388	Acc: 8.7% (872/10000)
[Test]  Epoch: 13	Loss: 0.064601	Acc: 8.5% (848/10000)
[Test]  Epoch: 14	Loss: 0.063041	Acc: 9.5% (947/10000)
[Test]  Epoch: 15	Loss: 0.061351	Acc: 10.8% (1080/10000)
[Test]  Epoch: 16	Loss: 0.064564	Acc: 9.5% (954/10000)
[Test]  Epoch: 17	Loss: 0.071441	Acc: 7.3% (729/10000)
[Test]  Epoch: 18	Loss: 0.073907	Acc: 4.6% (458/10000)
[Test]  Epoch: 19	Loss: 0.080797	Acc: 5.5% (554/10000)
[Test]  Epoch: 20	Loss: 0.066594	Acc: 9.3% (935/10000)
[Test]  Epoch: 21	Loss: 0.064892	Acc: 10.9% (1090/10000)
[Test]  Epoch: 22	Loss: 0.064035	Acc: 9.8% (978/10000)
[Test]  Epoch: 23	Loss: 0.065037	Acc: 10.1% (1009/10000)
[Test]  Epoch: 24	Loss: 0.062541	Acc: 11.1% (1107/10000)
[Test]  Epoch: 25	Loss: 0.061905	Acc: 11.3% (1133/10000)
[Test]  Epoch: 26	Loss: 0.063255	Acc: 11.0% (1102/10000)
[Test]  Epoch: 27	Loss: 0.062802	Acc: 11.7% (1165/10000)
[Test]  Epoch: 28	Loss: 0.061141	Acc: 12.3% (1230/10000)
[Test]  Epoch: 29	Loss: 0.065383	Acc: 10.9% (1088/10000)
[Test]  Epoch: 30	Loss: 0.079847	Acc: 5.3% (533/10000)
[Test]  Epoch: 31	Loss: 0.069774	Acc: 9.0% (904/10000)
[Test]  Epoch: 32	Loss: 0.063703	Acc: 10.7% (1073/10000)
[Test]  Epoch: 33	Loss: 0.089232	Acc: 3.4% (339/10000)
[Test]  Epoch: 34	Loss: 0.064512	Acc: 10.7% (1073/10000)
[Test]  Epoch: 35	Loss: 0.062184	Acc: 12.0% (1204/10000)
[Test]  Epoch: 36	Loss: 0.060809	Acc: 12.9% (1289/10000)
[Test]  Epoch: 37	Loss: 0.062977	Acc: 11.8% (1178/10000)
[Test]  Epoch: 38	Loss: 0.065139	Acc: 10.9% (1093/10000)
[Test]  Epoch: 39	Loss: 0.062554	Acc: 12.6% (1259/10000)
[Test]  Epoch: 40	Loss: 0.063052	Acc: 11.6% (1156/10000)
[Test]  Epoch: 41	Loss: 0.062869	Acc: 12.1% (1206/10000)
[Test]  Epoch: 42	Loss: 0.062130	Acc: 12.9% (1291/10000)
[Test]  Epoch: 43	Loss: 0.061331	Acc: 13.7% (1367/10000)
[Test]  Epoch: 44	Loss: 0.062281	Acc: 13.0% (1303/10000)
[Test]  Epoch: 45	Loss: 0.061740	Acc: 13.3% (1333/10000)
[Test]  Epoch: 46	Loss: 0.072376	Acc: 8.0% (795/10000)
[Test]  Epoch: 47	Loss: 0.063927	Acc: 12.2% (1224/10000)
[Test]  Epoch: 48	Loss: 0.064598	Acc: 12.6% (1262/10000)
[Test]  Epoch: 49	Loss: 0.060566	Acc: 13.7% (1369/10000)
[Test]  Epoch: 50	Loss: 0.062099	Acc: 12.6% (1262/10000)
[Test]  Epoch: 51	Loss: 0.062512	Acc: 12.8% (1278/10000)
[Test]  Epoch: 52	Loss: 0.063183	Acc: 12.3% (1228/10000)
[Test]  Epoch: 53	Loss: 0.061844	Acc: 13.0% (1298/10000)
[Test]  Epoch: 54	Loss: 0.059025	Acc: 14.8% (1475/10000)
[Test]  Epoch: 55	Loss: 0.059508	Acc: 14.8% (1481/10000)
[Test]  Epoch: 56	Loss: 0.058279	Acc: 15.4% (1538/10000)
[Test]  Epoch: 57	Loss: 0.060470	Acc: 13.8% (1379/10000)
[Test]  Epoch: 58	Loss: 0.060221	Acc: 13.6% (1364/10000)
[Test]  Epoch: 59	Loss: 0.059499	Acc: 15.3% (1527/10000)
[Test]  Epoch: 60	Loss: 0.065797	Acc: 10.8% (1077/10000)
[Test]  Epoch: 61	Loss: 0.059006	Acc: 14.5% (1446/10000)
[Test]  Epoch: 62	Loss: 0.058311	Acc: 15.0% (1504/10000)
[Test]  Epoch: 63	Loss: 0.058321	Acc: 15.3% (1532/10000)
[Test]  Epoch: 64	Loss: 0.058028	Acc: 15.9% (1588/10000)
[Test]  Epoch: 65	Loss: 0.057872	Acc: 16.0% (1597/10000)
[Test]  Epoch: 66	Loss: 0.057968	Acc: 15.8% (1583/10000)
[Test]  Epoch: 67	Loss: 0.057641	Acc: 15.9% (1587/10000)
[Test]  Epoch: 68	Loss: 0.057557	Acc: 16.0% (1602/10000)
[Test]  Epoch: 69	Loss: 0.057466	Acc: 16.0% (1603/10000)
[Test]  Epoch: 70	Loss: 0.057414	Acc: 16.2% (1616/10000)
[Test]  Epoch: 71	Loss: 0.057454	Acc: 16.0% (1604/10000)
[Test]  Epoch: 72	Loss: 0.057595	Acc: 16.1% (1606/10000)
[Test]  Epoch: 73	Loss: 0.057481	Acc: 16.0% (1601/10000)
[Test]  Epoch: 74	Loss: 0.057117	Acc: 16.1% (1608/10000)
[Test]  Epoch: 75	Loss: 0.057361	Acc: 16.1% (1610/10000)
[Test]  Epoch: 76	Loss: 0.057203	Acc: 16.2% (1621/10000)
[Test]  Epoch: 77	Loss: 0.057258	Acc: 16.4% (1640/10000)
[Test]  Epoch: 78	Loss: 0.057112	Acc: 16.1% (1608/10000)
[Test]  Epoch: 79	Loss: 0.057475	Acc: 16.1% (1607/10000)
[Test]  Epoch: 80	Loss: 0.057205	Acc: 16.0% (1604/10000)
[Test]  Epoch: 81	Loss: 0.057166	Acc: 16.2% (1620/10000)
[Test]  Epoch: 82	Loss: 0.057203	Acc: 16.2% (1624/10000)
[Test]  Epoch: 83	Loss: 0.057487	Acc: 16.1% (1607/10000)
[Test]  Epoch: 84	Loss: 0.057229	Acc: 15.9% (1587/10000)
[Test]  Epoch: 85	Loss: 0.057155	Acc: 16.2% (1617/10000)
[Test]  Epoch: 86	Loss: 0.057211	Acc: 16.1% (1613/10000)
[Test]  Epoch: 87	Loss: 0.057386	Acc: 16.1% (1605/10000)
[Test]  Epoch: 88	Loss: 0.057206	Acc: 16.1% (1613/10000)
[Test]  Epoch: 89	Loss: 0.057027	Acc: 16.2% (1621/10000)
[Test]  Epoch: 90	Loss: 0.057197	Acc: 16.1% (1606/10000)
[Test]  Epoch: 91	Loss: 0.057262	Acc: 16.1% (1612/10000)
[Test]  Epoch: 92	Loss: 0.057089	Acc: 16.3% (1628/10000)
[Test]  Epoch: 93	Loss: 0.057357	Acc: 16.3% (1626/10000)
[Test]  Epoch: 94	Loss: 0.057250	Acc: 16.4% (1635/10000)
[Test]  Epoch: 95	Loss: 0.057113	Acc: 16.1% (1615/10000)
[Test]  Epoch: 96	Loss: 0.057144	Acc: 16.3% (1630/10000)
[Test]  Epoch: 97	Loss: 0.056916	Acc: 16.3% (1631/10000)
[Test]  Epoch: 98	Loss: 0.057071	Acc: 16.2% (1623/10000)
[Test]  Epoch: 99	Loss: 0.057504	Acc: 16.2% (1616/10000)
[Test]  Epoch: 100	Loss: 0.057197	Acc: 16.3% (1627/10000)
===========finish==========
['2024-08-18', '16:58:27.312590', '100', 'test', '0.057197115325927736', '16.27', '16.4']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.8.conv.2.weight', '_features.11.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.7.conv.2.weight', '_features.9.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.10.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.13.conv.2.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight', '_features.18.1.weight', '_features.15.conv.2.weight', '_features.5.conv.2.weight', '_features.12.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.6.conv.2.weight', '_features.3.conv.1.0.weight', '_features.16.conv.0.0.weight', '_features.3.conv.2.weight', '_features.1.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.3.weight', '_features.9.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.12.conv.3.weight', '_features.5.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.1.0.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.4.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.8.conv.3.weight', '_features.16.conv.0.1.weight', '_features.5.conv.0.0.weight', '_features.6.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.7.conv.3.weight', '_features.7.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.110006	Acc: 1.0% (100/10000)
[Test]  Epoch: 2	Loss: 0.076620	Acc: 3.7% (368/10000)
[Test]  Epoch: 3	Loss: 0.067901	Acc: 4.3% (434/10000)
[Test]  Epoch: 4	Loss: 0.065753	Acc: 5.4% (539/10000)
[Test]  Epoch: 5	Loss: 0.073311	Acc: 3.0% (299/10000)
[Test]  Epoch: 6	Loss: 0.070453	Acc: 3.7% (367/10000)
[Test]  Epoch: 7	Loss: 0.066376	Acc: 5.4% (542/10000)
[Test]  Epoch: 8	Loss: 0.069433	Acc: 4.4% (436/10000)
[Test]  Epoch: 9	Loss: 0.075288	Acc: 3.2% (321/10000)
[Test]  Epoch: 10	Loss: 0.068517	Acc: 4.9% (488/10000)
[Test]  Epoch: 11	Loss: 0.072899	Acc: 3.5% (355/10000)
[Test]  Epoch: 12	Loss: 0.064658	Acc: 6.7% (673/10000)
[Test]  Epoch: 13	Loss: 0.074362	Acc: 3.9% (392/10000)
[Test]  Epoch: 14	Loss: 0.067121	Acc: 7.1% (708/10000)
[Test]  Epoch: 15	Loss: 0.064039	Acc: 8.2% (824/10000)
[Test]  Epoch: 16	Loss: 0.062230	Acc: 9.1% (911/10000)
[Test]  Epoch: 17	Loss: 0.061748	Acc: 9.9% (994/10000)
[Test]  Epoch: 18	Loss: 0.064370	Acc: 8.4% (839/10000)
[Test]  Epoch: 19	Loss: 0.063122	Acc: 9.3% (932/10000)
[Test]  Epoch: 20	Loss: 0.064793	Acc: 8.3% (834/10000)
[Test]  Epoch: 21	Loss: 0.063219	Acc: 10.0% (1000/10000)
[Test]  Epoch: 22	Loss: 0.061529	Acc: 11.1% (1113/10000)
[Test]  Epoch: 23	Loss: 0.062096	Acc: 10.5% (1053/10000)
[Test]  Epoch: 24	Loss: 0.063077	Acc: 10.2% (1021/10000)
[Test]  Epoch: 25	Loss: 0.065725	Acc: 9.2% (917/10000)
[Test]  Epoch: 26	Loss: 0.061225	Acc: 11.8% (1179/10000)
[Test]  Epoch: 27	Loss: 0.062639	Acc: 10.7% (1066/10000)
[Test]  Epoch: 28	Loss: 0.061861	Acc: 11.3% (1131/10000)
[Test]  Epoch: 29	Loss: 0.061434	Acc: 12.3% (1230/10000)
[Test]  Epoch: 30	Loss: 0.064628	Acc: 10.5% (1047/10000)
[Test]  Epoch: 31	Loss: 0.061709	Acc: 11.9% (1190/10000)
[Test]  Epoch: 32	Loss: 0.062415	Acc: 11.9% (1188/10000)
[Test]  Epoch: 33	Loss: 0.064002	Acc: 11.9% (1186/10000)
[Test]  Epoch: 34	Loss: 0.060395	Acc: 13.0% (1302/10000)
[Test]  Epoch: 35	Loss: 0.061618	Acc: 12.4% (1242/10000)
[Test]  Epoch: 36	Loss: 0.076594	Acc: 7.0% (701/10000)
[Test]  Epoch: 37	Loss: 0.074466	Acc: 6.5% (650/10000)
[Test]  Epoch: 38	Loss: 0.063709	Acc: 10.4% (1041/10000)
[Test]  Epoch: 39	Loss: 0.070343	Acc: 9.0% (900/10000)
[Test]  Epoch: 40	Loss: 0.063467	Acc: 11.2% (1121/10000)
[Test]  Epoch: 41	Loss: 0.064078	Acc: 10.9% (1088/10000)
[Test]  Epoch: 42	Loss: 0.066827	Acc: 9.4% (945/10000)
[Test]  Epoch: 43	Loss: 0.065335	Acc: 11.1% (1111/10000)
[Test]  Epoch: 44	Loss: 0.074973	Acc: 7.6% (761/10000)
[Test]  Epoch: 45	Loss: 0.062722	Acc: 11.8% (1175/10000)
[Test]  Epoch: 46	Loss: 0.064120	Acc: 11.0% (1103/10000)
[Test]  Epoch: 47	Loss: 0.064350	Acc: 11.2% (1116/10000)
[Test]  Epoch: 48	Loss: 0.060680	Acc: 13.0% (1302/10000)
[Test]  Epoch: 49	Loss: 0.059446	Acc: 14.2% (1417/10000)
[Test]  Epoch: 50	Loss: 0.058685	Acc: 14.7% (1468/10000)
[Test]  Epoch: 51	Loss: 0.059563	Acc: 14.0% (1404/10000)
[Test]  Epoch: 52	Loss: 0.071857	Acc: 6.8% (676/10000)
[Test]  Epoch: 53	Loss: 0.068077	Acc: 9.3% (926/10000)
[Test]  Epoch: 54	Loss: 0.060003	Acc: 13.2% (1319/10000)
[Test]  Epoch: 55	Loss: 0.065167	Acc: 11.2% (1117/10000)
[Test]  Epoch: 56	Loss: 0.059879	Acc: 14.1% (1413/10000)
[Test]  Epoch: 57	Loss: 0.059951	Acc: 13.2% (1320/10000)
[Test]  Epoch: 58	Loss: 0.060641	Acc: 13.1% (1312/10000)
[Test]  Epoch: 59	Loss: 0.059380	Acc: 14.3% (1429/10000)
[Test]  Epoch: 60	Loss: 0.062537	Acc: 11.9% (1195/10000)
[Test]  Epoch: 61	Loss: 0.058401	Acc: 14.4% (1445/10000)
[Test]  Epoch: 62	Loss: 0.057848	Acc: 14.9% (1492/10000)
[Test]  Epoch: 63	Loss: 0.057505	Acc: 15.1% (1509/10000)
[Test]  Epoch: 64	Loss: 0.057603	Acc: 15.3% (1532/10000)
[Test]  Epoch: 65	Loss: 0.057430	Acc: 15.2% (1524/10000)
[Test]  Epoch: 66	Loss: 0.057648	Acc: 15.4% (1537/10000)
[Test]  Epoch: 67	Loss: 0.057643	Acc: 15.2% (1524/10000)
[Test]  Epoch: 68	Loss: 0.057642	Acc: 15.2% (1522/10000)
[Test]  Epoch: 69	Loss: 0.057406	Acc: 15.5% (1551/10000)
[Test]  Epoch: 70	Loss: 0.057411	Acc: 15.4% (1537/10000)
[Test]  Epoch: 71	Loss: 0.057243	Acc: 15.6% (1561/10000)
[Test]  Epoch: 72	Loss: 0.057600	Acc: 15.6% (1560/10000)
[Test]  Epoch: 73	Loss: 0.057573	Acc: 15.5% (1548/10000)
[Test]  Epoch: 74	Loss: 0.057648	Acc: 15.4% (1541/10000)
[Test]  Epoch: 75	Loss: 0.057528	Acc: 15.8% (1578/10000)
[Test]  Epoch: 76	Loss: 0.057370	Acc: 15.4% (1542/10000)
[Test]  Epoch: 77	Loss: 0.057271	Acc: 15.5% (1552/10000)
[Test]  Epoch: 78	Loss: 0.057335	Acc: 15.6% (1563/10000)
[Test]  Epoch: 79	Loss: 0.057421	Acc: 15.6% (1561/10000)
[Test]  Epoch: 80	Loss: 0.057318	Acc: 15.4% (1536/10000)
[Test]  Epoch: 81	Loss: 0.057222	Acc: 15.7% (1571/10000)
[Test]  Epoch: 82	Loss: 0.057194	Acc: 15.7% (1567/10000)
[Test]  Epoch: 83	Loss: 0.057233	Acc: 15.6% (1563/10000)
[Test]  Epoch: 84	Loss: 0.057522	Acc: 15.6% (1558/10000)
[Test]  Epoch: 85	Loss: 0.057383	Acc: 15.8% (1575/10000)
[Test]  Epoch: 86	Loss: 0.057339	Acc: 15.8% (1585/10000)
[Test]  Epoch: 87	Loss: 0.057412	Acc: 15.7% (1570/10000)
[Test]  Epoch: 88	Loss: 0.057304	Acc: 15.8% (1576/10000)
[Test]  Epoch: 89	Loss: 0.057475	Acc: 15.8% (1581/10000)
[Test]  Epoch: 90	Loss: 0.057428	Acc: 15.8% (1583/10000)
[Test]  Epoch: 91	Loss: 0.057485	Acc: 15.8% (1576/10000)
[Test]  Epoch: 92	Loss: 0.057223	Acc: 15.8% (1577/10000)
[Test]  Epoch: 93	Loss: 0.057811	Acc: 15.6% (1555/10000)
[Test]  Epoch: 94	Loss: 0.057233	Acc: 15.9% (1586/10000)
[Test]  Epoch: 95	Loss: 0.057497	Acc: 15.7% (1574/10000)
[Test]  Epoch: 96	Loss: 0.057358	Acc: 16.1% (1608/10000)
[Test]  Epoch: 97	Loss: 0.057149	Acc: 15.9% (1590/10000)
[Test]  Epoch: 98	Loss: 0.057049	Acc: 16.1% (1609/10000)
[Test]  Epoch: 99	Loss: 0.057060	Acc: 16.0% (1598/10000)
[Test]  Epoch: 100	Loss: 0.057224	Acc: 15.9% (1595/10000)
===========finish==========
['2024-08-18', '17:00:44.519781', '100', 'test', '0.057223608112335206', '15.95', '16.09']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.8.conv.2.weight', '_features.11.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.7.conv.2.weight', '_features.9.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.10.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.13.conv.2.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight', '_features.18.1.weight', '_features.15.conv.2.weight', '_features.5.conv.2.weight', '_features.12.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.6.conv.2.weight', '_features.3.conv.1.0.weight', '_features.16.conv.0.0.weight', '_features.3.conv.2.weight', '_features.1.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.3.weight', '_features.9.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.12.conv.3.weight', '_features.5.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.1.0.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.4.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.8.conv.3.weight', '_features.16.conv.0.1.weight', '_features.5.conv.0.0.weight', '_features.6.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.7.conv.3.weight', '_features.7.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.5.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.123133	Acc: 1.1% (111/10000)
[Test]  Epoch: 2	Loss: 0.082674	Acc: 2.0% (205/10000)
[Test]  Epoch: 3	Loss: 0.078887	Acc: 4.3% (429/10000)
[Test]  Epoch: 4	Loss: 0.067212	Acc: 4.8% (481/10000)
[Test]  Epoch: 5	Loss: 0.065914	Acc: 6.5% (653/10000)
[Test]  Epoch: 6	Loss: 0.065925	Acc: 5.3% (529/10000)
[Test]  Epoch: 7	Loss: 0.072238	Acc: 5.7% (565/10000)
[Test]  Epoch: 8	Loss: 0.065546	Acc: 7.1% (711/10000)
[Test]  Epoch: 9	Loss: 0.073400	Acc: 4.3% (428/10000)
[Test]  Epoch: 10	Loss: 0.064495	Acc: 7.5% (755/10000)
[Test]  Epoch: 11	Loss: 0.063645	Acc: 8.4% (836/10000)
[Test]  Epoch: 12	Loss: 0.063976	Acc: 8.7% (865/10000)
[Test]  Epoch: 13	Loss: 0.063689	Acc: 9.6% (958/10000)
[Test]  Epoch: 14	Loss: 0.064531	Acc: 10.4% (1039/10000)
[Test]  Epoch: 15	Loss: 0.075245	Acc: 2.8% (280/10000)
[Test]  Epoch: 16	Loss: 0.064544	Acc: 6.8% (683/10000)
[Test]  Epoch: 17	Loss: 0.072515	Acc: 3.2% (316/10000)
[Test]  Epoch: 18	Loss: 0.064092	Acc: 7.5% (751/10000)
[Test]  Epoch: 19	Loss: 0.064460	Acc: 7.2% (724/10000)
[Test]  Epoch: 20	Loss: 0.063245	Acc: 7.9% (794/10000)
[Test]  Epoch: 21	Loss: 0.063629	Acc: 7.6% (760/10000)
[Test]  Epoch: 22	Loss: 0.062839	Acc: 9.1% (912/10000)
[Test]  Epoch: 23	Loss: 0.063525	Acc: 9.2% (917/10000)
[Test]  Epoch: 24	Loss: 0.063376	Acc: 9.8% (981/10000)
[Test]  Epoch: 25	Loss: 0.063240	Acc: 9.6% (957/10000)
[Test]  Epoch: 26	Loss: 0.062493	Acc: 10.7% (1070/10000)
[Test]  Epoch: 27	Loss: 0.068012	Acc: 8.1% (808/10000)
[Test]  Epoch: 28	Loss: 0.064653	Acc: 9.2% (919/10000)
[Test]  Epoch: 29	Loss: 0.063489	Acc: 11.0% (1098/10000)
[Test]  Epoch: 30	Loss: 0.062919	Acc: 10.4% (1045/10000)
[Test]  Epoch: 31	Loss: 0.079245	Acc: 4.1% (411/10000)
[Test]  Epoch: 32	Loss: 0.082231	Acc: 5.7% (567/10000)
[Test]  Epoch: 33	Loss: 0.064011	Acc: 9.9% (993/10000)
[Test]  Epoch: 34	Loss: 0.062881	Acc: 11.2% (1118/10000)
[Test]  Epoch: 35	Loss: 0.064324	Acc: 10.6% (1058/10000)
[Test]  Epoch: 36	Loss: 0.063878	Acc: 11.2% (1122/10000)
[Test]  Epoch: 37	Loss: 0.063020	Acc: 12.7% (1269/10000)
[Test]  Epoch: 38	Loss: 0.063400	Acc: 12.0% (1197/10000)
[Test]  Epoch: 39	Loss: 0.062675	Acc: 12.1% (1209/10000)
[Test]  Epoch: 40	Loss: 0.060932	Acc: 13.3% (1334/10000)
[Test]  Epoch: 41	Loss: 0.063809	Acc: 12.4% (1241/10000)
[Test]  Epoch: 42	Loss: 0.063634	Acc: 12.2% (1222/10000)
[Test]  Epoch: 43	Loss: 0.062189	Acc: 12.2% (1219/10000)
[Test]  Epoch: 44	Loss: 0.073999	Acc: 8.8% (875/10000)
[Test]  Epoch: 45	Loss: 0.066966	Acc: 10.4% (1039/10000)
[Test]  Epoch: 46	Loss: 0.070206	Acc: 8.8% (885/10000)
[Test]  Epoch: 47	Loss: 0.062513	Acc: 12.6% (1259/10000)
[Test]  Epoch: 48	Loss: 0.062120	Acc: 12.1% (1206/10000)
[Test]  Epoch: 49	Loss: 0.066732	Acc: 10.3% (1035/10000)
[Test]  Epoch: 50	Loss: 0.065192	Acc: 11.1% (1114/10000)
[Test]  Epoch: 51	Loss: 0.061439	Acc: 13.2% (1323/10000)
[Test]  Epoch: 52	Loss: 0.061263	Acc: 13.4% (1337/10000)
[Test]  Epoch: 53	Loss: 0.074709	Acc: 7.3% (728/10000)
[Test]  Epoch: 54	Loss: 0.061390	Acc: 13.8% (1378/10000)
[Test]  Epoch: 55	Loss: 0.061324	Acc: 13.4% (1337/10000)
[Test]  Epoch: 56	Loss: 0.060900	Acc: 13.8% (1385/10000)
[Test]  Epoch: 57	Loss: 0.060082	Acc: 13.8% (1382/10000)
[Test]  Epoch: 58	Loss: 0.060660	Acc: 13.8% (1385/10000)
[Test]  Epoch: 59	Loss: 0.061297	Acc: 13.5% (1346/10000)
[Test]  Epoch: 60	Loss: 0.060727	Acc: 13.5% (1352/10000)
[Test]  Epoch: 61	Loss: 0.059631	Acc: 14.4% (1444/10000)
[Test]  Epoch: 62	Loss: 0.059003	Acc: 14.8% (1480/10000)
[Test]  Epoch: 63	Loss: 0.058954	Acc: 14.9% (1494/10000)
[Test]  Epoch: 64	Loss: 0.058811	Acc: 15.0% (1499/10000)
[Test]  Epoch: 65	Loss: 0.059000	Acc: 15.2% (1521/10000)
[Test]  Epoch: 66	Loss: 0.058738	Acc: 15.4% (1539/10000)
[Test]  Epoch: 67	Loss: 0.058629	Acc: 15.0% (1502/10000)
[Test]  Epoch: 68	Loss: 0.058716	Acc: 15.3% (1530/10000)
[Test]  Epoch: 69	Loss: 0.058735	Acc: 15.2% (1518/10000)
[Test]  Epoch: 70	Loss: 0.058654	Acc: 15.2% (1517/10000)
[Test]  Epoch: 71	Loss: 0.058562	Acc: 15.1% (1514/10000)
[Test]  Epoch: 72	Loss: 0.058709	Acc: 15.1% (1512/10000)
[Test]  Epoch: 73	Loss: 0.058637	Acc: 14.7% (1472/10000)
[Test]  Epoch: 74	Loss: 0.058550	Acc: 15.2% (1515/10000)
[Test]  Epoch: 75	Loss: 0.058606	Acc: 15.3% (1530/10000)
[Test]  Epoch: 76	Loss: 0.058734	Acc: 15.1% (1507/10000)
[Test]  Epoch: 77	Loss: 0.058565	Acc: 15.1% (1513/10000)
[Test]  Epoch: 78	Loss: 0.058788	Acc: 15.4% (1538/10000)
[Test]  Epoch: 79	Loss: 0.058579	Acc: 15.3% (1526/10000)
[Test]  Epoch: 80	Loss: 0.058761	Acc: 15.1% (1514/10000)
[Test]  Epoch: 81	Loss: 0.058413	Acc: 15.4% (1544/10000)
[Test]  Epoch: 82	Loss: 0.058528	Acc: 15.3% (1530/10000)
[Test]  Epoch: 83	Loss: 0.058713	Acc: 15.2% (1516/10000)
[Test]  Epoch: 84	Loss: 0.058676	Acc: 15.2% (1522/10000)
[Test]  Epoch: 85	Loss: 0.058704	Acc: 15.2% (1525/10000)
[Test]  Epoch: 86	Loss: 0.058587	Acc: 15.2% (1522/10000)
[Test]  Epoch: 87	Loss: 0.058637	Acc: 15.1% (1511/10000)
[Test]  Epoch: 88	Loss: 0.058655	Acc: 15.1% (1509/10000)
[Test]  Epoch: 89	Loss: 0.058784	Acc: 15.1% (1512/10000)
[Test]  Epoch: 90	Loss: 0.058542	Acc: 15.3% (1532/10000)
[Test]  Epoch: 91	Loss: 0.058609	Acc: 15.2% (1516/10000)
[Test]  Epoch: 92	Loss: 0.058847	Acc: 15.0% (1501/10000)
[Test]  Epoch: 93	Loss: 0.058719	Acc: 15.1% (1511/10000)
[Test]  Epoch: 94	Loss: 0.058756	Acc: 15.2% (1522/10000)
[Test]  Epoch: 95	Loss: 0.058741	Acc: 15.0% (1503/10000)
[Test]  Epoch: 96	Loss: 0.058635	Acc: 14.8% (1485/10000)
[Test]  Epoch: 97	Loss: 0.058748	Acc: 15.2% (1515/10000)
[Test]  Epoch: 98	Loss: 0.058661	Acc: 15.2% (1519/10000)
[Test]  Epoch: 99	Loss: 0.058612	Acc: 15.0% (1503/10000)
[Test]  Epoch: 100	Loss: 0.058682	Acc: 15.2% (1516/10000)
===========finish==========
['2024-08-18', '17:02:59.545966', '100', 'test', '0.058682250356674195', '15.16', '15.44']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar100-mobilenetv2-channel mobilenetv2 CIFAR100 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar100-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 100 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.0.1.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.0.weight', '_features.16.conv.3.weight', '_features.14.conv.1.0.weight', '_features.16.conv.2.weight', '_features.12.conv.0.0.weight', '_features.11.conv.2.weight', '_features.9.conv.2.weight', '_features.11.conv.0.0.weight', '_features.10.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.8.conv.2.weight', '_features.11.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.7.conv.2.weight', '_features.9.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.12.conv.2.weight', '_features.10.conv.1.0.weight', '_features.8.conv.0.0.weight', '_features.11.conv.3.weight', '_features.4.conv.2.weight', '_features.10.conv.0.1.weight', '_features.8.conv.1.0.weight', '_features.2.conv.2.weight', '_features.13.conv.2.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.0.weight', '_features.18.1.weight', '_features.15.conv.2.weight', '_features.5.conv.2.weight', '_features.12.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.1.conv.1.weight', '_features.6.conv.2.weight', '_features.3.conv.1.0.weight', '_features.16.conv.0.0.weight', '_features.3.conv.2.weight', '_features.1.conv.0.0.weight', '_features.9.conv.0.1.weight', '_features.9.conv.3.weight', '_features.9.conv.1.1.weight', '_features.8.conv.0.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.13.conv.0.0.weight', '_features.7.conv.0.1.weight', '_features.12.conv.3.weight', '_features.5.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.7.conv.1.0.weight', '_features.13.conv.3.weight', '_features.6.conv.1.0.weight', '_features.15.conv.3.weight', '_features.8.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.2.conv.0.1.weight', '_features.12.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.4.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.8.conv.3.weight', '_features.16.conv.0.1.weight', '_features.5.conv.0.0.weight', '_features.6.conv.0.0.weight', '_features.4.conv.1.0.weight', '_features.7.conv.3.weight', '_features.7.conv.1.1.weight', '_features.5.conv.0.1.weight', '_features.6.conv.0.1.weight', '_features.3.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.2.conv.0.0.weight', '_features.1.conv.0.1.weight', '_features.0.0.weight', '_features.5.conv.1.1.weight', '_features.6.conv.1.1.weight', '_features.3.conv.0.0.weight', '_features.0.1.weight', '_features.3.conv.1.1.weight', '_features.4.conv.3.weight', '_features.2.conv.1.1.weight', '_features.6.conv.3.weight', '_features.5.conv.3.weight', '_features.2.conv.3.weight', '_features.1.conv.2.weight', '_features.3.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.097184	Acc: 1.4% (140/10000)
[Test]  Epoch: 2	Loss: 0.073726	Acc: 3.3% (332/10000)
[Test]  Epoch: 3	Loss: 0.065419	Acc: 6.7% (672/10000)
[Test]  Epoch: 4	Loss: 0.070011	Acc: 5.8% (578/10000)
[Test]  Epoch: 5	Loss: 0.062491	Acc: 8.9% (891/10000)
[Test]  Epoch: 6	Loss: 0.065147	Acc: 8.4% (839/10000)
[Test]  Epoch: 7	Loss: 0.062633	Acc: 10.4% (1038/10000)
[Test]  Epoch: 8	Loss: 0.064713	Acc: 9.1% (909/10000)
[Test]  Epoch: 9	Loss: 0.067058	Acc: 8.7% (868/10000)
[Test]  Epoch: 10	Loss: 0.060298	Acc: 12.3% (1232/10000)
[Test]  Epoch: 11	Loss: 0.061063	Acc: 12.1% (1206/10000)
[Test]  Epoch: 12	Loss: 0.067935	Acc: 9.5% (949/10000)
[Test]  Epoch: 13	Loss: 0.062306	Acc: 12.1% (1212/10000)
[Test]  Epoch: 14	Loss: 0.059401	Acc: 14.8% (1475/10000)
[Test]  Epoch: 15	Loss: 0.062758	Acc: 11.9% (1186/10000)
[Test]  Epoch: 16	Loss: 0.058408	Acc: 14.8% (1476/10000)
[Test]  Epoch: 17	Loss: 0.057364	Acc: 15.4% (1539/10000)
[Test]  Epoch: 18	Loss: 0.058971	Acc: 13.8% (1378/10000)
[Test]  Epoch: 19	Loss: 0.056165	Acc: 16.3% (1632/10000)
[Test]  Epoch: 20	Loss: 0.058545	Acc: 14.5% (1449/10000)
[Test]  Epoch: 21	Loss: 0.056315	Acc: 16.4% (1635/10000)
[Test]  Epoch: 22	Loss: 0.055212	Acc: 16.7% (1671/10000)
[Test]  Epoch: 23	Loss: 0.055538	Acc: 17.4% (1740/10000)
[Test]  Epoch: 24	Loss: 0.055345	Acc: 17.0% (1699/10000)
[Test]  Epoch: 25	Loss: 0.054871	Acc: 17.2% (1718/10000)
[Test]  Epoch: 26	Loss: 0.054292	Acc: 17.2% (1721/10000)
[Test]  Epoch: 27	Loss: 0.054332	Acc: 17.4% (1745/10000)
[Test]  Epoch: 28	Loss: 0.053534	Acc: 18.5% (1854/10000)
[Test]  Epoch: 29	Loss: 0.053808	Acc: 18.2% (1816/10000)
[Test]  Epoch: 30	Loss: 0.053206	Acc: 18.4% (1835/10000)
[Test]  Epoch: 31	Loss: 0.052960	Acc: 18.9% (1894/10000)
[Test]  Epoch: 32	Loss: 0.052579	Acc: 18.9% (1893/10000)
[Test]  Epoch: 33	Loss: 0.053093	Acc: 18.4% (1845/10000)
[Test]  Epoch: 34	Loss: 0.054564	Acc: 17.7% (1766/10000)
[Test]  Epoch: 35	Loss: 0.053455	Acc: 18.5% (1852/10000)
[Test]  Epoch: 36	Loss: 0.055827	Acc: 17.8% (1775/10000)
[Test]  Epoch: 37	Loss: 0.053632	Acc: 18.4% (1839/10000)
[Test]  Epoch: 38	Loss: 0.058511	Acc: 15.6% (1558/10000)
[Test]  Epoch: 39	Loss: 0.065895	Acc: 11.2% (1115/10000)
[Test]  Epoch: 40	Loss: 0.060576	Acc: 13.8% (1377/10000)
[Test]  Epoch: 41	Loss: 0.055208	Acc: 17.4% (1739/10000)
[Test]  Epoch: 42	Loss: 0.054680	Acc: 17.1% (1705/10000)
[Test]  Epoch: 43	Loss: 0.052517	Acc: 19.1% (1907/10000)
[Test]  Epoch: 44	Loss: 0.054004	Acc: 18.1% (1807/10000)
[Test]  Epoch: 45	Loss: 0.052958	Acc: 19.2% (1921/10000)
[Test]  Epoch: 46	Loss: 0.054852	Acc: 17.7% (1770/10000)
[Test]  Epoch: 47	Loss: 0.051790	Acc: 19.8% (1982/10000)
[Test]  Epoch: 48	Loss: 0.052560	Acc: 19.5% (1954/10000)
[Test]  Epoch: 49	Loss: 0.052498	Acc: 19.5% (1951/10000)
[Test]  Epoch: 50	Loss: 0.051930	Acc: 19.8% (1982/10000)
[Test]  Epoch: 51	Loss: 0.052215	Acc: 19.8% (1981/10000)
[Test]  Epoch: 52	Loss: 0.052330	Acc: 19.0% (1899/10000)
[Test]  Epoch: 53	Loss: 0.051901	Acc: 19.9% (1992/10000)
[Test]  Epoch: 54	Loss: 0.053295	Acc: 18.4% (1838/10000)
[Test]  Epoch: 55	Loss: 0.052057	Acc: 19.7% (1967/10000)
[Test]  Epoch: 56	Loss: 0.051549	Acc: 20.5% (2048/10000)
[Test]  Epoch: 57	Loss: 0.052664	Acc: 19.1% (1909/10000)
[Test]  Epoch: 58	Loss: 0.053133	Acc: 18.8% (1875/10000)
[Test]  Epoch: 59	Loss: 0.051385	Acc: 20.1% (2015/10000)
[Test]  Epoch: 60	Loss: 0.063989	Acc: 11.9% (1190/10000)
[Test]  Epoch: 61	Loss: 0.053128	Acc: 19.1% (1907/10000)
[Test]  Epoch: 62	Loss: 0.052564	Acc: 19.5% (1951/10000)
[Test]  Epoch: 63	Loss: 0.052396	Acc: 19.8% (1976/10000)
[Test]  Epoch: 64	Loss: 0.052074	Acc: 19.9% (1987/10000)
[Test]  Epoch: 65	Loss: 0.051706	Acc: 20.4% (2038/10000)
[Test]  Epoch: 66	Loss: 0.051590	Acc: 20.5% (2048/10000)
[Test]  Epoch: 67	Loss: 0.052064	Acc: 20.1% (2008/10000)
[Test]  Epoch: 68	Loss: 0.051568	Acc: 20.7% (2071/10000)
[Test]  Epoch: 69	Loss: 0.051338	Acc: 20.7% (2073/10000)
[Test]  Epoch: 70	Loss: 0.051375	Acc: 20.6% (2065/10000)
[Test]  Epoch: 71	Loss: 0.051360	Acc: 20.6% (2058/10000)
[Test]  Epoch: 72	Loss: 0.051231	Acc: 20.8% (2077/10000)
[Test]  Epoch: 73	Loss: 0.051230	Acc: 20.9% (2086/10000)
[Test]  Epoch: 74	Loss: 0.051094	Acc: 20.8% (2081/10000)
[Test]  Epoch: 75	Loss: 0.051007	Acc: 20.9% (2092/10000)
[Test]  Epoch: 76	Loss: 0.051053	Acc: 20.8% (2084/10000)
[Test]  Epoch: 77	Loss: 0.051036	Acc: 21.0% (2100/10000)
[Test]  Epoch: 78	Loss: 0.051153	Acc: 21.2% (2124/10000)
[Test]  Epoch: 79	Loss: 0.051247	Acc: 21.0% (2104/10000)
[Test]  Epoch: 80	Loss: 0.051114	Acc: 21.1% (2108/10000)
[Test]  Epoch: 81	Loss: 0.050879	Acc: 21.3% (2126/10000)
[Test]  Epoch: 82	Loss: 0.050904	Acc: 21.2% (2125/10000)
[Test]  Epoch: 83	Loss: 0.050866	Acc: 21.1% (2106/10000)
[Test]  Epoch: 84	Loss: 0.050785	Acc: 21.5% (2152/10000)
[Test]  Epoch: 85	Loss: 0.050959	Acc: 21.3% (2134/10000)
[Test]  Epoch: 86	Loss: 0.050853	Acc: 21.4% (2141/10000)
[Test]  Epoch: 87	Loss: 0.050839	Acc: 21.5% (2154/10000)
[Test]  Epoch: 88	Loss: 0.050803	Acc: 21.4% (2143/10000)
[Test]  Epoch: 89	Loss: 0.050751	Acc: 21.4% (2139/10000)
[Test]  Epoch: 90	Loss: 0.050865	Acc: 21.3% (2131/10000)
[Test]  Epoch: 91	Loss: 0.050705	Acc: 21.5% (2152/10000)
[Test]  Epoch: 92	Loss: 0.050678	Acc: 21.5% (2151/10000)
[Test]  Epoch: 93	Loss: 0.050699	Acc: 21.6% (2157/10000)
[Test]  Epoch: 94	Loss: 0.050575	Acc: 21.6% (2165/10000)
[Test]  Epoch: 95	Loss: 0.050656	Acc: 21.5% (2146/10000)
[Test]  Epoch: 96	Loss: 0.050790	Acc: 21.2% (2117/10000)
[Test]  Epoch: 97	Loss: 0.050559	Acc: 21.4% (2139/10000)
[Test]  Epoch: 98	Loss: 0.050524	Acc: 21.4% (2137/10000)
[Test]  Epoch: 99	Loss: 0.050609	Acc: 21.5% (2153/10000)
[Test]  Epoch: 100	Loss: 0.050685	Acc: 21.5% (2149/10000)
===========finish==========
['2024-08-18', '17:05:29.233614', '100', 'test', '0.05068468840122223', '21.49', '21.65']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-resnet18/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('layer3.0.conv1.weight', 62.75801086425781), ('layer2.0.conv2.weight', 61.10749435424805), ('layer2.0.conv1.weight', 50.03083038330078), ('layer2.1.conv1.weight', 48.81552505493164), ('conv1.weight', 46.83902359008789), ('layer1.1.conv1.weight', 30.37412452697754), ('layer2.1.conv2.weight', 30.058269500732422), ('layer3.0.conv2.weight', 27.23949432373047), ('layer1.0.conv1.weight', 25.57807159423828), ('layer1.1.conv2.weight', 19.3208065032959), ('layer1.0.conv2.weight', 16.214157104492188), ('layer2.0.downsample.0.weight', 14.906328201293945), ('layer4.0.downsample.0.weight', 9.898670196533203), ('bn1.weight', 5.948174953460693), ('layer2.0.bn2.weight', 4.401740074157715), ('layer2.1.bn1.weight', 4.382648468017578), ('layer3.0.bn1.weight', 4.238792419433594), ('layer4.0.conv1.weight', 4.108150005340576), ('layer2.0.bn1.weight', 3.689829111099243), ('last_linear.weight', 3.0293970108032227), ('layer2.0.downsample.1.weight', 2.4182581901550293), ('layer1.1.bn1.weight', 2.3272416591644287), ('layer3.0.bn2.weight', 2.219964027404785), ('layer1.0.bn2.weight', 2.162095546722412), ('layer2.1.bn2.weight', 2.1187658309936523), ('layer3.0.downsample.0.weight', 2.0991549491882324), ('layer1.0.bn1.weight', 1.8434184789657593), ('layer1.1.bn2.weight', 1.6546893119812012), ('layer4.0.conv2.weight', 1.5104596614837646), ('layer4.1.conv1.weight', 1.3401964902877808), ('layer4.1.conv2.weight', 0.9756286144256592), ('layer3.0.downsample.1.weight', 0.6801657676696777), ('layer4.0.bn1.weight', 0.4445880949497223), ('layer4.1.bn1.weight', 0.3142246901988983), ('layer3.1.conv2.weight', 0.15382979810237885), ('layer4.0.downsample.1.weight', 0.13262403011322021), ('layer4.1.bn2.weight', 0.08032067120075226), ('layer3.1.conv1.weight', 0.0724133551120758), ('layer3.1.bn2.weight', 0.06274770200252533), ('layer4.0.bn2.weight', 0.05207718908786774), ('layer3.1.bn1.weight', 0.0052599175833165646), ('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('layer3.0.conv1.weight', 62.75801086425781), ('layer2.0.conv2.weight', 61.10749435424805), ('layer2.0.conv1.weight', 50.03083038330078), ('layer2.1.conv1.weight', 48.81552505493164), ('conv1.weight', 46.83902359008789), ('layer1.1.conv1.weight', 30.37412452697754), ('layer2.1.conv2.weight', 30.058269500732422), ('layer3.0.conv2.weight', 27.23949432373047), ('layer1.0.conv1.weight', 25.57807159423828), ('layer1.1.conv2.weight', 19.3208065032959), ('layer1.0.conv2.weight', 16.214157104492188), ('layer4.0.conv1.weight', 4.108150005340576), ('last_linear.weight', 3.0293970108032227), ('layer4.0.conv2.weight', 1.5104596614837646), ('layer4.1.conv1.weight', 1.3401964902877808), ('layer4.1.conv2.weight', 0.9756286144256592), ('layer3.1.conv2.weight', 0.15382979810237885), ('layer3.1.conv1.weight', 0.0724133551120758), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=41  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.045127	Acc: 43.4% (4336/10000)
[Test]  Epoch: 2	Loss: 0.013478	Acc: 72.3% (7229/10000)
[Test]  Epoch: 3	Loss: 0.010803	Acc: 76.8% (7680/10000)
[Test]  Epoch: 4	Loss: 0.010875	Acc: 77.0% (7698/10000)
[Test]  Epoch: 5	Loss: 0.009794	Acc: 79.0% (7901/10000)
[Test]  Epoch: 6	Loss: 0.009511	Acc: 79.7% (7969/10000)
[Test]  Epoch: 7	Loss: 0.009509	Acc: 79.5% (7954/10000)
[Test]  Epoch: 8	Loss: 0.009412	Acc: 79.8% (7982/10000)
[Test]  Epoch: 9	Loss: 0.009447	Acc: 79.9% (7991/10000)
[Test]  Epoch: 10	Loss: 0.009449	Acc: 79.5% (7953/10000)
[Test]  Epoch: 11	Loss: 0.009348	Acc: 80.0% (7997/10000)
[Test]  Epoch: 12	Loss: 0.009148	Acc: 80.5% (8049/10000)
[Test]  Epoch: 13	Loss: 0.009209	Acc: 80.5% (8055/10000)
[Test]  Epoch: 14	Loss: 0.009232	Acc: 80.5% (8049/10000)
[Test]  Epoch: 15	Loss: 0.009216	Acc: 80.4% (8040/10000)
[Test]  Epoch: 16	Loss: 0.009139	Acc: 80.7% (8072/10000)
[Test]  Epoch: 17	Loss: 0.009026	Acc: 80.8% (8077/10000)
[Test]  Epoch: 18	Loss: 0.009065	Acc: 80.7% (8070/10000)
[Test]  Epoch: 19	Loss: 0.009076	Acc: 80.8% (8084/10000)
[Test]  Epoch: 20	Loss: 0.009103	Acc: 80.6% (8058/10000)
[Test]  Epoch: 21	Loss: 0.009069	Acc: 80.7% (8072/10000)
[Test]  Epoch: 22	Loss: 0.009072	Acc: 80.7% (8065/10000)
[Test]  Epoch: 23	Loss: 0.009029	Acc: 81.0% (8103/10000)
[Test]  Epoch: 24	Loss: 0.009009	Acc: 80.8% (8085/10000)
[Test]  Epoch: 25	Loss: 0.008998	Acc: 80.8% (8083/10000)
[Test]  Epoch: 26	Loss: 0.008901	Acc: 80.8% (8078/10000)
[Test]  Epoch: 27	Loss: 0.008950	Acc: 81.1% (8111/10000)
[Test]  Epoch: 28	Loss: 0.008933	Acc: 81.0% (8101/10000)
[Test]  Epoch: 29	Loss: 0.008918	Acc: 81.0% (8103/10000)
[Test]  Epoch: 30	Loss: 0.008945	Acc: 81.0% (8098/10000)
[Test]  Epoch: 31	Loss: 0.008842	Acc: 81.2% (8118/10000)
[Test]  Epoch: 32	Loss: 0.008915	Acc: 81.0% (8098/10000)
[Test]  Epoch: 33	Loss: 0.008850	Acc: 81.3% (8127/10000)
[Test]  Epoch: 34	Loss: 0.008877	Acc: 81.1% (8112/10000)
[Test]  Epoch: 35	Loss: 0.008859	Acc: 81.0% (8100/10000)
[Test]  Epoch: 36	Loss: 0.008823	Acc: 81.2% (8121/10000)
[Test]  Epoch: 37	Loss: 0.008855	Acc: 81.1% (8110/10000)
[Test]  Epoch: 38	Loss: 0.008911	Acc: 81.1% (8107/10000)
[Test]  Epoch: 39	Loss: 0.008881	Acc: 81.2% (8116/10000)
[Test]  Epoch: 40	Loss: 0.008820	Acc: 81.2% (8115/10000)
[Test]  Epoch: 41	Loss: 0.008837	Acc: 81.2% (8124/10000)
[Test]  Epoch: 42	Loss: 0.008804	Acc: 81.1% (8110/10000)
[Test]  Epoch: 43	Loss: 0.008780	Acc: 81.3% (8126/10000)
[Test]  Epoch: 44	Loss: 0.008788	Acc: 81.1% (8107/10000)
[Test]  Epoch: 45	Loss: 0.008806	Acc: 81.2% (8120/10000)
[Test]  Epoch: 46	Loss: 0.008797	Acc: 81.1% (8110/10000)
[Test]  Epoch: 47	Loss: 0.008863	Acc: 80.9% (8087/10000)
[Test]  Epoch: 48	Loss: 0.008806	Acc: 81.2% (8115/10000)
[Test]  Epoch: 49	Loss: 0.008820	Acc: 81.4% (8137/10000)
[Test]  Epoch: 50	Loss: 0.008837	Acc: 81.2% (8122/10000)
[Test]  Epoch: 51	Loss: 0.008806	Acc: 81.3% (8129/10000)
[Test]  Epoch: 52	Loss: 0.008762	Acc: 81.4% (8137/10000)
[Test]  Epoch: 53	Loss: 0.008840	Acc: 81.2% (8118/10000)
[Test]  Epoch: 54	Loss: 0.008819	Acc: 81.1% (8109/10000)
[Test]  Epoch: 55	Loss: 0.008782	Acc: 81.3% (8129/10000)
[Test]  Epoch: 56	Loss: 0.008753	Acc: 81.3% (8128/10000)
[Test]  Epoch: 57	Loss: 0.008758	Acc: 81.3% (8130/10000)
[Test]  Epoch: 58	Loss: 0.008706	Acc: 81.4% (8144/10000)
[Test]  Epoch: 59	Loss: 0.008742	Acc: 81.4% (8137/10000)
[Test]  Epoch: 60	Loss: 0.008825	Acc: 81.1% (8110/10000)
[Test]  Epoch: 61	Loss: 0.008807	Acc: 81.2% (8120/10000)
[Test]  Epoch: 62	Loss: 0.008791	Acc: 81.2% (8123/10000)
[Test]  Epoch: 63	Loss: 0.008780	Acc: 81.3% (8132/10000)
[Test]  Epoch: 64	Loss: 0.008756	Acc: 81.4% (8137/10000)
[Test]  Epoch: 65	Loss: 0.008755	Acc: 81.4% (8137/10000)
[Test]  Epoch: 66	Loss: 0.008732	Acc: 81.4% (8142/10000)
[Test]  Epoch: 67	Loss: 0.008765	Acc: 81.3% (8134/10000)
[Test]  Epoch: 68	Loss: 0.008734	Acc: 81.4% (8137/10000)
[Test]  Epoch: 69	Loss: 0.008756	Acc: 81.3% (8127/10000)
[Test]  Epoch: 70	Loss: 0.008752	Acc: 81.4% (8139/10000)
[Test]  Epoch: 71	Loss: 0.008744	Acc: 81.4% (8139/10000)
[Test]  Epoch: 72	Loss: 0.008735	Acc: 81.4% (8139/10000)
[Test]  Epoch: 73	Loss: 0.008731	Acc: 81.5% (8145/10000)
[Test]  Epoch: 74	Loss: 0.008755	Acc: 81.4% (8143/10000)
[Test]  Epoch: 75	Loss: 0.008728	Acc: 81.5% (8148/10000)
[Test]  Epoch: 76	Loss: 0.008727	Acc: 81.4% (8142/10000)
[Test]  Epoch: 77	Loss: 0.008731	Acc: 81.4% (8142/10000)
[Test]  Epoch: 78	Loss: 0.008738	Acc: 81.4% (8137/10000)
[Test]  Epoch: 79	Loss: 0.008708	Acc: 81.5% (8145/10000)
[Test]  Epoch: 80	Loss: 0.008737	Acc: 81.4% (8137/10000)
[Test]  Epoch: 81	Loss: 0.008751	Acc: 81.3% (8131/10000)
[Test]  Epoch: 82	Loss: 0.008747	Acc: 81.4% (8139/10000)
[Test]  Epoch: 83	Loss: 0.008725	Acc: 81.3% (8132/10000)
[Test]  Epoch: 84	Loss: 0.008724	Acc: 81.4% (8139/10000)
[Test]  Epoch: 85	Loss: 0.008725	Acc: 81.4% (8142/10000)
[Test]  Epoch: 86	Loss: 0.008724	Acc: 81.3% (8135/10000)
[Test]  Epoch: 87	Loss: 0.008718	Acc: 81.4% (8140/10000)
[Test]  Epoch: 88	Loss: 0.008734	Acc: 81.4% (8142/10000)
[Test]  Epoch: 89	Loss: 0.008737	Acc: 81.4% (8137/10000)
[Test]  Epoch: 90	Loss: 0.008741	Acc: 81.3% (8134/10000)
[Test]  Epoch: 91	Loss: 0.008758	Acc: 81.3% (8135/10000)
[Test]  Epoch: 92	Loss: 0.008743	Acc: 81.4% (8136/10000)
[Test]  Epoch: 93	Loss: 0.008736	Acc: 81.4% (8137/10000)
[Test]  Epoch: 94	Loss: 0.008725	Acc: 81.4% (8144/10000)
[Test]  Epoch: 95	Loss: 0.008722	Acc: 81.5% (8147/10000)
[Test]  Epoch: 96	Loss: 0.008700	Acc: 81.5% (8149/10000)
[Test]  Epoch: 97	Loss: 0.008729	Acc: 81.4% (8136/10000)
[Test]  Epoch: 98	Loss: 0.008744	Acc: 81.4% (8138/10000)
[Test]  Epoch: 99	Loss: 0.008713	Acc: 81.5% (8150/10000)
[Test]  Epoch: 100	Loss: 0.008735	Acc: 81.4% (8140/10000)
===========finish==========
['2024-08-18', '17:08:26.581988', '100', 'test', '0.008735415558516979', '81.4', '81.5']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.028198	Acc: 39.5% (3945/10000)
[Test]  Epoch: 2	Loss: 0.022772	Acc: 48.5% (4846/10000)
[Test]  Epoch: 3	Loss: 0.021512	Acc: 52.4% (5243/10000)
[Test]  Epoch: 4	Loss: 0.025718	Acc: 49.0% (4898/10000)
[Test]  Epoch: 5	Loss: 0.022986	Acc: 55.1% (5514/10000)
[Test]  Epoch: 6	Loss: 0.021536	Acc: 56.7% (5670/10000)
[Test]  Epoch: 7	Loss: 0.020341	Acc: 58.8% (5883/10000)
[Test]  Epoch: 8	Loss: 0.021469	Acc: 57.2% (5718/10000)
[Test]  Epoch: 9	Loss: 0.020528	Acc: 58.6% (5856/10000)
[Test]  Epoch: 10	Loss: 0.021142	Acc: 58.3% (5829/10000)
[Test]  Epoch: 11	Loss: 0.020146	Acc: 59.1% (5913/10000)
[Test]  Epoch: 12	Loss: 0.020054	Acc: 59.7% (5970/10000)
[Test]  Epoch: 13	Loss: 0.019674	Acc: 60.1% (6010/10000)
[Test]  Epoch: 14	Loss: 0.020148	Acc: 59.3% (5930/10000)
[Test]  Epoch: 15	Loss: 0.019416	Acc: 60.5% (6051/10000)
[Test]  Epoch: 16	Loss: 0.019970	Acc: 59.6% (5965/10000)
[Test]  Epoch: 17	Loss: 0.019624	Acc: 60.4% (6040/10000)
[Test]  Epoch: 18	Loss: 0.019759	Acc: 60.0% (5999/10000)
[Test]  Epoch: 19	Loss: 0.019198	Acc: 60.8% (6083/10000)
[Test]  Epoch: 20	Loss: 0.018907	Acc: 61.2% (6122/10000)
[Test]  Epoch: 21	Loss: 0.019302	Acc: 60.8% (6083/10000)
[Test]  Epoch: 22	Loss: 0.018944	Acc: 61.1% (6109/10000)
[Test]  Epoch: 23	Loss: 0.019208	Acc: 60.9% (6085/10000)
[Test]  Epoch: 24	Loss: 0.019121	Acc: 61.0% (6100/10000)
[Test]  Epoch: 25	Loss: 0.019247	Acc: 61.5% (6145/10000)
[Test]  Epoch: 26	Loss: 0.019066	Acc: 61.4% (6143/10000)
[Test]  Epoch: 27	Loss: 0.018897	Acc: 61.7% (6168/10000)
[Test]  Epoch: 28	Loss: 0.018925	Acc: 61.4% (6138/10000)
[Test]  Epoch: 29	Loss: 0.019198	Acc: 61.0% (6103/10000)
[Test]  Epoch: 30	Loss: 0.018935	Acc: 61.6% (6159/10000)
[Test]  Epoch: 31	Loss: 0.018947	Acc: 61.8% (6182/10000)
[Test]  Epoch: 32	Loss: 0.019111	Acc: 61.3% (6133/10000)
[Test]  Epoch: 33	Loss: 0.018916	Acc: 61.4% (6137/10000)
[Test]  Epoch: 34	Loss: 0.018990	Acc: 61.5% (6147/10000)
[Test]  Epoch: 35	Loss: 0.018865	Acc: 61.5% (6154/10000)
[Test]  Epoch: 36	Loss: 0.018838	Acc: 61.7% (6166/10000)
[Test]  Epoch: 37	Loss: 0.019075	Acc: 61.5% (6148/10000)
[Test]  Epoch: 38	Loss: 0.018894	Acc: 62.0% (6199/10000)
[Test]  Epoch: 39	Loss: 0.018820	Acc: 61.8% (6183/10000)
[Test]  Epoch: 40	Loss: 0.018944	Acc: 61.8% (6178/10000)
[Test]  Epoch: 41	Loss: 0.018754	Acc: 62.2% (6216/10000)
[Test]  Epoch: 42	Loss: 0.018994	Acc: 61.9% (6186/10000)
[Test]  Epoch: 43	Loss: 0.018799	Acc: 62.0% (6195/10000)
[Test]  Epoch: 44	Loss: 0.018785	Acc: 62.1% (6210/10000)
[Test]  Epoch: 45	Loss: 0.018881	Acc: 61.9% (6189/10000)
[Test]  Epoch: 46	Loss: 0.018751	Acc: 62.0% (6195/10000)
[Test]  Epoch: 47	Loss: 0.018949	Acc: 62.0% (6201/10000)
[Test]  Epoch: 48	Loss: 0.018868	Acc: 61.9% (6191/10000)
[Test]  Epoch: 49	Loss: 0.018800	Acc: 62.1% (6211/10000)
[Test]  Epoch: 50	Loss: 0.019055	Acc: 61.4% (6141/10000)
[Test]  Epoch: 51	Loss: 0.018838	Acc: 61.9% (6193/10000)
[Test]  Epoch: 52	Loss: 0.018637	Acc: 62.3% (6228/10000)
[Test]  Epoch: 53	Loss: 0.018738	Acc: 62.0% (6198/10000)
[Test]  Epoch: 54	Loss: 0.018721	Acc: 62.3% (6229/10000)
[Test]  Epoch: 55	Loss: 0.018792	Acc: 62.0% (6205/10000)
[Test]  Epoch: 56	Loss: 0.018879	Acc: 61.6% (6164/10000)
[Test]  Epoch: 57	Loss: 0.018780	Acc: 62.0% (6200/10000)
[Test]  Epoch: 58	Loss: 0.018609	Acc: 62.5% (6248/10000)
[Test]  Epoch: 59	Loss: 0.018771	Acc: 62.1% (6210/10000)
[Test]  Epoch: 60	Loss: 0.018751	Acc: 62.1% (6215/10000)
[Test]  Epoch: 61	Loss: 0.018727	Acc: 62.1% (6207/10000)
[Test]  Epoch: 62	Loss: 0.018709	Acc: 62.1% (6208/10000)
[Test]  Epoch: 63	Loss: 0.018731	Acc: 62.0% (6205/10000)
[Test]  Epoch: 64	Loss: 0.018662	Acc: 62.1% (6209/10000)
[Test]  Epoch: 65	Loss: 0.018632	Acc: 62.1% (6211/10000)
[Test]  Epoch: 66	Loss: 0.018624	Acc: 62.2% (6217/10000)
[Test]  Epoch: 67	Loss: 0.018684	Acc: 62.2% (6216/10000)
[Test]  Epoch: 68	Loss: 0.018664	Acc: 62.2% (6217/10000)
[Test]  Epoch: 69	Loss: 0.018710	Acc: 62.2% (6222/10000)
[Test]  Epoch: 70	Loss: 0.018655	Acc: 62.2% (6221/10000)
[Test]  Epoch: 71	Loss: 0.018637	Acc: 62.1% (6206/10000)
[Test]  Epoch: 72	Loss: 0.018642	Acc: 62.0% (6195/10000)
[Test]  Epoch: 73	Loss: 0.018639	Acc: 62.1% (6215/10000)
[Test]  Epoch: 74	Loss: 0.018665	Acc: 62.1% (6206/10000)
[Test]  Epoch: 75	Loss: 0.018659	Acc: 62.1% (6209/10000)
[Test]  Epoch: 76	Loss: 0.018627	Acc: 62.1% (6213/10000)
[Test]  Epoch: 77	Loss: 0.018648	Acc: 62.4% (6238/10000)
[Test]  Epoch: 78	Loss: 0.018649	Acc: 62.2% (6225/10000)
[Test]  Epoch: 79	Loss: 0.018651	Acc: 62.1% (6208/10000)
[Test]  Epoch: 80	Loss: 0.018671	Acc: 62.2% (6217/10000)
[Test]  Epoch: 81	Loss: 0.018675	Acc: 62.3% (6227/10000)
[Test]  Epoch: 82	Loss: 0.018727	Acc: 62.2% (6217/10000)
[Test]  Epoch: 83	Loss: 0.018674	Acc: 62.2% (6217/10000)
[Test]  Epoch: 84	Loss: 0.018640	Acc: 62.0% (6205/10000)
[Test]  Epoch: 85	Loss: 0.018650	Acc: 62.4% (6241/10000)
[Test]  Epoch: 86	Loss: 0.018677	Acc: 62.5% (6245/10000)
[Test]  Epoch: 87	Loss: 0.018662	Acc: 62.2% (6224/10000)
[Test]  Epoch: 88	Loss: 0.018684	Acc: 62.1% (6213/10000)
[Test]  Epoch: 89	Loss: 0.018719	Acc: 62.1% (6215/10000)
[Test]  Epoch: 90	Loss: 0.018686	Acc: 62.3% (6231/10000)
[Test]  Epoch: 91	Loss: 0.018667	Acc: 62.4% (6235/10000)
[Test]  Epoch: 92	Loss: 0.018665	Acc: 62.3% (6234/10000)
[Test]  Epoch: 93	Loss: 0.018675	Acc: 62.3% (6233/10000)
[Test]  Epoch: 94	Loss: 0.018686	Acc: 62.4% (6241/10000)
[Test]  Epoch: 95	Loss: 0.018675	Acc: 62.3% (6227/10000)
[Test]  Epoch: 96	Loss: 0.018645	Acc: 62.3% (6228/10000)
[Test]  Epoch: 97	Loss: 0.018670	Acc: 62.3% (6226/10000)
[Test]  Epoch: 98	Loss: 0.018718	Acc: 62.3% (6234/10000)
[Test]  Epoch: 99	Loss: 0.018683	Acc: 62.3% (6230/10000)
[Test]  Epoch: 100	Loss: 0.018685	Acc: 62.4% (6235/10000)
===========finish==========
['2024-08-18', '17:10:35.393241', '100', 'test', '0.018684703928232193', '62.35', '62.48']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.037533	Acc: 29.2% (2916/10000)
[Test]  Epoch: 2	Loss: 0.024552	Acc: 42.3% (4231/10000)
[Test]  Epoch: 3	Loss: 0.024944	Acc: 43.4% (4336/10000)
[Test]  Epoch: 4	Loss: 0.032625	Acc: 39.6% (3959/10000)
[Test]  Epoch: 5	Loss: 0.027582	Acc: 43.8% (4380/10000)
[Test]  Epoch: 6	Loss: 0.030813	Acc: 43.8% (4375/10000)
[Test]  Epoch: 7	Loss: 0.026036	Acc: 50.1% (5008/10000)
[Test]  Epoch: 8	Loss: 0.030793	Acc: 45.3% (4532/10000)
[Test]  Epoch: 9	Loss: 0.025109	Acc: 52.0% (5203/10000)
[Test]  Epoch: 10	Loss: 0.024806	Acc: 52.8% (5280/10000)
[Test]  Epoch: 11	Loss: 0.024022	Acc: 53.0% (5301/10000)
[Test]  Epoch: 12	Loss: 0.024127	Acc: 53.4% (5344/10000)
[Test]  Epoch: 13	Loss: 0.023511	Acc: 53.5% (5345/10000)
[Test]  Epoch: 14	Loss: 0.023636	Acc: 53.8% (5379/10000)
[Test]  Epoch: 15	Loss: 0.023373	Acc: 54.0% (5403/10000)
[Test]  Epoch: 16	Loss: 0.022879	Acc: 54.6% (5462/10000)
[Test]  Epoch: 17	Loss: 0.022774	Acc: 54.7% (5469/10000)
[Test]  Epoch: 18	Loss: 0.022738	Acc: 55.1% (5507/10000)
[Test]  Epoch: 19	Loss: 0.022837	Acc: 54.5% (5455/10000)
[Test]  Epoch: 20	Loss: 0.023064	Acc: 54.3% (5433/10000)
[Test]  Epoch: 21	Loss: 0.022732	Acc: 54.8% (5483/10000)
[Test]  Epoch: 22	Loss: 0.022803	Acc: 54.4% (5444/10000)
[Test]  Epoch: 23	Loss: 0.022725	Acc: 54.8% (5476/10000)
[Test]  Epoch: 24	Loss: 0.022633	Acc: 54.9% (5487/10000)
[Test]  Epoch: 25	Loss: 0.022718	Acc: 55.1% (5515/10000)
[Test]  Epoch: 26	Loss: 0.022690	Acc: 55.2% (5517/10000)
[Test]  Epoch: 27	Loss: 0.023303	Acc: 54.6% (5462/10000)
[Test]  Epoch: 28	Loss: 0.022811	Acc: 54.8% (5480/10000)
[Test]  Epoch: 29	Loss: 0.023072	Acc: 54.1% (5408/10000)
[Test]  Epoch: 30	Loss: 0.022550	Acc: 55.4% (5541/10000)
[Test]  Epoch: 31	Loss: 0.022594	Acc: 55.4% (5543/10000)
[Test]  Epoch: 32	Loss: 0.022323	Acc: 55.5% (5555/10000)
[Test]  Epoch: 33	Loss: 0.022408	Acc: 55.3% (5534/10000)
[Test]  Epoch: 34	Loss: 0.022334	Acc: 55.4% (5535/10000)
[Test]  Epoch: 35	Loss: 0.022272	Acc: 55.6% (5560/10000)
[Test]  Epoch: 36	Loss: 0.022750	Acc: 54.7% (5470/10000)
[Test]  Epoch: 37	Loss: 0.022198	Acc: 56.2% (5617/10000)
[Test]  Epoch: 38	Loss: 0.022448	Acc: 55.8% (5582/10000)
[Test]  Epoch: 39	Loss: 0.022970	Acc: 54.6% (5465/10000)
[Test]  Epoch: 40	Loss: 0.022324	Acc: 55.5% (5546/10000)
[Test]  Epoch: 41	Loss: 0.022463	Acc: 55.6% (5556/10000)
[Test]  Epoch: 42	Loss: 0.022473	Acc: 55.8% (5577/10000)
[Test]  Epoch: 43	Loss: 0.022495	Acc: 55.6% (5557/10000)
[Test]  Epoch: 44	Loss: 0.022224	Acc: 55.7% (5572/10000)
[Test]  Epoch: 45	Loss: 0.022355	Acc: 55.8% (5584/10000)
[Test]  Epoch: 46	Loss: 0.022383	Acc: 55.9% (5593/10000)
[Test]  Epoch: 47	Loss: 0.022137	Acc: 56.0% (5604/10000)
[Test]  Epoch: 48	Loss: 0.022193	Acc: 55.9% (5589/10000)
[Test]  Epoch: 49	Loss: 0.022340	Acc: 55.8% (5582/10000)
[Test]  Epoch: 50	Loss: 0.022610	Acc: 55.4% (5543/10000)
[Test]  Epoch: 51	Loss: 0.022426	Acc: 55.8% (5584/10000)
[Test]  Epoch: 52	Loss: 0.022376	Acc: 55.7% (5573/10000)
[Test]  Epoch: 53	Loss: 0.022429	Acc: 55.6% (5563/10000)
[Test]  Epoch: 54	Loss: 0.022168	Acc: 56.0% (5604/10000)
[Test]  Epoch: 55	Loss: 0.022087	Acc: 56.1% (5612/10000)
[Test]  Epoch: 56	Loss: 0.022249	Acc: 56.0% (5599/10000)
[Test]  Epoch: 57	Loss: 0.022288	Acc: 55.6% (5564/10000)
[Test]  Epoch: 58	Loss: 0.022185	Acc: 55.6% (5563/10000)
[Test]  Epoch: 59	Loss: 0.022379	Acc: 55.9% (5591/10000)
[Test]  Epoch: 60	Loss: 0.022582	Acc: 55.7% (5567/10000)
[Test]  Epoch: 61	Loss: 0.022424	Acc: 55.8% (5577/10000)
[Test]  Epoch: 62	Loss: 0.022356	Acc: 55.9% (5585/10000)
[Test]  Epoch: 63	Loss: 0.022276	Acc: 56.0% (5597/10000)
[Test]  Epoch: 64	Loss: 0.022205	Acc: 56.2% (5619/10000)
[Test]  Epoch: 65	Loss: 0.022213	Acc: 56.1% (5607/10000)
[Test]  Epoch: 66	Loss: 0.022155	Acc: 56.1% (5609/10000)
[Test]  Epoch: 67	Loss: 0.022180	Acc: 56.1% (5609/10000)
[Test]  Epoch: 68	Loss: 0.022179	Acc: 56.0% (5596/10000)
[Test]  Epoch: 69	Loss: 0.022243	Acc: 56.0% (5605/10000)
[Test]  Epoch: 70	Loss: 0.022163	Acc: 56.1% (5608/10000)
[Test]  Epoch: 71	Loss: 0.022085	Acc: 56.2% (5616/10000)
[Test]  Epoch: 72	Loss: 0.022160	Acc: 56.1% (5613/10000)
[Test]  Epoch: 73	Loss: 0.022138	Acc: 56.1% (5613/10000)
[Test]  Epoch: 74	Loss: 0.022121	Acc: 56.2% (5619/10000)
[Test]  Epoch: 75	Loss: 0.022136	Acc: 56.0% (5604/10000)
[Test]  Epoch: 76	Loss: 0.022100	Acc: 56.2% (5625/10000)
[Test]  Epoch: 77	Loss: 0.022154	Acc: 56.1% (5610/10000)
[Test]  Epoch: 78	Loss: 0.022078	Acc: 56.2% (5625/10000)
[Test]  Epoch: 79	Loss: 0.022037	Acc: 56.3% (5630/10000)
[Test]  Epoch: 80	Loss: 0.022139	Acc: 56.1% (5612/10000)
[Test]  Epoch: 81	Loss: 0.022141	Acc: 56.1% (5607/10000)
[Test]  Epoch: 82	Loss: 0.022203	Acc: 56.0% (5603/10000)
[Test]  Epoch: 83	Loss: 0.022197	Acc: 56.2% (5618/10000)
[Test]  Epoch: 84	Loss: 0.022139	Acc: 56.2% (5618/10000)
[Test]  Epoch: 85	Loss: 0.022134	Acc: 56.2% (5619/10000)
[Test]  Epoch: 86	Loss: 0.022148	Acc: 56.2% (5620/10000)
[Test]  Epoch: 87	Loss: 0.022184	Acc: 56.2% (5620/10000)
[Test]  Epoch: 88	Loss: 0.022227	Acc: 56.1% (5610/10000)
[Test]  Epoch: 89	Loss: 0.022216	Acc: 56.1% (5610/10000)
[Test]  Epoch: 90	Loss: 0.022163	Acc: 56.2% (5618/10000)
[Test]  Epoch: 91	Loss: 0.022174	Acc: 56.1% (5609/10000)
[Test]  Epoch: 92	Loss: 0.022187	Acc: 56.1% (5615/10000)
[Test]  Epoch: 93	Loss: 0.022174	Acc: 56.1% (5612/10000)
[Test]  Epoch: 94	Loss: 0.022137	Acc: 56.2% (5625/10000)
[Test]  Epoch: 95	Loss: 0.022102	Acc: 56.3% (5629/10000)
[Test]  Epoch: 96	Loss: 0.022069	Acc: 56.3% (5628/10000)
[Test]  Epoch: 97	Loss: 0.022152	Acc: 56.2% (5621/10000)
[Test]  Epoch: 98	Loss: 0.022221	Acc: 56.2% (5623/10000)
[Test]  Epoch: 99	Loss: 0.022169	Acc: 56.2% (5616/10000)
[Test]  Epoch: 100	Loss: 0.022140	Acc: 56.1% (5613/10000)
===========finish==========
['2024-08-18', '17:12:38.540416', '100', 'test', '0.022140352606773377', '56.13', '56.3']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv2.weight', 'layer2.0.downsample.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.031657	Acc: 32.5% (3255/10000)
[Test]  Epoch: 2	Loss: 0.025393	Acc: 41.6% (4161/10000)
[Test]  Epoch: 3	Loss: 0.027345	Acc: 43.0% (4303/10000)
[Test]  Epoch: 4	Loss: 0.029629	Acc: 42.0% (4200/10000)
[Test]  Epoch: 5	Loss: 0.028095	Acc: 44.9% (4488/10000)
[Test]  Epoch: 6	Loss: 0.025602	Acc: 51.4% (5143/10000)
[Test]  Epoch: 7	Loss: 0.024642	Acc: 52.1% (5207/10000)
[Test]  Epoch: 8	Loss: 0.024844	Acc: 52.2% (5219/10000)
[Test]  Epoch: 9	Loss: 0.025356	Acc: 52.4% (5240/10000)
[Test]  Epoch: 10	Loss: 0.024334	Acc: 53.8% (5375/10000)
[Test]  Epoch: 11	Loss: 0.024122	Acc: 53.5% (5346/10000)
[Test]  Epoch: 12	Loss: 0.023821	Acc: 54.1% (5407/10000)
[Test]  Epoch: 13	Loss: 0.023759	Acc: 53.5% (5355/10000)
[Test]  Epoch: 14	Loss: 0.024030	Acc: 53.7% (5366/10000)
[Test]  Epoch: 15	Loss: 0.023130	Acc: 54.9% (5491/10000)
[Test]  Epoch: 16	Loss: 0.022701	Acc: 54.7% (5468/10000)
[Test]  Epoch: 17	Loss: 0.022485	Acc: 54.8% (5477/10000)
[Test]  Epoch: 18	Loss: 0.022493	Acc: 55.2% (5524/10000)
[Test]  Epoch: 19	Loss: 0.022430	Acc: 54.7% (5470/10000)
[Test]  Epoch: 20	Loss: 0.022477	Acc: 54.9% (5485/10000)
[Test]  Epoch: 21	Loss: 0.022633	Acc: 55.2% (5521/10000)
[Test]  Epoch: 22	Loss: 0.022101	Acc: 55.6% (5563/10000)
[Test]  Epoch: 23	Loss: 0.022412	Acc: 55.5% (5553/10000)
[Test]  Epoch: 24	Loss: 0.022283	Acc: 55.7% (5571/10000)
[Test]  Epoch: 25	Loss: 0.022433	Acc: 55.0% (5501/10000)
[Test]  Epoch: 26	Loss: 0.022244	Acc: 55.5% (5545/10000)
[Test]  Epoch: 27	Loss: 0.023007	Acc: 54.7% (5470/10000)
[Test]  Epoch: 28	Loss: 0.022736	Acc: 55.4% (5539/10000)
[Test]  Epoch: 29	Loss: 0.022302	Acc: 55.5% (5545/10000)
[Test]  Epoch: 30	Loss: 0.022357	Acc: 55.2% (5516/10000)
[Test]  Epoch: 31	Loss: 0.022197	Acc: 55.4% (5540/10000)
[Test]  Epoch: 32	Loss: 0.022402	Acc: 55.3% (5532/10000)
[Test]  Epoch: 33	Loss: 0.022318	Acc: 55.5% (5552/10000)
[Test]  Epoch: 34	Loss: 0.022542	Acc: 55.5% (5548/10000)
[Test]  Epoch: 35	Loss: 0.022332	Acc: 55.2% (5518/10000)
[Test]  Epoch: 36	Loss: 0.022178	Acc: 55.4% (5542/10000)
[Test]  Epoch: 37	Loss: 0.022328	Acc: 55.2% (5520/10000)
[Test]  Epoch: 38	Loss: 0.022307	Acc: 55.8% (5581/10000)
[Test]  Epoch: 39	Loss: 0.022567	Acc: 55.1% (5510/10000)
[Test]  Epoch: 40	Loss: 0.022218	Acc: 55.2% (5522/10000)
[Test]  Epoch: 41	Loss: 0.022365	Acc: 55.4% (5543/10000)
[Test]  Epoch: 42	Loss: 0.022103	Acc: 55.6% (5561/10000)
[Test]  Epoch: 43	Loss: 0.022104	Acc: 55.5% (5545/10000)
[Test]  Epoch: 44	Loss: 0.022185	Acc: 55.2% (5516/10000)
[Test]  Epoch: 45	Loss: 0.022173	Acc: 55.8% (5579/10000)
[Test]  Epoch: 46	Loss: 0.021936	Acc: 55.7% (5571/10000)
[Test]  Epoch: 47	Loss: 0.022181	Acc: 55.8% (5576/10000)
[Test]  Epoch: 48	Loss: 0.022227	Acc: 55.4% (5541/10000)
[Test]  Epoch: 49	Loss: 0.022139	Acc: 55.7% (5573/10000)
[Test]  Epoch: 50	Loss: 0.022113	Acc: 55.7% (5574/10000)
[Test]  Epoch: 51	Loss: 0.022145	Acc: 55.2% (5519/10000)
[Test]  Epoch: 52	Loss: 0.021949	Acc: 55.8% (5584/10000)
[Test]  Epoch: 53	Loss: 0.022039	Acc: 55.5% (5549/10000)
[Test]  Epoch: 54	Loss: 0.021931	Acc: 55.7% (5567/10000)
[Test]  Epoch: 55	Loss: 0.021793	Acc: 56.0% (5603/10000)
[Test]  Epoch: 56	Loss: 0.022256	Acc: 55.5% (5546/10000)
[Test]  Epoch: 57	Loss: 0.022161	Acc: 55.5% (5553/10000)
[Test]  Epoch: 58	Loss: 0.022113	Acc: 55.8% (5579/10000)
[Test]  Epoch: 59	Loss: 0.021953	Acc: 55.8% (5579/10000)
[Test]  Epoch: 60	Loss: 0.022462	Acc: 55.5% (5552/10000)
[Test]  Epoch: 61	Loss: 0.022195	Acc: 55.5% (5553/10000)
[Test]  Epoch: 62	Loss: 0.022101	Acc: 55.4% (5541/10000)
[Test]  Epoch: 63	Loss: 0.022007	Acc: 55.6% (5557/10000)
[Test]  Epoch: 64	Loss: 0.022015	Acc: 55.7% (5574/10000)
[Test]  Epoch: 65	Loss: 0.021976	Acc: 55.6% (5560/10000)
[Test]  Epoch: 66	Loss: 0.021992	Acc: 55.6% (5559/10000)
[Test]  Epoch: 67	Loss: 0.022028	Acc: 55.5% (5545/10000)
[Test]  Epoch: 68	Loss: 0.021940	Acc: 55.7% (5572/10000)
[Test]  Epoch: 69	Loss: 0.022031	Acc: 55.6% (5565/10000)
[Test]  Epoch: 70	Loss: 0.021978	Acc: 55.5% (5551/10000)
[Test]  Epoch: 71	Loss: 0.021921	Acc: 55.8% (5575/10000)
[Test]  Epoch: 72	Loss: 0.021931	Acc: 55.8% (5582/10000)
[Test]  Epoch: 73	Loss: 0.021980	Acc: 55.5% (5546/10000)
[Test]  Epoch: 74	Loss: 0.021919	Acc: 55.7% (5569/10000)
[Test]  Epoch: 75	Loss: 0.021947	Acc: 55.7% (5572/10000)
[Test]  Epoch: 76	Loss: 0.021919	Acc: 55.7% (5567/10000)
[Test]  Epoch: 77	Loss: 0.021972	Acc: 55.8% (5579/10000)
[Test]  Epoch: 78	Loss: 0.021958	Acc: 55.9% (5590/10000)
[Test]  Epoch: 79	Loss: 0.021918	Acc: 55.9% (5591/10000)
[Test]  Epoch: 80	Loss: 0.021909	Acc: 55.8% (5581/10000)
[Test]  Epoch: 81	Loss: 0.021983	Acc: 55.7% (5572/10000)
[Test]  Epoch: 82	Loss: 0.022021	Acc: 55.8% (5577/10000)
[Test]  Epoch: 83	Loss: 0.021978	Acc: 55.9% (5587/10000)
[Test]  Epoch: 84	Loss: 0.021928	Acc: 55.8% (5584/10000)
[Test]  Epoch: 85	Loss: 0.021960	Acc: 55.7% (5574/10000)
[Test]  Epoch: 86	Loss: 0.021975	Acc: 55.9% (5588/10000)
[Test]  Epoch: 87	Loss: 0.022054	Acc: 55.7% (5573/10000)
[Test]  Epoch: 88	Loss: 0.022017	Acc: 55.7% (5567/10000)
[Test]  Epoch: 89	Loss: 0.022050	Acc: 55.8% (5575/10000)
[Test]  Epoch: 90	Loss: 0.021988	Acc: 55.8% (5578/10000)
[Test]  Epoch: 91	Loss: 0.021942	Acc: 55.8% (5575/10000)
[Test]  Epoch: 92	Loss: 0.021997	Acc: 55.7% (5571/10000)
[Test]  Epoch: 93	Loss: 0.022016	Acc: 55.8% (5584/10000)
[Test]  Epoch: 94	Loss: 0.021949	Acc: 55.9% (5586/10000)
[Test]  Epoch: 95	Loss: 0.021942	Acc: 55.9% (5590/10000)
[Test]  Epoch: 96	Loss: 0.021943	Acc: 55.8% (5583/10000)
[Test]  Epoch: 97	Loss: 0.021989	Acc: 55.9% (5585/10000)
[Test]  Epoch: 98	Loss: 0.022063	Acc: 55.7% (5571/10000)
[Test]  Epoch: 99	Loss: 0.022001	Acc: 55.8% (5580/10000)
[Test]  Epoch: 100	Loss: 0.021956	Acc: 55.8% (5580/10000)
===========finish==========
['2024-08-18', '17:14:44.018155', '100', 'test', '0.0219562822163105', '55.8', '56.03']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.downsample.0.weight', 'bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.030299	Acc: 34.0% (3402/10000)
[Test]  Epoch: 2	Loss: 0.026613	Acc: 41.6% (4162/10000)
[Test]  Epoch: 3	Loss: 0.028018	Acc: 44.1% (4413/10000)
[Test]  Epoch: 4	Loss: 0.026328	Acc: 48.7% (4870/10000)
[Test]  Epoch: 5	Loss: 0.024910	Acc: 50.4% (5043/10000)
[Test]  Epoch: 6	Loss: 0.025035	Acc: 53.0% (5304/10000)
[Test]  Epoch: 7	Loss: 0.022828	Acc: 54.6% (5464/10000)
[Test]  Epoch: 8	Loss: 0.023722	Acc: 54.5% (5455/10000)
[Test]  Epoch: 9	Loss: 0.022195	Acc: 55.6% (5564/10000)
[Test]  Epoch: 10	Loss: 0.022387	Acc: 55.4% (5537/10000)
[Test]  Epoch: 11	Loss: 0.021671	Acc: 56.2% (5621/10000)
[Test]  Epoch: 12	Loss: 0.021664	Acc: 56.5% (5650/10000)
[Test]  Epoch: 13	Loss: 0.021793	Acc: 56.3% (5628/10000)
[Test]  Epoch: 14	Loss: 0.021696	Acc: 56.1% (5607/10000)
[Test]  Epoch: 15	Loss: 0.021332	Acc: 56.7% (5673/10000)
[Test]  Epoch: 16	Loss: 0.021067	Acc: 56.8% (5681/10000)
[Test]  Epoch: 17	Loss: 0.021011	Acc: 57.3% (5726/10000)
[Test]  Epoch: 18	Loss: 0.020615	Acc: 57.4% (5738/10000)
[Test]  Epoch: 19	Loss: 0.021112	Acc: 57.1% (5714/10000)
[Test]  Epoch: 20	Loss: 0.020805	Acc: 57.4% (5739/10000)
[Test]  Epoch: 21	Loss: 0.020598	Acc: 57.8% (5783/10000)
[Test]  Epoch: 22	Loss: 0.020500	Acc: 57.8% (5784/10000)
[Test]  Epoch: 23	Loss: 0.020785	Acc: 57.7% (5770/10000)
[Test]  Epoch: 24	Loss: 0.020779	Acc: 58.3% (5832/10000)
[Test]  Epoch: 25	Loss: 0.020941	Acc: 57.4% (5742/10000)
[Test]  Epoch: 26	Loss: 0.020839	Acc: 57.6% (5758/10000)
[Test]  Epoch: 27	Loss: 0.020857	Acc: 58.0% (5795/10000)
[Test]  Epoch: 28	Loss: 0.020560	Acc: 58.2% (5816/10000)
[Test]  Epoch: 29	Loss: 0.020893	Acc: 57.5% (5755/10000)
[Test]  Epoch: 30	Loss: 0.020639	Acc: 57.8% (5783/10000)
[Test]  Epoch: 31	Loss: 0.020632	Acc: 58.4% (5839/10000)
[Test]  Epoch: 32	Loss: 0.020651	Acc: 58.0% (5805/10000)
[Test]  Epoch: 33	Loss: 0.020744	Acc: 58.0% (5802/10000)
[Test]  Epoch: 34	Loss: 0.020494	Acc: 58.2% (5822/10000)
[Test]  Epoch: 35	Loss: 0.020588	Acc: 58.2% (5821/10000)
[Test]  Epoch: 36	Loss: 0.020833	Acc: 57.8% (5778/10000)
[Test]  Epoch: 37	Loss: 0.020854	Acc: 57.9% (5793/10000)
[Test]  Epoch: 38	Loss: 0.020888	Acc: 58.0% (5796/10000)
[Test]  Epoch: 39	Loss: 0.021158	Acc: 57.5% (5755/10000)
[Test]  Epoch: 40	Loss: 0.020724	Acc: 58.0% (5801/10000)
[Test]  Epoch: 41	Loss: 0.020839	Acc: 58.1% (5812/10000)
[Test]  Epoch: 42	Loss: 0.020982	Acc: 57.6% (5764/10000)
[Test]  Epoch: 43	Loss: 0.020681	Acc: 58.0% (5799/10000)
[Test]  Epoch: 44	Loss: 0.020803	Acc: 58.1% (5811/10000)
[Test]  Epoch: 45	Loss: 0.020547	Acc: 58.2% (5821/10000)
[Test]  Epoch: 46	Loss: 0.020647	Acc: 58.0% (5805/10000)
[Test]  Epoch: 47	Loss: 0.020725	Acc: 58.0% (5799/10000)
[Test]  Epoch: 48	Loss: 0.020753	Acc: 58.0% (5801/10000)
[Test]  Epoch: 49	Loss: 0.020774	Acc: 58.2% (5818/10000)
[Test]  Epoch: 50	Loss: 0.020624	Acc: 58.2% (5820/10000)
[Test]  Epoch: 51	Loss: 0.020516	Acc: 58.0% (5800/10000)
[Test]  Epoch: 52	Loss: 0.020705	Acc: 57.9% (5788/10000)
[Test]  Epoch: 53	Loss: 0.020563	Acc: 58.4% (5837/10000)
[Test]  Epoch: 54	Loss: 0.020486	Acc: 58.2% (5820/10000)
[Test]  Epoch: 55	Loss: 0.020434	Acc: 58.6% (5856/10000)
[Test]  Epoch: 56	Loss: 0.020579	Acc: 58.0% (5802/10000)
[Test]  Epoch: 57	Loss: 0.020691	Acc: 58.6% (5864/10000)
[Test]  Epoch: 58	Loss: 0.020517	Acc: 58.5% (5854/10000)
[Test]  Epoch: 59	Loss: 0.020604	Acc: 58.5% (5850/10000)
[Test]  Epoch: 60	Loss: 0.020765	Acc: 57.9% (5786/10000)
[Test]  Epoch: 61	Loss: 0.020593	Acc: 58.0% (5803/10000)
[Test]  Epoch: 62	Loss: 0.020556	Acc: 58.2% (5824/10000)
[Test]  Epoch: 63	Loss: 0.020517	Acc: 58.4% (5839/10000)
[Test]  Epoch: 64	Loss: 0.020508	Acc: 58.3% (5830/10000)
[Test]  Epoch: 65	Loss: 0.020532	Acc: 58.4% (5836/10000)
[Test]  Epoch: 66	Loss: 0.020481	Acc: 58.4% (5838/10000)
[Test]  Epoch: 67	Loss: 0.020529	Acc: 58.1% (5808/10000)
[Test]  Epoch: 68	Loss: 0.020512	Acc: 58.1% (5810/10000)
[Test]  Epoch: 69	Loss: 0.020508	Acc: 58.3% (5827/10000)
[Test]  Epoch: 70	Loss: 0.020466	Acc: 58.1% (5815/10000)
[Test]  Epoch: 71	Loss: 0.020508	Acc: 58.1% (5808/10000)
[Test]  Epoch: 72	Loss: 0.020531	Acc: 58.1% (5806/10000)
[Test]  Epoch: 73	Loss: 0.020533	Acc: 58.1% (5810/10000)
[Test]  Epoch: 74	Loss: 0.020491	Acc: 58.2% (5822/10000)
[Test]  Epoch: 75	Loss: 0.020503	Acc: 58.0% (5800/10000)
[Test]  Epoch: 76	Loss: 0.020512	Acc: 58.2% (5821/10000)
[Test]  Epoch: 77	Loss: 0.020506	Acc: 58.1% (5815/10000)
[Test]  Epoch: 78	Loss: 0.020529	Acc: 58.3% (5831/10000)
[Test]  Epoch: 79	Loss: 0.020566	Acc: 58.4% (5842/10000)
[Test]  Epoch: 80	Loss: 0.020539	Acc: 58.3% (5828/10000)
[Test]  Epoch: 81	Loss: 0.020525	Acc: 58.4% (5836/10000)
[Test]  Epoch: 82	Loss: 0.020554	Acc: 58.3% (5826/10000)
[Test]  Epoch: 83	Loss: 0.020531	Acc: 58.4% (5835/10000)
[Test]  Epoch: 84	Loss: 0.020550	Acc: 58.3% (5829/10000)
[Test]  Epoch: 85	Loss: 0.020567	Acc: 58.2% (5820/10000)
[Test]  Epoch: 86	Loss: 0.020580	Acc: 58.5% (5846/10000)
[Test]  Epoch: 87	Loss: 0.020559	Acc: 58.3% (5828/10000)
[Test]  Epoch: 88	Loss: 0.020559	Acc: 58.3% (5829/10000)
[Test]  Epoch: 89	Loss: 0.020560	Acc: 58.3% (5834/10000)
[Test]  Epoch: 90	Loss: 0.020551	Acc: 58.3% (5826/10000)
[Test]  Epoch: 91	Loss: 0.020550	Acc: 58.4% (5842/10000)
[Test]  Epoch: 92	Loss: 0.020591	Acc: 58.2% (5816/10000)
[Test]  Epoch: 93	Loss: 0.020597	Acc: 58.2% (5821/10000)
[Test]  Epoch: 94	Loss: 0.020544	Acc: 58.3% (5828/10000)
[Test]  Epoch: 95	Loss: 0.020541	Acc: 58.1% (5813/10000)
[Test]  Epoch: 96	Loss: 0.020543	Acc: 58.4% (5841/10000)
[Test]  Epoch: 97	Loss: 0.020608	Acc: 58.4% (5839/10000)
[Test]  Epoch: 98	Loss: 0.020644	Acc: 58.2% (5824/10000)
[Test]  Epoch: 99	Loss: 0.020581	Acc: 58.3% (5833/10000)
[Test]  Epoch: 100	Loss: 0.020569	Acc: 58.4% (5838/10000)
===========finish==========
['2024-08-18', '17:16:47.596597', '100', 'test', '0.020568787425756456', '58.38', '58.64']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.downsample.0.weight', 'bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn1.weight', 'last_linear.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.028093	Acc: 34.7% (3466/10000)
[Test]  Epoch: 2	Loss: 0.028418	Acc: 37.4% (3735/10000)
[Test]  Epoch: 3	Loss: 0.024080	Acc: 48.5% (4849/10000)
[Test]  Epoch: 4	Loss: 0.027315	Acc: 46.5% (4653/10000)
[Test]  Epoch: 5	Loss: 0.023954	Acc: 52.1% (5209/10000)
[Test]  Epoch: 6	Loss: 0.024136	Acc: 52.5% (5248/10000)
[Test]  Epoch: 7	Loss: 0.021741	Acc: 55.5% (5553/10000)
[Test]  Epoch: 8	Loss: 0.022006	Acc: 56.0% (5597/10000)
[Test]  Epoch: 9	Loss: 0.020611	Acc: 57.8% (5782/10000)
[Test]  Epoch: 10	Loss: 0.020471	Acc: 58.1% (5811/10000)
[Test]  Epoch: 11	Loss: 0.019606	Acc: 59.2% (5925/10000)
[Test]  Epoch: 12	Loss: 0.019201	Acc: 59.6% (5963/10000)
[Test]  Epoch: 13	Loss: 0.019361	Acc: 59.4% (5938/10000)
[Test]  Epoch: 14	Loss: 0.019332	Acc: 59.5% (5945/10000)
[Test]  Epoch: 15	Loss: 0.019198	Acc: 59.7% (5967/10000)
[Test]  Epoch: 16	Loss: 0.018996	Acc: 59.8% (5977/10000)
[Test]  Epoch: 17	Loss: 0.019018	Acc: 59.7% (5968/10000)
[Test]  Epoch: 18	Loss: 0.018686	Acc: 60.6% (6064/10000)
[Test]  Epoch: 19	Loss: 0.018690	Acc: 60.6% (6056/10000)
[Test]  Epoch: 20	Loss: 0.018500	Acc: 60.8% (6079/10000)
[Test]  Epoch: 21	Loss: 0.018352	Acc: 61.3% (6128/10000)
[Test]  Epoch: 22	Loss: 0.018512	Acc: 60.9% (6088/10000)
[Test]  Epoch: 23	Loss: 0.018609	Acc: 60.9% (6092/10000)
[Test]  Epoch: 24	Loss: 0.018398	Acc: 61.6% (6157/10000)
[Test]  Epoch: 25	Loss: 0.018955	Acc: 60.7% (6072/10000)
[Test]  Epoch: 26	Loss: 0.018469	Acc: 61.2% (6120/10000)
[Test]  Epoch: 27	Loss: 0.018459	Acc: 61.1% (6110/10000)
[Test]  Epoch: 28	Loss: 0.018576	Acc: 60.9% (6089/10000)
[Test]  Epoch: 29	Loss: 0.018699	Acc: 61.1% (6111/10000)
[Test]  Epoch: 30	Loss: 0.018637	Acc: 60.8% (6077/10000)
[Test]  Epoch: 31	Loss: 0.018655	Acc: 61.0% (6104/10000)
[Test]  Epoch: 32	Loss: 0.018706	Acc: 60.6% (6063/10000)
[Test]  Epoch: 33	Loss: 0.018308	Acc: 61.2% (6119/10000)
[Test]  Epoch: 34	Loss: 0.018242	Acc: 61.4% (6138/10000)
[Test]  Epoch: 35	Loss: 0.018510	Acc: 61.4% (6136/10000)
[Test]  Epoch: 36	Loss: 0.018482	Acc: 61.5% (6150/10000)
[Test]  Epoch: 37	Loss: 0.018230	Acc: 61.8% (6177/10000)
[Test]  Epoch: 38	Loss: 0.018322	Acc: 61.4% (6140/10000)
[Test]  Epoch: 39	Loss: 0.018813	Acc: 60.5% (6049/10000)
[Test]  Epoch: 40	Loss: 0.018467	Acc: 61.4% (6140/10000)
[Test]  Epoch: 41	Loss: 0.018412	Acc: 61.2% (6119/10000)
[Test]  Epoch: 42	Loss: 0.018466	Acc: 61.1% (6106/10000)
[Test]  Epoch: 43	Loss: 0.018379	Acc: 61.3% (6128/10000)
[Test]  Epoch: 44	Loss: 0.018379	Acc: 61.3% (6132/10000)
[Test]  Epoch: 45	Loss: 0.018373	Acc: 60.9% (6090/10000)
[Test]  Epoch: 46	Loss: 0.018685	Acc: 61.1% (6110/10000)
[Test]  Epoch: 47	Loss: 0.018140	Acc: 62.1% (6214/10000)
[Test]  Epoch: 48	Loss: 0.018404	Acc: 62.0% (6198/10000)
[Test]  Epoch: 49	Loss: 0.018549	Acc: 62.0% (6198/10000)
[Test]  Epoch: 50	Loss: 0.018349	Acc: 62.3% (6234/10000)
[Test]  Epoch: 51	Loss: 0.018420	Acc: 61.6% (6159/10000)
[Test]  Epoch: 52	Loss: 0.018317	Acc: 61.8% (6179/10000)
[Test]  Epoch: 53	Loss: 0.018284	Acc: 62.2% (6223/10000)
[Test]  Epoch: 54	Loss: 0.018370	Acc: 61.7% (6169/10000)
[Test]  Epoch: 55	Loss: 0.018142	Acc: 62.2% (6219/10000)
[Test]  Epoch: 56	Loss: 0.018367	Acc: 61.8% (6180/10000)
[Test]  Epoch: 57	Loss: 0.018151	Acc: 62.3% (6234/10000)
[Test]  Epoch: 58	Loss: 0.018253	Acc: 62.0% (6205/10000)
[Test]  Epoch: 59	Loss: 0.018355	Acc: 62.1% (6214/10000)
[Test]  Epoch: 60	Loss: 0.018456	Acc: 61.8% (6182/10000)
[Test]  Epoch: 61	Loss: 0.018342	Acc: 61.9% (6187/10000)
[Test]  Epoch: 62	Loss: 0.018288	Acc: 61.9% (6193/10000)
[Test]  Epoch: 63	Loss: 0.018286	Acc: 62.0% (6204/10000)
[Test]  Epoch: 64	Loss: 0.018254	Acc: 62.1% (6206/10000)
[Test]  Epoch: 65	Loss: 0.018216	Acc: 61.9% (6189/10000)
[Test]  Epoch: 66	Loss: 0.018239	Acc: 61.9% (6194/10000)
[Test]  Epoch: 67	Loss: 0.018286	Acc: 62.1% (6212/10000)
[Test]  Epoch: 68	Loss: 0.018224	Acc: 62.3% (6227/10000)
[Test]  Epoch: 69	Loss: 0.018218	Acc: 62.1% (6214/10000)
[Test]  Epoch: 70	Loss: 0.018178	Acc: 62.3% (6231/10000)
[Test]  Epoch: 71	Loss: 0.018213	Acc: 62.2% (6218/10000)
[Test]  Epoch: 72	Loss: 0.018221	Acc: 62.3% (6231/10000)
[Test]  Epoch: 73	Loss: 0.018233	Acc: 62.1% (6214/10000)
[Test]  Epoch: 74	Loss: 0.018168	Acc: 62.0% (6201/10000)
[Test]  Epoch: 75	Loss: 0.018150	Acc: 62.2% (6217/10000)
[Test]  Epoch: 76	Loss: 0.018211	Acc: 62.3% (6226/10000)
[Test]  Epoch: 77	Loss: 0.018219	Acc: 62.1% (6214/10000)
[Test]  Epoch: 78	Loss: 0.018274	Acc: 62.1% (6208/10000)
[Test]  Epoch: 79	Loss: 0.018246	Acc: 61.9% (6194/10000)
[Test]  Epoch: 80	Loss: 0.018259	Acc: 61.9% (6192/10000)
[Test]  Epoch: 81	Loss: 0.018220	Acc: 62.0% (6202/10000)
[Test]  Epoch: 82	Loss: 0.018238	Acc: 62.0% (6204/10000)
[Test]  Epoch: 83	Loss: 0.018208	Acc: 62.1% (6211/10000)
[Test]  Epoch: 84	Loss: 0.018197	Acc: 62.3% (6234/10000)
[Test]  Epoch: 85	Loss: 0.018207	Acc: 62.0% (6205/10000)
[Test]  Epoch: 86	Loss: 0.018240	Acc: 62.3% (6227/10000)
[Test]  Epoch: 87	Loss: 0.018196	Acc: 62.1% (6214/10000)
[Test]  Epoch: 88	Loss: 0.018231	Acc: 62.2% (6218/10000)
[Test]  Epoch: 89	Loss: 0.018232	Acc: 62.1% (6212/10000)
[Test]  Epoch: 90	Loss: 0.018225	Acc: 62.1% (6208/10000)
[Test]  Epoch: 91	Loss: 0.018178	Acc: 62.4% (6237/10000)
[Test]  Epoch: 92	Loss: 0.018244	Acc: 62.1% (6207/10000)
[Test]  Epoch: 93	Loss: 0.018242	Acc: 62.1% (6212/10000)
[Test]  Epoch: 94	Loss: 0.018181	Acc: 62.2% (6218/10000)
[Test]  Epoch: 95	Loss: 0.018171	Acc: 62.2% (6221/10000)
[Test]  Epoch: 96	Loss: 0.018169	Acc: 62.2% (6223/10000)
[Test]  Epoch: 97	Loss: 0.018222	Acc: 62.1% (6214/10000)
[Test]  Epoch: 98	Loss: 0.018265	Acc: 62.3% (6234/10000)
[Test]  Epoch: 99	Loss: 0.018230	Acc: 62.1% (6213/10000)
[Test]  Epoch: 100	Loss: 0.018188	Acc: 62.2% (6217/10000)
===========finish==========
['2024-08-18', '17:18:51.916016', '100', 'test', '0.018188401848077775', '62.17', '62.37']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.downsample.0.weight', 'bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn1.weight', 'last_linear.weight', 'layer2.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.027182	Acc: 36.8% (3684/10000)
[Test]  Epoch: 2	Loss: 0.025304	Acc: 46.1% (4609/10000)
[Test]  Epoch: 3	Loss: 0.022022	Acc: 52.3% (5229/10000)
[Test]  Epoch: 4	Loss: 0.021753	Acc: 55.3% (5533/10000)
[Test]  Epoch: 5	Loss: 0.022700	Acc: 53.6% (5362/10000)
[Test]  Epoch: 6	Loss: 0.020219	Acc: 57.6% (5763/10000)
[Test]  Epoch: 7	Loss: 0.019461	Acc: 59.5% (5955/10000)
[Test]  Epoch: 8	Loss: 0.019767	Acc: 59.3% (5934/10000)
[Test]  Epoch: 9	Loss: 0.018391	Acc: 60.9% (6090/10000)
[Test]  Epoch: 10	Loss: 0.020254	Acc: 59.7% (5967/10000)
[Test]  Epoch: 11	Loss: 0.018885	Acc: 60.5% (6051/10000)
[Test]  Epoch: 12	Loss: 0.018214	Acc: 62.0% (6201/10000)
[Test]  Epoch: 13	Loss: 0.017843	Acc: 62.6% (6260/10000)
[Test]  Epoch: 14	Loss: 0.018682	Acc: 61.3% (6128/10000)
[Test]  Epoch: 15	Loss: 0.018286	Acc: 62.2% (6224/10000)
[Test]  Epoch: 16	Loss: 0.017599	Acc: 63.0% (6296/10000)
[Test]  Epoch: 17	Loss: 0.017661	Acc: 62.9% (6289/10000)
[Test]  Epoch: 18	Loss: 0.017858	Acc: 62.3% (6228/10000)
[Test]  Epoch: 19	Loss: 0.017788	Acc: 62.6% (6256/10000)
[Test]  Epoch: 20	Loss: 0.017372	Acc: 63.4% (6336/10000)
[Test]  Epoch: 21	Loss: 0.017345	Acc: 63.6% (6357/10000)
[Test]  Epoch: 22	Loss: 0.017309	Acc: 63.2% (6316/10000)
[Test]  Epoch: 23	Loss: 0.017288	Acc: 63.5% (6348/10000)
[Test]  Epoch: 24	Loss: 0.017341	Acc: 63.5% (6354/10000)
[Test]  Epoch: 25	Loss: 0.017480	Acc: 63.3% (6332/10000)
[Test]  Epoch: 26	Loss: 0.017092	Acc: 64.0% (6401/10000)
[Test]  Epoch: 27	Loss: 0.017422	Acc: 63.3% (6334/10000)
[Test]  Epoch: 28	Loss: 0.017359	Acc: 63.5% (6350/10000)
[Test]  Epoch: 29	Loss: 0.017526	Acc: 63.0% (6305/10000)
[Test]  Epoch: 30	Loss: 0.017202	Acc: 63.6% (6357/10000)
[Test]  Epoch: 31	Loss: 0.017240	Acc: 63.9% (6392/10000)
[Test]  Epoch: 32	Loss: 0.017334	Acc: 63.5% (6346/10000)
[Test]  Epoch: 33	Loss: 0.017098	Acc: 64.1% (6409/10000)
[Test]  Epoch: 34	Loss: 0.017153	Acc: 64.5% (6452/10000)
[Test]  Epoch: 35	Loss: 0.017369	Acc: 63.6% (6365/10000)
[Test]  Epoch: 36	Loss: 0.017388	Acc: 63.6% (6363/10000)
[Test]  Epoch: 37	Loss: 0.017066	Acc: 63.8% (6384/10000)
[Test]  Epoch: 38	Loss: 0.017312	Acc: 63.7% (6372/10000)
[Test]  Epoch: 39	Loss: 0.017711	Acc: 63.0% (6304/10000)
[Test]  Epoch: 40	Loss: 0.017315	Acc: 64.0% (6396/10000)
[Test]  Epoch: 41	Loss: 0.017421	Acc: 63.6% (6363/10000)
[Test]  Epoch: 42	Loss: 0.017391	Acc: 63.9% (6389/10000)
[Test]  Epoch: 43	Loss: 0.017133	Acc: 64.1% (6408/10000)
[Test]  Epoch: 44	Loss: 0.017222	Acc: 64.1% (6406/10000)
[Test]  Epoch: 45	Loss: 0.017054	Acc: 64.4% (6436/10000)
[Test]  Epoch: 46	Loss: 0.017144	Acc: 64.4% (6442/10000)
[Test]  Epoch: 47	Loss: 0.017036	Acc: 64.2% (6418/10000)
[Test]  Epoch: 48	Loss: 0.017153	Acc: 64.0% (6403/10000)
[Test]  Epoch: 49	Loss: 0.017241	Acc: 64.2% (6416/10000)
[Test]  Epoch: 50	Loss: 0.017313	Acc: 64.0% (6402/10000)
[Test]  Epoch: 51	Loss: 0.017163	Acc: 64.0% (6403/10000)
[Test]  Epoch: 52	Loss: 0.017254	Acc: 63.7% (6367/10000)
[Test]  Epoch: 53	Loss: 0.017246	Acc: 63.9% (6389/10000)
[Test]  Epoch: 54	Loss: 0.017230	Acc: 63.6% (6360/10000)
[Test]  Epoch: 55	Loss: 0.017102	Acc: 64.2% (6418/10000)
[Test]  Epoch: 56	Loss: 0.017260	Acc: 63.7% (6367/10000)
[Test]  Epoch: 57	Loss: 0.017053	Acc: 64.3% (6431/10000)
[Test]  Epoch: 58	Loss: 0.017127	Acc: 64.0% (6397/10000)
[Test]  Epoch: 59	Loss: 0.017180	Acc: 63.9% (6394/10000)
[Test]  Epoch: 60	Loss: 0.017540	Acc: 63.5% (6351/10000)
[Test]  Epoch: 61	Loss: 0.017257	Acc: 63.8% (6378/10000)
[Test]  Epoch: 62	Loss: 0.017187	Acc: 64.0% (6396/10000)
[Test]  Epoch: 63	Loss: 0.017169	Acc: 63.8% (6384/10000)
[Test]  Epoch: 64	Loss: 0.017133	Acc: 63.9% (6393/10000)
[Test]  Epoch: 65	Loss: 0.017122	Acc: 64.0% (6405/10000)
[Test]  Epoch: 66	Loss: 0.017138	Acc: 64.0% (6400/10000)
[Test]  Epoch: 67	Loss: 0.017164	Acc: 63.9% (6388/10000)
[Test]  Epoch: 68	Loss: 0.017128	Acc: 64.1% (6409/10000)
[Test]  Epoch: 69	Loss: 0.017143	Acc: 64.0% (6396/10000)
[Test]  Epoch: 70	Loss: 0.017081	Acc: 63.9% (6394/10000)
[Test]  Epoch: 71	Loss: 0.017100	Acc: 64.0% (6403/10000)
[Test]  Epoch: 72	Loss: 0.017142	Acc: 63.9% (6392/10000)
[Test]  Epoch: 73	Loss: 0.017139	Acc: 64.0% (6401/10000)
[Test]  Epoch: 74	Loss: 0.017085	Acc: 64.0% (6400/10000)
[Test]  Epoch: 75	Loss: 0.017092	Acc: 64.0% (6399/10000)
[Test]  Epoch: 76	Loss: 0.017110	Acc: 63.9% (6393/10000)
[Test]  Epoch: 77	Loss: 0.017118	Acc: 64.0% (6403/10000)
[Test]  Epoch: 78	Loss: 0.017155	Acc: 64.0% (6396/10000)
[Test]  Epoch: 79	Loss: 0.017112	Acc: 64.1% (6409/10000)
[Test]  Epoch: 80	Loss: 0.017124	Acc: 64.1% (6410/10000)
[Test]  Epoch: 81	Loss: 0.017137	Acc: 64.1% (6410/10000)
[Test]  Epoch: 82	Loss: 0.017144	Acc: 64.0% (6398/10000)
[Test]  Epoch: 83	Loss: 0.017123	Acc: 64.2% (6416/10000)
[Test]  Epoch: 84	Loss: 0.017096	Acc: 64.0% (6405/10000)
[Test]  Epoch: 85	Loss: 0.017122	Acc: 64.1% (6408/10000)
[Test]  Epoch: 86	Loss: 0.017142	Acc: 64.1% (6406/10000)
[Test]  Epoch: 87	Loss: 0.017102	Acc: 64.2% (6415/10000)
[Test]  Epoch: 88	Loss: 0.017132	Acc: 64.0% (6398/10000)
[Test]  Epoch: 89	Loss: 0.017135	Acc: 64.1% (6408/10000)
[Test]  Epoch: 90	Loss: 0.017103	Acc: 64.0% (6404/10000)
[Test]  Epoch: 91	Loss: 0.017079	Acc: 64.1% (6412/10000)
[Test]  Epoch: 92	Loss: 0.017095	Acc: 64.0% (6400/10000)
[Test]  Epoch: 93	Loss: 0.017140	Acc: 63.9% (6390/10000)
[Test]  Epoch: 94	Loss: 0.017119	Acc: 64.0% (6402/10000)
[Test]  Epoch: 95	Loss: 0.017092	Acc: 64.0% (6402/10000)
[Test]  Epoch: 96	Loss: 0.017096	Acc: 64.2% (6417/10000)
[Test]  Epoch: 97	Loss: 0.017159	Acc: 64.0% (6395/10000)
[Test]  Epoch: 98	Loss: 0.017196	Acc: 64.0% (6401/10000)
[Test]  Epoch: 99	Loss: 0.017124	Acc: 64.1% (6412/10000)
[Test]  Epoch: 100	Loss: 0.017119	Acc: 64.0% (6399/10000)
===========finish==========
['2024-08-18', '17:20:54.530178', '100', 'test', '0.017119386571645738', '63.99', '64.52']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.downsample.0.weight', 'bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn1.weight', 'last_linear.weight', 'layer2.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.034084	Acc: 31.2% (3124/10000)
[Test]  Epoch: 2	Loss: 0.021742	Acc: 51.8% (5183/10000)
[Test]  Epoch: 3	Loss: 0.020600	Acc: 55.3% (5531/10000)
[Test]  Epoch: 4	Loss: 0.022727	Acc: 53.4% (5337/10000)
[Test]  Epoch: 5	Loss: 0.020989	Acc: 56.8% (5676/10000)
[Test]  Epoch: 6	Loss: 0.020493	Acc: 58.4% (5840/10000)
[Test]  Epoch: 7	Loss: 0.018371	Acc: 61.9% (6185/10000)
[Test]  Epoch: 8	Loss: 0.020985	Acc: 57.7% (5773/10000)
[Test]  Epoch: 9	Loss: 0.018021	Acc: 61.8% (6180/10000)
[Test]  Epoch: 10	Loss: 0.019173	Acc: 60.6% (6062/10000)
[Test]  Epoch: 11	Loss: 0.017516	Acc: 62.9% (6289/10000)
[Test]  Epoch: 12	Loss: 0.018950	Acc: 60.4% (6040/10000)
[Test]  Epoch: 13	Loss: 0.017594	Acc: 63.1% (6313/10000)
[Test]  Epoch: 14	Loss: 0.017346	Acc: 62.8% (6280/10000)
[Test]  Epoch: 15	Loss: 0.017264	Acc: 63.1% (6310/10000)
[Test]  Epoch: 16	Loss: 0.016926	Acc: 64.2% (6424/10000)
[Test]  Epoch: 17	Loss: 0.017019	Acc: 63.5% (6350/10000)
[Test]  Epoch: 18	Loss: 0.017506	Acc: 62.6% (6261/10000)
[Test]  Epoch: 19	Loss: 0.017062	Acc: 63.7% (6371/10000)
[Test]  Epoch: 20	Loss: 0.017277	Acc: 63.5% (6352/10000)
[Test]  Epoch: 21	Loss: 0.016870	Acc: 64.3% (6430/10000)
[Test]  Epoch: 22	Loss: 0.016964	Acc: 63.8% (6382/10000)
[Test]  Epoch: 23	Loss: 0.016900	Acc: 64.1% (6412/10000)
[Test]  Epoch: 24	Loss: 0.017010	Acc: 64.1% (6408/10000)
[Test]  Epoch: 25	Loss: 0.016851	Acc: 64.3% (6426/10000)
[Test]  Epoch: 26	Loss: 0.016816	Acc: 64.1% (6412/10000)
[Test]  Epoch: 27	Loss: 0.016794	Acc: 64.2% (6424/10000)
[Test]  Epoch: 28	Loss: 0.017597	Acc: 63.2% (6320/10000)
[Test]  Epoch: 29	Loss: 0.016986	Acc: 64.3% (6426/10000)
[Test]  Epoch: 30	Loss: 0.016765	Acc: 64.4% (6441/10000)
[Test]  Epoch: 31	Loss: 0.016768	Acc: 64.7% (6465/10000)
[Test]  Epoch: 32	Loss: 0.016788	Acc: 64.4% (6443/10000)
[Test]  Epoch: 33	Loss: 0.016624	Acc: 64.8% (6477/10000)
[Test]  Epoch: 34	Loss: 0.016703	Acc: 64.8% (6478/10000)
[Test]  Epoch: 35	Loss: 0.016772	Acc: 64.6% (6456/10000)
[Test]  Epoch: 36	Loss: 0.016629	Acc: 64.8% (6485/10000)
[Test]  Epoch: 37	Loss: 0.016742	Acc: 64.3% (6433/10000)
[Test]  Epoch: 38	Loss: 0.016823	Acc: 64.7% (6470/10000)
[Test]  Epoch: 39	Loss: 0.017005	Acc: 64.3% (6427/10000)
[Test]  Epoch: 40	Loss: 0.016895	Acc: 64.6% (6459/10000)
[Test]  Epoch: 41	Loss: 0.016807	Acc: 64.7% (6473/10000)
[Test]  Epoch: 42	Loss: 0.016729	Acc: 64.6% (6462/10000)
[Test]  Epoch: 43	Loss: 0.016750	Acc: 64.6% (6462/10000)
[Test]  Epoch: 44	Loss: 0.016594	Acc: 64.8% (6477/10000)
[Test]  Epoch: 45	Loss: 0.016849	Acc: 64.3% (6433/10000)
[Test]  Epoch: 46	Loss: 0.016953	Acc: 64.2% (6415/10000)
[Test]  Epoch: 47	Loss: 0.016593	Acc: 64.6% (6457/10000)
[Test]  Epoch: 48	Loss: 0.016611	Acc: 64.9% (6488/10000)
[Test]  Epoch: 49	Loss: 0.016691	Acc: 64.7% (6465/10000)
[Test]  Epoch: 50	Loss: 0.016779	Acc: 64.7% (6472/10000)
[Test]  Epoch: 51	Loss: 0.016660	Acc: 64.5% (6452/10000)
[Test]  Epoch: 52	Loss: 0.016624	Acc: 64.6% (6460/10000)
[Test]  Epoch: 53	Loss: 0.016683	Acc: 64.7% (6467/10000)
[Test]  Epoch: 54	Loss: 0.016599	Acc: 64.7% (6465/10000)
[Test]  Epoch: 55	Loss: 0.016579	Acc: 64.7% (6471/10000)
[Test]  Epoch: 56	Loss: 0.016646	Acc: 64.9% (6490/10000)
[Test]  Epoch: 57	Loss: 0.016492	Acc: 65.2% (6517/10000)
[Test]  Epoch: 58	Loss: 0.016539	Acc: 64.8% (6477/10000)
[Test]  Epoch: 59	Loss: 0.016540	Acc: 65.0% (6499/10000)
[Test]  Epoch: 60	Loss: 0.016676	Acc: 65.1% (6509/10000)
[Test]  Epoch: 61	Loss: 0.016590	Acc: 64.9% (6487/10000)
[Test]  Epoch: 62	Loss: 0.016547	Acc: 64.9% (6492/10000)
[Test]  Epoch: 63	Loss: 0.016546	Acc: 65.1% (6514/10000)
[Test]  Epoch: 64	Loss: 0.016500	Acc: 65.0% (6505/10000)
[Test]  Epoch: 65	Loss: 0.016476	Acc: 65.2% (6520/10000)
[Test]  Epoch: 66	Loss: 0.016494	Acc: 65.1% (6512/10000)
[Test]  Epoch: 67	Loss: 0.016590	Acc: 64.9% (6494/10000)
[Test]  Epoch: 68	Loss: 0.016536	Acc: 65.0% (6498/10000)
[Test]  Epoch: 69	Loss: 0.016523	Acc: 65.1% (6512/10000)
[Test]  Epoch: 70	Loss: 0.016505	Acc: 65.2% (6518/10000)
[Test]  Epoch: 71	Loss: 0.016522	Acc: 65.1% (6513/10000)
[Test]  Epoch: 72	Loss: 0.016559	Acc: 65.0% (6498/10000)
[Test]  Epoch: 73	Loss: 0.016557	Acc: 65.1% (6510/10000)
[Test]  Epoch: 74	Loss: 0.016478	Acc: 65.3% (6527/10000)
[Test]  Epoch: 75	Loss: 0.016547	Acc: 65.2% (6520/10000)
[Test]  Epoch: 76	Loss: 0.016526	Acc: 65.0% (6497/10000)
[Test]  Epoch: 77	Loss: 0.016479	Acc: 65.4% (6537/10000)
[Test]  Epoch: 78	Loss: 0.016500	Acc: 65.1% (6506/10000)
[Test]  Epoch: 79	Loss: 0.016485	Acc: 65.2% (6519/10000)
[Test]  Epoch: 80	Loss: 0.016507	Acc: 65.1% (6508/10000)
[Test]  Epoch: 81	Loss: 0.016517	Acc: 65.2% (6523/10000)
[Test]  Epoch: 82	Loss: 0.016529	Acc: 65.1% (6512/10000)
[Test]  Epoch: 83	Loss: 0.016518	Acc: 65.1% (6509/10000)
[Test]  Epoch: 84	Loss: 0.016540	Acc: 65.0% (6502/10000)
[Test]  Epoch: 85	Loss: 0.016508	Acc: 65.1% (6510/10000)
[Test]  Epoch: 86	Loss: 0.016524	Acc: 65.1% (6514/10000)
[Test]  Epoch: 87	Loss: 0.016538	Acc: 65.1% (6508/10000)
[Test]  Epoch: 88	Loss: 0.016550	Acc: 65.1% (6511/10000)
[Test]  Epoch: 89	Loss: 0.016532	Acc: 65.1% (6507/10000)
[Test]  Epoch: 90	Loss: 0.016498	Acc: 65.2% (6523/10000)
[Test]  Epoch: 91	Loss: 0.016512	Acc: 65.2% (6520/10000)
[Test]  Epoch: 92	Loss: 0.016515	Acc: 65.1% (6513/10000)
[Test]  Epoch: 93	Loss: 0.016541	Acc: 65.2% (6520/10000)
[Test]  Epoch: 94	Loss: 0.016529	Acc: 65.1% (6509/10000)
[Test]  Epoch: 95	Loss: 0.016474	Acc: 65.2% (6520/10000)
[Test]  Epoch: 96	Loss: 0.016456	Acc: 65.2% (6517/10000)
[Test]  Epoch: 97	Loss: 0.016518	Acc: 65.3% (6527/10000)
[Test]  Epoch: 98	Loss: 0.016614	Acc: 65.1% (6508/10000)
[Test]  Epoch: 99	Loss: 0.016502	Acc: 65.3% (6528/10000)
[Test]  Epoch: 100	Loss: 0.016501	Acc: 65.2% (6523/10000)
===========finish==========
['2024-08-18', '17:22:56.458993', '100', 'test', '0.01650103403925896', '65.23', '65.37']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.downsample.0.weight', 'bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn1.weight', 'last_linear.weight', 'layer2.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'layer3.0.downsample.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.035156	Acc: 28.4% (2835/10000)
[Test]  Epoch: 2	Loss: 0.020800	Acc: 51.6% (5163/10000)
[Test]  Epoch: 3	Loss: 0.021064	Acc: 55.0% (5503/10000)
[Test]  Epoch: 4	Loss: 0.020885	Acc: 55.8% (5581/10000)
[Test]  Epoch: 5	Loss: 0.020058	Acc: 57.7% (5771/10000)
[Test]  Epoch: 6	Loss: 0.019763	Acc: 58.9% (5893/10000)
[Test]  Epoch: 7	Loss: 0.019148	Acc: 60.1% (6014/10000)
[Test]  Epoch: 8	Loss: 0.018747	Acc: 61.1% (6110/10000)
[Test]  Epoch: 9	Loss: 0.018100	Acc: 61.9% (6194/10000)
[Test]  Epoch: 10	Loss: 0.021163	Acc: 57.3% (5731/10000)
[Test]  Epoch: 11	Loss: 0.017664	Acc: 62.5% (6247/10000)
[Test]  Epoch: 12	Loss: 0.017784	Acc: 62.6% (6258/10000)
[Test]  Epoch: 13	Loss: 0.017377	Acc: 63.2% (6323/10000)
[Test]  Epoch: 14	Loss: 0.017566	Acc: 62.6% (6260/10000)
[Test]  Epoch: 15	Loss: 0.017359	Acc: 63.4% (6335/10000)
[Test]  Epoch: 16	Loss: 0.017215	Acc: 63.8% (6383/10000)
[Test]  Epoch: 17	Loss: 0.017351	Acc: 63.5% (6349/10000)
[Test]  Epoch: 18	Loss: 0.017128	Acc: 63.8% (6378/10000)
[Test]  Epoch: 19	Loss: 0.017006	Acc: 64.1% (6411/10000)
[Test]  Epoch: 20	Loss: 0.016917	Acc: 64.2% (6415/10000)
[Test]  Epoch: 21	Loss: 0.017020	Acc: 64.2% (6425/10000)
[Test]  Epoch: 22	Loss: 0.017196	Acc: 63.8% (6380/10000)
[Test]  Epoch: 23	Loss: 0.017005	Acc: 64.4% (6441/10000)
[Test]  Epoch: 24	Loss: 0.017429	Acc: 64.1% (6409/10000)
[Test]  Epoch: 25	Loss: 0.017124	Acc: 64.3% (6431/10000)
[Test]  Epoch: 26	Loss: 0.016865	Acc: 64.3% (6430/10000)
[Test]  Epoch: 27	Loss: 0.016892	Acc: 64.5% (6454/10000)
[Test]  Epoch: 28	Loss: 0.016948	Acc: 64.5% (6450/10000)
[Test]  Epoch: 29	Loss: 0.016813	Acc: 64.6% (6463/10000)
[Test]  Epoch: 30	Loss: 0.016718	Acc: 64.8% (6481/10000)
[Test]  Epoch: 31	Loss: 0.016738	Acc: 64.9% (6494/10000)
[Test]  Epoch: 32	Loss: 0.017155	Acc: 64.3% (6432/10000)
[Test]  Epoch: 33	Loss: 0.016673	Acc: 64.7% (6471/10000)
[Test]  Epoch: 34	Loss: 0.016771	Acc: 65.1% (6508/10000)
[Test]  Epoch: 35	Loss: 0.016838	Acc: 64.7% (6467/10000)
[Test]  Epoch: 36	Loss: 0.016801	Acc: 64.7% (6474/10000)
[Test]  Epoch: 37	Loss: 0.016723	Acc: 65.1% (6508/10000)
[Test]  Epoch: 38	Loss: 0.018655	Acc: 61.5% (6148/10000)
[Test]  Epoch: 39	Loss: 0.017504	Acc: 63.5% (6354/10000)
[Test]  Epoch: 40	Loss: 0.017372	Acc: 64.0% (6403/10000)
[Test]  Epoch: 41	Loss: 0.017103	Acc: 64.7% (6465/10000)
[Test]  Epoch: 42	Loss: 0.016980	Acc: 64.6% (6463/10000)
[Test]  Epoch: 43	Loss: 0.016930	Acc: 64.6% (6464/10000)
[Test]  Epoch: 44	Loss: 0.016890	Acc: 64.9% (6489/10000)
[Test]  Epoch: 45	Loss: 0.016995	Acc: 64.6% (6463/10000)
[Test]  Epoch: 46	Loss: 0.016911	Acc: 64.8% (6485/10000)
[Test]  Epoch: 47	Loss: 0.016804	Acc: 65.1% (6514/10000)
[Test]  Epoch: 48	Loss: 0.016915	Acc: 65.2% (6520/10000)
[Test]  Epoch: 49	Loss: 0.016980	Acc: 64.9% (6488/10000)
[Test]  Epoch: 50	Loss: 0.017145	Acc: 64.5% (6454/10000)
[Test]  Epoch: 51	Loss: 0.016864	Acc: 64.9% (6490/10000)
[Test]  Epoch: 52	Loss: 0.016918	Acc: 64.8% (6483/10000)
[Test]  Epoch: 53	Loss: 0.016971	Acc: 64.9% (6487/10000)
[Test]  Epoch: 54	Loss: 0.016888	Acc: 64.7% (6467/10000)
[Test]  Epoch: 55	Loss: 0.016897	Acc: 64.7% (6471/10000)
[Test]  Epoch: 56	Loss: 0.017019	Acc: 64.4% (6442/10000)
[Test]  Epoch: 57	Loss: 0.016819	Acc: 64.9% (6489/10000)
[Test]  Epoch: 58	Loss: 0.016893	Acc: 65.2% (6519/10000)
[Test]  Epoch: 59	Loss: 0.016976	Acc: 64.6% (6464/10000)
[Test]  Epoch: 60	Loss: 0.017103	Acc: 64.8% (6476/10000)
[Test]  Epoch: 61	Loss: 0.016995	Acc: 65.0% (6498/10000)
[Test]  Epoch: 62	Loss: 0.016962	Acc: 64.9% (6490/10000)
[Test]  Epoch: 63	Loss: 0.016953	Acc: 64.9% (6490/10000)
[Test]  Epoch: 64	Loss: 0.016924	Acc: 65.2% (6515/10000)
[Test]  Epoch: 65	Loss: 0.016873	Acc: 65.1% (6512/10000)
[Test]  Epoch: 66	Loss: 0.016884	Acc: 65.0% (6503/10000)
[Test]  Epoch: 67	Loss: 0.016969	Acc: 64.8% (6485/10000)
[Test]  Epoch: 68	Loss: 0.016912	Acc: 65.1% (6506/10000)
[Test]  Epoch: 69	Loss: 0.016901	Acc: 65.0% (6499/10000)
[Test]  Epoch: 70	Loss: 0.016853	Acc: 65.0% (6502/10000)
[Test]  Epoch: 71	Loss: 0.016886	Acc: 65.0% (6500/10000)
[Test]  Epoch: 72	Loss: 0.016947	Acc: 65.0% (6505/10000)
[Test]  Epoch: 73	Loss: 0.016918	Acc: 64.9% (6487/10000)
[Test]  Epoch: 74	Loss: 0.016831	Acc: 65.2% (6521/10000)
[Test]  Epoch: 75	Loss: 0.016858	Acc: 65.0% (6497/10000)
[Test]  Epoch: 76	Loss: 0.016924	Acc: 65.0% (6500/10000)
[Test]  Epoch: 77	Loss: 0.016860	Acc: 65.1% (6506/10000)
[Test]  Epoch: 78	Loss: 0.016936	Acc: 65.1% (6514/10000)
[Test]  Epoch: 79	Loss: 0.016905	Acc: 65.1% (6512/10000)
[Test]  Epoch: 80	Loss: 0.016900	Acc: 65.0% (6498/10000)
[Test]  Epoch: 81	Loss: 0.016888	Acc: 65.0% (6497/10000)
[Test]  Epoch: 82	Loss: 0.016887	Acc: 65.0% (6504/10000)
[Test]  Epoch: 83	Loss: 0.016879	Acc: 65.1% (6510/10000)
[Test]  Epoch: 84	Loss: 0.016915	Acc: 65.0% (6502/10000)
[Test]  Epoch: 85	Loss: 0.016868	Acc: 65.1% (6509/10000)
[Test]  Epoch: 86	Loss: 0.016889	Acc: 65.2% (6519/10000)
[Test]  Epoch: 87	Loss: 0.016895	Acc: 65.1% (6509/10000)
[Test]  Epoch: 88	Loss: 0.016913	Acc: 65.1% (6509/10000)
[Test]  Epoch: 89	Loss: 0.016893	Acc: 65.0% (6498/10000)
[Test]  Epoch: 90	Loss: 0.016877	Acc: 65.1% (6512/10000)
[Test]  Epoch: 91	Loss: 0.016871	Acc: 65.0% (6498/10000)
[Test]  Epoch: 92	Loss: 0.016885	Acc: 65.0% (6495/10000)
[Test]  Epoch: 93	Loss: 0.016930	Acc: 65.0% (6502/10000)
[Test]  Epoch: 94	Loss: 0.016880	Acc: 65.2% (6515/10000)
[Test]  Epoch: 95	Loss: 0.016852	Acc: 65.1% (6508/10000)
[Test]  Epoch: 96	Loss: 0.016819	Acc: 65.0% (6504/10000)
[Test]  Epoch: 97	Loss: 0.016901	Acc: 65.0% (6501/10000)
[Test]  Epoch: 98	Loss: 0.016974	Acc: 64.9% (6494/10000)
[Test]  Epoch: 99	Loss: 0.016843	Acc: 65.1% (6511/10000)
[Test]  Epoch: 100	Loss: 0.016852	Acc: 65.0% (6502/10000)
===========finish==========
['2024-08-18', '17:25:00.652502', '100', 'test', '0.016852336978912353', '65.02', '65.21']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.downsample.0.weight', 'bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn1.weight', 'last_linear.weight', 'layer2.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer4.1.bn1.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.036711	Acc: 26.4% (2645/10000)
[Test]  Epoch: 2	Loss: 0.022134	Acc: 50.4% (5039/10000)
[Test]  Epoch: 3	Loss: 0.020617	Acc: 54.5% (5454/10000)
[Test]  Epoch: 4	Loss: 0.020054	Acc: 57.1% (5708/10000)
[Test]  Epoch: 5	Loss: 0.019140	Acc: 59.6% (5964/10000)
[Test]  Epoch: 6	Loss: 0.018881	Acc: 60.3% (6026/10000)
[Test]  Epoch: 7	Loss: 0.018365	Acc: 60.9% (6092/10000)
[Test]  Epoch: 8	Loss: 0.018473	Acc: 61.4% (6140/10000)
[Test]  Epoch: 9	Loss: 0.018420	Acc: 61.2% (6118/10000)
[Test]  Epoch: 10	Loss: 0.018363	Acc: 61.2% (6119/10000)
[Test]  Epoch: 11	Loss: 0.017877	Acc: 62.2% (6217/10000)
[Test]  Epoch: 12	Loss: 0.017814	Acc: 62.4% (6243/10000)
[Test]  Epoch: 13	Loss: 0.017534	Acc: 62.9% (6288/10000)
[Test]  Epoch: 14	Loss: 0.017870	Acc: 62.0% (6203/10000)
[Test]  Epoch: 15	Loss: 0.017731	Acc: 62.4% (6241/10000)
[Test]  Epoch: 16	Loss: 0.017693	Acc: 63.0% (6296/10000)
[Test]  Epoch: 17	Loss: 0.017605	Acc: 62.9% (6288/10000)
[Test]  Epoch: 18	Loss: 0.017470	Acc: 63.1% (6309/10000)
[Test]  Epoch: 19	Loss: 0.017528	Acc: 62.8% (6282/10000)
[Test]  Epoch: 20	Loss: 0.017628	Acc: 63.1% (6309/10000)
[Test]  Epoch: 21	Loss: 0.017327	Acc: 63.6% (6363/10000)
[Test]  Epoch: 22	Loss: 0.017304	Acc: 63.5% (6347/10000)
[Test]  Epoch: 23	Loss: 0.017210	Acc: 63.6% (6362/10000)
[Test]  Epoch: 24	Loss: 0.017112	Acc: 64.5% (6446/10000)
[Test]  Epoch: 25	Loss: 0.017342	Acc: 63.7% (6374/10000)
[Test]  Epoch: 26	Loss: 0.017207	Acc: 63.8% (6382/10000)
[Test]  Epoch: 27	Loss: 0.017337	Acc: 63.3% (6331/10000)
[Test]  Epoch: 28	Loss: 0.017205	Acc: 63.8% (6380/10000)
[Test]  Epoch: 29	Loss: 0.017311	Acc: 63.6% (6356/10000)
[Test]  Epoch: 30	Loss: 0.017043	Acc: 64.0% (6395/10000)
[Test]  Epoch: 31	Loss: 0.017044	Acc: 63.9% (6386/10000)
[Test]  Epoch: 32	Loss: 0.017247	Acc: 63.8% (6383/10000)
[Test]  Epoch: 33	Loss: 0.017157	Acc: 63.9% (6393/10000)
[Test]  Epoch: 34	Loss: 0.017278	Acc: 64.1% (6413/10000)
[Test]  Epoch: 35	Loss: 0.017383	Acc: 63.7% (6368/10000)
[Test]  Epoch: 36	Loss: 0.017290	Acc: 63.9% (6392/10000)
[Test]  Epoch: 37	Loss: 0.017300	Acc: 64.0% (6401/10000)
[Test]  Epoch: 38	Loss: 0.017371	Acc: 63.8% (6379/10000)
[Test]  Epoch: 39	Loss: 0.017581	Acc: 63.8% (6382/10000)
[Test]  Epoch: 40	Loss: 0.017342	Acc: 64.1% (6406/10000)
[Test]  Epoch: 41	Loss: 0.017281	Acc: 63.9% (6392/10000)
[Test]  Epoch: 42	Loss: 0.017530	Acc: 63.4% (6337/10000)
[Test]  Epoch: 43	Loss: 0.017410	Acc: 64.0% (6404/10000)
[Test]  Epoch: 44	Loss: 0.017253	Acc: 64.0% (6397/10000)
[Test]  Epoch: 45	Loss: 0.017267	Acc: 63.9% (6390/10000)
[Test]  Epoch: 46	Loss: 0.017392	Acc: 63.8% (6379/10000)
[Test]  Epoch: 47	Loss: 0.017131	Acc: 63.9% (6388/10000)
[Test]  Epoch: 48	Loss: 0.017272	Acc: 63.9% (6387/10000)
[Test]  Epoch: 49	Loss: 0.017472	Acc: 63.8% (6382/10000)
[Test]  Epoch: 50	Loss: 0.017597	Acc: 63.4% (6338/10000)
[Test]  Epoch: 51	Loss: 0.017521	Acc: 63.4% (6343/10000)
[Test]  Epoch: 52	Loss: 0.017439	Acc: 63.7% (6373/10000)
[Test]  Epoch: 53	Loss: 0.017450	Acc: 63.9% (6389/10000)
[Test]  Epoch: 54	Loss: 0.017333	Acc: 64.1% (6412/10000)
[Test]  Epoch: 55	Loss: 0.017270	Acc: 64.0% (6402/10000)
[Test]  Epoch: 56	Loss: 0.017439	Acc: 63.6% (6363/10000)
[Test]  Epoch: 57	Loss: 0.017309	Acc: 64.3% (6431/10000)
[Test]  Epoch: 58	Loss: 0.017381	Acc: 64.0% (6399/10000)
[Test]  Epoch: 59	Loss: 0.017468	Acc: 63.9% (6385/10000)
[Test]  Epoch: 60	Loss: 0.017491	Acc: 63.9% (6392/10000)
[Test]  Epoch: 61	Loss: 0.017414	Acc: 64.1% (6412/10000)
[Test]  Epoch: 62	Loss: 0.017401	Acc: 64.0% (6403/10000)
[Test]  Epoch: 63	Loss: 0.017426	Acc: 64.0% (6403/10000)
[Test]  Epoch: 64	Loss: 0.017410	Acc: 64.1% (6406/10000)
[Test]  Epoch: 65	Loss: 0.017358	Acc: 64.1% (6410/10000)
[Test]  Epoch: 66	Loss: 0.017360	Acc: 64.2% (6416/10000)
[Test]  Epoch: 67	Loss: 0.017419	Acc: 63.9% (6389/10000)
[Test]  Epoch: 68	Loss: 0.017357	Acc: 64.0% (6405/10000)
[Test]  Epoch: 69	Loss: 0.017395	Acc: 64.1% (6407/10000)
[Test]  Epoch: 70	Loss: 0.017353	Acc: 64.1% (6413/10000)
[Test]  Epoch: 71	Loss: 0.017393	Acc: 63.8% (6384/10000)
[Test]  Epoch: 72	Loss: 0.017432	Acc: 63.8% (6383/10000)
[Test]  Epoch: 73	Loss: 0.017367	Acc: 63.9% (6392/10000)
[Test]  Epoch: 74	Loss: 0.017323	Acc: 64.0% (6401/10000)
[Test]  Epoch: 75	Loss: 0.017363	Acc: 64.0% (6398/10000)
[Test]  Epoch: 76	Loss: 0.017346	Acc: 64.0% (6399/10000)
[Test]  Epoch: 77	Loss: 0.017348	Acc: 64.0% (6401/10000)
[Test]  Epoch: 78	Loss: 0.017388	Acc: 64.0% (6395/10000)
[Test]  Epoch: 79	Loss: 0.017340	Acc: 64.0% (6395/10000)
[Test]  Epoch: 80	Loss: 0.017384	Acc: 64.0% (6405/10000)
[Test]  Epoch: 81	Loss: 0.017381	Acc: 63.8% (6382/10000)
[Test]  Epoch: 82	Loss: 0.017375	Acc: 63.9% (6386/10000)
[Test]  Epoch: 83	Loss: 0.017341	Acc: 64.2% (6421/10000)
[Test]  Epoch: 84	Loss: 0.017375	Acc: 64.1% (6407/10000)
[Test]  Epoch: 85	Loss: 0.017372	Acc: 63.8% (6384/10000)
[Test]  Epoch: 86	Loss: 0.017366	Acc: 64.1% (6414/10000)
[Test]  Epoch: 87	Loss: 0.017382	Acc: 64.0% (6401/10000)
[Test]  Epoch: 88	Loss: 0.017420	Acc: 63.9% (6388/10000)
[Test]  Epoch: 89	Loss: 0.017392	Acc: 63.9% (6393/10000)
[Test]  Epoch: 90	Loss: 0.017381	Acc: 64.1% (6409/10000)
[Test]  Epoch: 91	Loss: 0.017332	Acc: 64.1% (6411/10000)
[Test]  Epoch: 92	Loss: 0.017344	Acc: 63.9% (6393/10000)
[Test]  Epoch: 93	Loss: 0.017386	Acc: 64.1% (6406/10000)
[Test]  Epoch: 94	Loss: 0.017364	Acc: 64.0% (6397/10000)
[Test]  Epoch: 95	Loss: 0.017320	Acc: 63.9% (6393/10000)
[Test]  Epoch: 96	Loss: 0.017295	Acc: 63.9% (6387/10000)
[Test]  Epoch: 97	Loss: 0.017370	Acc: 63.9% (6387/10000)
[Test]  Epoch: 98	Loss: 0.017430	Acc: 63.9% (6393/10000)
[Test]  Epoch: 99	Loss: 0.017325	Acc: 64.0% (6402/10000)
[Test]  Epoch: 100	Loss: 0.017335	Acc: 64.0% (6400/10000)
===========finish==========
['2024-08-18', '17:27:09.993296', '100', 'test', '0.017334768879413605', '64.0', '64.46']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-resnet18-channel resnet18 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['layer3.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.conv1.weight', 'layer2.1.conv1.weight', 'conv1.weight', 'layer1.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv2.weight', 'layer1.0.conv1.weight', 'layer1.1.conv2.weight', 'layer1.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer4.0.downsample.0.weight', 'bn1.weight', 'layer2.0.bn2.weight', 'layer2.1.bn1.weight', 'layer3.0.bn1.weight', 'layer4.0.conv1.weight', 'layer2.0.bn1.weight', 'last_linear.weight', 'layer2.0.downsample.1.weight', 'layer1.1.bn1.weight', 'layer3.0.bn2.weight', 'layer1.0.bn2.weight', 'layer2.1.bn2.weight', 'layer3.0.downsample.0.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.0.conv2.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'layer3.0.downsample.1.weight', 'layer4.0.bn1.weight', 'layer4.1.bn1.weight', 'layer3.1.conv2.weight', 'layer4.0.downsample.1.weight', 'layer4.1.bn2.weight', 'layer3.1.conv1.weight', 'layer3.1.bn2.weight', 'layer4.0.bn2.weight', 'layer3.1.bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.170901	Acc: 10.9% (1095/10000)
[Test]  Epoch: 2	Loss: 389.803607	Acc: 10.0% (1000/10000)
[Test]  Epoch: 3	Loss: 0.041311	Acc: 10.9% (1094/10000)
[Test]  Epoch: 4	Loss: 0.063863	Acc: 18.1% (1811/10000)
[Test]  Epoch: 5	Loss: 0.050258	Acc: 19.9% (1989/10000)
[Test]  Epoch: 6	Loss: 0.279368	Acc: 21.7% (2167/10000)
[Test]  Epoch: 7	Loss: 0.035717	Acc: 28.2% (2823/10000)
[Test]  Epoch: 8	Loss: 0.039889	Acc: 27.1% (2706/10000)
[Test]  Epoch: 9	Loss: 0.045175	Acc: 27.4% (2739/10000)
[Test]  Epoch: 10	Loss: 0.033042	Acc: 27.1% (2711/10000)
[Test]  Epoch: 11	Loss: 0.031697	Acc: 31.0% (3101/10000)
[Test]  Epoch: 12	Loss: 0.040626	Acc: 34.1% (3414/10000)
[Test]  Epoch: 13	Loss: 0.032298	Acc: 33.8% (3376/10000)
[Test]  Epoch: 14	Loss: 0.033701	Acc: 35.0% (3495/10000)
[Test]  Epoch: 15	Loss: 0.034571	Acc: 34.6% (3461/10000)
[Test]  Epoch: 16	Loss: 0.043097	Acc: 33.5% (3352/10000)
[Test]  Epoch: 17	Loss: 0.032997	Acc: 37.2% (3720/10000)
[Test]  Epoch: 18	Loss: 0.036013	Acc: 33.0% (3296/10000)
[Test]  Epoch: 19	Loss: 0.034942	Acc: 36.6% (3659/10000)
[Test]  Epoch: 20	Loss: 0.094444	Acc: 31.5% (3146/10000)
[Test]  Epoch: 21	Loss: 0.039395	Acc: 29.3% (2929/10000)
[Test]  Epoch: 22	Loss: 0.034777	Acc: 35.0% (3498/10000)
[Test]  Epoch: 23	Loss: 0.036052	Acc: 35.2% (3522/10000)
[Test]  Epoch: 24	Loss: 0.039244	Acc: 36.9% (3691/10000)
[Test]  Epoch: 25	Loss: 0.037924	Acc: 35.6% (3558/10000)
[Test]  Epoch: 26	Loss: 0.037898	Acc: 37.4% (3740/10000)
[Test]  Epoch: 27	Loss: 0.036889	Acc: 36.7% (3671/10000)
[Test]  Epoch: 28	Loss: 0.046536	Acc: 35.7% (3571/10000)
[Test]  Epoch: 29	Loss: 0.036486	Acc: 39.4% (3944/10000)
[Test]  Epoch: 30	Loss: 0.044383	Acc: 35.1% (3509/10000)
[Test]  Epoch: 31	Loss: 0.039401	Acc: 39.1% (3913/10000)
[Test]  Epoch: 32	Loss: 0.040112	Acc: 39.8% (3982/10000)
[Test]  Epoch: 33	Loss: 0.038875	Acc: 40.8% (4080/10000)
[Test]  Epoch: 34	Loss: 0.041028	Acc: 38.9% (3892/10000)
[Test]  Epoch: 35	Loss: 0.037573	Acc: 41.4% (4139/10000)
[Test]  Epoch: 36	Loss: 0.036614	Acc: 42.1% (4214/10000)
[Test]  Epoch: 37	Loss: 0.043271	Acc: 38.5% (3855/10000)
[Test]  Epoch: 38	Loss: 0.059372	Acc: 36.6% (3661/10000)
[Test]  Epoch: 39	Loss: 0.041976	Acc: 39.8% (3980/10000)
[Test]  Epoch: 40	Loss: 0.037973	Acc: 41.9% (4186/10000)
[Test]  Epoch: 41	Loss: 0.037561	Acc: 42.2% (4222/10000)
[Test]  Epoch: 42	Loss: 0.041228	Acc: 37.3% (3726/10000)
[Test]  Epoch: 43	Loss: 0.038724	Acc: 40.4% (4041/10000)
[Test]  Epoch: 44	Loss: 0.046625	Acc: 41.4% (4140/10000)
[Test]  Epoch: 45	Loss: 0.042695	Acc: 42.2% (4218/10000)
[Test]  Epoch: 46	Loss: 0.037533	Acc: 42.1% (4209/10000)
[Test]  Epoch: 47	Loss: 0.035109	Acc: 42.2% (4222/10000)
[Test]  Epoch: 48	Loss: 0.043955	Acc: 42.1% (4206/10000)
[Test]  Epoch: 49	Loss: 0.039111	Acc: 41.9% (4186/10000)
[Test]  Epoch: 50	Loss: 0.037770	Acc: 42.3% (4231/10000)
[Test]  Epoch: 51	Loss: 0.037227	Acc: 42.8% (4275/10000)
[Test]  Epoch: 52	Loss: 0.038638	Acc: 42.2% (4216/10000)
[Test]  Epoch: 53	Loss: 0.039518	Acc: 43.1% (4314/10000)
[Test]  Epoch: 54	Loss: 0.040422	Acc: 43.2% (4318/10000)
[Test]  Epoch: 55	Loss: 0.043512	Acc: 41.5% (4145/10000)
[Test]  Epoch: 56	Loss: 0.039998	Acc: 42.2% (4221/10000)
[Test]  Epoch: 57	Loss: 0.037612	Acc: 42.5% (4251/10000)
[Test]  Epoch: 58	Loss: 0.040186	Acc: 42.3% (4234/10000)
[Test]  Epoch: 59	Loss: 0.039185	Acc: 43.0% (4300/10000)
[Test]  Epoch: 60	Loss: 0.042802	Acc: 42.4% (4242/10000)
[Test]  Epoch: 61	Loss: 0.038674	Acc: 42.9% (4291/10000)
[Test]  Epoch: 62	Loss: 0.037530	Acc: 43.0% (4303/10000)
[Test]  Epoch: 63	Loss: 0.038293	Acc: 43.1% (4307/10000)
[Test]  Epoch: 64	Loss: 0.037628	Acc: 43.2% (4319/10000)
[Test]  Epoch: 65	Loss: 0.037940	Acc: 43.2% (4322/10000)
[Test]  Epoch: 66	Loss: 0.037638	Acc: 43.2% (4323/10000)
[Test]  Epoch: 67	Loss: 0.038544	Acc: 43.3% (4334/10000)
[Test]  Epoch: 68	Loss: 0.038026	Acc: 43.2% (4317/10000)
[Test]  Epoch: 69	Loss: 0.037862	Acc: 43.5% (4346/10000)
[Test]  Epoch: 70	Loss: 0.037601	Acc: 43.6% (4360/10000)
[Test]  Epoch: 71	Loss: 0.037801	Acc: 43.4% (4340/10000)
[Test]  Epoch: 72	Loss: 0.036726	Acc: 43.3% (4327/10000)
[Test]  Epoch: 73	Loss: 0.038904	Acc: 43.4% (4340/10000)
[Test]  Epoch: 74	Loss: 0.037856	Acc: 43.5% (4346/10000)
[Test]  Epoch: 75	Loss: 0.037238	Acc: 43.3% (4334/10000)
[Test]  Epoch: 76	Loss: 0.040295	Acc: 43.1% (4312/10000)
[Test]  Epoch: 77	Loss: 0.038358	Acc: 43.2% (4323/10000)
[Test]  Epoch: 78	Loss: 0.037390	Acc: 43.4% (4343/10000)
[Test]  Epoch: 79	Loss: 0.037961	Acc: 43.4% (4339/10000)
[Test]  Epoch: 80	Loss: 0.038468	Acc: 43.3% (4332/10000)
[Test]  Epoch: 81	Loss: 0.037913	Acc: 43.1% (4307/10000)
[Test]  Epoch: 82	Loss: 0.037575	Acc: 43.3% (4329/10000)
[Test]  Epoch: 83	Loss: 0.037035	Acc: 43.5% (4350/10000)
[Test]  Epoch: 84	Loss: 0.037256	Acc: 43.5% (4355/10000)
[Test]  Epoch: 85	Loss: 0.037405	Acc: 43.5% (4354/10000)
[Test]  Epoch: 86	Loss: 0.038779	Acc: 43.4% (4336/10000)
[Test]  Epoch: 87	Loss: 0.038937	Acc: 43.2% (4325/10000)
[Test]  Epoch: 88	Loss: 0.039800	Acc: 43.5% (4348/10000)
[Test]  Epoch: 89	Loss: 0.037434	Acc: 43.5% (4354/10000)
[Test]  Epoch: 90	Loss: 0.036724	Acc: 43.4% (4340/10000)
[Test]  Epoch: 91	Loss: 0.038965	Acc: 43.3% (4329/10000)
[Test]  Epoch: 92	Loss: 0.038544	Acc: 43.3% (4334/10000)
[Test]  Epoch: 93	Loss: 0.036909	Acc: 43.5% (4349/10000)
[Test]  Epoch: 94	Loss: 0.037713	Acc: 43.7% (4372/10000)
[Test]  Epoch: 95	Loss: 0.038221	Acc: 43.5% (4345/10000)
[Test]  Epoch: 96	Loss: 0.038088	Acc: 43.5% (4351/10000)
[Test]  Epoch: 97	Loss: 0.039106	Acc: 43.4% (4339/10000)
[Test]  Epoch: 98	Loss: 0.039102	Acc: 43.3% (4332/10000)
[Test]  Epoch: 99	Loss: 0.039690	Acc: 43.3% (4330/10000)
[Test]  Epoch: 100	Loss: 0.037741	Acc: 43.5% (4355/10000)
===========finish==========
['2024-08-18', '17:29:17.850400', '100', 'test', '0.03774069429636002', '43.55', '43.72']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-vgg16_bn/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('features.14.weight', 37.039459228515625), ('features.17.weight', 34.163047790527344), ('features.20.weight', 26.6197509765625), ('features.10.weight', 26.077749252319336), ('features.7.weight', 21.536518096923828), ('features.24.weight', 17.302988052368164), ('features.3.weight', 17.10669708251953), ('features.27.weight', 7.762469291687012), ('features.0.weight', 5.131490707397461), ('features.30.weight', 4.365870475769043), ('features.11.weight', 2.9411582946777344), ('features.34.weight', 2.8689284324645996), ('features.21.weight', 2.2353477478027344), ('features.15.weight', 2.2321677207946777), ('features.37.weight', 2.0077996253967285), ('features.18.weight', 1.9947340488433838), ('features.40.weight', 1.8346385955810547), ('features.4.weight', 1.7528544664382935), ('classifier.weight', 1.5057778358459473), ('features.8.weight', 1.4587770700454712), ('features.1.weight', 0.4725804328918457), ('features.25.weight', 0.3220002353191376), ('features.28.weight', 0.14242592453956604), ('features.41.weight', 0.11260149627923965), ('features.31.weight', 0.10417690873146057), ('features.35.weight', 0.047189563512802124), ('features.38.weight', 0.04665716364979744), ('features.0.bias', 0.0), ('features.1.bias', 0.0), ('features.3.bias', 0.0), ('features.4.bias', 0.0), ('features.7.bias', 0.0), ('features.8.bias', 0.0), ('features.10.bias', 0.0), ('features.11.bias', 0.0), ('features.14.bias', 0.0), ('features.15.bias', 0.0), ('features.17.bias', 0.0), ('features.18.bias', 0.0), ('features.20.bias', 0.0), ('features.21.bias', 0.0), ('features.24.bias', 0.0), ('features.25.bias', 0.0), ('features.27.bias', 0.0), ('features.28.bias', 0.0), ('features.30.bias', 0.0), ('features.31.bias', 0.0), ('features.34.bias', 0.0), ('features.35.bias', 0.0), ('features.37.bias', 0.0), ('features.38.bias', 0.0), ('features.40.bias', 0.0), ('features.41.bias', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('features.14.weight', 37.039459228515625), ('features.17.weight', 34.163047790527344), ('features.20.weight', 26.6197509765625), ('features.10.weight', 26.077749252319336), ('features.7.weight', 21.536518096923828), ('features.24.weight', 17.302988052368164), ('features.3.weight', 17.10669708251953), ('features.27.weight', 7.762469291687012), ('features.0.weight', 5.131490707397461), ('features.30.weight', 4.365870475769043), ('features.11.weight', 2.9411582946777344), ('features.34.weight', 2.8689284324645996), ('features.21.weight', 2.2353477478027344), ('features.15.weight', 2.2321677207946777), ('features.37.weight', 2.0077996253967285), ('features.18.weight', 1.9947340488433838), ('features.40.weight', 1.8346385955810547), ('features.4.weight', 1.7528544664382935), ('classifier.weight', 1.5057778358459473), ('features.8.weight', 1.4587770700454712), ('features.1.weight', 0.4725804328918457), ('features.25.weight', 0.3220002353191376), ('features.28.weight', 0.14242592453956604), ('features.41.weight', 0.11260149627923965), ('features.31.weight', 0.10417690873146057), ('features.35.weight', 0.047189563512802124), ('features.38.weight', 0.04665716364979744), ('features.0.bias', 0.0), ('features.1.bias', 0.0), ('features.3.bias', 0.0), ('features.4.bias', 0.0), ('features.7.bias', 0.0), ('features.8.bias', 0.0), ('features.10.bias', 0.0), ('features.11.bias', 0.0), ('features.14.bias', 0.0), ('features.15.bias', 0.0), ('features.17.bias', 0.0), ('features.18.bias', 0.0), ('features.20.bias', 0.0), ('features.21.bias', 0.0), ('features.24.bias', 0.0), ('features.25.bias', 0.0), ('features.27.bias', 0.0), ('features.28.bias', 0.0), ('features.30.bias', 0.0), ('features.31.bias', 0.0), ('features.34.bias', 0.0), ('features.35.bias', 0.0), ('features.37.bias', 0.0), ('features.38.bias', 0.0), ('features.40.bias', 0.0), ('features.41.bias', 0.0), ('classifier.bias', 0.0)]
--------------------------------------------
get_sample_layers n=27  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.008345	Acc: 82.2% (8222/10000)
[Test]  Epoch: 2	Loss: 0.006351	Acc: 86.7% (8669/10000)
[Test]  Epoch: 3	Loss: 0.006403	Acc: 86.8% (8682/10000)
[Test]  Epoch: 4	Loss: 0.007469	Acc: 85.2% (8520/10000)
[Test]  Epoch: 5	Loss: 0.008000	Acc: 84.8% (8478/10000)
[Test]  Epoch: 6	Loss: 0.008809	Acc: 83.5% (8349/10000)
[Test]  Epoch: 7	Loss: 0.007360	Acc: 86.1% (8614/10000)
[Test]  Epoch: 8	Loss: 0.010217	Acc: 82.0% (8202/10000)
[Test]  Epoch: 9	Loss: 0.007527	Acc: 86.2% (8625/10000)
[Test]  Epoch: 10	Loss: 0.008352	Acc: 85.6% (8557/10000)
[Test]  Epoch: 11	Loss: 0.008395	Acc: 85.0% (8504/10000)
[Test]  Epoch: 12	Loss: 0.007637	Acc: 86.4% (8637/10000)
[Test]  Epoch: 13	Loss: 0.008080	Acc: 85.5% (8549/10000)
[Test]  Epoch: 14	Loss: 0.007970	Acc: 85.6% (8563/10000)
[Test]  Epoch: 15	Loss: 0.008372	Acc: 85.7% (8568/10000)
[Test]  Epoch: 16	Loss: 0.008340	Acc: 86.0% (8601/10000)
[Test]  Epoch: 17	Loss: 0.007994	Acc: 86.3% (8633/10000)
[Test]  Epoch: 18	Loss: 0.008494	Acc: 85.7% (8568/10000)
[Test]  Epoch: 19	Loss: 0.007884	Acc: 86.7% (8668/10000)
[Test]  Epoch: 20	Loss: 0.009321	Acc: 85.0% (8499/10000)
[Test]  Epoch: 21	Loss: 0.008776	Acc: 85.8% (8579/10000)
[Test]  Epoch: 22	Loss: 0.009813	Acc: 83.9% (8388/10000)
[Test]  Epoch: 23	Loss: 0.010688	Acc: 82.8% (8277/10000)
[Test]  Epoch: 24	Loss: 0.010126	Acc: 84.1% (8413/10000)
[Test]  Epoch: 25	Loss: 0.009385	Acc: 85.0% (8495/10000)
[Test]  Epoch: 26	Loss: 0.008973	Acc: 85.5% (8548/10000)
[Test]  Epoch: 27	Loss: 0.009368	Acc: 84.8% (8485/10000)
[Test]  Epoch: 28	Loss: 0.011142	Acc: 82.5% (8253/10000)
[Test]  Epoch: 29	Loss: 0.010667	Acc: 83.5% (8349/10000)
[Test]  Epoch: 30	Loss: 0.009526	Acc: 84.9% (8494/10000)
[Test]  Epoch: 31	Loss: 0.009142	Acc: 85.5% (8545/10000)
[Test]  Epoch: 32	Loss: 0.010188	Acc: 84.3% (8433/10000)
[Test]  Epoch: 33	Loss: 0.010183	Acc: 84.0% (8402/10000)
[Test]  Epoch: 34	Loss: 0.009074	Acc: 85.4% (8542/10000)
[Test]  Epoch: 35	Loss: 0.008420	Acc: 86.3% (8633/10000)
[Test]  Epoch: 36	Loss: 0.008459	Acc: 86.4% (8636/10000)
[Test]  Epoch: 37	Loss: 0.008780	Acc: 86.4% (8640/10000)
[Test]  Epoch: 38	Loss: 0.009792	Acc: 85.0% (8495/10000)
[Test]  Epoch: 39	Loss: 0.010221	Acc: 84.2% (8417/10000)
[Test]  Epoch: 40	Loss: 0.008823	Acc: 86.2% (8619/10000)
[Test]  Epoch: 41	Loss: 0.008526	Acc: 86.4% (8638/10000)
[Test]  Epoch: 42	Loss: 0.008440	Acc: 86.1% (8607/10000)
[Test]  Epoch: 43	Loss: 0.010048	Acc: 84.7% (8465/10000)
[Test]  Epoch: 44	Loss: 0.009269	Acc: 85.4% (8542/10000)
[Test]  Epoch: 45	Loss: 0.008748	Acc: 86.1% (8606/10000)
[Test]  Epoch: 46	Loss: 0.008487	Acc: 86.2% (8621/10000)
[Test]  Epoch: 47	Loss: 0.008626	Acc: 86.1% (8607/10000)
[Test]  Epoch: 48	Loss: 0.008763	Acc: 86.3% (8634/10000)
[Test]  Epoch: 49	Loss: 0.008507	Acc: 86.3% (8635/10000)
[Test]  Epoch: 50	Loss: 0.008622	Acc: 86.3% (8633/10000)
[Test]  Epoch: 51	Loss: 0.008419	Acc: 86.2% (8620/10000)
[Test]  Epoch: 52	Loss: 0.008527	Acc: 86.2% (8623/10000)
[Test]  Epoch: 53	Loss: 0.008383	Acc: 86.2% (8625/10000)
[Test]  Epoch: 54	Loss: 0.008542	Acc: 86.4% (8641/10000)
[Test]  Epoch: 55	Loss: 0.008999	Acc: 85.7% (8572/10000)
[Test]  Epoch: 56	Loss: 0.008628	Acc: 86.2% (8619/10000)
[Test]  Epoch: 57	Loss: 0.008337	Acc: 87.0% (8703/10000)
[Test]  Epoch: 58	Loss: 0.008463	Acc: 86.7% (8668/10000)
[Test]  Epoch: 59	Loss: 0.008519	Acc: 86.0% (8603/10000)
[Test]  Epoch: 60	Loss: 0.008754	Acc: 85.7% (8565/10000)
[Test]  Epoch: 61	Loss: 0.008303	Acc: 86.6% (8663/10000)
[Test]  Epoch: 62	Loss: 0.008655	Acc: 86.3% (8629/10000)
[Test]  Epoch: 63	Loss: 0.008584	Acc: 86.6% (8661/10000)
[Test]  Epoch: 64	Loss: 0.008490	Acc: 86.3% (8633/10000)
[Test]  Epoch: 65	Loss: 0.008363	Acc: 86.3% (8627/10000)
[Test]  Epoch: 66	Loss: 0.008465	Acc: 86.1% (8612/10000)
[Test]  Epoch: 67	Loss: 0.008536	Acc: 86.4% (8639/10000)
[Test]  Epoch: 68	Loss: 0.008504	Acc: 86.2% (8621/10000)
[Test]  Epoch: 69	Loss: 0.008313	Acc: 86.6% (8657/10000)
[Test]  Epoch: 70	Loss: 0.008486	Acc: 86.5% (8652/10000)
[Test]  Epoch: 71	Loss: 0.008343	Acc: 86.5% (8652/10000)
[Test]  Epoch: 72	Loss: 0.008417	Acc: 86.3% (8627/10000)
[Test]  Epoch: 73	Loss: 0.008554	Acc: 86.1% (8611/10000)
[Test]  Epoch: 74	Loss: 0.008697	Acc: 86.2% (8620/10000)
[Test]  Epoch: 75	Loss: 0.008382	Acc: 86.8% (8677/10000)
[Test]  Epoch: 76	Loss: 0.008412	Acc: 86.6% (8661/10000)
[Test]  Epoch: 77	Loss: 0.008504	Acc: 86.5% (8648/10000)
[Test]  Epoch: 78	Loss: 0.008350	Acc: 86.4% (8637/10000)
[Test]  Epoch: 79	Loss: 0.008505	Acc: 86.2% (8620/10000)
[Test]  Epoch: 80	Loss: 0.008546	Acc: 86.3% (8631/10000)
[Test]  Epoch: 81	Loss: 0.008454	Acc: 86.8% (8675/10000)
[Test]  Epoch: 82	Loss: 0.008422	Acc: 86.4% (8640/10000)
[Test]  Epoch: 83	Loss: 0.008329	Acc: 86.6% (8660/10000)
[Test]  Epoch: 84	Loss: 0.008309	Acc: 86.8% (8681/10000)
[Test]  Epoch: 85	Loss: 0.008321	Acc: 86.5% (8647/10000)
[Test]  Epoch: 86	Loss: 0.008388	Acc: 86.7% (8669/10000)
[Test]  Epoch: 87	Loss: 0.008393	Acc: 86.6% (8662/10000)
[Test]  Epoch: 88	Loss: 0.008388	Acc: 86.7% (8668/10000)
[Test]  Epoch: 89	Loss: 0.008388	Acc: 86.6% (8661/10000)
[Test]  Epoch: 90	Loss: 0.008315	Acc: 86.4% (8642/10000)
[Test]  Epoch: 91	Loss: 0.008358	Acc: 86.5% (8647/10000)
[Test]  Epoch: 92	Loss: 0.008440	Acc: 86.6% (8662/10000)
[Test]  Epoch: 93	Loss: 0.008081	Acc: 87.1% (8709/10000)
[Test]  Epoch: 94	Loss: 0.008331	Acc: 86.4% (8641/10000)
[Test]  Epoch: 95	Loss: 0.008313	Acc: 86.6% (8656/10000)
[Test]  Epoch: 96	Loss: 0.008165	Acc: 86.8% (8676/10000)
[Test]  Epoch: 97	Loss: 0.008323	Acc: 86.5% (8652/10000)
[Test]  Epoch: 98	Loss: 0.008502	Acc: 86.5% (8654/10000)
[Test]  Epoch: 99	Loss: 0.008304	Acc: 86.6% (8659/10000)
[Test]  Epoch: 100	Loss: 0.008288	Acc: 86.9% (8693/10000)
===========finish==========
['2024-08-18', '17:32:34.244535', '100', 'test', '0.008288244690746068', '86.93', '87.09']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.14.weight', 'features.17.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.043280	Acc: 35.6% (3563/10000)
[Test]  Epoch: 2	Loss: 0.023223	Acc: 49.8% (4976/10000)
[Test]  Epoch: 3	Loss: 0.017668	Acc: 61.4% (6137/10000)
[Test]  Epoch: 4	Loss: 0.023292	Acc: 54.4% (5436/10000)
[Test]  Epoch: 5	Loss: 0.023387	Acc: 54.4% (5444/10000)
[Test]  Epoch: 6	Loss: 0.019500	Acc: 62.9% (6292/10000)
[Test]  Epoch: 7	Loss: 0.020403	Acc: 61.6% (6161/10000)
[Test]  Epoch: 8	Loss: 0.022322	Acc: 61.2% (6116/10000)
[Test]  Epoch: 9	Loss: 0.019567	Acc: 64.7% (6468/10000)
[Test]  Epoch: 10	Loss: 0.021080	Acc: 63.0% (6303/10000)
[Test]  Epoch: 11	Loss: 0.021653	Acc: 63.4% (6336/10000)
[Test]  Epoch: 12	Loss: 0.020356	Acc: 65.1% (6509/10000)
[Test]  Epoch: 13	Loss: 0.018249	Acc: 67.8% (6776/10000)
[Test]  Epoch: 14	Loss: 0.023738	Acc: 63.2% (6322/10000)
[Test]  Epoch: 15	Loss: 0.019633	Acc: 67.6% (6761/10000)
[Test]  Epoch: 16	Loss: 0.022339	Acc: 66.6% (6659/10000)
[Test]  Epoch: 17	Loss: 0.026282	Acc: 61.5% (6149/10000)
[Test]  Epoch: 18	Loss: 0.024730	Acc: 62.2% (6225/10000)
[Test]  Epoch: 19	Loss: 0.021568	Acc: 66.1% (6610/10000)
[Test]  Epoch: 20	Loss: 0.027227	Acc: 62.3% (6231/10000)
[Test]  Epoch: 21	Loss: 0.024553	Acc: 64.8% (6478/10000)
[Test]  Epoch: 22	Loss: 0.025808	Acc: 63.7% (6372/10000)
[Test]  Epoch: 23	Loss: 0.020890	Acc: 69.1% (6908/10000)
[Test]  Epoch: 24	Loss: 0.022514	Acc: 67.1% (6709/10000)
[Test]  Epoch: 25	Loss: 0.021594	Acc: 67.9% (6787/10000)
[Test]  Epoch: 26	Loss: 0.019007	Acc: 70.3% (7026/10000)
[Test]  Epoch: 27	Loss: 0.019521	Acc: 70.2% (7021/10000)
[Test]  Epoch: 28	Loss: 0.019172	Acc: 70.1% (7007/10000)
[Test]  Epoch: 29	Loss: 0.021345	Acc: 67.9% (6791/10000)
[Test]  Epoch: 30	Loss: 0.017750	Acc: 71.8% (7181/10000)
[Test]  Epoch: 31	Loss: 0.017263	Acc: 73.2% (7322/10000)
[Test]  Epoch: 32	Loss: 0.017822	Acc: 72.4% (7240/10000)
[Test]  Epoch: 33	Loss: 0.019599	Acc: 71.0% (7097/10000)
[Test]  Epoch: 34	Loss: 0.017255	Acc: 73.5% (7349/10000)
[Test]  Epoch: 35	Loss: 0.025303	Acc: 64.6% (6464/10000)
[Test]  Epoch: 36	Loss: 0.020052	Acc: 70.5% (7052/10000)
[Test]  Epoch: 37	Loss: 0.017681	Acc: 73.2% (7317/10000)
[Test]  Epoch: 38	Loss: 0.018690	Acc: 72.4% (7239/10000)
[Test]  Epoch: 39	Loss: 0.019948	Acc: 70.4% (7042/10000)
[Test]  Epoch: 40	Loss: 0.028548	Acc: 62.7% (6266/10000)
[Test]  Epoch: 41	Loss: 0.030086	Acc: 62.5% (6249/10000)
[Test]  Epoch: 42	Loss: 0.032751	Acc: 58.5% (5851/10000)
[Test]  Epoch: 43	Loss: 0.025419	Acc: 65.7% (6572/10000)
[Test]  Epoch: 44	Loss: 0.025453	Acc: 64.7% (6474/10000)
[Test]  Epoch: 45	Loss: 0.019601	Acc: 70.1% (7014/10000)
[Test]  Epoch: 46	Loss: 0.018510	Acc: 71.6% (7161/10000)
[Test]  Epoch: 47	Loss: 0.019100	Acc: 70.6% (7062/10000)
[Test]  Epoch: 48	Loss: 0.019799	Acc: 70.8% (7075/10000)
[Test]  Epoch: 49	Loss: 0.019606	Acc: 70.6% (7056/10000)
[Test]  Epoch: 50	Loss: 0.018071	Acc: 72.6% (7263/10000)
[Test]  Epoch: 51	Loss: 0.018687	Acc: 72.0% (7201/10000)
[Test]  Epoch: 52	Loss: 0.018124	Acc: 72.2% (7215/10000)
[Test]  Epoch: 53	Loss: 0.017988	Acc: 72.8% (7278/10000)
[Test]  Epoch: 54	Loss: 0.019035	Acc: 72.0% (7196/10000)
[Test]  Epoch: 55	Loss: 0.017922	Acc: 73.4% (7344/10000)
[Test]  Epoch: 56	Loss: 0.017706	Acc: 73.2% (7319/10000)
[Test]  Epoch: 57	Loss: 0.017949	Acc: 73.9% (7392/10000)
[Test]  Epoch: 58	Loss: 0.019826	Acc: 71.2% (7124/10000)
[Test]  Epoch: 59	Loss: 0.017760	Acc: 73.4% (7337/10000)
[Test]  Epoch: 60	Loss: 0.017580	Acc: 74.3% (7428/10000)
[Test]  Epoch: 61	Loss: 0.017380	Acc: 74.0% (7399/10000)
[Test]  Epoch: 62	Loss: 0.017428	Acc: 74.0% (7401/10000)
[Test]  Epoch: 63	Loss: 0.017304	Acc: 74.2% (7421/10000)
[Test]  Epoch: 64	Loss: 0.017151	Acc: 74.8% (7480/10000)
[Test]  Epoch: 65	Loss: 0.017059	Acc: 74.0% (7400/10000)
[Test]  Epoch: 66	Loss: 0.017090	Acc: 74.3% (7428/10000)
[Test]  Epoch: 67	Loss: 0.016809	Acc: 74.6% (7457/10000)
[Test]  Epoch: 68	Loss: 0.016893	Acc: 74.5% (7452/10000)
[Test]  Epoch: 69	Loss: 0.016724	Acc: 74.9% (7494/10000)
[Test]  Epoch: 70	Loss: 0.016834	Acc: 74.9% (7489/10000)
[Test]  Epoch: 71	Loss: 0.016959	Acc: 74.7% (7472/10000)
[Test]  Epoch: 72	Loss: 0.017295	Acc: 74.7% (7471/10000)
[Test]  Epoch: 73	Loss: 0.017117	Acc: 74.4% (7443/10000)
[Test]  Epoch: 74	Loss: 0.016813	Acc: 74.7% (7472/10000)
[Test]  Epoch: 75	Loss: 0.016906	Acc: 74.7% (7467/10000)
[Test]  Epoch: 76	Loss: 0.016751	Acc: 74.8% (7475/10000)
[Test]  Epoch: 77	Loss: 0.017049	Acc: 74.5% (7447/10000)
[Test]  Epoch: 78	Loss: 0.017086	Acc: 74.4% (7443/10000)
[Test]  Epoch: 79	Loss: 0.016748	Acc: 75.3% (7528/10000)
[Test]  Epoch: 80	Loss: 0.016886	Acc: 74.8% (7484/10000)
[Test]  Epoch: 81	Loss: 0.016807	Acc: 74.6% (7462/10000)
[Test]  Epoch: 82	Loss: 0.017054	Acc: 74.6% (7463/10000)
[Test]  Epoch: 83	Loss: 0.016708	Acc: 75.1% (7507/10000)
[Test]  Epoch: 84	Loss: 0.017000	Acc: 74.8% (7481/10000)
[Test]  Epoch: 85	Loss: 0.016821	Acc: 75.0% (7501/10000)
[Test]  Epoch: 86	Loss: 0.016855	Acc: 74.8% (7475/10000)
[Test]  Epoch: 87	Loss: 0.016669	Acc: 75.0% (7505/10000)
[Test]  Epoch: 88	Loss: 0.016612	Acc: 75.1% (7512/10000)
[Test]  Epoch: 89	Loss: 0.016808	Acc: 74.8% (7483/10000)
[Test]  Epoch: 90	Loss: 0.016681	Acc: 74.9% (7487/10000)
[Test]  Epoch: 91	Loss: 0.016506	Acc: 75.0% (7496/10000)
[Test]  Epoch: 92	Loss: 0.016841	Acc: 74.8% (7484/10000)
[Test]  Epoch: 93	Loss: 0.016424	Acc: 75.5% (7555/10000)
[Test]  Epoch: 94	Loss: 0.016919	Acc: 74.8% (7480/10000)
[Test]  Epoch: 95	Loss: 0.016692	Acc: 74.8% (7481/10000)
[Test]  Epoch: 96	Loss: 0.016722	Acc: 75.0% (7495/10000)
[Test]  Epoch: 97	Loss: 0.016707	Acc: 75.1% (7511/10000)
[Test]  Epoch: 98	Loss: 0.016744	Acc: 75.1% (7513/10000)
[Test]  Epoch: 99	Loss: 0.016740	Acc: 75.2% (7522/10000)
[Test]  Epoch: 100	Loss: 0.016879	Acc: 74.8% (7476/10000)
===========finish==========
['2024-08-18', '17:35:10.125402', '100', 'test', '0.0168791225284338', '74.76', '75.55']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.066682	Acc: 19.1% (1915/10000)
[Test]  Epoch: 2	Loss: 0.033174	Acc: 26.1% (2614/10000)
[Test]  Epoch: 3	Loss: 0.032933	Acc: 28.1% (2812/10000)
[Test]  Epoch: 4	Loss: 0.027102	Acc: 36.6% (3661/10000)
[Test]  Epoch: 5	Loss: 0.024370	Acc: 44.9% (4491/10000)
[Test]  Epoch: 6	Loss: 0.026090	Acc: 42.5% (4249/10000)
[Test]  Epoch: 7	Loss: 0.026309	Acc: 43.9% (4392/10000)
[Test]  Epoch: 8	Loss: 0.028982	Acc: 41.0% (4098/10000)
[Test]  Epoch: 9	Loss: 0.024211	Acc: 49.4% (4940/10000)
[Test]  Epoch: 10	Loss: 0.038333	Acc: 35.8% (3578/10000)
[Test]  Epoch: 11	Loss: 0.026332	Acc: 48.8% (4876/10000)
[Test]  Epoch: 12	Loss: 0.021454	Acc: 55.5% (5546/10000)
[Test]  Epoch: 13	Loss: 0.026292	Acc: 49.2% (4922/10000)
[Test]  Epoch: 14	Loss: 0.022348	Acc: 54.6% (5464/10000)
[Test]  Epoch: 15	Loss: 0.030106	Acc: 45.8% (4582/10000)
[Test]  Epoch: 16	Loss: 0.028735	Acc: 50.5% (5050/10000)
[Test]  Epoch: 17	Loss: 0.025765	Acc: 54.5% (5455/10000)
[Test]  Epoch: 18	Loss: 0.024732	Acc: 54.2% (5421/10000)
[Test]  Epoch: 19	Loss: 0.030071	Acc: 50.0% (4995/10000)
[Test]  Epoch: 20	Loss: 0.034233	Acc: 49.2% (4924/10000)
[Test]  Epoch: 21	Loss: 0.031773	Acc: 52.0% (5197/10000)
[Test]  Epoch: 22	Loss: 0.028740	Acc: 53.1% (5311/10000)
[Test]  Epoch: 23	Loss: 0.030338	Acc: 51.2% (5119/10000)
[Test]  Epoch: 24	Loss: 0.034948	Acc: 48.0% (4803/10000)
[Test]  Epoch: 25	Loss: 0.030240	Acc: 55.5% (5554/10000)
[Test]  Epoch: 26	Loss: 0.025250	Acc: 58.8% (5884/10000)
[Test]  Epoch: 27	Loss: 0.025929	Acc: 58.3% (5832/10000)
[Test]  Epoch: 28	Loss: 0.026435	Acc: 56.1% (5609/10000)
[Test]  Epoch: 29	Loss: 0.033705	Acc: 54.5% (5454/10000)
[Test]  Epoch: 30	Loss: 0.027573	Acc: 57.9% (5787/10000)
[Test]  Epoch: 31	Loss: 0.026808	Acc: 58.4% (5843/10000)
[Test]  Epoch: 32	Loss: 0.029255	Acc: 56.1% (5613/10000)
[Test]  Epoch: 33	Loss: 0.029734	Acc: 57.3% (5733/10000)
[Test]  Epoch: 34	Loss: 0.029890	Acc: 56.5% (5650/10000)
[Test]  Epoch: 35	Loss: 0.027837	Acc: 59.0% (5897/10000)
[Test]  Epoch: 36	Loss: 0.027691	Acc: 60.5% (6046/10000)
[Test]  Epoch: 37	Loss: 0.026825	Acc: 61.6% (6164/10000)
[Test]  Epoch: 38	Loss: 0.026980	Acc: 60.7% (6071/10000)
[Test]  Epoch: 39	Loss: 0.032347	Acc: 56.7% (5667/10000)
[Test]  Epoch: 40	Loss: 0.043792	Acc: 48.8% (4883/10000)
[Test]  Epoch: 41	Loss: 0.028780	Acc: 58.4% (5839/10000)
[Test]  Epoch: 42	Loss: 0.030090	Acc: 57.3% (5730/10000)
[Test]  Epoch: 43	Loss: 0.026244	Acc: 59.9% (5988/10000)
[Test]  Epoch: 44	Loss: 0.032012	Acc: 57.0% (5696/10000)
[Test]  Epoch: 45	Loss: 0.031579	Acc: 55.2% (5521/10000)
[Test]  Epoch: 46	Loss: 0.028094	Acc: 59.1% (5915/10000)
[Test]  Epoch: 47	Loss: 0.025914	Acc: 61.5% (6149/10000)
[Test]  Epoch: 48	Loss: 0.028420	Acc: 59.2% (5922/10000)
[Test]  Epoch: 49	Loss: 0.025168	Acc: 62.6% (6258/10000)
[Test]  Epoch: 50	Loss: 0.031237	Acc: 57.0% (5697/10000)
[Test]  Epoch: 51	Loss: 0.032823	Acc: 54.7% (5469/10000)
[Test]  Epoch: 52	Loss: 0.027125	Acc: 59.8% (5979/10000)
[Test]  Epoch: 53	Loss: 0.034335	Acc: 53.4% (5339/10000)
[Test]  Epoch: 54	Loss: 0.025527	Acc: 62.3% (6227/10000)
[Test]  Epoch: 55	Loss: 0.027889	Acc: 59.9% (5985/10000)
[Test]  Epoch: 56	Loss: 0.025159	Acc: 62.5% (6255/10000)
[Test]  Epoch: 57	Loss: 0.025138	Acc: 62.5% (6246/10000)
[Test]  Epoch: 58	Loss: 0.027753	Acc: 61.4% (6141/10000)
[Test]  Epoch: 59	Loss: 0.024111	Acc: 64.4% (6439/10000)
[Test]  Epoch: 60	Loss: 0.026317	Acc: 62.4% (6239/10000)
[Test]  Epoch: 61	Loss: 0.023820	Acc: 65.0% (6505/10000)
[Test]  Epoch: 62	Loss: 0.023511	Acc: 65.2% (6522/10000)
[Test]  Epoch: 63	Loss: 0.023251	Acc: 65.8% (6576/10000)
[Test]  Epoch: 64	Loss: 0.023091	Acc: 65.3% (6526/10000)
[Test]  Epoch: 65	Loss: 0.023027	Acc: 65.7% (6566/10000)
[Test]  Epoch: 66	Loss: 0.022971	Acc: 65.8% (6579/10000)
[Test]  Epoch: 67	Loss: 0.022961	Acc: 65.9% (6591/10000)
[Test]  Epoch: 68	Loss: 0.022850	Acc: 66.1% (6613/10000)
[Test]  Epoch: 69	Loss: 0.022736	Acc: 65.9% (6589/10000)
[Test]  Epoch: 70	Loss: 0.022716	Acc: 66.2% (6621/10000)
[Test]  Epoch: 71	Loss: 0.022703	Acc: 66.2% (6621/10000)
[Test]  Epoch: 72	Loss: 0.022724	Acc: 65.8% (6579/10000)
[Test]  Epoch: 73	Loss: 0.022662	Acc: 66.0% (6597/10000)
[Test]  Epoch: 74	Loss: 0.022643	Acc: 66.4% (6643/10000)
[Test]  Epoch: 75	Loss: 0.022521	Acc: 66.5% (6649/10000)
[Test]  Epoch: 76	Loss: 0.022756	Acc: 65.8% (6583/10000)
[Test]  Epoch: 77	Loss: 0.022533	Acc: 66.5% (6655/10000)
[Test]  Epoch: 78	Loss: 0.022630	Acc: 66.7% (6669/10000)
[Test]  Epoch: 79	Loss: 0.022648	Acc: 66.4% (6641/10000)
[Test]  Epoch: 80	Loss: 0.022414	Acc: 66.5% (6655/10000)
[Test]  Epoch: 81	Loss: 0.022945	Acc: 65.9% (6591/10000)
[Test]  Epoch: 82	Loss: 0.022457	Acc: 66.3% (6626/10000)
[Test]  Epoch: 83	Loss: 0.022396	Acc: 66.7% (6673/10000)
[Test]  Epoch: 84	Loss: 0.022495	Acc: 66.6% (6663/10000)
[Test]  Epoch: 85	Loss: 0.022775	Acc: 66.2% (6618/10000)
[Test]  Epoch: 86	Loss: 0.022737	Acc: 66.7% (6669/10000)
[Test]  Epoch: 87	Loss: 0.022468	Acc: 66.4% (6642/10000)
[Test]  Epoch: 88	Loss: 0.022469	Acc: 66.8% (6676/10000)
[Test]  Epoch: 89	Loss: 0.022543	Acc: 66.2% (6621/10000)
[Test]  Epoch: 90	Loss: 0.022509	Acc: 67.0% (6701/10000)
[Test]  Epoch: 91	Loss: 0.022564	Acc: 66.5% (6649/10000)
[Test]  Epoch: 92	Loss: 0.022401	Acc: 66.7% (6671/10000)
[Test]  Epoch: 93	Loss: 0.022330	Acc: 66.7% (6667/10000)
[Test]  Epoch: 94	Loss: 0.022445	Acc: 66.6% (6664/10000)
[Test]  Epoch: 95	Loss: 0.022383	Acc: 66.8% (6675/10000)
[Test]  Epoch: 96	Loss: 0.022606	Acc: 66.6% (6662/10000)
[Test]  Epoch: 97	Loss: 0.022544	Acc: 66.8% (6683/10000)
[Test]  Epoch: 98	Loss: 0.022912	Acc: 66.4% (6640/10000)
[Test]  Epoch: 99	Loss: 0.022571	Acc: 66.7% (6671/10000)
[Test]  Epoch: 100	Loss: 0.022598	Acc: 66.9% (6692/10000)
===========finish==========
['2024-08-18', '17:37:38.853619', '100', 'test', '0.022597734081745147', '66.92', '67.01']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight', 'features.24.weight', 'features.3.weight', 'features.27.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.107329	Acc: 9.2% (920/10000)
[Test]  Epoch: 2	Loss: 0.040358	Acc: 11.7% (1174/10000)
[Test]  Epoch: 3	Loss: 0.036739	Acc: 11.9% (1194/10000)
[Test]  Epoch: 4	Loss: 0.036010	Acc: 13.3% (1327/10000)
[Test]  Epoch: 5	Loss: 0.035909	Acc: 14.0% (1397/10000)
[Test]  Epoch: 6	Loss: 0.034427	Acc: 16.6% (1655/10000)
[Test]  Epoch: 7	Loss: 0.032467	Acc: 21.3% (2127/10000)
[Test]  Epoch: 8	Loss: 0.032169	Acc: 22.5% (2247/10000)
[Test]  Epoch: 9	Loss: 0.030747	Acc: 25.4% (2542/10000)
[Test]  Epoch: 10	Loss: 0.031051	Acc: 26.0% (2598/10000)
[Test]  Epoch: 11	Loss: 0.034187	Acc: 21.6% (2161/10000)
[Test]  Epoch: 12	Loss: 0.029587	Acc: 32.2% (3219/10000)
[Test]  Epoch: 13	Loss: 0.028163	Acc: 33.1% (3313/10000)
[Test]  Epoch: 14	Loss: 0.031955	Acc: 30.0% (2997/10000)
[Test]  Epoch: 15	Loss: 0.028059	Acc: 34.6% (3460/10000)
[Test]  Epoch: 16	Loss: 0.028883	Acc: 34.1% (3411/10000)
[Test]  Epoch: 17	Loss: 0.037682	Acc: 30.5% (3048/10000)
[Test]  Epoch: 18	Loss: 0.028031	Acc: 34.6% (3456/10000)
[Test]  Epoch: 19	Loss: 0.033164	Acc: 32.6% (3258/10000)
[Test]  Epoch: 20	Loss: 0.030368	Acc: 35.4% (3538/10000)
[Test]  Epoch: 21	Loss: 0.032551	Acc: 31.7% (3172/10000)
[Test]  Epoch: 22	Loss: 0.028956	Acc: 38.6% (3862/10000)
[Test]  Epoch: 23	Loss: 0.028247	Acc: 38.0% (3798/10000)
[Test]  Epoch: 24	Loss: 0.028494	Acc: 39.9% (3985/10000)
[Test]  Epoch: 25	Loss: 0.027833	Acc: 41.2% (4125/10000)
[Test]  Epoch: 26	Loss: 0.028787	Acc: 40.6% (4057/10000)
[Test]  Epoch: 27	Loss: 0.028904	Acc: 37.7% (3771/10000)
[Test]  Epoch: 28	Loss: 0.027679	Acc: 44.3% (4428/10000)
[Test]  Epoch: 29	Loss: 0.030269	Acc: 40.9% (4086/10000)
[Test]  Epoch: 30	Loss: 0.035205	Acc: 37.2% (3720/10000)
[Test]  Epoch: 31	Loss: 0.029776	Acc: 43.3% (4331/10000)
[Test]  Epoch: 32	Loss: 0.031492	Acc: 43.1% (4313/10000)
[Test]  Epoch: 33	Loss: 0.025545	Acc: 48.1% (4811/10000)
[Test]  Epoch: 34	Loss: 0.031216	Acc: 43.6% (4358/10000)
[Test]  Epoch: 35	Loss: 0.036707	Acc: 39.3% (3929/10000)
[Test]  Epoch: 36	Loss: 0.036335	Acc: 42.0% (4204/10000)
[Test]  Epoch: 37	Loss: 0.036017	Acc: 41.1% (4112/10000)
[Test]  Epoch: 38	Loss: 0.033453	Acc: 43.6% (4364/10000)
[Test]  Epoch: 39	Loss: 0.029948	Acc: 46.0% (4597/10000)
[Test]  Epoch: 40	Loss: 0.034443	Acc: 43.4% (4338/10000)
[Test]  Epoch: 41	Loss: 0.040131	Acc: 40.8% (4075/10000)
[Test]  Epoch: 42	Loss: 0.034297	Acc: 46.4% (4636/10000)
[Test]  Epoch: 43	Loss: 0.031528	Acc: 47.4% (4742/10000)
[Test]  Epoch: 44	Loss: 0.032889	Acc: 47.0% (4704/10000)
[Test]  Epoch: 45	Loss: 0.035504	Acc: 45.8% (4582/10000)
[Test]  Epoch: 46	Loss: 0.036379	Acc: 45.9% (4593/10000)
[Test]  Epoch: 47	Loss: 0.033694	Acc: 47.1% (4711/10000)
[Test]  Epoch: 48	Loss: 0.033006	Acc: 48.1% (4815/10000)
[Test]  Epoch: 49	Loss: 0.034718	Acc: 48.8% (4875/10000)
[Test]  Epoch: 50	Loss: 0.038932	Acc: 43.0% (4304/10000)
[Test]  Epoch: 51	Loss: 0.029853	Acc: 51.7% (5174/10000)
[Test]  Epoch: 52	Loss: 0.035942	Acc: 47.6% (4763/10000)
[Test]  Epoch: 53	Loss: 0.033730	Acc: 49.6% (4965/10000)
[Test]  Epoch: 54	Loss: 0.030517	Acc: 52.0% (5197/10000)
[Test]  Epoch: 55	Loss: 0.034177	Acc: 50.0% (5002/10000)
[Test]  Epoch: 56	Loss: 0.050004	Acc: 37.3% (3733/10000)
[Test]  Epoch: 57	Loss: 0.033251	Acc: 50.8% (5080/10000)
[Test]  Epoch: 58	Loss: 0.039059	Acc: 44.8% (4481/10000)
[Test]  Epoch: 59	Loss: 0.036282	Acc: 46.6% (4665/10000)
[Test]  Epoch: 60	Loss: 0.031548	Acc: 52.4% (5240/10000)
[Test]  Epoch: 61	Loss: 0.029839	Acc: 53.8% (5379/10000)
[Test]  Epoch: 62	Loss: 0.029024	Acc: 55.0% (5498/10000)
[Test]  Epoch: 63	Loss: 0.028731	Acc: 55.2% (5524/10000)
[Test]  Epoch: 64	Loss: 0.028515	Acc: 55.3% (5527/10000)
[Test]  Epoch: 65	Loss: 0.028467	Acc: 55.0% (5501/10000)
[Test]  Epoch: 66	Loss: 0.028390	Acc: 55.2% (5524/10000)
[Test]  Epoch: 67	Loss: 0.028418	Acc: 55.4% (5536/10000)
[Test]  Epoch: 68	Loss: 0.028260	Acc: 55.8% (5578/10000)
[Test]  Epoch: 69	Loss: 0.028215	Acc: 55.5% (5549/10000)
[Test]  Epoch: 70	Loss: 0.028592	Acc: 55.9% (5589/10000)
[Test]  Epoch: 71	Loss: 0.028366	Acc: 55.6% (5557/10000)
[Test]  Epoch: 72	Loss: 0.028290	Acc: 55.9% (5591/10000)
[Test]  Epoch: 73	Loss: 0.028740	Acc: 55.5% (5547/10000)
[Test]  Epoch: 74	Loss: 0.028549	Acc: 55.4% (5544/10000)
[Test]  Epoch: 75	Loss: 0.028618	Acc: 55.7% (5566/10000)
[Test]  Epoch: 76	Loss: 0.029095	Acc: 55.6% (5558/10000)
[Test]  Epoch: 77	Loss: 0.028963	Acc: 55.6% (5559/10000)
[Test]  Epoch: 78	Loss: 0.028596	Acc: 56.0% (5601/10000)
[Test]  Epoch: 79	Loss: 0.028635	Acc: 55.7% (5574/10000)
[Test]  Epoch: 80	Loss: 0.028818	Acc: 56.2% (5625/10000)
[Test]  Epoch: 81	Loss: 0.029109	Acc: 55.5% (5549/10000)
[Test]  Epoch: 82	Loss: 0.028675	Acc: 56.0% (5599/10000)
[Test]  Epoch: 83	Loss: 0.028902	Acc: 55.7% (5574/10000)
[Test]  Epoch: 84	Loss: 0.029218	Acc: 55.6% (5565/10000)
[Test]  Epoch: 85	Loss: 0.029153	Acc: 56.1% (5606/10000)
[Test]  Epoch: 86	Loss: 0.028786	Acc: 56.3% (5634/10000)
[Test]  Epoch: 87	Loss: 0.028982	Acc: 56.0% (5601/10000)
[Test]  Epoch: 88	Loss: 0.028835	Acc: 55.9% (5593/10000)
[Test]  Epoch: 89	Loss: 0.029144	Acc: 55.8% (5576/10000)
[Test]  Epoch: 90	Loss: 0.028900	Acc: 56.1% (5610/10000)
[Test]  Epoch: 91	Loss: 0.028973	Acc: 55.9% (5587/10000)
[Test]  Epoch: 92	Loss: 0.029367	Acc: 55.4% (5542/10000)
[Test]  Epoch: 93	Loss: 0.028852	Acc: 56.1% (5608/10000)
[Test]  Epoch: 94	Loss: 0.029260	Acc: 55.6% (5559/10000)
[Test]  Epoch: 95	Loss: 0.029133	Acc: 56.3% (5629/10000)
[Test]  Epoch: 96	Loss: 0.029086	Acc: 56.0% (5600/10000)
[Test]  Epoch: 97	Loss: 0.029192	Acc: 56.4% (5637/10000)
[Test]  Epoch: 98	Loss: 0.029673	Acc: 55.8% (5584/10000)
[Test]  Epoch: 99	Loss: 0.029726	Acc: 55.2% (5523/10000)
[Test]  Epoch: 100	Loss: 0.029334	Acc: 56.4% (5639/10000)
===========finish==========
['2024-08-18', '17:40:08.586314', '100', 'test', '0.029334091997146608', '56.39', '56.39']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight', 'features.24.weight', 'features.3.weight', 'features.27.weight', 'features.0.weight', 'features.30.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.133035	Acc: 8.7% (870/10000)
[Test]  Epoch: 2	Loss: 0.036593	Acc: 13.5% (1347/10000)
[Test]  Epoch: 3	Loss: 0.035948	Acc: 12.3% (1232/10000)
[Test]  Epoch: 4	Loss: 0.036155	Acc: 12.5% (1253/10000)
[Test]  Epoch: 5	Loss: 0.033953	Acc: 19.4% (1941/10000)
[Test]  Epoch: 6	Loss: 0.031693	Acc: 24.4% (2436/10000)
[Test]  Epoch: 7	Loss: 0.033360	Acc: 23.1% (2315/10000)
[Test]  Epoch: 8	Loss: 0.031248	Acc: 25.6% (2558/10000)
[Test]  Epoch: 9	Loss: 0.030373	Acc: 27.9% (2793/10000)
[Test]  Epoch: 10	Loss: 0.030837	Acc: 29.1% (2912/10000)
[Test]  Epoch: 11	Loss: 0.028254	Acc: 35.0% (3502/10000)
[Test]  Epoch: 12	Loss: 0.029433	Acc: 33.5% (3352/10000)
[Test]  Epoch: 13	Loss: 0.028549	Acc: 34.9% (3492/10000)
[Test]  Epoch: 14	Loss: 0.030294	Acc: 31.9% (3190/10000)
[Test]  Epoch: 15	Loss: 0.030796	Acc: 33.8% (3378/10000)
[Test]  Epoch: 16	Loss: 0.033728	Acc: 33.8% (3380/10000)
[Test]  Epoch: 17	Loss: 0.046548	Acc: 28.8% (2880/10000)
[Test]  Epoch: 18	Loss: 0.034989	Acc: 31.8% (3176/10000)
[Test]  Epoch: 19	Loss: 0.028775	Acc: 37.0% (3703/10000)
[Test]  Epoch: 20	Loss: 0.029691	Acc: 36.5% (3648/10000)
[Test]  Epoch: 21	Loss: 0.036293	Acc: 32.6% (3261/10000)
[Test]  Epoch: 22	Loss: 0.031085	Acc: 37.8% (3784/10000)
[Test]  Epoch: 23	Loss: 0.028944	Acc: 40.0% (4001/10000)
[Test]  Epoch: 24	Loss: 0.030366	Acc: 39.6% (3959/10000)
[Test]  Epoch: 25	Loss: 0.029469	Acc: 40.6% (4058/10000)
[Test]  Epoch: 26	Loss: 0.035859	Acc: 35.6% (3558/10000)
[Test]  Epoch: 27	Loss: 0.038663	Acc: 33.1% (3309/10000)
[Test]  Epoch: 28	Loss: 0.033867	Acc: 36.8% (3678/10000)
[Test]  Epoch: 29	Loss: 0.029810	Acc: 41.2% (4119/10000)
[Test]  Epoch: 30	Loss: 0.039063	Acc: 37.1% (3712/10000)
[Test]  Epoch: 31	Loss: 0.033506	Acc: 37.8% (3780/10000)
[Test]  Epoch: 32	Loss: 0.032213	Acc: 43.1% (4306/10000)
[Test]  Epoch: 33	Loss: 0.032852	Acc: 43.0% (4300/10000)
[Test]  Epoch: 34	Loss: 0.038692	Acc: 37.0% (3700/10000)
[Test]  Epoch: 35	Loss: 0.035396	Acc: 40.3% (4030/10000)
[Test]  Epoch: 36	Loss: 0.034251	Acc: 42.6% (4262/10000)
[Test]  Epoch: 37	Loss: 0.040503	Acc: 38.9% (3886/10000)
[Test]  Epoch: 38	Loss: 0.039627	Acc: 39.0% (3902/10000)
[Test]  Epoch: 39	Loss: 0.040503	Acc: 39.0% (3896/10000)
[Test]  Epoch: 40	Loss: 0.034275	Acc: 43.1% (4307/10000)
[Test]  Epoch: 41	Loss: 0.046711	Acc: 36.5% (3650/10000)
[Test]  Epoch: 42	Loss: 0.037154	Acc: 42.8% (4284/10000)
[Test]  Epoch: 43	Loss: 0.045889	Acc: 38.8% (3878/10000)
[Test]  Epoch: 44	Loss: 0.034129	Acc: 46.1% (4607/10000)
[Test]  Epoch: 45	Loss: 0.037781	Acc: 42.5% (4249/10000)
[Test]  Epoch: 46	Loss: 0.038213	Acc: 42.3% (4229/10000)
[Test]  Epoch: 47	Loss: 0.035123	Acc: 45.1% (4509/10000)
[Test]  Epoch: 48	Loss: 0.043421	Acc: 38.5% (3848/10000)
[Test]  Epoch: 49	Loss: 0.046939	Acc: 40.0% (3995/10000)
[Test]  Epoch: 50	Loss: 0.041362	Acc: 40.0% (3997/10000)
[Test]  Epoch: 51	Loss: 0.036418	Acc: 45.8% (4584/10000)
[Test]  Epoch: 52	Loss: 0.036618	Acc: 45.2% (4524/10000)
[Test]  Epoch: 53	Loss: 0.035709	Acc: 46.9% (4686/10000)
[Test]  Epoch: 54	Loss: 0.036521	Acc: 45.6% (4558/10000)
[Test]  Epoch: 55	Loss: 0.036762	Acc: 46.5% (4645/10000)
[Test]  Epoch: 56	Loss: 0.040409	Acc: 44.9% (4490/10000)
[Test]  Epoch: 57	Loss: 0.036215	Acc: 48.1% (4814/10000)
[Test]  Epoch: 58	Loss: 0.034722	Acc: 48.9% (4885/10000)
[Test]  Epoch: 59	Loss: 0.039931	Acc: 45.7% (4566/10000)
[Test]  Epoch: 60	Loss: 0.040583	Acc: 46.4% (4642/10000)
[Test]  Epoch: 61	Loss: 0.035661	Acc: 49.6% (4964/10000)
[Test]  Epoch: 62	Loss: 0.035033	Acc: 49.8% (4976/10000)
[Test]  Epoch: 63	Loss: 0.034261	Acc: 49.8% (4981/10000)
[Test]  Epoch: 64	Loss: 0.034023	Acc: 50.5% (5050/10000)
[Test]  Epoch: 65	Loss: 0.034113	Acc: 50.2% (5016/10000)
[Test]  Epoch: 66	Loss: 0.033888	Acc: 50.6% (5056/10000)
[Test]  Epoch: 67	Loss: 0.034441	Acc: 50.5% (5048/10000)
[Test]  Epoch: 68	Loss: 0.034008	Acc: 50.4% (5043/10000)
[Test]  Epoch: 69	Loss: 0.033956	Acc: 50.1% (5010/10000)
[Test]  Epoch: 70	Loss: 0.033905	Acc: 50.6% (5056/10000)
[Test]  Epoch: 71	Loss: 0.034029	Acc: 50.6% (5058/10000)
[Test]  Epoch: 72	Loss: 0.034181	Acc: 50.1% (5014/10000)
[Test]  Epoch: 73	Loss: 0.034300	Acc: 50.4% (5036/10000)
[Test]  Epoch: 74	Loss: 0.034603	Acc: 49.8% (4979/10000)
[Test]  Epoch: 75	Loss: 0.034548	Acc: 50.4% (5040/10000)
[Test]  Epoch: 76	Loss: 0.034321	Acc: 50.4% (5040/10000)
[Test]  Epoch: 77	Loss: 0.034321	Acc: 50.8% (5080/10000)
[Test]  Epoch: 78	Loss: 0.034228	Acc: 50.9% (5090/10000)
[Test]  Epoch: 79	Loss: 0.033970	Acc: 51.2% (5118/10000)
[Test]  Epoch: 80	Loss: 0.034279	Acc: 51.0% (5097/10000)
[Test]  Epoch: 81	Loss: 0.034454	Acc: 50.5% (5053/10000)
[Test]  Epoch: 82	Loss: 0.034355	Acc: 50.6% (5061/10000)
[Test]  Epoch: 83	Loss: 0.034285	Acc: 51.2% (5117/10000)
[Test]  Epoch: 84	Loss: 0.034334	Acc: 50.6% (5065/10000)
[Test]  Epoch: 85	Loss: 0.034133	Acc: 50.9% (5093/10000)
[Test]  Epoch: 86	Loss: 0.034136	Acc: 50.8% (5081/10000)
[Test]  Epoch: 87	Loss: 0.034218	Acc: 50.5% (5045/10000)
[Test]  Epoch: 88	Loss: 0.034242	Acc: 50.6% (5065/10000)
[Test]  Epoch: 89	Loss: 0.034354	Acc: 50.6% (5063/10000)
[Test]  Epoch: 90	Loss: 0.034257	Acc: 51.1% (5106/10000)
[Test]  Epoch: 91	Loss: 0.034473	Acc: 51.1% (5108/10000)
[Test]  Epoch: 92	Loss: 0.034273	Acc: 51.0% (5096/10000)
[Test]  Epoch: 93	Loss: 0.034426	Acc: 50.6% (5057/10000)
[Test]  Epoch: 94	Loss: 0.034139	Acc: 51.4% (5139/10000)
[Test]  Epoch: 95	Loss: 0.034067	Acc: 51.2% (5123/10000)
[Test]  Epoch: 96	Loss: 0.034262	Acc: 51.5% (5149/10000)
[Test]  Epoch: 97	Loss: 0.034185	Acc: 51.1% (5107/10000)
[Test]  Epoch: 98	Loss: 0.034733	Acc: 51.3% (5127/10000)
[Test]  Epoch: 99	Loss: 0.034737	Acc: 51.0% (5101/10000)
[Test]  Epoch: 100	Loss: 0.034322	Acc: 51.4% (5136/10000)
===========finish==========
['2024-08-18', '17:42:34.238201', '100', 'test', '0.034322408080101015', '51.36', '51.49']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight', 'features.24.weight', 'features.3.weight', 'features.27.weight', 'features.0.weight', 'features.30.weight', 'features.11.weight', 'features.34.weight', 'features.21.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.172712	Acc: 11.2% (1118/10000)
[Test]  Epoch: 2	Loss: 0.037004	Acc: 14.6% (1457/10000)
[Test]  Epoch: 3	Loss: 0.034999	Acc: 14.4% (1443/10000)
[Test]  Epoch: 4	Loss: 0.032262	Acc: 21.4% (2144/10000)
[Test]  Epoch: 5	Loss: 0.031689	Acc: 23.0% (2297/10000)
[Test]  Epoch: 6	Loss: 0.032770	Acc: 22.7% (2273/10000)
[Test]  Epoch: 7	Loss: 0.034645	Acc: 23.8% (2382/10000)
[Test]  Epoch: 8	Loss: 0.035446	Acc: 22.8% (2278/10000)
[Test]  Epoch: 9	Loss: 0.031608	Acc: 27.9% (2786/10000)
[Test]  Epoch: 10	Loss: 0.030629	Acc: 31.2% (3124/10000)
[Test]  Epoch: 11	Loss: 0.028646	Acc: 34.3% (3434/10000)
[Test]  Epoch: 12	Loss: 0.033175	Acc: 31.8% (3180/10000)
[Test]  Epoch: 13	Loss: 0.030906	Acc: 29.8% (2982/10000)
[Test]  Epoch: 14	Loss: 0.028367	Acc: 34.3% (3430/10000)
[Test]  Epoch: 15	Loss: 0.034966	Acc: 31.8% (3179/10000)
[Test]  Epoch: 16	Loss: 0.030258	Acc: 34.4% (3442/10000)
[Test]  Epoch: 17	Loss: 0.032118	Acc: 33.1% (3312/10000)
[Test]  Epoch: 18	Loss: 0.030352	Acc: 34.8% (3482/10000)
[Test]  Epoch: 19	Loss: 0.028564	Acc: 36.7% (3668/10000)
[Test]  Epoch: 20	Loss: 0.028049	Acc: 40.3% (4033/10000)
[Test]  Epoch: 21	Loss: 0.028481	Acc: 37.9% (3793/10000)
[Test]  Epoch: 22	Loss: 0.031690	Acc: 35.0% (3501/10000)
[Test]  Epoch: 23	Loss: 0.030985	Acc: 37.6% (3760/10000)
[Test]  Epoch: 24	Loss: 0.030954	Acc: 38.1% (3810/10000)
[Test]  Epoch: 25	Loss: 0.028230	Acc: 42.2% (4222/10000)
[Test]  Epoch: 26	Loss: 0.030690	Acc: 38.8% (3879/10000)
[Test]  Epoch: 27	Loss: 0.040779	Acc: 32.3% (3227/10000)
[Test]  Epoch: 28	Loss: 0.034451	Acc: 36.2% (3617/10000)
[Test]  Epoch: 29	Loss: 0.029046	Acc: 41.1% (4110/10000)
[Test]  Epoch: 30	Loss: 0.035821	Acc: 38.2% (3822/10000)
[Test]  Epoch: 31	Loss: 0.034980	Acc: 36.9% (3691/10000)
[Test]  Epoch: 32	Loss: 0.033217	Acc: 40.4% (4038/10000)
[Test]  Epoch: 33	Loss: 0.035464	Acc: 37.1% (3713/10000)
[Test]  Epoch: 34	Loss: 0.033696	Acc: 41.3% (4134/10000)
[Test]  Epoch: 35	Loss: 0.036977	Acc: 39.0% (3896/10000)
[Test]  Epoch: 36	Loss: 0.033726	Acc: 41.9% (4188/10000)
[Test]  Epoch: 37	Loss: 0.037315	Acc: 39.2% (3916/10000)
[Test]  Epoch: 38	Loss: 0.039603	Acc: 37.7% (3771/10000)
[Test]  Epoch: 39	Loss: 0.034908	Acc: 43.1% (4313/10000)
[Test]  Epoch: 40	Loss: 0.037713	Acc: 38.5% (3845/10000)
[Test]  Epoch: 41	Loss: 0.044223	Acc: 37.2% (3717/10000)
[Test]  Epoch: 42	Loss: 0.037417	Acc: 43.6% (4359/10000)
[Test]  Epoch: 43	Loss: 0.043315	Acc: 37.5% (3751/10000)
[Test]  Epoch: 44	Loss: 0.033891	Acc: 43.1% (4307/10000)
[Test]  Epoch: 45	Loss: 0.037684	Acc: 40.4% (4041/10000)
[Test]  Epoch: 46	Loss: 0.040566	Acc: 39.5% (3951/10000)
[Test]  Epoch: 47	Loss: 0.036403	Acc: 43.5% (4345/10000)
[Test]  Epoch: 48	Loss: 0.034800	Acc: 46.0% (4596/10000)
[Test]  Epoch: 49	Loss: 0.045813	Acc: 41.9% (4190/10000)
[Test]  Epoch: 50	Loss: 0.040854	Acc: 41.6% (4161/10000)
[Test]  Epoch: 51	Loss: 0.037029	Acc: 44.3% (4427/10000)
[Test]  Epoch: 52	Loss: 0.050327	Acc: 37.5% (3751/10000)
[Test]  Epoch: 53	Loss: 0.042764	Acc: 43.3% (4334/10000)
[Test]  Epoch: 54	Loss: 0.038280	Acc: 43.4% (4339/10000)
[Test]  Epoch: 55	Loss: 0.039810	Acc: 42.7% (4273/10000)
[Test]  Epoch: 56	Loss: 0.041960	Acc: 42.2% (4223/10000)
[Test]  Epoch: 57	Loss: 0.037774	Acc: 46.2% (4619/10000)
[Test]  Epoch: 58	Loss: 0.040206	Acc: 44.2% (4424/10000)
[Test]  Epoch: 59	Loss: 0.042408	Acc: 42.4% (4244/10000)
[Test]  Epoch: 60	Loss: 0.039083	Acc: 46.2% (4617/10000)
[Test]  Epoch: 61	Loss: 0.035248	Acc: 48.4% (4838/10000)
[Test]  Epoch: 62	Loss: 0.034530	Acc: 49.2% (4921/10000)
[Test]  Epoch: 63	Loss: 0.034477	Acc: 49.0% (4905/10000)
[Test]  Epoch: 64	Loss: 0.034603	Acc: 49.2% (4921/10000)
[Test]  Epoch: 65	Loss: 0.034041	Acc: 49.5% (4954/10000)
[Test]  Epoch: 66	Loss: 0.033858	Acc: 49.6% (4965/10000)
[Test]  Epoch: 67	Loss: 0.034368	Acc: 49.5% (4948/10000)
[Test]  Epoch: 68	Loss: 0.034154	Acc: 50.0% (5000/10000)
[Test]  Epoch: 69	Loss: 0.034077	Acc: 49.7% (4967/10000)
[Test]  Epoch: 70	Loss: 0.033806	Acc: 50.1% (5008/10000)
[Test]  Epoch: 71	Loss: 0.034018	Acc: 49.5% (4955/10000)
[Test]  Epoch: 72	Loss: 0.034184	Acc: 49.6% (4958/10000)
[Test]  Epoch: 73	Loss: 0.033943	Acc: 49.8% (4977/10000)
[Test]  Epoch: 74	Loss: 0.034503	Acc: 49.8% (4981/10000)
[Test]  Epoch: 75	Loss: 0.034475	Acc: 49.4% (4941/10000)
[Test]  Epoch: 76	Loss: 0.034547	Acc: 49.8% (4979/10000)
[Test]  Epoch: 77	Loss: 0.034215	Acc: 49.9% (4987/10000)
[Test]  Epoch: 78	Loss: 0.034426	Acc: 49.9% (4990/10000)
[Test]  Epoch: 79	Loss: 0.034417	Acc: 49.8% (4976/10000)
[Test]  Epoch: 80	Loss: 0.034278	Acc: 50.2% (5017/10000)
[Test]  Epoch: 81	Loss: 0.034735	Acc: 49.8% (4981/10000)
[Test]  Epoch: 82	Loss: 0.034678	Acc: 49.9% (4992/10000)
[Test]  Epoch: 83	Loss: 0.034588	Acc: 49.8% (4983/10000)
[Test]  Epoch: 84	Loss: 0.034828	Acc: 49.3% (4928/10000)
[Test]  Epoch: 85	Loss: 0.034509	Acc: 50.5% (5047/10000)
[Test]  Epoch: 86	Loss: 0.034853	Acc: 49.7% (4969/10000)
[Test]  Epoch: 87	Loss: 0.034706	Acc: 49.7% (4974/10000)
[Test]  Epoch: 88	Loss: 0.034739	Acc: 49.7% (4966/10000)
[Test]  Epoch: 89	Loss: 0.034647	Acc: 50.2% (5022/10000)
[Test]  Epoch: 90	Loss: 0.034633	Acc: 50.3% (5034/10000)
[Test]  Epoch: 91	Loss: 0.035041	Acc: 49.5% (4945/10000)
[Test]  Epoch: 92	Loss: 0.034670	Acc: 49.8% (4975/10000)
[Test]  Epoch: 93	Loss: 0.034859	Acc: 50.1% (5010/10000)
[Test]  Epoch: 94	Loss: 0.034805	Acc: 49.9% (4990/10000)
[Test]  Epoch: 95	Loss: 0.034747	Acc: 49.8% (4980/10000)
[Test]  Epoch: 96	Loss: 0.035018	Acc: 49.7% (4967/10000)
[Test]  Epoch: 97	Loss: 0.034654	Acc: 49.8% (4978/10000)
[Test]  Epoch: 98	Loss: 0.035032	Acc: 49.8% (4982/10000)
[Test]  Epoch: 99	Loss: 0.035068	Acc: 49.9% (4986/10000)
[Test]  Epoch: 100	Loss: 0.034938	Acc: 50.0% (5002/10000)
===========finish==========
['2024-08-18', '17:45:06.768744', '100', 'test', '0.03493821411132812', '50.02', '50.47']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight', 'features.24.weight', 'features.3.weight', 'features.27.weight', 'features.0.weight', 'features.30.weight', 'features.11.weight', 'features.34.weight', 'features.21.weight', 'features.15.weight', 'features.37.weight', 'features.18.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.041380	Acc: 16.4% (1641/10000)
[Test]  Epoch: 2	Loss: 0.031977	Acc: 27.3% (2728/10000)
[Test]  Epoch: 3	Loss: 0.029282	Acc: 30.6% (3058/10000)
[Test]  Epoch: 4	Loss: 0.030819	Acc: 29.7% (2974/10000)
[Test]  Epoch: 5	Loss: 0.028808	Acc: 35.2% (3523/10000)
[Test]  Epoch: 6	Loss: 0.028417	Acc: 32.5% (3248/10000)
[Test]  Epoch: 7	Loss: 0.031890	Acc: 29.3% (2932/10000)
[Test]  Epoch: 8	Loss: 0.035587	Acc: 30.6% (3065/10000)
[Test]  Epoch: 9	Loss: 0.035039	Acc: 30.2% (3016/10000)
[Test]  Epoch: 10	Loss: 0.030026	Acc: 34.8% (3483/10000)
[Test]  Epoch: 11	Loss: 0.032004	Acc: 36.2% (3625/10000)
[Test]  Epoch: 12	Loss: 0.030374	Acc: 37.6% (3761/10000)
[Test]  Epoch: 13	Loss: 0.028532	Acc: 39.7% (3968/10000)
[Test]  Epoch: 14	Loss: 0.028572	Acc: 38.0% (3798/10000)
[Test]  Epoch: 15	Loss: 0.035081	Acc: 37.3% (3729/10000)
[Test]  Epoch: 16	Loss: 0.029866	Acc: 40.5% (4051/10000)
[Test]  Epoch: 17	Loss: 0.030359	Acc: 40.2% (4024/10000)
[Test]  Epoch: 18	Loss: 0.029989	Acc: 41.5% (4146/10000)
[Test]  Epoch: 19	Loss: 0.032918	Acc: 39.6% (3958/10000)
[Test]  Epoch: 20	Loss: 0.032197	Acc: 41.8% (4183/10000)
[Test]  Epoch: 21	Loss: 0.033789	Acc: 37.1% (3710/10000)
[Test]  Epoch: 22	Loss: 0.035798	Acc: 38.9% (3893/10000)
[Test]  Epoch: 23	Loss: 0.032398	Acc: 41.6% (4165/10000)
[Test]  Epoch: 24	Loss: 0.036213	Acc: 39.8% (3979/10000)
[Test]  Epoch: 25	Loss: 0.032909	Acc: 42.0% (4196/10000)
[Test]  Epoch: 26	Loss: 0.031199	Acc: 43.6% (4364/10000)
[Test]  Epoch: 27	Loss: 0.038976	Acc: 37.5% (3752/10000)
[Test]  Epoch: 28	Loss: 0.037262	Acc: 37.9% (3793/10000)
[Test]  Epoch: 29	Loss: 0.032257	Acc: 45.8% (4584/10000)
[Test]  Epoch: 30	Loss: 0.034012	Acc: 42.9% (4288/10000)
[Test]  Epoch: 31	Loss: 0.035066	Acc: 44.0% (4396/10000)
[Test]  Epoch: 32	Loss: 0.039745	Acc: 41.5% (4152/10000)
[Test]  Epoch: 33	Loss: 0.032406	Acc: 45.9% (4594/10000)
[Test]  Epoch: 34	Loss: 0.045200	Acc: 40.3% (4034/10000)
[Test]  Epoch: 35	Loss: 0.034954	Acc: 44.0% (4401/10000)
[Test]  Epoch: 36	Loss: 0.036501	Acc: 42.9% (4292/10000)
[Test]  Epoch: 37	Loss: 0.043260	Acc: 42.0% (4195/10000)
[Test]  Epoch: 38	Loss: 0.035239	Acc: 45.9% (4585/10000)
[Test]  Epoch: 39	Loss: 0.036579	Acc: 44.8% (4476/10000)
[Test]  Epoch: 40	Loss: 0.034699	Acc: 47.5% (4748/10000)
[Test]  Epoch: 41	Loss: 0.038695	Acc: 44.1% (4407/10000)
[Test]  Epoch: 42	Loss: 0.040786	Acc: 45.9% (4585/10000)
[Test]  Epoch: 43	Loss: 0.041711	Acc: 42.7% (4266/10000)
[Test]  Epoch: 44	Loss: 0.036376	Acc: 46.7% (4671/10000)
[Test]  Epoch: 45	Loss: 0.038745	Acc: 45.0% (4502/10000)
[Test]  Epoch: 46	Loss: 0.037584	Acc: 46.3% (4627/10000)
[Test]  Epoch: 47	Loss: 0.034985	Acc: 49.6% (4959/10000)
[Test]  Epoch: 48	Loss: 0.034841	Acc: 49.0% (4896/10000)
[Test]  Epoch: 49	Loss: 0.042046	Acc: 45.6% (4563/10000)
[Test]  Epoch: 50	Loss: 0.041788	Acc: 43.6% (4358/10000)
[Test]  Epoch: 51	Loss: 0.041552	Acc: 44.2% (4425/10000)
[Test]  Epoch: 52	Loss: 0.041529	Acc: 44.7% (4468/10000)
[Test]  Epoch: 53	Loss: 0.042739	Acc: 43.7% (4373/10000)
[Test]  Epoch: 54	Loss: 0.039062	Acc: 46.3% (4628/10000)
[Test]  Epoch: 55	Loss: 0.041994	Acc: 46.1% (4615/10000)
[Test]  Epoch: 56	Loss: 0.036134	Acc: 48.7% (4870/10000)
[Test]  Epoch: 57	Loss: 0.040822	Acc: 43.3% (4330/10000)
[Test]  Epoch: 58	Loss: 0.037403	Acc: 48.5% (4851/10000)
[Test]  Epoch: 59	Loss: 0.038235	Acc: 46.6% (4664/10000)
[Test]  Epoch: 60	Loss: 0.037410	Acc: 48.3% (4832/10000)
[Test]  Epoch: 61	Loss: 0.035061	Acc: 49.9% (4993/10000)
[Test]  Epoch: 62	Loss: 0.034580	Acc: 50.1% (5011/10000)
[Test]  Epoch: 63	Loss: 0.034219	Acc: 50.2% (5017/10000)
[Test]  Epoch: 64	Loss: 0.034215	Acc: 50.8% (5075/10000)
[Test]  Epoch: 65	Loss: 0.033713	Acc: 50.8% (5079/10000)
[Test]  Epoch: 66	Loss: 0.033717	Acc: 50.6% (5059/10000)
[Test]  Epoch: 67	Loss: 0.033966	Acc: 50.8% (5078/10000)
[Test]  Epoch: 68	Loss: 0.034093	Acc: 50.8% (5082/10000)
[Test]  Epoch: 69	Loss: 0.033588	Acc: 50.9% (5092/10000)
[Test]  Epoch: 70	Loss: 0.033506	Acc: 51.7% (5172/10000)
[Test]  Epoch: 71	Loss: 0.033417	Acc: 51.1% (5115/10000)
[Test]  Epoch: 72	Loss: 0.033745	Acc: 50.9% (5093/10000)
[Test]  Epoch: 73	Loss: 0.033566	Acc: 51.0% (5099/10000)
[Test]  Epoch: 74	Loss: 0.033738	Acc: 51.2% (5121/10000)
[Test]  Epoch: 75	Loss: 0.033465	Acc: 51.2% (5124/10000)
[Test]  Epoch: 76	Loss: 0.033631	Acc: 50.9% (5090/10000)
[Test]  Epoch: 77	Loss: 0.033177	Acc: 51.8% (5176/10000)
[Test]  Epoch: 78	Loss: 0.033344	Acc: 52.2% (5217/10000)
[Test]  Epoch: 79	Loss: 0.033547	Acc: 51.8% (5181/10000)
[Test]  Epoch: 80	Loss: 0.033710	Acc: 51.1% (5109/10000)
[Test]  Epoch: 81	Loss: 0.033442	Acc: 51.2% (5116/10000)
[Test]  Epoch: 82	Loss: 0.033245	Acc: 51.5% (5150/10000)
[Test]  Epoch: 83	Loss: 0.033465	Acc: 52.0% (5198/10000)
[Test]  Epoch: 84	Loss: 0.033891	Acc: 51.9% (5191/10000)
[Test]  Epoch: 85	Loss: 0.033574	Acc: 50.9% (5091/10000)
[Test]  Epoch: 86	Loss: 0.033322	Acc: 51.8% (5183/10000)
[Test]  Epoch: 87	Loss: 0.033417	Acc: 51.4% (5142/10000)
[Test]  Epoch: 88	Loss: 0.033380	Acc: 52.0% (5195/10000)
[Test]  Epoch: 89	Loss: 0.033399	Acc: 51.8% (5180/10000)
[Test]  Epoch: 90	Loss: 0.033519	Acc: 51.4% (5144/10000)
[Test]  Epoch: 91	Loss: 0.033048	Acc: 52.0% (5205/10000)
[Test]  Epoch: 92	Loss: 0.033273	Acc: 51.9% (5185/10000)
[Test]  Epoch: 93	Loss: 0.033455	Acc: 51.8% (5181/10000)
[Test]  Epoch: 94	Loss: 0.033136	Acc: 52.1% (5214/10000)
[Test]  Epoch: 95	Loss: 0.033505	Acc: 51.9% (5185/10000)
[Test]  Epoch: 96	Loss: 0.033503	Acc: 51.7% (5174/10000)
[Test]  Epoch: 97	Loss: 0.033251	Acc: 52.2% (5222/10000)
[Test]  Epoch: 98	Loss: 0.033425	Acc: 51.6% (5156/10000)
[Test]  Epoch: 99	Loss: 0.033403	Acc: 52.2% (5225/10000)
[Test]  Epoch: 100	Loss: 0.033301	Acc: 52.1% (5207/10000)
===========finish==========
['2024-08-18', '17:47:35.988651', '100', 'test', '0.03330054165124893', '52.07', '52.25']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight', 'features.24.weight', 'features.3.weight', 'features.27.weight', 'features.0.weight', 'features.30.weight', 'features.11.weight', 'features.34.weight', 'features.21.weight', 'features.15.weight', 'features.37.weight', 'features.18.weight', 'features.40.weight', 'features.4.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.046473	Acc: 12.9% (1293/10000)
[Test]  Epoch: 2	Loss: 0.035278	Acc: 26.3% (2632/10000)
[Test]  Epoch: 3	Loss: 0.031352	Acc: 31.4% (3143/10000)
[Test]  Epoch: 4	Loss: 0.030092	Acc: 30.4% (3040/10000)
[Test]  Epoch: 5	Loss: 0.028396	Acc: 36.3% (3629/10000)
[Test]  Epoch: 6	Loss: 0.028615	Acc: 34.9% (3488/10000)
[Test]  Epoch: 7	Loss: 0.031040	Acc: 31.7% (3171/10000)
[Test]  Epoch: 8	Loss: 0.028087	Acc: 36.9% (3686/10000)
[Test]  Epoch: 9	Loss: 0.028951	Acc: 36.4% (3635/10000)
[Test]  Epoch: 10	Loss: 0.031265	Acc: 34.6% (3461/10000)
[Test]  Epoch: 11	Loss: 0.029431	Acc: 37.0% (3705/10000)
[Test]  Epoch: 12	Loss: 0.034517	Acc: 34.1% (3414/10000)
[Test]  Epoch: 13	Loss: 0.027414	Acc: 41.9% (4191/10000)
[Test]  Epoch: 14	Loss: 0.028113	Acc: 40.1% (4014/10000)
[Test]  Epoch: 15	Loss: 0.031429	Acc: 38.6% (3856/10000)
[Test]  Epoch: 16	Loss: 0.031136	Acc: 39.4% (3936/10000)
[Test]  Epoch: 17	Loss: 0.027074	Acc: 43.4% (4344/10000)
[Test]  Epoch: 18	Loss: 0.036710	Acc: 37.4% (3742/10000)
[Test]  Epoch: 19	Loss: 0.034795	Acc: 38.3% (3828/10000)
[Test]  Epoch: 20	Loss: 0.029829	Acc: 42.6% (4265/10000)
[Test]  Epoch: 21	Loss: 0.031032	Acc: 40.7% (4074/10000)
[Test]  Epoch: 22	Loss: 0.033475	Acc: 40.1% (4006/10000)
[Test]  Epoch: 23	Loss: 0.031435	Acc: 44.2% (4416/10000)
[Test]  Epoch: 24	Loss: 0.031131	Acc: 43.8% (4377/10000)
[Test]  Epoch: 25	Loss: 0.034552	Acc: 43.1% (4311/10000)
[Test]  Epoch: 26	Loss: 0.033091	Acc: 42.6% (4261/10000)
[Test]  Epoch: 27	Loss: 0.032841	Acc: 43.3% (4334/10000)
[Test]  Epoch: 28	Loss: 0.035066	Acc: 41.7% (4173/10000)
[Test]  Epoch: 29	Loss: 0.029033	Acc: 48.1% (4806/10000)
[Test]  Epoch: 30	Loss: 0.037653	Acc: 43.5% (4349/10000)
[Test]  Epoch: 31	Loss: 0.031503	Acc: 47.6% (4757/10000)
[Test]  Epoch: 32	Loss: 0.038378	Acc: 42.5% (4246/10000)
[Test]  Epoch: 33	Loss: 0.034540	Acc: 46.4% (4638/10000)
[Test]  Epoch: 34	Loss: 0.033859	Acc: 45.9% (4585/10000)
[Test]  Epoch: 35	Loss: 0.038827	Acc: 44.2% (4421/10000)
[Test]  Epoch: 36	Loss: 0.035769	Acc: 45.1% (4509/10000)
[Test]  Epoch: 37	Loss: 0.038486	Acc: 43.0% (4298/10000)
[Test]  Epoch: 38	Loss: 0.043460	Acc: 41.3% (4131/10000)
[Test]  Epoch: 39	Loss: 0.036078	Acc: 47.2% (4722/10000)
[Test]  Epoch: 40	Loss: 0.037801	Acc: 46.1% (4613/10000)
[Test]  Epoch: 41	Loss: 0.037463	Acc: 44.8% (4478/10000)
[Test]  Epoch: 42	Loss: 0.034964	Acc: 47.8% (4778/10000)
[Test]  Epoch: 43	Loss: 0.035364	Acc: 48.3% (4826/10000)
[Test]  Epoch: 44	Loss: 0.034268	Acc: 49.2% (4917/10000)
[Test]  Epoch: 45	Loss: 0.035296	Acc: 49.0% (4895/10000)
[Test]  Epoch: 46	Loss: 0.033723	Acc: 48.6% (4859/10000)
[Test]  Epoch: 47	Loss: 0.034807	Acc: 49.4% (4943/10000)
[Test]  Epoch: 48	Loss: 0.033710	Acc: 50.4% (5037/10000)
[Test]  Epoch: 49	Loss: 0.041293	Acc: 46.5% (4651/10000)
[Test]  Epoch: 50	Loss: 0.036409	Acc: 47.7% (4766/10000)
[Test]  Epoch: 51	Loss: 0.035552	Acc: 49.9% (4986/10000)
[Test]  Epoch: 52	Loss: 0.034153	Acc: 50.2% (5023/10000)
[Test]  Epoch: 53	Loss: 0.048713	Acc: 43.5% (4347/10000)
[Test]  Epoch: 54	Loss: 0.036390	Acc: 49.2% (4917/10000)
[Test]  Epoch: 55	Loss: 0.035605	Acc: 49.6% (4965/10000)
[Test]  Epoch: 56	Loss: 0.038917	Acc: 47.6% (4765/10000)
[Test]  Epoch: 57	Loss: 0.038693	Acc: 46.5% (4655/10000)
[Test]  Epoch: 58	Loss: 0.035379	Acc: 49.8% (4979/10000)
[Test]  Epoch: 59	Loss: 0.039462	Acc: 46.3% (4631/10000)
[Test]  Epoch: 60	Loss: 0.037165	Acc: 49.5% (4951/10000)
[Test]  Epoch: 61	Loss: 0.034349	Acc: 51.3% (5128/10000)
[Test]  Epoch: 62	Loss: 0.033720	Acc: 52.1% (5207/10000)
[Test]  Epoch: 63	Loss: 0.034010	Acc: 51.4% (5140/10000)
[Test]  Epoch: 64	Loss: 0.033602	Acc: 51.9% (5187/10000)
[Test]  Epoch: 65	Loss: 0.032946	Acc: 51.9% (5192/10000)
[Test]  Epoch: 66	Loss: 0.032924	Acc: 52.0% (5203/10000)
[Test]  Epoch: 67	Loss: 0.033090	Acc: 52.2% (5224/10000)
[Test]  Epoch: 68	Loss: 0.033173	Acc: 52.3% (5229/10000)
[Test]  Epoch: 69	Loss: 0.033147	Acc: 52.7% (5273/10000)
[Test]  Epoch: 70	Loss: 0.032972	Acc: 52.4% (5236/10000)
[Test]  Epoch: 71	Loss: 0.032988	Acc: 52.4% (5240/10000)
[Test]  Epoch: 72	Loss: 0.032815	Acc: 53.0% (5304/10000)
[Test]  Epoch: 73	Loss: 0.032456	Acc: 52.9% (5291/10000)
[Test]  Epoch: 74	Loss: 0.032633	Acc: 53.2% (5317/10000)
[Test]  Epoch: 75	Loss: 0.032945	Acc: 53.0% (5297/10000)
[Test]  Epoch: 76	Loss: 0.032799	Acc: 53.1% (5307/10000)
[Test]  Epoch: 77	Loss: 0.032824	Acc: 53.0% (5301/10000)
[Test]  Epoch: 78	Loss: 0.032844	Acc: 53.0% (5297/10000)
[Test]  Epoch: 79	Loss: 0.032492	Acc: 53.0% (5303/10000)
[Test]  Epoch: 80	Loss: 0.032621	Acc: 53.4% (5344/10000)
[Test]  Epoch: 81	Loss: 0.032644	Acc: 53.4% (5339/10000)
[Test]  Epoch: 82	Loss: 0.032823	Acc: 53.1% (5311/10000)
[Test]  Epoch: 83	Loss: 0.032502	Acc: 53.5% (5353/10000)
[Test]  Epoch: 84	Loss: 0.032880	Acc: 53.4% (5335/10000)
[Test]  Epoch: 85	Loss: 0.033076	Acc: 53.1% (5314/10000)
[Test]  Epoch: 86	Loss: 0.032780	Acc: 53.0% (5302/10000)
[Test]  Epoch: 87	Loss: 0.032530	Acc: 53.5% (5354/10000)
[Test]  Epoch: 88	Loss: 0.032663	Acc: 53.6% (5358/10000)
[Test]  Epoch: 89	Loss: 0.032917	Acc: 52.5% (5248/10000)
[Test]  Epoch: 90	Loss: 0.032861	Acc: 53.3% (5334/10000)
[Test]  Epoch: 91	Loss: 0.032703	Acc: 53.0% (5303/10000)
[Test]  Epoch: 92	Loss: 0.032749	Acc: 53.5% (5348/10000)
[Test]  Epoch: 93	Loss: 0.032742	Acc: 53.5% (5352/10000)
[Test]  Epoch: 94	Loss: 0.032654	Acc: 53.6% (5359/10000)
[Test]  Epoch: 95	Loss: 0.032104	Acc: 53.4% (5337/10000)
[Test]  Epoch: 96	Loss: 0.032621	Acc: 53.3% (5331/10000)
[Test]  Epoch: 97	Loss: 0.032578	Acc: 53.6% (5362/10000)
[Test]  Epoch: 98	Loss: 0.032853	Acc: 52.9% (5287/10000)
[Test]  Epoch: 99	Loss: 0.032991	Acc: 53.1% (5315/10000)
[Test]  Epoch: 100	Loss: 0.032602	Acc: 53.7% (5370/10000)
===========finish==========
['2024-08-18', '17:50:09.775559', '100', 'test', '0.03260186779499054', '53.7', '53.7']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight', 'features.24.weight', 'features.3.weight', 'features.27.weight', 'features.0.weight', 'features.30.weight', 'features.11.weight', 'features.34.weight', 'features.21.weight', 'features.15.weight', 'features.37.weight', 'features.18.weight', 'features.40.weight', 'features.4.weight', 'classifier.weight', 'features.8.weight', 'features.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.053339	Acc: 10.0% (1000/10000)
[Test]  Epoch: 2	Loss: 0.041054	Acc: 21.9% (2194/10000)
[Test]  Epoch: 3	Loss: 0.031239	Acc: 27.4% (2740/10000)
[Test]  Epoch: 4	Loss: 0.029598	Acc: 28.4% (2842/10000)
[Test]  Epoch: 5	Loss: 0.032868	Acc: 31.5% (3154/10000)
[Test]  Epoch: 6	Loss: 0.029595	Acc: 29.7% (2974/10000)
[Test]  Epoch: 7	Loss: 0.030392	Acc: 29.1% (2912/10000)
[Test]  Epoch: 8	Loss: 0.029530	Acc: 33.6% (3360/10000)
[Test]  Epoch: 9	Loss: 0.031230	Acc: 32.4% (3235/10000)
[Test]  Epoch: 10	Loss: 0.029159	Acc: 36.0% (3598/10000)
[Test]  Epoch: 11	Loss: 0.027830	Acc: 37.7% (3770/10000)
[Test]  Epoch: 12	Loss: 0.038450	Acc: 32.2% (3220/10000)
[Test]  Epoch: 13	Loss: 0.029195	Acc: 36.6% (3661/10000)
[Test]  Epoch: 14	Loss: 0.030770	Acc: 34.4% (3444/10000)
[Test]  Epoch: 15	Loss: 0.034995	Acc: 36.4% (3641/10000)
[Test]  Epoch: 16	Loss: 0.030289	Acc: 38.1% (3808/10000)
[Test]  Epoch: 17	Loss: 0.026084	Acc: 43.7% (4371/10000)
[Test]  Epoch: 18	Loss: 0.036647	Acc: 33.0% (3298/10000)
[Test]  Epoch: 19	Loss: 0.029250	Acc: 41.5% (4154/10000)
[Test]  Epoch: 20	Loss: 0.028299	Acc: 43.5% (4348/10000)
[Test]  Epoch: 21	Loss: 0.028819	Acc: 42.8% (4279/10000)
[Test]  Epoch: 22	Loss: 0.029889	Acc: 40.9% (4085/10000)
[Test]  Epoch: 23	Loss: 0.039533	Acc: 36.4% (3644/10000)
[Test]  Epoch: 24	Loss: 0.032591	Acc: 38.9% (3887/10000)
[Test]  Epoch: 25	Loss: 0.030757	Acc: 41.9% (4187/10000)
[Test]  Epoch: 26	Loss: 0.034520	Acc: 38.7% (3868/10000)
[Test]  Epoch: 27	Loss: 0.031793	Acc: 42.7% (4272/10000)
[Test]  Epoch: 28	Loss: 0.033248	Acc: 41.5% (4151/10000)
[Test]  Epoch: 29	Loss: 0.031843	Acc: 45.2% (4520/10000)
[Test]  Epoch: 30	Loss: 0.032133	Acc: 43.3% (4329/10000)
[Test]  Epoch: 31	Loss: 0.032349	Acc: 46.3% (4631/10000)
[Test]  Epoch: 32	Loss: 0.037843	Acc: 42.4% (4236/10000)
[Test]  Epoch: 33	Loss: 0.031664	Acc: 45.4% (4535/10000)
[Test]  Epoch: 34	Loss: 0.035905	Acc: 43.0% (4295/10000)
[Test]  Epoch: 35	Loss: 0.032461	Acc: 47.3% (4730/10000)
[Test]  Epoch: 36	Loss: 0.038475	Acc: 40.7% (4073/10000)
[Test]  Epoch: 37	Loss: 0.038162	Acc: 42.6% (4265/10000)
[Test]  Epoch: 38	Loss: 0.037135	Acc: 43.2% (4321/10000)
[Test]  Epoch: 39	Loss: 0.036177	Acc: 44.8% (4484/10000)
[Test]  Epoch: 40	Loss: 0.033593	Acc: 46.3% (4629/10000)
[Test]  Epoch: 41	Loss: 0.037272	Acc: 45.8% (4582/10000)
[Test]  Epoch: 42	Loss: 0.041564	Acc: 44.0% (4405/10000)
[Test]  Epoch: 43	Loss: 0.036659	Acc: 45.1% (4507/10000)
[Test]  Epoch: 44	Loss: 0.033139	Acc: 46.7% (4670/10000)
[Test]  Epoch: 45	Loss: 0.038510	Acc: 44.9% (4494/10000)
[Test]  Epoch: 46	Loss: 0.042935	Acc: 42.7% (4267/10000)
[Test]  Epoch: 47	Loss: 0.036268	Acc: 47.7% (4770/10000)
[Test]  Epoch: 48	Loss: 0.033254	Acc: 49.3% (4934/10000)
[Test]  Epoch: 49	Loss: 0.035940	Acc: 47.2% (4721/10000)
[Test]  Epoch: 50	Loss: 0.040708	Acc: 44.5% (4452/10000)
[Test]  Epoch: 51	Loss: 0.034769	Acc: 50.2% (5019/10000)
[Test]  Epoch: 52	Loss: 0.046283	Acc: 42.6% (4262/10000)
[Test]  Epoch: 53	Loss: 0.042651	Acc: 46.3% (4630/10000)
[Test]  Epoch: 54	Loss: 0.035911	Acc: 47.2% (4717/10000)
[Test]  Epoch: 55	Loss: 0.037186	Acc: 48.4% (4842/10000)
[Test]  Epoch: 56	Loss: 0.035547	Acc: 50.0% (5003/10000)
[Test]  Epoch: 57	Loss: 0.035587	Acc: 50.2% (5016/10000)
[Test]  Epoch: 58	Loss: 0.033244	Acc: 50.7% (5068/10000)
[Test]  Epoch: 59	Loss: 0.038137	Acc: 48.8% (4879/10000)
[Test]  Epoch: 60	Loss: 0.037679	Acc: 48.8% (4877/10000)
[Test]  Epoch: 61	Loss: 0.033957	Acc: 51.9% (5190/10000)
[Test]  Epoch: 62	Loss: 0.033216	Acc: 52.0% (5205/10000)
[Test]  Epoch: 63	Loss: 0.032611	Acc: 52.9% (5285/10000)
[Test]  Epoch: 64	Loss: 0.032724	Acc: 52.2% (5218/10000)
[Test]  Epoch: 65	Loss: 0.032282	Acc: 52.9% (5286/10000)
[Test]  Epoch: 66	Loss: 0.032499	Acc: 52.7% (5266/10000)
[Test]  Epoch: 67	Loss: 0.032430	Acc: 52.7% (5271/10000)
[Test]  Epoch: 68	Loss: 0.032426	Acc: 53.1% (5314/10000)
[Test]  Epoch: 69	Loss: 0.032756	Acc: 52.6% (5262/10000)
[Test]  Epoch: 70	Loss: 0.032464	Acc: 52.3% (5234/10000)
[Test]  Epoch: 71	Loss: 0.032300	Acc: 52.1% (5213/10000)
[Test]  Epoch: 72	Loss: 0.032686	Acc: 52.4% (5239/10000)
[Test]  Epoch: 73	Loss: 0.032619	Acc: 52.3% (5230/10000)
[Test]  Epoch: 74	Loss: 0.032596	Acc: 52.3% (5227/10000)
[Test]  Epoch: 75	Loss: 0.032405	Acc: 52.6% (5264/10000)
[Test]  Epoch: 76	Loss: 0.032503	Acc: 53.2% (5323/10000)
[Test]  Epoch: 77	Loss: 0.032566	Acc: 52.4% (5241/10000)
[Test]  Epoch: 78	Loss: 0.032412	Acc: 52.8% (5281/10000)
[Test]  Epoch: 79	Loss: 0.032202	Acc: 53.0% (5295/10000)
[Test]  Epoch: 80	Loss: 0.032392	Acc: 53.0% (5301/10000)
[Test]  Epoch: 81	Loss: 0.032462	Acc: 52.5% (5254/10000)
[Test]  Epoch: 82	Loss: 0.032529	Acc: 53.0% (5304/10000)
[Test]  Epoch: 83	Loss: 0.032184	Acc: 53.3% (5326/10000)
[Test]  Epoch: 84	Loss: 0.032577	Acc: 53.0% (5298/10000)
[Test]  Epoch: 85	Loss: 0.032418	Acc: 53.4% (5340/10000)
[Test]  Epoch: 86	Loss: 0.032772	Acc: 52.5% (5248/10000)
[Test]  Epoch: 87	Loss: 0.032234	Acc: 52.9% (5289/10000)
[Test]  Epoch: 88	Loss: 0.032033	Acc: 53.1% (5311/10000)
[Test]  Epoch: 89	Loss: 0.032663	Acc: 52.4% (5237/10000)
[Test]  Epoch: 90	Loss: 0.032481	Acc: 52.7% (5271/10000)
[Test]  Epoch: 91	Loss: 0.032158	Acc: 53.3% (5329/10000)
[Test]  Epoch: 92	Loss: 0.032518	Acc: 53.0% (5305/10000)
[Test]  Epoch: 93	Loss: 0.032063	Acc: 53.2% (5325/10000)
[Test]  Epoch: 94	Loss: 0.032326	Acc: 53.1% (5308/10000)
[Test]  Epoch: 95	Loss: 0.032505	Acc: 52.7% (5270/10000)
[Test]  Epoch: 96	Loss: 0.032257	Acc: 52.9% (5289/10000)
[Test]  Epoch: 97	Loss: 0.032183	Acc: 53.3% (5334/10000)
[Test]  Epoch: 98	Loss: 0.032823	Acc: 52.6% (5260/10000)
[Test]  Epoch: 99	Loss: 0.032995	Acc: 52.6% (5259/10000)
[Test]  Epoch: 100	Loss: 0.032211	Acc: 53.4% (5337/10000)
===========finish==========
['2024-08-18', '17:52:39.490480', '100', 'test', '0.03221119540929794', '53.37', '53.4']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight', 'features.24.weight', 'features.3.weight', 'features.27.weight', 'features.0.weight', 'features.30.weight', 'features.11.weight', 'features.34.weight', 'features.21.weight', 'features.15.weight', 'features.37.weight', 'features.18.weight', 'features.40.weight', 'features.4.weight', 'classifier.weight', 'features.8.weight', 'features.1.weight', 'features.25.weight', 'features.28.weight', 'features.41.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.110150	Acc: 12.8% (1283/10000)
[Test]  Epoch: 2	Loss: 0.039236	Acc: 10.6% (1060/10000)
[Test]  Epoch: 3	Loss: 0.039210	Acc: 10.9% (1089/10000)
[Test]  Epoch: 4	Loss: 0.054375	Acc: 12.1% (1206/10000)
[Test]  Epoch: 5	Loss: 0.062413	Acc: 11.6% (1160/10000)
[Test]  Epoch: 6	Loss: 0.044348	Acc: 10.3% (1032/10000)
[Test]  Epoch: 7	Loss: 0.037899	Acc: 10.6% (1058/10000)
[Test]  Epoch: 8	Loss: 0.036117	Acc: 10.6% (1061/10000)
[Test]  Epoch: 9	Loss: 0.041298	Acc: 10.2% (1016/10000)
[Test]  Epoch: 10	Loss: 0.038786	Acc: 12.1% (1211/10000)
[Test]  Epoch: 11	Loss: 0.035859	Acc: 13.6% (1357/10000)
[Test]  Epoch: 12	Loss: 0.035640	Acc: 14.4% (1441/10000)
[Test]  Epoch: 13	Loss: 0.034481	Acc: 16.5% (1647/10000)
[Test]  Epoch: 14	Loss: 0.032955	Acc: 20.6% (2063/10000)
[Test]  Epoch: 15	Loss: 0.078811	Acc: 18.0% (1799/10000)
[Test]  Epoch: 16	Loss: 0.154926	Acc: 14.9% (1492/10000)
[Test]  Epoch: 17	Loss: 0.035550	Acc: 23.5% (2348/10000)
[Test]  Epoch: 18	Loss: 0.032321	Acc: 20.0% (2000/10000)
[Test]  Epoch: 19	Loss: 0.032306	Acc: 21.1% (2114/10000)
[Test]  Epoch: 20	Loss: 0.032744	Acc: 17.6% (1765/10000)
[Test]  Epoch: 21	Loss: 0.034474	Acc: 23.5% (2352/10000)
[Test]  Epoch: 22	Loss: 0.031101	Acc: 26.8% (2680/10000)
[Test]  Epoch: 23	Loss: 0.030437	Acc: 25.6% (2564/10000)
[Test]  Epoch: 24	Loss: 0.030831	Acc: 30.6% (3058/10000)
[Test]  Epoch: 25	Loss: 0.029025	Acc: 29.8% (2981/10000)
[Test]  Epoch: 26	Loss: 0.029169	Acc: 31.0% (3100/10000)
[Test]  Epoch: 27	Loss: 0.033064	Acc: 25.2% (2517/10000)
[Test]  Epoch: 28	Loss: 0.031021	Acc: 29.7% (2969/10000)
[Test]  Epoch: 29	Loss: 0.032339	Acc: 32.0% (3197/10000)
[Test]  Epoch: 30	Loss: 0.041317	Acc: 30.7% (3073/10000)
[Test]  Epoch: 31	Loss: 0.030015	Acc: 31.8% (3182/10000)
[Test]  Epoch: 32	Loss: 0.027932	Acc: 34.3% (3427/10000)
[Test]  Epoch: 33	Loss: 0.026868	Acc: 37.0% (3701/10000)
[Test]  Epoch: 34	Loss: 0.028135	Acc: 34.7% (3467/10000)
[Test]  Epoch: 35	Loss: 0.035444	Acc: 37.1% (3714/10000)
[Test]  Epoch: 36	Loss: 0.038549	Acc: 30.4% (3035/10000)
[Test]  Epoch: 37	Loss: 0.027750	Acc: 38.0% (3795/10000)
[Test]  Epoch: 38	Loss: 0.031312	Acc: 34.3% (3430/10000)
[Test]  Epoch: 39	Loss: 0.031701	Acc: 36.9% (3694/10000)
[Test]  Epoch: 40	Loss: 0.046296	Acc: 26.2% (2620/10000)
[Test]  Epoch: 41	Loss: 0.027866	Acc: 36.0% (3605/10000)
[Test]  Epoch: 42	Loss: 0.027042	Acc: 40.1% (4013/10000)
[Test]  Epoch: 43	Loss: 0.026524	Acc: 40.3% (4031/10000)
[Test]  Epoch: 44	Loss: 0.030358	Acc: 39.1% (3911/10000)
[Test]  Epoch: 45	Loss: 0.030427	Acc: 35.3% (3529/10000)
[Test]  Epoch: 46	Loss: 0.032982	Acc: 32.9% (3287/10000)
[Test]  Epoch: 47	Loss: 0.027644	Acc: 39.9% (3991/10000)
[Test]  Epoch: 48	Loss: 0.034487	Acc: 32.9% (3292/10000)
[Test]  Epoch: 49	Loss: 0.029058	Acc: 38.2% (3816/10000)
[Test]  Epoch: 50	Loss: 0.031688	Acc: 36.5% (3649/10000)
[Test]  Epoch: 51	Loss: 0.026418	Acc: 43.1% (4306/10000)
[Test]  Epoch: 52	Loss: 0.035864	Acc: 36.2% (3622/10000)
[Test]  Epoch: 53	Loss: 0.029508	Acc: 40.1% (4015/10000)
[Test]  Epoch: 54	Loss: 0.033551	Acc: 38.6% (3864/10000)
[Test]  Epoch: 55	Loss: 0.045039	Acc: 32.9% (3287/10000)
[Test]  Epoch: 56	Loss: 0.033035	Acc: 38.0% (3798/10000)
[Test]  Epoch: 57	Loss: 0.030983	Acc: 41.0% (4100/10000)
[Test]  Epoch: 58	Loss: 0.029563	Acc: 42.6% (4258/10000)
[Test]  Epoch: 59	Loss: 0.036971	Acc: 39.7% (3966/10000)
[Test]  Epoch: 60	Loss: 0.030531	Acc: 42.0% (4197/10000)
[Test]  Epoch: 61	Loss: 0.026735	Acc: 48.5% (4848/10000)
[Test]  Epoch: 62	Loss: 0.026307	Acc: 49.0% (4900/10000)
[Test]  Epoch: 63	Loss: 0.026823	Acc: 49.1% (4907/10000)
[Test]  Epoch: 64	Loss: 0.027538	Acc: 48.6% (4857/10000)
[Test]  Epoch: 65	Loss: 0.028057	Acc: 48.7% (4874/10000)
[Test]  Epoch: 66	Loss: 0.028873	Acc: 49.3% (4926/10000)
[Test]  Epoch: 67	Loss: 0.029974	Acc: 49.1% (4910/10000)
[Test]  Epoch: 68	Loss: 0.030703	Acc: 49.1% (4909/10000)
[Test]  Epoch: 69	Loss: 0.030929	Acc: 49.7% (4974/10000)
[Test]  Epoch: 70	Loss: 0.031194	Acc: 49.8% (4984/10000)
[Test]  Epoch: 71	Loss: 0.033867	Acc: 49.9% (4990/10000)
[Test]  Epoch: 72	Loss: 0.034099	Acc: 49.1% (4909/10000)
[Test]  Epoch: 73	Loss: 0.033191	Acc: 50.1% (5009/10000)
[Test]  Epoch: 74	Loss: 0.034347	Acc: 49.3% (4928/10000)
[Test]  Epoch: 75	Loss: 0.035584	Acc: 49.1% (4906/10000)
[Test]  Epoch: 76	Loss: 0.035141	Acc: 49.3% (4934/10000)
[Test]  Epoch: 77	Loss: 0.036316	Acc: 49.3% (4932/10000)
[Test]  Epoch: 78	Loss: 0.038568	Acc: 48.1% (4810/10000)
[Test]  Epoch: 79	Loss: 0.038688	Acc: 49.6% (4961/10000)
[Test]  Epoch: 80	Loss: 0.038648	Acc: 46.6% (4665/10000)
[Test]  Epoch: 81	Loss: 0.039052	Acc: 48.9% (4888/10000)
[Test]  Epoch: 82	Loss: 0.040230	Acc: 48.7% (4868/10000)
[Test]  Epoch: 83	Loss: 0.039003	Acc: 49.5% (4948/10000)
[Test]  Epoch: 84	Loss: 0.040598	Acc: 48.7% (4866/10000)
[Test]  Epoch: 85	Loss: 0.041436	Acc: 48.9% (4885/10000)
[Test]  Epoch: 86	Loss: 0.042914	Acc: 48.5% (4848/10000)
[Test]  Epoch: 87	Loss: 0.041497	Acc: 48.4% (4835/10000)
[Test]  Epoch: 88	Loss: 0.042768	Acc: 48.5% (4853/10000)
[Test]  Epoch: 89	Loss: 0.042291	Acc: 48.9% (4890/10000)
[Test]  Epoch: 90	Loss: 0.042941	Acc: 49.1% (4908/10000)
[Test]  Epoch: 91	Loss: 0.043479	Acc: 48.5% (4852/10000)
[Test]  Epoch: 92	Loss: 0.043667	Acc: 48.9% (4889/10000)
[Test]  Epoch: 93	Loss: 0.042848	Acc: 50.1% (5015/10000)
[Test]  Epoch: 94	Loss: 0.044379	Acc: 49.2% (4923/10000)
[Test]  Epoch: 95	Loss: 0.046874	Acc: 47.7% (4769/10000)
[Test]  Epoch: 96	Loss: 0.043633	Acc: 49.1% (4911/10000)
[Test]  Epoch: 97	Loss: 0.043717	Acc: 49.8% (4981/10000)
[Test]  Epoch: 98	Loss: 0.046282	Acc: 48.3% (4831/10000)
[Test]  Epoch: 99	Loss: 0.045552	Acc: 49.4% (4941/10000)
[Test]  Epoch: 100	Loss: 0.045308	Acc: 49.8% (4978/10000)
===========finish==========
['2024-08-18', '17:55:11.614324', '100', 'test', '0.04530764973163605', '49.78', '50.15']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-vgg16_bn-channel vgg16_bn CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.14.weight', 'features.17.weight', 'features.20.weight', 'features.10.weight', 'features.7.weight', 'features.24.weight', 'features.3.weight', 'features.27.weight', 'features.0.weight', 'features.30.weight', 'features.11.weight', 'features.34.weight', 'features.21.weight', 'features.15.weight', 'features.37.weight', 'features.18.weight', 'features.40.weight', 'features.4.weight', 'classifier.weight', 'features.8.weight', 'features.1.weight', 'features.25.weight', 'features.28.weight', 'features.41.weight', 'features.31.weight', 'features.35.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 1.193423	Acc: 12.8% (1282/10000)
[Test]  Epoch: 2	Loss: 0.038363	Acc: 11.9% (1191/10000)
[Test]  Epoch: 3	Loss: 0.036454	Acc: 10.7% (1071/10000)
[Test]  Epoch: 4	Loss: 0.036252	Acc: 10.3% (1028/10000)
[Test]  Epoch: 5	Loss: 0.038376	Acc: 10.9% (1086/10000)
[Test]  Epoch: 6	Loss: 0.038062	Acc: 11.3% (1131/10000)
[Test]  Epoch: 7	Loss: 0.036262	Acc: 10.1% (1013/10000)
[Test]  Epoch: 8	Loss: 0.035807	Acc: 11.6% (1163/10000)
[Test]  Epoch: 9	Loss: 0.035539	Acc: 12.2% (1225/10000)
[Test]  Epoch: 10	Loss: 0.034852	Acc: 15.8% (1579/10000)
[Test]  Epoch: 11	Loss: 0.034950	Acc: 17.4% (1738/10000)
[Test]  Epoch: 12	Loss: 0.035143	Acc: 20.3% (2027/10000)
[Test]  Epoch: 13	Loss: 0.032283	Acc: 20.2% (2022/10000)
[Test]  Epoch: 14	Loss: 0.031289	Acc: 21.9% (2194/10000)
[Test]  Epoch: 15	Loss: 0.038000	Acc: 22.7% (2270/10000)
[Test]  Epoch: 16	Loss: 0.031969	Acc: 26.0% (2598/10000)
[Test]  Epoch: 17	Loss: 0.035544	Acc: 26.4% (2643/10000)
[Test]  Epoch: 18	Loss: 0.030436	Acc: 27.1% (2709/10000)
[Test]  Epoch: 19	Loss: 0.034686	Acc: 25.6% (2557/10000)
[Test]  Epoch: 20	Loss: 0.030627	Acc: 28.4% (2843/10000)
[Test]  Epoch: 21	Loss: 0.030000	Acc: 27.4% (2737/10000)
[Test]  Epoch: 22	Loss: 0.030411	Acc: 30.1% (3010/10000)
[Test]  Epoch: 23	Loss: 0.033453	Acc: 28.1% (2815/10000)
[Test]  Epoch: 24	Loss: 0.028245	Acc: 32.3% (3226/10000)
[Test]  Epoch: 25	Loss: 0.030088	Acc: 33.3% (3333/10000)
[Test]  Epoch: 26	Loss: 0.028732	Acc: 33.9% (3386/10000)
[Test]  Epoch: 27	Loss: 0.030473	Acc: 30.9% (3094/10000)
[Test]  Epoch: 28	Loss: 0.028054	Acc: 34.5% (3453/10000)
[Test]  Epoch: 29	Loss: 0.029740	Acc: 34.6% (3459/10000)
[Test]  Epoch: 30	Loss: 0.029695	Acc: 35.9% (3585/10000)
[Test]  Epoch: 31	Loss: 0.028185	Acc: 37.7% (3767/10000)
[Test]  Epoch: 32	Loss: 0.028798	Acc: 35.6% (3558/10000)
[Test]  Epoch: 33	Loss: 0.036038	Acc: 36.1% (3611/10000)
[Test]  Epoch: 34	Loss: 0.028804	Acc: 34.8% (3482/10000)
[Test]  Epoch: 35	Loss: 0.031148	Acc: 36.5% (3651/10000)
[Test]  Epoch: 36	Loss: 0.031406	Acc: 33.6% (3365/10000)
[Test]  Epoch: 37	Loss: 0.030287	Acc: 37.1% (3710/10000)
[Test]  Epoch: 38	Loss: 0.028688	Acc: 38.4% (3835/10000)
[Test]  Epoch: 39	Loss: 0.030085	Acc: 38.0% (3796/10000)
[Test]  Epoch: 40	Loss: 0.029592	Acc: 39.2% (3917/10000)
[Test]  Epoch: 41	Loss: 0.032301	Acc: 39.3% (3933/10000)
[Test]  Epoch: 42	Loss: 0.028930	Acc: 41.8% (4180/10000)
[Test]  Epoch: 43	Loss: 0.034706	Acc: 40.7% (4066/10000)
[Test]  Epoch: 44	Loss: 0.032614	Acc: 42.8% (4282/10000)
[Test]  Epoch: 45	Loss: 0.037369	Acc: 36.4% (3638/10000)
[Test]  Epoch: 46	Loss: 0.036369	Acc: 39.9% (3990/10000)
[Test]  Epoch: 47	Loss: 0.037015	Acc: 38.2% (3818/10000)
[Test]  Epoch: 48	Loss: 0.037686	Acc: 39.5% (3948/10000)
[Test]  Epoch: 49	Loss: 0.035500	Acc: 38.2% (3825/10000)
[Test]  Epoch: 50	Loss: 0.036224	Acc: 39.5% (3952/10000)
[Test]  Epoch: 51	Loss: 0.032977	Acc: 41.7% (4173/10000)
[Test]  Epoch: 52	Loss: 0.041145	Acc: 36.5% (3652/10000)
[Test]  Epoch: 53	Loss: 0.038189	Acc: 34.2% (3416/10000)
[Test]  Epoch: 54	Loss: 0.034974	Acc: 40.0% (4002/10000)
[Test]  Epoch: 55	Loss: 0.039455	Acc: 38.1% (3809/10000)
[Test]  Epoch: 56	Loss: 0.040018	Acc: 40.8% (4081/10000)
[Test]  Epoch: 57	Loss: 0.041454	Acc: 37.2% (3725/10000)
[Test]  Epoch: 58	Loss: 0.042598	Acc: 41.1% (4108/10000)
[Test]  Epoch: 59	Loss: 0.032516	Acc: 45.6% (4560/10000)
[Test]  Epoch: 60	Loss: 0.033483	Acc: 45.0% (4501/10000)
[Test]  Epoch: 61	Loss: 0.029548	Acc: 47.9% (4789/10000)
[Test]  Epoch: 62	Loss: 0.029760	Acc: 48.7% (4869/10000)
[Test]  Epoch: 63	Loss: 0.030361	Acc: 48.7% (4869/10000)
[Test]  Epoch: 64	Loss: 0.031325	Acc: 48.4% (4843/10000)
[Test]  Epoch: 65	Loss: 0.032285	Acc: 48.8% (4884/10000)
[Test]  Epoch: 66	Loss: 0.032896	Acc: 48.8% (4883/10000)
[Test]  Epoch: 67	Loss: 0.034056	Acc: 49.0% (4904/10000)
[Test]  Epoch: 68	Loss: 0.035003	Acc: 48.9% (4894/10000)
[Test]  Epoch: 69	Loss: 0.035295	Acc: 49.1% (4913/10000)
[Test]  Epoch: 70	Loss: 0.036330	Acc: 49.3% (4927/10000)
[Test]  Epoch: 71	Loss: 0.036554	Acc: 48.9% (4887/10000)
[Test]  Epoch: 72	Loss: 0.037451	Acc: 48.8% (4879/10000)
[Test]  Epoch: 73	Loss: 0.037984	Acc: 48.5% (4852/10000)
[Test]  Epoch: 74	Loss: 0.038290	Acc: 48.8% (4879/10000)
[Test]  Epoch: 75	Loss: 0.038847	Acc: 48.7% (4874/10000)
[Test]  Epoch: 76	Loss: 0.038371	Acc: 48.9% (4886/10000)
[Test]  Epoch: 77	Loss: 0.039423	Acc: 48.1% (4807/10000)
[Test]  Epoch: 78	Loss: 0.041454	Acc: 47.6% (4765/10000)
[Test]  Epoch: 79	Loss: 0.040396	Acc: 48.6% (4856/10000)
[Test]  Epoch: 80	Loss: 0.040263	Acc: 49.3% (4932/10000)
[Test]  Epoch: 81	Loss: 0.042087	Acc: 48.6% (4861/10000)
[Test]  Epoch: 82	Loss: 0.043396	Acc: 48.9% (4885/10000)
[Test]  Epoch: 83	Loss: 0.041110	Acc: 49.8% (4978/10000)
[Test]  Epoch: 84	Loss: 0.042542	Acc: 48.9% (4886/10000)
[Test]  Epoch: 85	Loss: 0.042482	Acc: 49.0% (4902/10000)
[Test]  Epoch: 86	Loss: 0.042686	Acc: 49.0% (4896/10000)
[Test]  Epoch: 87	Loss: 0.043586	Acc: 48.6% (4863/10000)
[Test]  Epoch: 88	Loss: 0.043057	Acc: 49.9% (4993/10000)
[Test]  Epoch: 89	Loss: 0.045203	Acc: 49.1% (4908/10000)
[Test]  Epoch: 90	Loss: 0.043978	Acc: 49.0% (4896/10000)
[Test]  Epoch: 91	Loss: 0.044854	Acc: 48.9% (4889/10000)
[Test]  Epoch: 92	Loss: 0.044861	Acc: 49.1% (4913/10000)
[Test]  Epoch: 93	Loss: 0.045340	Acc: 49.1% (4911/10000)
[Test]  Epoch: 94	Loss: 0.045459	Acc: 49.0% (4902/10000)
[Test]  Epoch: 95	Loss: 0.045721	Acc: 48.4% (4843/10000)
[Test]  Epoch: 96	Loss: 0.045538	Acc: 49.1% (4910/10000)
[Test]  Epoch: 97	Loss: 0.046757	Acc: 49.3% (4929/10000)
[Test]  Epoch: 98	Loss: 0.047442	Acc: 48.8% (4877/10000)
[Test]  Epoch: 99	Loss: 0.046045	Acc: 49.3% (4927/10000)
[Test]  Epoch: 100	Loss: 0.046263	Acc: 49.5% (4954/10000)
===========finish==========
['2024-08-18', '17:57:44.256405', '100', 'test', '0.04626308670043945', '49.54', '49.93']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info does not exist. Calculating...
Load model from models/victim/cifar10-mobilenetv2/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('_features.18.0.weight', 21258047488.0), ('last_linear.weight', 600940.3125), ('_features.17.conv.3.weight', 26528.8671875), ('_features.17.conv.2.weight', 16884.78515625), ('_features.17.conv.0.0.weight', 1648.6458740234375), ('_features.17.conv.1.0.weight', 1068.0887451171875), ('_features.17.conv.0.1.weight', 844.4431762695312), ('_features.14.conv.0.1.weight', 598.0960083007812), ('_features.17.conv.1.1.weight', 512.1967163085938), ('_features.14.conv.2.weight', 103.00106811523438), ('_features.14.conv.1.0.weight', 87.28956604003906), ('_features.14.conv.1.1.weight', 62.855648040771484), ('_features.14.conv.3.weight', 50.39166259765625), ('_features.16.conv.2.weight', 45.34526443481445), ('_features.4.conv.2.weight', 41.198387145996094), ('_features.15.conv.2.weight', 35.18214416503906), ('_features.14.conv.0.0.weight', 34.2755241394043), ('_features.16.conv.0.0.weight', 31.965084075927734), ('_features.11.conv.2.weight', 30.192562103271484), ('_features.11.conv.0.0.weight', 29.891780853271484), ('_features.9.conv.2.weight', 27.648359298706055), ('_features.1.conv.1.weight', 27.055866241455078), ('_features.7.conv.2.weight', 24.98480224609375), ('_features.11.conv.1.0.weight', 23.782424926757812), ('_features.6.conv.2.weight', 23.24270248413086), ('_features.1.conv.0.0.weight', 22.72513198852539), ('_features.16.conv.1.1.weight', 22.615304946899414), ('_features.16.conv.3.weight', 21.896507263183594), ('_features.10.conv.2.weight', 21.720027923583984), ('_features.3.conv.2.weight', 21.662799835205078), ('_features.2.conv.2.weight', 20.987621307373047), ('_features.9.conv.0.0.weight', 19.095090866088867), ('_features.3.conv.1.0.weight', 17.588451385498047), ('_features.15.conv.0.0.weight', 17.161771774291992), ('_features.15.conv.3.weight', 16.91714096069336), ('_features.7.conv.0.0.weight', 14.894979476928711), ('_features.5.conv.2.weight', 13.844121932983398), ('_features.8.conv.2.weight', 13.077081680297852), ('_features.4.conv.0.0.weight', 12.738638877868652), ('_features.11.conv.0.1.weight', 12.237621307373047), ('_features.6.conv.1.0.weight', 11.879636764526367), ('_features.6.conv.0.0.weight', 11.096017837524414), ('_features.16.conv.1.0.weight', 10.733545303344727), ('_features.11.conv.3.weight', 10.724739074707031), ('_features.9.conv.0.1.weight', 10.633391380310059), ('_features.16.conv.0.1.weight', 9.797460556030273), ('_features.12.conv.2.weight', 9.790807723999023), ('_features.18.1.weight', 9.677230834960938), ('_features.9.conv.1.0.weight', 9.656977653503418), ('_features.13.conv.2.weight', 9.584346771240234), ('_features.10.conv.0.0.weight', 9.50909423828125), ('_features.11.conv.1.1.weight', 9.447509765625), ('_features.4.conv.0.1.weight', 9.28803539276123), ('_features.12.conv.1.0.weight', 8.947385787963867), ('_features.6.conv.0.1.weight', 7.804950714111328), ('_features.8.conv.0.0.weight', 7.200871467590332), ('_features.2.conv.1.0.weight', 6.871476173400879), ('_features.7.conv.0.1.weight', 6.741973400115967), ('_features.7.conv.1.0.weight', 5.96169376373291), ('_features.2.conv.0.1.weight', 5.862419605255127), ('_features.4.conv.1.0.weight', 5.543389320373535), ('_features.3.conv.0.1.weight', 5.541204929351807), ('_features.5.conv.1.0.weight', 5.179028511047363), ('_features.8.conv.1.0.weight', 4.897006034851074), ('_features.6.conv.1.1.weight', 4.242184638977051), ('_features.2.conv.0.0.weight', 4.161554336547852), ('_features.0.0.weight', 4.142943859100342), ('_features.10.conv.1.0.weight', 3.9631600379943848), ('_features.4.conv.1.1.weight', 3.9430348873138428), ('_features.5.conv.0.0.weight', 3.559892416000366), ('_features.9.conv.1.1.weight', 3.0380523204803467), ('_features.12.conv.0.0.weight', 2.578495979309082), ('_features.3.conv.1.1.weight', 2.474125385284424), ('_features.7.conv.1.1.weight', 2.3523378372192383), ('_features.7.conv.3.weight', 2.2441539764404297), ('_features.12.conv.0.1.weight', 2.2329893112182617), ('_features.9.conv.3.weight', 2.12808895111084), ('_features.15.conv.0.1.weight', 2.1001462936401367), ('_features.3.conv.0.0.weight', 2.092738151550293), ('_features.4.conv.3.weight', 2.064646005630493), ('_features.5.conv.0.1.weight', 1.8339357376098633), ('_features.2.conv.1.1.weight', 1.812380313873291), ('_features.10.conv.3.weight', 1.786267638206482), ('_features.13.conv.1.0.weight', 1.7823450565338135), ('_features.8.conv.0.1.weight', 1.7506654262542725), ('_features.8.conv.1.1.weight', 1.7157334089279175), ('_features.12.conv.3.weight', 1.585655689239502), ('_features.10.conv.0.1.weight', 1.4203755855560303), ('_features.10.conv.1.1.weight', 1.3814899921417236), ('_features.15.conv.1.0.weight', 1.2476004362106323), ('_features.5.conv.1.1.weight', 1.1807622909545898), ('_features.13.conv.3.weight', 1.084709644317627), ('_features.2.conv.3.weight', 1.0810484886169434), ('_features.6.conv.3.weight', 1.001222848892212), ('_features.8.conv.3.weight', 0.9308938384056091), ('_features.12.conv.1.1.weight', 0.7756733894348145), ('_features.1.conv.2.weight', 0.7668923139572144), ('_features.15.conv.1.1.weight', 0.7535480856895447), ('_features.3.conv.3.weight', 0.7351734042167664), ('_features.5.conv.3.weight', 0.6219322085380554), ('_features.0.1.weight', 0.5564303398132324), ('_features.1.conv.0.1.weight', 0.5295460224151611), ('_features.13.conv.0.1.weight', 0.4405973553657532), ('_features.13.conv.1.1.weight', 0.40185701847076416), ('_features.13.conv.0.0.weight', 0.1520349085330963), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('_features.18.0.weight', 21258047488.0), ('last_linear.weight', 600940.3125), ('_features.17.conv.3.weight', 26528.8671875), ('_features.17.conv.2.weight', 16884.78515625), ('_features.17.conv.0.0.weight', 1648.6458740234375), ('_features.17.conv.1.0.weight', 1068.0887451171875), ('_features.17.conv.0.1.weight', 844.4431762695312), ('_features.14.conv.0.1.weight', 598.0960083007812), ('_features.17.conv.1.1.weight', 512.1967163085938), ('_features.14.conv.2.weight', 103.00106811523438), ('_features.14.conv.1.0.weight', 87.28956604003906), ('_features.14.conv.1.1.weight', 62.855648040771484), ('_features.14.conv.3.weight', 50.39166259765625), ('_features.16.conv.2.weight', 45.34526443481445), ('_features.4.conv.2.weight', 41.198387145996094), ('_features.15.conv.2.weight', 35.18214416503906), ('_features.14.conv.0.0.weight', 34.2755241394043), ('_features.16.conv.0.0.weight', 31.965084075927734), ('_features.11.conv.2.weight', 30.192562103271484), ('_features.11.conv.0.0.weight', 29.891780853271484), ('_features.9.conv.2.weight', 27.648359298706055), ('_features.1.conv.1.weight', 27.055866241455078), ('_features.7.conv.2.weight', 24.98480224609375), ('_features.11.conv.1.0.weight', 23.782424926757812), ('_features.6.conv.2.weight', 23.24270248413086), ('_features.1.conv.0.0.weight', 22.72513198852539), ('_features.16.conv.1.1.weight', 22.615304946899414), ('_features.16.conv.3.weight', 21.896507263183594), ('_features.10.conv.2.weight', 21.720027923583984), ('_features.3.conv.2.weight', 21.662799835205078), ('_features.2.conv.2.weight', 20.987621307373047), ('_features.9.conv.0.0.weight', 19.095090866088867), ('_features.3.conv.1.0.weight', 17.588451385498047), ('_features.15.conv.0.0.weight', 17.161771774291992), ('_features.15.conv.3.weight', 16.91714096069336), ('_features.7.conv.0.0.weight', 14.894979476928711), ('_features.5.conv.2.weight', 13.844121932983398), ('_features.8.conv.2.weight', 13.077081680297852), ('_features.4.conv.0.0.weight', 12.738638877868652), ('_features.11.conv.0.1.weight', 12.237621307373047), ('_features.6.conv.1.0.weight', 11.879636764526367), ('_features.6.conv.0.0.weight', 11.096017837524414), ('_features.16.conv.1.0.weight', 10.733545303344727), ('_features.11.conv.3.weight', 10.724739074707031), ('_features.9.conv.0.1.weight', 10.633391380310059), ('_features.16.conv.0.1.weight', 9.797460556030273), ('_features.12.conv.2.weight', 9.790807723999023), ('_features.18.1.weight', 9.677230834960938), ('_features.9.conv.1.0.weight', 9.656977653503418), ('_features.13.conv.2.weight', 9.584346771240234), ('_features.10.conv.0.0.weight', 9.50909423828125), ('_features.11.conv.1.1.weight', 9.447509765625), ('_features.4.conv.0.1.weight', 9.28803539276123), ('_features.12.conv.1.0.weight', 8.947385787963867), ('_features.6.conv.0.1.weight', 7.804950714111328), ('_features.8.conv.0.0.weight', 7.200871467590332), ('_features.2.conv.1.0.weight', 6.871476173400879), ('_features.7.conv.0.1.weight', 6.741973400115967), ('_features.7.conv.1.0.weight', 5.96169376373291), ('_features.2.conv.0.1.weight', 5.862419605255127), ('_features.4.conv.1.0.weight', 5.543389320373535), ('_features.3.conv.0.1.weight', 5.541204929351807), ('_features.5.conv.1.0.weight', 5.179028511047363), ('_features.8.conv.1.0.weight', 4.897006034851074), ('_features.6.conv.1.1.weight', 4.242184638977051), ('_features.2.conv.0.0.weight', 4.161554336547852), ('_features.0.0.weight', 4.142943859100342), ('_features.10.conv.1.0.weight', 3.9631600379943848), ('_features.4.conv.1.1.weight', 3.9430348873138428), ('_features.5.conv.0.0.weight', 3.559892416000366), ('_features.9.conv.1.1.weight', 3.0380523204803467), ('_features.12.conv.0.0.weight', 2.578495979309082), ('_features.3.conv.1.1.weight', 2.474125385284424), ('_features.7.conv.1.1.weight', 2.3523378372192383), ('_features.7.conv.3.weight', 2.2441539764404297), ('_features.12.conv.0.1.weight', 2.2329893112182617), ('_features.9.conv.3.weight', 2.12808895111084), ('_features.15.conv.0.1.weight', 2.1001462936401367), ('_features.3.conv.0.0.weight', 2.092738151550293), ('_features.4.conv.3.weight', 2.064646005630493), ('_features.5.conv.0.1.weight', 1.8339357376098633), ('_features.2.conv.1.1.weight', 1.812380313873291), ('_features.10.conv.3.weight', 1.786267638206482), ('_features.13.conv.1.0.weight', 1.7823450565338135), ('_features.8.conv.0.1.weight', 1.7506654262542725), ('_features.8.conv.1.1.weight', 1.7157334089279175), ('_features.12.conv.3.weight', 1.585655689239502), ('_features.10.conv.0.1.weight', 1.4203755855560303), ('_features.10.conv.1.1.weight', 1.3814899921417236), ('_features.15.conv.1.0.weight', 1.2476004362106323), ('_features.5.conv.1.1.weight', 1.1807622909545898), ('_features.13.conv.3.weight', 1.084709644317627), ('_features.2.conv.3.weight', 1.0810484886169434), ('_features.6.conv.3.weight', 1.001222848892212), ('_features.8.conv.3.weight', 0.9308938384056091), ('_features.12.conv.1.1.weight', 0.7756733894348145), ('_features.1.conv.2.weight', 0.7668923139572144), ('_features.15.conv.1.1.weight', 0.7535480856895447), ('_features.3.conv.3.weight', 0.7351734042167664), ('_features.5.conv.3.weight', 0.6219322085380554), ('_features.0.1.weight', 0.5564303398132324), ('_features.1.conv.0.1.weight', 0.5295460224151611), ('_features.13.conv.0.1.weight', 0.4405973553657532), ('_features.13.conv.1.1.weight', 0.40185701847076416), ('_features.13.conv.0.0.weight', 0.1520349085330963), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=105  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.030100	Acc: 51.6% (5163/10000)
[Test]  Epoch: 2	Loss: 0.014154	Acc: 70.3% (7034/10000)
[Test]  Epoch: 3	Loss: 0.016382	Acc: 66.1% (6611/10000)
[Test]  Epoch: 4	Loss: 0.011821	Acc: 73.9% (7394/10000)
[Test]  Epoch: 5	Loss: 0.011302	Acc: 75.5% (7552/10000)
[Test]  Epoch: 6	Loss: 0.010738	Acc: 77.4% (7741/10000)
[Test]  Epoch: 7	Loss: 0.010517	Acc: 77.0% (7704/10000)
[Test]  Epoch: 8	Loss: 0.010750	Acc: 76.5% (7652/10000)
[Test]  Epoch: 9	Loss: 0.010140	Acc: 78.4% (7838/10000)
[Test]  Epoch: 10	Loss: 0.010193	Acc: 78.1% (7810/10000)
[Test]  Epoch: 11	Loss: 0.010121	Acc: 78.0% (7805/10000)
[Test]  Epoch: 12	Loss: 0.010011	Acc: 78.4% (7844/10000)
[Test]  Epoch: 13	Loss: 0.009994	Acc: 78.3% (7834/10000)
[Test]  Epoch: 14	Loss: 0.009935	Acc: 78.4% (7844/10000)
[Test]  Epoch: 15	Loss: 0.009954	Acc: 78.8% (7878/10000)
[Test]  Epoch: 16	Loss: 0.010257	Acc: 78.3% (7827/10000)
[Test]  Epoch: 17	Loss: 0.010049	Acc: 78.7% (7867/10000)
[Test]  Epoch: 18	Loss: 0.009981	Acc: 78.5% (7851/10000)
[Test]  Epoch: 19	Loss: 0.009990	Acc: 78.3% (7830/10000)
[Test]  Epoch: 20	Loss: 0.009975	Acc: 78.5% (7852/10000)
[Test]  Epoch: 21	Loss: 0.009812	Acc: 79.2% (7916/10000)
[Test]  Epoch: 22	Loss: 0.009723	Acc: 78.9% (7891/10000)
[Test]  Epoch: 23	Loss: 0.010096	Acc: 78.7% (7869/10000)
[Test]  Epoch: 24	Loss: 0.010049	Acc: 78.3% (7835/10000)
[Test]  Epoch: 25	Loss: 0.009804	Acc: 79.0% (7896/10000)
[Test]  Epoch: 26	Loss: 0.009791	Acc: 79.0% (7895/10000)
[Test]  Epoch: 27	Loss: 0.010045	Acc: 78.8% (7881/10000)
[Test]  Epoch: 28	Loss: 0.010133	Acc: 78.0% (7799/10000)
[Test]  Epoch: 29	Loss: 0.009946	Acc: 78.6% (7864/10000)
[Test]  Epoch: 30	Loss: 0.009948	Acc: 78.7% (7874/10000)
[Test]  Epoch: 31	Loss: 0.009733	Acc: 79.3% (7928/10000)
[Test]  Epoch: 32	Loss: 0.009866	Acc: 79.2% (7919/10000)
[Test]  Epoch: 33	Loss: 0.009828	Acc: 78.6% (7861/10000)
[Test]  Epoch: 34	Loss: 0.010056	Acc: 78.5% (7855/10000)
[Test]  Epoch: 35	Loss: 0.009778	Acc: 79.4% (7937/10000)
[Test]  Epoch: 36	Loss: 0.009974	Acc: 78.9% (7892/10000)
[Test]  Epoch: 37	Loss: 0.009797	Acc: 79.0% (7899/10000)
[Test]  Epoch: 38	Loss: 0.010793	Acc: 75.8% (7576/10000)
[Test]  Epoch: 39	Loss: 0.009972	Acc: 78.6% (7859/10000)
[Test]  Epoch: 40	Loss: 0.009907	Acc: 78.8% (7880/10000)
[Test]  Epoch: 41	Loss: 0.009920	Acc: 78.6% (7856/10000)
[Test]  Epoch: 42	Loss: 0.009912	Acc: 78.8% (7879/10000)
[Test]  Epoch: 43	Loss: 0.010089	Acc: 78.3% (7835/10000)
[Test]  Epoch: 44	Loss: 0.009800	Acc: 79.0% (7903/10000)
[Test]  Epoch: 45	Loss: 0.009985	Acc: 78.6% (7859/10000)
[Test]  Epoch: 46	Loss: 0.009909	Acc: 78.9% (7887/10000)
[Test]  Epoch: 47	Loss: 0.010009	Acc: 78.3% (7835/10000)
[Test]  Epoch: 48	Loss: 0.009927	Acc: 78.8% (7875/10000)
[Test]  Epoch: 49	Loss: 0.010084	Acc: 78.4% (7841/10000)
[Test]  Epoch: 50	Loss: 0.009963	Acc: 78.7% (7868/10000)
[Test]  Epoch: 51	Loss: 0.009887	Acc: 78.6% (7860/10000)
[Test]  Epoch: 52	Loss: 0.009951	Acc: 78.9% (7891/10000)
[Test]  Epoch: 53	Loss: 0.010122	Acc: 78.6% (7863/10000)
[Test]  Epoch: 54	Loss: 0.009974	Acc: 78.6% (7862/10000)
[Test]  Epoch: 55	Loss: 0.009933	Acc: 78.7% (7870/10000)
[Test]  Epoch: 56	Loss: 0.009862	Acc: 79.1% (7912/10000)
[Test]  Epoch: 57	Loss: 0.010158	Acc: 78.2% (7822/10000)
[Test]  Epoch: 58	Loss: 0.010003	Acc: 78.6% (7862/10000)
[Test]  Epoch: 59	Loss: 0.009923	Acc: 78.7% (7866/10000)
[Test]  Epoch: 60	Loss: 0.010047	Acc: 78.6% (7862/10000)
[Test]  Epoch: 61	Loss: 0.009862	Acc: 78.9% (7890/10000)
[Test]  Epoch: 62	Loss: 0.009837	Acc: 78.9% (7887/10000)
[Test]  Epoch: 63	Loss: 0.009849	Acc: 79.1% (7907/10000)
[Test]  Epoch: 64	Loss: 0.009820	Acc: 79.0% (7901/10000)
[Test]  Epoch: 65	Loss: 0.009790	Acc: 79.1% (7908/10000)
[Test]  Epoch: 66	Loss: 0.009811	Acc: 79.1% (7911/10000)
[Test]  Epoch: 67	Loss: 0.009858	Acc: 79.0% (7897/10000)
[Test]  Epoch: 68	Loss: 0.009798	Acc: 79.0% (7902/10000)
[Test]  Epoch: 69	Loss: 0.009791	Acc: 79.0% (7899/10000)
[Test]  Epoch: 70	Loss: 0.009790	Acc: 79.1% (7912/10000)
[Test]  Epoch: 71	Loss: 0.009812	Acc: 79.1% (7911/10000)
[Test]  Epoch: 72	Loss: 0.009803	Acc: 79.1% (7914/10000)
[Test]  Epoch: 73	Loss: 0.009815	Acc: 78.9% (7894/10000)
[Test]  Epoch: 74	Loss: 0.009806	Acc: 79.1% (7909/10000)
[Test]  Epoch: 75	Loss: 0.009793	Acc: 79.1% (7907/10000)
[Test]  Epoch: 76	Loss: 0.009804	Acc: 79.1% (7909/10000)
[Test]  Epoch: 77	Loss: 0.009798	Acc: 79.1% (7906/10000)
[Test]  Epoch: 78	Loss: 0.009765	Acc: 79.1% (7909/10000)
[Test]  Epoch: 79	Loss: 0.009747	Acc: 79.2% (7915/10000)
[Test]  Epoch: 80	Loss: 0.009797	Acc: 79.1% (7913/10000)
[Test]  Epoch: 81	Loss: 0.009772	Acc: 79.3% (7927/10000)
[Test]  Epoch: 82	Loss: 0.009813	Acc: 79.2% (7919/10000)
[Test]  Epoch: 83	Loss: 0.009793	Acc: 79.1% (7907/10000)
[Test]  Epoch: 84	Loss: 0.009770	Acc: 79.2% (7921/10000)
[Test]  Epoch: 85	Loss: 0.009812	Acc: 79.1% (7913/10000)
[Test]  Epoch: 86	Loss: 0.009764	Acc: 79.2% (7919/10000)
[Test]  Epoch: 87	Loss: 0.009790	Acc: 79.2% (7918/10000)
[Test]  Epoch: 88	Loss: 0.009790	Acc: 79.0% (7902/10000)
[Test]  Epoch: 89	Loss: 0.009770	Acc: 79.0% (7902/10000)
[Test]  Epoch: 90	Loss: 0.009768	Acc: 79.1% (7911/10000)
[Test]  Epoch: 91	Loss: 0.009785	Acc: 79.2% (7919/10000)
[Test]  Epoch: 92	Loss: 0.009780	Acc: 79.2% (7919/10000)
[Test]  Epoch: 93	Loss: 0.009787	Acc: 79.1% (7913/10000)
[Test]  Epoch: 94	Loss: 0.009792	Acc: 79.0% (7902/10000)
[Test]  Epoch: 95	Loss: 0.009782	Acc: 79.2% (7915/10000)
[Test]  Epoch: 96	Loss: 0.009774	Acc: 79.0% (7896/10000)
[Test]  Epoch: 97	Loss: 0.009754	Acc: 79.1% (7908/10000)
[Test]  Epoch: 98	Loss: 0.009804	Acc: 79.0% (7901/10000)
[Test]  Epoch: 99	Loss: 0.009793	Acc: 79.1% (7912/10000)
[Test]  Epoch: 100	Loss: 0.009787	Acc: 79.2% (7918/10000)
===========finish==========
['2024-08-18', '18:01:02.802150', '100', 'test', '0.009787455523014068', '79.18', '79.37']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.013735	Acc: 74.7% (7467/10000)
[Test]  Epoch: 2	Loss: 0.014331	Acc: 71.7% (7166/10000)
[Test]  Epoch: 3	Loss: 0.019646	Acc: 60.1% (6011/10000)
[Test]  Epoch: 4	Loss: 0.017171	Acc: 64.1% (6412/10000)
[Test]  Epoch: 5	Loss: 0.013864	Acc: 70.7% (7070/10000)
[Test]  Epoch: 6	Loss: 0.012892	Acc: 73.4% (7343/10000)
[Test]  Epoch: 7	Loss: 0.012109	Acc: 74.7% (7473/10000)
[Test]  Epoch: 8	Loss: 0.014853	Acc: 69.2% (6923/10000)
[Test]  Epoch: 9	Loss: 0.012073	Acc: 74.2% (7420/10000)
[Test]  Epoch: 10	Loss: 0.012187	Acc: 74.9% (7486/10000)
[Test]  Epoch: 11	Loss: 0.012509	Acc: 72.9% (7293/10000)
[Test]  Epoch: 12	Loss: 0.011875	Acc: 74.3% (7435/10000)
[Test]  Epoch: 13	Loss: 0.011074	Acc: 76.5% (7650/10000)
[Test]  Epoch: 14	Loss: 0.011148	Acc: 76.7% (7669/10000)
[Test]  Epoch: 15	Loss: 0.011365	Acc: 76.2% (7616/10000)
[Test]  Epoch: 16	Loss: 0.011431	Acc: 75.8% (7584/10000)
[Test]  Epoch: 17	Loss: 0.011091	Acc: 76.2% (7617/10000)
[Test]  Epoch: 18	Loss: 0.011623	Acc: 75.4% (7538/10000)
[Test]  Epoch: 19	Loss: 0.011025	Acc: 76.2% (7622/10000)
[Test]  Epoch: 20	Loss: 0.010918	Acc: 76.7% (7672/10000)
[Test]  Epoch: 21	Loss: 0.010928	Acc: 77.0% (7700/10000)
[Test]  Epoch: 22	Loss: 0.010575	Acc: 77.4% (7739/10000)
[Test]  Epoch: 23	Loss: 0.010927	Acc: 76.6% (7657/10000)
[Test]  Epoch: 24	Loss: 0.010864	Acc: 76.4% (7638/10000)
[Test]  Epoch: 25	Loss: 0.010595	Acc: 77.5% (7748/10000)
[Test]  Epoch: 26	Loss: 0.010517	Acc: 77.6% (7764/10000)
[Test]  Epoch: 27	Loss: 0.011110	Acc: 76.0% (7595/10000)
[Test]  Epoch: 28	Loss: 0.011105	Acc: 76.3% (7628/10000)
[Test]  Epoch: 29	Loss: 0.010631	Acc: 77.4% (7741/10000)
[Test]  Epoch: 30	Loss: 0.010463	Acc: 77.3% (7732/10000)
[Test]  Epoch: 31	Loss: 0.010371	Acc: 77.8% (7779/10000)
[Test]  Epoch: 32	Loss: 0.010494	Acc: 78.2% (7818/10000)
[Test]  Epoch: 33	Loss: 0.010654	Acc: 77.5% (7754/10000)
[Test]  Epoch: 34	Loss: 0.010576	Acc: 77.4% (7739/10000)
[Test]  Epoch: 35	Loss: 0.010440	Acc: 78.0% (7800/10000)
[Test]  Epoch: 36	Loss: 0.010433	Acc: 77.9% (7791/10000)
[Test]  Epoch: 37	Loss: 0.010661	Acc: 77.4% (7741/10000)
[Test]  Epoch: 38	Loss: 0.011063	Acc: 76.3% (7626/10000)
[Test]  Epoch: 39	Loss: 0.010605	Acc: 77.4% (7741/10000)
[Test]  Epoch: 40	Loss: 0.010417	Acc: 77.9% (7794/10000)
[Test]  Epoch: 41	Loss: 0.010539	Acc: 77.5% (7745/10000)
[Test]  Epoch: 42	Loss: 0.010675	Acc: 77.8% (7776/10000)
[Test]  Epoch: 43	Loss: 0.010668	Acc: 77.0% (7700/10000)
[Test]  Epoch: 44	Loss: 0.010373	Acc: 78.0% (7802/10000)
[Test]  Epoch: 45	Loss: 0.010726	Acc: 77.0% (7702/10000)
[Test]  Epoch: 46	Loss: 0.010543	Acc: 77.7% (7774/10000)
[Test]  Epoch: 47	Loss: 0.010699	Acc: 76.9% (7692/10000)
[Test]  Epoch: 48	Loss: 0.010430	Acc: 77.7% (7770/10000)
[Test]  Epoch: 49	Loss: 0.010697	Acc: 76.8% (7682/10000)
[Test]  Epoch: 50	Loss: 0.010900	Acc: 76.4% (7636/10000)
[Test]  Epoch: 51	Loss: 0.010458	Acc: 77.3% (7733/10000)
[Test]  Epoch: 52	Loss: 0.010547	Acc: 77.5% (7752/10000)
[Test]  Epoch: 53	Loss: 0.010414	Acc: 78.1% (7812/10000)
[Test]  Epoch: 54	Loss: 0.010357	Acc: 78.2% (7819/10000)
[Test]  Epoch: 55	Loss: 0.010370	Acc: 78.0% (7805/10000)
[Test]  Epoch: 56	Loss: 0.010278	Acc: 78.1% (7812/10000)
[Test]  Epoch: 57	Loss: 0.010526	Acc: 77.3% (7732/10000)
[Test]  Epoch: 58	Loss: 0.010528	Acc: 77.2% (7722/10000)
[Test]  Epoch: 59	Loss: 0.010355	Acc: 77.8% (7785/10000)
[Test]  Epoch: 60	Loss: 0.010495	Acc: 77.3% (7735/10000)
[Test]  Epoch: 61	Loss: 0.010257	Acc: 78.3% (7833/10000)
[Test]  Epoch: 62	Loss: 0.010246	Acc: 78.3% (7834/10000)
[Test]  Epoch: 63	Loss: 0.010246	Acc: 78.4% (7838/10000)
[Test]  Epoch: 64	Loss: 0.010240	Acc: 78.4% (7837/10000)
[Test]  Epoch: 65	Loss: 0.010247	Acc: 78.2% (7820/10000)
[Test]  Epoch: 66	Loss: 0.010251	Acc: 78.3% (7831/10000)
[Test]  Epoch: 67	Loss: 0.010281	Acc: 78.3% (7829/10000)
[Test]  Epoch: 68	Loss: 0.010206	Acc: 78.3% (7829/10000)
[Test]  Epoch: 69	Loss: 0.010225	Acc: 78.3% (7829/10000)
[Test]  Epoch: 70	Loss: 0.010214	Acc: 78.4% (7842/10000)
[Test]  Epoch: 71	Loss: 0.010242	Acc: 78.3% (7834/10000)
[Test]  Epoch: 72	Loss: 0.010229	Acc: 78.3% (7833/10000)
[Test]  Epoch: 73	Loss: 0.010227	Acc: 78.3% (7831/10000)
[Test]  Epoch: 74	Loss: 0.010211	Acc: 78.4% (7838/10000)
[Test]  Epoch: 75	Loss: 0.010199	Acc: 78.3% (7834/10000)
[Test]  Epoch: 76	Loss: 0.010237	Acc: 78.4% (7838/10000)
[Test]  Epoch: 77	Loss: 0.010238	Acc: 78.4% (7836/10000)
[Test]  Epoch: 78	Loss: 0.010203	Acc: 78.3% (7830/10000)
[Test]  Epoch: 79	Loss: 0.010176	Acc: 78.5% (7848/10000)
[Test]  Epoch: 80	Loss: 0.010224	Acc: 78.4% (7837/10000)
[Test]  Epoch: 81	Loss: 0.010211	Acc: 78.4% (7841/10000)
[Test]  Epoch: 82	Loss: 0.010245	Acc: 78.2% (7822/10000)
[Test]  Epoch: 83	Loss: 0.010223	Acc: 78.3% (7830/10000)
[Test]  Epoch: 84	Loss: 0.010187	Acc: 78.2% (7819/10000)
[Test]  Epoch: 85	Loss: 0.010225	Acc: 78.4% (7839/10000)
[Test]  Epoch: 86	Loss: 0.010182	Acc: 78.3% (7833/10000)
[Test]  Epoch: 87	Loss: 0.010210	Acc: 78.3% (7826/10000)
[Test]  Epoch: 88	Loss: 0.010209	Acc: 78.3% (7827/10000)
[Test]  Epoch: 89	Loss: 0.010205	Acc: 78.3% (7831/10000)
[Test]  Epoch: 90	Loss: 0.010195	Acc: 78.4% (7841/10000)
[Test]  Epoch: 91	Loss: 0.010208	Acc: 78.3% (7833/10000)
[Test]  Epoch: 92	Loss: 0.010227	Acc: 78.3% (7832/10000)
[Test]  Epoch: 93	Loss: 0.010222	Acc: 78.3% (7834/10000)
[Test]  Epoch: 94	Loss: 0.010204	Acc: 78.4% (7838/10000)
[Test]  Epoch: 95	Loss: 0.010197	Acc: 78.3% (7835/10000)
[Test]  Epoch: 96	Loss: 0.010184	Acc: 78.5% (7847/10000)
[Test]  Epoch: 97	Loss: 0.010187	Acc: 78.3% (7831/10000)
[Test]  Epoch: 98	Loss: 0.010230	Acc: 78.3% (7830/10000)
[Test]  Epoch: 99	Loss: 0.010204	Acc: 78.4% (7844/10000)
[Test]  Epoch: 100	Loss: 0.010183	Acc: 78.4% (7841/10000)
===========finish==========
['2024-08-18', '18:03:26.380782', '100', 'test', '0.010183396458625793', '78.41', '78.48']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.058934	Acc: 26.6% (2660/10000)
[Test]  Epoch: 2	Loss: 0.021227	Acc: 56.3% (5632/10000)
[Test]  Epoch: 3	Loss: 0.019188	Acc: 58.8% (5884/10000)
[Test]  Epoch: 4	Loss: 0.023365	Acc: 52.2% (5219/10000)
[Test]  Epoch: 5	Loss: 0.021249	Acc: 56.3% (5628/10000)
[Test]  Epoch: 6	Loss: 0.016075	Acc: 65.5% (6545/10000)
[Test]  Epoch: 7	Loss: 0.016689	Acc: 64.6% (6464/10000)
[Test]  Epoch: 8	Loss: 0.017948	Acc: 61.6% (6163/10000)
[Test]  Epoch: 9	Loss: 0.015183	Acc: 67.2% (6723/10000)
[Test]  Epoch: 10	Loss: 0.015050	Acc: 68.0% (6799/10000)
[Test]  Epoch: 11	Loss: 0.014764	Acc: 67.5% (6751/10000)
[Test]  Epoch: 12	Loss: 0.015878	Acc: 65.6% (6560/10000)
[Test]  Epoch: 13	Loss: 0.014273	Acc: 68.7% (6865/10000)
[Test]  Epoch: 14	Loss: 0.014993	Acc: 66.7% (6666/10000)
[Test]  Epoch: 15	Loss: 0.014368	Acc: 68.2% (6823/10000)
[Test]  Epoch: 16	Loss: 0.015655	Acc: 65.8% (6576/10000)
[Test]  Epoch: 17	Loss: 0.014410	Acc: 68.4% (6839/10000)
[Test]  Epoch: 18	Loss: 0.015624	Acc: 66.0% (6602/10000)
[Test]  Epoch: 19	Loss: 0.013872	Acc: 69.6% (6961/10000)
[Test]  Epoch: 20	Loss: 0.014536	Acc: 68.2% (6817/10000)
[Test]  Epoch: 21	Loss: 0.014071	Acc: 69.4% (6938/10000)
[Test]  Epoch: 22	Loss: 0.013605	Acc: 69.9% (6994/10000)
[Test]  Epoch: 23	Loss: 0.013684	Acc: 70.1% (7007/10000)
[Test]  Epoch: 24	Loss: 0.013690	Acc: 69.5% (6955/10000)
[Test]  Epoch: 25	Loss: 0.013643	Acc: 70.0% (7005/10000)
[Test]  Epoch: 26	Loss: 0.013641	Acc: 69.7% (6967/10000)
[Test]  Epoch: 27	Loss: 0.013760	Acc: 69.5% (6952/10000)
[Test]  Epoch: 28	Loss: 0.013315	Acc: 70.2% (7024/10000)
[Test]  Epoch: 29	Loss: 0.013313	Acc: 70.5% (7054/10000)
[Test]  Epoch: 30	Loss: 0.013337	Acc: 70.5% (7047/10000)
[Test]  Epoch: 31	Loss: 0.013333	Acc: 71.1% (7106/10000)
[Test]  Epoch: 32	Loss: 0.013226	Acc: 70.7% (7073/10000)
[Test]  Epoch: 33	Loss: 0.013304	Acc: 70.6% (7058/10000)
[Test]  Epoch: 34	Loss: 0.013514	Acc: 70.3% (7027/10000)
[Test]  Epoch: 35	Loss: 0.013404	Acc: 70.3% (7029/10000)
[Test]  Epoch: 36	Loss: 0.013244	Acc: 70.8% (7077/10000)
[Test]  Epoch: 37	Loss: 0.012966	Acc: 71.1% (7112/10000)
[Test]  Epoch: 38	Loss: 0.015348	Acc: 66.6% (6660/10000)
[Test]  Epoch: 39	Loss: 0.013543	Acc: 69.9% (6987/10000)
[Test]  Epoch: 40	Loss: 0.013322	Acc: 70.4% (7037/10000)
[Test]  Epoch: 41	Loss: 0.013492	Acc: 69.9% (6986/10000)
[Test]  Epoch: 42	Loss: 0.013449	Acc: 69.7% (6972/10000)
[Test]  Epoch: 43	Loss: 0.013210	Acc: 70.4% (7044/10000)
[Test]  Epoch: 44	Loss: 0.012986	Acc: 71.1% (7112/10000)
[Test]  Epoch: 45	Loss: 0.012912	Acc: 71.4% (7137/10000)
[Test]  Epoch: 46	Loss: 0.012918	Acc: 71.2% (7115/10000)
[Test]  Epoch: 47	Loss: 0.013102	Acc: 71.3% (7127/10000)
[Test]  Epoch: 48	Loss: 0.013047	Acc: 71.3% (7129/10000)
[Test]  Epoch: 49	Loss: 0.013485	Acc: 70.2% (7015/10000)
[Test]  Epoch: 50	Loss: 0.013019	Acc: 70.8% (7081/10000)
[Test]  Epoch: 51	Loss: 0.013016	Acc: 70.5% (7050/10000)
[Test]  Epoch: 52	Loss: 0.013115	Acc: 71.0% (7101/10000)
[Test]  Epoch: 53	Loss: 0.013129	Acc: 70.8% (7082/10000)
[Test]  Epoch: 54	Loss: 0.012850	Acc: 71.1% (7109/10000)
[Test]  Epoch: 55	Loss: 0.012882	Acc: 71.2% (7115/10000)
[Test]  Epoch: 56	Loss: 0.012698	Acc: 71.8% (7175/10000)
[Test]  Epoch: 57	Loss: 0.013002	Acc: 71.0% (7096/10000)
[Test]  Epoch: 58	Loss: 0.012853	Acc: 71.4% (7142/10000)
[Test]  Epoch: 59	Loss: 0.013127	Acc: 70.6% (7057/10000)
[Test]  Epoch: 60	Loss: 0.013060	Acc: 71.2% (7125/10000)
[Test]  Epoch: 61	Loss: 0.012756	Acc: 71.8% (7179/10000)
[Test]  Epoch: 62	Loss: 0.012724	Acc: 71.7% (7173/10000)
[Test]  Epoch: 63	Loss: 0.012699	Acc: 71.6% (7159/10000)
[Test]  Epoch: 64	Loss: 0.012672	Acc: 71.7% (7169/10000)
[Test]  Epoch: 65	Loss: 0.012684	Acc: 71.8% (7184/10000)
[Test]  Epoch: 66	Loss: 0.012709	Acc: 71.7% (7168/10000)
[Test]  Epoch: 67	Loss: 0.012745	Acc: 71.5% (7154/10000)
[Test]  Epoch: 68	Loss: 0.012665	Acc: 71.8% (7179/10000)
[Test]  Epoch: 69	Loss: 0.012698	Acc: 71.8% (7180/10000)
[Test]  Epoch: 70	Loss: 0.012667	Acc: 71.8% (7177/10000)
[Test]  Epoch: 71	Loss: 0.012736	Acc: 71.6% (7162/10000)
[Test]  Epoch: 72	Loss: 0.012697	Acc: 71.7% (7165/10000)
[Test]  Epoch: 73	Loss: 0.012714	Acc: 71.7% (7172/10000)
[Test]  Epoch: 74	Loss: 0.012707	Acc: 71.6% (7158/10000)
[Test]  Epoch: 75	Loss: 0.012697	Acc: 71.7% (7168/10000)
[Test]  Epoch: 76	Loss: 0.012696	Acc: 71.8% (7183/10000)
[Test]  Epoch: 77	Loss: 0.012685	Acc: 71.7% (7165/10000)
[Test]  Epoch: 78	Loss: 0.012647	Acc: 71.8% (7182/10000)
[Test]  Epoch: 79	Loss: 0.012623	Acc: 71.7% (7172/10000)
[Test]  Epoch: 80	Loss: 0.012684	Acc: 71.7% (7168/10000)
[Test]  Epoch: 81	Loss: 0.012689	Acc: 71.6% (7161/10000)
[Test]  Epoch: 82	Loss: 0.012716	Acc: 71.5% (7145/10000)
[Test]  Epoch: 83	Loss: 0.012688	Acc: 71.7% (7169/10000)
[Test]  Epoch: 84	Loss: 0.012658	Acc: 71.8% (7181/10000)
[Test]  Epoch: 85	Loss: 0.012693	Acc: 71.9% (7190/10000)
[Test]  Epoch: 86	Loss: 0.012627	Acc: 71.8% (7183/10000)
[Test]  Epoch: 87	Loss: 0.012685	Acc: 71.9% (7186/10000)
[Test]  Epoch: 88	Loss: 0.012691	Acc: 71.8% (7177/10000)
[Test]  Epoch: 89	Loss: 0.012670	Acc: 71.7% (7168/10000)
[Test]  Epoch: 90	Loss: 0.012650	Acc: 71.9% (7190/10000)
[Test]  Epoch: 91	Loss: 0.012673	Acc: 71.7% (7169/10000)
[Test]  Epoch: 92	Loss: 0.012703	Acc: 71.6% (7164/10000)
[Test]  Epoch: 93	Loss: 0.012681	Acc: 71.5% (7155/10000)
[Test]  Epoch: 94	Loss: 0.012712	Acc: 71.6% (7164/10000)
[Test]  Epoch: 95	Loss: 0.012685	Acc: 71.6% (7164/10000)
[Test]  Epoch: 96	Loss: 0.012669	Acc: 71.7% (7170/10000)
[Test]  Epoch: 97	Loss: 0.012660	Acc: 71.7% (7174/10000)
[Test]  Epoch: 98	Loss: 0.012681	Acc: 71.6% (7158/10000)
[Test]  Epoch: 99	Loss: 0.012679	Acc: 71.6% (7159/10000)
[Test]  Epoch: 100	Loss: 0.012673	Acc: 71.6% (7161/10000)
===========finish==========
['2024-08-18', '18:05:48.227465', '100', 'test', '0.012673159819841386', '71.61', '71.9']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.1.conv.1.weight', '_features.7.conv.2.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.0.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.3.weight', '_features.10.conv.2.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.079674	Acc: 13.8% (1376/10000)
[Test]  Epoch: 2	Loss: 0.060823	Acc: 20.3% (2034/10000)
[Test]  Epoch: 3	Loss: 0.035696	Acc: 26.2% (2618/10000)
[Test]  Epoch: 4	Loss: 0.030823	Acc: 31.4% (3139/10000)
[Test]  Epoch: 5	Loss: 0.029749	Acc: 33.1% (3309/10000)
[Test]  Epoch: 6	Loss: 0.029922	Acc: 31.1% (3115/10000)
[Test]  Epoch: 7	Loss: 0.031021	Acc: 31.6% (3163/10000)
[Test]  Epoch: 8	Loss: 0.032205	Acc: 34.3% (3434/10000)
[Test]  Epoch: 9	Loss: 0.032056	Acc: 31.8% (3176/10000)
[Test]  Epoch: 10	Loss: 0.037769	Acc: 30.7% (3070/10000)
[Test]  Epoch: 11	Loss: 0.029153	Acc: 37.1% (3706/10000)
[Test]  Epoch: 12	Loss: 0.030404	Acc: 36.2% (3624/10000)
[Test]  Epoch: 13	Loss: 0.031479	Acc: 36.1% (3607/10000)
[Test]  Epoch: 14	Loss: 0.028318	Acc: 38.8% (3883/10000)
[Test]  Epoch: 15	Loss: 0.026358	Acc: 41.9% (4194/10000)
[Test]  Epoch: 16	Loss: 0.028658	Acc: 40.4% (4043/10000)
[Test]  Epoch: 17	Loss: 0.030392	Acc: 38.8% (3879/10000)
[Test]  Epoch: 18	Loss: 0.030176	Acc: 37.3% (3732/10000)
[Test]  Epoch: 19	Loss: 0.030454	Acc: 40.8% (4080/10000)
[Test]  Epoch: 20	Loss: 0.028020	Acc: 41.5% (4155/10000)
[Test]  Epoch: 21	Loss: 0.029735	Acc: 38.3% (3827/10000)
[Test]  Epoch: 22	Loss: 0.028712	Acc: 40.3% (4033/10000)
[Test]  Epoch: 23	Loss: 0.028239	Acc: 39.5% (3955/10000)
[Test]  Epoch: 24	Loss: 0.026383	Acc: 45.0% (4498/10000)
[Test]  Epoch: 25	Loss: 0.027069	Acc: 44.0% (4396/10000)
[Test]  Epoch: 26	Loss: 0.027881	Acc: 41.8% (4183/10000)
[Test]  Epoch: 27	Loss: 0.028059	Acc: 42.4% (4242/10000)
[Test]  Epoch: 28	Loss: 0.027377	Acc: 43.7% (4369/10000)
[Test]  Epoch: 29	Loss: 0.026180	Acc: 45.0% (4503/10000)
[Test]  Epoch: 30	Loss: 0.025494	Acc: 46.0% (4598/10000)
[Test]  Epoch: 31	Loss: 0.026162	Acc: 45.9% (4591/10000)
[Test]  Epoch: 32	Loss: 0.026530	Acc: 43.4% (4339/10000)
[Test]  Epoch: 33	Loss: 0.025477	Acc: 45.8% (4578/10000)
[Test]  Epoch: 34	Loss: 0.025619	Acc: 45.5% (4554/10000)
[Test]  Epoch: 35	Loss: 0.026155	Acc: 44.9% (4485/10000)
[Test]  Epoch: 36	Loss: 0.026664	Acc: 44.1% (4414/10000)
[Test]  Epoch: 37	Loss: 0.025762	Acc: 44.6% (4460/10000)
[Test]  Epoch: 38	Loss: 0.035077	Acc: 39.6% (3957/10000)
[Test]  Epoch: 39	Loss: 0.028001	Acc: 42.1% (4214/10000)
[Test]  Epoch: 40	Loss: 0.025365	Acc: 46.2% (4624/10000)
[Test]  Epoch: 41	Loss: 0.025893	Acc: 45.1% (4515/10000)
[Test]  Epoch: 42	Loss: 0.025325	Acc: 46.6% (4656/10000)
[Test]  Epoch: 43	Loss: 0.025356	Acc: 46.5% (4655/10000)
[Test]  Epoch: 44	Loss: 0.025633	Acc: 46.7% (4673/10000)
[Test]  Epoch: 45	Loss: 0.024440	Acc: 47.2% (4725/10000)
[Test]  Epoch: 46	Loss: 0.024848	Acc: 46.8% (4675/10000)
[Test]  Epoch: 47	Loss: 0.024947	Acc: 46.4% (4637/10000)
[Test]  Epoch: 48	Loss: 0.024512	Acc: 47.0% (4704/10000)
[Test]  Epoch: 49	Loss: 0.024978	Acc: 46.9% (4694/10000)
[Test]  Epoch: 50	Loss: 0.024699	Acc: 47.0% (4695/10000)
[Test]  Epoch: 51	Loss: 0.024462	Acc: 47.2% (4717/10000)
[Test]  Epoch: 52	Loss: 0.024579	Acc: 47.0% (4696/10000)
[Test]  Epoch: 53	Loss: 0.025054	Acc: 46.6% (4656/10000)
[Test]  Epoch: 54	Loss: 0.024434	Acc: 46.7% (4673/10000)
[Test]  Epoch: 55	Loss: 0.024917	Acc: 46.8% (4684/10000)
[Test]  Epoch: 56	Loss: 0.024362	Acc: 47.3% (4729/10000)
[Test]  Epoch: 57	Loss: 0.024487	Acc: 46.5% (4649/10000)
[Test]  Epoch: 58	Loss: 0.024605	Acc: 46.7% (4667/10000)
[Test]  Epoch: 59	Loss: 0.024098	Acc: 47.6% (4761/10000)
[Test]  Epoch: 60	Loss: 0.024296	Acc: 47.7% (4771/10000)
[Test]  Epoch: 61	Loss: 0.023951	Acc: 48.0% (4803/10000)
[Test]  Epoch: 62	Loss: 0.023867	Acc: 48.1% (4812/10000)
[Test]  Epoch: 63	Loss: 0.023742	Acc: 48.2% (4818/10000)
[Test]  Epoch: 64	Loss: 0.023791	Acc: 48.2% (4820/10000)
[Test]  Epoch: 65	Loss: 0.023795	Acc: 48.0% (4800/10000)
[Test]  Epoch: 66	Loss: 0.023839	Acc: 48.0% (4798/10000)
[Test]  Epoch: 67	Loss: 0.023828	Acc: 47.8% (4781/10000)
[Test]  Epoch: 68	Loss: 0.023855	Acc: 48.0% (4802/10000)
[Test]  Epoch: 69	Loss: 0.023919	Acc: 47.9% (4786/10000)
[Test]  Epoch: 70	Loss: 0.023763	Acc: 48.2% (4823/10000)
[Test]  Epoch: 71	Loss: 0.023795	Acc: 47.8% (4780/10000)
[Test]  Epoch: 72	Loss: 0.023772	Acc: 48.1% (4808/10000)
[Test]  Epoch: 73	Loss: 0.023835	Acc: 47.9% (4793/10000)
[Test]  Epoch: 74	Loss: 0.023786	Acc: 48.0% (4797/10000)
[Test]  Epoch: 75	Loss: 0.023830	Acc: 48.0% (4798/10000)
[Test]  Epoch: 76	Loss: 0.023825	Acc: 48.0% (4802/10000)
[Test]  Epoch: 77	Loss: 0.023809	Acc: 48.1% (4809/10000)
[Test]  Epoch: 78	Loss: 0.023814	Acc: 48.3% (4826/10000)
[Test]  Epoch: 79	Loss: 0.023865	Acc: 48.1% (4810/10000)
[Test]  Epoch: 80	Loss: 0.023869	Acc: 48.0% (4804/10000)
[Test]  Epoch: 81	Loss: 0.023860	Acc: 48.0% (4802/10000)
[Test]  Epoch: 82	Loss: 0.023880	Acc: 48.0% (4795/10000)
[Test]  Epoch: 83	Loss: 0.023905	Acc: 48.1% (4811/10000)
[Test]  Epoch: 84	Loss: 0.023924	Acc: 48.2% (4816/10000)
[Test]  Epoch: 85	Loss: 0.023924	Acc: 47.9% (4788/10000)
[Test]  Epoch: 86	Loss: 0.023890	Acc: 48.0% (4801/10000)
[Test]  Epoch: 87	Loss: 0.023943	Acc: 47.9% (4785/10000)
[Test]  Epoch: 88	Loss: 0.023903	Acc: 47.8% (4783/10000)
[Test]  Epoch: 89	Loss: 0.023964	Acc: 47.8% (4781/10000)
[Test]  Epoch: 90	Loss: 0.023921	Acc: 47.9% (4794/10000)
[Test]  Epoch: 91	Loss: 0.023841	Acc: 47.9% (4786/10000)
[Test]  Epoch: 92	Loss: 0.023969	Acc: 47.7% (4768/10000)
[Test]  Epoch: 93	Loss: 0.023858	Acc: 47.8% (4776/10000)
[Test]  Epoch: 94	Loss: 0.023948	Acc: 47.8% (4782/10000)
[Test]  Epoch: 95	Loss: 0.023901	Acc: 48.0% (4802/10000)
[Test]  Epoch: 96	Loss: 0.023972	Acc: 47.8% (4778/10000)
[Test]  Epoch: 97	Loss: 0.023885	Acc: 47.8% (4780/10000)
[Test]  Epoch: 98	Loss: 0.023916	Acc: 47.7% (4771/10000)
[Test]  Epoch: 99	Loss: 0.023951	Acc: 47.8% (4782/10000)
[Test]  Epoch: 100	Loss: 0.023858	Acc: 47.8% (4775/10000)
===========finish==========
['2024-08-18', '18:08:08.944329', '100', 'test', '0.023858225500583648', '47.75', '48.26']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.1.conv.1.weight', '_features.7.conv.2.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.0.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.3.weight', '_features.10.conv.2.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.9.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.15.conv.3.weight', '_features.7.conv.0.0.weight', '_features.5.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.150854	Acc: 13.5% (1347/10000)
[Test]  Epoch: 2	Loss: 0.040690	Acc: 24.9% (2485/10000)
[Test]  Epoch: 3	Loss: 0.032631	Acc: 27.0% (2698/10000)
[Test]  Epoch: 4	Loss: 0.035895	Acc: 23.6% (2357/10000)
[Test]  Epoch: 5	Loss: 0.028445	Acc: 36.0% (3598/10000)
[Test]  Epoch: 6	Loss: 0.028144	Acc: 37.1% (3710/10000)
[Test]  Epoch: 7	Loss: 0.029340	Acc: 34.2% (3424/10000)
[Test]  Epoch: 8	Loss: 0.028540	Acc: 36.7% (3671/10000)
[Test]  Epoch: 9	Loss: 0.031593	Acc: 34.5% (3453/10000)
[Test]  Epoch: 10	Loss: 0.032397	Acc: 33.4% (3344/10000)
[Test]  Epoch: 11	Loss: 0.028788	Acc: 37.3% (3729/10000)
[Test]  Epoch: 12	Loss: 0.028623	Acc: 39.0% (3895/10000)
[Test]  Epoch: 13	Loss: 0.030174	Acc: 37.8% (3783/10000)
[Test]  Epoch: 14	Loss: 0.028986	Acc: 37.1% (3713/10000)
[Test]  Epoch: 15	Loss: 0.029616	Acc: 39.3% (3929/10000)
[Test]  Epoch: 16	Loss: 0.033755	Acc: 32.3% (3229/10000)
[Test]  Epoch: 17	Loss: 0.030140	Acc: 38.2% (3821/10000)
[Test]  Epoch: 18	Loss: 0.032220	Acc: 36.5% (3648/10000)
[Test]  Epoch: 19	Loss: 0.028337	Acc: 41.4% (4140/10000)
[Test]  Epoch: 20	Loss: 0.031268	Acc: 38.6% (3862/10000)
[Test]  Epoch: 21	Loss: 0.029141	Acc: 40.1% (4014/10000)
[Test]  Epoch: 22	Loss: 0.028777	Acc: 41.8% (4182/10000)
[Test]  Epoch: 23	Loss: 0.029979	Acc: 39.5% (3952/10000)
[Test]  Epoch: 24	Loss: 0.031435	Acc: 38.8% (3881/10000)
[Test]  Epoch: 25	Loss: 0.030609	Acc: 39.1% (3913/10000)
[Test]  Epoch: 26	Loss: 0.029068	Acc: 41.3% (4133/10000)
[Test]  Epoch: 27	Loss: 0.028424	Acc: 42.0% (4195/10000)
[Test]  Epoch: 28	Loss: 0.027270	Acc: 44.1% (4413/10000)
[Test]  Epoch: 29	Loss: 0.026831	Acc: 44.0% (4403/10000)
[Test]  Epoch: 30	Loss: 0.026862	Acc: 44.0% (4403/10000)
[Test]  Epoch: 31	Loss: 0.026499	Acc: 44.5% (4447/10000)
[Test]  Epoch: 32	Loss: 0.025972	Acc: 44.8% (4482/10000)
[Test]  Epoch: 33	Loss: 0.026184	Acc: 44.9% (4485/10000)
[Test]  Epoch: 34	Loss: 0.026548	Acc: 43.1% (4306/10000)
[Test]  Epoch: 35	Loss: 0.025945	Acc: 44.8% (4484/10000)
[Test]  Epoch: 36	Loss: 0.026234	Acc: 45.0% (4505/10000)
[Test]  Epoch: 37	Loss: 0.026450	Acc: 43.2% (4320/10000)
[Test]  Epoch: 38	Loss: 0.028822	Acc: 42.1% (4206/10000)
[Test]  Epoch: 39	Loss: 0.026484	Acc: 44.9% (4487/10000)
[Test]  Epoch: 40	Loss: 0.025109	Acc: 47.2% (4724/10000)
[Test]  Epoch: 41	Loss: 0.026321	Acc: 45.0% (4498/10000)
[Test]  Epoch: 42	Loss: 0.026962	Acc: 44.1% (4415/10000)
[Test]  Epoch: 43	Loss: 0.025567	Acc: 45.7% (4574/10000)
[Test]  Epoch: 44	Loss: 0.025426	Acc: 46.7% (4668/10000)
[Test]  Epoch: 45	Loss: 0.025274	Acc: 46.2% (4624/10000)
[Test]  Epoch: 46	Loss: 0.025280	Acc: 46.0% (4596/10000)
[Test]  Epoch: 47	Loss: 0.024905	Acc: 47.0% (4701/10000)
[Test]  Epoch: 48	Loss: 0.025069	Acc: 46.9% (4688/10000)
[Test]  Epoch: 49	Loss: 0.025100	Acc: 46.7% (4673/10000)
[Test]  Epoch: 50	Loss: 0.025060	Acc: 46.6% (4658/10000)
[Test]  Epoch: 51	Loss: 0.024531	Acc: 47.1% (4710/10000)
[Test]  Epoch: 52	Loss: 0.025379	Acc: 46.0% (4603/10000)
[Test]  Epoch: 53	Loss: 0.024789	Acc: 47.2% (4717/10000)
[Test]  Epoch: 54	Loss: 0.024562	Acc: 47.1% (4714/10000)
[Test]  Epoch: 55	Loss: 0.024943	Acc: 46.8% (4678/10000)
[Test]  Epoch: 56	Loss: 0.024257	Acc: 47.5% (4745/10000)
[Test]  Epoch: 57	Loss: 0.024937	Acc: 46.5% (4649/10000)
[Test]  Epoch: 58	Loss: 0.024644	Acc: 47.0% (4705/10000)
[Test]  Epoch: 59	Loss: 0.024625	Acc: 47.1% (4710/10000)
[Test]  Epoch: 60	Loss: 0.025059	Acc: 47.0% (4701/10000)
[Test]  Epoch: 61	Loss: 0.024601	Acc: 47.6% (4763/10000)
[Test]  Epoch: 62	Loss: 0.024443	Acc: 47.6% (4764/10000)
[Test]  Epoch: 63	Loss: 0.024401	Acc: 47.5% (4752/10000)
[Test]  Epoch: 64	Loss: 0.024414	Acc: 47.7% (4770/10000)
[Test]  Epoch: 65	Loss: 0.024419	Acc: 47.7% (4768/10000)
[Test]  Epoch: 66	Loss: 0.024386	Acc: 47.4% (4741/10000)
[Test]  Epoch: 67	Loss: 0.024338	Acc: 47.3% (4732/10000)
[Test]  Epoch: 68	Loss: 0.024371	Acc: 47.4% (4742/10000)
[Test]  Epoch: 69	Loss: 0.024440	Acc: 47.6% (4758/10000)
[Test]  Epoch: 70	Loss: 0.024377	Acc: 47.6% (4764/10000)
[Test]  Epoch: 71	Loss: 0.024345	Acc: 47.5% (4745/10000)
[Test]  Epoch: 72	Loss: 0.024361	Acc: 47.3% (4733/10000)
[Test]  Epoch: 73	Loss: 0.024285	Acc: 47.4% (4739/10000)
[Test]  Epoch: 74	Loss: 0.024293	Acc: 47.5% (4745/10000)
[Test]  Epoch: 75	Loss: 0.024315	Acc: 47.4% (4738/10000)
[Test]  Epoch: 76	Loss: 0.024333	Acc: 47.5% (4748/10000)
[Test]  Epoch: 77	Loss: 0.024349	Acc: 47.4% (4744/10000)
[Test]  Epoch: 78	Loss: 0.024354	Acc: 47.5% (4747/10000)
[Test]  Epoch: 79	Loss: 0.024384	Acc: 47.5% (4750/10000)
[Test]  Epoch: 80	Loss: 0.024344	Acc: 47.5% (4748/10000)
[Test]  Epoch: 81	Loss: 0.024306	Acc: 47.4% (4741/10000)
[Test]  Epoch: 82	Loss: 0.024323	Acc: 47.5% (4748/10000)
[Test]  Epoch: 83	Loss: 0.024340	Acc: 47.3% (4731/10000)
[Test]  Epoch: 84	Loss: 0.024309	Acc: 47.4% (4743/10000)
[Test]  Epoch: 85	Loss: 0.024320	Acc: 47.4% (4741/10000)
[Test]  Epoch: 86	Loss: 0.024380	Acc: 47.3% (4733/10000)
[Test]  Epoch: 87	Loss: 0.024323	Acc: 47.3% (4733/10000)
[Test]  Epoch: 88	Loss: 0.024329	Acc: 47.3% (4731/10000)
[Test]  Epoch: 89	Loss: 0.024344	Acc: 47.6% (4757/10000)
[Test]  Epoch: 90	Loss: 0.024351	Acc: 47.4% (4738/10000)
[Test]  Epoch: 91	Loss: 0.024325	Acc: 47.2% (4717/10000)
[Test]  Epoch: 92	Loss: 0.024347	Acc: 47.2% (4721/10000)
[Test]  Epoch: 93	Loss: 0.024301	Acc: 47.3% (4730/10000)
[Test]  Epoch: 94	Loss: 0.024374	Acc: 47.3% (4729/10000)
[Test]  Epoch: 95	Loss: 0.024278	Acc: 47.4% (4739/10000)
[Test]  Epoch: 96	Loss: 0.024356	Acc: 47.3% (4726/10000)
[Test]  Epoch: 97	Loss: 0.024306	Acc: 47.3% (4734/10000)
[Test]  Epoch: 98	Loss: 0.024354	Acc: 47.4% (4736/10000)
[Test]  Epoch: 99	Loss: 0.024331	Acc: 47.2% (4722/10000)
[Test]  Epoch: 100	Loss: 0.024319	Acc: 47.3% (4734/10000)
===========finish==========
['2024-08-18', '18:10:25.546188', '100', 'test', '0.024319212996959685', '47.34', '47.7']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.1.conv.1.weight', '_features.7.conv.2.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.0.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.3.weight', '_features.10.conv.2.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.9.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.15.conv.3.weight', '_features.7.conv.0.0.weight', '_features.5.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.16.conv.0.1.weight', '_features.12.conv.2.weight', '_features.18.1.weight', '_features.9.conv.1.0.weight', '_features.13.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.297639	Acc: 10.7% (1071/10000)
[Test]  Epoch: 2	Loss: 0.081895	Acc: 12.5% (1250/10000)
[Test]  Epoch: 3	Loss: 0.042822	Acc: 9.7% (974/10000)
[Test]  Epoch: 4	Loss: 0.038301	Acc: 12.0% (1197/10000)
[Test]  Epoch: 5	Loss: 0.052470	Acc: 8.8% (876/10000)
[Test]  Epoch: 6	Loss: 0.304394	Acc: 10.0% (998/10000)
[Test]  Epoch: 7	Loss: 0.053852	Acc: 9.6% (958/10000)
[Test]  Epoch: 8	Loss: 0.043501	Acc: 12.8% (1277/10000)
[Test]  Epoch: 9	Loss: 0.037731	Acc: 13.9% (1395/10000)
[Test]  Epoch: 10	Loss: 0.040270	Acc: 10.8% (1075/10000)
[Test]  Epoch: 11	Loss: 0.042214	Acc: 11.5% (1152/10000)
[Test]  Epoch: 12	Loss: 0.035566	Acc: 16.0% (1600/10000)
[Test]  Epoch: 13	Loss: 0.035928	Acc: 17.3% (1727/10000)
[Test]  Epoch: 14	Loss: 0.038841	Acc: 10.8% (1085/10000)
[Test]  Epoch: 15	Loss: 0.036186	Acc: 14.8% (1480/10000)
[Test]  Epoch: 16	Loss: 0.041007	Acc: 10.2% (1019/10000)
[Test]  Epoch: 17	Loss: 0.036088	Acc: 13.4% (1341/10000)
[Test]  Epoch: 18	Loss: 0.037217	Acc: 13.9% (1388/10000)
[Test]  Epoch: 19	Loss: 0.036182	Acc: 17.0% (1696/10000)
[Test]  Epoch: 20	Loss: 0.036200	Acc: 17.3% (1727/10000)
[Test]  Epoch: 21	Loss: 0.036592	Acc: 14.6% (1459/10000)
[Test]  Epoch: 22	Loss: 0.045906	Acc: 14.2% (1419/10000)
[Test]  Epoch: 23	Loss: 0.036241	Acc: 15.9% (1591/10000)
[Test]  Epoch: 24	Loss: 0.035717	Acc: 15.8% (1581/10000)
[Test]  Epoch: 25	Loss: 0.034242	Acc: 19.1% (1911/10000)
[Test]  Epoch: 26	Loss: 0.035149	Acc: 17.9% (1791/10000)
[Test]  Epoch: 27	Loss: 0.034898	Acc: 18.7% (1870/10000)
[Test]  Epoch: 28	Loss: 0.040448	Acc: 10.2% (1021/10000)
[Test]  Epoch: 29	Loss: 0.036896	Acc: 10.4% (1040/10000)
[Test]  Epoch: 30	Loss: 0.038797	Acc: 11.5% (1154/10000)
[Test]  Epoch: 31	Loss: 0.036071	Acc: 15.9% (1588/10000)
[Test]  Epoch: 32	Loss: 0.037220	Acc: 16.0% (1602/10000)
[Test]  Epoch: 33	Loss: 0.036080	Acc: 17.7% (1772/10000)
[Test]  Epoch: 34	Loss: 0.037252	Acc: 14.0% (1397/10000)
[Test]  Epoch: 35	Loss: 0.036401	Acc: 19.7% (1971/10000)
[Test]  Epoch: 36	Loss: 0.035551	Acc: 17.6% (1762/10000)
[Test]  Epoch: 37	Loss: 0.037382	Acc: 19.0% (1896/10000)
[Test]  Epoch: 38	Loss: 0.035971	Acc: 16.8% (1681/10000)
[Test]  Epoch: 39	Loss: 0.040896	Acc: 17.7% (1767/10000)
[Test]  Epoch: 40	Loss: 0.038111	Acc: 17.6% (1764/10000)
[Test]  Epoch: 41	Loss: 0.034923	Acc: 18.8% (1877/10000)
[Test]  Epoch: 42	Loss: 0.034749	Acc: 17.9% (1788/10000)
[Test]  Epoch: 43	Loss: 0.036159	Acc: 17.3% (1726/10000)
[Test]  Epoch: 44	Loss: 0.035290	Acc: 20.5% (2046/10000)
[Test]  Epoch: 45	Loss: 0.035249	Acc: 17.9% (1787/10000)
[Test]  Epoch: 46	Loss: 0.035528	Acc: 16.1% (1607/10000)
[Test]  Epoch: 47	Loss: 0.034489	Acc: 21.5% (2150/10000)
[Test]  Epoch: 48	Loss: 0.035083	Acc: 22.5% (2253/10000)
[Test]  Epoch: 49	Loss: 0.034897	Acc: 22.5% (2253/10000)
[Test]  Epoch: 50	Loss: 0.034452	Acc: 22.6% (2261/10000)
[Test]  Epoch: 51	Loss: 0.034116	Acc: 21.4% (2141/10000)
[Test]  Epoch: 52	Loss: 0.035127	Acc: 22.1% (2205/10000)
[Test]  Epoch: 53	Loss: 0.033773	Acc: 22.1% (2208/10000)
[Test]  Epoch: 54	Loss: 0.039931	Acc: 18.6% (1857/10000)
[Test]  Epoch: 55	Loss: 0.034538	Acc: 20.8% (2077/10000)
[Test]  Epoch: 56	Loss: 0.033395	Acc: 24.0% (2397/10000)
[Test]  Epoch: 57	Loss: 0.034909	Acc: 22.3% (2230/10000)
[Test]  Epoch: 58	Loss: 0.036224	Acc: 18.2% (1818/10000)
[Test]  Epoch: 59	Loss: 0.033457	Acc: 22.3% (2233/10000)
[Test]  Epoch: 60	Loss: 0.040143	Acc: 17.2% (1717/10000)
[Test]  Epoch: 61	Loss: 0.032839	Acc: 22.4% (2235/10000)
[Test]  Epoch: 62	Loss: 0.032603	Acc: 23.2% (2319/10000)
[Test]  Epoch: 63	Loss: 0.032491	Acc: 22.8% (2276/10000)
[Test]  Epoch: 64	Loss: 0.032416	Acc: 23.0% (2299/10000)
[Test]  Epoch: 65	Loss: 0.032356	Acc: 22.9% (2286/10000)
[Test]  Epoch: 66	Loss: 0.032403	Acc: 23.5% (2346/10000)
[Test]  Epoch: 67	Loss: 0.032260	Acc: 23.0% (2304/10000)
[Test]  Epoch: 68	Loss: 0.032343	Acc: 22.9% (2295/10000)
[Test]  Epoch: 69	Loss: 0.032212	Acc: 23.8% (2376/10000)
[Test]  Epoch: 70	Loss: 0.032273	Acc: 23.7% (2373/10000)
[Test]  Epoch: 71	Loss: 0.032222	Acc: 23.4% (2340/10000)
[Test]  Epoch: 72	Loss: 0.032230	Acc: 23.4% (2340/10000)
[Test]  Epoch: 73	Loss: 0.032343	Acc: 23.4% (2342/10000)
[Test]  Epoch: 74	Loss: 0.032238	Acc: 23.5% (2347/10000)
[Test]  Epoch: 75	Loss: 0.032181	Acc: 23.7% (2367/10000)
[Test]  Epoch: 76	Loss: 0.032108	Acc: 23.9% (2389/10000)
[Test]  Epoch: 77	Loss: 0.032107	Acc: 23.9% (2394/10000)
[Test]  Epoch: 78	Loss: 0.032137	Acc: 24.0% (2400/10000)
[Test]  Epoch: 79	Loss: 0.032251	Acc: 23.9% (2395/10000)
[Test]  Epoch: 80	Loss: 0.032377	Acc: 24.6% (2462/10000)
[Test]  Epoch: 81	Loss: 0.032581	Acc: 23.9% (2388/10000)
[Test]  Epoch: 82	Loss: 0.032264	Acc: 23.9% (2387/10000)
[Test]  Epoch: 83	Loss: 0.032280	Acc: 24.0% (2396/10000)
[Test]  Epoch: 84	Loss: 0.032197	Acc: 24.4% (2442/10000)
[Test]  Epoch: 85	Loss: 0.032182	Acc: 24.5% (2452/10000)
[Test]  Epoch: 86	Loss: 0.032223	Acc: 24.8% (2475/10000)
[Test]  Epoch: 87	Loss: 0.032181	Acc: 24.6% (2456/10000)
[Test]  Epoch: 88	Loss: 0.032230	Acc: 24.9% (2494/10000)
[Test]  Epoch: 89	Loss: 0.032213	Acc: 24.9% (2485/10000)
[Test]  Epoch: 90	Loss: 0.032245	Acc: 24.8% (2477/10000)
[Test]  Epoch: 91	Loss: 0.032325	Acc: 24.7% (2473/10000)
[Test]  Epoch: 92	Loss: 0.032355	Acc: 24.5% (2454/10000)
[Test]  Epoch: 93	Loss: 0.032750	Acc: 23.9% (2387/10000)
[Test]  Epoch: 94	Loss: 0.032389	Acc: 24.7% (2472/10000)
[Test]  Epoch: 95	Loss: 0.032362	Acc: 24.8% (2482/10000)
[Test]  Epoch: 96	Loss: 0.032387	Acc: 24.1% (2408/10000)
[Test]  Epoch: 97	Loss: 0.032310	Acc: 25.0% (2500/10000)
[Test]  Epoch: 98	Loss: 0.032507	Acc: 25.1% (2512/10000)
[Test]  Epoch: 99	Loss: 0.032699	Acc: 24.6% (2456/10000)
[Test]  Epoch: 100	Loss: 0.032596	Acc: 24.7% (2466/10000)
===========finish==========
['2024-08-18', '18:12:46.564081', '100', 'test', '0.03259649217128754', '24.66', '25.12']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.1.conv.1.weight', '_features.7.conv.2.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.0.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.3.weight', '_features.10.conv.2.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.9.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.15.conv.3.weight', '_features.7.conv.0.0.weight', '_features.5.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.16.conv.0.1.weight', '_features.12.conv.2.weight', '_features.18.1.weight', '_features.9.conv.1.0.weight', '_features.13.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.8.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 1.520166	Acc: 10.0% (1000/10000)
[Test]  Epoch: 2	Loss: 0.571216	Acc: 10.0% (1001/10000)
[Test]  Epoch: 3	Loss: 0.079620	Acc: 10.6% (1058/10000)
[Test]  Epoch: 4	Loss: 0.037213	Acc: 11.7% (1173/10000)
[Test]  Epoch: 5	Loss: 0.037737	Acc: 10.2% (1025/10000)
[Test]  Epoch: 6	Loss: 0.358059	Acc: 10.9% (1093/10000)
[Test]  Epoch: 7	Loss: 0.040298	Acc: 12.9% (1286/10000)
[Test]  Epoch: 8	Loss: 0.052587	Acc: 10.5% (1051/10000)
[Test]  Epoch: 9	Loss: 0.038925	Acc: 11.5% (1154/10000)
[Test]  Epoch: 10	Loss: 0.038651	Acc: 12.4% (1241/10000)
[Test]  Epoch: 11	Loss: 0.163009	Acc: 9.9% (992/10000)
[Test]  Epoch: 12	Loss: 0.039333	Acc: 10.1% (1009/10000)
[Test]  Epoch: 13	Loss: 0.045376	Acc: 10.5% (1048/10000)
[Test]  Epoch: 14	Loss: 0.036845	Acc: 13.4% (1341/10000)
[Test]  Epoch: 15	Loss: 0.038092	Acc: 11.8% (1179/10000)
[Test]  Epoch: 16	Loss: 0.069598	Acc: 11.3% (1133/10000)
[Test]  Epoch: 17	Loss: 0.035960	Acc: 16.1% (1606/10000)
[Test]  Epoch: 18	Loss: 0.039481	Acc: 12.7% (1272/10000)
[Test]  Epoch: 19	Loss: 0.042015	Acc: 16.7% (1669/10000)
[Test]  Epoch: 20	Loss: 0.036468	Acc: 18.4% (1839/10000)
[Test]  Epoch: 21	Loss: 0.038198	Acc: 16.1% (1613/10000)
[Test]  Epoch: 22	Loss: 0.047125	Acc: 17.7% (1770/10000)
[Test]  Epoch: 23	Loss: 0.037983	Acc: 17.9% (1788/10000)
[Test]  Epoch: 24	Loss: 0.037727	Acc: 18.5% (1847/10000)
[Test]  Epoch: 25	Loss: 0.036315	Acc: 18.7% (1869/10000)
[Test]  Epoch: 26	Loss: 0.046098	Acc: 12.2% (1216/10000)
[Test]  Epoch: 27	Loss: 0.039142	Acc: 14.6% (1455/10000)
[Test]  Epoch: 28	Loss: 0.069436	Acc: 9.7% (967/10000)
[Test]  Epoch: 29	Loss: 0.261474	Acc: 10.0% (1001/10000)
[Test]  Epoch: 30	Loss: 0.038348	Acc: 10.0% (1000/10000)
[Test]  Epoch: 31	Loss: 0.040744	Acc: 10.0% (1000/10000)
[Test]  Epoch: 32	Loss: 0.114440	Acc: 13.1% (1309/10000)
[Test]  Epoch: 33	Loss: 0.037104	Acc: 10.6% (1055/10000)
[Test]  Epoch: 34	Loss: 0.042880	Acc: 10.0% (996/10000)
[Test]  Epoch: 35	Loss: 0.134898	Acc: 10.0% (1000/10000)
[Test]  Epoch: 36	Loss: 0.036878	Acc: 15.0% (1497/10000)
[Test]  Epoch: 37	Loss: 0.037263	Acc: 11.3% (1128/10000)
[Test]  Epoch: 38	Loss: 0.037201	Acc: 13.8% (1376/10000)
[Test]  Epoch: 39	Loss: 0.068996	Acc: 10.3% (1033/10000)
[Test]  Epoch: 40	Loss: 0.039333	Acc: 16.8% (1675/10000)
[Test]  Epoch: 41	Loss: 0.079648	Acc: 8.8% (885/10000)
[Test]  Epoch: 42	Loss: 0.044875	Acc: 10.0% (1000/10000)
[Test]  Epoch: 43	Loss: 0.038515	Acc: 15.3% (1532/10000)
[Test]  Epoch: 44	Loss: 0.036359	Acc: 16.8% (1678/10000)
[Test]  Epoch: 45	Loss: 0.036117	Acc: 13.8% (1379/10000)
[Test]  Epoch: 46	Loss: 0.034953	Acc: 18.0% (1798/10000)
[Test]  Epoch: 47	Loss: 0.037109	Acc: 10.7% (1074/10000)
[Test]  Epoch: 48	Loss: 0.036171	Acc: 17.1% (1714/10000)
[Test]  Epoch: 49	Loss: 0.037319	Acc: 13.6% (1358/10000)
[Test]  Epoch: 50	Loss: 0.035670	Acc: 17.6% (1759/10000)
[Test]  Epoch: 51	Loss: 0.035159	Acc: 17.1% (1714/10000)
[Test]  Epoch: 52	Loss: 0.036106	Acc: 15.8% (1583/10000)
[Test]  Epoch: 53	Loss: 0.044932	Acc: 9.9% (986/10000)
[Test]  Epoch: 54	Loss: 0.036677	Acc: 14.4% (1437/10000)
[Test]  Epoch: 55	Loss: 0.042749	Acc: 13.9% (1386/10000)
[Test]  Epoch: 56	Loss: 0.036343	Acc: 13.5% (1353/10000)
[Test]  Epoch: 57	Loss: 0.036165	Acc: 13.6% (1359/10000)
[Test]  Epoch: 58	Loss: 0.048481	Acc: 13.1% (1309/10000)
[Test]  Epoch: 59	Loss: 0.038771	Acc: 15.0% (1503/10000)
[Test]  Epoch: 60	Loss: 0.037608	Acc: 14.9% (1492/10000)
[Test]  Epoch: 61	Loss: 0.035093	Acc: 16.6% (1657/10000)
[Test]  Epoch: 62	Loss: 0.034855	Acc: 16.6% (1660/10000)
[Test]  Epoch: 63	Loss: 0.035035	Acc: 16.9% (1686/10000)
[Test]  Epoch: 64	Loss: 0.034889	Acc: 17.1% (1713/10000)
[Test]  Epoch: 65	Loss: 0.034867	Acc: 17.8% (1776/10000)
[Test]  Epoch: 66	Loss: 0.034800	Acc: 17.9% (1792/10000)
[Test]  Epoch: 67	Loss: 0.034830	Acc: 17.6% (1763/10000)
[Test]  Epoch: 68	Loss: 0.034809	Acc: 18.0% (1798/10000)
[Test]  Epoch: 69	Loss: 0.034837	Acc: 17.6% (1764/10000)
[Test]  Epoch: 70	Loss: 0.034813	Acc: 17.8% (1783/10000)
[Test]  Epoch: 71	Loss: 0.034698	Acc: 17.9% (1795/10000)
[Test]  Epoch: 72	Loss: 0.034781	Acc: 18.3% (1829/10000)
[Test]  Epoch: 73	Loss: 0.034812	Acc: 17.8% (1784/10000)
[Test]  Epoch: 74	Loss: 0.034748	Acc: 18.2% (1819/10000)
[Test]  Epoch: 75	Loss: 0.034613	Acc: 18.7% (1867/10000)
[Test]  Epoch: 76	Loss: 0.034601	Acc: 18.4% (1840/10000)
[Test]  Epoch: 77	Loss: 0.034591	Acc: 18.7% (1868/10000)
[Test]  Epoch: 78	Loss: 0.035017	Acc: 17.9% (1787/10000)
[Test]  Epoch: 79	Loss: 0.034760	Acc: 18.2% (1821/10000)
[Test]  Epoch: 80	Loss: 0.034536	Acc: 18.4% (1845/10000)
[Test]  Epoch: 81	Loss: 0.034791	Acc: 18.0% (1804/10000)
[Test]  Epoch: 82	Loss: 0.034475	Acc: 18.9% (1888/10000)
[Test]  Epoch: 83	Loss: 0.034526	Acc: 18.3% (1833/10000)
[Test]  Epoch: 84	Loss: 0.034583	Acc: 18.6% (1859/10000)
[Test]  Epoch: 85	Loss: 0.034470	Acc: 18.9% (1888/10000)
[Test]  Epoch: 86	Loss: 0.034699	Acc: 18.9% (1888/10000)
[Test]  Epoch: 87	Loss: 0.034585	Acc: 18.9% (1885/10000)
[Test]  Epoch: 88	Loss: 0.034474	Acc: 18.9% (1895/10000)
[Test]  Epoch: 89	Loss: 0.034461	Acc: 19.1% (1907/10000)
[Test]  Epoch: 90	Loss: 0.034562	Acc: 18.8% (1880/10000)
[Test]  Epoch: 91	Loss: 0.034472	Acc: 19.1% (1911/10000)
[Test]  Epoch: 92	Loss: 0.034498	Acc: 19.2% (1919/10000)
[Test]  Epoch: 93	Loss: 0.034461	Acc: 19.6% (1960/10000)
[Test]  Epoch: 94	Loss: 0.034316	Acc: 19.4% (1935/10000)
[Test]  Epoch: 95	Loss: 0.034625	Acc: 19.6% (1965/10000)
[Test]  Epoch: 96	Loss: 0.034434	Acc: 18.6% (1856/10000)
[Test]  Epoch: 97	Loss: 0.034590	Acc: 19.0% (1904/10000)
[Test]  Epoch: 98	Loss: 0.034580	Acc: 19.4% (1945/10000)
[Test]  Epoch: 99	Loss: 0.034470	Acc: 18.1% (1815/10000)
[Test]  Epoch: 100	Loss: 0.034330	Acc: 19.4% (1937/10000)
===========finish==========
['2024-08-18', '18:15:07.919170', '100', 'test', '0.034329626178741454', '19.37', '19.65']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.1.conv.1.weight', '_features.7.conv.2.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.0.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.3.weight', '_features.10.conv.2.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.9.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.15.conv.3.weight', '_features.7.conv.0.0.weight', '_features.5.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.16.conv.0.1.weight', '_features.12.conv.2.weight', '_features.18.1.weight', '_features.9.conv.1.0.weight', '_features.13.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.8.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.2.conv.0.0.weight', '_features.0.0.weight', '_features.10.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.3.conv.1.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 1.045986	Acc: 10.0% (1000/10000)
[Test]  Epoch: 2	Loss: 0.344484	Acc: 10.9% (1093/10000)
[Test]  Epoch: 3	Loss: 0.239803	Acc: 9.7% (974/10000)
[Test]  Epoch: 4	Loss: 0.057396	Acc: 13.3% (1333/10000)
[Test]  Epoch: 5	Loss: 0.046099	Acc: 13.8% (1383/10000)
[Test]  Epoch: 6	Loss: 0.037714	Acc: 15.7% (1566/10000)
[Test]  Epoch: 7	Loss: 0.062981	Acc: 16.2% (1620/10000)
[Test]  Epoch: 8	Loss: 0.391792	Acc: 10.7% (1065/10000)
[Test]  Epoch: 9	Loss: 0.052181	Acc: 10.5% (1047/10000)
[Test]  Epoch: 10	Loss: 0.036190	Acc: 13.7% (1372/10000)
[Test]  Epoch: 11	Loss: 0.035862	Acc: 13.8% (1380/10000)
[Test]  Epoch: 12	Loss: 0.035232	Acc: 15.5% (1546/10000)
[Test]  Epoch: 13	Loss: 0.068865	Acc: 11.7% (1168/10000)
[Test]  Epoch: 14	Loss: 0.035885	Acc: 14.4% (1439/10000)
[Test]  Epoch: 15	Loss: 0.038614	Acc: 11.1% (1110/10000)
[Test]  Epoch: 16	Loss: 0.046601	Acc: 10.8% (1081/10000)
[Test]  Epoch: 17	Loss: 0.134536	Acc: 11.2% (1120/10000)
[Test]  Epoch: 18	Loss: 0.072894	Acc: 12.0% (1204/10000)
[Test]  Epoch: 19	Loss: 0.036693	Acc: 12.6% (1256/10000)
[Test]  Epoch: 20	Loss: 0.038149	Acc: 9.4% (943/10000)
[Test]  Epoch: 21	Loss: 0.101443	Acc: 9.7% (970/10000)
[Test]  Epoch: 22	Loss: 0.055076	Acc: 10.6% (1058/10000)
[Test]  Epoch: 23	Loss: 0.039506	Acc: 10.7% (1071/10000)
[Test]  Epoch: 24	Loss: 0.037235	Acc: 11.7% (1173/10000)
[Test]  Epoch: 25	Loss: 0.037756	Acc: 13.0% (1301/10000)
[Test]  Epoch: 26	Loss: 0.036292	Acc: 14.9% (1493/10000)
[Test]  Epoch: 27	Loss: 0.035080	Acc: 14.8% (1482/10000)
[Test]  Epoch: 28	Loss: 0.036407	Acc: 14.7% (1470/10000)
[Test]  Epoch: 29	Loss: 0.035289	Acc: 11.7% (1170/10000)
[Test]  Epoch: 30	Loss: 0.034695	Acc: 15.5% (1554/10000)
[Test]  Epoch: 31	Loss: 0.037053	Acc: 18.4% (1844/10000)
[Test]  Epoch: 32	Loss: 0.038141	Acc: 9.9% (995/10000)
[Test]  Epoch: 33	Loss: 0.034973	Acc: 15.7% (1568/10000)
[Test]  Epoch: 34	Loss: 0.037082	Acc: 15.3% (1532/10000)
[Test]  Epoch: 35	Loss: 0.035540	Acc: 15.7% (1574/10000)
[Test]  Epoch: 36	Loss: 0.048307	Acc: 13.9% (1389/10000)
[Test]  Epoch: 37	Loss: 0.040410	Acc: 14.5% (1450/10000)
[Test]  Epoch: 38	Loss: 0.037604	Acc: 14.9% (1489/10000)
[Test]  Epoch: 39	Loss: 0.036467	Acc: 16.5% (1651/10000)
[Test]  Epoch: 40	Loss: 0.036562	Acc: 18.3% (1834/10000)
[Test]  Epoch: 41	Loss: 0.037231	Acc: 14.7% (1467/10000)
[Test]  Epoch: 42	Loss: 0.036377	Acc: 15.2% (1521/10000)
[Test]  Epoch: 43	Loss: 0.036133	Acc: 17.7% (1769/10000)
[Test]  Epoch: 44	Loss: 0.035624	Acc: 20.5% (2053/10000)
[Test]  Epoch: 45	Loss: 0.035186	Acc: 19.0% (1898/10000)
[Test]  Epoch: 46	Loss: 0.037588	Acc: 19.2% (1918/10000)
[Test]  Epoch: 47	Loss: 0.036689	Acc: 18.7% (1870/10000)
[Test]  Epoch: 48	Loss: 0.034810	Acc: 18.3% (1826/10000)
[Test]  Epoch: 49	Loss: 0.034213	Acc: 21.5% (2148/10000)
[Test]  Epoch: 50	Loss: 0.039740	Acc: 16.2% (1620/10000)
[Test]  Epoch: 51	Loss: 0.036198	Acc: 17.5% (1748/10000)
[Test]  Epoch: 52	Loss: 0.034215	Acc: 18.0% (1802/10000)
[Test]  Epoch: 53	Loss: 0.040324	Acc: 10.8% (1083/10000)
[Test]  Epoch: 54	Loss: 0.038178	Acc: 17.5% (1751/10000)
[Test]  Epoch: 55	Loss: 0.038195	Acc: 15.8% (1576/10000)
[Test]  Epoch: 56	Loss: 0.035014	Acc: 18.6% (1857/10000)
[Test]  Epoch: 57	Loss: 0.036241	Acc: 18.9% (1889/10000)
[Test]  Epoch: 58	Loss: 0.037290	Acc: 17.4% (1741/10000)
[Test]  Epoch: 59	Loss: 0.034463	Acc: 17.1% (1707/10000)
[Test]  Epoch: 60	Loss: 0.040129	Acc: 16.6% (1661/10000)
[Test]  Epoch: 61	Loss: 0.035097	Acc: 18.4% (1842/10000)
[Test]  Epoch: 62	Loss: 0.034395	Acc: 18.5% (1846/10000)
[Test]  Epoch: 63	Loss: 0.034344	Acc: 17.7% (1773/10000)
[Test]  Epoch: 64	Loss: 0.034093	Acc: 19.0% (1901/10000)
[Test]  Epoch: 65	Loss: 0.034005	Acc: 19.0% (1896/10000)
[Test]  Epoch: 66	Loss: 0.033921	Acc: 19.0% (1899/10000)
[Test]  Epoch: 67	Loss: 0.034027	Acc: 19.1% (1906/10000)
[Test]  Epoch: 68	Loss: 0.033846	Acc: 19.4% (1944/10000)
[Test]  Epoch: 69	Loss: 0.033727	Acc: 19.0% (1902/10000)
[Test]  Epoch: 70	Loss: 0.033733	Acc: 19.4% (1943/10000)
[Test]  Epoch: 71	Loss: 0.033704	Acc: 19.7% (1974/10000)
[Test]  Epoch: 72	Loss: 0.033371	Acc: 19.9% (1990/10000)
[Test]  Epoch: 73	Loss: 0.033561	Acc: 19.7% (1966/10000)
[Test]  Epoch: 74	Loss: 0.033470	Acc: 19.8% (1979/10000)
[Test]  Epoch: 75	Loss: 0.033404	Acc: 20.5% (2048/10000)
[Test]  Epoch: 76	Loss: 0.033251	Acc: 20.5% (2053/10000)
[Test]  Epoch: 77	Loss: 0.033172	Acc: 20.4% (2043/10000)
[Test]  Epoch: 78	Loss: 0.033274	Acc: 20.8% (2075/10000)
[Test]  Epoch: 79	Loss: 0.033023	Acc: 21.1% (2105/10000)
[Test]  Epoch: 80	Loss: 0.033041	Acc: 20.8% (2082/10000)
[Test]  Epoch: 81	Loss: 0.033232	Acc: 20.7% (2074/10000)
[Test]  Epoch: 82	Loss: 0.033061	Acc: 21.1% (2111/10000)
[Test]  Epoch: 83	Loss: 0.032968	Acc: 21.2% (2124/10000)
[Test]  Epoch: 84	Loss: 0.032995	Acc: 20.8% (2082/10000)
[Test]  Epoch: 85	Loss: 0.033207	Acc: 21.1% (2111/10000)
[Test]  Epoch: 86	Loss: 0.033127	Acc: 21.4% (2138/10000)
[Test]  Epoch: 87	Loss: 0.033063	Acc: 21.6% (2158/10000)
[Test]  Epoch: 88	Loss: 0.033302	Acc: 21.9% (2186/10000)
[Test]  Epoch: 89	Loss: 0.032852	Acc: 21.9% (2192/10000)
[Test]  Epoch: 90	Loss: 0.032710	Acc: 21.6% (2159/10000)
[Test]  Epoch: 91	Loss: 0.032723	Acc: 22.0% (2199/10000)
[Test]  Epoch: 92	Loss: 0.032638	Acc: 22.3% (2229/10000)
[Test]  Epoch: 93	Loss: 0.032686	Acc: 22.0% (2197/10000)
[Test]  Epoch: 94	Loss: 0.032975	Acc: 22.9% (2287/10000)
[Test]  Epoch: 95	Loss: 0.033016	Acc: 23.0% (2299/10000)
[Test]  Epoch: 96	Loss: 0.032837	Acc: 22.7% (2270/10000)
[Test]  Epoch: 97	Loss: 0.032511	Acc: 22.6% (2262/10000)
[Test]  Epoch: 98	Loss: 0.032490	Acc: 22.8% (2282/10000)
[Test]  Epoch: 99	Loss: 0.032864	Acc: 22.7% (2274/10000)
[Test]  Epoch: 100	Loss: 0.032670	Acc: 23.1% (2306/10000)
===========finish==========
['2024-08-18', '18:17:27.555888', '100', 'test', '0.032669758570194246', '23.06', '23.06']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.1.conv.1.weight', '_features.7.conv.2.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.0.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.3.weight', '_features.10.conv.2.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.9.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.15.conv.3.weight', '_features.7.conv.0.0.weight', '_features.5.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.16.conv.0.1.weight', '_features.12.conv.2.weight', '_features.18.1.weight', '_features.9.conv.1.0.weight', '_features.13.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.8.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.2.conv.0.0.weight', '_features.0.0.weight', '_features.10.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.3.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.9.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.4.conv.3.weight', '_features.5.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 1.018893	Acc: 10.0% (1000/10000)
[Test]  Epoch: 2	Loss: 0.279112	Acc: 13.5% (1346/10000)
[Test]  Epoch: 3	Loss: 0.077725	Acc: 15.3% (1529/10000)
[Test]  Epoch: 4	Loss: 0.075825	Acc: 10.9% (1087/10000)
[Test]  Epoch: 5	Loss: 0.062882	Acc: 11.9% (1191/10000)
[Test]  Epoch: 6	Loss: 0.036564	Acc: 11.3% (1127/10000)
[Test]  Epoch: 7	Loss: 0.058025	Acc: 12.7% (1269/10000)
[Test]  Epoch: 8	Loss: 0.037087	Acc: 9.9% (988/10000)
[Test]  Epoch: 9	Loss: 0.040712	Acc: 11.2% (1116/10000)
[Test]  Epoch: 10	Loss: 0.052980	Acc: 10.4% (1044/10000)
[Test]  Epoch: 11	Loss: 0.062784	Acc: 11.4% (1145/10000)
[Test]  Epoch: 12	Loss: 0.063066	Acc: 12.3% (1228/10000)
[Test]  Epoch: 13	Loss: 0.046158	Acc: 13.4% (1344/10000)
[Test]  Epoch: 14	Loss: 0.038126	Acc: 10.4% (1037/10000)
[Test]  Epoch: 15	Loss: 0.037382	Acc: 15.1% (1508/10000)
[Test]  Epoch: 16	Loss: 0.035801	Acc: 14.1% (1411/10000)
[Test]  Epoch: 17	Loss: 0.105229	Acc: 11.7% (1165/10000)
[Test]  Epoch: 18	Loss: 0.036585	Acc: 12.6% (1255/10000)
[Test]  Epoch: 19	Loss: 0.038171	Acc: 10.3% (1035/10000)
[Test]  Epoch: 20	Loss: 0.039978	Acc: 11.4% (1145/10000)
[Test]  Epoch: 21	Loss: 0.040436	Acc: 15.1% (1509/10000)
[Test]  Epoch: 22	Loss: 0.037969	Acc: 13.3% (1327/10000)
[Test]  Epoch: 23	Loss: 0.043542	Acc: 17.5% (1749/10000)
[Test]  Epoch: 24	Loss: 0.040901	Acc: 18.1% (1808/10000)
[Test]  Epoch: 25	Loss: 0.035930	Acc: 12.4% (1244/10000)
[Test]  Epoch: 26	Loss: 0.040953	Acc: 10.9% (1093/10000)
[Test]  Epoch: 27	Loss: 0.202074	Acc: 8.9% (891/10000)
[Test]  Epoch: 28	Loss: 0.101336	Acc: 11.1% (1107/10000)
[Test]  Epoch: 29	Loss: 0.037158	Acc: 12.4% (1244/10000)
[Test]  Epoch: 30	Loss: 0.036098	Acc: 12.5% (1254/10000)
[Test]  Epoch: 31	Loss: 0.041976	Acc: 12.4% (1243/10000)
[Test]  Epoch: 32	Loss: 0.043724	Acc: 13.3% (1332/10000)
[Test]  Epoch: 33	Loss: 0.039174	Acc: 11.4% (1137/10000)
[Test]  Epoch: 34	Loss: 0.050671	Acc: 10.1% (1009/10000)
[Test]  Epoch: 35	Loss: 0.048817	Acc: 14.9% (1489/10000)
[Test]  Epoch: 36	Loss: 0.045072	Acc: 12.5% (1252/10000)
[Test]  Epoch: 37	Loss: 0.043044	Acc: 13.3% (1334/10000)
[Test]  Epoch: 38	Loss: 0.040764	Acc: 12.6% (1262/10000)
[Test]  Epoch: 39	Loss: 0.036381	Acc: 10.9% (1095/10000)
[Test]  Epoch: 40	Loss: 0.045601	Acc: 10.3% (1031/10000)
[Test]  Epoch: 41	Loss: 0.042312	Acc: 9.4% (938/10000)
[Test]  Epoch: 42	Loss: 0.038447	Acc: 11.4% (1141/10000)
[Test]  Epoch: 43	Loss: 0.036462	Acc: 10.8% (1076/10000)
[Test]  Epoch: 44	Loss: 0.047677	Acc: 10.9% (1086/10000)
[Test]  Epoch: 45	Loss: 0.447871	Acc: 10.0% (1003/10000)
[Test]  Epoch: 46	Loss: 0.358707	Acc: 10.7% (1067/10000)
[Test]  Epoch: 47	Loss: 0.052181	Acc: 13.5% (1346/10000)
[Test]  Epoch: 48	Loss: 0.041808	Acc: 17.7% (1768/10000)
[Test]  Epoch: 49	Loss: 0.035858	Acc: 15.2% (1522/10000)
[Test]  Epoch: 50	Loss: 0.110123	Acc: 12.6% (1259/10000)
[Test]  Epoch: 51	Loss: 0.056572	Acc: 15.0% (1503/10000)
[Test]  Epoch: 52	Loss: 0.034432	Acc: 18.1% (1808/10000)
[Test]  Epoch: 53	Loss: 0.038872	Acc: 13.4% (1341/10000)
[Test]  Epoch: 54	Loss: 0.037182	Acc: 9.2% (920/10000)
[Test]  Epoch: 55	Loss: 0.040289	Acc: 13.1% (1312/10000)
[Test]  Epoch: 56	Loss: 0.049003	Acc: 13.4% (1344/10000)
[Test]  Epoch: 57	Loss: 0.040695	Acc: 9.9% (991/10000)
[Test]  Epoch: 58	Loss: 0.035011	Acc: 15.7% (1570/10000)
[Test]  Epoch: 59	Loss: 0.050826	Acc: 12.2% (1215/10000)
[Test]  Epoch: 60	Loss: 0.036717	Acc: 13.2% (1320/10000)
[Test]  Epoch: 61	Loss: 0.035464	Acc: 14.6% (1455/10000)
[Test]  Epoch: 62	Loss: 0.035889	Acc: 15.2% (1524/10000)
[Test]  Epoch: 63	Loss: 0.034950	Acc: 16.0% (1603/10000)
[Test]  Epoch: 64	Loss: 0.034902	Acc: 15.7% (1574/10000)
[Test]  Epoch: 65	Loss: 0.034710	Acc: 16.0% (1603/10000)
[Test]  Epoch: 66	Loss: 0.034697	Acc: 16.3% (1629/10000)
[Test]  Epoch: 67	Loss: 0.035076	Acc: 16.8% (1683/10000)
[Test]  Epoch: 68	Loss: 0.034884	Acc: 16.2% (1616/10000)
[Test]  Epoch: 69	Loss: 0.034956	Acc: 16.6% (1663/10000)
[Test]  Epoch: 70	Loss: 0.034799	Acc: 16.5% (1646/10000)
[Test]  Epoch: 71	Loss: 0.034553	Acc: 16.9% (1695/10000)
[Test]  Epoch: 72	Loss: 0.034631	Acc: 17.4% (1736/10000)
[Test]  Epoch: 73	Loss: 0.035453	Acc: 16.8% (1679/10000)
[Test]  Epoch: 74	Loss: 0.034765	Acc: 17.4% (1739/10000)
[Test]  Epoch: 75	Loss: 0.034481	Acc: 17.3% (1729/10000)
[Test]  Epoch: 76	Loss: 0.034516	Acc: 17.7% (1767/10000)
[Test]  Epoch: 77	Loss: 0.034554	Acc: 17.9% (1790/10000)
[Test]  Epoch: 78	Loss: 0.034559	Acc: 18.1% (1813/10000)
[Test]  Epoch: 79	Loss: 0.034450	Acc: 18.2% (1818/10000)
[Test]  Epoch: 80	Loss: 0.034575	Acc: 18.2% (1817/10000)
[Test]  Epoch: 81	Loss: 0.034388	Acc: 18.6% (1858/10000)
[Test]  Epoch: 82	Loss: 0.034541	Acc: 18.1% (1811/10000)
[Test]  Epoch: 83	Loss: 0.034317	Acc: 18.1% (1815/10000)
[Test]  Epoch: 84	Loss: 0.034328	Acc: 18.1% (1815/10000)
[Test]  Epoch: 85	Loss: 0.034598	Acc: 18.6% (1858/10000)
[Test]  Epoch: 86	Loss: 0.034498	Acc: 18.5% (1853/10000)
[Test]  Epoch: 87	Loss: 0.034502	Acc: 18.2% (1823/10000)
[Test]  Epoch: 88	Loss: 0.034411	Acc: 17.9% (1788/10000)
[Test]  Epoch: 89	Loss: 0.034325	Acc: 18.0% (1801/10000)
[Test]  Epoch: 90	Loss: 0.034365	Acc: 19.0% (1900/10000)
[Test]  Epoch: 91	Loss: 0.034406	Acc: 18.4% (1835/10000)
[Test]  Epoch: 92	Loss: 0.034258	Acc: 18.9% (1885/10000)
[Test]  Epoch: 93	Loss: 0.034471	Acc: 18.3% (1827/10000)
[Test]  Epoch: 94	Loss: 0.034458	Acc: 18.8% (1876/10000)
[Test]  Epoch: 95	Loss: 0.034612	Acc: 17.8% (1783/10000)
[Test]  Epoch: 96	Loss: 0.034260	Acc: 18.9% (1895/10000)
[Test]  Epoch: 97	Loss: 0.034514	Acc: 19.2% (1920/10000)
[Test]  Epoch: 98	Loss: 0.034316	Acc: 19.1% (1911/10000)
[Test]  Epoch: 99	Loss: 0.034262	Acc: 19.1% (1909/10000)
[Test]  Epoch: 100	Loss: 0.034297	Acc: 18.8% (1877/10000)
===========finish==========
['2024-08-18', '18:19:47.072991', '100', 'test', '0.0342967942237854', '18.77', '19.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.1.conv.1.weight', '_features.7.conv.2.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.0.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.3.weight', '_features.10.conv.2.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.9.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.15.conv.3.weight', '_features.7.conv.0.0.weight', '_features.5.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.16.conv.0.1.weight', '_features.12.conv.2.weight', '_features.18.1.weight', '_features.9.conv.1.0.weight', '_features.13.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.8.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.2.conv.0.0.weight', '_features.0.0.weight', '_features.10.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.3.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.9.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.4.conv.3.weight', '_features.5.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.12.conv.3.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.13.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.931482	Acc: 10.0% (1000/10000)
[Test]  Epoch: 2	Loss: 0.809885	Acc: 9.8% (978/10000)
[Test]  Epoch: 3	Loss: 0.182733	Acc: 13.8% (1378/10000)
[Test]  Epoch: 4	Loss: 0.071772	Acc: 9.6% (958/10000)
[Test]  Epoch: 5	Loss: 0.046951	Acc: 11.3% (1130/10000)
[Test]  Epoch: 6	Loss: 0.038067	Acc: 11.9% (1190/10000)
[Test]  Epoch: 7	Loss: 0.040174	Acc: 15.6% (1556/10000)
[Test]  Epoch: 8	Loss: 0.036914	Acc: 12.8% (1280/10000)
[Test]  Epoch: 9	Loss: 0.036801	Acc: 11.5% (1150/10000)
[Test]  Epoch: 10	Loss: 0.038804	Acc: 9.9% (994/10000)
[Test]  Epoch: 11	Loss: 0.184910	Acc: 10.0% (1000/10000)
[Test]  Epoch: 12	Loss: 0.123713	Acc: 13.1% (1310/10000)
[Test]  Epoch: 13	Loss: 0.036631	Acc: 18.1% (1807/10000)
[Test]  Epoch: 14	Loss: 0.055871	Acc: 10.8% (1077/10000)
[Test]  Epoch: 15	Loss: 0.042798	Acc: 14.3% (1428/10000)
[Test]  Epoch: 16	Loss: 0.042092	Acc: 10.1% (1010/10000)
[Test]  Epoch: 17	Loss: 0.038441	Acc: 15.1% (1513/10000)
[Test]  Epoch: 18	Loss: 0.035295	Acc: 17.6% (1757/10000)
[Test]  Epoch: 19	Loss: 0.037385	Acc: 16.6% (1659/10000)
[Test]  Epoch: 20	Loss: 0.037407	Acc: 19.1% (1913/10000)
[Test]  Epoch: 21	Loss: 0.068015	Acc: 10.2% (1018/10000)
[Test]  Epoch: 22	Loss: 0.036123	Acc: 16.0% (1604/10000)
[Test]  Epoch: 23	Loss: 0.035175	Acc: 16.5% (1654/10000)
[Test]  Epoch: 24	Loss: 0.074430	Acc: 15.0% (1497/10000)
[Test]  Epoch: 25	Loss: 0.036898	Acc: 9.6% (960/10000)
[Test]  Epoch: 26	Loss: 0.036335	Acc: 9.7% (972/10000)
[Test]  Epoch: 27	Loss: 0.037543	Acc: 9.2% (922/10000)
[Test]  Epoch: 28	Loss: 0.072923	Acc: 9.3% (934/10000)
[Test]  Epoch: 29	Loss: 0.036623	Acc: 9.9% (994/10000)
[Test]  Epoch: 30	Loss: 0.247771	Acc: 10.0% (1000/10000)
[Test]  Epoch: 31	Loss: 0.300568	Acc: 10.0% (1001/10000)
[Test]  Epoch: 32	Loss: 0.037269	Acc: 11.1% (1107/10000)
[Test]  Epoch: 33	Loss: 0.061494	Acc: 10.9% (1095/10000)
[Test]  Epoch: 34	Loss: 0.313671	Acc: 10.0% (1000/10000)
[Test]  Epoch: 35	Loss: 0.085778	Acc: 8.3% (830/10000)
[Test]  Epoch: 36	Loss: 0.043168	Acc: 12.0% (1199/10000)
[Test]  Epoch: 37	Loss: 0.036295	Acc: 15.1% (1508/10000)
[Test]  Epoch: 38	Loss: 0.064129	Acc: 15.4% (1536/10000)
[Test]  Epoch: 39	Loss: 0.038195	Acc: 16.4% (1636/10000)
[Test]  Epoch: 40	Loss: 0.035313	Acc: 16.9% (1685/10000)
[Test]  Epoch: 41	Loss: 0.043487	Acc: 12.5% (1253/10000)
[Test]  Epoch: 42	Loss: 0.046973	Acc: 12.4% (1240/10000)
[Test]  Epoch: 43	Loss: 0.038150	Acc: 8.6% (857/10000)
[Test]  Epoch: 44	Loss: 0.037202	Acc: 10.9% (1092/10000)
[Test]  Epoch: 45	Loss: 0.037640	Acc: 12.0% (1196/10000)
[Test]  Epoch: 46	Loss: 0.054115	Acc: 11.0% (1098/10000)
[Test]  Epoch: 47	Loss: 0.037562	Acc: 10.6% (1061/10000)
[Test]  Epoch: 48	Loss: 0.038121	Acc: 12.3% (1229/10000)
[Test]  Epoch: 49	Loss: 0.036492	Acc: 15.5% (1546/10000)
[Test]  Epoch: 50	Loss: 0.036920	Acc: 13.0% (1299/10000)
[Test]  Epoch: 51	Loss: 0.038483	Acc: 10.4% (1039/10000)
[Test]  Epoch: 52	Loss: 0.036068	Acc: 11.7% (1173/10000)
[Test]  Epoch: 53	Loss: 0.294664	Acc: 9.3% (926/10000)
[Test]  Epoch: 54	Loss: 0.039794	Acc: 10.1% (1005/10000)
[Test]  Epoch: 55	Loss: 0.036905	Acc: 13.7% (1371/10000)
[Test]  Epoch: 56	Loss: 0.037592	Acc: 13.8% (1381/10000)
[Test]  Epoch: 57	Loss: 0.053464	Acc: 15.1% (1513/10000)
[Test]  Epoch: 58	Loss: 0.042128	Acc: 13.4% (1337/10000)
[Test]  Epoch: 59	Loss: 0.035715	Acc: 16.9% (1685/10000)
[Test]  Epoch: 60	Loss: 0.039531	Acc: 17.3% (1730/10000)
[Test]  Epoch: 61	Loss: 0.034444	Acc: 18.2% (1824/10000)
[Test]  Epoch: 62	Loss: 0.034338	Acc: 19.3% (1932/10000)
[Test]  Epoch: 63	Loss: 0.034141	Acc: 19.2% (1916/10000)
[Test]  Epoch: 64	Loss: 0.034074	Acc: 18.9% (1893/10000)
[Test]  Epoch: 65	Loss: 0.034086	Acc: 19.1% (1912/10000)
[Test]  Epoch: 66	Loss: 0.034088	Acc: 18.6% (1864/10000)
[Test]  Epoch: 67	Loss: 0.033940	Acc: 19.0% (1898/10000)
[Test]  Epoch: 68	Loss: 0.033754	Acc: 19.1% (1911/10000)
[Test]  Epoch: 69	Loss: 0.033826	Acc: 18.9% (1894/10000)
[Test]  Epoch: 70	Loss: 0.034011	Acc: 18.6% (1855/10000)
[Test]  Epoch: 71	Loss: 0.033876	Acc: 19.0% (1896/10000)
[Test]  Epoch: 72	Loss: 0.033624	Acc: 19.1% (1915/10000)
[Test]  Epoch: 73	Loss: 0.033649	Acc: 18.8% (1879/10000)
[Test]  Epoch: 74	Loss: 0.033597	Acc: 19.1% (1907/10000)
[Test]  Epoch: 75	Loss: 0.033535	Acc: 18.9% (1887/10000)
[Test]  Epoch: 76	Loss: 0.033579	Acc: 18.9% (1892/10000)
[Test]  Epoch: 77	Loss: 0.033757	Acc: 19.7% (1972/10000)
[Test]  Epoch: 78	Loss: 0.034481	Acc: 19.2% (1919/10000)
[Test]  Epoch: 79	Loss: 0.033755	Acc: 20.4% (2044/10000)
[Test]  Epoch: 80	Loss: 0.033571	Acc: 20.5% (2051/10000)
[Test]  Epoch: 81	Loss: 0.033841	Acc: 18.8% (1881/10000)
[Test]  Epoch: 82	Loss: 0.033659	Acc: 19.2% (1924/10000)
[Test]  Epoch: 83	Loss: 0.033462	Acc: 19.1% (1912/10000)
[Test]  Epoch: 84	Loss: 0.033806	Acc: 19.1% (1911/10000)
[Test]  Epoch: 85	Loss: 0.033579	Acc: 20.3% (2034/10000)
[Test]  Epoch: 86	Loss: 0.033516	Acc: 19.8% (1979/10000)
[Test]  Epoch: 87	Loss: 0.034103	Acc: 19.4% (1936/10000)
[Test]  Epoch: 88	Loss: 0.034089	Acc: 19.0% (1904/10000)
[Test]  Epoch: 89	Loss: 0.034222	Acc: 19.0% (1904/10000)
[Test]  Epoch: 90	Loss: 0.033652	Acc: 21.2% (2116/10000)
[Test]  Epoch: 91	Loss: 0.034334	Acc: 16.9% (1694/10000)
[Test]  Epoch: 92	Loss: 0.033781	Acc: 19.2% (1919/10000)
[Test]  Epoch: 93	Loss: 0.033557	Acc: 19.6% (1957/10000)
[Test]  Epoch: 94	Loss: 0.033855	Acc: 19.4% (1938/10000)
[Test]  Epoch: 95	Loss: 0.033654	Acc: 19.2% (1919/10000)
[Test]  Epoch: 96	Loss: 0.033511	Acc: 19.9% (1991/10000)
[Test]  Epoch: 97	Loss: 0.033434	Acc: 20.3% (2032/10000)
[Test]  Epoch: 98	Loss: 0.033835	Acc: 19.8% (1980/10000)
[Test]  Epoch: 99	Loss: 0.033424	Acc: 19.6% (1959/10000)
[Test]  Epoch: 100	Loss: 0.033416	Acc: 19.4% (1940/10000)
===========finish==========
['2024-08-18', '18:22:10.950736', '100', 'test', '0.03341618931293488', '19.4', '21.16']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/cifar10-mobilenetv2-channel mobilenetv2 CIFAR10 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/cifar10-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 10 classes
Files already downloaded and verified
[INFO] channel_percent = -1.0
Files already downloaded and verified
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.18.0.weight', 'last_linear.weight', '_features.17.conv.3.weight', '_features.17.conv.2.weight', '_features.17.conv.0.0.weight', '_features.17.conv.1.0.weight', '_features.17.conv.0.1.weight', '_features.14.conv.0.1.weight', '_features.17.conv.1.1.weight', '_features.14.conv.2.weight', '_features.14.conv.1.0.weight', '_features.14.conv.1.1.weight', '_features.14.conv.3.weight', '_features.16.conv.2.weight', '_features.4.conv.2.weight', '_features.15.conv.2.weight', '_features.14.conv.0.0.weight', '_features.16.conv.0.0.weight', '_features.11.conv.2.weight', '_features.11.conv.0.0.weight', '_features.9.conv.2.weight', '_features.1.conv.1.weight', '_features.7.conv.2.weight', '_features.11.conv.1.0.weight', '_features.6.conv.2.weight', '_features.1.conv.0.0.weight', '_features.16.conv.1.1.weight', '_features.16.conv.3.weight', '_features.10.conv.2.weight', '_features.3.conv.2.weight', '_features.2.conv.2.weight', '_features.9.conv.0.0.weight', '_features.3.conv.1.0.weight', '_features.15.conv.0.0.weight', '_features.15.conv.3.weight', '_features.7.conv.0.0.weight', '_features.5.conv.2.weight', '_features.8.conv.2.weight', '_features.4.conv.0.0.weight', '_features.11.conv.0.1.weight', '_features.6.conv.1.0.weight', '_features.6.conv.0.0.weight', '_features.16.conv.1.0.weight', '_features.11.conv.3.weight', '_features.9.conv.0.1.weight', '_features.16.conv.0.1.weight', '_features.12.conv.2.weight', '_features.18.1.weight', '_features.9.conv.1.0.weight', '_features.13.conv.2.weight', '_features.10.conv.0.0.weight', '_features.11.conv.1.1.weight', '_features.4.conv.0.1.weight', '_features.12.conv.1.0.weight', '_features.6.conv.0.1.weight', '_features.8.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.1.0.weight', '_features.8.conv.1.0.weight', '_features.6.conv.1.1.weight', '_features.2.conv.0.0.weight', '_features.0.0.weight', '_features.10.conv.1.0.weight', '_features.4.conv.1.1.weight', '_features.5.conv.0.0.weight', '_features.9.conv.1.1.weight', '_features.12.conv.0.0.weight', '_features.3.conv.1.1.weight', '_features.7.conv.1.1.weight', '_features.7.conv.3.weight', '_features.12.conv.0.1.weight', '_features.9.conv.3.weight', '_features.15.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.4.conv.3.weight', '_features.5.conv.0.1.weight', '_features.2.conv.1.1.weight', '_features.10.conv.3.weight', '_features.13.conv.1.0.weight', '_features.8.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.12.conv.3.weight', '_features.10.conv.0.1.weight', '_features.10.conv.1.1.weight', '_features.15.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.13.conv.3.weight', '_features.2.conv.3.weight', '_features.6.conv.3.weight', '_features.8.conv.3.weight', '_features.12.conv.1.1.weight', '_features.1.conv.2.weight', '_features.15.conv.1.1.weight', '_features.3.conv.3.weight', '_features.5.conv.3.weight', '_features.0.1.weight', '_features.1.conv.0.1.weight', '_features.13.conv.0.1.weight', '_features.13.conv.1.1.weight', '_features.13.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 1.128316	Acc: 10.0% (1000/10000)
[Test]  Epoch: 2	Loss: 0.397232	Acc: 12.3% (1229/10000)
[Test]  Epoch: 3	Loss: 0.036262	Acc: 11.6% (1157/10000)
[Test]  Epoch: 4	Loss: 0.503080	Acc: 10.3% (1030/10000)
[Test]  Epoch: 5	Loss: 0.036248	Acc: 12.4% (1237/10000)
[Test]  Epoch: 6	Loss: 0.424054	Acc: 9.4% (941/10000)
[Test]  Epoch: 7	Loss: 0.036468	Acc: 13.1% (1311/10000)
[Test]  Epoch: 8	Loss: 0.088486	Acc: 8.9% (890/10000)
[Test]  Epoch: 9	Loss: 0.037866	Acc: 7.8% (781/10000)
[Test]  Epoch: 10	Loss: 0.041862	Acc: 10.8% (1076/10000)
[Test]  Epoch: 11	Loss: 0.104283	Acc: 10.9% (1086/10000)
[Test]  Epoch: 12	Loss: 0.585990	Acc: 10.0% (1001/10000)
[Test]  Epoch: 13	Loss: 0.170595	Acc: 10.5% (1048/10000)
[Test]  Epoch: 14	Loss: 0.081799	Acc: 11.0% (1097/10000)
[Test]  Epoch: 15	Loss: 0.036074	Acc: 12.4% (1241/10000)
[Test]  Epoch: 16	Loss: 0.037687	Acc: 10.0% (999/10000)
[Test]  Epoch: 17	Loss: 0.036106	Acc: 12.7% (1270/10000)
[Test]  Epoch: 18	Loss: 0.049097	Acc: 13.5% (1346/10000)
[Test]  Epoch: 19	Loss: 0.037140	Acc: 10.1% (1014/10000)
[Test]  Epoch: 20	Loss: 0.057985	Acc: 12.6% (1256/10000)
[Test]  Epoch: 21	Loss: 0.037016	Acc: 10.4% (1036/10000)
[Test]  Epoch: 22	Loss: 0.038792	Acc: 11.2% (1117/10000)
[Test]  Epoch: 23	Loss: 0.037549	Acc: 15.0% (1497/10000)
[Test]  Epoch: 24	Loss: 0.036779	Acc: 11.9% (1187/10000)
[Test]  Epoch: 25	Loss: 0.035520	Acc: 15.2% (1516/10000)
[Test]  Epoch: 26	Loss: 0.035721	Acc: 16.3% (1634/10000)
[Test]  Epoch: 27	Loss: 0.037420	Acc: 13.9% (1389/10000)
[Test]  Epoch: 28	Loss: 0.043190	Acc: 14.3% (1426/10000)
[Test]  Epoch: 29	Loss: 0.037412	Acc: 10.0% (998/10000)
[Test]  Epoch: 30	Loss: 0.051259	Acc: 10.8% (1079/10000)
[Test]  Epoch: 31	Loss: 0.039012	Acc: 11.7% (1166/10000)
[Test]  Epoch: 32	Loss: 0.036631	Acc: 12.5% (1250/10000)
[Test]  Epoch: 33	Loss: 0.112077	Acc: 10.4% (1042/10000)
[Test]  Epoch: 34	Loss: 0.051582	Acc: 12.8% (1275/10000)
[Test]  Epoch: 35	Loss: 0.036413	Acc: 12.7% (1268/10000)
[Test]  Epoch: 36	Loss: 0.042057	Acc: 15.3% (1530/10000)
[Test]  Epoch: 37	Loss: 0.041712	Acc: 11.2% (1115/10000)
[Test]  Epoch: 38	Loss: 0.037193	Acc: 12.7% (1267/10000)
[Test]  Epoch: 39	Loss: 0.036626	Acc: 15.0% (1497/10000)
[Test]  Epoch: 40	Loss: 0.051110	Acc: 8.3% (831/10000)
[Test]  Epoch: 41	Loss: 0.036428	Acc: 12.1% (1210/10000)
[Test]  Epoch: 42	Loss: 0.038563	Acc: 12.6% (1264/10000)
[Test]  Epoch: 43	Loss: 0.040721	Acc: 12.9% (1287/10000)
[Test]  Epoch: 44	Loss: 0.036659	Acc: 12.1% (1208/10000)
[Test]  Epoch: 45	Loss: 0.036787	Acc: 13.5% (1352/10000)
[Test]  Epoch: 46	Loss: 0.038871	Acc: 12.8% (1281/10000)
[Test]  Epoch: 47	Loss: 0.043326	Acc: 14.3% (1426/10000)
[Test]  Epoch: 48	Loss: 0.035358	Acc: 17.1% (1715/10000)
[Test]  Epoch: 49	Loss: 0.035483	Acc: 17.1% (1710/10000)
[Test]  Epoch: 50	Loss: 0.036720	Acc: 15.6% (1557/10000)
[Test]  Epoch: 51	Loss: 0.036088	Acc: 16.9% (1692/10000)
[Test]  Epoch: 52	Loss: 0.058082	Acc: 12.6% (1256/10000)
[Test]  Epoch: 53	Loss: 0.042505	Acc: 11.4% (1137/10000)
[Test]  Epoch: 54	Loss: 0.037223	Acc: 12.9% (1291/10000)
[Test]  Epoch: 55	Loss: 0.036469	Acc: 14.1% (1407/10000)
[Test]  Epoch: 56	Loss: 0.062050	Acc: 11.9% (1188/10000)
[Test]  Epoch: 57	Loss: 0.042146	Acc: 12.4% (1241/10000)
[Test]  Epoch: 58	Loss: 0.042626	Acc: 14.1% (1414/10000)
[Test]  Epoch: 59	Loss: 0.036244	Acc: 17.1% (1708/10000)
[Test]  Epoch: 60	Loss: 0.036546	Acc: 15.5% (1549/10000)
[Test]  Epoch: 61	Loss: 0.035508	Acc: 17.5% (1752/10000)
[Test]  Epoch: 62	Loss: 0.035320	Acc: 17.9% (1791/10000)
[Test]  Epoch: 63	Loss: 0.035260	Acc: 17.9% (1794/10000)
[Test]  Epoch: 64	Loss: 0.035070	Acc: 17.6% (1762/10000)
[Test]  Epoch: 65	Loss: 0.035067	Acc: 17.7% (1770/10000)
[Test]  Epoch: 66	Loss: 0.035015	Acc: 17.8% (1784/10000)
[Test]  Epoch: 67	Loss: 0.035059	Acc: 18.1% (1808/10000)
[Test]  Epoch: 68	Loss: 0.035063	Acc: 18.0% (1796/10000)
[Test]  Epoch: 69	Loss: 0.035257	Acc: 17.9% (1785/10000)
[Test]  Epoch: 70	Loss: 0.035141	Acc: 17.9% (1791/10000)
[Test]  Epoch: 71	Loss: 0.035022	Acc: 17.8% (1780/10000)
[Test]  Epoch: 72	Loss: 0.034952	Acc: 18.2% (1818/10000)
[Test]  Epoch: 73	Loss: 0.034998	Acc: 17.5% (1750/10000)
[Test]  Epoch: 74	Loss: 0.035052	Acc: 16.8% (1678/10000)
[Test]  Epoch: 75	Loss: 0.035019	Acc: 17.9% (1791/10000)
[Test]  Epoch: 76	Loss: 0.034974	Acc: 17.9% (1786/10000)
[Test]  Epoch: 77	Loss: 0.035122	Acc: 17.1% (1706/10000)
[Test]  Epoch: 78	Loss: 0.035071	Acc: 17.0% (1696/10000)
[Test]  Epoch: 79	Loss: 0.035091	Acc: 16.6% (1661/10000)
[Test]  Epoch: 80	Loss: 0.035005	Acc: 16.8% (1678/10000)
[Test]  Epoch: 81	Loss: 0.034913	Acc: 17.3% (1731/10000)
[Test]  Epoch: 82	Loss: 0.034913	Acc: 18.3% (1828/10000)
[Test]  Epoch: 83	Loss: 0.034877	Acc: 17.9% (1793/10000)
[Test]  Epoch: 84	Loss: 0.034873	Acc: 16.8% (1683/10000)
[Test]  Epoch: 85	Loss: 0.035125	Acc: 18.3% (1830/10000)
[Test]  Epoch: 86	Loss: 0.034938	Acc: 18.1% (1814/10000)
[Test]  Epoch: 87	Loss: 0.034863	Acc: 17.4% (1735/10000)
[Test]  Epoch: 88	Loss: 0.034781	Acc: 18.1% (1810/10000)
[Test]  Epoch: 89	Loss: 0.034834	Acc: 18.3% (1830/10000)
[Test]  Epoch: 90	Loss: 0.034855	Acc: 18.1% (1813/10000)
[Test]  Epoch: 91	Loss: 0.034906	Acc: 16.7% (1671/10000)
[Test]  Epoch: 92	Loss: 0.034962	Acc: 16.8% (1680/10000)
[Test]  Epoch: 93	Loss: 0.034721	Acc: 17.6% (1758/10000)
[Test]  Epoch: 94	Loss: 0.034781	Acc: 18.3% (1828/10000)
[Test]  Epoch: 95	Loss: 0.034761	Acc: 17.6% (1764/10000)
[Test]  Epoch: 96	Loss: 0.034756	Acc: 17.8% (1778/10000)
[Test]  Epoch: 97	Loss: 0.034660	Acc: 18.6% (1856/10000)
[Test]  Epoch: 98	Loss: 0.034660	Acc: 18.6% (1860/10000)
[Test]  Epoch: 99	Loss: 0.034740	Acc: 17.1% (1706/10000)
[Test]  Epoch: 100	Loss: 0.034765	Acc: 17.4% (1738/10000)
===========finish==========
['2024-08-18', '18:25:12.097501', '100', 'test', '0.034765366005897524', '17.38', '18.6']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-resnet18/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('layer3.0.conv2.weight', 179.83779907226562), ('layer3.0.conv1.weight', 170.54595947265625), ('conv1.weight', 143.78366088867188), ('layer2.0.conv1.weight', 139.1171112060547), ('layer4.0.conv1.weight', 137.6432342529297), ('layer2.0.conv2.weight', 125.38401794433594), ('layer4.0.downsample.0.weight', 112.9768295288086), ('layer4.0.conv2.weight', 105.53776550292969), ('last_linear.weight', 104.96109771728516), ('layer2.1.conv1.weight', 79.29855346679688), ('layer1.0.conv1.weight', 74.96281433105469), ('layer2.0.downsample.0.weight', 63.480812072753906), ('layer2.1.conv2.weight', 56.582401275634766), ('layer1.1.conv1.weight', 56.3514404296875), ('layer1.0.conv2.weight', 45.145294189453125), ('layer4.1.conv1.weight', 41.95225524902344), ('layer1.1.conv2.weight', 41.46173858642578), ('layer3.0.downsample.0.weight', 34.669158935546875), ('layer4.1.conv2.weight', 33.74342346191406), ('bn1.weight', 27.559120178222656), ('layer3.0.bn1.weight', 20.991518020629883), ('layer3.0.bn2.weight', 19.74270248413086), ('layer4.0.bn1.weight', 18.035314559936523), ('layer2.0.downsample.1.weight', 13.875690460205078), ('layer2.0.bn1.weight', 13.105155944824219), ('layer2.0.bn2.weight', 12.641998291015625), ('layer1.0.bn2.weight', 11.704584121704102), ('layer3.0.downsample.1.weight', 9.855453491210938), ('layer2.1.bn2.weight', 8.355103492736816), ('layer2.1.bn1.weight', 8.251007080078125), ('layer1.0.bn1.weight', 8.050554275512695), ('layer1.1.bn2.weight', 7.666687488555908), ('layer4.1.bn1.weight', 7.451047420501709), ('layer1.1.bn1.weight', 6.965150833129883), ('layer3.1.conv1.weight', 2.616044759750366), ('layer4.0.downsample.1.weight', 1.548879623413086), ('layer3.1.conv2.weight', 1.378527283668518), ('layer4.0.bn2.weight', 0.8603057861328125), ('layer3.1.bn2.weight', 0.49764499068260193), ('layer3.1.bn1.weight', 0.4225747585296631), ('layer4.1.bn2.weight', 0.19079045951366425), ('bn1.bias', 0.0), ('layer1.0.bn1.bias', 0.0), ('layer1.0.bn2.bias', 0.0), ('layer1.1.bn1.bias', 0.0), ('layer1.1.bn2.bias', 0.0), ('layer2.0.bn1.bias', 0.0), ('layer2.0.bn2.bias', 0.0), ('layer2.0.downsample.1.bias', 0.0), ('layer2.1.bn1.bias', 0.0), ('layer2.1.bn2.bias', 0.0), ('layer3.0.bn1.bias', 0.0), ('layer3.0.bn2.bias', 0.0), ('layer3.0.downsample.1.bias', 0.0), ('layer3.1.bn1.bias', 0.0), ('layer3.1.bn2.bias', 0.0), ('layer4.0.bn1.bias', 0.0), ('layer4.0.bn2.bias', 0.0), ('layer4.0.downsample.1.bias', 0.0), ('layer4.1.bn1.bias', 0.0), ('layer4.1.bn2.bias', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('layer3.0.conv2.weight', 179.83779907226562), ('layer3.0.conv1.weight', 170.54595947265625), ('conv1.weight', 143.78366088867188), ('layer2.0.conv1.weight', 139.1171112060547), ('layer4.0.conv1.weight', 137.6432342529297), ('layer2.0.conv2.weight', 125.38401794433594), ('layer4.0.conv2.weight', 105.53776550292969), ('last_linear.weight', 104.96109771728516), ('layer2.1.conv1.weight', 79.29855346679688), ('layer1.0.conv1.weight', 74.96281433105469), ('layer2.1.conv2.weight', 56.582401275634766), ('layer1.1.conv1.weight', 56.3514404296875), ('layer1.0.conv2.weight', 45.145294189453125), ('layer4.1.conv1.weight', 41.95225524902344), ('layer1.1.conv2.weight', 41.46173858642578), ('layer4.1.conv2.weight', 33.74342346191406), ('layer3.1.conv1.weight', 2.616044759750366), ('layer3.1.conv2.weight', 1.378527283668518), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=41  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
protect_percent = 0.0 0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.043270	Acc: 37.5% (3748/10000)
[Test]  Epoch: 2	Loss: 0.042514	Acc: 39.8% (3976/10000)
[Test]  Epoch: 3	Loss: 0.042485	Acc: 40.1% (4014/10000)
[Test]  Epoch: 4	Loss: 0.042225	Acc: 41.4% (4139/10000)
[Test]  Epoch: 5	Loss: 0.043039	Acc: 41.1% (4107/10000)
[Test]  Epoch: 6	Loss: 0.042249	Acc: 41.8% (4182/10000)
[Test]  Epoch: 7	Loss: 0.042274	Acc: 41.5% (4152/10000)
[Test]  Epoch: 8	Loss: 0.042108	Acc: 42.2% (4221/10000)
[Test]  Epoch: 9	Loss: 0.042748	Acc: 41.6% (4161/10000)
[Test]  Epoch: 10	Loss: 0.042180	Acc: 42.1% (4208/10000)
[Test]  Epoch: 11	Loss: 0.042687	Acc: 42.3% (4226/10000)
[Test]  Epoch: 12	Loss: 0.042573	Acc: 42.2% (4217/10000)
[Test]  Epoch: 13	Loss: 0.042573	Acc: 41.7% (4174/10000)
[Test]  Epoch: 14	Loss: 0.042180	Acc: 42.3% (4230/10000)
[Test]  Epoch: 15	Loss: 0.042745	Acc: 42.1% (4208/10000)
[Test]  Epoch: 16	Loss: 0.042823	Acc: 41.9% (4185/10000)
[Test]  Epoch: 17	Loss: 0.042436	Acc: 42.5% (4246/10000)
[Test]  Epoch: 18	Loss: 0.042676	Acc: 42.2% (4219/10000)
[Test]  Epoch: 19	Loss: 0.042850	Acc: 42.3% (4228/10000)
[Test]  Epoch: 20	Loss: 0.042851	Acc: 42.0% (4202/10000)
[Test]  Epoch: 21	Loss: 0.042801	Acc: 42.1% (4207/10000)
[Test]  Epoch: 22	Loss: 0.043120	Acc: 42.0% (4200/10000)
[Test]  Epoch: 23	Loss: 0.042788	Acc: 42.3% (4228/10000)
[Test]  Epoch: 24	Loss: 0.042929	Acc: 41.9% (4187/10000)
[Test]  Epoch: 25	Loss: 0.042820	Acc: 42.3% (4230/10000)
[Test]  Epoch: 26	Loss: 0.043129	Acc: 41.5% (4155/10000)
[Test]  Epoch: 27	Loss: 0.042750	Acc: 42.6% (4261/10000)
[Test]  Epoch: 28	Loss: 0.043068	Acc: 42.2% (4219/10000)
[Test]  Epoch: 29	Loss: 0.043579	Acc: 41.0% (4103/10000)
[Test]  Epoch: 30	Loss: 0.043153	Acc: 42.1% (4212/10000)
[Test]  Epoch: 31	Loss: 0.043263	Acc: 41.8% (4181/10000)
[Test]  Epoch: 32	Loss: 0.043116	Acc: 42.3% (4228/10000)
[Test]  Epoch: 33	Loss: 0.043056	Acc: 42.3% (4234/10000)
[Test]  Epoch: 34	Loss: 0.043441	Acc: 41.6% (4159/10000)
[Test]  Epoch: 35	Loss: 0.043278	Acc: 41.9% (4189/10000)
[Test]  Epoch: 36	Loss: 0.043111	Acc: 42.4% (4243/10000)
[Test]  Epoch: 37	Loss: 0.043236	Acc: 42.2% (4222/10000)
[Test]  Epoch: 38	Loss: 0.043269	Acc: 42.3% (4230/10000)
[Test]  Epoch: 39	Loss: 0.043328	Acc: 42.1% (4208/10000)
[Test]  Epoch: 40	Loss: 0.043422	Acc: 42.1% (4206/10000)
[Test]  Epoch: 41	Loss: 0.043441	Acc: 41.7% (4173/10000)
[Test]  Epoch: 42	Loss: 0.043261	Acc: 42.1% (4213/10000)
[Test]  Epoch: 43	Loss: 0.043440	Acc: 41.9% (4189/10000)
[Test]  Epoch: 44	Loss: 0.043448	Acc: 42.1% (4214/10000)
[Test]  Epoch: 45	Loss: 0.043506	Acc: 42.0% (4205/10000)
[Test]  Epoch: 46	Loss: 0.043521	Acc: 41.6% (4164/10000)
[Test]  Epoch: 47	Loss: 0.043537	Acc: 42.3% (4227/10000)
[Test]  Epoch: 48	Loss: 0.043645	Acc: 42.0% (4202/10000)
[Test]  Epoch: 49	Loss: 0.043576	Acc: 41.8% (4179/10000)
[Test]  Epoch: 50	Loss: 0.043615	Acc: 41.7% (4173/10000)
[Test]  Epoch: 51	Loss: 0.043535	Acc: 42.4% (4238/10000)
[Test]  Epoch: 52	Loss: 0.043606	Acc: 42.1% (4209/10000)
[Test]  Epoch: 53	Loss: 0.043661	Acc: 42.0% (4196/10000)
[Test]  Epoch: 54	Loss: 0.043723	Acc: 42.4% (4236/10000)
[Test]  Epoch: 55	Loss: 0.043689	Acc: 41.9% (4193/10000)
[Test]  Epoch: 56	Loss: 0.043859	Acc: 42.2% (4217/10000)
[Test]  Epoch: 57	Loss: 0.043535	Acc: 42.2% (4223/10000)
[Test]  Epoch: 58	Loss: 0.043806	Acc: 42.3% (4230/10000)
[Test]  Epoch: 59	Loss: 0.043791	Acc: 42.2% (4218/10000)
[Test]  Epoch: 60	Loss: 0.043850	Acc: 41.7% (4169/10000)
[Test]  Epoch: 61	Loss: 0.043823	Acc: 41.8% (4180/10000)
[Test]  Epoch: 62	Loss: 0.043773	Acc: 42.1% (4206/10000)
[Test]  Epoch: 63	Loss: 0.043711	Acc: 42.0% (4200/10000)
[Test]  Epoch: 64	Loss: 0.043704	Acc: 42.1% (4209/10000)
[Test]  Epoch: 65	Loss: 0.043762	Acc: 42.0% (4197/10000)
[Test]  Epoch: 66	Loss: 0.043773	Acc: 42.2% (4219/10000)
[Test]  Epoch: 67	Loss: 0.043806	Acc: 42.1% (4207/10000)
[Test]  Epoch: 68	Loss: 0.043829	Acc: 41.9% (4187/10000)
[Test]  Epoch: 69	Loss: 0.043779	Acc: 42.1% (4211/10000)
[Test]  Epoch: 70	Loss: 0.043756	Acc: 42.1% (4212/10000)
[Test]  Epoch: 71	Loss: 0.043771	Acc: 42.1% (4213/10000)
[Test]  Epoch: 72	Loss: 0.043785	Acc: 42.0% (4205/10000)
[Test]  Epoch: 73	Loss: 0.043767	Acc: 42.1% (4215/10000)
[Test]  Epoch: 74	Loss: 0.043733	Acc: 42.1% (4213/10000)
[Test]  Epoch: 75	Loss: 0.043783	Acc: 42.0% (4198/10000)
[Test]  Epoch: 76	Loss: 0.043752	Acc: 42.2% (4221/10000)
[Test]  Epoch: 77	Loss: 0.043738	Acc: 42.2% (4219/10000)
[Test]  Epoch: 78	Loss: 0.043749	Acc: 42.1% (4209/10000)
[Test]  Epoch: 79	Loss: 0.043762	Acc: 42.3% (4229/10000)
[Test]  Epoch: 80	Loss: 0.043738	Acc: 42.1% (4209/10000)
[Test]  Epoch: 81	Loss: 0.043711	Acc: 42.1% (4212/10000)
[Test]  Epoch: 82	Loss: 0.043755	Acc: 42.0% (4205/10000)
[Test]  Epoch: 83	Loss: 0.043764	Acc: 42.0% (4200/10000)
[Test]  Epoch: 84	Loss: 0.043795	Acc: 42.2% (4216/10000)
[Test]  Epoch: 85	Loss: 0.043798	Acc: 42.2% (4217/10000)
[Test]  Epoch: 86	Loss: 0.043751	Acc: 42.0% (4201/10000)
[Test]  Epoch: 87	Loss: 0.043721	Acc: 42.2% (4225/10000)
[Test]  Epoch: 88	Loss: 0.043787	Acc: 42.1% (4215/10000)
[Test]  Epoch: 89	Loss: 0.043753	Acc: 42.2% (4222/10000)
[Test]  Epoch: 90	Loss: 0.043769	Acc: 42.2% (4224/10000)
[Test]  Epoch: 91	Loss: 0.043779	Acc: 42.1% (4207/10000)
[Test]  Epoch: 92	Loss: 0.043736	Acc: 42.2% (4221/10000)
[Test]  Epoch: 93	Loss: 0.043785	Acc: 42.1% (4210/10000)
[Test]  Epoch: 94	Loss: 0.043758	Acc: 42.2% (4219/10000)
[Test]  Epoch: 95	Loss: 0.043734	Acc: 42.1% (4213/10000)
[Test]  Epoch: 96	Loss: 0.043733	Acc: 42.1% (4210/10000)
[Test]  Epoch: 97	Loss: 0.043774	Acc: 42.0% (4204/10000)
[Test]  Epoch: 98	Loss: 0.043734	Acc: 42.0% (4203/10000)
[Test]  Epoch: 99	Loss: 0.043753	Acc: 42.1% (4209/10000)
[Test]  Epoch: 100	Loss: 0.043789	Acc: 42.0% (4199/10000)
===========finish==========
['2024-08-18', '18:28:35.436264', '100', 'test', '0.04378942053318024', '41.99', '42.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=4
get_sample_layers not_random
protect_percent = 0.1 4 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.062227	Acc: 13.6% (1359/10000)
[Test]  Epoch: 2	Loss: 0.055621	Acc: 22.1% (2209/10000)
[Test]  Epoch: 3	Loss: 0.053552	Acc: 25.2% (2521/10000)
[Test]  Epoch: 4	Loss: 0.054111	Acc: 24.1% (2410/10000)
[Test]  Epoch: 5	Loss: 0.053401	Acc: 25.4% (2540/10000)
[Test]  Epoch: 6	Loss: 0.053194	Acc: 25.8% (2577/10000)
[Test]  Epoch: 7	Loss: 0.052961	Acc: 26.2% (2622/10000)
[Test]  Epoch: 8	Loss: 0.052690	Acc: 27.0% (2701/10000)
[Test]  Epoch: 9	Loss: 0.052922	Acc: 26.6% (2665/10000)
[Test]  Epoch: 10	Loss: 0.052695	Acc: 26.7% (2668/10000)
[Test]  Epoch: 11	Loss: 0.052681	Acc: 27.4% (2736/10000)
[Test]  Epoch: 12	Loss: 0.052752	Acc: 26.9% (2688/10000)
[Test]  Epoch: 13	Loss: 0.052844	Acc: 26.9% (2689/10000)
[Test]  Epoch: 14	Loss: 0.052327	Acc: 28.3% (2834/10000)
[Test]  Epoch: 15	Loss: 0.052690	Acc: 27.0% (2701/10000)
[Test]  Epoch: 16	Loss: 0.052806	Acc: 27.4% (2735/10000)
[Test]  Epoch: 17	Loss: 0.052689	Acc: 27.7% (2774/10000)
[Test]  Epoch: 18	Loss: 0.052965	Acc: 27.1% (2710/10000)
[Test]  Epoch: 19	Loss: 0.052930	Acc: 27.3% (2727/10000)
[Test]  Epoch: 20	Loss: 0.052741	Acc: 27.6% (2759/10000)
[Test]  Epoch: 21	Loss: 0.052533	Acc: 27.9% (2791/10000)
[Test]  Epoch: 22	Loss: 0.052955	Acc: 27.4% (2743/10000)
[Test]  Epoch: 23	Loss: 0.052504	Acc: 28.1% (2807/10000)
[Test]  Epoch: 24	Loss: 0.052822	Acc: 27.6% (2758/10000)
[Test]  Epoch: 25	Loss: 0.052466	Acc: 28.5% (2847/10000)
[Test]  Epoch: 26	Loss: 0.052744	Acc: 28.1% (2814/10000)
[Test]  Epoch: 27	Loss: 0.052405	Acc: 28.4% (2837/10000)
[Test]  Epoch: 28	Loss: 0.052646	Acc: 27.9% (2791/10000)
[Test]  Epoch: 29	Loss: 0.053202	Acc: 27.3% (2726/10000)
[Test]  Epoch: 30	Loss: 0.052650	Acc: 27.9% (2788/10000)
[Test]  Epoch: 31	Loss: 0.052882	Acc: 27.7% (2774/10000)
[Test]  Epoch: 32	Loss: 0.052610	Acc: 28.3% (2832/10000)
[Test]  Epoch: 33	Loss: 0.052695	Acc: 28.4% (2835/10000)
[Test]  Epoch: 34	Loss: 0.052773	Acc: 28.1% (2808/10000)
[Test]  Epoch: 35	Loss: 0.052742	Acc: 28.1% (2806/10000)
[Test]  Epoch: 36	Loss: 0.052770	Acc: 28.0% (2803/10000)
[Test]  Epoch: 37	Loss: 0.052739	Acc: 28.0% (2803/10000)
[Test]  Epoch: 38	Loss: 0.052692	Acc: 28.4% (2845/10000)
[Test]  Epoch: 39	Loss: 0.052960	Acc: 28.1% (2805/10000)
[Test]  Epoch: 40	Loss: 0.052980	Acc: 27.8% (2781/10000)
[Test]  Epoch: 41	Loss: 0.052808	Acc: 28.3% (2830/10000)
[Test]  Epoch: 42	Loss: 0.052676	Acc: 28.6% (2860/10000)
[Test]  Epoch: 43	Loss: 0.052906	Acc: 27.8% (2784/10000)
[Test]  Epoch: 44	Loss: 0.053007	Acc: 28.2% (2825/10000)
[Test]  Epoch: 45	Loss: 0.052863	Acc: 28.6% (2860/10000)
[Test]  Epoch: 46	Loss: 0.052817	Acc: 28.0% (2800/10000)
[Test]  Epoch: 47	Loss: 0.053065	Acc: 28.0% (2804/10000)
[Test]  Epoch: 48	Loss: 0.052917	Acc: 28.5% (2849/10000)
[Test]  Epoch: 49	Loss: 0.053041	Acc: 28.2% (2816/10000)
[Test]  Epoch: 50	Loss: 0.053274	Acc: 27.7% (2767/10000)
[Test]  Epoch: 51	Loss: 0.052892	Acc: 28.2% (2821/10000)
[Test]  Epoch: 52	Loss: 0.052941	Acc: 28.5% (2854/10000)
[Test]  Epoch: 53	Loss: 0.053093	Acc: 27.9% (2793/10000)
[Test]  Epoch: 54	Loss: 0.053040	Acc: 28.1% (2813/10000)
[Test]  Epoch: 55	Loss: 0.052956	Acc: 28.4% (2835/10000)
[Test]  Epoch: 56	Loss: 0.052978	Acc: 28.6% (2862/10000)
[Test]  Epoch: 57	Loss: 0.052824	Acc: 28.8% (2877/10000)
[Test]  Epoch: 58	Loss: 0.053274	Acc: 27.8% (2784/10000)
[Test]  Epoch: 59	Loss: 0.053031	Acc: 28.1% (2810/10000)
[Test]  Epoch: 60	Loss: 0.053154	Acc: 28.0% (2804/10000)
[Test]  Epoch: 61	Loss: 0.053162	Acc: 27.9% (2786/10000)
[Test]  Epoch: 62	Loss: 0.053132	Acc: 28.0% (2801/10000)
[Test]  Epoch: 63	Loss: 0.053024	Acc: 28.0% (2802/10000)
[Test]  Epoch: 64	Loss: 0.053038	Acc: 28.1% (2805/10000)
[Test]  Epoch: 65	Loss: 0.053102	Acc: 28.2% (2820/10000)
[Test]  Epoch: 66	Loss: 0.053097	Acc: 28.3% (2829/10000)
[Test]  Epoch: 67	Loss: 0.053145	Acc: 28.0% (2799/10000)
[Test]  Epoch: 68	Loss: 0.053177	Acc: 28.0% (2798/10000)
[Test]  Epoch: 69	Loss: 0.053086	Acc: 28.2% (2818/10000)
[Test]  Epoch: 70	Loss: 0.053085	Acc: 28.2% (2817/10000)
[Test]  Epoch: 71	Loss: 0.053108	Acc: 28.3% (2831/10000)
[Test]  Epoch: 72	Loss: 0.053112	Acc: 28.2% (2817/10000)
[Test]  Epoch: 73	Loss: 0.053056	Acc: 28.4% (2838/10000)
[Test]  Epoch: 74	Loss: 0.053057	Acc: 28.3% (2833/10000)
[Test]  Epoch: 75	Loss: 0.053090	Acc: 28.2% (2824/10000)
[Test]  Epoch: 76	Loss: 0.053062	Acc: 28.3% (2831/10000)
[Test]  Epoch: 77	Loss: 0.053065	Acc: 28.3% (2832/10000)
[Test]  Epoch: 78	Loss: 0.053103	Acc: 28.3% (2829/10000)
[Test]  Epoch: 79	Loss: 0.053098	Acc: 28.4% (2838/10000)
[Test]  Epoch: 80	Loss: 0.053061	Acc: 28.4% (2837/10000)
[Test]  Epoch: 81	Loss: 0.053032	Acc: 28.6% (2861/10000)
[Test]  Epoch: 82	Loss: 0.053081	Acc: 28.3% (2832/10000)
[Test]  Epoch: 83	Loss: 0.053120	Acc: 28.1% (2813/10000)
[Test]  Epoch: 84	Loss: 0.053099	Acc: 28.2% (2824/10000)
[Test]  Epoch: 85	Loss: 0.053127	Acc: 28.2% (2818/10000)
[Test]  Epoch: 86	Loss: 0.053091	Acc: 28.3% (2830/10000)
[Test]  Epoch: 87	Loss: 0.053064	Acc: 28.2% (2816/10000)
[Test]  Epoch: 88	Loss: 0.053113	Acc: 28.2% (2821/10000)
[Test]  Epoch: 89	Loss: 0.053097	Acc: 28.2% (2823/10000)
[Test]  Epoch: 90	Loss: 0.053101	Acc: 28.3% (2831/10000)
[Test]  Epoch: 91	Loss: 0.053078	Acc: 28.4% (2837/10000)
[Test]  Epoch: 92	Loss: 0.053049	Acc: 28.4% (2835/10000)
[Test]  Epoch: 93	Loss: 0.053095	Acc: 28.2% (2816/10000)
[Test]  Epoch: 94	Loss: 0.053079	Acc: 28.1% (2812/10000)
[Test]  Epoch: 95	Loss: 0.053082	Acc: 28.2% (2820/10000)
[Test]  Epoch: 96	Loss: 0.053062	Acc: 28.3% (2834/10000)
[Test]  Epoch: 97	Loss: 0.053092	Acc: 28.1% (2815/10000)
[Test]  Epoch: 98	Loss: 0.053076	Acc: 28.1% (2814/10000)
[Test]  Epoch: 99	Loss: 0.053100	Acc: 28.3% (2827/10000)
[Test]  Epoch: 100	Loss: 0.053126	Acc: 28.3% (2833/10000)
===========finish==========
['2024-08-18', '18:31:07.707813', '100', 'test', '0.053126406145095825', '28.33', '28.77']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=8
get_sample_layers not_random
protect_percent = 0.2 8 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.069786	Acc: 8.7% (873/10000)
[Test]  Epoch: 2	Loss: 0.062447	Acc: 12.8% (1282/10000)
[Test]  Epoch: 3	Loss: 0.058670	Acc: 17.3% (1732/10000)
[Test]  Epoch: 4	Loss: 0.058795	Acc: 17.8% (1776/10000)
[Test]  Epoch: 5	Loss: 0.058099	Acc: 19.0% (1897/10000)
[Test]  Epoch: 6	Loss: 0.057777	Acc: 18.6% (1864/10000)
[Test]  Epoch: 7	Loss: 0.057317	Acc: 19.7% (1971/10000)
[Test]  Epoch: 8	Loss: 0.057174	Acc: 20.0% (2003/10000)
[Test]  Epoch: 9	Loss: 0.057110	Acc: 19.7% (1968/10000)
[Test]  Epoch: 10	Loss: 0.057115	Acc: 19.9% (1993/10000)
[Test]  Epoch: 11	Loss: 0.057174	Acc: 20.5% (2052/10000)
[Test]  Epoch: 12	Loss: 0.057201	Acc: 19.9% (1990/10000)
[Test]  Epoch: 13	Loss: 0.057128	Acc: 20.2% (2019/10000)
[Test]  Epoch: 14	Loss: 0.056449	Acc: 21.0% (2097/10000)
[Test]  Epoch: 15	Loss: 0.056875	Acc: 20.6% (2061/10000)
[Test]  Epoch: 16	Loss: 0.057127	Acc: 20.3% (2026/10000)
[Test]  Epoch: 17	Loss: 0.056713	Acc: 20.8% (2079/10000)
[Test]  Epoch: 18	Loss: 0.056758	Acc: 20.6% (2059/10000)
[Test]  Epoch: 19	Loss: 0.056943	Acc: 20.9% (2094/10000)
[Test]  Epoch: 20	Loss: 0.057077	Acc: 20.9% (2094/10000)
[Test]  Epoch: 21	Loss: 0.056626	Acc: 21.2% (2117/10000)
[Test]  Epoch: 22	Loss: 0.056957	Acc: 21.4% (2137/10000)
[Test]  Epoch: 23	Loss: 0.056588	Acc: 21.5% (2150/10000)
[Test]  Epoch: 24	Loss: 0.056736	Acc: 21.1% (2111/10000)
[Test]  Epoch: 25	Loss: 0.056659	Acc: 21.5% (2154/10000)
[Test]  Epoch: 26	Loss: 0.056930	Acc: 21.3% (2127/10000)
[Test]  Epoch: 27	Loss: 0.056458	Acc: 21.7% (2171/10000)
[Test]  Epoch: 28	Loss: 0.056698	Acc: 21.7% (2170/10000)
[Test]  Epoch: 29	Loss: 0.057111	Acc: 20.7% (2074/10000)
[Test]  Epoch: 30	Loss: 0.056639	Acc: 21.4% (2138/10000)
[Test]  Epoch: 31	Loss: 0.056861	Acc: 21.4% (2136/10000)
[Test]  Epoch: 32	Loss: 0.056678	Acc: 21.6% (2159/10000)
[Test]  Epoch: 33	Loss: 0.056785	Acc: 21.2% (2122/10000)
[Test]  Epoch: 34	Loss: 0.056677	Acc: 21.2% (2120/10000)
[Test]  Epoch: 35	Loss: 0.056678	Acc: 21.7% (2169/10000)
[Test]  Epoch: 36	Loss: 0.056688	Acc: 21.9% (2185/10000)
[Test]  Epoch: 37	Loss: 0.056694	Acc: 21.7% (2172/10000)
[Test]  Epoch: 38	Loss: 0.056717	Acc: 21.5% (2150/10000)
[Test]  Epoch: 39	Loss: 0.056786	Acc: 21.7% (2174/10000)
[Test]  Epoch: 40	Loss: 0.056874	Acc: 21.6% (2156/10000)
[Test]  Epoch: 41	Loss: 0.056831	Acc: 21.4% (2140/10000)
[Test]  Epoch: 42	Loss: 0.056420	Acc: 22.1% (2206/10000)
[Test]  Epoch: 43	Loss: 0.056765	Acc: 21.4% (2137/10000)
[Test]  Epoch: 44	Loss: 0.056865	Acc: 21.3% (2126/10000)
[Test]  Epoch: 45	Loss: 0.056733	Acc: 21.5% (2151/10000)
[Test]  Epoch: 46	Loss: 0.056717	Acc: 21.4% (2136/10000)
[Test]  Epoch: 47	Loss: 0.056798	Acc: 21.9% (2185/10000)
[Test]  Epoch: 48	Loss: 0.056757	Acc: 22.1% (2211/10000)
[Test]  Epoch: 49	Loss: 0.056988	Acc: 21.2% (2116/10000)
[Test]  Epoch: 50	Loss: 0.057059	Acc: 21.2% (2125/10000)
[Test]  Epoch: 51	Loss: 0.056679	Acc: 21.9% (2185/10000)
[Test]  Epoch: 52	Loss: 0.056887	Acc: 21.5% (2154/10000)
[Test]  Epoch: 53	Loss: 0.056714	Acc: 22.0% (2198/10000)
[Test]  Epoch: 54	Loss: 0.056838	Acc: 21.8% (2178/10000)
[Test]  Epoch: 55	Loss: 0.056715	Acc: 21.9% (2188/10000)
[Test]  Epoch: 56	Loss: 0.056953	Acc: 21.8% (2178/10000)
[Test]  Epoch: 57	Loss: 0.056639	Acc: 22.2% (2223/10000)
[Test]  Epoch: 58	Loss: 0.056961	Acc: 21.6% (2158/10000)
[Test]  Epoch: 59	Loss: 0.056913	Acc: 21.6% (2159/10000)
[Test]  Epoch: 60	Loss: 0.056922	Acc: 21.7% (2174/10000)
[Test]  Epoch: 61	Loss: 0.056941	Acc: 21.8% (2181/10000)
[Test]  Epoch: 62	Loss: 0.056888	Acc: 21.9% (2191/10000)
[Test]  Epoch: 63	Loss: 0.056811	Acc: 22.0% (2202/10000)
[Test]  Epoch: 64	Loss: 0.056806	Acc: 22.1% (2209/10000)
[Test]  Epoch: 65	Loss: 0.056847	Acc: 22.0% (2203/10000)
[Test]  Epoch: 66	Loss: 0.056861	Acc: 22.0% (2197/10000)
[Test]  Epoch: 67	Loss: 0.056919	Acc: 21.9% (2189/10000)
[Test]  Epoch: 68	Loss: 0.056947	Acc: 21.8% (2184/10000)
[Test]  Epoch: 69	Loss: 0.056841	Acc: 22.1% (2213/10000)
[Test]  Epoch: 70	Loss: 0.056848	Acc: 22.0% (2204/10000)
[Test]  Epoch: 71	Loss: 0.056824	Acc: 22.1% (2205/10000)
[Test]  Epoch: 72	Loss: 0.056871	Acc: 21.9% (2193/10000)
[Test]  Epoch: 73	Loss: 0.056839	Acc: 22.1% (2214/10000)
[Test]  Epoch: 74	Loss: 0.056819	Acc: 22.1% (2206/10000)
[Test]  Epoch: 75	Loss: 0.056889	Acc: 22.0% (2201/10000)
[Test]  Epoch: 76	Loss: 0.056853	Acc: 22.1% (2205/10000)
[Test]  Epoch: 77	Loss: 0.056844	Acc: 22.0% (2197/10000)
[Test]  Epoch: 78	Loss: 0.056917	Acc: 22.1% (2208/10000)
[Test]  Epoch: 79	Loss: 0.056904	Acc: 22.2% (2222/10000)
[Test]  Epoch: 80	Loss: 0.056858	Acc: 22.2% (2218/10000)
[Test]  Epoch: 81	Loss: 0.056797	Acc: 22.1% (2215/10000)
[Test]  Epoch: 82	Loss: 0.056868	Acc: 22.1% (2208/10000)
[Test]  Epoch: 83	Loss: 0.056905	Acc: 21.9% (2191/10000)
[Test]  Epoch: 84	Loss: 0.056875	Acc: 22.1% (2211/10000)
[Test]  Epoch: 85	Loss: 0.056922	Acc: 22.1% (2214/10000)
[Test]  Epoch: 86	Loss: 0.056911	Acc: 21.9% (2193/10000)
[Test]  Epoch: 87	Loss: 0.056877	Acc: 22.0% (2202/10000)
[Test]  Epoch: 88	Loss: 0.056919	Acc: 22.0% (2202/10000)
[Test]  Epoch: 89	Loss: 0.056898	Acc: 22.1% (2205/10000)
[Test]  Epoch: 90	Loss: 0.056920	Acc: 22.1% (2205/10000)
[Test]  Epoch: 91	Loss: 0.056895	Acc: 22.1% (2215/10000)
[Test]  Epoch: 92	Loss: 0.056867	Acc: 22.1% (2208/10000)
[Test]  Epoch: 93	Loss: 0.056882	Acc: 22.0% (2203/10000)
[Test]  Epoch: 94	Loss: 0.056889	Acc: 22.1% (2215/10000)
[Test]  Epoch: 95	Loss: 0.056886	Acc: 22.0% (2196/10000)
[Test]  Epoch: 96	Loss: 0.056861	Acc: 22.3% (2229/10000)
[Test]  Epoch: 97	Loss: 0.056888	Acc: 22.1% (2206/10000)
[Test]  Epoch: 98	Loss: 0.056924	Acc: 22.0% (2204/10000)
[Test]  Epoch: 99	Loss: 0.056913	Acc: 22.1% (2208/10000)
[Test]  Epoch: 100	Loss: 0.056933	Acc: 21.9% (2190/10000)
===========finish==========
['2024-08-18', '18:33:40.760393', '100', 'test', '0.05693250269889832', '21.9', '22.29']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=12
get_sample_layers not_random
protect_percent = 0.3 12 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.0.downsample.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.079839	Acc: 3.1% (306/10000)
[Test]  Epoch: 2	Loss: 0.075710	Acc: 5.9% (586/10000)
[Test]  Epoch: 3	Loss: 0.073384	Acc: 6.8% (683/10000)
[Test]  Epoch: 4	Loss: 0.071408	Acc: 8.3% (826/10000)
[Test]  Epoch: 5	Loss: 0.069333	Acc: 9.1% (909/10000)
[Test]  Epoch: 6	Loss: 0.067987	Acc: 10.3% (1027/10000)
[Test]  Epoch: 7	Loss: 0.066729	Acc: 10.5% (1051/10000)
[Test]  Epoch: 8	Loss: 0.065636	Acc: 11.9% (1191/10000)
[Test]  Epoch: 9	Loss: 0.065362	Acc: 12.1% (1210/10000)
[Test]  Epoch: 10	Loss: 0.065043	Acc: 12.0% (1196/10000)
[Test]  Epoch: 11	Loss: 0.064399	Acc: 12.9% (1290/10000)
[Test]  Epoch: 12	Loss: 0.063620	Acc: 13.2% (1322/10000)
[Test]  Epoch: 13	Loss: 0.063693	Acc: 12.7% (1274/10000)
[Test]  Epoch: 14	Loss: 0.062877	Acc: 13.7% (1372/10000)
[Test]  Epoch: 15	Loss: 0.063408	Acc: 13.5% (1351/10000)
[Test]  Epoch: 16	Loss: 0.062785	Acc: 13.6% (1364/10000)
[Test]  Epoch: 17	Loss: 0.062792	Acc: 13.9% (1388/10000)
[Test]  Epoch: 18	Loss: 0.061846	Acc: 14.3% (1428/10000)
[Test]  Epoch: 19	Loss: 0.062722	Acc: 14.0% (1397/10000)
[Test]  Epoch: 20	Loss: 0.061812	Acc: 14.9% (1489/10000)
[Test]  Epoch: 21	Loss: 0.061735	Acc: 15.1% (1508/10000)
[Test]  Epoch: 22	Loss: 0.061753	Acc: 15.2% (1521/10000)
[Test]  Epoch: 23	Loss: 0.061429	Acc: 15.4% (1538/10000)
[Test]  Epoch: 24	Loss: 0.061613	Acc: 15.1% (1509/10000)
[Test]  Epoch: 25	Loss: 0.061460	Acc: 15.3% (1533/10000)
[Test]  Epoch: 26	Loss: 0.061171	Acc: 15.8% (1575/10000)
[Test]  Epoch: 27	Loss: 0.060780	Acc: 16.0% (1597/10000)
[Test]  Epoch: 28	Loss: 0.061632	Acc: 14.6% (1462/10000)
[Test]  Epoch: 29	Loss: 0.061665	Acc: 14.7% (1469/10000)
[Test]  Epoch: 30	Loss: 0.061041	Acc: 15.7% (1572/10000)
[Test]  Epoch: 31	Loss: 0.060899	Acc: 16.1% (1606/10000)
[Test]  Epoch: 32	Loss: 0.060978	Acc: 16.0% (1601/10000)
[Test]  Epoch: 33	Loss: 0.060935	Acc: 16.0% (1599/10000)
[Test]  Epoch: 34	Loss: 0.060704	Acc: 16.3% (1633/10000)
[Test]  Epoch: 35	Loss: 0.060873	Acc: 16.3% (1632/10000)
[Test]  Epoch: 36	Loss: 0.060572	Acc: 16.0% (1603/10000)
[Test]  Epoch: 37	Loss: 0.060646	Acc: 16.1% (1615/10000)
[Test]  Epoch: 38	Loss: 0.060462	Acc: 16.4% (1637/10000)
[Test]  Epoch: 39	Loss: 0.060717	Acc: 16.3% (1628/10000)
[Test]  Epoch: 40	Loss: 0.060693	Acc: 16.2% (1618/10000)
[Test]  Epoch: 41	Loss: 0.060254	Acc: 16.7% (1666/10000)
[Test]  Epoch: 42	Loss: 0.060258	Acc: 16.9% (1686/10000)
[Test]  Epoch: 43	Loss: 0.060439	Acc: 16.4% (1637/10000)
[Test]  Epoch: 44	Loss: 0.060286	Acc: 17.0% (1697/10000)
[Test]  Epoch: 45	Loss: 0.060244	Acc: 16.4% (1637/10000)
[Test]  Epoch: 46	Loss: 0.060287	Acc: 16.6% (1665/10000)
[Test]  Epoch: 47	Loss: 0.060462	Acc: 16.4% (1640/10000)
[Test]  Epoch: 48	Loss: 0.060348	Acc: 16.9% (1687/10000)
[Test]  Epoch: 49	Loss: 0.060242	Acc: 16.6% (1658/10000)
[Test]  Epoch: 50	Loss: 0.060737	Acc: 16.4% (1638/10000)
[Test]  Epoch: 51	Loss: 0.059844	Acc: 17.1% (1715/10000)
[Test]  Epoch: 52	Loss: 0.060274	Acc: 17.0% (1701/10000)
[Test]  Epoch: 53	Loss: 0.060289	Acc: 16.9% (1685/10000)
[Test]  Epoch: 54	Loss: 0.060066	Acc: 17.3% (1731/10000)
[Test]  Epoch: 55	Loss: 0.059906	Acc: 17.4% (1743/10000)
[Test]  Epoch: 56	Loss: 0.060118	Acc: 17.1% (1706/10000)
[Test]  Epoch: 57	Loss: 0.059732	Acc: 17.8% (1781/10000)
[Test]  Epoch: 58	Loss: 0.060197	Acc: 17.0% (1697/10000)
[Test]  Epoch: 59	Loss: 0.060159	Acc: 16.9% (1685/10000)
[Test]  Epoch: 60	Loss: 0.060244	Acc: 16.9% (1685/10000)
[Test]  Epoch: 61	Loss: 0.060001	Acc: 17.4% (1735/10000)
[Test]  Epoch: 62	Loss: 0.059902	Acc: 17.5% (1750/10000)
[Test]  Epoch: 63	Loss: 0.059790	Acc: 17.5% (1753/10000)
[Test]  Epoch: 64	Loss: 0.059796	Acc: 17.6% (1756/10000)
[Test]  Epoch: 65	Loss: 0.059864	Acc: 17.4% (1745/10000)
[Test]  Epoch: 66	Loss: 0.059844	Acc: 17.6% (1759/10000)
[Test]  Epoch: 67	Loss: 0.059878	Acc: 17.5% (1746/10000)
[Test]  Epoch: 68	Loss: 0.059950	Acc: 17.4% (1735/10000)
[Test]  Epoch: 69	Loss: 0.059861	Acc: 17.5% (1752/10000)
[Test]  Epoch: 70	Loss: 0.059826	Acc: 17.6% (1755/10000)
[Test]  Epoch: 71	Loss: 0.059849	Acc: 17.3% (1729/10000)
[Test]  Epoch: 72	Loss: 0.059888	Acc: 17.4% (1741/10000)
[Test]  Epoch: 73	Loss: 0.059845	Acc: 17.3% (1731/10000)
[Test]  Epoch: 74	Loss: 0.059817	Acc: 17.4% (1744/10000)
[Test]  Epoch: 75	Loss: 0.059861	Acc: 17.4% (1742/10000)
[Test]  Epoch: 76	Loss: 0.059842	Acc: 17.6% (1756/10000)
[Test]  Epoch: 77	Loss: 0.059795	Acc: 17.4% (1744/10000)
[Test]  Epoch: 78	Loss: 0.059866	Acc: 17.3% (1731/10000)
[Test]  Epoch: 79	Loss: 0.059859	Acc: 17.5% (1750/10000)
[Test]  Epoch: 80	Loss: 0.059822	Acc: 17.5% (1753/10000)
[Test]  Epoch: 81	Loss: 0.059778	Acc: 17.6% (1756/10000)
[Test]  Epoch: 82	Loss: 0.059816	Acc: 17.5% (1746/10000)
[Test]  Epoch: 83	Loss: 0.059861	Acc: 17.4% (1737/10000)
[Test]  Epoch: 84	Loss: 0.059866	Acc: 17.6% (1757/10000)
[Test]  Epoch: 85	Loss: 0.059885	Acc: 17.5% (1748/10000)
[Test]  Epoch: 86	Loss: 0.059855	Acc: 17.4% (1743/10000)
[Test]  Epoch: 87	Loss: 0.059846	Acc: 17.4% (1743/10000)
[Test]  Epoch: 88	Loss: 0.059873	Acc: 17.4% (1738/10000)
[Test]  Epoch: 89	Loss: 0.059828	Acc: 17.5% (1747/10000)
[Test]  Epoch: 90	Loss: 0.059849	Acc: 17.5% (1747/10000)
[Test]  Epoch: 91	Loss: 0.059857	Acc: 17.4% (1741/10000)
[Test]  Epoch: 92	Loss: 0.059764	Acc: 17.4% (1743/10000)
[Test]  Epoch: 93	Loss: 0.059822	Acc: 17.3% (1734/10000)
[Test]  Epoch: 94	Loss: 0.059782	Acc: 17.4% (1742/10000)
[Test]  Epoch: 95	Loss: 0.059861	Acc: 17.2% (1722/10000)
[Test]  Epoch: 96	Loss: 0.059809	Acc: 17.3% (1728/10000)
[Test]  Epoch: 97	Loss: 0.059843	Acc: 17.3% (1730/10000)
[Test]  Epoch: 98	Loss: 0.059835	Acc: 17.3% (1726/10000)
[Test]  Epoch: 99	Loss: 0.059819	Acc: 17.4% (1742/10000)
[Test]  Epoch: 100	Loss: 0.059830	Acc: 17.4% (1738/10000)
===========finish==========
['2024-08-18', '18:36:16.457138', '100', 'test', '0.0598296656370163', '17.38', '17.81']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=16
get_sample_layers not_random
protect_percent = 0.4 16 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv2.weight', 'layer4.1.conv1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.080141	Acc: 2.5% (252/10000)
[Test]  Epoch: 2	Loss: 0.076514	Acc: 6.1% (610/10000)
[Test]  Epoch: 3	Loss: 0.073414	Acc: 7.7% (770/10000)
[Test]  Epoch: 4	Loss: 0.071221	Acc: 8.8% (883/10000)
[Test]  Epoch: 5	Loss: 0.069176	Acc: 10.2% (1017/10000)
[Test]  Epoch: 6	Loss: 0.067345	Acc: 10.9% (1095/10000)
[Test]  Epoch: 7	Loss: 0.066469	Acc: 11.3% (1128/10000)
[Test]  Epoch: 8	Loss: 0.065327	Acc: 12.0% (1199/10000)
[Test]  Epoch: 9	Loss: 0.064280	Acc: 13.4% (1337/10000)
[Test]  Epoch: 10	Loss: 0.063926	Acc: 13.5% (1349/10000)
[Test]  Epoch: 11	Loss: 0.063397	Acc: 13.6% (1364/10000)
[Test]  Epoch: 12	Loss: 0.062820	Acc: 13.9% (1386/10000)
[Test]  Epoch: 13	Loss: 0.062547	Acc: 14.0% (1399/10000)
[Test]  Epoch: 14	Loss: 0.061744	Acc: 15.1% (1512/10000)
[Test]  Epoch: 15	Loss: 0.062077	Acc: 14.2% (1416/10000)
[Test]  Epoch: 16	Loss: 0.061133	Acc: 15.6% (1559/10000)
[Test]  Epoch: 17	Loss: 0.061364	Acc: 15.3% (1530/10000)
[Test]  Epoch: 18	Loss: 0.060787	Acc: 15.5% (1550/10000)
[Test]  Epoch: 19	Loss: 0.060670	Acc: 15.8% (1581/10000)
[Test]  Epoch: 20	Loss: 0.060161	Acc: 16.6% (1658/10000)
[Test]  Epoch: 21	Loss: 0.060346	Acc: 16.1% (1615/10000)
[Test]  Epoch: 22	Loss: 0.060034	Acc: 17.2% (1720/10000)
[Test]  Epoch: 23	Loss: 0.059790	Acc: 17.1% (1711/10000)
[Test]  Epoch: 24	Loss: 0.060066	Acc: 16.9% (1687/10000)
[Test]  Epoch: 25	Loss: 0.059783	Acc: 17.2% (1717/10000)
[Test]  Epoch: 26	Loss: 0.059475	Acc: 17.6% (1762/10000)
[Test]  Epoch: 27	Loss: 0.059014	Acc: 17.8% (1784/10000)
[Test]  Epoch: 28	Loss: 0.059690	Acc: 16.8% (1680/10000)
[Test]  Epoch: 29	Loss: 0.059877	Acc: 16.8% (1680/10000)
[Test]  Epoch: 30	Loss: 0.059613	Acc: 16.8% (1676/10000)
[Test]  Epoch: 31	Loss: 0.059189	Acc: 17.6% (1762/10000)
[Test]  Epoch: 32	Loss: 0.059150	Acc: 17.9% (1786/10000)
[Test]  Epoch: 33	Loss: 0.059155	Acc: 17.9% (1787/10000)
[Test]  Epoch: 34	Loss: 0.059143	Acc: 17.6% (1764/10000)
[Test]  Epoch: 35	Loss: 0.059211	Acc: 17.7% (1773/10000)
[Test]  Epoch: 36	Loss: 0.058923	Acc: 18.2% (1819/10000)
[Test]  Epoch: 37	Loss: 0.059132	Acc: 18.1% (1807/10000)
[Test]  Epoch: 38	Loss: 0.058944	Acc: 18.0% (1802/10000)
[Test]  Epoch: 39	Loss: 0.058896	Acc: 18.0% (1802/10000)
[Test]  Epoch: 40	Loss: 0.059045	Acc: 17.8% (1784/10000)
[Test]  Epoch: 41	Loss: 0.058685	Acc: 18.3% (1827/10000)
[Test]  Epoch: 42	Loss: 0.058567	Acc: 18.6% (1856/10000)
[Test]  Epoch: 43	Loss: 0.058707	Acc: 18.0% (1800/10000)
[Test]  Epoch: 44	Loss: 0.058625	Acc: 18.9% (1889/10000)
[Test]  Epoch: 45	Loss: 0.058568	Acc: 18.6% (1858/10000)
[Test]  Epoch: 46	Loss: 0.058466	Acc: 18.6% (1856/10000)
[Test]  Epoch: 47	Loss: 0.058770	Acc: 18.4% (1837/10000)
[Test]  Epoch: 48	Loss: 0.058532	Acc: 18.6% (1865/10000)
[Test]  Epoch: 49	Loss: 0.058879	Acc: 18.0% (1801/10000)
[Test]  Epoch: 50	Loss: 0.058596	Acc: 18.5% (1848/10000)
[Test]  Epoch: 51	Loss: 0.058294	Acc: 19.0% (1904/10000)
[Test]  Epoch: 52	Loss: 0.058334	Acc: 18.7% (1874/10000)
[Test]  Epoch: 53	Loss: 0.058112	Acc: 19.1% (1910/10000)
[Test]  Epoch: 54	Loss: 0.058403	Acc: 18.7% (1869/10000)
[Test]  Epoch: 55	Loss: 0.058188	Acc: 19.3% (1928/10000)
[Test]  Epoch: 56	Loss: 0.058391	Acc: 18.8% (1883/10000)
[Test]  Epoch: 57	Loss: 0.058004	Acc: 19.4% (1944/10000)
[Test]  Epoch: 58	Loss: 0.058497	Acc: 18.6% (1865/10000)
[Test]  Epoch: 59	Loss: 0.058276	Acc: 19.0% (1900/10000)
[Test]  Epoch: 60	Loss: 0.058406	Acc: 19.2% (1925/10000)
[Test]  Epoch: 61	Loss: 0.058271	Acc: 19.3% (1931/10000)
[Test]  Epoch: 62	Loss: 0.058193	Acc: 19.4% (1943/10000)
[Test]  Epoch: 63	Loss: 0.058077	Acc: 19.6% (1958/10000)
[Test]  Epoch: 64	Loss: 0.058086	Acc: 19.5% (1947/10000)
[Test]  Epoch: 65	Loss: 0.058135	Acc: 19.4% (1939/10000)
[Test]  Epoch: 66	Loss: 0.058096	Acc: 19.4% (1942/10000)
[Test]  Epoch: 67	Loss: 0.058153	Acc: 19.4% (1943/10000)
[Test]  Epoch: 68	Loss: 0.058201	Acc: 19.1% (1913/10000)
[Test]  Epoch: 69	Loss: 0.058082	Acc: 19.5% (1949/10000)
[Test]  Epoch: 70	Loss: 0.058120	Acc: 19.3% (1932/10000)
[Test]  Epoch: 71	Loss: 0.058115	Acc: 19.3% (1927/10000)
[Test]  Epoch: 72	Loss: 0.058162	Acc: 19.2% (1922/10000)
[Test]  Epoch: 73	Loss: 0.058108	Acc: 19.3% (1931/10000)
[Test]  Epoch: 74	Loss: 0.058058	Acc: 19.2% (1923/10000)
[Test]  Epoch: 75	Loss: 0.058133	Acc: 19.4% (1935/10000)
[Test]  Epoch: 76	Loss: 0.058108	Acc: 19.2% (1925/10000)
[Test]  Epoch: 77	Loss: 0.058099	Acc: 19.4% (1936/10000)
[Test]  Epoch: 78	Loss: 0.058136	Acc: 19.4% (1940/10000)
[Test]  Epoch: 79	Loss: 0.058111	Acc: 19.4% (1936/10000)
[Test]  Epoch: 80	Loss: 0.058074	Acc: 19.2% (1923/10000)
[Test]  Epoch: 81	Loss: 0.058050	Acc: 19.4% (1945/10000)
[Test]  Epoch: 82	Loss: 0.058091	Acc: 19.3% (1929/10000)
[Test]  Epoch: 83	Loss: 0.058106	Acc: 19.3% (1929/10000)
[Test]  Epoch: 84	Loss: 0.058110	Acc: 19.4% (1939/10000)
[Test]  Epoch: 85	Loss: 0.058096	Acc: 19.4% (1936/10000)
[Test]  Epoch: 86	Loss: 0.058094	Acc: 19.4% (1935/10000)
[Test]  Epoch: 87	Loss: 0.058112	Acc: 19.2% (1924/10000)
[Test]  Epoch: 88	Loss: 0.058146	Acc: 19.3% (1933/10000)
[Test]  Epoch: 89	Loss: 0.058111	Acc: 19.4% (1936/10000)
[Test]  Epoch: 90	Loss: 0.058104	Acc: 19.2% (1921/10000)
[Test]  Epoch: 91	Loss: 0.058113	Acc: 19.3% (1933/10000)
[Test]  Epoch: 92	Loss: 0.058039	Acc: 19.4% (1943/10000)
[Test]  Epoch: 93	Loss: 0.058077	Acc: 19.2% (1923/10000)
[Test]  Epoch: 94	Loss: 0.058062	Acc: 19.3% (1927/10000)
[Test]  Epoch: 95	Loss: 0.058113	Acc: 19.3% (1934/10000)
[Test]  Epoch: 96	Loss: 0.058055	Acc: 19.3% (1929/10000)
[Test]  Epoch: 97	Loss: 0.058102	Acc: 19.3% (1926/10000)
[Test]  Epoch: 98	Loss: 0.058079	Acc: 19.5% (1949/10000)
[Test]  Epoch: 99	Loss: 0.058056	Acc: 19.4% (1938/10000)
[Test]  Epoch: 100	Loss: 0.058084	Acc: 19.3% (1929/10000)
===========finish==========
['2024-08-18', '18:38:53.194836', '100', 'test', '0.058084192037582395', '19.29', '19.58']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=20
get_sample_layers not_random
protect_percent = 0.5 20 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'bn1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.081037	Acc: 1.7% (168/10000)
[Test]  Epoch: 2	Loss: 0.077417	Acc: 4.4% (436/10000)
[Test]  Epoch: 3	Loss: 0.074426	Acc: 6.7% (672/10000)
[Test]  Epoch: 4	Loss: 0.071753	Acc: 8.3% (828/10000)
[Test]  Epoch: 5	Loss: 0.069712	Acc: 9.8% (976/10000)
[Test]  Epoch: 6	Loss: 0.067773	Acc: 10.0% (996/10000)
[Test]  Epoch: 7	Loss: 0.066302	Acc: 11.1% (1109/10000)
[Test]  Epoch: 8	Loss: 0.065186	Acc: 12.2% (1217/10000)
[Test]  Epoch: 9	Loss: 0.064589	Acc: 12.7% (1274/10000)
[Test]  Epoch: 10	Loss: 0.064031	Acc: 13.2% (1321/10000)
[Test]  Epoch: 11	Loss: 0.063139	Acc: 13.7% (1373/10000)
[Test]  Epoch: 12	Loss: 0.062802	Acc: 13.3% (1328/10000)
[Test]  Epoch: 13	Loss: 0.063041	Acc: 13.5% (1349/10000)
[Test]  Epoch: 14	Loss: 0.061798	Acc: 14.6% (1460/10000)
[Test]  Epoch: 15	Loss: 0.061841	Acc: 14.3% (1426/10000)
[Test]  Epoch: 16	Loss: 0.060800	Acc: 15.2% (1520/10000)
[Test]  Epoch: 17	Loss: 0.061126	Acc: 15.0% (1497/10000)
[Test]  Epoch: 18	Loss: 0.060250	Acc: 15.8% (1578/10000)
[Test]  Epoch: 19	Loss: 0.060162	Acc: 16.1% (1608/10000)
[Test]  Epoch: 20	Loss: 0.059915	Acc: 16.6% (1655/10000)
[Test]  Epoch: 21	Loss: 0.059620	Acc: 16.8% (1681/10000)
[Test]  Epoch: 22	Loss: 0.059694	Acc: 17.0% (1698/10000)
[Test]  Epoch: 23	Loss: 0.059508	Acc: 17.0% (1697/10000)
[Test]  Epoch: 24	Loss: 0.059338	Acc: 16.8% (1680/10000)
[Test]  Epoch: 25	Loss: 0.059071	Acc: 17.3% (1729/10000)
[Test]  Epoch: 26	Loss: 0.059358	Acc: 16.8% (1681/10000)
[Test]  Epoch: 27	Loss: 0.058464	Acc: 18.0% (1800/10000)
[Test]  Epoch: 28	Loss: 0.058868	Acc: 17.2% (1720/10000)
[Test]  Epoch: 29	Loss: 0.059192	Acc: 17.0% (1698/10000)
[Test]  Epoch: 30	Loss: 0.058712	Acc: 17.5% (1748/10000)
[Test]  Epoch: 31	Loss: 0.058959	Acc: 17.2% (1725/10000)
[Test]  Epoch: 32	Loss: 0.058594	Acc: 17.8% (1783/10000)
[Test]  Epoch: 33	Loss: 0.058907	Acc: 17.2% (1723/10000)
[Test]  Epoch: 34	Loss: 0.058840	Acc: 17.3% (1728/10000)
[Test]  Epoch: 35	Loss: 0.058511	Acc: 17.9% (1788/10000)
[Test]  Epoch: 36	Loss: 0.058402	Acc: 17.6% (1763/10000)
[Test]  Epoch: 37	Loss: 0.058543	Acc: 18.0% (1801/10000)
[Test]  Epoch: 38	Loss: 0.058248	Acc: 18.0% (1803/10000)
[Test]  Epoch: 39	Loss: 0.058187	Acc: 18.2% (1817/10000)
[Test]  Epoch: 40	Loss: 0.058448	Acc: 18.0% (1801/10000)
[Test]  Epoch: 41	Loss: 0.058356	Acc: 18.2% (1822/10000)
[Test]  Epoch: 42	Loss: 0.058055	Acc: 18.7% (1873/10000)
[Test]  Epoch: 43	Loss: 0.058129	Acc: 18.2% (1822/10000)
[Test]  Epoch: 44	Loss: 0.058259	Acc: 18.4% (1845/10000)
[Test]  Epoch: 45	Loss: 0.057829	Acc: 18.8% (1880/10000)
[Test]  Epoch: 46	Loss: 0.058134	Acc: 18.5% (1847/10000)
[Test]  Epoch: 47	Loss: 0.058021	Acc: 18.4% (1843/10000)
[Test]  Epoch: 48	Loss: 0.058066	Acc: 18.6% (1861/10000)
[Test]  Epoch: 49	Loss: 0.058080	Acc: 18.8% (1879/10000)
[Test]  Epoch: 50	Loss: 0.058166	Acc: 18.7% (1866/10000)
[Test]  Epoch: 51	Loss: 0.058110	Acc: 18.5% (1847/10000)
[Test]  Epoch: 52	Loss: 0.057873	Acc: 18.9% (1887/10000)
[Test]  Epoch: 53	Loss: 0.057817	Acc: 19.1% (1906/10000)
[Test]  Epoch: 54	Loss: 0.057711	Acc: 18.8% (1884/10000)
[Test]  Epoch: 55	Loss: 0.057799	Acc: 19.2% (1916/10000)
[Test]  Epoch: 56	Loss: 0.057615	Acc: 18.9% (1895/10000)
[Test]  Epoch: 57	Loss: 0.057307	Acc: 19.5% (1949/10000)
[Test]  Epoch: 58	Loss: 0.057904	Acc: 19.0% (1901/10000)
[Test]  Epoch: 59	Loss: 0.057626	Acc: 19.3% (1926/10000)
[Test]  Epoch: 60	Loss: 0.057756	Acc: 19.2% (1917/10000)
[Test]  Epoch: 61	Loss: 0.057624	Acc: 19.4% (1944/10000)
[Test]  Epoch: 62	Loss: 0.057565	Acc: 19.5% (1949/10000)
[Test]  Epoch: 63	Loss: 0.057440	Acc: 19.7% (1967/10000)
[Test]  Epoch: 64	Loss: 0.057488	Acc: 19.6% (1960/10000)
[Test]  Epoch: 65	Loss: 0.057521	Acc: 19.5% (1951/10000)
[Test]  Epoch: 66	Loss: 0.057509	Acc: 19.5% (1954/10000)
[Test]  Epoch: 67	Loss: 0.057558	Acc: 19.4% (1945/10000)
[Test]  Epoch: 68	Loss: 0.057580	Acc: 19.5% (1949/10000)
[Test]  Epoch: 69	Loss: 0.057443	Acc: 19.4% (1944/10000)
[Test]  Epoch: 70	Loss: 0.057459	Acc: 19.6% (1960/10000)
[Test]  Epoch: 71	Loss: 0.057486	Acc: 19.6% (1955/10000)
[Test]  Epoch: 72	Loss: 0.057527	Acc: 19.4% (1942/10000)
[Test]  Epoch: 73	Loss: 0.057516	Acc: 19.4% (1944/10000)
[Test]  Epoch: 74	Loss: 0.057441	Acc: 19.6% (1962/10000)
[Test]  Epoch: 75	Loss: 0.057469	Acc: 19.8% (1976/10000)
[Test]  Epoch: 76	Loss: 0.057475	Acc: 19.6% (1956/10000)
[Test]  Epoch: 77	Loss: 0.057462	Acc: 19.7% (1969/10000)
[Test]  Epoch: 78	Loss: 0.057499	Acc: 19.4% (1945/10000)
[Test]  Epoch: 79	Loss: 0.057476	Acc: 19.4% (1944/10000)
[Test]  Epoch: 80	Loss: 0.057446	Acc: 19.6% (1961/10000)
[Test]  Epoch: 81	Loss: 0.057418	Acc: 19.6% (1955/10000)
[Test]  Epoch: 82	Loss: 0.057477	Acc: 19.4% (1942/10000)
[Test]  Epoch: 83	Loss: 0.057496	Acc: 19.4% (1943/10000)
[Test]  Epoch: 84	Loss: 0.057502	Acc: 19.6% (1959/10000)
[Test]  Epoch: 85	Loss: 0.057496	Acc: 19.5% (1953/10000)
[Test]  Epoch: 86	Loss: 0.057475	Acc: 19.5% (1953/10000)
[Test]  Epoch: 87	Loss: 0.057479	Acc: 19.5% (1948/10000)
[Test]  Epoch: 88	Loss: 0.057515	Acc: 19.6% (1955/10000)
[Test]  Epoch: 89	Loss: 0.057491	Acc: 19.6% (1956/10000)
[Test]  Epoch: 90	Loss: 0.057477	Acc: 19.5% (1954/10000)
[Test]  Epoch: 91	Loss: 0.057486	Acc: 19.6% (1959/10000)
[Test]  Epoch: 92	Loss: 0.057436	Acc: 19.5% (1948/10000)
[Test]  Epoch: 93	Loss: 0.057459	Acc: 19.5% (1952/10000)
[Test]  Epoch: 94	Loss: 0.057458	Acc: 19.6% (1961/10000)
[Test]  Epoch: 95	Loss: 0.057491	Acc: 19.4% (1937/10000)
[Test]  Epoch: 96	Loss: 0.057464	Acc: 19.4% (1943/10000)
[Test]  Epoch: 97	Loss: 0.057453	Acc: 19.7% (1966/10000)
[Test]  Epoch: 98	Loss: 0.057488	Acc: 19.6% (1959/10000)
[Test]  Epoch: 99	Loss: 0.057418	Acc: 19.6% (1957/10000)
[Test]  Epoch: 100	Loss: 0.057469	Acc: 19.4% (1943/10000)
===========finish==========
['2024-08-18', '18:41:30.451006', '100', 'test', '0.057468765330314635', '19.43', '19.76']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=24
get_sample_layers not_random
protect_percent = 0.6 24 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.081111	Acc: 2.0% (201/10000)
[Test]  Epoch: 2	Loss: 0.077356	Acc: 4.6% (462/10000)
[Test]  Epoch: 3	Loss: 0.074168	Acc: 6.8% (680/10000)
[Test]  Epoch: 4	Loss: 0.071838	Acc: 8.4% (843/10000)
[Test]  Epoch: 5	Loss: 0.069568	Acc: 9.9% (987/10000)
[Test]  Epoch: 6	Loss: 0.067639	Acc: 10.8% (1083/10000)
[Test]  Epoch: 7	Loss: 0.066117	Acc: 11.4% (1143/10000)
[Test]  Epoch: 8	Loss: 0.064589	Acc: 13.0% (1297/10000)
[Test]  Epoch: 9	Loss: 0.064156	Acc: 13.2% (1315/10000)
[Test]  Epoch: 10	Loss: 0.063882	Acc: 13.2% (1317/10000)
[Test]  Epoch: 11	Loss: 0.062227	Acc: 14.0% (1400/10000)
[Test]  Epoch: 12	Loss: 0.062429	Acc: 13.6% (1359/10000)
[Test]  Epoch: 13	Loss: 0.062174	Acc: 14.4% (1444/10000)
[Test]  Epoch: 14	Loss: 0.060952	Acc: 15.2% (1523/10000)
[Test]  Epoch: 15	Loss: 0.061105	Acc: 14.9% (1487/10000)
[Test]  Epoch: 16	Loss: 0.060552	Acc: 15.6% (1560/10000)
[Test]  Epoch: 17	Loss: 0.060521	Acc: 15.4% (1544/10000)
[Test]  Epoch: 18	Loss: 0.060179	Acc: 16.2% (1618/10000)
[Test]  Epoch: 19	Loss: 0.060050	Acc: 16.4% (1643/10000)
[Test]  Epoch: 20	Loss: 0.059380	Acc: 16.9% (1690/10000)
[Test]  Epoch: 21	Loss: 0.059148	Acc: 17.3% (1732/10000)
[Test]  Epoch: 22	Loss: 0.059267	Acc: 17.6% (1765/10000)
[Test]  Epoch: 23	Loss: 0.058826	Acc: 17.8% (1781/10000)
[Test]  Epoch: 24	Loss: 0.059059	Acc: 17.8% (1784/10000)
[Test]  Epoch: 25	Loss: 0.058752	Acc: 17.8% (1780/10000)
[Test]  Epoch: 26	Loss: 0.058819	Acc: 17.8% (1775/10000)
[Test]  Epoch: 27	Loss: 0.058052	Acc: 18.5% (1851/10000)
[Test]  Epoch: 28	Loss: 0.058179	Acc: 18.0% (1804/10000)
[Test]  Epoch: 29	Loss: 0.058932	Acc: 17.4% (1744/10000)
[Test]  Epoch: 30	Loss: 0.058478	Acc: 17.8% (1782/10000)
[Test]  Epoch: 31	Loss: 0.058351	Acc: 18.5% (1847/10000)
[Test]  Epoch: 32	Loss: 0.058100	Acc: 18.7% (1872/10000)
[Test]  Epoch: 33	Loss: 0.058341	Acc: 18.2% (1824/10000)
[Test]  Epoch: 34	Loss: 0.058439	Acc: 18.2% (1818/10000)
[Test]  Epoch: 35	Loss: 0.057819	Acc: 19.2% (1916/10000)
[Test]  Epoch: 36	Loss: 0.057883	Acc: 18.6% (1858/10000)
[Test]  Epoch: 37	Loss: 0.058016	Acc: 19.0% (1896/10000)
[Test]  Epoch: 38	Loss: 0.057674	Acc: 19.6% (1958/10000)
[Test]  Epoch: 39	Loss: 0.057893	Acc: 19.2% (1924/10000)
[Test]  Epoch: 40	Loss: 0.057894	Acc: 18.9% (1895/10000)
[Test]  Epoch: 41	Loss: 0.057445	Acc: 19.6% (1955/10000)
[Test]  Epoch: 42	Loss: 0.057406	Acc: 19.9% (1994/10000)
[Test]  Epoch: 43	Loss: 0.057426	Acc: 19.6% (1961/10000)
[Test]  Epoch: 44	Loss: 0.057355	Acc: 19.6% (1960/10000)
[Test]  Epoch: 45	Loss: 0.057691	Acc: 19.4% (1943/10000)
[Test]  Epoch: 46	Loss: 0.057492	Acc: 19.4% (1943/10000)
[Test]  Epoch: 47	Loss: 0.057367	Acc: 19.9% (1994/10000)
[Test]  Epoch: 48	Loss: 0.057256	Acc: 20.2% (2019/10000)
[Test]  Epoch: 49	Loss: 0.057537	Acc: 19.7% (1970/10000)
[Test]  Epoch: 50	Loss: 0.057567	Acc: 19.6% (1965/10000)
[Test]  Epoch: 51	Loss: 0.057112	Acc: 20.0% (2004/10000)
[Test]  Epoch: 52	Loss: 0.057170	Acc: 20.0% (2000/10000)
[Test]  Epoch: 53	Loss: 0.057111	Acc: 20.5% (2046/10000)
[Test]  Epoch: 54	Loss: 0.057049	Acc: 20.3% (2026/10000)
[Test]  Epoch: 55	Loss: 0.057471	Acc: 19.8% (1983/10000)
[Test]  Epoch: 56	Loss: 0.056978	Acc: 20.3% (2029/10000)
[Test]  Epoch: 57	Loss: 0.056705	Acc: 20.9% (2090/10000)
[Test]  Epoch: 58	Loss: 0.057182	Acc: 19.9% (1992/10000)
[Test]  Epoch: 59	Loss: 0.056921	Acc: 20.7% (2067/10000)
[Test]  Epoch: 60	Loss: 0.057022	Acc: 20.5% (2049/10000)
[Test]  Epoch: 61	Loss: 0.057003	Acc: 20.5% (2049/10000)
[Test]  Epoch: 62	Loss: 0.056955	Acc: 20.5% (2054/10000)
[Test]  Epoch: 63	Loss: 0.056827	Acc: 20.6% (2063/10000)
[Test]  Epoch: 64	Loss: 0.056827	Acc: 20.6% (2056/10000)
[Test]  Epoch: 65	Loss: 0.056894	Acc: 20.6% (2064/10000)
[Test]  Epoch: 66	Loss: 0.056871	Acc: 20.7% (2072/10000)
[Test]  Epoch: 67	Loss: 0.056907	Acc: 20.7% (2071/10000)
[Test]  Epoch: 68	Loss: 0.056943	Acc: 20.6% (2064/10000)
[Test]  Epoch: 69	Loss: 0.056837	Acc: 20.7% (2071/10000)
[Test]  Epoch: 70	Loss: 0.056864	Acc: 20.8% (2084/10000)
[Test]  Epoch: 71	Loss: 0.056870	Acc: 20.6% (2061/10000)
[Test]  Epoch: 72	Loss: 0.056917	Acc: 20.5% (2052/10000)
[Test]  Epoch: 73	Loss: 0.056859	Acc: 20.7% (2072/10000)
[Test]  Epoch: 74	Loss: 0.056828	Acc: 20.7% (2072/10000)
[Test]  Epoch: 75	Loss: 0.056876	Acc: 20.6% (2064/10000)
[Test]  Epoch: 76	Loss: 0.056844	Acc: 20.8% (2075/10000)
[Test]  Epoch: 77	Loss: 0.056810	Acc: 20.7% (2070/10000)
[Test]  Epoch: 78	Loss: 0.056902	Acc: 20.6% (2065/10000)
[Test]  Epoch: 79	Loss: 0.056870	Acc: 20.6% (2059/10000)
[Test]  Epoch: 80	Loss: 0.056830	Acc: 20.8% (2075/10000)
[Test]  Epoch: 81	Loss: 0.056801	Acc: 20.8% (2080/10000)
[Test]  Epoch: 82	Loss: 0.056875	Acc: 20.7% (2072/10000)
[Test]  Epoch: 83	Loss: 0.056892	Acc: 20.6% (2065/10000)
[Test]  Epoch: 84	Loss: 0.056880	Acc: 20.8% (2084/10000)
[Test]  Epoch: 85	Loss: 0.056875	Acc: 20.8% (2080/10000)
[Test]  Epoch: 86	Loss: 0.056811	Acc: 20.8% (2077/10000)
[Test]  Epoch: 87	Loss: 0.056850	Acc: 20.7% (2066/10000)
[Test]  Epoch: 88	Loss: 0.056875	Acc: 20.9% (2089/10000)
[Test]  Epoch: 89	Loss: 0.056843	Acc: 20.6% (2064/10000)
[Test]  Epoch: 90	Loss: 0.056840	Acc: 20.8% (2075/10000)
[Test]  Epoch: 91	Loss: 0.056867	Acc: 20.7% (2068/10000)
[Test]  Epoch: 92	Loss: 0.056806	Acc: 20.7% (2073/10000)
[Test]  Epoch: 93	Loss: 0.056859	Acc: 20.7% (2071/10000)
[Test]  Epoch: 94	Loss: 0.056824	Acc: 20.7% (2068/10000)
[Test]  Epoch: 95	Loss: 0.056923	Acc: 20.6% (2063/10000)
[Test]  Epoch: 96	Loss: 0.056847	Acc: 20.8% (2080/10000)
[Test]  Epoch: 97	Loss: 0.056848	Acc: 20.7% (2067/10000)
[Test]  Epoch: 98	Loss: 0.056849	Acc: 20.7% (2066/10000)
[Test]  Epoch: 99	Loss: 0.056820	Acc: 20.7% (2071/10000)
[Test]  Epoch: 100	Loss: 0.056845	Acc: 20.7% (2071/10000)
===========finish==========
['2024-08-18', '18:44:08.402174', '100', 'test', '0.05684518346786499', '20.71', '20.9']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=28
get_sample_layers not_random
protect_percent = 0.7 28 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.0.downsample.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.081251	Acc: 2.0% (198/10000)
[Test]  Epoch: 2	Loss: 0.077280	Acc: 5.0% (500/10000)
[Test]  Epoch: 3	Loss: 0.073895	Acc: 7.3% (732/10000)
[Test]  Epoch: 4	Loss: 0.071363	Acc: 9.1% (909/10000)
[Test]  Epoch: 5	Loss: 0.068767	Acc: 10.7% (1070/10000)
[Test]  Epoch: 6	Loss: 0.067195	Acc: 10.9% (1087/10000)
[Test]  Epoch: 7	Loss: 0.065285	Acc: 12.5% (1246/10000)
[Test]  Epoch: 8	Loss: 0.063927	Acc: 13.4% (1343/10000)
[Test]  Epoch: 9	Loss: 0.063557	Acc: 13.5% (1351/10000)
[Test]  Epoch: 10	Loss: 0.062420	Acc: 14.7% (1472/10000)
[Test]  Epoch: 11	Loss: 0.061685	Acc: 15.1% (1506/10000)
[Test]  Epoch: 12	Loss: 0.062003	Acc: 14.4% (1443/10000)
[Test]  Epoch: 13	Loss: 0.061050	Acc: 15.1% (1514/10000)
[Test]  Epoch: 14	Loss: 0.060050	Acc: 16.3% (1631/10000)
[Test]  Epoch: 15	Loss: 0.060226	Acc: 15.8% (1583/10000)
[Test]  Epoch: 16	Loss: 0.059557	Acc: 16.7% (1674/10000)
[Test]  Epoch: 17	Loss: 0.060154	Acc: 16.0% (1599/10000)
[Test]  Epoch: 18	Loss: 0.059154	Acc: 17.4% (1741/10000)
[Test]  Epoch: 19	Loss: 0.059404	Acc: 17.4% (1744/10000)
[Test]  Epoch: 20	Loss: 0.058538	Acc: 18.4% (1839/10000)
[Test]  Epoch: 21	Loss: 0.058857	Acc: 17.6% (1762/10000)
[Test]  Epoch: 22	Loss: 0.058397	Acc: 18.6% (1865/10000)
[Test]  Epoch: 23	Loss: 0.058192	Acc: 18.7% (1866/10000)
[Test]  Epoch: 24	Loss: 0.058651	Acc: 18.2% (1821/10000)
[Test]  Epoch: 25	Loss: 0.057922	Acc: 18.9% (1891/10000)
[Test]  Epoch: 26	Loss: 0.058069	Acc: 19.4% (1941/10000)
[Test]  Epoch: 27	Loss: 0.057731	Acc: 19.3% (1931/10000)
[Test]  Epoch: 28	Loss: 0.057767	Acc: 18.8% (1881/10000)
[Test]  Epoch: 29	Loss: 0.058210	Acc: 18.9% (1890/10000)
[Test]  Epoch: 30	Loss: 0.057824	Acc: 19.0% (1900/10000)
[Test]  Epoch: 31	Loss: 0.057726	Acc: 19.6% (1956/10000)
[Test]  Epoch: 32	Loss: 0.057627	Acc: 19.2% (1917/10000)
[Test]  Epoch: 33	Loss: 0.057423	Acc: 20.0% (1997/10000)
[Test]  Epoch: 34	Loss: 0.057392	Acc: 19.7% (1971/10000)
[Test]  Epoch: 35	Loss: 0.057335	Acc: 19.4% (1940/10000)
[Test]  Epoch: 36	Loss: 0.057021	Acc: 20.2% (2025/10000)
[Test]  Epoch: 37	Loss: 0.057351	Acc: 19.8% (1977/10000)
[Test]  Epoch: 38	Loss: 0.056752	Acc: 20.9% (2086/10000)
[Test]  Epoch: 39	Loss: 0.057394	Acc: 20.0% (2000/10000)
[Test]  Epoch: 40	Loss: 0.057438	Acc: 19.6% (1963/10000)
[Test]  Epoch: 41	Loss: 0.057080	Acc: 20.4% (2042/10000)
[Test]  Epoch: 42	Loss: 0.057071	Acc: 20.4% (2038/10000)
[Test]  Epoch: 43	Loss: 0.057106	Acc: 20.3% (2029/10000)
[Test]  Epoch: 44	Loss: 0.057175	Acc: 20.0% (2001/10000)
[Test]  Epoch: 45	Loss: 0.057007	Acc: 20.7% (2074/10000)
[Test]  Epoch: 46	Loss: 0.056870	Acc: 20.4% (2040/10000)
[Test]  Epoch: 47	Loss: 0.056947	Acc: 20.4% (2040/10000)
[Test]  Epoch: 48	Loss: 0.056559	Acc: 20.9% (2093/10000)
[Test]  Epoch: 49	Loss: 0.056769	Acc: 20.7% (2070/10000)
[Test]  Epoch: 50	Loss: 0.056991	Acc: 20.6% (2057/10000)
[Test]  Epoch: 51	Loss: 0.056864	Acc: 20.5% (2053/10000)
[Test]  Epoch: 52	Loss: 0.056409	Acc: 21.6% (2160/10000)
[Test]  Epoch: 53	Loss: 0.056572	Acc: 21.3% (2133/10000)
[Test]  Epoch: 54	Loss: 0.056548	Acc: 21.4% (2142/10000)
[Test]  Epoch: 55	Loss: 0.056837	Acc: 21.1% (2107/10000)
[Test]  Epoch: 56	Loss: 0.056384	Acc: 21.2% (2121/10000)
[Test]  Epoch: 57	Loss: 0.056400	Acc: 21.7% (2166/10000)
[Test]  Epoch: 58	Loss: 0.056389	Acc: 21.6% (2155/10000)
[Test]  Epoch: 59	Loss: 0.056741	Acc: 21.3% (2132/10000)
[Test]  Epoch: 60	Loss: 0.056416	Acc: 21.3% (2131/10000)
[Test]  Epoch: 61	Loss: 0.056425	Acc: 21.7% (2166/10000)
[Test]  Epoch: 62	Loss: 0.056354	Acc: 21.9% (2185/10000)
[Test]  Epoch: 63	Loss: 0.056288	Acc: 21.9% (2193/10000)
[Test]  Epoch: 64	Loss: 0.056276	Acc: 21.9% (2195/10000)
[Test]  Epoch: 65	Loss: 0.056325	Acc: 21.8% (2181/10000)
[Test]  Epoch: 66	Loss: 0.056286	Acc: 22.0% (2202/10000)
[Test]  Epoch: 67	Loss: 0.056384	Acc: 21.8% (2182/10000)
[Test]  Epoch: 68	Loss: 0.056400	Acc: 21.8% (2176/10000)
[Test]  Epoch: 69	Loss: 0.056288	Acc: 22.0% (2196/10000)
[Test]  Epoch: 70	Loss: 0.056313	Acc: 21.9% (2193/10000)
[Test]  Epoch: 71	Loss: 0.056333	Acc: 21.8% (2175/10000)
[Test]  Epoch: 72	Loss: 0.056388	Acc: 21.8% (2179/10000)
[Test]  Epoch: 73	Loss: 0.056351	Acc: 21.9% (2192/10000)
[Test]  Epoch: 74	Loss: 0.056286	Acc: 21.9% (2191/10000)
[Test]  Epoch: 75	Loss: 0.056337	Acc: 21.9% (2186/10000)
[Test]  Epoch: 76	Loss: 0.056285	Acc: 22.2% (2219/10000)
[Test]  Epoch: 77	Loss: 0.056280	Acc: 22.1% (2210/10000)
[Test]  Epoch: 78	Loss: 0.056354	Acc: 21.8% (2175/10000)
[Test]  Epoch: 79	Loss: 0.056304	Acc: 22.1% (2211/10000)
[Test]  Epoch: 80	Loss: 0.056300	Acc: 22.0% (2197/10000)
[Test]  Epoch: 81	Loss: 0.056278	Acc: 21.9% (2186/10000)
[Test]  Epoch: 82	Loss: 0.056352	Acc: 21.9% (2195/10000)
[Test]  Epoch: 83	Loss: 0.056358	Acc: 21.8% (2184/10000)
[Test]  Epoch: 84	Loss: 0.056320	Acc: 22.1% (2205/10000)
[Test]  Epoch: 85	Loss: 0.056319	Acc: 22.1% (2206/10000)
[Test]  Epoch: 86	Loss: 0.056284	Acc: 22.0% (2201/10000)
[Test]  Epoch: 87	Loss: 0.056291	Acc: 21.9% (2194/10000)
[Test]  Epoch: 88	Loss: 0.056362	Acc: 21.9% (2195/10000)
[Test]  Epoch: 89	Loss: 0.056335	Acc: 21.9% (2191/10000)
[Test]  Epoch: 90	Loss: 0.056297	Acc: 22.1% (2214/10000)
[Test]  Epoch: 91	Loss: 0.056298	Acc: 22.1% (2210/10000)
[Test]  Epoch: 92	Loss: 0.056245	Acc: 22.1% (2205/10000)
[Test]  Epoch: 93	Loss: 0.056296	Acc: 21.8% (2183/10000)
[Test]  Epoch: 94	Loss: 0.056264	Acc: 22.1% (2206/10000)
[Test]  Epoch: 95	Loss: 0.056337	Acc: 21.9% (2195/10000)
[Test]  Epoch: 96	Loss: 0.056287	Acc: 21.9% (2190/10000)
[Test]  Epoch: 97	Loss: 0.056323	Acc: 22.0% (2204/10000)
[Test]  Epoch: 98	Loss: 0.056300	Acc: 22.0% (2197/10000)
[Test]  Epoch: 99	Loss: 0.056290	Acc: 22.1% (2205/10000)
[Test]  Epoch: 100	Loss: 0.056266	Acc: 22.2% (2220/10000)
===========finish==========
['2024-08-18', '18:46:44.521651', '100', 'test', '0.05626578333377838', '22.2', '22.2']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=32
get_sample_layers not_random
protect_percent = 0.8 32 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer2.1.bn1.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.082044	Acc: 1.3% (131/10000)
[Test]  Epoch: 2	Loss: 0.077587	Acc: 4.2% (422/10000)
[Test]  Epoch: 3	Loss: 0.073600	Acc: 7.4% (737/10000)
[Test]  Epoch: 4	Loss: 0.070999	Acc: 9.6% (962/10000)
[Test]  Epoch: 5	Loss: 0.068457	Acc: 10.7% (1069/10000)
[Test]  Epoch: 6	Loss: 0.066695	Acc: 12.2% (1216/10000)
[Test]  Epoch: 7	Loss: 0.065325	Acc: 12.9% (1291/10000)
[Test]  Epoch: 8	Loss: 0.063667	Acc: 14.2% (1425/10000)
[Test]  Epoch: 9	Loss: 0.063261	Acc: 14.4% (1445/10000)
[Test]  Epoch: 10	Loss: 0.062239	Acc: 15.2% (1517/10000)
[Test]  Epoch: 11	Loss: 0.061583	Acc: 15.7% (1565/10000)
[Test]  Epoch: 12	Loss: 0.061122	Acc: 16.1% (1612/10000)
[Test]  Epoch: 13	Loss: 0.060851	Acc: 16.2% (1622/10000)
[Test]  Epoch: 14	Loss: 0.059771	Acc: 17.2% (1725/10000)
[Test]  Epoch: 15	Loss: 0.059967	Acc: 16.3% (1627/10000)
[Test]  Epoch: 16	Loss: 0.059525	Acc: 17.0% (1703/10000)
[Test]  Epoch: 17	Loss: 0.059143	Acc: 17.4% (1736/10000)
[Test]  Epoch: 18	Loss: 0.059123	Acc: 17.7% (1773/10000)
[Test]  Epoch: 19	Loss: 0.059282	Acc: 17.4% (1737/10000)
[Test]  Epoch: 20	Loss: 0.058649	Acc: 18.0% (1796/10000)
[Test]  Epoch: 21	Loss: 0.058415	Acc: 18.2% (1820/10000)
[Test]  Epoch: 22	Loss: 0.058486	Acc: 18.5% (1849/10000)
[Test]  Epoch: 23	Loss: 0.058279	Acc: 18.1% (1813/10000)
[Test]  Epoch: 24	Loss: 0.058348	Acc: 18.5% (1850/10000)
[Test]  Epoch: 25	Loss: 0.057993	Acc: 19.1% (1912/10000)
[Test]  Epoch: 26	Loss: 0.057876	Acc: 18.9% (1891/10000)
[Test]  Epoch: 27	Loss: 0.057253	Acc: 19.6% (1961/10000)
[Test]  Epoch: 28	Loss: 0.057546	Acc: 18.8% (1884/10000)
[Test]  Epoch: 29	Loss: 0.057692	Acc: 19.0% (1897/10000)
[Test]  Epoch: 30	Loss: 0.057455	Acc: 19.4% (1944/10000)
[Test]  Epoch: 31	Loss: 0.057312	Acc: 19.2% (1923/10000)
[Test]  Epoch: 32	Loss: 0.057410	Acc: 19.3% (1934/10000)
[Test]  Epoch: 33	Loss: 0.057279	Acc: 19.9% (1988/10000)
[Test]  Epoch: 34	Loss: 0.057254	Acc: 19.8% (1980/10000)
[Test]  Epoch: 35	Loss: 0.057233	Acc: 19.8% (1984/10000)
[Test]  Epoch: 36	Loss: 0.057056	Acc: 19.9% (1987/10000)
[Test]  Epoch: 37	Loss: 0.056972	Acc: 20.1% (2013/10000)
[Test]  Epoch: 38	Loss: 0.057063	Acc: 20.0% (1998/10000)
[Test]  Epoch: 39	Loss: 0.056824	Acc: 20.2% (2019/10000)
[Test]  Epoch: 40	Loss: 0.057045	Acc: 19.8% (1981/10000)
[Test]  Epoch: 41	Loss: 0.056631	Acc: 20.3% (2028/10000)
[Test]  Epoch: 42	Loss: 0.056803	Acc: 20.4% (2036/10000)
[Test]  Epoch: 43	Loss: 0.056546	Acc: 20.6% (2064/10000)
[Test]  Epoch: 44	Loss: 0.056906	Acc: 20.2% (2022/10000)
[Test]  Epoch: 45	Loss: 0.056597	Acc: 20.3% (2033/10000)
[Test]  Epoch: 46	Loss: 0.056399	Acc: 20.6% (2063/10000)
[Test]  Epoch: 47	Loss: 0.056807	Acc: 20.4% (2043/10000)
[Test]  Epoch: 48	Loss: 0.056521	Acc: 20.9% (2094/10000)
[Test]  Epoch: 49	Loss: 0.056494	Acc: 20.7% (2070/10000)
[Test]  Epoch: 50	Loss: 0.056782	Acc: 20.5% (2049/10000)
[Test]  Epoch: 51	Loss: 0.056503	Acc: 21.1% (2113/10000)
[Test]  Epoch: 52	Loss: 0.056394	Acc: 21.3% (2133/10000)
[Test]  Epoch: 53	Loss: 0.056412	Acc: 21.1% (2112/10000)
[Test]  Epoch: 54	Loss: 0.056322	Acc: 20.8% (2083/10000)
[Test]  Epoch: 55	Loss: 0.056462	Acc: 21.4% (2137/10000)
[Test]  Epoch: 56	Loss: 0.056212	Acc: 21.4% (2144/10000)
[Test]  Epoch: 57	Loss: 0.056109	Acc: 21.4% (2138/10000)
[Test]  Epoch: 58	Loss: 0.056217	Acc: 21.2% (2121/10000)
[Test]  Epoch: 59	Loss: 0.056151	Acc: 21.6% (2155/10000)
[Test]  Epoch: 60	Loss: 0.056001	Acc: 21.7% (2166/10000)
[Test]  Epoch: 61	Loss: 0.056212	Acc: 21.5% (2148/10000)
[Test]  Epoch: 62	Loss: 0.056206	Acc: 21.6% (2159/10000)
[Test]  Epoch: 63	Loss: 0.056088	Acc: 21.8% (2179/10000)
[Test]  Epoch: 64	Loss: 0.056064	Acc: 21.8% (2177/10000)
[Test]  Epoch: 65	Loss: 0.056121	Acc: 21.5% (2153/10000)
[Test]  Epoch: 66	Loss: 0.056108	Acc: 21.9% (2185/10000)
[Test]  Epoch: 67	Loss: 0.056173	Acc: 21.6% (2164/10000)
[Test]  Epoch: 68	Loss: 0.056189	Acc: 21.5% (2149/10000)
[Test]  Epoch: 69	Loss: 0.056052	Acc: 21.8% (2175/10000)
[Test]  Epoch: 70	Loss: 0.056049	Acc: 21.9% (2186/10000)
[Test]  Epoch: 71	Loss: 0.056091	Acc: 21.7% (2173/10000)
[Test]  Epoch: 72	Loss: 0.056165	Acc: 21.6% (2160/10000)
[Test]  Epoch: 73	Loss: 0.056109	Acc: 21.6% (2163/10000)
[Test]  Epoch: 74	Loss: 0.056041	Acc: 21.8% (2179/10000)
[Test]  Epoch: 75	Loss: 0.056090	Acc: 21.8% (2178/10000)
[Test]  Epoch: 76	Loss: 0.056076	Acc: 21.7% (2166/10000)
[Test]  Epoch: 77	Loss: 0.056065	Acc: 21.7% (2169/10000)
[Test]  Epoch: 78	Loss: 0.056099	Acc: 21.7% (2166/10000)
[Test]  Epoch: 79	Loss: 0.056052	Acc: 21.7% (2174/10000)
[Test]  Epoch: 80	Loss: 0.056062	Acc: 21.7% (2169/10000)
[Test]  Epoch: 81	Loss: 0.056042	Acc: 21.7% (2172/10000)
[Test]  Epoch: 82	Loss: 0.056104	Acc: 21.7% (2170/10000)
[Test]  Epoch: 83	Loss: 0.056134	Acc: 21.7% (2169/10000)
[Test]  Epoch: 84	Loss: 0.056090	Acc: 21.8% (2181/10000)
[Test]  Epoch: 85	Loss: 0.056100	Acc: 21.8% (2182/10000)
[Test]  Epoch: 86	Loss: 0.056056	Acc: 21.6% (2164/10000)
[Test]  Epoch: 87	Loss: 0.056072	Acc: 21.6% (2165/10000)
[Test]  Epoch: 88	Loss: 0.056125	Acc: 21.7% (2173/10000)
[Test]  Epoch: 89	Loss: 0.056071	Acc: 21.8% (2175/10000)
[Test]  Epoch: 90	Loss: 0.056073	Acc: 21.9% (2185/10000)
[Test]  Epoch: 91	Loss: 0.056072	Acc: 21.8% (2177/10000)
[Test]  Epoch: 92	Loss: 0.056014	Acc: 21.8% (2178/10000)
[Test]  Epoch: 93	Loss: 0.056087	Acc: 21.7% (2166/10000)
[Test]  Epoch: 94	Loss: 0.056065	Acc: 21.7% (2170/10000)
[Test]  Epoch: 95	Loss: 0.056091	Acc: 21.9% (2186/10000)
[Test]  Epoch: 96	Loss: 0.056031	Acc: 21.8% (2184/10000)
[Test]  Epoch: 97	Loss: 0.056111	Acc: 21.7% (2168/10000)
[Test]  Epoch: 98	Loss: 0.056058	Acc: 21.8% (2181/10000)
[Test]  Epoch: 99	Loss: 0.056035	Acc: 21.8% (2182/10000)
[Test]  Epoch: 100	Loss: 0.056054	Acc: 21.9% (2193/10000)
===========finish==========
['2024-08-18', '18:49:13.937826', '100', 'test', '0.056053570199012755', '21.93', '21.93']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=36
get_sample_layers not_random
protect_percent = 0.9 36 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer2.1.bn1.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.1.conv1.weight', 'layer4.0.downsample.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.081964	Acc: 1.5% (153/10000)
[Test]  Epoch: 2	Loss: 0.078251	Acc: 4.4% (442/10000)
[Test]  Epoch: 3	Loss: 0.074862	Acc: 6.4% (641/10000)
[Test]  Epoch: 4	Loss: 0.072315	Acc: 8.7% (869/10000)
[Test]  Epoch: 5	Loss: 0.069840	Acc: 10.2% (1016/10000)
[Test]  Epoch: 6	Loss: 0.068241	Acc: 11.3% (1126/10000)
[Test]  Epoch: 7	Loss: 0.066603	Acc: 12.2% (1216/10000)
[Test]  Epoch: 8	Loss: 0.064881	Acc: 13.4% (1337/10000)
[Test]  Epoch: 9	Loss: 0.063979	Acc: 14.4% (1444/10000)
[Test]  Epoch: 10	Loss: 0.063324	Acc: 14.7% (1474/10000)
[Test]  Epoch: 11	Loss: 0.062237	Acc: 15.5% (1548/10000)
[Test]  Epoch: 12	Loss: 0.061784	Acc: 15.6% (1562/10000)
[Test]  Epoch: 13	Loss: 0.061095	Acc: 16.5% (1648/10000)
[Test]  Epoch: 14	Loss: 0.060617	Acc: 16.7% (1670/10000)
[Test]  Epoch: 15	Loss: 0.060503	Acc: 16.4% (1638/10000)
[Test]  Epoch: 16	Loss: 0.059944	Acc: 16.7% (1667/10000)
[Test]  Epoch: 17	Loss: 0.059861	Acc: 17.0% (1700/10000)
[Test]  Epoch: 18	Loss: 0.059558	Acc: 17.2% (1721/10000)
[Test]  Epoch: 19	Loss: 0.059082	Acc: 17.9% (1786/10000)
[Test]  Epoch: 20	Loss: 0.059258	Acc: 17.9% (1789/10000)
[Test]  Epoch: 21	Loss: 0.058725	Acc: 18.0% (1804/10000)
[Test]  Epoch: 22	Loss: 0.058710	Acc: 18.4% (1843/10000)
[Test]  Epoch: 23	Loss: 0.058440	Acc: 18.3% (1828/10000)
[Test]  Epoch: 24	Loss: 0.058513	Acc: 18.1% (1811/10000)
[Test]  Epoch: 25	Loss: 0.058285	Acc: 18.8% (1879/10000)
[Test]  Epoch: 26	Loss: 0.057674	Acc: 19.3% (1929/10000)
[Test]  Epoch: 27	Loss: 0.057841	Acc: 18.9% (1887/10000)
[Test]  Epoch: 28	Loss: 0.057800	Acc: 19.1% (1908/10000)
[Test]  Epoch: 29	Loss: 0.057981	Acc: 18.8% (1876/10000)
[Test]  Epoch: 30	Loss: 0.057369	Acc: 19.1% (1913/10000)
[Test]  Epoch: 31	Loss: 0.057601	Acc: 19.5% (1948/10000)
[Test]  Epoch: 32	Loss: 0.057485	Acc: 19.3% (1928/10000)
[Test]  Epoch: 33	Loss: 0.057572	Acc: 19.4% (1938/10000)
[Test]  Epoch: 34	Loss: 0.057532	Acc: 19.4% (1937/10000)
[Test]  Epoch: 35	Loss: 0.057483	Acc: 19.6% (1965/10000)
[Test]  Epoch: 36	Loss: 0.056652	Acc: 20.1% (2014/10000)
[Test]  Epoch: 37	Loss: 0.057189	Acc: 19.8% (1984/10000)
[Test]  Epoch: 38	Loss: 0.057032	Acc: 20.0% (2002/10000)
[Test]  Epoch: 39	Loss: 0.057094	Acc: 19.8% (1981/10000)
[Test]  Epoch: 40	Loss: 0.057222	Acc: 19.6% (1960/10000)
[Test]  Epoch: 41	Loss: 0.056781	Acc: 20.1% (2008/10000)
[Test]  Epoch: 42	Loss: 0.056868	Acc: 20.6% (2060/10000)
[Test]  Epoch: 43	Loss: 0.056697	Acc: 20.3% (2033/10000)
[Test]  Epoch: 44	Loss: 0.056819	Acc: 20.2% (2025/10000)
[Test]  Epoch: 45	Loss: 0.056703	Acc: 20.2% (2022/10000)
[Test]  Epoch: 46	Loss: 0.056696	Acc: 20.4% (2042/10000)
[Test]  Epoch: 47	Loss: 0.056887	Acc: 20.5% (2049/10000)
[Test]  Epoch: 48	Loss: 0.056582	Acc: 20.8% (2083/10000)
[Test]  Epoch: 49	Loss: 0.056563	Acc: 20.7% (2066/10000)
[Test]  Epoch: 50	Loss: 0.056656	Acc: 20.7% (2069/10000)
[Test]  Epoch: 51	Loss: 0.056548	Acc: 20.7% (2072/10000)
[Test]  Epoch: 52	Loss: 0.056600	Acc: 20.8% (2077/10000)
[Test]  Epoch: 53	Loss: 0.056389	Acc: 21.1% (2109/10000)
[Test]  Epoch: 54	Loss: 0.056465	Acc: 20.9% (2090/10000)
[Test]  Epoch: 55	Loss: 0.056646	Acc: 20.9% (2093/10000)
[Test]  Epoch: 56	Loss: 0.056285	Acc: 21.2% (2119/10000)
[Test]  Epoch: 57	Loss: 0.056339	Acc: 21.5% (2152/10000)
[Test]  Epoch: 58	Loss: 0.056393	Acc: 21.1% (2114/10000)
[Test]  Epoch: 59	Loss: 0.056332	Acc: 21.0% (2101/10000)
[Test]  Epoch: 60	Loss: 0.056189	Acc: 21.3% (2133/10000)
[Test]  Epoch: 61	Loss: 0.056300	Acc: 21.2% (2117/10000)
[Test]  Epoch: 62	Loss: 0.056286	Acc: 21.2% (2119/10000)
[Test]  Epoch: 63	Loss: 0.056185	Acc: 21.4% (2144/10000)
[Test]  Epoch: 64	Loss: 0.056186	Acc: 21.6% (2158/10000)
[Test]  Epoch: 65	Loss: 0.056227	Acc: 21.6% (2158/10000)
[Test]  Epoch: 66	Loss: 0.056252	Acc: 21.6% (2159/10000)
[Test]  Epoch: 67	Loss: 0.056282	Acc: 21.4% (2137/10000)
[Test]  Epoch: 68	Loss: 0.056337	Acc: 21.5% (2147/10000)
[Test]  Epoch: 69	Loss: 0.056205	Acc: 21.5% (2152/10000)
[Test]  Epoch: 70	Loss: 0.056207	Acc: 21.6% (2164/10000)
[Test]  Epoch: 71	Loss: 0.056219	Acc: 21.6% (2158/10000)
[Test]  Epoch: 72	Loss: 0.056321	Acc: 21.4% (2142/10000)
[Test]  Epoch: 73	Loss: 0.056225	Acc: 21.6% (2160/10000)
[Test]  Epoch: 74	Loss: 0.056168	Acc: 21.6% (2161/10000)
[Test]  Epoch: 75	Loss: 0.056213	Acc: 21.8% (2177/10000)
[Test]  Epoch: 76	Loss: 0.056203	Acc: 21.7% (2174/10000)
[Test]  Epoch: 77	Loss: 0.056178	Acc: 21.7% (2166/10000)
[Test]  Epoch: 78	Loss: 0.056233	Acc: 21.7% (2174/10000)
[Test]  Epoch: 79	Loss: 0.056183	Acc: 21.7% (2171/10000)
[Test]  Epoch: 80	Loss: 0.056165	Acc: 21.6% (2165/10000)
[Test]  Epoch: 81	Loss: 0.056181	Acc: 21.7% (2168/10000)
[Test]  Epoch: 82	Loss: 0.056236	Acc: 21.7% (2168/10000)
[Test]  Epoch: 83	Loss: 0.056238	Acc: 21.7% (2174/10000)
[Test]  Epoch: 84	Loss: 0.056222	Acc: 21.7% (2167/10000)
[Test]  Epoch: 85	Loss: 0.056246	Acc: 21.6% (2155/10000)
[Test]  Epoch: 86	Loss: 0.056182	Acc: 21.6% (2159/10000)
[Test]  Epoch: 87	Loss: 0.056223	Acc: 21.7% (2174/10000)
[Test]  Epoch: 88	Loss: 0.056258	Acc: 21.5% (2154/10000)
[Test]  Epoch: 89	Loss: 0.056181	Acc: 21.5% (2153/10000)
[Test]  Epoch: 90	Loss: 0.056207	Acc: 21.6% (2160/10000)
[Test]  Epoch: 91	Loss: 0.056209	Acc: 21.6% (2158/10000)
[Test]  Epoch: 92	Loss: 0.056149	Acc: 21.6% (2165/10000)
[Test]  Epoch: 93	Loss: 0.056229	Acc: 21.5% (2154/10000)
[Test]  Epoch: 94	Loss: 0.056182	Acc: 21.6% (2163/10000)
[Test]  Epoch: 95	Loss: 0.056203	Acc: 21.6% (2156/10000)
[Test]  Epoch: 96	Loss: 0.056138	Acc: 21.6% (2162/10000)
[Test]  Epoch: 97	Loss: 0.056190	Acc: 21.6% (2157/10000)
[Test]  Epoch: 98	Loss: 0.056188	Acc: 21.7% (2170/10000)
[Test]  Epoch: 99	Loss: 0.056174	Acc: 21.6% (2160/10000)
[Test]  Epoch: 100	Loss: 0.056181	Acc: 21.8% (2183/10000)
===========finish==========
['2024-08-18', '18:51:44.640864', '100', 'test', '0.05618089406490326', '21.83', '21.83']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-resnet18-channel resnet18 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-resnet18 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=41  num_select=41
get_sample_layers not_random
protect_percent = 1.0 41 ['layer3.0.conv2.weight', 'layer3.0.conv1.weight', 'conv1.weight', 'layer2.0.conv1.weight', 'layer4.0.conv1.weight', 'layer2.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.0.conv2.weight', 'last_linear.weight', 'layer2.1.conv1.weight', 'layer1.0.conv1.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv2.weight', 'layer1.1.conv1.weight', 'layer1.0.conv2.weight', 'layer4.1.conv1.weight', 'layer1.1.conv2.weight', 'layer3.0.downsample.0.weight', 'layer4.1.conv2.weight', 'bn1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn2.weight', 'layer4.0.bn1.weight', 'layer2.0.downsample.1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn2.weight', 'layer1.0.bn2.weight', 'layer3.0.downsample.1.weight', 'layer2.1.bn2.weight', 'layer2.1.bn1.weight', 'layer1.0.bn1.weight', 'layer1.1.bn2.weight', 'layer4.1.bn1.weight', 'layer1.1.bn1.weight', 'layer3.1.conv1.weight', 'layer4.0.downsample.1.weight', 'layer3.1.conv2.weight', 'layer4.0.bn2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn1.weight', 'layer4.1.bn2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.086141	Acc: 3.7% (374/10000)
[Test]  Epoch: 2	Loss: 0.065993	Acc: 10.5% (1047/10000)
[Test]  Epoch: 3	Loss: 0.061724	Acc: 13.7% (1366/10000)
[Test]  Epoch: 4	Loss: 0.060669	Acc: 15.3% (1531/10000)
[Test]  Epoch: 5	Loss: 0.060236	Acc: 15.5% (1548/10000)
[Test]  Epoch: 6	Loss: 0.058948	Acc: 16.9% (1685/10000)
[Test]  Epoch: 7	Loss: 0.058837	Acc: 17.6% (1758/10000)
[Test]  Epoch: 8	Loss: 0.058518	Acc: 17.4% (1736/10000)
[Test]  Epoch: 9	Loss: 0.058155	Acc: 17.4% (1741/10000)
[Test]  Epoch: 10	Loss: 0.058053	Acc: 18.1% (1806/10000)
[Test]  Epoch: 11	Loss: 0.057525	Acc: 19.0% (1899/10000)
[Test]  Epoch: 12	Loss: 0.057738	Acc: 18.8% (1884/10000)
[Test]  Epoch: 13	Loss: 0.057150	Acc: 19.1% (1908/10000)
[Test]  Epoch: 14	Loss: 0.057068	Acc: 19.7% (1973/10000)
[Test]  Epoch: 15	Loss: 0.057050	Acc: 19.5% (1954/10000)
[Test]  Epoch: 16	Loss: 0.057137	Acc: 19.2% (1920/10000)
[Test]  Epoch: 17	Loss: 0.057246	Acc: 19.3% (1928/10000)
[Test]  Epoch: 18	Loss: 0.056751	Acc: 19.9% (1987/10000)
[Test]  Epoch: 19	Loss: 0.056995	Acc: 20.2% (2019/10000)
[Test]  Epoch: 20	Loss: 0.056588	Acc: 20.6% (2059/10000)
[Test]  Epoch: 21	Loss: 0.056496	Acc: 20.1% (2006/10000)
[Test]  Epoch: 22	Loss: 0.056825	Acc: 20.0% (1998/10000)
[Test]  Epoch: 23	Loss: 0.056526	Acc: 20.7% (2071/10000)
[Test]  Epoch: 24	Loss: 0.056585	Acc: 20.4% (2044/10000)
[Test]  Epoch: 25	Loss: 0.056349	Acc: 20.9% (2088/10000)
[Test]  Epoch: 26	Loss: 0.056392	Acc: 20.6% (2062/10000)
[Test]  Epoch: 27	Loss: 0.056478	Acc: 20.8% (2082/10000)
[Test]  Epoch: 28	Loss: 0.056327	Acc: 20.7% (2071/10000)
[Test]  Epoch: 29	Loss: 0.056277	Acc: 21.1% (2112/10000)
[Test]  Epoch: 30	Loss: 0.056375	Acc: 20.4% (2045/10000)
[Test]  Epoch: 31	Loss: 0.056390	Acc: 21.5% (2152/10000)
[Test]  Epoch: 32	Loss: 0.056045	Acc: 21.2% (2125/10000)
[Test]  Epoch: 33	Loss: 0.056356	Acc: 20.9% (2091/10000)
[Test]  Epoch: 34	Loss: 0.056178	Acc: 21.2% (2125/10000)
[Test]  Epoch: 35	Loss: 0.056094	Acc: 21.7% (2170/10000)
[Test]  Epoch: 36	Loss: 0.056409	Acc: 20.8% (2076/10000)
[Test]  Epoch: 37	Loss: 0.056087	Acc: 21.6% (2164/10000)
[Test]  Epoch: 38	Loss: 0.055848	Acc: 22.0% (2204/10000)
[Test]  Epoch: 39	Loss: 0.056094	Acc: 21.5% (2154/10000)
[Test]  Epoch: 40	Loss: 0.056321	Acc: 21.3% (2134/10000)
[Test]  Epoch: 41	Loss: 0.055985	Acc: 21.9% (2195/10000)
[Test]  Epoch: 42	Loss: 0.055869	Acc: 21.9% (2194/10000)
[Test]  Epoch: 43	Loss: 0.056203	Acc: 21.3% (2132/10000)
[Test]  Epoch: 44	Loss: 0.056192	Acc: 21.6% (2159/10000)
[Test]  Epoch: 45	Loss: 0.055935	Acc: 21.9% (2186/10000)
[Test]  Epoch: 46	Loss: 0.055923	Acc: 21.9% (2194/10000)
[Test]  Epoch: 47	Loss: 0.056167	Acc: 21.8% (2178/10000)
[Test]  Epoch: 48	Loss: 0.056138	Acc: 22.1% (2206/10000)
[Test]  Epoch: 49	Loss: 0.055926	Acc: 22.1% (2212/10000)
[Test]  Epoch: 50	Loss: 0.056153	Acc: 21.6% (2157/10000)
[Test]  Epoch: 51	Loss: 0.055944	Acc: 21.9% (2195/10000)
[Test]  Epoch: 52	Loss: 0.056148	Acc: 22.1% (2210/10000)
[Test]  Epoch: 53	Loss: 0.055957	Acc: 22.5% (2250/10000)
[Test]  Epoch: 54	Loss: 0.056147	Acc: 22.1% (2212/10000)
[Test]  Epoch: 55	Loss: 0.055980	Acc: 22.4% (2240/10000)
[Test]  Epoch: 56	Loss: 0.055949	Acc: 22.6% (2263/10000)
[Test]  Epoch: 57	Loss: 0.055803	Acc: 22.4% (2243/10000)
[Test]  Epoch: 58	Loss: 0.056154	Acc: 22.2% (2224/10000)
[Test]  Epoch: 59	Loss: 0.056048	Acc: 22.1% (2215/10000)
[Test]  Epoch: 60	Loss: 0.056027	Acc: 21.9% (2188/10000)
[Test]  Epoch: 61	Loss: 0.056028	Acc: 22.4% (2242/10000)
[Test]  Epoch: 62	Loss: 0.055979	Acc: 22.6% (2255/10000)
[Test]  Epoch: 63	Loss: 0.055887	Acc: 22.8% (2275/10000)
[Test]  Epoch: 64	Loss: 0.055907	Acc: 22.8% (2280/10000)
[Test]  Epoch: 65	Loss: 0.055911	Acc: 22.7% (2273/10000)
[Test]  Epoch: 66	Loss: 0.055926	Acc: 22.8% (2279/10000)
[Test]  Epoch: 67	Loss: 0.055946	Acc: 22.7% (2267/10000)
[Test]  Epoch: 68	Loss: 0.055963	Acc: 22.9% (2291/10000)
[Test]  Epoch: 69	Loss: 0.055901	Acc: 22.9% (2287/10000)
[Test]  Epoch: 70	Loss: 0.055890	Acc: 22.8% (2284/10000)
[Test]  Epoch: 71	Loss: 0.055889	Acc: 23.0% (2298/10000)
[Test]  Epoch: 72	Loss: 0.055954	Acc: 22.9% (2292/10000)
[Test]  Epoch: 73	Loss: 0.055915	Acc: 22.9% (2294/10000)
[Test]  Epoch: 74	Loss: 0.055858	Acc: 22.8% (2278/10000)
[Test]  Epoch: 75	Loss: 0.055881	Acc: 22.7% (2271/10000)
[Test]  Epoch: 76	Loss: 0.055858	Acc: 22.8% (2276/10000)
[Test]  Epoch: 77	Loss: 0.055872	Acc: 22.8% (2284/10000)
[Test]  Epoch: 78	Loss: 0.055890	Acc: 22.6% (2256/10000)
[Test]  Epoch: 79	Loss: 0.055855	Acc: 22.9% (2293/10000)
[Test]  Epoch: 80	Loss: 0.055842	Acc: 22.9% (2287/10000)
[Test]  Epoch: 81	Loss: 0.055806	Acc: 22.8% (2281/10000)
[Test]  Epoch: 82	Loss: 0.055889	Acc: 22.8% (2277/10000)
[Test]  Epoch: 83	Loss: 0.055913	Acc: 22.7% (2271/10000)
[Test]  Epoch: 84	Loss: 0.055902	Acc: 22.9% (2286/10000)
[Test]  Epoch: 85	Loss: 0.055950	Acc: 22.8% (2279/10000)
[Test]  Epoch: 86	Loss: 0.055876	Acc: 22.7% (2274/10000)
[Test]  Epoch: 87	Loss: 0.055895	Acc: 22.8% (2276/10000)
[Test]  Epoch: 88	Loss: 0.055927	Acc: 22.9% (2286/10000)
[Test]  Epoch: 89	Loss: 0.055877	Acc: 22.7% (2272/10000)
[Test]  Epoch: 90	Loss: 0.055888	Acc: 22.7% (2272/10000)
[Test]  Epoch: 91	Loss: 0.055913	Acc: 22.9% (2288/10000)
[Test]  Epoch: 92	Loss: 0.055842	Acc: 22.8% (2281/10000)
[Test]  Epoch: 93	Loss: 0.055878	Acc: 22.7% (2266/10000)
[Test]  Epoch: 94	Loss: 0.055883	Acc: 22.7% (2271/10000)
[Test]  Epoch: 95	Loss: 0.055921	Acc: 22.9% (2295/10000)
[Test]  Epoch: 96	Loss: 0.055877	Acc: 22.9% (2286/10000)
[Test]  Epoch: 97	Loss: 0.055917	Acc: 22.8% (2278/10000)
[Test]  Epoch: 98	Loss: 0.055885	Acc: 22.8% (2277/10000)
[Test]  Epoch: 99	Loss: 0.055869	Acc: 22.6% (2263/10000)
[Test]  Epoch: 100	Loss: 0.055869	Acc: 22.9% (2286/10000)
===========finish==========
['2024-08-18', '18:54:16.110551', '100', 'test', '0.05586922781467438', '22.86', '22.98']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-vgg16_bn/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('features.0.weight', nan), ('features.0.bias', 0.0), ('features.1.weight', nan), ('features.1.bias', 0.0), ('features.3.weight', nan), ('features.3.bias', 0.0), ('features.4.weight', nan), ('features.4.bias', 0.0), ('features.7.weight', nan), ('features.7.bias', 0.0), ('features.8.weight', nan), ('features.8.bias', 0.0), ('features.10.weight', nan), ('features.10.bias', 0.0), ('features.11.weight', nan), ('features.11.bias', 0.0), ('features.14.weight', nan), ('features.14.bias', 0.0), ('features.15.weight', nan), ('features.15.bias', 0.0), ('features.17.weight', nan), ('features.17.bias', 0.0), ('features.18.weight', nan), ('features.18.bias', 0.0), ('features.20.weight', nan), ('features.20.bias', 0.0), ('features.21.weight', nan), ('features.21.bias', 0.0), ('features.24.weight', nan), ('features.24.bias', 0.0), ('features.25.weight', nan), ('features.25.bias', 0.0), ('features.27.weight', nan), ('features.27.bias', 0.0), ('features.28.weight', nan), ('features.28.bias', 0.0), ('features.30.weight', nan), ('features.30.bias', 0.0), ('features.31.weight', nan), ('features.31.bias', 0.0), ('features.34.weight', nan), ('features.34.bias', 0.0), ('features.35.weight', nan), ('features.35.bias', 0.0), ('features.37.weight', nan), ('features.37.bias', 0.0), ('features.38.weight', nan), ('features.38.bias', 0.0), ('features.40.weight', nan), ('features.40.bias', 0.0), ('features.41.weight', nan), ('features.41.bias', 0.0), ('classifier.weight', nan), ('classifier.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('features.0.weight', nan), ('features.0.bias', 0.0), ('features.1.weight', nan), ('features.1.bias', 0.0), ('features.3.weight', nan), ('features.3.bias', 0.0), ('features.4.weight', nan), ('features.4.bias', 0.0), ('features.7.weight', nan), ('features.7.bias', 0.0), ('features.8.weight', nan), ('features.8.bias', 0.0), ('features.10.weight', nan), ('features.10.bias', 0.0), ('features.11.weight', nan), ('features.11.bias', 0.0), ('features.14.weight', nan), ('features.14.bias', 0.0), ('features.15.weight', nan), ('features.15.bias', 0.0), ('features.17.weight', nan), ('features.17.bias', 0.0), ('features.18.weight', nan), ('features.18.bias', 0.0), ('features.20.weight', nan), ('features.20.bias', 0.0), ('features.21.weight', nan), ('features.21.bias', 0.0), ('features.24.weight', nan), ('features.24.bias', 0.0), ('features.25.weight', nan), ('features.25.bias', 0.0), ('features.27.weight', nan), ('features.27.bias', 0.0), ('features.28.weight', nan), ('features.28.bias', 0.0), ('features.30.weight', nan), ('features.30.bias', 0.0), ('features.31.weight', nan), ('features.31.bias', 0.0), ('features.34.weight', nan), ('features.34.bias', 0.0), ('features.35.weight', nan), ('features.35.bias', 0.0), ('features.37.weight', nan), ('features.37.bias', 0.0), ('features.38.weight', nan), ('features.38.bias', 0.0), ('features.40.weight', nan), ('features.40.bias', 0.0), ('features.41.weight', nan), ('features.41.bias', 0.0), ('classifier.weight', nan), ('classifier.bias', 0.0)]
--------------------------------------------
get_sample_layers n=27  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.090715	Acc: 5.8% (580/10000)
[Test]  Epoch: 2	Loss: 0.079608	Acc: 8.1% (813/10000)
[Test]  Epoch: 3	Loss: 0.064916	Acc: 13.3% (1330/10000)
[Test]  Epoch: 4	Loss: 0.067778	Acc: 13.1% (1313/10000)
[Test]  Epoch: 5	Loss: 0.066599	Acc: 12.4% (1241/10000)
[Test]  Epoch: 6	Loss: 0.060128	Acc: 17.1% (1714/10000)
[Test]  Epoch: 7	Loss: 0.065161	Acc: 14.5% (1447/10000)
[Test]  Epoch: 8	Loss: 0.063455	Acc: 15.7% (1568/10000)
[Test]  Epoch: 9	Loss: 0.061645	Acc: 16.4% (1637/10000)
[Test]  Epoch: 10	Loss: 0.062763	Acc: 15.2% (1523/10000)
[Test]  Epoch: 11	Loss: 0.058884	Acc: 18.5% (1852/10000)
[Test]  Epoch: 12	Loss: 0.058900	Acc: 19.0% (1898/10000)
[Test]  Epoch: 13	Loss: 0.059155	Acc: 18.3% (1832/10000)
[Test]  Epoch: 14	Loss: 0.061297	Acc: 17.1% (1708/10000)
[Test]  Epoch: 15	Loss: 0.058461	Acc: 18.7% (1867/10000)
[Test]  Epoch: 16	Loss: 0.059703	Acc: 18.3% (1826/10000)
[Test]  Epoch: 17	Loss: 0.061168	Acc: 16.7% (1673/10000)
[Test]  Epoch: 18	Loss: 0.058169	Acc: 18.8% (1876/10000)
[Test]  Epoch: 19	Loss: 0.058157	Acc: 19.5% (1947/10000)
[Test]  Epoch: 20	Loss: 0.058633	Acc: 19.0% (1897/10000)
[Test]  Epoch: 21	Loss: 0.057228	Acc: 20.0% (2000/10000)
[Test]  Epoch: 22	Loss: 0.059122	Acc: 18.1% (1805/10000)
[Test]  Epoch: 23	Loss: 0.058446	Acc: 19.1% (1906/10000)
[Test]  Epoch: 24	Loss: 0.058003	Acc: 18.7% (1873/10000)
[Test]  Epoch: 25	Loss: 0.056109	Acc: 21.0% (2104/10000)
[Test]  Epoch: 26	Loss: 0.058457	Acc: 19.5% (1947/10000)
[Test]  Epoch: 27	Loss: 0.059114	Acc: 18.4% (1843/10000)
[Test]  Epoch: 28	Loss: 0.058452	Acc: 19.1% (1913/10000)
[Test]  Epoch: 29	Loss: 0.058624	Acc: 19.5% (1952/10000)
[Test]  Epoch: 30	Loss: 0.059066	Acc: 18.5% (1850/10000)
[Test]  Epoch: 31	Loss: 0.058322	Acc: 18.2% (1825/10000)
[Test]  Epoch: 32	Loss: 0.057217	Acc: 19.9% (1990/10000)
[Test]  Epoch: 33	Loss: 0.059833	Acc: 18.0% (1800/10000)
[Test]  Epoch: 34	Loss: 0.058032	Acc: 19.1% (1915/10000)
[Test]  Epoch: 35	Loss: 0.059146	Acc: 18.1% (1815/10000)
[Test]  Epoch: 36	Loss: 0.057948	Acc: 19.5% (1952/10000)
[Test]  Epoch: 37	Loss: 0.058539	Acc: 18.7% (1868/10000)
[Test]  Epoch: 38	Loss: 0.058644	Acc: 18.8% (1883/10000)
[Test]  Epoch: 39	Loss: 0.057082	Acc: 20.0% (2002/10000)
[Test]  Epoch: 40	Loss: 0.057016	Acc: 20.1% (2006/10000)
[Test]  Epoch: 41	Loss: 0.056334	Acc: 20.2% (2024/10000)
[Test]  Epoch: 42	Loss: 0.056942	Acc: 19.8% (1980/10000)
[Test]  Epoch: 43	Loss: 0.056463	Acc: 20.5% (2054/10000)
[Test]  Epoch: 44	Loss: 0.056978	Acc: 20.3% (2030/10000)
[Test]  Epoch: 45	Loss: 0.057226	Acc: 20.7% (2068/10000)
[Test]  Epoch: 46	Loss: 0.058049	Acc: 19.2% (1920/10000)
[Test]  Epoch: 47	Loss: 0.056798	Acc: 20.6% (2055/10000)
[Test]  Epoch: 48	Loss: 0.057131	Acc: 20.4% (2038/10000)
[Test]  Epoch: 49	Loss: 0.056752	Acc: 20.2% (2019/10000)
[Test]  Epoch: 50	Loss: 0.056800	Acc: 20.2% (2021/10000)
[Test]  Epoch: 51	Loss: 0.056195	Acc: 21.2% (2125/10000)
[Test]  Epoch: 52	Loss: 0.056944	Acc: 20.2% (2025/10000)
[Test]  Epoch: 53	Loss: 0.056883	Acc: 20.6% (2058/10000)
[Test]  Epoch: 54	Loss: 0.056889	Acc: 20.2% (2018/10000)
[Test]  Epoch: 55	Loss: 0.057441	Acc: 20.1% (2011/10000)
[Test]  Epoch: 56	Loss: 0.057161	Acc: 20.5% (2048/10000)
[Test]  Epoch: 57	Loss: 0.056724	Acc: 21.0% (2096/10000)
[Test]  Epoch: 58	Loss: 0.059361	Acc: 18.1% (1809/10000)
[Test]  Epoch: 59	Loss: 0.057586	Acc: 19.1% (1906/10000)
[Test]  Epoch: 60	Loss: 0.058035	Acc: 19.4% (1938/10000)
[Test]  Epoch: 61	Loss: 0.055834	Acc: 21.5% (2150/10000)
[Test]  Epoch: 62	Loss: 0.055245	Acc: 21.7% (2169/10000)
[Test]  Epoch: 63	Loss: 0.055211	Acc: 21.9% (2195/10000)
[Test]  Epoch: 64	Loss: 0.054757	Acc: 22.3% (2230/10000)
[Test]  Epoch: 65	Loss: 0.054571	Acc: 22.4% (2243/10000)
[Test]  Epoch: 66	Loss: 0.054674	Acc: 22.7% (2271/10000)
[Test]  Epoch: 67	Loss: 0.054778	Acc: 22.3% (2227/10000)
[Test]  Epoch: 68	Loss: 0.054711	Acc: 22.8% (2279/10000)
[Test]  Epoch: 69	Loss: 0.054663	Acc: 22.5% (2247/10000)
[Test]  Epoch: 70	Loss: 0.054169	Acc: 23.2% (2324/10000)
[Test]  Epoch: 71	Loss: 0.054405	Acc: 22.9% (2289/10000)
[Test]  Epoch: 72	Loss: 0.054199	Acc: 23.7% (2371/10000)
[Test]  Epoch: 73	Loss: 0.054351	Acc: 23.2% (2319/10000)
[Test]  Epoch: 74	Loss: 0.054411	Acc: 23.1% (2308/10000)
[Test]  Epoch: 75	Loss: 0.054108	Acc: 23.5% (2353/10000)
[Test]  Epoch: 76	Loss: 0.054536	Acc: 23.6% (2356/10000)
[Test]  Epoch: 77	Loss: 0.054363	Acc: 23.0% (2301/10000)
[Test]  Epoch: 78	Loss: 0.054275	Acc: 23.2% (2316/10000)
[Test]  Epoch: 79	Loss: 0.054380	Acc: 23.0% (2299/10000)
[Test]  Epoch: 80	Loss: 0.053871	Acc: 23.4% (2341/10000)
[Test]  Epoch: 81	Loss: 0.054439	Acc: 23.0% (2296/10000)
[Test]  Epoch: 82	Loss: 0.054120	Acc: 23.6% (2361/10000)
[Test]  Epoch: 83	Loss: 0.054189	Acc: 23.2% (2324/10000)
[Test]  Epoch: 84	Loss: 0.054240	Acc: 23.4% (2335/10000)
[Test]  Epoch: 85	Loss: 0.054097	Acc: 23.6% (2362/10000)
[Test]  Epoch: 86	Loss: 0.054308	Acc: 23.1% (2309/10000)
[Test]  Epoch: 87	Loss: 0.054164	Acc: 23.0% (2303/10000)
[Test]  Epoch: 88	Loss: 0.053861	Acc: 23.4% (2342/10000)
[Test]  Epoch: 89	Loss: 0.054037	Acc: 23.3% (2334/10000)
[Test]  Epoch: 90	Loss: 0.054213	Acc: 23.2% (2316/10000)
[Test]  Epoch: 91	Loss: 0.054162	Acc: 23.8% (2377/10000)
[Test]  Epoch: 92	Loss: 0.054141	Acc: 23.2% (2319/10000)
[Test]  Epoch: 93	Loss: 0.054418	Acc: 23.6% (2355/10000)
[Test]  Epoch: 94	Loss: 0.054329	Acc: 23.5% (2354/10000)
[Test]  Epoch: 95	Loss: 0.054291	Acc: 23.5% (2346/10000)
[Test]  Epoch: 96	Loss: 0.054178	Acc: 23.5% (2349/10000)
[Test]  Epoch: 97	Loss: 0.053951	Acc: 23.5% (2348/10000)
[Test]  Epoch: 98	Loss: 0.054303	Acc: 23.4% (2342/10000)
[Test]  Epoch: 99	Loss: 0.054121	Acc: 23.5% (2349/10000)
[Test]  Epoch: 100	Loss: 0.054168	Acc: 23.3% (2333/10000)
===========finish==========
['2024-08-18', '18:59:39.680117', '100', 'test', '0.05416817088127136', '23.33', '23.77']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.1 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=2
get_sample_layers not_random
2 ['features.0.weight', 'features.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.183828	Acc: 1.5% (150/10000)
[Test]  Epoch: 2	Loss: 0.079946	Acc: 2.9% (287/10000)
[Test]  Epoch: 3	Loss: 0.080007	Acc: 3.0% (303/10000)
[Test]  Epoch: 4	Loss: 0.078706	Acc: 3.7% (373/10000)
[Test]  Epoch: 5	Loss: 0.076699	Acc: 5.0% (498/10000)
[Test]  Epoch: 6	Loss: 0.077562	Acc: 5.4% (539/10000)
[Test]  Epoch: 7	Loss: 0.074407	Acc: 6.1% (612/10000)
[Test]  Epoch: 8	Loss: 0.070849	Acc: 9.0% (900/10000)
[Test]  Epoch: 9	Loss: 0.072796	Acc: 8.1% (814/10000)
[Test]  Epoch: 10	Loss: 0.070540	Acc: 9.5% (954/10000)
[Test]  Epoch: 11	Loss: 0.067285	Acc: 12.1% (1210/10000)
[Test]  Epoch: 12	Loss: 0.074258	Acc: 9.0% (904/10000)
[Test]  Epoch: 13	Loss: 0.072002	Acc: 9.5% (949/10000)
[Test]  Epoch: 14	Loss: 0.064471	Acc: 13.8% (1383/10000)
[Test]  Epoch: 15	Loss: 0.066879	Acc: 11.8% (1181/10000)
[Test]  Epoch: 16	Loss: 0.064165	Acc: 14.0% (1401/10000)
[Test]  Epoch: 17	Loss: 0.063529	Acc: 14.5% (1452/10000)
[Test]  Epoch: 18	Loss: 0.062787	Acc: 14.9% (1487/10000)
[Test]  Epoch: 19	Loss: 0.065173	Acc: 14.0% (1400/10000)
[Test]  Epoch: 20	Loss: 0.063548	Acc: 14.7% (1468/10000)
[Test]  Epoch: 21	Loss: 0.064859	Acc: 13.5% (1348/10000)
[Test]  Epoch: 22	Loss: 0.063627	Acc: 13.5% (1351/10000)
[Test]  Epoch: 23	Loss: 0.061260	Acc: 16.6% (1665/10000)
[Test]  Epoch: 24	Loss: 0.062209	Acc: 16.0% (1597/10000)
[Test]  Epoch: 25	Loss: 0.060671	Acc: 16.9% (1687/10000)
[Test]  Epoch: 26	Loss: 0.062067	Acc: 16.3% (1630/10000)
[Test]  Epoch: 27	Loss: 0.065014	Acc: 13.8% (1379/10000)
[Test]  Epoch: 28	Loss: 0.060181	Acc: 17.8% (1784/10000)
[Test]  Epoch: 29	Loss: 0.061526	Acc: 16.3% (1634/10000)
[Test]  Epoch: 30	Loss: 0.060153	Acc: 17.3% (1733/10000)
[Test]  Epoch: 31	Loss: 0.061321	Acc: 16.6% (1660/10000)
[Test]  Epoch: 32	Loss: 0.061053	Acc: 16.7% (1668/10000)
[Test]  Epoch: 33	Loss: 0.062184	Acc: 15.5% (1553/10000)
[Test]  Epoch: 34	Loss: 0.061245	Acc: 16.9% (1685/10000)
[Test]  Epoch: 35	Loss: 0.061190	Acc: 16.8% (1683/10000)
[Test]  Epoch: 36	Loss: 0.062386	Acc: 15.4% (1541/10000)
[Test]  Epoch: 37	Loss: 0.060209	Acc: 17.5% (1748/10000)
[Test]  Epoch: 38	Loss: 0.060672	Acc: 16.9% (1691/10000)
[Test]  Epoch: 39	Loss: 0.060136	Acc: 17.5% (1748/10000)
[Test]  Epoch: 40	Loss: 0.059944	Acc: 17.5% (1752/10000)
[Test]  Epoch: 41	Loss: 0.060468	Acc: 17.4% (1735/10000)
[Test]  Epoch: 42	Loss: 0.061296	Acc: 16.3% (1628/10000)
[Test]  Epoch: 43	Loss: 0.060240	Acc: 17.4% (1743/10000)
[Test]  Epoch: 44	Loss: 0.059812	Acc: 17.6% (1759/10000)
[Test]  Epoch: 45	Loss: 0.059482	Acc: 18.4% (1835/10000)
[Test]  Epoch: 46	Loss: 0.060110	Acc: 17.6% (1755/10000)
[Test]  Epoch: 47	Loss: 0.061029	Acc: 16.6% (1661/10000)
[Test]  Epoch: 48	Loss: 0.059630	Acc: 18.2% (1817/10000)
[Test]  Epoch: 49	Loss: 0.060327	Acc: 17.5% (1753/10000)
[Test]  Epoch: 50	Loss: 0.060685	Acc: 16.9% (1686/10000)
[Test]  Epoch: 51	Loss: 0.060069	Acc: 17.7% (1772/10000)
[Test]  Epoch: 52	Loss: 0.059504	Acc: 17.9% (1785/10000)
[Test]  Epoch: 53	Loss: 0.059692	Acc: 18.3% (1833/10000)
[Test]  Epoch: 54	Loss: 0.059150	Acc: 17.9% (1792/10000)
[Test]  Epoch: 55	Loss: 0.060738	Acc: 16.8% (1683/10000)
[Test]  Epoch: 56	Loss: 0.060060	Acc: 17.2% (1717/10000)
[Test]  Epoch: 57	Loss: 0.059831	Acc: 17.8% (1783/10000)
[Test]  Epoch: 58	Loss: 0.060034	Acc: 17.4% (1741/10000)
[Test]  Epoch: 59	Loss: 0.060632	Acc: 16.1% (1611/10000)
[Test]  Epoch: 60	Loss: 0.060857	Acc: 16.9% (1686/10000)
[Test]  Epoch: 61	Loss: 0.058039	Acc: 19.2% (1921/10000)
[Test]  Epoch: 62	Loss: 0.057210	Acc: 19.7% (1973/10000)
[Test]  Epoch: 63	Loss: 0.057169	Acc: 20.0% (2003/10000)
[Test]  Epoch: 64	Loss: 0.056948	Acc: 20.2% (2020/10000)
[Test]  Epoch: 65	Loss: 0.056741	Acc: 20.6% (2062/10000)
[Test]  Epoch: 66	Loss: 0.056569	Acc: 20.6% (2055/10000)
[Test]  Epoch: 67	Loss: 0.056788	Acc: 20.1% (2012/10000)
[Test]  Epoch: 68	Loss: 0.056624	Acc: 20.4% (2036/10000)
[Test]  Epoch: 69	Loss: 0.056638	Acc: 20.5% (2049/10000)
[Test]  Epoch: 70	Loss: 0.056292	Acc: 20.8% (2079/10000)
[Test]  Epoch: 71	Loss: 0.056360	Acc: 21.2% (2119/10000)
[Test]  Epoch: 72	Loss: 0.056185	Acc: 21.2% (2121/10000)
[Test]  Epoch: 73	Loss: 0.056154	Acc: 20.9% (2092/10000)
[Test]  Epoch: 74	Loss: 0.056360	Acc: 21.1% (2112/10000)
[Test]  Epoch: 75	Loss: 0.056140	Acc: 21.1% (2110/10000)
[Test]  Epoch: 76	Loss: 0.056285	Acc: 21.2% (2116/10000)
[Test]  Epoch: 77	Loss: 0.056255	Acc: 21.2% (2117/10000)
[Test]  Epoch: 78	Loss: 0.056211	Acc: 21.1% (2115/10000)
[Test]  Epoch: 79	Loss: 0.056267	Acc: 21.4% (2136/10000)
[Test]  Epoch: 80	Loss: 0.055715	Acc: 21.7% (2167/10000)
[Test]  Epoch: 81	Loss: 0.056279	Acc: 20.9% (2094/10000)
[Test]  Epoch: 82	Loss: 0.056238	Acc: 21.6% (2155/10000)
[Test]  Epoch: 83	Loss: 0.055979	Acc: 20.9% (2091/10000)
[Test]  Epoch: 84	Loss: 0.056078	Acc: 21.7% (2169/10000)
[Test]  Epoch: 85	Loss: 0.056177	Acc: 20.9% (2095/10000)
[Test]  Epoch: 86	Loss: 0.056084	Acc: 21.0% (2103/10000)
[Test]  Epoch: 87	Loss: 0.055979	Acc: 21.1% (2114/10000)
[Test]  Epoch: 88	Loss: 0.055856	Acc: 21.3% (2127/10000)
[Test]  Epoch: 89	Loss: 0.055958	Acc: 21.7% (2167/10000)
[Test]  Epoch: 90	Loss: 0.055935	Acc: 21.6% (2163/10000)
[Test]  Epoch: 91	Loss: 0.055984	Acc: 21.9% (2186/10000)
[Test]  Epoch: 92	Loss: 0.056068	Acc: 21.3% (2128/10000)
[Test]  Epoch: 93	Loss: 0.056201	Acc: 21.2% (2124/10000)
[Test]  Epoch: 94	Loss: 0.055830	Acc: 21.6% (2156/10000)
[Test]  Epoch: 95	Loss: 0.056078	Acc: 21.5% (2146/10000)
[Test]  Epoch: 96	Loss: 0.056010	Acc: 21.2% (2118/10000)
[Test]  Epoch: 97	Loss: 0.055736	Acc: 21.5% (2152/10000)
[Test]  Epoch: 98	Loss: 0.056012	Acc: 21.5% (2151/10000)
[Test]  Epoch: 99	Loss: 0.055987	Acc: 21.2% (2122/10000)
[Test]  Epoch: 100	Loss: 0.055955	Acc: 21.1% (2114/10000)
===========finish==========
['2024-08-18', '19:02:39.012816', '100', 'test', '0.05595466009378433', '21.14', '21.86']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.2 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=5
get_sample_layers not_random
5 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.090929	Acc: 0.9% (92/10000)
[Test]  Epoch: 2	Loss: 0.090805	Acc: 1.8% (183/10000)
[Test]  Epoch: 3	Loss: 0.082502	Acc: 2.2% (222/10000)
[Test]  Epoch: 4	Loss: 0.082192	Acc: 2.0% (200/10000)
[Test]  Epoch: 5	Loss: 0.081177	Acc: 2.6% (257/10000)
[Test]  Epoch: 6	Loss: 0.080136	Acc: 3.2% (316/10000)
[Test]  Epoch: 7	Loss: 0.077462	Acc: 3.3% (334/10000)
[Test]  Epoch: 8	Loss: 0.080332	Acc: 3.0% (296/10000)
[Test]  Epoch: 9	Loss: 0.077056	Acc: 3.5% (351/10000)
[Test]  Epoch: 10	Loss: 0.079158	Acc: 3.4% (337/10000)
[Test]  Epoch: 11	Loss: 0.076767	Acc: 4.1% (407/10000)
[Test]  Epoch: 12	Loss: 0.077701	Acc: 4.2% (418/10000)
[Test]  Epoch: 13	Loss: 0.076048	Acc: 4.7% (466/10000)
[Test]  Epoch: 14	Loss: 0.078308	Acc: 3.6% (364/10000)
[Test]  Epoch: 15	Loss: 0.079121	Acc: 4.1% (414/10000)
[Test]  Epoch: 16	Loss: 0.080581	Acc: 3.9% (386/10000)
[Test]  Epoch: 17	Loss: 0.076895	Acc: 4.9% (492/10000)
[Test]  Epoch: 18	Loss: 0.080453	Acc: 4.7% (471/10000)
[Test]  Epoch: 19	Loss: 0.075352	Acc: 5.7% (566/10000)
[Test]  Epoch: 20	Loss: 0.074521	Acc: 6.4% (637/10000)
[Test]  Epoch: 21	Loss: 0.077765	Acc: 5.3% (530/10000)
[Test]  Epoch: 22	Loss: 0.076812	Acc: 5.6% (557/10000)
[Test]  Epoch: 23	Loss: 0.076209	Acc: 5.2% (522/10000)
[Test]  Epoch: 24	Loss: 0.073847	Acc: 6.6% (656/10000)
[Test]  Epoch: 25	Loss: 0.074656	Acc: 6.5% (655/10000)
[Test]  Epoch: 26	Loss: 0.075350	Acc: 6.9% (686/10000)
[Test]  Epoch: 27	Loss: 0.075825	Acc: 6.2% (625/10000)
[Test]  Epoch: 28	Loss: 0.073897	Acc: 7.1% (706/10000)
[Test]  Epoch: 29	Loss: 0.074953	Acc: 6.6% (657/10000)
[Test]  Epoch: 30	Loss: 0.071902	Acc: 8.1% (811/10000)
[Test]  Epoch: 31	Loss: 0.074885	Acc: 6.3% (628/10000)
[Test]  Epoch: 32	Loss: 0.071994	Acc: 8.4% (843/10000)
[Test]  Epoch: 33	Loss: 0.075610	Acc: 6.3% (631/10000)
[Test]  Epoch: 34	Loss: 0.073305	Acc: 7.9% (793/10000)
[Test]  Epoch: 35	Loss: 0.074325	Acc: 6.9% (694/10000)
[Test]  Epoch: 36	Loss: 0.074482	Acc: 7.5% (747/10000)
[Test]  Epoch: 37	Loss: 0.071604	Acc: 8.4% (843/10000)
[Test]  Epoch: 38	Loss: 0.070564	Acc: 9.2% (920/10000)
[Test]  Epoch: 39	Loss: 0.071544	Acc: 8.4% (843/10000)
[Test]  Epoch: 40	Loss: 0.072975	Acc: 7.8% (777/10000)
[Test]  Epoch: 41	Loss: 0.073973	Acc: 7.9% (789/10000)
[Test]  Epoch: 42	Loss: 0.072292	Acc: 8.1% (812/10000)
[Test]  Epoch: 43	Loss: 0.073329	Acc: 7.8% (776/10000)
[Test]  Epoch: 44	Loss: 0.072554	Acc: 8.8% (875/10000)
[Test]  Epoch: 45	Loss: 0.071810	Acc: 8.6% (861/10000)
[Test]  Epoch: 46	Loss: 0.072181	Acc: 8.7% (867/10000)
[Test]  Epoch: 47	Loss: 0.071597	Acc: 8.2% (825/10000)
[Test]  Epoch: 48	Loss: 0.072202	Acc: 9.3% (933/10000)
[Test]  Epoch: 49	Loss: 0.073384	Acc: 7.6% (760/10000)
[Test]  Epoch: 50	Loss: 0.070395	Acc: 9.4% (944/10000)
[Test]  Epoch: 51	Loss: 0.072545	Acc: 8.3% (833/10000)
[Test]  Epoch: 52	Loss: 0.071984	Acc: 8.8% (885/10000)
[Test]  Epoch: 53	Loss: 0.069707	Acc: 9.8% (982/10000)
[Test]  Epoch: 54	Loss: 0.070728	Acc: 9.1% (908/10000)
[Test]  Epoch: 55	Loss: 0.070384	Acc: 9.3% (930/10000)
[Test]  Epoch: 56	Loss: 0.070416	Acc: 9.3% (928/10000)
[Test]  Epoch: 57	Loss: 0.071430	Acc: 9.0% (902/10000)
[Test]  Epoch: 58	Loss: 0.071898	Acc: 9.0% (897/10000)
[Test]  Epoch: 59	Loss: 0.070590	Acc: 9.1% (913/10000)
[Test]  Epoch: 60	Loss: 0.071307	Acc: 9.3% (927/10000)
[Test]  Epoch: 61	Loss: 0.067528	Acc: 11.2% (1120/10000)
[Test]  Epoch: 62	Loss: 0.066737	Acc: 11.5% (1150/10000)
[Test]  Epoch: 63	Loss: 0.066538	Acc: 11.4% (1139/10000)
[Test]  Epoch: 64	Loss: 0.066315	Acc: 11.8% (1175/10000)
[Test]  Epoch: 65	Loss: 0.066176	Acc: 11.7% (1170/10000)
[Test]  Epoch: 66	Loss: 0.066039	Acc: 11.8% (1178/10000)
[Test]  Epoch: 67	Loss: 0.066112	Acc: 12.3% (1229/10000)
[Test]  Epoch: 68	Loss: 0.065931	Acc: 12.2% (1220/10000)
[Test]  Epoch: 69	Loss: 0.065875	Acc: 12.3% (1226/10000)
[Test]  Epoch: 70	Loss: 0.065907	Acc: 12.4% (1242/10000)
[Test]  Epoch: 71	Loss: 0.065730	Acc: 12.0% (1203/10000)
[Test]  Epoch: 72	Loss: 0.065545	Acc: 12.3% (1228/10000)
[Test]  Epoch: 73	Loss: 0.065686	Acc: 12.7% (1267/10000)
[Test]  Epoch: 74	Loss: 0.065818	Acc: 12.0% (1196/10000)
[Test]  Epoch: 75	Loss: 0.065553	Acc: 12.7% (1265/10000)
[Test]  Epoch: 76	Loss: 0.065532	Acc: 12.7% (1267/10000)
[Test]  Epoch: 77	Loss: 0.065673	Acc: 12.2% (1218/10000)
[Test]  Epoch: 78	Loss: 0.065619	Acc: 12.3% (1227/10000)
[Test]  Epoch: 79	Loss: 0.065621	Acc: 12.5% (1251/10000)
[Test]  Epoch: 80	Loss: 0.064916	Acc: 12.7% (1274/10000)
[Test]  Epoch: 81	Loss: 0.065611	Acc: 12.1% (1206/10000)
[Test]  Epoch: 82	Loss: 0.065481	Acc: 12.3% (1235/10000)
[Test]  Epoch: 83	Loss: 0.065724	Acc: 12.3% (1234/10000)
[Test]  Epoch: 84	Loss: 0.065433	Acc: 12.6% (1264/10000)
[Test]  Epoch: 85	Loss: 0.065655	Acc: 12.1% (1209/10000)
[Test]  Epoch: 86	Loss: 0.065611	Acc: 12.4% (1239/10000)
[Test]  Epoch: 87	Loss: 0.065462	Acc: 12.7% (1274/10000)
[Test]  Epoch: 88	Loss: 0.065247	Acc: 12.4% (1245/10000)
[Test]  Epoch: 89	Loss: 0.065141	Acc: 12.5% (1246/10000)
[Test]  Epoch: 90	Loss: 0.065120	Acc: 12.5% (1246/10000)
[Test]  Epoch: 91	Loss: 0.065155	Acc: 12.8% (1284/10000)
[Test]  Epoch: 92	Loss: 0.065131	Acc: 12.9% (1294/10000)
[Test]  Epoch: 93	Loss: 0.065464	Acc: 12.6% (1262/10000)
[Test]  Epoch: 94	Loss: 0.065230	Acc: 12.7% (1273/10000)
[Test]  Epoch: 95	Loss: 0.065445	Acc: 12.4% (1241/10000)
[Test]  Epoch: 96	Loss: 0.065308	Acc: 12.5% (1251/10000)
[Test]  Epoch: 97	Loss: 0.064966	Acc: 12.9% (1290/10000)
[Test]  Epoch: 98	Loss: 0.065093	Acc: 12.6% (1259/10000)
[Test]  Epoch: 99	Loss: 0.065276	Acc: 12.8% (1276/10000)
[Test]  Epoch: 100	Loss: 0.065130	Acc: 12.9% (1287/10000)
===========finish==========
['2024-08-18', '19:05:29.538823', '100', 'test', '0.06513001742362975', '12.87', '12.94']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.3 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=8
get_sample_layers not_random
8 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.313047	Acc: 1.0% (104/10000)
[Test]  Epoch: 2	Loss: 0.081137	Acc: 1.7% (174/10000)
[Test]  Epoch: 3	Loss: 0.081419	Acc: 2.1% (214/10000)
[Test]  Epoch: 4	Loss: 0.083100	Acc: 2.1% (207/10000)
[Test]  Epoch: 5	Loss: 0.083823	Acc: 2.0% (197/10000)
[Test]  Epoch: 6	Loss: 0.080800	Acc: 2.3% (229/10000)
[Test]  Epoch: 7	Loss: 0.080647	Acc: 2.2% (224/10000)
[Test]  Epoch: 8	Loss: 0.080159	Acc: 2.5% (248/10000)
[Test]  Epoch: 9	Loss: 0.078204	Acc: 3.0% (300/10000)
[Test]  Epoch: 10	Loss: 0.079587	Acc: 3.0% (298/10000)
[Test]  Epoch: 11	Loss: 0.079129	Acc: 3.0% (300/10000)
[Test]  Epoch: 12	Loss: 0.080708	Acc: 3.0% (305/10000)
[Test]  Epoch: 13	Loss: 0.078078	Acc: 3.6% (361/10000)
[Test]  Epoch: 14	Loss: 0.078561	Acc: 3.6% (360/10000)
[Test]  Epoch: 15	Loss: 0.077364	Acc: 3.8% (376/10000)
[Test]  Epoch: 16	Loss: 0.077361	Acc: 3.8% (381/10000)
[Test]  Epoch: 17	Loss: 0.076951	Acc: 4.5% (448/10000)
[Test]  Epoch: 18	Loss: 0.078311	Acc: 4.2% (419/10000)
[Test]  Epoch: 19	Loss: 0.075995	Acc: 4.9% (488/10000)
[Test]  Epoch: 20	Loss: 0.077326	Acc: 4.8% (484/10000)
[Test]  Epoch: 21	Loss: 0.077181	Acc: 4.4% (439/10000)
[Test]  Epoch: 22	Loss: 0.076301	Acc: 4.9% (486/10000)
[Test]  Epoch: 23	Loss: 0.077319	Acc: 4.3% (435/10000)
[Test]  Epoch: 24	Loss: 0.077234	Acc: 4.6% (458/10000)
[Test]  Epoch: 25	Loss: 0.075793	Acc: 5.1% (509/10000)
[Test]  Epoch: 26	Loss: 0.076758	Acc: 5.1% (511/10000)
[Test]  Epoch: 27	Loss: 0.076674	Acc: 4.7% (470/10000)
[Test]  Epoch: 28	Loss: 0.075000	Acc: 5.4% (544/10000)
[Test]  Epoch: 29	Loss: 0.074135	Acc: 5.9% (591/10000)
[Test]  Epoch: 30	Loss: 0.076042	Acc: 5.2% (522/10000)
[Test]  Epoch: 31	Loss: 0.077379	Acc: 5.2% (524/10000)
[Test]  Epoch: 32	Loss: 0.076182	Acc: 6.0% (599/10000)
[Test]  Epoch: 33	Loss: 0.075505	Acc: 5.4% (539/10000)
[Test]  Epoch: 34	Loss: 0.075933	Acc: 5.4% (538/10000)
[Test]  Epoch: 35	Loss: 0.076841	Acc: 5.1% (512/10000)
[Test]  Epoch: 36	Loss: 0.075780	Acc: 6.0% (595/10000)
[Test]  Epoch: 37	Loss: 0.076172	Acc: 6.2% (617/10000)
[Test]  Epoch: 38	Loss: 0.074765	Acc: 6.3% (627/10000)
[Test]  Epoch: 39	Loss: 0.077594	Acc: 5.5% (545/10000)
[Test]  Epoch: 40	Loss: 0.074528	Acc: 6.8% (677/10000)
[Test]  Epoch: 41	Loss: 0.077410	Acc: 5.6% (559/10000)
[Test]  Epoch: 42	Loss: 0.075927	Acc: 6.2% (625/10000)
[Test]  Epoch: 43	Loss: 0.075460	Acc: 6.1% (607/10000)
[Test]  Epoch: 44	Loss: 0.075579	Acc: 6.8% (681/10000)
[Test]  Epoch: 45	Loss: 0.075343	Acc: 6.5% (655/10000)
[Test]  Epoch: 46	Loss: 0.077021	Acc: 5.7% (568/10000)
[Test]  Epoch: 47	Loss: 0.073555	Acc: 7.0% (698/10000)
[Test]  Epoch: 48	Loss: 0.074981	Acc: 7.1% (709/10000)
[Test]  Epoch: 49	Loss: 0.075138	Acc: 6.5% (648/10000)
[Test]  Epoch: 50	Loss: 0.075716	Acc: 6.7% (666/10000)
[Test]  Epoch: 51	Loss: 0.076225	Acc: 6.5% (650/10000)
[Test]  Epoch: 52	Loss: 0.075625	Acc: 6.7% (667/10000)
[Test]  Epoch: 53	Loss: 0.074271	Acc: 7.3% (729/10000)
[Test]  Epoch: 54	Loss: 0.075687	Acc: 6.5% (653/10000)
[Test]  Epoch: 55	Loss: 0.074800	Acc: 7.4% (743/10000)
[Test]  Epoch: 56	Loss: 0.073189	Acc: 7.8% (775/10000)
[Test]  Epoch: 57	Loss: 0.076153	Acc: 6.3% (633/10000)
[Test]  Epoch: 58	Loss: 0.075122	Acc: 6.6% (662/10000)
[Test]  Epoch: 59	Loss: 0.075815	Acc: 6.5% (647/10000)
[Test]  Epoch: 60	Loss: 0.074705	Acc: 7.0% (700/10000)
[Test]  Epoch: 61	Loss: 0.070981	Acc: 8.8% (880/10000)
[Test]  Epoch: 62	Loss: 0.070229	Acc: 9.0% (904/10000)
[Test]  Epoch: 63	Loss: 0.070171	Acc: 8.9% (891/10000)
[Test]  Epoch: 64	Loss: 0.069833	Acc: 9.2% (920/10000)
[Test]  Epoch: 65	Loss: 0.069729	Acc: 8.8% (882/10000)
[Test]  Epoch: 66	Loss: 0.069609	Acc: 9.2% (920/10000)
[Test]  Epoch: 67	Loss: 0.069597	Acc: 9.5% (950/10000)
[Test]  Epoch: 68	Loss: 0.069527	Acc: 9.1% (909/10000)
[Test]  Epoch: 69	Loss: 0.069459	Acc: 9.3% (933/10000)
[Test]  Epoch: 70	Loss: 0.069427	Acc: 9.6% (959/10000)
[Test]  Epoch: 71	Loss: 0.069547	Acc: 9.5% (948/10000)
[Test]  Epoch: 72	Loss: 0.069411	Acc: 9.8% (982/10000)
[Test]  Epoch: 73	Loss: 0.069567	Acc: 9.7% (965/10000)
[Test]  Epoch: 74	Loss: 0.069637	Acc: 9.2% (918/10000)
[Test]  Epoch: 75	Loss: 0.069350	Acc: 9.5% (948/10000)
[Test]  Epoch: 76	Loss: 0.069090	Acc: 9.6% (963/10000)
[Test]  Epoch: 77	Loss: 0.069589	Acc: 9.7% (972/10000)
[Test]  Epoch: 78	Loss: 0.069593	Acc: 9.1% (910/10000)
[Test]  Epoch: 79	Loss: 0.069434	Acc: 9.4% (942/10000)
[Test]  Epoch: 80	Loss: 0.068766	Acc: 9.9% (989/10000)
[Test]  Epoch: 81	Loss: 0.069399	Acc: 9.1% (907/10000)
[Test]  Epoch: 82	Loss: 0.069209	Acc: 9.7% (968/10000)
[Test]  Epoch: 83	Loss: 0.069676	Acc: 9.0% (903/10000)
[Test]  Epoch: 84	Loss: 0.069357	Acc: 9.2% (925/10000)
[Test]  Epoch: 85	Loss: 0.069326	Acc: 9.9% (992/10000)
[Test]  Epoch: 86	Loss: 0.069312	Acc: 9.4% (944/10000)
[Test]  Epoch: 87	Loss: 0.069302	Acc: 9.7% (969/10000)
[Test]  Epoch: 88	Loss: 0.069071	Acc: 9.7% (967/10000)
[Test]  Epoch: 89	Loss: 0.069003	Acc: 9.2% (925/10000)
[Test]  Epoch: 90	Loss: 0.069006	Acc: 9.2% (919/10000)
[Test]  Epoch: 91	Loss: 0.069099	Acc: 9.9% (992/10000)
[Test]  Epoch: 92	Loss: 0.069006	Acc: 9.5% (949/10000)
[Test]  Epoch: 93	Loss: 0.069246	Acc: 9.9% (989/10000)
[Test]  Epoch: 94	Loss: 0.069151	Acc: 9.9% (994/10000)
[Test]  Epoch: 95	Loss: 0.069352	Acc: 9.4% (940/10000)
[Test]  Epoch: 96	Loss: 0.069159	Acc: 9.8% (985/10000)
[Test]  Epoch: 97	Loss: 0.068925	Acc: 9.3% (935/10000)
[Test]  Epoch: 98	Loss: 0.069176	Acc: 9.6% (961/10000)
[Test]  Epoch: 99	Loss: 0.069095	Acc: 9.9% (987/10000)
[Test]  Epoch: 100	Loss: 0.069212	Acc: 9.4% (945/10000)
===========finish==========
['2024-08-18', '19:08:26.331135', '100', 'test', '0.0692115199804306', '9.45', '9.94']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.4 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=10
get_sample_layers not_random
10 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.237120	Acc: 0.8% (80/10000)
[Test]  Epoch: 2	Loss: 0.087084	Acc: 0.8% (82/10000)
[Test]  Epoch: 3	Loss: 0.087097	Acc: 1.0% (98/10000)
[Test]  Epoch: 4	Loss: 0.086991	Acc: 0.9% (95/10000)
[Test]  Epoch: 5	Loss: 0.084207	Acc: 0.8% (85/10000)
[Test]  Epoch: 6	Loss: 0.083185	Acc: 0.9% (88/10000)
[Test]  Epoch: 7	Loss: 0.082138	Acc: 1.1% (113/10000)
[Test]  Epoch: 8	Loss: 0.083437	Acc: 1.1% (106/10000)
[Test]  Epoch: 9	Loss: 0.081463	Acc: 1.8% (176/10000)
[Test]  Epoch: 10	Loss: 0.082127	Acc: 1.8% (175/10000)
[Test]  Epoch: 11	Loss: 0.080698	Acc: 1.6% (159/10000)
[Test]  Epoch: 12	Loss: 0.080956	Acc: 2.0% (198/10000)
[Test]  Epoch: 13	Loss: 0.079502	Acc: 2.3% (229/10000)
[Test]  Epoch: 14	Loss: 0.079777	Acc: 2.2% (222/10000)
[Test]  Epoch: 15	Loss: 0.079672	Acc: 2.5% (254/10000)
[Test]  Epoch: 16	Loss: 0.079148	Acc: 2.6% (263/10000)
[Test]  Epoch: 17	Loss: 0.079484	Acc: 2.5% (249/10000)
[Test]  Epoch: 18	Loss: 0.083493	Acc: 2.5% (252/10000)
[Test]  Epoch: 19	Loss: 0.078331	Acc: 2.9% (292/10000)
[Test]  Epoch: 20	Loss: 0.078439	Acc: 2.9% (289/10000)
[Test]  Epoch: 21	Loss: 0.079538	Acc: 3.1% (310/10000)
[Test]  Epoch: 22	Loss: 0.078172	Acc: 2.9% (290/10000)
[Test]  Epoch: 23	Loss: 0.078961	Acc: 3.1% (306/10000)
[Test]  Epoch: 24	Loss: 0.078610	Acc: 2.9% (291/10000)
[Test]  Epoch: 25	Loss: 0.078596	Acc: 3.4% (339/10000)
[Test]  Epoch: 26	Loss: 0.077244	Acc: 3.7% (372/10000)
[Test]  Epoch: 27	Loss: 0.078505	Acc: 3.8% (375/10000)
[Test]  Epoch: 28	Loss: 0.079650	Acc: 3.4% (336/10000)
[Test]  Epoch: 29	Loss: 0.078756	Acc: 3.5% (355/10000)
[Test]  Epoch: 30	Loss: 0.078210	Acc: 3.5% (349/10000)
[Test]  Epoch: 31	Loss: 0.077697	Acc: 4.2% (415/10000)
[Test]  Epoch: 32	Loss: 0.077966	Acc: 4.0% (402/10000)
[Test]  Epoch: 33	Loss: 0.078202	Acc: 3.7% (366/10000)
[Test]  Epoch: 34	Loss: 0.077264	Acc: 4.2% (424/10000)
[Test]  Epoch: 35	Loss: 0.078275	Acc: 3.6% (362/10000)
[Test]  Epoch: 36	Loss: 0.077978	Acc: 4.3% (432/10000)
[Test]  Epoch: 37	Loss: 0.077054	Acc: 3.9% (388/10000)
[Test]  Epoch: 38	Loss: 0.077911	Acc: 4.1% (409/10000)
[Test]  Epoch: 39	Loss: 0.077883	Acc: 4.2% (419/10000)
[Test]  Epoch: 40	Loss: 0.078501	Acc: 4.3% (429/10000)
[Test]  Epoch: 41	Loss: 0.078897	Acc: 3.9% (393/10000)
[Test]  Epoch: 42	Loss: 0.078630	Acc: 3.9% (393/10000)
[Test]  Epoch: 43	Loss: 0.080091	Acc: 4.4% (443/10000)
[Test]  Epoch: 44	Loss: 0.077219	Acc: 4.7% (466/10000)
[Test]  Epoch: 45	Loss: 0.077141	Acc: 4.2% (421/10000)
[Test]  Epoch: 46	Loss: 0.077651	Acc: 4.6% (462/10000)
[Test]  Epoch: 47	Loss: 0.076478	Acc: 4.8% (476/10000)
[Test]  Epoch: 48	Loss: 0.079338	Acc: 4.3% (427/10000)
[Test]  Epoch: 49	Loss: 0.077239	Acc: 4.3% (431/10000)
[Test]  Epoch: 50	Loss: 0.077672	Acc: 4.6% (460/10000)
[Test]  Epoch: 51	Loss: 0.077080	Acc: 5.1% (510/10000)
[Test]  Epoch: 52	Loss: 0.077668	Acc: 4.6% (461/10000)
[Test]  Epoch: 53	Loss: 0.077104	Acc: 4.8% (483/10000)
[Test]  Epoch: 54	Loss: 0.077781	Acc: 4.8% (485/10000)
[Test]  Epoch: 55	Loss: 0.076603	Acc: 5.1% (511/10000)
[Test]  Epoch: 56	Loss: 0.075933	Acc: 5.5% (546/10000)
[Test]  Epoch: 57	Loss: 0.079362	Acc: 4.4% (444/10000)
[Test]  Epoch: 58	Loss: 0.077306	Acc: 4.8% (476/10000)
[Test]  Epoch: 59	Loss: 0.076810	Acc: 5.6% (560/10000)
[Test]  Epoch: 60	Loss: 0.080618	Acc: 4.5% (454/10000)
[Test]  Epoch: 61	Loss: 0.073888	Acc: 6.5% (653/10000)
[Test]  Epoch: 62	Loss: 0.073423	Acc: 6.9% (688/10000)
[Test]  Epoch: 63	Loss: 0.073319	Acc: 6.9% (687/10000)
[Test]  Epoch: 64	Loss: 0.072937	Acc: 7.0% (703/10000)
[Test]  Epoch: 65	Loss: 0.072851	Acc: 7.2% (718/10000)
[Test]  Epoch: 66	Loss: 0.072828	Acc: 7.1% (712/10000)
[Test]  Epoch: 67	Loss: 0.073139	Acc: 7.2% (724/10000)
[Test]  Epoch: 68	Loss: 0.072719	Acc: 7.1% (709/10000)
[Test]  Epoch: 69	Loss: 0.072676	Acc: 7.0% (699/10000)
[Test]  Epoch: 70	Loss: 0.072774	Acc: 7.3% (733/10000)
[Test]  Epoch: 71	Loss: 0.072793	Acc: 7.6% (759/10000)
[Test]  Epoch: 72	Loss: 0.072729	Acc: 7.4% (742/10000)
[Test]  Epoch: 73	Loss: 0.072794	Acc: 7.1% (709/10000)
[Test]  Epoch: 74	Loss: 0.072872	Acc: 7.3% (735/10000)
[Test]  Epoch: 75	Loss: 0.072702	Acc: 7.3% (734/10000)
[Test]  Epoch: 76	Loss: 0.072483	Acc: 7.5% (747/10000)
[Test]  Epoch: 77	Loss: 0.072842	Acc: 7.3% (729/10000)
[Test]  Epoch: 78	Loss: 0.072905	Acc: 7.4% (741/10000)
[Test]  Epoch: 79	Loss: 0.072658	Acc: 7.4% (737/10000)
[Test]  Epoch: 80	Loss: 0.072326	Acc: 7.8% (781/10000)
[Test]  Epoch: 81	Loss: 0.072903	Acc: 7.4% (739/10000)
[Test]  Epoch: 82	Loss: 0.072592	Acc: 7.3% (726/10000)
[Test]  Epoch: 83	Loss: 0.072935	Acc: 7.6% (762/10000)
[Test]  Epoch: 84	Loss: 0.072738	Acc: 7.6% (761/10000)
[Test]  Epoch: 85	Loss: 0.072589	Acc: 7.8% (779/10000)
[Test]  Epoch: 86	Loss: 0.072671	Acc: 7.7% (765/10000)
[Test]  Epoch: 87	Loss: 0.072650	Acc: 7.4% (741/10000)
[Test]  Epoch: 88	Loss: 0.072591	Acc: 7.3% (732/10000)
[Test]  Epoch: 89	Loss: 0.072609	Acc: 7.2% (722/10000)
[Test]  Epoch: 90	Loss: 0.072774	Acc: 7.4% (742/10000)
[Test]  Epoch: 91	Loss: 0.072697	Acc: 7.6% (759/10000)
[Test]  Epoch: 92	Loss: 0.072547	Acc: 7.9% (794/10000)
[Test]  Epoch: 93	Loss: 0.072530	Acc: 7.6% (763/10000)
[Test]  Epoch: 94	Loss: 0.072465	Acc: 7.3% (735/10000)
[Test]  Epoch: 95	Loss: 0.072734	Acc: 7.7% (769/10000)
[Test]  Epoch: 96	Loss: 0.072610	Acc: 7.7% (771/10000)
[Test]  Epoch: 97	Loss: 0.072277	Acc: 7.8% (778/10000)
[Test]  Epoch: 98	Loss: 0.072673	Acc: 7.6% (756/10000)
[Test]  Epoch: 99	Loss: 0.072641	Acc: 7.7% (772/10000)
[Test]  Epoch: 100	Loss: 0.072584	Acc: 7.7% (768/10000)
===========finish==========
['2024-08-18', '19:11:19.989217', '100', 'test', '0.07258410651683807', '7.68', '7.94']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.5 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=13
get_sample_layers not_random
13 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.606910	Acc: 0.5% (54/10000)
[Test]  Epoch: 2	Loss: 0.088066	Acc: 0.5% (48/10000)
[Test]  Epoch: 3	Loss: 0.091092	Acc: 0.5% (53/10000)
[Test]  Epoch: 4	Loss: 0.085458	Acc: 0.5% (50/10000)
[Test]  Epoch: 5	Loss: 0.086130	Acc: 0.5% (51/10000)
[Test]  Epoch: 6	Loss: 0.084207	Acc: 0.6% (55/10000)
[Test]  Epoch: 7	Loss: 0.084454	Acc: 0.5% (49/10000)
[Test]  Epoch: 8	Loss: 0.083567	Acc: 0.5% (50/10000)
[Test]  Epoch: 9	Loss: 0.084432	Acc: 0.5% (53/10000)
[Test]  Epoch: 10	Loss: 0.084296	Acc: 0.6% (57/10000)
[Test]  Epoch: 11	Loss: 0.083779	Acc: 0.6% (56/10000)
[Test]  Epoch: 12	Loss: 0.083293	Acc: 0.6% (58/10000)
[Test]  Epoch: 13	Loss: 0.083280	Acc: 0.7% (71/10000)
[Test]  Epoch: 14	Loss: 0.082983	Acc: 0.8% (85/10000)
[Test]  Epoch: 15	Loss: 0.083047	Acc: 1.0% (98/10000)
[Test]  Epoch: 16	Loss: 0.082315	Acc: 1.0% (98/10000)
[Test]  Epoch: 17	Loss: 0.081865	Acc: 1.2% (117/10000)
[Test]  Epoch: 18	Loss: 0.082008	Acc: 1.0% (103/10000)
[Test]  Epoch: 19	Loss: 0.084034	Acc: 1.5% (150/10000)
[Test]  Epoch: 20	Loss: 0.082166	Acc: 1.3% (126/10000)
[Test]  Epoch: 21	Loss: 0.093419	Acc: 1.2% (120/10000)
[Test]  Epoch: 22	Loss: 0.081589	Acc: 1.5% (154/10000)
[Test]  Epoch: 23	Loss: 0.081700	Acc: 1.2% (121/10000)
[Test]  Epoch: 24	Loss: 0.081198	Acc: 1.5% (151/10000)
[Test]  Epoch: 25	Loss: 0.082031	Acc: 1.5% (148/10000)
[Test]  Epoch: 26	Loss: 0.081585	Acc: 1.8% (175/10000)
[Test]  Epoch: 27	Loss: 0.080997	Acc: 2.0% (199/10000)
[Test]  Epoch: 28	Loss: 0.081520	Acc: 1.8% (176/10000)
[Test]  Epoch: 29	Loss: 0.082164	Acc: 1.9% (188/10000)
[Test]  Epoch: 30	Loss: 0.081061	Acc: 2.5% (253/10000)
[Test]  Epoch: 31	Loss: 0.081295	Acc: 2.1% (207/10000)
[Test]  Epoch: 32	Loss: 0.081889	Acc: 2.2% (224/10000)
[Test]  Epoch: 33	Loss: 0.078800	Acc: 2.8% (280/10000)
[Test]  Epoch: 34	Loss: 0.082162	Acc: 2.3% (230/10000)
[Test]  Epoch: 35	Loss: 0.080134	Acc: 2.1% (208/10000)
[Test]  Epoch: 36	Loss: 0.079518	Acc: 2.4% (243/10000)
[Test]  Epoch: 37	Loss: 0.078675	Acc: 2.8% (282/10000)
[Test]  Epoch: 38	Loss: 0.082210	Acc: 2.5% (249/10000)
[Test]  Epoch: 39	Loss: 0.079580	Acc: 2.8% (276/10000)
[Test]  Epoch: 40	Loss: 0.077704	Acc: 3.2% (317/10000)
[Test]  Epoch: 41	Loss: 0.078501	Acc: 3.1% (312/10000)
[Test]  Epoch: 42	Loss: 0.078413	Acc: 3.1% (308/10000)
[Test]  Epoch: 43	Loss: 0.081188	Acc: 3.4% (339/10000)
[Test]  Epoch: 44	Loss: 0.078896	Acc: 3.3% (328/10000)
[Test]  Epoch: 45	Loss: 0.078572	Acc: 3.5% (345/10000)
[Test]  Epoch: 46	Loss: 0.078461	Acc: 3.5% (354/10000)
[Test]  Epoch: 47	Loss: 0.076925	Acc: 3.8% (375/10000)
[Test]  Epoch: 48	Loss: 0.077715	Acc: 3.9% (388/10000)
[Test]  Epoch: 49	Loss: 0.078630	Acc: 3.6% (359/10000)
[Test]  Epoch: 50	Loss: 0.081175	Acc: 3.0% (299/10000)
[Test]  Epoch: 51	Loss: 0.078316	Acc: 3.8% (384/10000)
[Test]  Epoch: 52	Loss: 0.078244	Acc: 3.8% (378/10000)
[Test]  Epoch: 53	Loss: 0.077568	Acc: 3.7% (367/10000)
[Test]  Epoch: 54	Loss: 0.077430	Acc: 4.0% (402/10000)
[Test]  Epoch: 55	Loss: 0.077356	Acc: 3.8% (384/10000)
[Test]  Epoch: 56	Loss: 0.077349	Acc: 3.9% (394/10000)
[Test]  Epoch: 57	Loss: 0.078812	Acc: 4.1% (407/10000)
[Test]  Epoch: 58	Loss: 0.078209	Acc: 4.0% (399/10000)
[Test]  Epoch: 59	Loss: 0.076684	Acc: 4.2% (415/10000)
[Test]  Epoch: 60	Loss: 0.078093	Acc: 4.5% (449/10000)
[Test]  Epoch: 61	Loss: 0.074464	Acc: 5.4% (542/10000)
[Test]  Epoch: 62	Loss: 0.074118	Acc: 6.2% (617/10000)
[Test]  Epoch: 63	Loss: 0.074103	Acc: 5.8% (585/10000)
[Test]  Epoch: 64	Loss: 0.073743	Acc: 6.0% (596/10000)
[Test]  Epoch: 65	Loss: 0.073829	Acc: 6.0% (596/10000)
[Test]  Epoch: 66	Loss: 0.073598	Acc: 6.5% (646/10000)
[Test]  Epoch: 67	Loss: 0.073658	Acc: 6.3% (627/10000)
[Test]  Epoch: 68	Loss: 0.073680	Acc: 6.2% (619/10000)
[Test]  Epoch: 69	Loss: 0.073497	Acc: 6.0% (603/10000)
[Test]  Epoch: 70	Loss: 0.073182	Acc: 6.6% (660/10000)
[Test]  Epoch: 71	Loss: 0.073447	Acc: 6.4% (643/10000)
[Test]  Epoch: 72	Loss: 0.073653	Acc: 6.1% (613/10000)
[Test]  Epoch: 73	Loss: 0.073407	Acc: 6.3% (628/10000)
[Test]  Epoch: 74	Loss: 0.073399	Acc: 6.2% (624/10000)
[Test]  Epoch: 75	Loss: 0.073513	Acc: 6.1% (610/10000)
[Test]  Epoch: 76	Loss: 0.073502	Acc: 6.1% (608/10000)
[Test]  Epoch: 77	Loss: 0.073684	Acc: 6.3% (634/10000)
[Test]  Epoch: 78	Loss: 0.073365	Acc: 6.7% (670/10000)
[Test]  Epoch: 79	Loss: 0.073406	Acc: 6.5% (653/10000)
[Test]  Epoch: 80	Loss: 0.073008	Acc: 6.4% (643/10000)
[Test]  Epoch: 81	Loss: 0.073350	Acc: 6.6% (663/10000)
[Test]  Epoch: 82	Loss: 0.073075	Acc: 6.4% (643/10000)
[Test]  Epoch: 83	Loss: 0.073210	Acc: 6.5% (652/10000)
[Test]  Epoch: 84	Loss: 0.073160	Acc: 6.6% (657/10000)
[Test]  Epoch: 85	Loss: 0.073292	Acc: 6.7% (667/10000)
[Test]  Epoch: 86	Loss: 0.073247	Acc: 6.8% (685/10000)
[Test]  Epoch: 87	Loss: 0.073365	Acc: 6.8% (678/10000)
[Test]  Epoch: 88	Loss: 0.073101	Acc: 6.7% (666/10000)
[Test]  Epoch: 89	Loss: 0.073043	Acc: 6.2% (619/10000)
[Test]  Epoch: 90	Loss: 0.073172	Acc: 6.6% (659/10000)
[Test]  Epoch: 91	Loss: 0.073322	Acc: 6.9% (686/10000)
[Test]  Epoch: 92	Loss: 0.073322	Acc: 6.7% (672/10000)
[Test]  Epoch: 93	Loss: 0.073009	Acc: 6.8% (679/10000)
[Test]  Epoch: 94	Loss: 0.073571	Acc: 6.7% (670/10000)
[Test]  Epoch: 95	Loss: 0.073314	Acc: 6.7% (671/10000)
[Test]  Epoch: 96	Loss: 0.073271	Acc: 6.7% (671/10000)
[Test]  Epoch: 97	Loss: 0.073131	Acc: 6.8% (681/10000)
[Test]  Epoch: 98	Loss: 0.073707	Acc: 6.6% (658/10000)
[Test]  Epoch: 99	Loss: 0.073558	Acc: 6.6% (657/10000)
[Test]  Epoch: 100	Loss: 0.073939	Acc: 6.1% (606/10000)
===========finish==========
['2024-08-18', '19:14:11.101558', '100', 'test', '0.07393932511806488', '6.06', '6.86']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.6 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=16
get_sample_layers not_random
16 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.319345	Acc: 0.5% (52/10000)
[Test]  Epoch: 2	Loss: 0.093760	Acc: 1.1% (114/10000)
[Test]  Epoch: 3	Loss: 0.083125	Acc: 1.8% (178/10000)
[Test]  Epoch: 4	Loss: 0.081085	Acc: 1.9% (188/10000)
[Test]  Epoch: 5	Loss: 0.083416	Acc: 1.8% (184/10000)
[Test]  Epoch: 6	Loss: 0.080950	Acc: 2.0% (198/10000)
[Test]  Epoch: 7	Loss: 0.080353	Acc: 2.3% (232/10000)
[Test]  Epoch: 8	Loss: 0.083776	Acc: 1.8% (181/10000)
[Test]  Epoch: 9	Loss: 0.078311	Acc: 2.8% (280/10000)
[Test]  Epoch: 10	Loss: 0.078557	Acc: 3.1% (307/10000)
[Test]  Epoch: 11	Loss: 0.078766	Acc: 3.0% (304/10000)
[Test]  Epoch: 12	Loss: 0.078427	Acc: 3.0% (296/10000)
[Test]  Epoch: 13	Loss: 0.079503	Acc: 2.8% (281/10000)
[Test]  Epoch: 14	Loss: 0.078089	Acc: 3.1% (306/10000)
[Test]  Epoch: 15	Loss: 0.078652	Acc: 2.6% (263/10000)
[Test]  Epoch: 16	Loss: 0.079461	Acc: 2.8% (284/10000)
[Test]  Epoch: 17	Loss: 0.080095	Acc: 2.9% (289/10000)
[Test]  Epoch: 18	Loss: 0.077405	Acc: 3.6% (358/10000)
[Test]  Epoch: 19	Loss: 0.077282	Acc: 3.9% (393/10000)
[Test]  Epoch: 20	Loss: 0.077415	Acc: 3.7% (371/10000)
[Test]  Epoch: 21	Loss: 0.078133	Acc: 3.6% (363/10000)
[Test]  Epoch: 22	Loss: 0.077092	Acc: 4.0% (395/10000)
[Test]  Epoch: 23	Loss: 0.077630	Acc: 3.2% (325/10000)
[Test]  Epoch: 24	Loss: 0.078308	Acc: 3.7% (373/10000)
[Test]  Epoch: 25	Loss: 0.077577	Acc: 4.2% (419/10000)
[Test]  Epoch: 26	Loss: 0.077652	Acc: 3.7% (374/10000)
[Test]  Epoch: 27	Loss: 0.079239	Acc: 3.8% (381/10000)
[Test]  Epoch: 28	Loss: 0.078183	Acc: 3.6% (364/10000)
[Test]  Epoch: 29	Loss: 0.076666	Acc: 3.9% (390/10000)
[Test]  Epoch: 30	Loss: 0.076661	Acc: 4.2% (415/10000)
[Test]  Epoch: 31	Loss: 0.078105	Acc: 4.1% (408/10000)
[Test]  Epoch: 32	Loss: 0.076487	Acc: 4.7% (466/10000)
[Test]  Epoch: 33	Loss: 0.076724	Acc: 4.2% (425/10000)
[Test]  Epoch: 34	Loss: 0.076110	Acc: 4.8% (481/10000)
[Test]  Epoch: 35	Loss: 0.079485	Acc: 4.0% (396/10000)
[Test]  Epoch: 36	Loss: 0.076399	Acc: 4.8% (485/10000)
[Test]  Epoch: 37	Loss: 0.077611	Acc: 4.7% (469/10000)
[Test]  Epoch: 38	Loss: 0.076404	Acc: 4.8% (484/10000)
[Test]  Epoch: 39	Loss: 0.077893	Acc: 4.9% (488/10000)
[Test]  Epoch: 40	Loss: 0.077372	Acc: 4.8% (478/10000)
[Test]  Epoch: 41	Loss: 0.077063	Acc: 5.0% (495/10000)
[Test]  Epoch: 42	Loss: 0.076141	Acc: 4.4% (438/10000)
[Test]  Epoch: 43	Loss: 0.076163	Acc: 5.2% (520/10000)
[Test]  Epoch: 44	Loss: 0.075022	Acc: 5.4% (543/10000)
[Test]  Epoch: 45	Loss: 0.078093	Acc: 4.4% (436/10000)
[Test]  Epoch: 46	Loss: 0.077049	Acc: 5.2% (518/10000)
[Test]  Epoch: 47	Loss: 0.076260	Acc: 4.7% (470/10000)
[Test]  Epoch: 48	Loss: 0.075426	Acc: 5.7% (568/10000)
[Test]  Epoch: 49	Loss: 0.078750	Acc: 5.0% (499/10000)
[Test]  Epoch: 50	Loss: 0.077513	Acc: 4.8% (485/10000)
[Test]  Epoch: 51	Loss: 0.075602	Acc: 5.3% (528/10000)
[Test]  Epoch: 52	Loss: 0.076453	Acc: 5.5% (548/10000)
[Test]  Epoch: 53	Loss: 0.074984	Acc: 6.0% (599/10000)
[Test]  Epoch: 54	Loss: 0.079063	Acc: 4.2% (425/10000)
[Test]  Epoch: 55	Loss: 0.075474	Acc: 5.9% (589/10000)
[Test]  Epoch: 56	Loss: 0.074659	Acc: 5.7% (567/10000)
[Test]  Epoch: 57	Loss: 0.078205	Acc: 5.2% (515/10000)
[Test]  Epoch: 58	Loss: 0.077684	Acc: 5.1% (508/10000)
[Test]  Epoch: 59	Loss: 0.078040	Acc: 5.0% (496/10000)
[Test]  Epoch: 60	Loss: 0.075532	Acc: 6.0% (596/10000)
[Test]  Epoch: 61	Loss: 0.072255	Acc: 7.5% (753/10000)
[Test]  Epoch: 62	Loss: 0.071984	Acc: 7.5% (750/10000)
[Test]  Epoch: 63	Loss: 0.071986	Acc: 7.8% (776/10000)
[Test]  Epoch: 64	Loss: 0.071518	Acc: 8.0% (804/10000)
[Test]  Epoch: 65	Loss: 0.071434	Acc: 7.6% (757/10000)
[Test]  Epoch: 66	Loss: 0.071394	Acc: 8.1% (811/10000)
[Test]  Epoch: 67	Loss: 0.071485	Acc: 8.2% (824/10000)
[Test]  Epoch: 68	Loss: 0.071271	Acc: 7.6% (757/10000)
[Test]  Epoch: 69	Loss: 0.071196	Acc: 8.2% (822/10000)
[Test]  Epoch: 70	Loss: 0.071032	Acc: 8.1% (814/10000)
[Test]  Epoch: 71	Loss: 0.071127	Acc: 7.7% (765/10000)
[Test]  Epoch: 72	Loss: 0.071181	Acc: 8.1% (813/10000)
[Test]  Epoch: 73	Loss: 0.071021	Acc: 8.1% (811/10000)
[Test]  Epoch: 74	Loss: 0.071445	Acc: 7.6% (763/10000)
[Test]  Epoch: 75	Loss: 0.071139	Acc: 8.1% (807/10000)
[Test]  Epoch: 76	Loss: 0.071068	Acc: 8.4% (843/10000)
[Test]  Epoch: 77	Loss: 0.071384	Acc: 8.2% (824/10000)
[Test]  Epoch: 78	Loss: 0.071485	Acc: 8.1% (810/10000)
[Test]  Epoch: 79	Loss: 0.071272	Acc: 7.9% (793/10000)
[Test]  Epoch: 80	Loss: 0.070681	Acc: 8.4% (844/10000)
[Test]  Epoch: 81	Loss: 0.071127	Acc: 8.2% (824/10000)
[Test]  Epoch: 82	Loss: 0.070958	Acc: 8.0% (800/10000)
[Test]  Epoch: 83	Loss: 0.071292	Acc: 7.9% (790/10000)
[Test]  Epoch: 84	Loss: 0.071140	Acc: 8.0% (795/10000)
[Test]  Epoch: 85	Loss: 0.071227	Acc: 7.9% (792/10000)
[Test]  Epoch: 86	Loss: 0.071245	Acc: 8.2% (825/10000)
[Test]  Epoch: 87	Loss: 0.070988	Acc: 8.0% (804/10000)
[Test]  Epoch: 88	Loss: 0.071065	Acc: 8.3% (831/10000)
[Test]  Epoch: 89	Loss: 0.070910	Acc: 8.0% (796/10000)
[Test]  Epoch: 90	Loss: 0.070914	Acc: 8.4% (838/10000)
[Test]  Epoch: 91	Loss: 0.071204	Acc: 8.5% (847/10000)
[Test]  Epoch: 92	Loss: 0.070802	Acc: 8.4% (838/10000)
[Test]  Epoch: 93	Loss: 0.071051	Acc: 8.5% (847/10000)
[Test]  Epoch: 94	Loss: 0.070984	Acc: 8.3% (831/10000)
[Test]  Epoch: 95	Loss: 0.071058	Acc: 8.3% (832/10000)
[Test]  Epoch: 96	Loss: 0.071341	Acc: 8.0% (804/10000)
[Test]  Epoch: 97	Loss: 0.070761	Acc: 8.3% (832/10000)
[Test]  Epoch: 98	Loss: 0.071242	Acc: 8.6% (855/10000)
[Test]  Epoch: 99	Loss: 0.071178	Acc: 8.2% (818/10000)
[Test]  Epoch: 100	Loss: 0.071125	Acc: 8.0% (799/10000)
===========finish==========
['2024-08-18', '19:17:06.033897', '100', 'test', '0.07112454850673676', '7.99', '8.55']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.7 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=18
get_sample_layers not_random
18 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.511824	Acc: 0.5% (48/10000)
[Test]  Epoch: 2	Loss: 0.089459	Acc: 0.4% (44/10000)
[Test]  Epoch: 3	Loss: 0.087853	Acc: 0.5% (51/10000)
[Test]  Epoch: 4	Loss: 0.084366	Acc: 0.5% (49/10000)
[Test]  Epoch: 5	Loss: 0.085574	Acc: 0.6% (55/10000)
[Test]  Epoch: 6	Loss: 0.083383	Acc: 0.6% (59/10000)
[Test]  Epoch: 7	Loss: 0.083578	Acc: 0.7% (66/10000)
[Test]  Epoch: 8	Loss: 0.083344	Acc: 0.7% (69/10000)
[Test]  Epoch: 9	Loss: 0.083159	Acc: 0.7% (70/10000)
[Test]  Epoch: 10	Loss: 0.085049	Acc: 0.9% (86/10000)
[Test]  Epoch: 11	Loss: 0.081757	Acc: 1.3% (130/10000)
[Test]  Epoch: 12	Loss: 0.081697	Acc: 1.7% (168/10000)
[Test]  Epoch: 13	Loss: 0.081758	Acc: 1.7% (174/10000)
[Test]  Epoch: 14	Loss: 0.080405	Acc: 2.0% (199/10000)
[Test]  Epoch: 15	Loss: 0.080904	Acc: 2.2% (222/10000)
[Test]  Epoch: 16	Loss: 0.080407	Acc: 2.1% (210/10000)
[Test]  Epoch: 17	Loss: 0.079940	Acc: 2.5% (255/10000)
[Test]  Epoch: 18	Loss: 0.081507	Acc: 2.4% (237/10000)
[Test]  Epoch: 19	Loss: 0.084073	Acc: 2.6% (259/10000)
[Test]  Epoch: 20	Loss: 0.078942	Acc: 2.7% (272/10000)
[Test]  Epoch: 21	Loss: 0.079228	Acc: 2.8% (282/10000)
[Test]  Epoch: 22	Loss: 0.078763	Acc: 3.0% (297/10000)
[Test]  Epoch: 23	Loss: 0.079490	Acc: 2.7% (266/10000)
[Test]  Epoch: 24	Loss: 0.078638	Acc: 3.2% (320/10000)
[Test]  Epoch: 25	Loss: 0.077905	Acc: 3.5% (353/10000)
[Test]  Epoch: 26	Loss: 0.077737	Acc: 3.8% (376/10000)
[Test]  Epoch: 27	Loss: 0.078883	Acc: 3.5% (354/10000)
[Test]  Epoch: 28	Loss: 0.079583	Acc: 2.8% (275/10000)
[Test]  Epoch: 29	Loss: 0.078555	Acc: 3.5% (347/10000)
[Test]  Epoch: 30	Loss: 0.078390	Acc: 3.6% (359/10000)
[Test]  Epoch: 31	Loss: 0.078052	Acc: 3.7% (368/10000)
[Test]  Epoch: 32	Loss: 0.077858	Acc: 4.0% (399/10000)
[Test]  Epoch: 33	Loss: 0.077307	Acc: 3.9% (391/10000)
[Test]  Epoch: 34	Loss: 0.076742	Acc: 4.2% (416/10000)
[Test]  Epoch: 35	Loss: 0.078835	Acc: 3.5% (352/10000)
[Test]  Epoch: 36	Loss: 0.078435	Acc: 3.9% (390/10000)
[Test]  Epoch: 37	Loss: 0.077031	Acc: 3.7% (366/10000)
[Test]  Epoch: 38	Loss: 0.077104	Acc: 4.3% (426/10000)
[Test]  Epoch: 39	Loss: 0.077897	Acc: 4.2% (423/10000)
[Test]  Epoch: 40	Loss: 0.077486	Acc: 4.2% (415/10000)
[Test]  Epoch: 41	Loss: 0.076881	Acc: 3.9% (394/10000)
[Test]  Epoch: 42	Loss: 0.077644	Acc: 3.8% (377/10000)
[Test]  Epoch: 43	Loss: 0.076112	Acc: 4.9% (491/10000)
[Test]  Epoch: 44	Loss: 0.075292	Acc: 5.0% (499/10000)
[Test]  Epoch: 45	Loss: 0.078607	Acc: 4.0% (401/10000)
[Test]  Epoch: 46	Loss: 0.076784	Acc: 4.1% (409/10000)
[Test]  Epoch: 47	Loss: 0.075943	Acc: 4.8% (477/10000)
[Test]  Epoch: 48	Loss: 0.076604	Acc: 4.9% (486/10000)
[Test]  Epoch: 49	Loss: 0.078476	Acc: 4.1% (406/10000)
[Test]  Epoch: 50	Loss: 0.076676	Acc: 4.8% (476/10000)
[Test]  Epoch: 51	Loss: 0.077215	Acc: 4.7% (470/10000)
[Test]  Epoch: 52	Loss: 0.079482	Acc: 4.3% (429/10000)
[Test]  Epoch: 53	Loss: 0.079688	Acc: 3.8% (378/10000)
[Test]  Epoch: 54	Loss: 0.077992	Acc: 4.5% (451/10000)
[Test]  Epoch: 55	Loss: 0.077121	Acc: 5.0% (503/10000)
[Test]  Epoch: 56	Loss: 0.076987	Acc: 5.1% (513/10000)
[Test]  Epoch: 57	Loss: 0.079032	Acc: 4.0% (403/10000)
[Test]  Epoch: 58	Loss: 0.076788	Acc: 5.2% (518/10000)
[Test]  Epoch: 59	Loss: 0.075890	Acc: 5.2% (515/10000)
[Test]  Epoch: 60	Loss: 0.077444	Acc: 5.2% (518/10000)
[Test]  Epoch: 61	Loss: 0.073136	Acc: 7.1% (713/10000)
[Test]  Epoch: 62	Loss: 0.072535	Acc: 7.4% (740/10000)
[Test]  Epoch: 63	Loss: 0.072625	Acc: 7.0% (703/10000)
[Test]  Epoch: 64	Loss: 0.072255	Acc: 7.7% (772/10000)
[Test]  Epoch: 65	Loss: 0.072249	Acc: 7.0% (705/10000)
[Test]  Epoch: 66	Loss: 0.072295	Acc: 7.2% (723/10000)
[Test]  Epoch: 67	Loss: 0.072347	Acc: 7.0% (701/10000)
[Test]  Epoch: 68	Loss: 0.072214	Acc: 7.2% (719/10000)
[Test]  Epoch: 69	Loss: 0.072048	Acc: 7.6% (757/10000)
[Test]  Epoch: 70	Loss: 0.071843	Acc: 7.2% (717/10000)
[Test]  Epoch: 71	Loss: 0.072055	Acc: 7.3% (735/10000)
[Test]  Epoch: 72	Loss: 0.072103	Acc: 7.4% (744/10000)
[Test]  Epoch: 73	Loss: 0.071981	Acc: 7.5% (748/10000)
[Test]  Epoch: 74	Loss: 0.072034	Acc: 7.4% (744/10000)
[Test]  Epoch: 75	Loss: 0.071884	Acc: 7.6% (763/10000)
[Test]  Epoch: 76	Loss: 0.071991	Acc: 7.6% (763/10000)
[Test]  Epoch: 77	Loss: 0.072267	Acc: 7.5% (748/10000)
[Test]  Epoch: 78	Loss: 0.072053	Acc: 7.4% (739/10000)
[Test]  Epoch: 79	Loss: 0.072075	Acc: 7.6% (758/10000)
[Test]  Epoch: 80	Loss: 0.071587	Acc: 8.2% (819/10000)
[Test]  Epoch: 81	Loss: 0.072035	Acc: 7.7% (765/10000)
[Test]  Epoch: 82	Loss: 0.071674	Acc: 7.8% (778/10000)
[Test]  Epoch: 83	Loss: 0.072109	Acc: 7.5% (752/10000)
[Test]  Epoch: 84	Loss: 0.071957	Acc: 7.6% (759/10000)
[Test]  Epoch: 85	Loss: 0.071915	Acc: 7.6% (761/10000)
[Test]  Epoch: 86	Loss: 0.071734	Acc: 7.7% (771/10000)
[Test]  Epoch: 87	Loss: 0.071805	Acc: 8.0% (796/10000)
[Test]  Epoch: 88	Loss: 0.071644	Acc: 7.9% (788/10000)
[Test]  Epoch: 89	Loss: 0.071722	Acc: 7.3% (734/10000)
[Test]  Epoch: 90	Loss: 0.071804	Acc: 8.0% (795/10000)
[Test]  Epoch: 91	Loss: 0.071858	Acc: 7.8% (785/10000)
[Test]  Epoch: 92	Loss: 0.071834	Acc: 7.9% (790/10000)
[Test]  Epoch: 93	Loss: 0.071796	Acc: 8.2% (821/10000)
[Test]  Epoch: 94	Loss: 0.071758	Acc: 8.1% (806/10000)
[Test]  Epoch: 95	Loss: 0.072010	Acc: 8.1% (812/10000)
[Test]  Epoch: 96	Loss: 0.072247	Acc: 7.8% (782/10000)
[Test]  Epoch: 97	Loss: 0.071711	Acc: 7.8% (783/10000)
[Test]  Epoch: 98	Loss: 0.072021	Acc: 7.6% (760/10000)
[Test]  Epoch: 99	Loss: 0.072070	Acc: 7.6% (762/10000)
[Test]  Epoch: 100	Loss: 0.072192	Acc: 7.6% (764/10000)
===========finish==========
['2024-08-18', '19:19:53.311996', '100', 'test', '0.07219153430461883', '7.64', '8.21']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.8 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=21
get_sample_layers not_random
21 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.254663	Acc: 0.6% (59/10000)
[Test]  Epoch: 2	Loss: 0.088904	Acc: 0.8% (83/10000)
[Test]  Epoch: 3	Loss: 0.084787	Acc: 0.7% (67/10000)
[Test]  Epoch: 4	Loss: 0.083923	Acc: 0.9% (90/10000)
[Test]  Epoch: 5	Loss: 0.082581	Acc: 0.9% (95/10000)
[Test]  Epoch: 6	Loss: 0.083862	Acc: 1.1% (108/10000)
[Test]  Epoch: 7	Loss: 0.082299	Acc: 1.2% (123/10000)
[Test]  Epoch: 8	Loss: 0.081929	Acc: 1.4% (137/10000)
[Test]  Epoch: 9	Loss: 0.081277	Acc: 1.9% (187/10000)
[Test]  Epoch: 10	Loss: 0.081193	Acc: 2.0% (200/10000)
[Test]  Epoch: 11	Loss: 0.079690	Acc: 2.1% (211/10000)
[Test]  Epoch: 12	Loss: 0.079215	Acc: 2.3% (229/10000)
[Test]  Epoch: 13	Loss: 0.080725	Acc: 2.5% (252/10000)
[Test]  Epoch: 14	Loss: 0.079029	Acc: 2.3% (232/10000)
[Test]  Epoch: 15	Loss: 0.078811	Acc: 2.4% (238/10000)
[Test]  Epoch: 16	Loss: 0.080472	Acc: 2.1% (212/10000)
[Test]  Epoch: 17	Loss: 0.080786	Acc: 2.5% (247/10000)
[Test]  Epoch: 18	Loss: 0.085883	Acc: 2.6% (260/10000)
[Test]  Epoch: 19	Loss: 0.078353	Acc: 3.2% (318/10000)
[Test]  Epoch: 20	Loss: 0.078430	Acc: 2.9% (288/10000)
[Test]  Epoch: 21	Loss: 0.079190	Acc: 3.2% (319/10000)
[Test]  Epoch: 22	Loss: 0.077288	Acc: 3.4% (335/10000)
[Test]  Epoch: 23	Loss: 0.079892	Acc: 2.7% (267/10000)
[Test]  Epoch: 24	Loss: 0.077381	Acc: 3.2% (320/10000)
[Test]  Epoch: 25	Loss: 0.078569	Acc: 3.2% (319/10000)
[Test]  Epoch: 26	Loss: 0.078412	Acc: 3.6% (358/10000)
[Test]  Epoch: 27	Loss: 0.079183	Acc: 3.3% (332/10000)
[Test]  Epoch: 28	Loss: 0.077795	Acc: 3.9% (387/10000)
[Test]  Epoch: 29	Loss: 0.077174	Acc: 3.9% (388/10000)
[Test]  Epoch: 30	Loss: 0.077585	Acc: 4.2% (422/10000)
[Test]  Epoch: 31	Loss: 0.077749	Acc: 4.0% (395/10000)
[Test]  Epoch: 32	Loss: 0.076449	Acc: 4.3% (430/10000)
[Test]  Epoch: 33	Loss: 0.077725	Acc: 4.1% (410/10000)
[Test]  Epoch: 34	Loss: 0.076526	Acc: 4.1% (408/10000)
[Test]  Epoch: 35	Loss: 0.079070	Acc: 3.1% (307/10000)
[Test]  Epoch: 36	Loss: 0.076294	Acc: 4.8% (478/10000)
[Test]  Epoch: 37	Loss: 0.077694	Acc: 4.0% (395/10000)
[Test]  Epoch: 38	Loss: 0.076032	Acc: 4.3% (434/10000)
[Test]  Epoch: 39	Loss: 0.076615	Acc: 5.1% (507/10000)
[Test]  Epoch: 40	Loss: 0.075986	Acc: 5.1% (514/10000)
[Test]  Epoch: 41	Loss: 0.077669	Acc: 3.9% (389/10000)
[Test]  Epoch: 42	Loss: 0.077370	Acc: 4.5% (450/10000)
[Test]  Epoch: 43	Loss: 0.078140	Acc: 4.8% (483/10000)
[Test]  Epoch: 44	Loss: 0.079062	Acc: 4.7% (468/10000)
[Test]  Epoch: 45	Loss: 0.079614	Acc: 3.6% (357/10000)
[Test]  Epoch: 46	Loss: 0.076456	Acc: 4.7% (465/10000)
[Test]  Epoch: 47	Loss: 0.076079	Acc: 5.2% (516/10000)
[Test]  Epoch: 48	Loss: 0.074261	Acc: 5.5% (545/10000)
[Test]  Epoch: 49	Loss: 0.075620	Acc: 5.2% (516/10000)
[Test]  Epoch: 50	Loss: 0.075887	Acc: 4.8% (479/10000)
[Test]  Epoch: 51	Loss: 0.075772	Acc: 5.1% (507/10000)
[Test]  Epoch: 52	Loss: 0.075237	Acc: 5.4% (544/10000)
[Test]  Epoch: 53	Loss: 0.075211	Acc: 5.0% (500/10000)
[Test]  Epoch: 54	Loss: 0.074144	Acc: 5.5% (547/10000)
[Test]  Epoch: 55	Loss: 0.075566	Acc: 5.1% (510/10000)
[Test]  Epoch: 56	Loss: 0.075973	Acc: 5.0% (497/10000)
[Test]  Epoch: 57	Loss: 0.077017	Acc: 5.0% (504/10000)
[Test]  Epoch: 58	Loss: 0.076958	Acc: 4.6% (462/10000)
[Test]  Epoch: 59	Loss: 0.075701	Acc: 5.2% (515/10000)
[Test]  Epoch: 60	Loss: 0.076401	Acc: 5.4% (543/10000)
[Test]  Epoch: 61	Loss: 0.072338	Acc: 7.2% (718/10000)
[Test]  Epoch: 62	Loss: 0.071986	Acc: 7.3% (731/10000)
[Test]  Epoch: 63	Loss: 0.072216	Acc: 7.2% (721/10000)
[Test]  Epoch: 64	Loss: 0.071866	Acc: 7.5% (745/10000)
[Test]  Epoch: 65	Loss: 0.071919	Acc: 7.3% (730/10000)
[Test]  Epoch: 66	Loss: 0.071589	Acc: 7.7% (765/10000)
[Test]  Epoch: 67	Loss: 0.071764	Acc: 7.6% (758/10000)
[Test]  Epoch: 68	Loss: 0.071756	Acc: 7.1% (712/10000)
[Test]  Epoch: 69	Loss: 0.071581	Acc: 7.8% (780/10000)
[Test]  Epoch: 70	Loss: 0.071510	Acc: 7.7% (765/10000)
[Test]  Epoch: 71	Loss: 0.071596	Acc: 7.4% (744/10000)
[Test]  Epoch: 72	Loss: 0.071583	Acc: 7.6% (757/10000)
[Test]  Epoch: 73	Loss: 0.071483	Acc: 7.7% (768/10000)
[Test]  Epoch: 74	Loss: 0.071760	Acc: 7.7% (765/10000)
[Test]  Epoch: 75	Loss: 0.071557	Acc: 7.8% (782/10000)
[Test]  Epoch: 76	Loss: 0.071439	Acc: 8.0% (801/10000)
[Test]  Epoch: 77	Loss: 0.071706	Acc: 8.0% (798/10000)
[Test]  Epoch: 78	Loss: 0.071705	Acc: 7.8% (775/10000)
[Test]  Epoch: 79	Loss: 0.071697	Acc: 7.6% (762/10000)
[Test]  Epoch: 80	Loss: 0.070907	Acc: 8.2% (818/10000)
[Test]  Epoch: 81	Loss: 0.071657	Acc: 7.7% (770/10000)
[Test]  Epoch: 82	Loss: 0.071411	Acc: 7.8% (776/10000)
[Test]  Epoch: 83	Loss: 0.071765	Acc: 7.9% (786/10000)
[Test]  Epoch: 84	Loss: 0.071650	Acc: 7.7% (771/10000)
[Test]  Epoch: 85	Loss: 0.071516	Acc: 7.8% (776/10000)
[Test]  Epoch: 86	Loss: 0.071538	Acc: 7.8% (781/10000)
[Test]  Epoch: 87	Loss: 0.071815	Acc: 7.4% (741/10000)
[Test]  Epoch: 88	Loss: 0.071009	Acc: 8.2% (815/10000)
[Test]  Epoch: 89	Loss: 0.071412	Acc: 7.6% (759/10000)
[Test]  Epoch: 90	Loss: 0.071518	Acc: 7.5% (752/10000)
[Test]  Epoch: 91	Loss: 0.071539	Acc: 7.8% (783/10000)
[Test]  Epoch: 92	Loss: 0.071559	Acc: 7.9% (788/10000)
[Test]  Epoch: 93	Loss: 0.071673	Acc: 8.0% (795/10000)
[Test]  Epoch: 94	Loss: 0.071661	Acc: 8.1% (813/10000)
[Test]  Epoch: 95	Loss: 0.071711	Acc: 8.1% (814/10000)
[Test]  Epoch: 96	Loss: 0.072069	Acc: 7.8% (776/10000)
[Test]  Epoch: 97	Loss: 0.071428	Acc: 8.0% (796/10000)
[Test]  Epoch: 98	Loss: 0.071847	Acc: 7.7% (769/10000)
[Test]  Epoch: 99	Loss: 0.071740	Acc: 7.6% (756/10000)
[Test]  Epoch: 100	Loss: 0.071676	Acc: 7.9% (792/10000)
===========finish==========
['2024-08-18', '19:22:46.109756', '100', 'test', '0.07167626941204071', '7.92', '8.18']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 0.9 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=24
get_sample_layers not_random
24 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.462191	Acc: 0.5% (50/10000)
[Test]  Epoch: 2	Loss: 0.085207	Acc: 1.3% (129/10000)
[Test]  Epoch: 3	Loss: 0.086348	Acc: 1.6% (157/10000)
[Test]  Epoch: 4	Loss: 0.080804	Acc: 1.5% (151/10000)
[Test]  Epoch: 5	Loss: 0.082910	Acc: 1.8% (180/10000)
[Test]  Epoch: 6	Loss: 0.080421	Acc: 2.3% (228/10000)
[Test]  Epoch: 7	Loss: 0.079532	Acc: 2.1% (211/10000)
[Test]  Epoch: 8	Loss: 0.082076	Acc: 2.0% (197/10000)
[Test]  Epoch: 9	Loss: 0.079044	Acc: 2.4% (243/10000)
[Test]  Epoch: 10	Loss: 0.080604	Acc: 2.3% (230/10000)
[Test]  Epoch: 11	Loss: 0.082276	Acc: 2.2% (219/10000)
[Test]  Epoch: 12	Loss: 0.079162	Acc: 3.1% (306/10000)
[Test]  Epoch: 13	Loss: 0.077795	Acc: 3.1% (308/10000)
[Test]  Epoch: 14	Loss: 0.079729	Acc: 3.0% (302/10000)
[Test]  Epoch: 15	Loss: 0.077209	Acc: 3.4% (339/10000)
[Test]  Epoch: 16	Loss: 0.082085	Acc: 2.8% (280/10000)
[Test]  Epoch: 17	Loss: 0.081633	Acc: 2.6% (260/10000)
[Test]  Epoch: 18	Loss: 0.078445	Acc: 3.5% (347/10000)
[Test]  Epoch: 19	Loss: 0.077850	Acc: 3.7% (371/10000)
[Test]  Epoch: 20	Loss: 0.077510	Acc: 4.0% (396/10000)
[Test]  Epoch: 21	Loss: 0.078718	Acc: 4.1% (409/10000)
[Test]  Epoch: 22	Loss: 0.078959	Acc: 3.9% (385/10000)
[Test]  Epoch: 23	Loss: 0.076930	Acc: 4.1% (408/10000)
[Test]  Epoch: 24	Loss: 0.076223	Acc: 4.2% (424/10000)
[Test]  Epoch: 25	Loss: 0.078587	Acc: 3.9% (394/10000)
[Test]  Epoch: 26	Loss: 0.076826	Acc: 4.4% (438/10000)
[Test]  Epoch: 27	Loss: 0.077664	Acc: 4.5% (450/10000)
[Test]  Epoch: 28	Loss: 0.079305	Acc: 3.4% (338/10000)
[Test]  Epoch: 29	Loss: 0.076387	Acc: 4.4% (440/10000)
[Test]  Epoch: 30	Loss: 0.076600	Acc: 4.4% (436/10000)
[Test]  Epoch: 31	Loss: 0.078364	Acc: 3.5% (346/10000)
[Test]  Epoch: 32	Loss: 0.076135	Acc: 4.9% (494/10000)
[Test]  Epoch: 33	Loss: 0.075937	Acc: 4.6% (456/10000)
[Test]  Epoch: 34	Loss: 0.076498	Acc: 4.9% (490/10000)
[Test]  Epoch: 35	Loss: 0.077430	Acc: 3.9% (394/10000)
[Test]  Epoch: 36	Loss: 0.075751	Acc: 5.3% (526/10000)
[Test]  Epoch: 37	Loss: 0.077517	Acc: 4.5% (445/10000)
[Test]  Epoch: 38	Loss: 0.076668	Acc: 4.6% (456/10000)
[Test]  Epoch: 39	Loss: 0.076395	Acc: 4.9% (486/10000)
[Test]  Epoch: 40	Loss: 0.077589	Acc: 4.5% (453/10000)
[Test]  Epoch: 41	Loss: 0.075699	Acc: 4.9% (493/10000)
[Test]  Epoch: 42	Loss: 0.076419	Acc: 4.7% (468/10000)
[Test]  Epoch: 43	Loss: 0.075867	Acc: 4.8% (480/10000)
[Test]  Epoch: 44	Loss: 0.076790	Acc: 5.2% (523/10000)
[Test]  Epoch: 45	Loss: 0.080481	Acc: 4.5% (446/10000)
[Test]  Epoch: 46	Loss: 0.075696	Acc: 5.5% (549/10000)
[Test]  Epoch: 47	Loss: 0.075737	Acc: 5.2% (524/10000)
[Test]  Epoch: 48	Loss: 0.075086	Acc: 5.6% (556/10000)
[Test]  Epoch: 49	Loss: 0.076179	Acc: 4.9% (491/10000)
[Test]  Epoch: 50	Loss: 0.079240	Acc: 4.6% (456/10000)
[Test]  Epoch: 51	Loss: 0.077766	Acc: 4.8% (479/10000)
[Test]  Epoch: 52	Loss: 0.078855	Acc: 4.8% (482/10000)
[Test]  Epoch: 53	Loss: 0.075936	Acc: 5.4% (537/10000)
[Test]  Epoch: 54	Loss: 0.079352	Acc: 4.5% (446/10000)
[Test]  Epoch: 55	Loss: 0.078007	Acc: 5.0% (496/10000)
[Test]  Epoch: 56	Loss: 0.075533	Acc: 5.7% (566/10000)
[Test]  Epoch: 57	Loss: 0.077109	Acc: 4.9% (493/10000)
[Test]  Epoch: 58	Loss: 0.076228	Acc: 5.4% (536/10000)
[Test]  Epoch: 59	Loss: 0.077411	Acc: 5.3% (526/10000)
[Test]  Epoch: 60	Loss: 0.077454	Acc: 5.5% (546/10000)
[Test]  Epoch: 61	Loss: 0.072456	Acc: 7.1% (711/10000)
[Test]  Epoch: 62	Loss: 0.072232	Acc: 7.4% (742/10000)
[Test]  Epoch: 63	Loss: 0.072082	Acc: 7.6% (764/10000)
[Test]  Epoch: 64	Loss: 0.071995	Acc: 7.7% (771/10000)
[Test]  Epoch: 65	Loss: 0.072011	Acc: 7.0% (705/10000)
[Test]  Epoch: 66	Loss: 0.071689	Acc: 7.8% (782/10000)
[Test]  Epoch: 67	Loss: 0.071892	Acc: 7.5% (745/10000)
[Test]  Epoch: 68	Loss: 0.071784	Acc: 7.1% (714/10000)
[Test]  Epoch: 69	Loss: 0.071708	Acc: 7.9% (788/10000)
[Test]  Epoch: 70	Loss: 0.071505	Acc: 8.0% (797/10000)
[Test]  Epoch: 71	Loss: 0.071816	Acc: 7.4% (742/10000)
[Test]  Epoch: 72	Loss: 0.071714	Acc: 7.9% (786/10000)
[Test]  Epoch: 73	Loss: 0.071447	Acc: 8.3% (826/10000)
[Test]  Epoch: 74	Loss: 0.071906	Acc: 7.9% (791/10000)
[Test]  Epoch: 75	Loss: 0.071492	Acc: 8.0% (801/10000)
[Test]  Epoch: 76	Loss: 0.071531	Acc: 7.9% (790/10000)
[Test]  Epoch: 77	Loss: 0.071812	Acc: 7.7% (766/10000)
[Test]  Epoch: 78	Loss: 0.071889	Acc: 7.9% (790/10000)
[Test]  Epoch: 79	Loss: 0.071791	Acc: 7.9% (791/10000)
[Test]  Epoch: 80	Loss: 0.071346	Acc: 8.0% (800/10000)
[Test]  Epoch: 81	Loss: 0.071901	Acc: 7.6% (760/10000)
[Test]  Epoch: 82	Loss: 0.071412	Acc: 7.7% (771/10000)
[Test]  Epoch: 83	Loss: 0.071953	Acc: 7.8% (775/10000)
[Test]  Epoch: 84	Loss: 0.071738	Acc: 7.8% (776/10000)
[Test]  Epoch: 85	Loss: 0.071718	Acc: 8.0% (803/10000)
[Test]  Epoch: 86	Loss: 0.071891	Acc: 7.9% (793/10000)
[Test]  Epoch: 87	Loss: 0.071575	Acc: 7.9% (790/10000)
[Test]  Epoch: 88	Loss: 0.071399	Acc: 8.1% (806/10000)
[Test]  Epoch: 89	Loss: 0.071561	Acc: 7.8% (780/10000)
[Test]  Epoch: 90	Loss: 0.071455	Acc: 8.1% (807/10000)
[Test]  Epoch: 91	Loss: 0.071784	Acc: 8.2% (818/10000)
[Test]  Epoch: 92	Loss: 0.071607	Acc: 8.0% (799/10000)
[Test]  Epoch: 93	Loss: 0.071824	Acc: 7.8% (776/10000)
[Test]  Epoch: 94	Loss: 0.071680	Acc: 7.9% (793/10000)
[Test]  Epoch: 95	Loss: 0.071794	Acc: 8.1% (808/10000)
[Test]  Epoch: 96	Loss: 0.071919	Acc: 7.8% (784/10000)
[Test]  Epoch: 97	Loss: 0.071351	Acc: 8.3% (827/10000)
[Test]  Epoch: 98	Loss: 0.072039	Acc: 7.9% (787/10000)
[Test]  Epoch: 99	Loss: 0.071831	Acc: 7.8% (776/10000)
[Test]  Epoch: 100	Loss: 0.071777	Acc: 8.0% (799/10000)
===========finish==========
['2024-08-18', '19:25:45.642059', '100', 'test', '0.07177660949230194', '7.99', '8.27']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-vgg16_bn-channel vgg16_bn TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-vgg16_bn --protect_percent 1 --channel_percent -1
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.vgg16_bn
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=27  num_select=27
get_sample_layers not_random
27 ['features.0.weight', 'features.1.weight', 'features.3.weight', 'features.4.weight', 'features.7.weight', 'features.8.weight', 'features.10.weight', 'features.11.weight', 'features.14.weight', 'features.15.weight', 'features.17.weight', 'features.18.weight', 'features.20.weight', 'features.21.weight', 'features.24.weight', 'features.25.weight', 'features.27.weight', 'features.28.weight', 'features.30.weight', 'features.31.weight', 'features.34.weight', 'features.35.weight', 'features.37.weight', 'features.38.weight', 'features.40.weight', 'features.41.weight', 'classifier.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.823884	Acc: 0.5% (52/10000)
[Test]  Epoch: 2	Loss: 0.086873	Acc: 0.9% (93/10000)
[Test]  Epoch: 3	Loss: 0.082996	Acc: 1.5% (147/10000)
[Test]  Epoch: 4	Loss: 0.084391	Acc: 1.5% (150/10000)
[Test]  Epoch: 5	Loss: 0.081115	Acc: 1.6% (163/10000)
[Test]  Epoch: 6	Loss: 0.083752	Acc: 1.9% (194/10000)
[Test]  Epoch: 7	Loss: 0.081230	Acc: 1.6% (156/10000)
[Test]  Epoch: 8	Loss: 0.080882	Acc: 1.9% (191/10000)
[Test]  Epoch: 9	Loss: 0.079946	Acc: 2.5% (246/10000)
[Test]  Epoch: 10	Loss: 0.080223	Acc: 2.2% (220/10000)
[Test]  Epoch: 11	Loss: 0.078989	Acc: 2.6% (257/10000)
[Test]  Epoch: 12	Loss: 0.080979	Acc: 2.4% (239/10000)
[Test]  Epoch: 13	Loss: 0.079209	Acc: 2.7% (269/10000)
[Test]  Epoch: 14	Loss: 0.079569	Acc: 2.8% (275/10000)
[Test]  Epoch: 15	Loss: 0.079295	Acc: 2.4% (238/10000)
[Test]  Epoch: 16	Loss: 0.079289	Acc: 2.8% (275/10000)
[Test]  Epoch: 17	Loss: 0.079417	Acc: 2.6% (258/10000)
[Test]  Epoch: 18	Loss: 0.081976	Acc: 3.0% (302/10000)
[Test]  Epoch: 19	Loss: 0.078985	Acc: 2.9% (287/10000)
[Test]  Epoch: 20	Loss: 0.078649	Acc: 3.1% (309/10000)
[Test]  Epoch: 21	Loss: 0.079421	Acc: 3.4% (336/10000)
[Test]  Epoch: 22	Loss: 0.077111	Acc: 3.7% (367/10000)
[Test]  Epoch: 23	Loss: 0.078349	Acc: 3.4% (337/10000)
[Test]  Epoch: 24	Loss: 0.078658	Acc: 3.3% (329/10000)
[Test]  Epoch: 25	Loss: 0.079001	Acc: 2.8% (280/10000)
[Test]  Epoch: 26	Loss: 0.078626	Acc: 3.6% (360/10000)
[Test]  Epoch: 27	Loss: 0.077105	Acc: 4.0% (397/10000)
[Test]  Epoch: 28	Loss: 0.080569	Acc: 2.7% (274/10000)
[Test]  Epoch: 29	Loss: 0.078093	Acc: 3.3% (326/10000)
[Test]  Epoch: 30	Loss: 0.077117	Acc: 3.6% (360/10000)
[Test]  Epoch: 31	Loss: 0.078463	Acc: 3.3% (331/10000)
[Test]  Epoch: 32	Loss: 0.078333	Acc: 3.6% (364/10000)
[Test]  Epoch: 33	Loss: 0.078659	Acc: 3.5% (351/10000)
[Test]  Epoch: 34	Loss: 0.078297	Acc: 3.8% (384/10000)
[Test]  Epoch: 35	Loss: 0.079514	Acc: 2.9% (287/10000)
[Test]  Epoch: 36	Loss: 0.076613	Acc: 4.2% (420/10000)
[Test]  Epoch: 37	Loss: 0.078269	Acc: 3.6% (364/10000)
[Test]  Epoch: 38	Loss: 0.077094	Acc: 4.1% (413/10000)
[Test]  Epoch: 39	Loss: 0.078455	Acc: 4.3% (432/10000)
[Test]  Epoch: 40	Loss: 0.075976	Acc: 4.4% (437/10000)
[Test]  Epoch: 41	Loss: 0.076822	Acc: 4.1% (407/10000)
[Test]  Epoch: 42	Loss: 0.078746	Acc: 3.8% (383/10000)
[Test]  Epoch: 43	Loss: 0.079452	Acc: 4.2% (423/10000)
[Test]  Epoch: 44	Loss: 0.077310	Acc: 4.5% (445/10000)
[Test]  Epoch: 45	Loss: 0.077415	Acc: 4.2% (425/10000)
[Test]  Epoch: 46	Loss: 0.076940	Acc: 4.3% (435/10000)
[Test]  Epoch: 47	Loss: 0.084638	Acc: 3.3% (333/10000)
[Test]  Epoch: 48	Loss: 0.076990	Acc: 4.6% (457/10000)
[Test]  Epoch: 49	Loss: 0.077381	Acc: 4.3% (434/10000)
[Test]  Epoch: 50	Loss: 0.078356	Acc: 3.9% (394/10000)
[Test]  Epoch: 51	Loss: 0.078972	Acc: 4.1% (407/10000)
[Test]  Epoch: 52	Loss: 0.078480	Acc: 4.3% (433/10000)
[Test]  Epoch: 53	Loss: 0.077352	Acc: 4.2% (419/10000)
[Test]  Epoch: 54	Loss: 0.080499	Acc: 3.4% (344/10000)
[Test]  Epoch: 55	Loss: 0.077128	Acc: 5.2% (517/10000)
[Test]  Epoch: 56	Loss: 0.075738	Acc: 5.0% (499/10000)
[Test]  Epoch: 57	Loss: 0.077219	Acc: 4.7% (469/10000)
[Test]  Epoch: 58	Loss: 0.077213	Acc: 4.8% (477/10000)
[Test]  Epoch: 59	Loss: 0.075869	Acc: 4.8% (483/10000)
[Test]  Epoch: 60	Loss: 0.077035	Acc: 5.1% (514/10000)
[Test]  Epoch: 61	Loss: 0.073605	Acc: 6.4% (639/10000)
[Test]  Epoch: 62	Loss: 0.073180	Acc: 6.7% (671/10000)
[Test]  Epoch: 63	Loss: 0.073262	Acc: 6.8% (683/10000)
[Test]  Epoch: 64	Loss: 0.073183	Acc: 6.9% (688/10000)
[Test]  Epoch: 65	Loss: 0.073117	Acc: 6.5% (651/10000)
[Test]  Epoch: 66	Loss: 0.072870	Acc: 6.8% (678/10000)
[Test]  Epoch: 67	Loss: 0.073162	Acc: 7.0% (700/10000)
[Test]  Epoch: 68	Loss: 0.072923	Acc: 6.6% (661/10000)
[Test]  Epoch: 69	Loss: 0.073256	Acc: 6.8% (679/10000)
[Test]  Epoch: 70	Loss: 0.072921	Acc: 6.9% (691/10000)
[Test]  Epoch: 71	Loss: 0.073332	Acc: 6.7% (667/10000)
[Test]  Epoch: 72	Loss: 0.073193	Acc: 6.6% (663/10000)
[Test]  Epoch: 73	Loss: 0.073036	Acc: 7.0% (701/10000)
[Test]  Epoch: 74	Loss: 0.073108	Acc: 7.0% (701/10000)
[Test]  Epoch: 75	Loss: 0.072911	Acc: 7.1% (711/10000)
[Test]  Epoch: 76	Loss: 0.072955	Acc: 7.1% (711/10000)
[Test]  Epoch: 77	Loss: 0.073159	Acc: 6.9% (689/10000)
[Test]  Epoch: 78	Loss: 0.073263	Acc: 7.1% (713/10000)
[Test]  Epoch: 79	Loss: 0.073106	Acc: 7.0% (698/10000)
[Test]  Epoch: 80	Loss: 0.072451	Acc: 7.6% (760/10000)
[Test]  Epoch: 81	Loss: 0.072910	Acc: 6.9% (689/10000)
[Test]  Epoch: 82	Loss: 0.072749	Acc: 6.8% (675/10000)
[Test]  Epoch: 83	Loss: 0.073242	Acc: 6.9% (688/10000)
[Test]  Epoch: 84	Loss: 0.073159	Acc: 6.9% (687/10000)
[Test]  Epoch: 85	Loss: 0.073143	Acc: 7.4% (737/10000)
[Test]  Epoch: 86	Loss: 0.073118	Acc: 6.9% (694/10000)
[Test]  Epoch: 87	Loss: 0.072979	Acc: 7.0% (695/10000)
[Test]  Epoch: 88	Loss: 0.072334	Acc: 7.8% (778/10000)
[Test]  Epoch: 89	Loss: 0.072649	Acc: 7.0% (697/10000)
[Test]  Epoch: 90	Loss: 0.072864	Acc: 7.2% (722/10000)
[Test]  Epoch: 91	Loss: 0.072854	Acc: 7.1% (713/10000)
[Test]  Epoch: 92	Loss: 0.072665	Acc: 7.3% (730/10000)
[Test]  Epoch: 93	Loss: 0.072712	Acc: 7.3% (727/10000)
[Test]  Epoch: 94	Loss: 0.072910	Acc: 7.3% (731/10000)
[Test]  Epoch: 95	Loss: 0.072984	Acc: 7.4% (738/10000)
[Test]  Epoch: 96	Loss: 0.073075	Acc: 6.9% (691/10000)
[Test]  Epoch: 97	Loss: 0.072727	Acc: 7.3% (730/10000)
[Test]  Epoch: 98	Loss: 0.073271	Acc: 6.8% (681/10000)
[Test]  Epoch: 99	Loss: 0.072878	Acc: 7.3% (727/10000)
[Test]  Epoch: 100	Loss: 0.073039	Acc: 7.0% (705/10000)
===========finish==========
['2024-08-18', '19:28:33.101527', '100', 'test', '0.07303935923576355', '7.05', '7.78']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info does not exist. Calculating...
Load model from models/victim/tinyimagenet200-mobilenetv2/checkpoint.pth.tar
--------------------------------------------
sorted_importances: [('_features.17.conv.0.0.weight', 141.13685607910156), ('_features.17.conv.2.weight', 126.27576446533203), ('_features.2.conv.2.weight', 116.34464263916016), ('_features.14.conv.2.weight', 115.59678649902344), ('_features.7.conv.2.weight', 114.58633422851562), ('_features.14.conv.0.0.weight', 109.83724975585938), ('_features.4.conv.2.weight', 109.06727600097656), ('_features.11.conv.2.weight', 104.99269104003906), ('_features.18.0.weight', 95.09846496582031), ('_features.4.conv.0.0.weight', 88.39439392089844), ('last_linear.weight', 79.20014953613281), ('_features.11.conv.0.0.weight', 76.93975830078125), ('_features.2.conv.1.0.weight', 74.13568115234375), ('_features.1.conv.1.weight', 73.980712890625), ('_features.7.conv.0.0.weight', 72.94187927246094), ('_features.1.conv.0.0.weight', 70.01956939697266), ('_features.0.0.weight', 60.88936233520508), ('_features.2.conv.0.1.weight', 59.124542236328125), ('_features.2.conv.0.0.weight', 58.642818450927734), ('_features.3.conv.2.weight', 51.93452072143555), ('_features.4.conv.0.1.weight', 45.35820007324219), ('_features.4.conv.1.0.weight', 44.600852966308594), ('_features.2.conv.1.1.weight', 41.79132080078125), ('_features.11.conv.1.0.weight', 41.509979248046875), ('_features.1.conv.0.1.weight', 39.725502014160156), ('_features.3.conv.1.0.weight', 38.535728454589844), ('_features.1.conv.2.weight', 37.12483215332031), ('_features.7.conv.0.1.weight', 35.774208068847656), ('_features.7.conv.1.0.weight', 34.8358039855957), ('_features.2.conv.3.weight', 31.838953018188477), ('_features.14.conv.0.1.weight', 31.5360050201416), ('_features.4.conv.1.1.weight', 31.27114486694336), ('_features.17.conv.3.weight', 31.261598587036133), ('_features.11.conv.0.1.weight', 30.50000762939453), ('_features.3.conv.0.0.weight', 28.928550720214844), ('_features.14.conv.1.0.weight', 28.289386749267578), ('_features.0.1.weight', 28.013532638549805), ('_features.17.conv.1.0.weight', 27.00010108947754), ('_features.7.conv.1.1.weight', 25.930023193359375), ('_features.11.conv.1.1.weight', 25.311203002929688), ('_features.14.conv.1.1.weight', 25.03226089477539), ('_features.6.conv.2.weight', 22.8952693939209), ('_features.5.conv.2.weight', 22.685752868652344), ('_features.17.conv.1.1.weight', 22.579500198364258), ('_features.17.conv.0.1.weight', 21.943132400512695), ('_features.13.conv.0.0.weight', 20.65536117553711), ('_features.12.conv.0.0.weight', 19.924022674560547), ('_features.12.conv.2.weight', 19.627710342407227), ('_features.13.conv.2.weight', 17.29399871826172), ('_features.5.conv.1.0.weight', 17.196189880371094), ('_features.3.conv.0.1.weight', 16.557537078857422), ('_features.5.conv.0.0.weight', 16.469356536865234), ('_features.6.conv.0.0.weight', 14.976268768310547), ('_features.4.conv.3.weight', 14.693595886230469), ('_features.6.conv.1.0.weight', 13.272452354431152), ('_features.11.conv.3.weight', 12.558784484863281), ('_features.3.conv.1.1.weight', 12.077814102172852), ('_features.14.conv.3.weight', 10.957170486450195), ('_features.8.conv.2.weight', 10.506233215332031), ('_features.8.conv.0.0.weight', 10.286831855773926), ('_features.3.conv.3.weight', 10.23135757446289), ('_features.10.conv.0.0.weight', 9.487852096557617), ('_features.9.conv.2.weight', 9.485433578491211), ('_features.9.conv.0.0.weight', 9.397378921508789), ('_features.7.conv.3.weight', 9.002188682556152), ('_features.10.conv.2.weight', 8.912237167358398), ('_features.12.conv.1.0.weight', 7.497399806976318), ('_features.5.conv.0.1.weight', 7.457813262939453), ('_features.6.conv.3.weight', 6.672262191772461), ('_features.16.conv.2.weight', 6.249387741088867), ('_features.16.conv.0.0.weight', 6.22741174697876), ('_features.6.conv.0.1.weight', 6.181740760803223), ('_features.13.conv.1.0.weight', 6.152912139892578), ('_features.5.conv.3.weight', 5.349498748779297), ('_features.8.conv.1.0.weight', 5.284809112548828), ('_features.5.conv.1.1.weight', 5.181466579437256), ('_features.12.conv.0.1.weight', 5.057946681976318), ('_features.13.conv.3.weight', 4.988582611083984), ('_features.15.conv.0.0.weight', 4.899039268493652), ('_features.15.conv.2.weight', 4.822112560272217), ('_features.13.conv.0.1.weight', 4.717461109161377), ('_features.12.conv.3.weight', 4.698787689208984), ('_features.6.conv.1.1.weight', 4.653088092803955), ('_features.9.conv.1.0.weight', 4.494771957397461), ('_features.10.conv.1.0.weight', 4.1148176193237305), ('_features.12.conv.1.1.weight', 3.7188234329223633), ('_features.13.conv.1.1.weight', 3.4613001346588135), ('_features.8.conv.3.weight', 3.163945198059082), ('_features.10.conv.3.weight', 2.9317784309387207), ('_features.10.conv.0.1.weight', 2.829403877258301), ('_features.16.conv.3.weight', 2.8145010471343994), ('_features.8.conv.0.1.weight', 2.719602346420288), ('_features.9.conv.3.weight', 2.6735680103302), ('_features.15.conv.3.weight', 2.3530328273773193), ('_features.9.conv.0.1.weight', 2.2544236183166504), ('_features.8.conv.1.1.weight', 2.1951022148132324), ('_features.10.conv.1.1.weight', 1.9528836011886597), ('_features.9.conv.1.1.weight', 1.834132432937622), ('_features.16.conv.0.1.weight', 1.3258705139160156), ('_features.15.conv.0.1.weight', 1.2093055248260498), ('_features.15.conv.1.0.weight', 0.9226931929588318), ('_features.16.conv.1.0.weight', 0.9044426679611206), ('_features.15.conv.1.1.weight', 0.7924290895462036), ('_features.16.conv.1.1.weight', 0.7734917402267456), ('_features.18.1.weight', 0.5632115006446838), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
--------------------------------------------
sorted_filtered_importances: [('_features.17.conv.0.0.weight', 141.13685607910156), ('_features.17.conv.2.weight', 126.27576446533203), ('_features.2.conv.2.weight', 116.34464263916016), ('_features.14.conv.2.weight', 115.59678649902344), ('_features.7.conv.2.weight', 114.58633422851562), ('_features.14.conv.0.0.weight', 109.83724975585938), ('_features.4.conv.2.weight', 109.06727600097656), ('_features.11.conv.2.weight', 104.99269104003906), ('_features.18.0.weight', 95.09846496582031), ('_features.4.conv.0.0.weight', 88.39439392089844), ('last_linear.weight', 79.20014953613281), ('_features.11.conv.0.0.weight', 76.93975830078125), ('_features.2.conv.1.0.weight', 74.13568115234375), ('_features.1.conv.1.weight', 73.980712890625), ('_features.7.conv.0.0.weight', 72.94187927246094), ('_features.1.conv.0.0.weight', 70.01956939697266), ('_features.0.0.weight', 60.88936233520508), ('_features.2.conv.0.1.weight', 59.124542236328125), ('_features.2.conv.0.0.weight', 58.642818450927734), ('_features.3.conv.2.weight', 51.93452072143555), ('_features.4.conv.0.1.weight', 45.35820007324219), ('_features.4.conv.1.0.weight', 44.600852966308594), ('_features.2.conv.1.1.weight', 41.79132080078125), ('_features.11.conv.1.0.weight', 41.509979248046875), ('_features.1.conv.0.1.weight', 39.725502014160156), ('_features.3.conv.1.0.weight', 38.535728454589844), ('_features.1.conv.2.weight', 37.12483215332031), ('_features.7.conv.0.1.weight', 35.774208068847656), ('_features.7.conv.1.0.weight', 34.8358039855957), ('_features.2.conv.3.weight', 31.838953018188477), ('_features.14.conv.0.1.weight', 31.5360050201416), ('_features.4.conv.1.1.weight', 31.27114486694336), ('_features.17.conv.3.weight', 31.261598587036133), ('_features.11.conv.0.1.weight', 30.50000762939453), ('_features.3.conv.0.0.weight', 28.928550720214844), ('_features.14.conv.1.0.weight', 28.289386749267578), ('_features.0.1.weight', 28.013532638549805), ('_features.17.conv.1.0.weight', 27.00010108947754), ('_features.7.conv.1.1.weight', 25.930023193359375), ('_features.11.conv.1.1.weight', 25.311203002929688), ('_features.14.conv.1.1.weight', 25.03226089477539), ('_features.6.conv.2.weight', 22.8952693939209), ('_features.5.conv.2.weight', 22.685752868652344), ('_features.17.conv.1.1.weight', 22.579500198364258), ('_features.17.conv.0.1.weight', 21.943132400512695), ('_features.13.conv.0.0.weight', 20.65536117553711), ('_features.12.conv.0.0.weight', 19.924022674560547), ('_features.12.conv.2.weight', 19.627710342407227), ('_features.13.conv.2.weight', 17.29399871826172), ('_features.5.conv.1.0.weight', 17.196189880371094), ('_features.3.conv.0.1.weight', 16.557537078857422), ('_features.5.conv.0.0.weight', 16.469356536865234), ('_features.6.conv.0.0.weight', 14.976268768310547), ('_features.4.conv.3.weight', 14.693595886230469), ('_features.6.conv.1.0.weight', 13.272452354431152), ('_features.11.conv.3.weight', 12.558784484863281), ('_features.3.conv.1.1.weight', 12.077814102172852), ('_features.14.conv.3.weight', 10.957170486450195), ('_features.8.conv.2.weight', 10.506233215332031), ('_features.8.conv.0.0.weight', 10.286831855773926), ('_features.3.conv.3.weight', 10.23135757446289), ('_features.10.conv.0.0.weight', 9.487852096557617), ('_features.9.conv.2.weight', 9.485433578491211), ('_features.9.conv.0.0.weight', 9.397378921508789), ('_features.7.conv.3.weight', 9.002188682556152), ('_features.10.conv.2.weight', 8.912237167358398), ('_features.12.conv.1.0.weight', 7.497399806976318), ('_features.5.conv.0.1.weight', 7.457813262939453), ('_features.6.conv.3.weight', 6.672262191772461), ('_features.16.conv.2.weight', 6.249387741088867), ('_features.16.conv.0.0.weight', 6.22741174697876), ('_features.6.conv.0.1.weight', 6.181740760803223), ('_features.13.conv.1.0.weight', 6.152912139892578), ('_features.5.conv.3.weight', 5.349498748779297), ('_features.8.conv.1.0.weight', 5.284809112548828), ('_features.5.conv.1.1.weight', 5.181466579437256), ('_features.12.conv.0.1.weight', 5.057946681976318), ('_features.13.conv.3.weight', 4.988582611083984), ('_features.15.conv.0.0.weight', 4.899039268493652), ('_features.15.conv.2.weight', 4.822112560272217), ('_features.13.conv.0.1.weight', 4.717461109161377), ('_features.12.conv.3.weight', 4.698787689208984), ('_features.6.conv.1.1.weight', 4.653088092803955), ('_features.9.conv.1.0.weight', 4.494771957397461), ('_features.10.conv.1.0.weight', 4.1148176193237305), ('_features.12.conv.1.1.weight', 3.7188234329223633), ('_features.13.conv.1.1.weight', 3.4613001346588135), ('_features.8.conv.3.weight', 3.163945198059082), ('_features.10.conv.3.weight', 2.9317784309387207), ('_features.10.conv.0.1.weight', 2.829403877258301), ('_features.16.conv.3.weight', 2.8145010471343994), ('_features.8.conv.0.1.weight', 2.719602346420288), ('_features.9.conv.3.weight', 2.6735680103302), ('_features.15.conv.3.weight', 2.3530328273773193), ('_features.9.conv.0.1.weight', 2.2544236183166504), ('_features.8.conv.1.1.weight', 2.1951022148132324), ('_features.10.conv.1.1.weight', 1.9528836011886597), ('_features.9.conv.1.1.weight', 1.834132432937622), ('_features.16.conv.0.1.weight', 1.3258705139160156), ('_features.15.conv.0.1.weight', 1.2093055248260498), ('_features.15.conv.1.0.weight', 0.9226931929588318), ('_features.16.conv.1.0.weight', 0.9044426679611206), ('_features.15.conv.1.1.weight', 0.7924290895462036), ('_features.16.conv.1.1.weight', 0.7734917402267456), ('_features.18.1.weight', 0.5632115006446838), ('_features.0.1.bias', 0.0), ('_features.1.conv.0.1.bias', 0.0), ('_features.1.conv.2.bias', 0.0), ('_features.2.conv.0.1.bias', 0.0), ('_features.2.conv.1.1.bias', 0.0), ('_features.2.conv.3.bias', 0.0), ('_features.3.conv.0.1.bias', 0.0), ('_features.3.conv.1.1.bias', 0.0), ('_features.3.conv.3.bias', 0.0), ('_features.4.conv.0.1.bias', 0.0), ('_features.4.conv.1.1.bias', 0.0), ('_features.4.conv.3.bias', 0.0), ('_features.5.conv.0.1.bias', 0.0), ('_features.5.conv.1.1.bias', 0.0), ('_features.5.conv.3.bias', 0.0), ('_features.6.conv.0.1.bias', 0.0), ('_features.6.conv.1.1.bias', 0.0), ('_features.6.conv.3.bias', 0.0), ('_features.7.conv.0.1.bias', 0.0), ('_features.7.conv.1.1.bias', 0.0), ('_features.7.conv.3.bias', 0.0), ('_features.8.conv.0.1.bias', 0.0), ('_features.8.conv.1.1.bias', 0.0), ('_features.8.conv.3.bias', 0.0), ('_features.9.conv.0.1.bias', 0.0), ('_features.9.conv.1.1.bias', 0.0), ('_features.9.conv.3.bias', 0.0), ('_features.10.conv.0.1.bias', 0.0), ('_features.10.conv.1.1.bias', 0.0), ('_features.10.conv.3.bias', 0.0), ('_features.11.conv.0.1.bias', 0.0), ('_features.11.conv.1.1.bias', 0.0), ('_features.11.conv.3.bias', 0.0), ('_features.12.conv.0.1.bias', 0.0), ('_features.12.conv.1.1.bias', 0.0), ('_features.12.conv.3.bias', 0.0), ('_features.13.conv.0.1.bias', 0.0), ('_features.13.conv.1.1.bias', 0.0), ('_features.13.conv.3.bias', 0.0), ('_features.14.conv.0.1.bias', 0.0), ('_features.14.conv.1.1.bias', 0.0), ('_features.14.conv.3.bias', 0.0), ('_features.15.conv.0.1.bias', 0.0), ('_features.15.conv.1.1.bias', 0.0), ('_features.15.conv.3.bias', 0.0), ('_features.16.conv.0.1.bias', 0.0), ('_features.16.conv.1.1.bias', 0.0), ('_features.16.conv.3.bias', 0.0), ('_features.17.conv.0.1.bias', 0.0), ('_features.17.conv.1.1.bias', 0.0), ('_features.17.conv.3.bias', 0.0), ('_features.18.1.bias', 0.0), ('last_linear.bias', 0.0)]
--------------------------------------------
get_sample_layers n=105  num_select=0
get_sample_layers not_random
--------------------------------------------
protect_list: []
--------------------------------------------
0 []

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.043716	Acc: 38.1% (3813/10000)
[Test]  Epoch: 2	Loss: 0.043003	Acc: 40.7% (4070/10000)
[Test]  Epoch: 3	Loss: 0.042792	Acc: 41.5% (4155/10000)
[Test]  Epoch: 4	Loss: 0.042899	Acc: 42.4% (4243/10000)
[Test]  Epoch: 5	Loss: 0.043029	Acc: 42.6% (4259/10000)
[Test]  Epoch: 6	Loss: 0.042833	Acc: 43.5% (4348/10000)
[Test]  Epoch: 7	Loss: 0.042593	Acc: 43.5% (4355/10000)
[Test]  Epoch: 8	Loss: 0.042630	Acc: 42.8% (4283/10000)
[Test]  Epoch: 9	Loss: 0.042629	Acc: 44.0% (4401/10000)
[Test]  Epoch: 10	Loss: 0.043363	Acc: 42.7% (4268/10000)
[Test]  Epoch: 11	Loss: 0.043642	Acc: 42.5% (4246/10000)
[Test]  Epoch: 12	Loss: 0.043237	Acc: 43.4% (4343/10000)
[Test]  Epoch: 13	Loss: 0.043159	Acc: 43.4% (4336/10000)
[Test]  Epoch: 14	Loss: 0.043276	Acc: 43.2% (4317/10000)
[Test]  Epoch: 15	Loss: 0.043611	Acc: 43.4% (4335/10000)
[Test]  Epoch: 16	Loss: 0.043287	Acc: 43.1% (4315/10000)
[Test]  Epoch: 17	Loss: 0.043582	Acc: 42.9% (4291/10000)
[Test]  Epoch: 18	Loss: 0.043244	Acc: 43.4% (4342/10000)
[Test]  Epoch: 19	Loss: 0.043731	Acc: 42.8% (4277/10000)
[Test]  Epoch: 20	Loss: 0.043972	Acc: 42.9% (4291/10000)
[Test]  Epoch: 21	Loss: 0.043632	Acc: 42.9% (4290/10000)
[Test]  Epoch: 22	Loss: 0.044349	Acc: 42.4% (4239/10000)
[Test]  Epoch: 23	Loss: 0.044102	Acc: 42.6% (4262/10000)
[Test]  Epoch: 24	Loss: 0.043786	Acc: 43.3% (4331/10000)
[Test]  Epoch: 25	Loss: 0.043546	Acc: 43.3% (4329/10000)
[Test]  Epoch: 26	Loss: 0.043686	Acc: 43.3% (4328/10000)
[Test]  Epoch: 27	Loss: 0.043864	Acc: 42.5% (4251/10000)
[Test]  Epoch: 28	Loss: 0.044061	Acc: 43.2% (4325/10000)
[Test]  Epoch: 29	Loss: 0.043673	Acc: 43.3% (4328/10000)
[Test]  Epoch: 30	Loss: 0.044254	Acc: 42.5% (4255/10000)
[Test]  Epoch: 31	Loss: 0.044024	Acc: 42.8% (4284/10000)
[Test]  Epoch: 32	Loss: 0.043769	Acc: 43.5% (4347/10000)
[Test]  Epoch: 33	Loss: 0.044044	Acc: 43.3% (4328/10000)
[Test]  Epoch: 34	Loss: 0.044002	Acc: 43.4% (4341/10000)
[Test]  Epoch: 35	Loss: 0.044059	Acc: 42.9% (4290/10000)
[Test]  Epoch: 36	Loss: 0.044070	Acc: 43.1% (4313/10000)
[Test]  Epoch: 37	Loss: 0.044362	Acc: 43.2% (4325/10000)
[Test]  Epoch: 38	Loss: 0.044262	Acc: 42.9% (4286/10000)
[Test]  Epoch: 39	Loss: 0.044396	Acc: 42.9% (4293/10000)
[Test]  Epoch: 40	Loss: 0.044528	Acc: 42.6% (4256/10000)
[Test]  Epoch: 41	Loss: 0.044422	Acc: 42.5% (4252/10000)
[Test]  Epoch: 42	Loss: 0.043852	Acc: 43.8% (4382/10000)
[Test]  Epoch: 43	Loss: 0.044155	Acc: 43.5% (4346/10000)
[Test]  Epoch: 44	Loss: 0.044752	Acc: 42.9% (4291/10000)
[Test]  Epoch: 45	Loss: 0.044504	Acc: 42.8% (4279/10000)
[Test]  Epoch: 46	Loss: 0.044565	Acc: 42.7% (4266/10000)
[Test]  Epoch: 47	Loss: 0.044556	Acc: 43.1% (4310/10000)
[Test]  Epoch: 48	Loss: 0.045156	Acc: 42.0% (4196/10000)
[Test]  Epoch: 49	Loss: 0.044859	Acc: 43.0% (4303/10000)
[Test]  Epoch: 50	Loss: 0.044509	Acc: 43.5% (4349/10000)
[Test]  Epoch: 51	Loss: 0.044646	Acc: 42.9% (4293/10000)
[Test]  Epoch: 52	Loss: 0.044498	Acc: 43.6% (4357/10000)
[Test]  Epoch: 53	Loss: 0.044899	Acc: 42.8% (4277/10000)
[Test]  Epoch: 54	Loss: 0.044789	Acc: 42.7% (4270/10000)
[Test]  Epoch: 55	Loss: 0.044625	Acc: 42.8% (4281/10000)
[Test]  Epoch: 56	Loss: 0.044727	Acc: 42.5% (4247/10000)
[Test]  Epoch: 57	Loss: 0.044599	Acc: 42.9% (4290/10000)
[Test]  Epoch: 58	Loss: 0.044843	Acc: 43.4% (4343/10000)
[Test]  Epoch: 59	Loss: 0.044900	Acc: 42.6% (4257/10000)
[Test]  Epoch: 60	Loss: 0.044750	Acc: 43.2% (4325/10000)
[Test]  Epoch: 61	Loss: 0.044880	Acc: 43.1% (4309/10000)
[Test]  Epoch: 62	Loss: 0.044938	Acc: 43.2% (4318/10000)
[Test]  Epoch: 63	Loss: 0.044717	Acc: 43.3% (4334/10000)
[Test]  Epoch: 64	Loss: 0.044742	Acc: 43.2% (4319/10000)
[Test]  Epoch: 65	Loss: 0.044833	Acc: 43.3% (4327/10000)
[Test]  Epoch: 66	Loss: 0.044766	Acc: 43.2% (4325/10000)
[Test]  Epoch: 67	Loss: 0.044854	Acc: 43.3% (4334/10000)
[Test]  Epoch: 68	Loss: 0.044969	Acc: 43.1% (4308/10000)
[Test]  Epoch: 69	Loss: 0.044892	Acc: 43.1% (4314/10000)
[Test]  Epoch: 70	Loss: 0.044877	Acc: 43.0% (4303/10000)
[Test]  Epoch: 71	Loss: 0.044787	Acc: 43.2% (4323/10000)
[Test]  Epoch: 72	Loss: 0.044893	Acc: 43.0% (4297/10000)
[Test]  Epoch: 73	Loss: 0.044782	Acc: 43.2% (4322/10000)
[Test]  Epoch: 74	Loss: 0.044855	Acc: 43.1% (4310/10000)
[Test]  Epoch: 75	Loss: 0.044937	Acc: 43.0% (4303/10000)
[Test]  Epoch: 76	Loss: 0.044779	Acc: 43.4% (4335/10000)
[Test]  Epoch: 77	Loss: 0.044888	Acc: 43.2% (4320/10000)
[Test]  Epoch: 78	Loss: 0.044884	Acc: 43.0% (4295/10000)
[Test]  Epoch: 79	Loss: 0.044939	Acc: 43.2% (4323/10000)
[Test]  Epoch: 80	Loss: 0.044825	Acc: 43.2% (4321/10000)
[Test]  Epoch: 81	Loss: 0.044880	Acc: 43.1% (4307/10000)
[Test]  Epoch: 82	Loss: 0.044854	Acc: 43.0% (4303/10000)
[Test]  Epoch: 83	Loss: 0.044918	Acc: 42.9% (4292/10000)
[Test]  Epoch: 84	Loss: 0.044999	Acc: 43.1% (4310/10000)
[Test]  Epoch: 85	Loss: 0.044917	Acc: 43.0% (4299/10000)
[Test]  Epoch: 86	Loss: 0.044937	Acc: 42.9% (4288/10000)
[Test]  Epoch: 87	Loss: 0.044856	Acc: 43.1% (4312/10000)
[Test]  Epoch: 88	Loss: 0.044865	Acc: 43.1% (4307/10000)
[Test]  Epoch: 89	Loss: 0.044842	Acc: 43.2% (4320/10000)
[Test]  Epoch: 90	Loss: 0.044949	Acc: 43.0% (4304/10000)
[Test]  Epoch: 91	Loss: 0.044916	Acc: 43.0% (4297/10000)
[Test]  Epoch: 92	Loss: 0.044790	Acc: 43.2% (4325/10000)
[Test]  Epoch: 93	Loss: 0.044913	Acc: 43.1% (4310/10000)
[Test]  Epoch: 94	Loss: 0.044940	Acc: 43.1% (4309/10000)
[Test]  Epoch: 95	Loss: 0.044837	Acc: 43.0% (4296/10000)
[Test]  Epoch: 96	Loss: 0.044811	Acc: 43.2% (4322/10000)
[Test]  Epoch: 97	Loss: 0.044872	Acc: 43.1% (4312/10000)
[Test]  Epoch: 98	Loss: 0.044818	Acc: 43.2% (4320/10000)
[Test]  Epoch: 99	Loss: 0.044901	Acc: 43.1% (4308/10000)
[Test]  Epoch: 100	Loss: 0.044818	Acc: 43.1% (4312/10000)
===========finish==========
['2024-08-18', '19:32:08.808772', '100', 'test', '0.04481813275814056', '43.12', '44.01']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=10
get_sample_layers not_random
10 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.059425	Acc: 16.0% (1597/10000)
[Test]  Epoch: 2	Loss: 0.054162	Acc: 24.1% (2409/10000)
[Test]  Epoch: 3	Loss: 0.051918	Acc: 28.8% (2881/10000)
[Test]  Epoch: 4	Loss: 0.052233	Acc: 29.4% (2937/10000)
[Test]  Epoch: 5	Loss: 0.051974	Acc: 30.0% (3002/10000)
[Test]  Epoch: 6	Loss: 0.051162	Acc: 30.4% (3045/10000)
[Test]  Epoch: 7	Loss: 0.050974	Acc: 30.5% (3054/10000)
[Test]  Epoch: 8	Loss: 0.050608	Acc: 30.9% (3095/10000)
[Test]  Epoch: 9	Loss: 0.050758	Acc: 31.6% (3155/10000)
[Test]  Epoch: 10	Loss: 0.051377	Acc: 30.7% (3074/10000)
[Test]  Epoch: 11	Loss: 0.050978	Acc: 31.3% (3134/10000)
[Test]  Epoch: 12	Loss: 0.051099	Acc: 31.9% (3186/10000)
[Test]  Epoch: 13	Loss: 0.050619	Acc: 32.0% (3205/10000)
[Test]  Epoch: 14	Loss: 0.050956	Acc: 31.5% (3147/10000)
[Test]  Epoch: 15	Loss: 0.050947	Acc: 31.8% (3183/10000)
[Test]  Epoch: 16	Loss: 0.050833	Acc: 31.9% (3194/10000)
[Test]  Epoch: 17	Loss: 0.050686	Acc: 32.2% (3219/10000)
[Test]  Epoch: 18	Loss: 0.050177	Acc: 32.4% (3237/10000)
[Test]  Epoch: 19	Loss: 0.050518	Acc: 32.0% (3203/10000)
[Test]  Epoch: 20	Loss: 0.050901	Acc: 32.2% (3216/10000)
[Test]  Epoch: 21	Loss: 0.050773	Acc: 32.4% (3242/10000)
[Test]  Epoch: 22	Loss: 0.050946	Acc: 32.3% (3227/10000)
[Test]  Epoch: 23	Loss: 0.050443	Acc: 32.4% (3243/10000)
[Test]  Epoch: 24	Loss: 0.050818	Acc: 32.1% (3210/10000)
[Test]  Epoch: 25	Loss: 0.050467	Acc: 32.5% (3251/10000)
[Test]  Epoch: 26	Loss: 0.050624	Acc: 32.7% (3266/10000)
[Test]  Epoch: 27	Loss: 0.050516	Acc: 32.0% (3202/10000)
[Test]  Epoch: 28	Loss: 0.050837	Acc: 32.3% (3234/10000)
[Test]  Epoch: 29	Loss: 0.050568	Acc: 32.2% (3218/10000)
[Test]  Epoch: 30	Loss: 0.050819	Acc: 32.6% (3257/10000)
[Test]  Epoch: 31	Loss: 0.050754	Acc: 32.5% (3250/10000)
[Test]  Epoch: 32	Loss: 0.050601	Acc: 32.6% (3264/10000)
[Test]  Epoch: 33	Loss: 0.050637	Acc: 32.5% (3250/10000)
[Test]  Epoch: 34	Loss: 0.050626	Acc: 32.9% (3285/10000)
[Test]  Epoch: 35	Loss: 0.050270	Acc: 32.9% (3291/10000)
[Test]  Epoch: 36	Loss: 0.050657	Acc: 32.8% (3275/10000)
[Test]  Epoch: 37	Loss: 0.050683	Acc: 32.8% (3279/10000)
[Test]  Epoch: 38	Loss: 0.050460	Acc: 33.1% (3314/10000)
[Test]  Epoch: 39	Loss: 0.050827	Acc: 32.7% (3268/10000)
[Test]  Epoch: 40	Loss: 0.050953	Acc: 32.6% (3257/10000)
[Test]  Epoch: 41	Loss: 0.050778	Acc: 32.7% (3269/10000)
[Test]  Epoch: 42	Loss: 0.050673	Acc: 33.2% (3316/10000)
[Test]  Epoch: 43	Loss: 0.050701	Acc: 33.0% (3304/10000)
[Test]  Epoch: 44	Loss: 0.051103	Acc: 32.4% (3235/10000)
[Test]  Epoch: 45	Loss: 0.050915	Acc: 32.6% (3259/10000)
[Test]  Epoch: 46	Loss: 0.050739	Acc: 32.9% (3286/10000)
[Test]  Epoch: 47	Loss: 0.050956	Acc: 33.0% (3305/10000)
[Test]  Epoch: 48	Loss: 0.050907	Acc: 32.8% (3279/10000)
[Test]  Epoch: 49	Loss: 0.050902	Acc: 33.1% (3315/10000)
[Test]  Epoch: 50	Loss: 0.050915	Acc: 33.2% (3317/10000)
[Test]  Epoch: 51	Loss: 0.050733	Acc: 33.2% (3323/10000)
[Test]  Epoch: 52	Loss: 0.050765	Acc: 33.2% (3320/10000)
[Test]  Epoch: 53	Loss: 0.050992	Acc: 33.1% (3310/10000)
[Test]  Epoch: 54	Loss: 0.050930	Acc: 33.3% (3332/10000)
[Test]  Epoch: 55	Loss: 0.050575	Acc: 33.7% (3366/10000)
[Test]  Epoch: 56	Loss: 0.050987	Acc: 32.6% (3257/10000)
[Test]  Epoch: 57	Loss: 0.050712	Acc: 33.2% (3321/10000)
[Test]  Epoch: 58	Loss: 0.050814	Acc: 33.3% (3329/10000)
[Test]  Epoch: 59	Loss: 0.051025	Acc: 32.9% (3291/10000)
[Test]  Epoch: 60	Loss: 0.050831	Acc: 33.3% (3326/10000)
[Test]  Epoch: 61	Loss: 0.050844	Acc: 33.5% (3354/10000)
[Test]  Epoch: 62	Loss: 0.050917	Acc: 33.3% (3332/10000)
[Test]  Epoch: 63	Loss: 0.050689	Acc: 33.6% (3360/10000)
[Test]  Epoch: 64	Loss: 0.050690	Acc: 33.6% (3363/10000)
[Test]  Epoch: 65	Loss: 0.050796	Acc: 33.7% (3371/10000)
[Test]  Epoch: 66	Loss: 0.050776	Acc: 33.6% (3363/10000)
[Test]  Epoch: 67	Loss: 0.050801	Acc: 33.6% (3365/10000)
[Test]  Epoch: 68	Loss: 0.050920	Acc: 33.4% (3341/10000)
[Test]  Epoch: 69	Loss: 0.050807	Acc: 33.7% (3369/10000)
[Test]  Epoch: 70	Loss: 0.050815	Acc: 33.6% (3360/10000)
[Test]  Epoch: 71	Loss: 0.050725	Acc: 33.9% (3389/10000)
[Test]  Epoch: 72	Loss: 0.050849	Acc: 33.6% (3357/10000)
[Test]  Epoch: 73	Loss: 0.050752	Acc: 33.6% (3362/10000)
[Test]  Epoch: 74	Loss: 0.050754	Acc: 33.6% (3359/10000)
[Test]  Epoch: 75	Loss: 0.050871	Acc: 33.5% (3352/10000)
[Test]  Epoch: 76	Loss: 0.050739	Acc: 33.8% (3378/10000)
[Test]  Epoch: 77	Loss: 0.050831	Acc: 33.8% (3379/10000)
[Test]  Epoch: 78	Loss: 0.050852	Acc: 33.5% (3351/10000)
[Test]  Epoch: 79	Loss: 0.050900	Acc: 33.4% (3341/10000)
[Test]  Epoch: 80	Loss: 0.050806	Acc: 33.6% (3364/10000)
[Test]  Epoch: 81	Loss: 0.050790	Acc: 33.7% (3370/10000)
[Test]  Epoch: 82	Loss: 0.050821	Acc: 33.7% (3370/10000)
[Test]  Epoch: 83	Loss: 0.050869	Acc: 33.7% (3367/10000)
[Test]  Epoch: 84	Loss: 0.050931	Acc: 33.4% (3344/10000)
[Test]  Epoch: 85	Loss: 0.050902	Acc: 33.6% (3365/10000)
[Test]  Epoch: 86	Loss: 0.050899	Acc: 33.6% (3364/10000)
[Test]  Epoch: 87	Loss: 0.050816	Acc: 33.7% (3369/10000)
[Test]  Epoch: 88	Loss: 0.050875	Acc: 33.5% (3348/10000)
[Test]  Epoch: 89	Loss: 0.050770	Acc: 33.6% (3361/10000)
[Test]  Epoch: 90	Loss: 0.050884	Acc: 33.5% (3355/10000)
[Test]  Epoch: 91	Loss: 0.050881	Acc: 33.6% (3360/10000)
[Test]  Epoch: 92	Loss: 0.050790	Acc: 33.7% (3373/10000)
[Test]  Epoch: 93	Loss: 0.050820	Acc: 33.6% (3364/10000)
[Test]  Epoch: 94	Loss: 0.050883	Acc: 33.5% (3353/10000)
[Test]  Epoch: 95	Loss: 0.050835	Acc: 33.5% (3352/10000)
[Test]  Epoch: 96	Loss: 0.050767	Acc: 33.7% (3374/10000)
[Test]  Epoch: 97	Loss: 0.050796	Acc: 33.6% (3359/10000)
[Test]  Epoch: 98	Loss: 0.050792	Acc: 33.6% (3361/10000)
[Test]  Epoch: 99	Loss: 0.050848	Acc: 33.3% (3334/10000)
[Test]  Epoch: 100	Loss: 0.050794	Acc: 33.6% (3361/10000)
===========finish==========
['2024-08-18', '19:34:19.256284', '100', 'test', '0.05079393825531006', '33.61', '33.89']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.2 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=21
get_sample_layers not_random
21 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.082460	Acc: 1.4% (144/10000)
[Test]  Epoch: 2	Loss: 0.078468	Acc: 4.4% (437/10000)
[Test]  Epoch: 3	Loss: 0.074633	Acc: 6.6% (663/10000)
[Test]  Epoch: 4	Loss: 0.071839	Acc: 9.3% (934/10000)
[Test]  Epoch: 5	Loss: 0.069017	Acc: 10.4% (1045/10000)
[Test]  Epoch: 6	Loss: 0.067324	Acc: 12.3% (1226/10000)
[Test]  Epoch: 7	Loss: 0.065920	Acc: 13.2% (1318/10000)
[Test]  Epoch: 8	Loss: 0.064471	Acc: 13.8% (1381/10000)
[Test]  Epoch: 9	Loss: 0.063376	Acc: 15.4% (1537/10000)
[Test]  Epoch: 10	Loss: 0.062348	Acc: 16.4% (1645/10000)
[Test]  Epoch: 11	Loss: 0.061770	Acc: 17.3% (1730/10000)
[Test]  Epoch: 12	Loss: 0.061127	Acc: 17.7% (1772/10000)
[Test]  Epoch: 13	Loss: 0.060407	Acc: 18.2% (1816/10000)
[Test]  Epoch: 14	Loss: 0.060480	Acc: 18.5% (1846/10000)
[Test]  Epoch: 15	Loss: 0.059769	Acc: 19.1% (1907/10000)
[Test]  Epoch: 16	Loss: 0.059089	Acc: 19.4% (1942/10000)
[Test]  Epoch: 17	Loss: 0.058903	Acc: 19.7% (1971/10000)
[Test]  Epoch: 18	Loss: 0.058844	Acc: 20.1% (2005/10000)
[Test]  Epoch: 19	Loss: 0.058504	Acc: 20.3% (2028/10000)
[Test]  Epoch: 20	Loss: 0.058228	Acc: 20.6% (2062/10000)
[Test]  Epoch: 21	Loss: 0.058104	Acc: 20.4% (2040/10000)
[Test]  Epoch: 22	Loss: 0.057785	Acc: 21.5% (2147/10000)
[Test]  Epoch: 23	Loss: 0.057478	Acc: 21.5% (2148/10000)
[Test]  Epoch: 24	Loss: 0.057426	Acc: 21.8% (2184/10000)
[Test]  Epoch: 25	Loss: 0.057203	Acc: 22.1% (2205/10000)
[Test]  Epoch: 26	Loss: 0.057289	Acc: 22.4% (2244/10000)
[Test]  Epoch: 27	Loss: 0.057170	Acc: 21.8% (2175/10000)
[Test]  Epoch: 28	Loss: 0.057154	Acc: 21.8% (2180/10000)
[Test]  Epoch: 29	Loss: 0.056691	Acc: 22.5% (2249/10000)
[Test]  Epoch: 30	Loss: 0.056532	Acc: 22.7% (2267/10000)
[Test]  Epoch: 31	Loss: 0.056836	Acc: 22.4% (2236/10000)
[Test]  Epoch: 32	Loss: 0.056585	Acc: 23.0% (2298/10000)
[Test]  Epoch: 33	Loss: 0.056466	Acc: 22.7% (2273/10000)
[Test]  Epoch: 34	Loss: 0.056474	Acc: 22.9% (2290/10000)
[Test]  Epoch: 35	Loss: 0.056241	Acc: 23.0% (2298/10000)
[Test]  Epoch: 36	Loss: 0.056315	Acc: 23.4% (2342/10000)
[Test]  Epoch: 37	Loss: 0.056104	Acc: 23.6% (2355/10000)
[Test]  Epoch: 38	Loss: 0.056443	Acc: 23.0% (2296/10000)
[Test]  Epoch: 39	Loss: 0.056207	Acc: 23.6% (2359/10000)
[Test]  Epoch: 40	Loss: 0.056130	Acc: 23.5% (2348/10000)
[Test]  Epoch: 41	Loss: 0.056290	Acc: 23.1% (2308/10000)
[Test]  Epoch: 42	Loss: 0.056055	Acc: 23.6% (2359/10000)
[Test]  Epoch: 43	Loss: 0.055951	Acc: 23.8% (2381/10000)
[Test]  Epoch: 44	Loss: 0.056089	Acc: 23.6% (2355/10000)
[Test]  Epoch: 45	Loss: 0.056031	Acc: 23.5% (2354/10000)
[Test]  Epoch: 46	Loss: 0.055833	Acc: 23.7% (2373/10000)
[Test]  Epoch: 47	Loss: 0.055906	Acc: 23.5% (2351/10000)
[Test]  Epoch: 48	Loss: 0.055949	Acc: 24.0% (2397/10000)
[Test]  Epoch: 49	Loss: 0.055894	Acc: 24.0% (2399/10000)
[Test]  Epoch: 50	Loss: 0.055814	Acc: 23.6% (2361/10000)
[Test]  Epoch: 51	Loss: 0.055847	Acc: 24.5% (2452/10000)
[Test]  Epoch: 52	Loss: 0.055642	Acc: 24.4% (2441/10000)
[Test]  Epoch: 53	Loss: 0.055709	Acc: 24.5% (2453/10000)
[Test]  Epoch: 54	Loss: 0.055552	Acc: 23.9% (2394/10000)
[Test]  Epoch: 55	Loss: 0.055570	Acc: 24.4% (2436/10000)
[Test]  Epoch: 56	Loss: 0.055584	Acc: 23.8% (2379/10000)
[Test]  Epoch: 57	Loss: 0.055332	Acc: 24.7% (2472/10000)
[Test]  Epoch: 58	Loss: 0.055596	Acc: 24.3% (2432/10000)
[Test]  Epoch: 59	Loss: 0.055685	Acc: 24.2% (2418/10000)
[Test]  Epoch: 60	Loss: 0.055577	Acc: 24.9% (2491/10000)
[Test]  Epoch: 61	Loss: 0.055390	Acc: 25.0% (2498/10000)
[Test]  Epoch: 62	Loss: 0.055406	Acc: 25.0% (2498/10000)
[Test]  Epoch: 63	Loss: 0.055184	Acc: 25.3% (2526/10000)
[Test]  Epoch: 64	Loss: 0.055173	Acc: 25.4% (2536/10000)
[Test]  Epoch: 65	Loss: 0.055276	Acc: 25.1% (2515/10000)
[Test]  Epoch: 66	Loss: 0.055293	Acc: 25.0% (2504/10000)
[Test]  Epoch: 67	Loss: 0.055304	Acc: 25.1% (2505/10000)
[Test]  Epoch: 68	Loss: 0.055408	Acc: 25.0% (2497/10000)
[Test]  Epoch: 69	Loss: 0.055295	Acc: 24.9% (2494/10000)
[Test]  Epoch: 70	Loss: 0.055296	Acc: 25.0% (2498/10000)
[Test]  Epoch: 71	Loss: 0.055206	Acc: 25.1% (2513/10000)
[Test]  Epoch: 72	Loss: 0.055320	Acc: 25.1% (2515/10000)
[Test]  Epoch: 73	Loss: 0.055238	Acc: 25.1% (2514/10000)
[Test]  Epoch: 74	Loss: 0.055235	Acc: 25.1% (2509/10000)
[Test]  Epoch: 75	Loss: 0.055328	Acc: 24.9% (2491/10000)
[Test]  Epoch: 76	Loss: 0.055271	Acc: 25.1% (2506/10000)
[Test]  Epoch: 77	Loss: 0.055304	Acc: 25.1% (2515/10000)
[Test]  Epoch: 78	Loss: 0.055286	Acc: 25.0% (2498/10000)
[Test]  Epoch: 79	Loss: 0.055349	Acc: 25.1% (2505/10000)
[Test]  Epoch: 80	Loss: 0.055233	Acc: 25.2% (2521/10000)
[Test]  Epoch: 81	Loss: 0.055225	Acc: 25.1% (2515/10000)
[Test]  Epoch: 82	Loss: 0.055221	Acc: 25.2% (2524/10000)
[Test]  Epoch: 83	Loss: 0.055284	Acc: 25.1% (2515/10000)
[Test]  Epoch: 84	Loss: 0.055358	Acc: 25.1% (2505/10000)
[Test]  Epoch: 85	Loss: 0.055306	Acc: 25.2% (2518/10000)
[Test]  Epoch: 86	Loss: 0.055269	Acc: 25.2% (2517/10000)
[Test]  Epoch: 87	Loss: 0.055276	Acc: 25.1% (2512/10000)
[Test]  Epoch: 88	Loss: 0.055276	Acc: 25.1% (2514/10000)
[Test]  Epoch: 89	Loss: 0.055191	Acc: 25.2% (2517/10000)
[Test]  Epoch: 90	Loss: 0.055313	Acc: 25.2% (2517/10000)
[Test]  Epoch: 91	Loss: 0.055314	Acc: 25.1% (2508/10000)
[Test]  Epoch: 92	Loss: 0.055233	Acc: 25.3% (2526/10000)
[Test]  Epoch: 93	Loss: 0.055222	Acc: 25.3% (2533/10000)
[Test]  Epoch: 94	Loss: 0.055324	Acc: 24.9% (2492/10000)
[Test]  Epoch: 95	Loss: 0.055264	Acc: 25.1% (2509/10000)
[Test]  Epoch: 96	Loss: 0.055185	Acc: 25.2% (2519/10000)
[Test]  Epoch: 97	Loss: 0.055198	Acc: 25.3% (2533/10000)
[Test]  Epoch: 98	Loss: 0.055249	Acc: 25.2% (2520/10000)
[Test]  Epoch: 99	Loss: 0.055232	Acc: 25.2% (2523/10000)
[Test]  Epoch: 100	Loss: 0.055200	Acc: 25.2% (2520/10000)
===========finish==========
['2024-08-18', '19:36:36.497794', '100', 'test', '0.05520043444633484', '25.2', '25.36']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.3 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=31
get_sample_layers not_random
31 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.1.conv.2.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.3.weight', '_features.14.conv.0.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.082479	Acc: 1.4% (140/10000)
[Test]  Epoch: 2	Loss: 0.077987	Acc: 5.0% (498/10000)
[Test]  Epoch: 3	Loss: 0.074167	Acc: 7.0% (704/10000)
[Test]  Epoch: 4	Loss: 0.071098	Acc: 9.6% (959/10000)
[Test]  Epoch: 5	Loss: 0.068804	Acc: 11.0% (1102/10000)
[Test]  Epoch: 6	Loss: 0.067033	Acc: 12.4% (1237/10000)
[Test]  Epoch: 7	Loss: 0.065410	Acc: 14.0% (1396/10000)
[Test]  Epoch: 8	Loss: 0.064064	Acc: 14.2% (1418/10000)
[Test]  Epoch: 9	Loss: 0.062977	Acc: 15.8% (1575/10000)
[Test]  Epoch: 10	Loss: 0.061931	Acc: 16.7% (1668/10000)
[Test]  Epoch: 11	Loss: 0.061082	Acc: 17.5% (1750/10000)
[Test]  Epoch: 12	Loss: 0.060867	Acc: 17.7% (1767/10000)
[Test]  Epoch: 13	Loss: 0.059933	Acc: 18.5% (1847/10000)
[Test]  Epoch: 14	Loss: 0.060013	Acc: 18.8% (1877/10000)
[Test]  Epoch: 15	Loss: 0.059626	Acc: 19.6% (1962/10000)
[Test]  Epoch: 16	Loss: 0.058635	Acc: 20.1% (2006/10000)
[Test]  Epoch: 17	Loss: 0.058442	Acc: 20.0% (2000/10000)
[Test]  Epoch: 18	Loss: 0.058224	Acc: 20.8% (2082/10000)
[Test]  Epoch: 19	Loss: 0.057923	Acc: 21.6% (2163/10000)
[Test]  Epoch: 20	Loss: 0.058000	Acc: 20.9% (2085/10000)
[Test]  Epoch: 21	Loss: 0.057345	Acc: 21.6% (2157/10000)
[Test]  Epoch: 22	Loss: 0.057387	Acc: 22.2% (2225/10000)
[Test]  Epoch: 23	Loss: 0.056798	Acc: 22.3% (2232/10000)
[Test]  Epoch: 24	Loss: 0.057039	Acc: 22.0% (2200/10000)
[Test]  Epoch: 25	Loss: 0.056892	Acc: 22.4% (2242/10000)
[Test]  Epoch: 26	Loss: 0.056751	Acc: 22.9% (2293/10000)
[Test]  Epoch: 27	Loss: 0.056677	Acc: 22.5% (2253/10000)
[Test]  Epoch: 28	Loss: 0.056631	Acc: 23.0% (2302/10000)
[Test]  Epoch: 29	Loss: 0.056342	Acc: 22.8% (2277/10000)
[Test]  Epoch: 30	Loss: 0.056028	Acc: 23.3% (2327/10000)
[Test]  Epoch: 31	Loss: 0.056247	Acc: 23.2% (2324/10000)
[Test]  Epoch: 32	Loss: 0.056036	Acc: 23.7% (2366/10000)
[Test]  Epoch: 33	Loss: 0.056004	Acc: 23.3% (2334/10000)
[Test]  Epoch: 34	Loss: 0.055917	Acc: 24.0% (2396/10000)
[Test]  Epoch: 35	Loss: 0.055754	Acc: 24.1% (2410/10000)
[Test]  Epoch: 36	Loss: 0.055603	Acc: 24.0% (2399/10000)
[Test]  Epoch: 37	Loss: 0.055404	Acc: 24.7% (2471/10000)
[Test]  Epoch: 38	Loss: 0.055932	Acc: 24.1% (2407/10000)
[Test]  Epoch: 39	Loss: 0.055553	Acc: 24.4% (2443/10000)
[Test]  Epoch: 40	Loss: 0.055803	Acc: 23.9% (2388/10000)
[Test]  Epoch: 41	Loss: 0.055700	Acc: 23.6% (2355/10000)
[Test]  Epoch: 42	Loss: 0.055402	Acc: 24.8% (2478/10000)
[Test]  Epoch: 43	Loss: 0.055481	Acc: 24.9% (2487/10000)
[Test]  Epoch: 44	Loss: 0.055363	Acc: 24.9% (2490/10000)
[Test]  Epoch: 45	Loss: 0.055476	Acc: 24.4% (2444/10000)
[Test]  Epoch: 46	Loss: 0.055432	Acc: 24.8% (2482/10000)
[Test]  Epoch: 47	Loss: 0.055518	Acc: 25.1% (2505/10000)
[Test]  Epoch: 48	Loss: 0.055370	Acc: 25.0% (2503/10000)
[Test]  Epoch: 49	Loss: 0.055233	Acc: 25.3% (2527/10000)
[Test]  Epoch: 50	Loss: 0.055303	Acc: 25.4% (2535/10000)
[Test]  Epoch: 51	Loss: 0.055192	Acc: 25.3% (2528/10000)
[Test]  Epoch: 52	Loss: 0.055102	Acc: 25.3% (2528/10000)
[Test]  Epoch: 53	Loss: 0.055359	Acc: 25.1% (2509/10000)
[Test]  Epoch: 54	Loss: 0.055133	Acc: 25.2% (2523/10000)
[Test]  Epoch: 55	Loss: 0.055039	Acc: 25.5% (2553/10000)
[Test]  Epoch: 56	Loss: 0.054809	Acc: 25.5% (2550/10000)
[Test]  Epoch: 57	Loss: 0.055040	Acc: 25.4% (2544/10000)
[Test]  Epoch: 58	Loss: 0.054999	Acc: 25.7% (2571/10000)
[Test]  Epoch: 59	Loss: 0.055116	Acc: 25.3% (2532/10000)
[Test]  Epoch: 60	Loss: 0.055125	Acc: 25.7% (2573/10000)
[Test]  Epoch: 61	Loss: 0.054825	Acc: 26.1% (2611/10000)
[Test]  Epoch: 62	Loss: 0.054842	Acc: 26.1% (2609/10000)
[Test]  Epoch: 63	Loss: 0.054583	Acc: 26.6% (2659/10000)
[Test]  Epoch: 64	Loss: 0.054578	Acc: 26.5% (2647/10000)
[Test]  Epoch: 65	Loss: 0.054711	Acc: 26.4% (2640/10000)
[Test]  Epoch: 66	Loss: 0.054711	Acc: 26.5% (2647/10000)
[Test]  Epoch: 67	Loss: 0.054757	Acc: 26.4% (2641/10000)
[Test]  Epoch: 68	Loss: 0.054820	Acc: 26.2% (2616/10000)
[Test]  Epoch: 69	Loss: 0.054715	Acc: 26.3% (2630/10000)
[Test]  Epoch: 70	Loss: 0.054737	Acc: 26.2% (2619/10000)
[Test]  Epoch: 71	Loss: 0.054673	Acc: 26.3% (2629/10000)
[Test]  Epoch: 72	Loss: 0.054783	Acc: 26.2% (2623/10000)
[Test]  Epoch: 73	Loss: 0.054713	Acc: 26.3% (2630/10000)
[Test]  Epoch: 74	Loss: 0.054718	Acc: 26.3% (2631/10000)
[Test]  Epoch: 75	Loss: 0.054792	Acc: 26.2% (2625/10000)
[Test]  Epoch: 76	Loss: 0.054701	Acc: 26.6% (2661/10000)
[Test]  Epoch: 77	Loss: 0.054738	Acc: 26.4% (2641/10000)
[Test]  Epoch: 78	Loss: 0.054718	Acc: 26.4% (2635/10000)
[Test]  Epoch: 79	Loss: 0.054802	Acc: 26.2% (2623/10000)
[Test]  Epoch: 80	Loss: 0.054705	Acc: 26.4% (2642/10000)
[Test]  Epoch: 81	Loss: 0.054670	Acc: 26.4% (2641/10000)
[Test]  Epoch: 82	Loss: 0.054683	Acc: 26.5% (2649/10000)
[Test]  Epoch: 83	Loss: 0.054727	Acc: 26.3% (2629/10000)
[Test]  Epoch: 84	Loss: 0.054821	Acc: 26.2% (2617/10000)
[Test]  Epoch: 85	Loss: 0.054749	Acc: 26.2% (2625/10000)
[Test]  Epoch: 86	Loss: 0.054739	Acc: 26.1% (2615/10000)
[Test]  Epoch: 87	Loss: 0.054732	Acc: 26.3% (2634/10000)
[Test]  Epoch: 88	Loss: 0.054717	Acc: 26.2% (2625/10000)
[Test]  Epoch: 89	Loss: 0.054609	Acc: 26.4% (2636/10000)
[Test]  Epoch: 90	Loss: 0.054785	Acc: 26.2% (2625/10000)
[Test]  Epoch: 91	Loss: 0.054743	Acc: 26.6% (2661/10000)
[Test]  Epoch: 92	Loss: 0.054688	Acc: 26.5% (2647/10000)
[Test]  Epoch: 93	Loss: 0.054688	Acc: 26.6% (2656/10000)
[Test]  Epoch: 94	Loss: 0.054776	Acc: 26.5% (2646/10000)
[Test]  Epoch: 95	Loss: 0.054724	Acc: 26.4% (2641/10000)
[Test]  Epoch: 96	Loss: 0.054666	Acc: 26.4% (2643/10000)
[Test]  Epoch: 97	Loss: 0.054673	Acc: 26.5% (2646/10000)
[Test]  Epoch: 98	Loss: 0.054679	Acc: 26.6% (2655/10000)
[Test]  Epoch: 99	Loss: 0.054675	Acc: 26.2% (2624/10000)
[Test]  Epoch: 100	Loss: 0.054664	Acc: 26.3% (2632/10000)
===========finish==========
['2024-08-18', '19:38:51.519118', '100', 'test', '0.05466423382759094', '26.32', '26.61']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.4 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=42
get_sample_layers not_random
42 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.1.conv.2.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.3.weight', '_features.14.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.17.conv.3.weight', '_features.11.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.0.1.weight', '_features.17.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.1.weight', '_features.6.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.082272	Acc: 1.8% (177/10000)
[Test]  Epoch: 2	Loss: 0.075541	Acc: 6.8% (676/10000)
[Test]  Epoch: 3	Loss: 0.071955	Acc: 8.9% (894/10000)
[Test]  Epoch: 4	Loss: 0.069550	Acc: 11.6% (1160/10000)
[Test]  Epoch: 5	Loss: 0.067512	Acc: 12.7% (1268/10000)
[Test]  Epoch: 6	Loss: 0.065376	Acc: 13.9% (1392/10000)
[Test]  Epoch: 7	Loss: 0.064047	Acc: 15.4% (1536/10000)
[Test]  Epoch: 8	Loss: 0.062944	Acc: 16.1% (1615/10000)
[Test]  Epoch: 9	Loss: 0.062363	Acc: 16.6% (1663/10000)
[Test]  Epoch: 10	Loss: 0.061270	Acc: 18.1% (1811/10000)
[Test]  Epoch: 11	Loss: 0.060723	Acc: 18.2% (1824/10000)
[Test]  Epoch: 12	Loss: 0.060598	Acc: 18.9% (1895/10000)
[Test]  Epoch: 13	Loss: 0.059564	Acc: 19.2% (1921/10000)
[Test]  Epoch: 14	Loss: 0.059018	Acc: 20.1% (2007/10000)
[Test]  Epoch: 15	Loss: 0.058829	Acc: 20.5% (2046/10000)
[Test]  Epoch: 16	Loss: 0.058264	Acc: 20.7% (2070/10000)
[Test]  Epoch: 17	Loss: 0.058270	Acc: 20.6% (2060/10000)
[Test]  Epoch: 18	Loss: 0.058273	Acc: 20.6% (2058/10000)
[Test]  Epoch: 19	Loss: 0.057600	Acc: 21.3% (2134/10000)
[Test]  Epoch: 20	Loss: 0.057597	Acc: 21.9% (2185/10000)
[Test]  Epoch: 21	Loss: 0.057308	Acc: 22.2% (2219/10000)
[Test]  Epoch: 22	Loss: 0.057211	Acc: 22.2% (2221/10000)
[Test]  Epoch: 23	Loss: 0.056823	Acc: 22.5% (2251/10000)
[Test]  Epoch: 24	Loss: 0.057102	Acc: 22.4% (2238/10000)
[Test]  Epoch: 25	Loss: 0.057064	Acc: 22.0% (2200/10000)
[Test]  Epoch: 26	Loss: 0.056781	Acc: 22.9% (2292/10000)
[Test]  Epoch: 27	Loss: 0.056675	Acc: 22.3% (2229/10000)
[Test]  Epoch: 28	Loss: 0.056799	Acc: 22.9% (2291/10000)
[Test]  Epoch: 29	Loss: 0.056547	Acc: 22.8% (2279/10000)
[Test]  Epoch: 30	Loss: 0.056166	Acc: 23.7% (2373/10000)
[Test]  Epoch: 31	Loss: 0.056311	Acc: 23.6% (2357/10000)
[Test]  Epoch: 32	Loss: 0.056295	Acc: 23.5% (2352/10000)
[Test]  Epoch: 33	Loss: 0.056153	Acc: 23.4% (2345/10000)
[Test]  Epoch: 34	Loss: 0.056031	Acc: 24.0% (2396/10000)
[Test]  Epoch: 35	Loss: 0.056017	Acc: 23.7% (2369/10000)
[Test]  Epoch: 36	Loss: 0.055832	Acc: 24.1% (2411/10000)
[Test]  Epoch: 37	Loss: 0.055762	Acc: 24.2% (2422/10000)
[Test]  Epoch: 38	Loss: 0.055950	Acc: 24.3% (2429/10000)
[Test]  Epoch: 39	Loss: 0.055530	Acc: 24.6% (2456/10000)
[Test]  Epoch: 40	Loss: 0.055652	Acc: 24.5% (2454/10000)
[Test]  Epoch: 41	Loss: 0.055875	Acc: 23.7% (2369/10000)
[Test]  Epoch: 42	Loss: 0.055731	Acc: 24.8% (2477/10000)
[Test]  Epoch: 43	Loss: 0.055724	Acc: 23.9% (2395/10000)
[Test]  Epoch: 44	Loss: 0.055520	Acc: 24.8% (2481/10000)
[Test]  Epoch: 45	Loss: 0.055605	Acc: 24.4% (2444/10000)
[Test]  Epoch: 46	Loss: 0.055761	Acc: 24.3% (2433/10000)
[Test]  Epoch: 47	Loss: 0.055542	Acc: 24.8% (2477/10000)
[Test]  Epoch: 48	Loss: 0.055480	Acc: 24.8% (2484/10000)
[Test]  Epoch: 49	Loss: 0.055524	Acc: 24.8% (2480/10000)
[Test]  Epoch: 50	Loss: 0.055723	Acc: 24.6% (2464/10000)
[Test]  Epoch: 51	Loss: 0.055201	Acc: 25.2% (2522/10000)
[Test]  Epoch: 52	Loss: 0.055318	Acc: 25.7% (2572/10000)
[Test]  Epoch: 53	Loss: 0.055245	Acc: 25.4% (2545/10000)
[Test]  Epoch: 54	Loss: 0.055163	Acc: 25.3% (2534/10000)
[Test]  Epoch: 55	Loss: 0.054983	Acc: 25.4% (2535/10000)
[Test]  Epoch: 56	Loss: 0.055094	Acc: 25.1% (2509/10000)
[Test]  Epoch: 57	Loss: 0.055216	Acc: 25.3% (2530/10000)
[Test]  Epoch: 58	Loss: 0.055154	Acc: 25.5% (2551/10000)
[Test]  Epoch: 59	Loss: 0.055518	Acc: 25.0% (2497/10000)
[Test]  Epoch: 60	Loss: 0.055339	Acc: 25.6% (2563/10000)
[Test]  Epoch: 61	Loss: 0.055033	Acc: 25.9% (2592/10000)
[Test]  Epoch: 62	Loss: 0.055101	Acc: 26.1% (2609/10000)
[Test]  Epoch: 63	Loss: 0.054882	Acc: 26.1% (2611/10000)
[Test]  Epoch: 64	Loss: 0.054828	Acc: 26.3% (2628/10000)
[Test]  Epoch: 65	Loss: 0.054889	Acc: 26.1% (2610/10000)
[Test]  Epoch: 66	Loss: 0.054929	Acc: 26.1% (2611/10000)
[Test]  Epoch: 67	Loss: 0.055014	Acc: 26.0% (2602/10000)
[Test]  Epoch: 68	Loss: 0.055107	Acc: 25.9% (2587/10000)
[Test]  Epoch: 69	Loss: 0.054966	Acc: 26.0% (2603/10000)
[Test]  Epoch: 70	Loss: 0.054950	Acc: 25.9% (2592/10000)
[Test]  Epoch: 71	Loss: 0.054891	Acc: 26.1% (2611/10000)
[Test]  Epoch: 72	Loss: 0.055038	Acc: 25.8% (2579/10000)
[Test]  Epoch: 73	Loss: 0.054892	Acc: 25.9% (2587/10000)
[Test]  Epoch: 74	Loss: 0.054891	Acc: 26.1% (2614/10000)
[Test]  Epoch: 75	Loss: 0.055013	Acc: 25.8% (2581/10000)
[Test]  Epoch: 76	Loss: 0.054924	Acc: 26.0% (2597/10000)
[Test]  Epoch: 77	Loss: 0.054976	Acc: 26.1% (2605/10000)
[Test]  Epoch: 78	Loss: 0.055063	Acc: 25.9% (2586/10000)
[Test]  Epoch: 79	Loss: 0.055070	Acc: 26.0% (2596/10000)
[Test]  Epoch: 80	Loss: 0.054961	Acc: 26.1% (2609/10000)
[Test]  Epoch: 81	Loss: 0.054955	Acc: 26.1% (2607/10000)
[Test]  Epoch: 82	Loss: 0.054939	Acc: 26.3% (2632/10000)
[Test]  Epoch: 83	Loss: 0.054970	Acc: 26.2% (2617/10000)
[Test]  Epoch: 84	Loss: 0.055063	Acc: 26.1% (2611/10000)
[Test]  Epoch: 85	Loss: 0.055018	Acc: 26.0% (2596/10000)
[Test]  Epoch: 86	Loss: 0.054984	Acc: 26.0% (2603/10000)
[Test]  Epoch: 87	Loss: 0.054971	Acc: 26.1% (2612/10000)
[Test]  Epoch: 88	Loss: 0.054989	Acc: 26.0% (2596/10000)
[Test]  Epoch: 89	Loss: 0.054886	Acc: 25.9% (2594/10000)
[Test]  Epoch: 90	Loss: 0.055029	Acc: 26.0% (2597/10000)
[Test]  Epoch: 91	Loss: 0.055014	Acc: 25.9% (2595/10000)
[Test]  Epoch: 92	Loss: 0.054920	Acc: 26.2% (2620/10000)
[Test]  Epoch: 93	Loss: 0.054950	Acc: 26.0% (2604/10000)
[Test]  Epoch: 94	Loss: 0.055011	Acc: 26.1% (2613/10000)
[Test]  Epoch: 95	Loss: 0.054962	Acc: 26.0% (2604/10000)
[Test]  Epoch: 96	Loss: 0.054945	Acc: 26.2% (2620/10000)
[Test]  Epoch: 97	Loss: 0.054921	Acc: 26.0% (2603/10000)
[Test]  Epoch: 98	Loss: 0.054993	Acc: 26.1% (2605/10000)
[Test]  Epoch: 99	Loss: 0.054956	Acc: 25.7% (2571/10000)
[Test]  Epoch: 100	Loss: 0.054956	Acc: 25.9% (2587/10000)
===========finish==========
['2024-08-18', '19:41:05.215993', '100', 'test', '0.05495577867031098', '25.87', '26.32']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.5 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=52
get_sample_layers not_random
52 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.1.conv.2.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.3.weight', '_features.14.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.17.conv.3.weight', '_features.11.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.0.1.weight', '_features.17.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.1.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.17.conv.1.1.weight', '_features.17.conv.0.1.weight', '_features.13.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.12.conv.2.weight', '_features.13.conv.2.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.0.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.082901	Acc: 1.6% (163/10000)
[Test]  Epoch: 2	Loss: 0.076541	Acc: 6.3% (626/10000)
[Test]  Epoch: 3	Loss: 0.073493	Acc: 8.3% (830/10000)
[Test]  Epoch: 4	Loss: 0.070932	Acc: 10.2% (1024/10000)
[Test]  Epoch: 5	Loss: 0.069149	Acc: 11.0% (1104/10000)
[Test]  Epoch: 6	Loss: 0.067527	Acc: 12.4% (1240/10000)
[Test]  Epoch: 7	Loss: 0.065977	Acc: 13.3% (1326/10000)
[Test]  Epoch: 8	Loss: 0.064321	Acc: 14.2% (1424/10000)
[Test]  Epoch: 9	Loss: 0.063589	Acc: 15.3% (1535/10000)
[Test]  Epoch: 10	Loss: 0.062563	Acc: 16.3% (1634/10000)
[Test]  Epoch: 11	Loss: 0.061963	Acc: 16.0% (1596/10000)
[Test]  Epoch: 12	Loss: 0.062085	Acc: 17.3% (1733/10000)
[Test]  Epoch: 13	Loss: 0.061030	Acc: 17.6% (1760/10000)
[Test]  Epoch: 14	Loss: 0.060594	Acc: 18.4% (1840/10000)
[Test]  Epoch: 15	Loss: 0.060525	Acc: 18.0% (1802/10000)
[Test]  Epoch: 16	Loss: 0.059538	Acc: 18.8% (1881/10000)
[Test]  Epoch: 17	Loss: 0.059283	Acc: 18.8% (1884/10000)
[Test]  Epoch: 18	Loss: 0.059168	Acc: 19.0% (1904/10000)
[Test]  Epoch: 19	Loss: 0.058979	Acc: 19.6% (1959/10000)
[Test]  Epoch: 20	Loss: 0.058785	Acc: 19.7% (1970/10000)
[Test]  Epoch: 21	Loss: 0.058396	Acc: 20.1% (2008/10000)
[Test]  Epoch: 22	Loss: 0.058633	Acc: 20.1% (2013/10000)
[Test]  Epoch: 23	Loss: 0.057725	Acc: 20.9% (2087/10000)
[Test]  Epoch: 24	Loss: 0.058190	Acc: 20.9% (2091/10000)
[Test]  Epoch: 25	Loss: 0.057739	Acc: 21.1% (2108/10000)
[Test]  Epoch: 26	Loss: 0.058208	Acc: 20.9% (2092/10000)
[Test]  Epoch: 27	Loss: 0.057883	Acc: 21.2% (2121/10000)
[Test]  Epoch: 28	Loss: 0.057757	Acc: 20.8% (2081/10000)
[Test]  Epoch: 29	Loss: 0.057284	Acc: 21.5% (2147/10000)
[Test]  Epoch: 30	Loss: 0.057387	Acc: 21.5% (2151/10000)
[Test]  Epoch: 31	Loss: 0.057444	Acc: 21.3% (2134/10000)
[Test]  Epoch: 32	Loss: 0.057174	Acc: 22.0% (2198/10000)
[Test]  Epoch: 33	Loss: 0.057053	Acc: 22.4% (2241/10000)
[Test]  Epoch: 34	Loss: 0.056853	Acc: 22.3% (2231/10000)
[Test]  Epoch: 35	Loss: 0.057087	Acc: 22.3% (2227/10000)
[Test]  Epoch: 36	Loss: 0.056897	Acc: 22.4% (2241/10000)
[Test]  Epoch: 37	Loss: 0.056896	Acc: 22.6% (2261/10000)
[Test]  Epoch: 38	Loss: 0.057050	Acc: 22.6% (2262/10000)
[Test]  Epoch: 39	Loss: 0.056824	Acc: 22.6% (2259/10000)
[Test]  Epoch: 40	Loss: 0.056731	Acc: 22.8% (2282/10000)
[Test]  Epoch: 41	Loss: 0.056580	Acc: 22.9% (2286/10000)
[Test]  Epoch: 42	Loss: 0.056712	Acc: 23.3% (2334/10000)
[Test]  Epoch: 43	Loss: 0.056570	Acc: 23.1% (2307/10000)
[Test]  Epoch: 44	Loss: 0.056837	Acc: 22.6% (2258/10000)
[Test]  Epoch: 45	Loss: 0.056631	Acc: 22.6% (2264/10000)
[Test]  Epoch: 46	Loss: 0.056698	Acc: 23.0% (2300/10000)
[Test]  Epoch: 47	Loss: 0.056451	Acc: 23.4% (2335/10000)
[Test]  Epoch: 48	Loss: 0.056336	Acc: 23.4% (2341/10000)
[Test]  Epoch: 49	Loss: 0.056410	Acc: 23.5% (2347/10000)
[Test]  Epoch: 50	Loss: 0.056674	Acc: 23.3% (2328/10000)
[Test]  Epoch: 51	Loss: 0.056542	Acc: 23.5% (2351/10000)
[Test]  Epoch: 52	Loss: 0.056095	Acc: 24.1% (2405/10000)
[Test]  Epoch: 53	Loss: 0.056423	Acc: 23.9% (2390/10000)
[Test]  Epoch: 54	Loss: 0.056186	Acc: 23.7% (2366/10000)
[Test]  Epoch: 55	Loss: 0.056266	Acc: 23.8% (2376/10000)
[Test]  Epoch: 56	Loss: 0.056168	Acc: 23.9% (2393/10000)
[Test]  Epoch: 57	Loss: 0.056249	Acc: 23.7% (2371/10000)
[Test]  Epoch: 58	Loss: 0.055948	Acc: 24.3% (2427/10000)
[Test]  Epoch: 59	Loss: 0.056180	Acc: 24.1% (2406/10000)
[Test]  Epoch: 60	Loss: 0.056339	Acc: 23.8% (2376/10000)
[Test]  Epoch: 61	Loss: 0.056165	Acc: 24.2% (2416/10000)
[Test]  Epoch: 62	Loss: 0.056159	Acc: 24.1% (2411/10000)
[Test]  Epoch: 63	Loss: 0.055901	Acc: 24.4% (2443/10000)
[Test]  Epoch: 64	Loss: 0.055897	Acc: 24.3% (2427/10000)
[Test]  Epoch: 65	Loss: 0.055972	Acc: 24.3% (2433/10000)
[Test]  Epoch: 66	Loss: 0.055961	Acc: 24.4% (2443/10000)
[Test]  Epoch: 67	Loss: 0.056011	Acc: 24.4% (2443/10000)
[Test]  Epoch: 68	Loss: 0.056118	Acc: 24.3% (2427/10000)
[Test]  Epoch: 69	Loss: 0.055959	Acc: 24.3% (2432/10000)
[Test]  Epoch: 70	Loss: 0.055988	Acc: 24.4% (2441/10000)
[Test]  Epoch: 71	Loss: 0.055846	Acc: 24.5% (2450/10000)
[Test]  Epoch: 72	Loss: 0.056008	Acc: 24.2% (2421/10000)
[Test]  Epoch: 73	Loss: 0.055914	Acc: 24.6% (2459/10000)
[Test]  Epoch: 74	Loss: 0.055913	Acc: 24.4% (2438/10000)
[Test]  Epoch: 75	Loss: 0.056024	Acc: 24.3% (2431/10000)
[Test]  Epoch: 76	Loss: 0.055930	Acc: 24.4% (2445/10000)
[Test]  Epoch: 77	Loss: 0.056002	Acc: 24.4% (2436/10000)
[Test]  Epoch: 78	Loss: 0.056042	Acc: 24.1% (2414/10000)
[Test]  Epoch: 79	Loss: 0.056064	Acc: 24.3% (2434/10000)
[Test]  Epoch: 80	Loss: 0.055918	Acc: 24.5% (2449/10000)
[Test]  Epoch: 81	Loss: 0.055933	Acc: 24.4% (2441/10000)
[Test]  Epoch: 82	Loss: 0.055918	Acc: 24.6% (2460/10000)
[Test]  Epoch: 83	Loss: 0.055977	Acc: 24.6% (2464/10000)
[Test]  Epoch: 84	Loss: 0.056024	Acc: 24.4% (2443/10000)
[Test]  Epoch: 85	Loss: 0.055964	Acc: 24.5% (2448/10000)
[Test]  Epoch: 86	Loss: 0.055984	Acc: 24.5% (2452/10000)
[Test]  Epoch: 87	Loss: 0.055961	Acc: 24.4% (2435/10000)
[Test]  Epoch: 88	Loss: 0.055946	Acc: 24.5% (2448/10000)
[Test]  Epoch: 89	Loss: 0.055871	Acc: 24.4% (2436/10000)
[Test]  Epoch: 90	Loss: 0.055967	Acc: 24.2% (2421/10000)
[Test]  Epoch: 91	Loss: 0.055944	Acc: 24.5% (2451/10000)
[Test]  Epoch: 92	Loss: 0.055872	Acc: 24.6% (2462/10000)
[Test]  Epoch: 93	Loss: 0.055920	Acc: 24.6% (2461/10000)
[Test]  Epoch: 94	Loss: 0.055995	Acc: 24.6% (2463/10000)
[Test]  Epoch: 95	Loss: 0.055929	Acc: 24.4% (2439/10000)
[Test]  Epoch: 96	Loss: 0.055898	Acc: 24.6% (2456/10000)
[Test]  Epoch: 97	Loss: 0.055907	Acc: 24.4% (2443/10000)
[Test]  Epoch: 98	Loss: 0.055955	Acc: 24.4% (2441/10000)
[Test]  Epoch: 99	Loss: 0.055907	Acc: 24.2% (2422/10000)
[Test]  Epoch: 100	Loss: 0.055909	Acc: 24.4% (2442/10000)
===========finish==========
['2024-08-18', '19:43:23.455313', '100', 'test', '0.05590940651893616', '24.42', '24.64']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.6 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=63
get_sample_layers not_random
63 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.1.conv.2.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.3.weight', '_features.14.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.17.conv.3.weight', '_features.11.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.0.1.weight', '_features.17.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.1.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.17.conv.1.1.weight', '_features.17.conv.0.1.weight', '_features.13.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.12.conv.2.weight', '_features.13.conv.2.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.0.0.weight', '_features.6.conv.0.0.weight', '_features.4.conv.3.weight', '_features.6.conv.1.0.weight', '_features.11.conv.3.weight', '_features.3.conv.1.1.weight', '_features.14.conv.3.weight', '_features.8.conv.2.weight', '_features.8.conv.0.0.weight', '_features.3.conv.3.weight', '_features.10.conv.0.0.weight', '_features.9.conv.2.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.083138	Acc: 1.1% (106/10000)
[Test]  Epoch: 2	Loss: 0.076764	Acc: 6.3% (626/10000)
[Test]  Epoch: 3	Loss: 0.073422	Acc: 8.1% (811/10000)
[Test]  Epoch: 4	Loss: 0.071088	Acc: 10.0% (1003/10000)
[Test]  Epoch: 5	Loss: 0.069304	Acc: 11.1% (1111/10000)
[Test]  Epoch: 6	Loss: 0.067677	Acc: 12.5% (1247/10000)
[Test]  Epoch: 7	Loss: 0.066333	Acc: 12.9% (1295/10000)
[Test]  Epoch: 8	Loss: 0.064515	Acc: 13.9% (1395/10000)
[Test]  Epoch: 9	Loss: 0.063831	Acc: 15.0% (1498/10000)
[Test]  Epoch: 10	Loss: 0.063242	Acc: 15.6% (1557/10000)
[Test]  Epoch: 11	Loss: 0.062446	Acc: 15.8% (1581/10000)
[Test]  Epoch: 12	Loss: 0.062028	Acc: 16.8% (1677/10000)
[Test]  Epoch: 13	Loss: 0.061670	Acc: 17.0% (1699/10000)
[Test]  Epoch: 14	Loss: 0.060415	Acc: 17.8% (1778/10000)
[Test]  Epoch: 15	Loss: 0.060437	Acc: 18.0% (1796/10000)
[Test]  Epoch: 16	Loss: 0.059632	Acc: 18.5% (1854/10000)
[Test]  Epoch: 17	Loss: 0.059670	Acc: 18.8% (1883/10000)
[Test]  Epoch: 18	Loss: 0.059395	Acc: 19.4% (1935/10000)
[Test]  Epoch: 19	Loss: 0.059025	Acc: 19.4% (1942/10000)
[Test]  Epoch: 20	Loss: 0.058952	Acc: 19.6% (1961/10000)
[Test]  Epoch: 21	Loss: 0.058886	Acc: 19.3% (1931/10000)
[Test]  Epoch: 22	Loss: 0.059050	Acc: 20.2% (2024/10000)
[Test]  Epoch: 23	Loss: 0.058244	Acc: 20.2% (2024/10000)
[Test]  Epoch: 24	Loss: 0.058413	Acc: 19.8% (1983/10000)
[Test]  Epoch: 25	Loss: 0.058198	Acc: 20.3% (2034/10000)
[Test]  Epoch: 26	Loss: 0.058348	Acc: 20.7% (2071/10000)
[Test]  Epoch: 27	Loss: 0.058199	Acc: 20.4% (2044/10000)
[Test]  Epoch: 28	Loss: 0.058070	Acc: 20.9% (2091/10000)
[Test]  Epoch: 29	Loss: 0.057477	Acc: 20.6% (2064/10000)
[Test]  Epoch: 30	Loss: 0.057803	Acc: 21.6% (2156/10000)
[Test]  Epoch: 31	Loss: 0.057846	Acc: 21.3% (2128/10000)
[Test]  Epoch: 32	Loss: 0.057479	Acc: 21.3% (2131/10000)
[Test]  Epoch: 33	Loss: 0.057461	Acc: 22.2% (2219/10000)
[Test]  Epoch: 34	Loss: 0.057427	Acc: 21.5% (2147/10000)
[Test]  Epoch: 35	Loss: 0.057278	Acc: 21.8% (2175/10000)
[Test]  Epoch: 36	Loss: 0.057313	Acc: 21.3% (2132/10000)
[Test]  Epoch: 37	Loss: 0.057161	Acc: 21.9% (2191/10000)
[Test]  Epoch: 38	Loss: 0.057171	Acc: 22.0% (2197/10000)
[Test]  Epoch: 39	Loss: 0.056936	Acc: 22.0% (2199/10000)
[Test]  Epoch: 40	Loss: 0.057212	Acc: 21.8% (2184/10000)
[Test]  Epoch: 41	Loss: 0.057125	Acc: 22.0% (2196/10000)
[Test]  Epoch: 42	Loss: 0.057026	Acc: 22.4% (2244/10000)
[Test]  Epoch: 43	Loss: 0.057207	Acc: 22.0% (2202/10000)
[Test]  Epoch: 44	Loss: 0.057060	Acc: 22.3% (2228/10000)
[Test]  Epoch: 45	Loss: 0.056844	Acc: 22.4% (2244/10000)
[Test]  Epoch: 46	Loss: 0.056798	Acc: 22.6% (2258/10000)
[Test]  Epoch: 47	Loss: 0.057043	Acc: 22.5% (2252/10000)
[Test]  Epoch: 48	Loss: 0.056788	Acc: 22.4% (2236/10000)
[Test]  Epoch: 49	Loss: 0.056821	Acc: 22.9% (2285/10000)
[Test]  Epoch: 50	Loss: 0.056870	Acc: 22.7% (2270/10000)
[Test]  Epoch: 51	Loss: 0.056643	Acc: 22.9% (2287/10000)
[Test]  Epoch: 52	Loss: 0.056389	Acc: 23.4% (2343/10000)
[Test]  Epoch: 53	Loss: 0.056643	Acc: 23.2% (2319/10000)
[Test]  Epoch: 54	Loss: 0.056588	Acc: 22.9% (2295/10000)
[Test]  Epoch: 55	Loss: 0.056446	Acc: 23.2% (2321/10000)
[Test]  Epoch: 56	Loss: 0.056550	Acc: 22.9% (2295/10000)
[Test]  Epoch: 57	Loss: 0.056491	Acc: 23.2% (2325/10000)
[Test]  Epoch: 58	Loss: 0.056420	Acc: 22.8% (2277/10000)
[Test]  Epoch: 59	Loss: 0.056517	Acc: 23.1% (2307/10000)
[Test]  Epoch: 60	Loss: 0.056638	Acc: 23.2% (2324/10000)
[Test]  Epoch: 61	Loss: 0.056419	Acc: 23.4% (2343/10000)
[Test]  Epoch: 62	Loss: 0.056446	Acc: 23.5% (2354/10000)
[Test]  Epoch: 63	Loss: 0.056250	Acc: 23.5% (2353/10000)
[Test]  Epoch: 64	Loss: 0.056216	Acc: 23.6% (2361/10000)
[Test]  Epoch: 65	Loss: 0.056299	Acc: 23.4% (2340/10000)
[Test]  Epoch: 66	Loss: 0.056252	Acc: 23.6% (2360/10000)
[Test]  Epoch: 67	Loss: 0.056316	Acc: 23.5% (2349/10000)
[Test]  Epoch: 68	Loss: 0.056443	Acc: 23.3% (2333/10000)
[Test]  Epoch: 69	Loss: 0.056289	Acc: 23.6% (2360/10000)
[Test]  Epoch: 70	Loss: 0.056296	Acc: 23.5% (2349/10000)
[Test]  Epoch: 71	Loss: 0.056190	Acc: 23.7% (2367/10000)
[Test]  Epoch: 72	Loss: 0.056326	Acc: 23.5% (2347/10000)
[Test]  Epoch: 73	Loss: 0.056249	Acc: 23.6% (2357/10000)
[Test]  Epoch: 74	Loss: 0.056211	Acc: 23.5% (2347/10000)
[Test]  Epoch: 75	Loss: 0.056344	Acc: 23.5% (2350/10000)
[Test]  Epoch: 76	Loss: 0.056284	Acc: 23.4% (2345/10000)
[Test]  Epoch: 77	Loss: 0.056315	Acc: 23.5% (2352/10000)
[Test]  Epoch: 78	Loss: 0.056346	Acc: 23.4% (2336/10000)
[Test]  Epoch: 79	Loss: 0.056396	Acc: 23.6% (2358/10000)
[Test]  Epoch: 80	Loss: 0.056271	Acc: 23.7% (2370/10000)
[Test]  Epoch: 81	Loss: 0.056269	Acc: 23.6% (2358/10000)
[Test]  Epoch: 82	Loss: 0.056257	Acc: 23.5% (2348/10000)
[Test]  Epoch: 83	Loss: 0.056330	Acc: 23.6% (2358/10000)
[Test]  Epoch: 84	Loss: 0.056340	Acc: 23.6% (2355/10000)
[Test]  Epoch: 85	Loss: 0.056299	Acc: 23.7% (2374/10000)
[Test]  Epoch: 86	Loss: 0.056325	Acc: 23.5% (2349/10000)
[Test]  Epoch: 87	Loss: 0.056306	Acc: 23.7% (2370/10000)
[Test]  Epoch: 88	Loss: 0.056282	Acc: 23.6% (2361/10000)
[Test]  Epoch: 89	Loss: 0.056221	Acc: 23.5% (2351/10000)
[Test]  Epoch: 90	Loss: 0.056311	Acc: 23.5% (2349/10000)
[Test]  Epoch: 91	Loss: 0.056290	Acc: 23.5% (2346/10000)
[Test]  Epoch: 92	Loss: 0.056235	Acc: 23.8% (2375/10000)
[Test]  Epoch: 93	Loss: 0.056272	Acc: 23.7% (2366/10000)
[Test]  Epoch: 94	Loss: 0.056336	Acc: 23.6% (2359/10000)
[Test]  Epoch: 95	Loss: 0.056290	Acc: 23.6% (2362/10000)
[Test]  Epoch: 96	Loss: 0.056273	Acc: 23.7% (2372/10000)
[Test]  Epoch: 97	Loss: 0.056264	Acc: 23.6% (2362/10000)
[Test]  Epoch: 98	Loss: 0.056311	Acc: 23.5% (2346/10000)
[Test]  Epoch: 99	Loss: 0.056272	Acc: 23.6% (2357/10000)
[Test]  Epoch: 100	Loss: 0.056261	Acc: 23.4% (2345/10000)
===========finish==========
['2024-08-18', '19:45:39.758839', '100', 'test', '0.05626138904094696', '23.45', '23.75']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.7 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=73
get_sample_layers not_random
73 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.1.conv.2.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.3.weight', '_features.14.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.17.conv.3.weight', '_features.11.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.0.1.weight', '_features.17.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.1.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.17.conv.1.1.weight', '_features.17.conv.0.1.weight', '_features.13.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.12.conv.2.weight', '_features.13.conv.2.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.0.0.weight', '_features.6.conv.0.0.weight', '_features.4.conv.3.weight', '_features.6.conv.1.0.weight', '_features.11.conv.3.weight', '_features.3.conv.1.1.weight', '_features.14.conv.3.weight', '_features.8.conv.2.weight', '_features.8.conv.0.0.weight', '_features.3.conv.3.weight', '_features.10.conv.0.0.weight', '_features.9.conv.2.weight', '_features.9.conv.0.0.weight', '_features.7.conv.3.weight', '_features.10.conv.2.weight', '_features.12.conv.1.0.weight', '_features.5.conv.0.1.weight', '_features.6.conv.3.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.13.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.084277	Acc: 1.2% (117/10000)
[Test]  Epoch: 2	Loss: 0.076929	Acc: 6.1% (606/10000)
[Test]  Epoch: 3	Loss: 0.073752	Acc: 8.3% (834/10000)
[Test]  Epoch: 4	Loss: 0.071473	Acc: 9.8% (985/10000)
[Test]  Epoch: 5	Loss: 0.069561	Acc: 11.0% (1103/10000)
[Test]  Epoch: 6	Loss: 0.067859	Acc: 12.5% (1254/10000)
[Test]  Epoch: 7	Loss: 0.066580	Acc: 12.9% (1294/10000)
[Test]  Epoch: 8	Loss: 0.064560	Acc: 14.0% (1400/10000)
[Test]  Epoch: 9	Loss: 0.064116	Acc: 14.9% (1487/10000)
[Test]  Epoch: 10	Loss: 0.063152	Acc: 15.0% (1504/10000)
[Test]  Epoch: 11	Loss: 0.062420	Acc: 15.9% (1591/10000)
[Test]  Epoch: 12	Loss: 0.062485	Acc: 16.6% (1656/10000)
[Test]  Epoch: 13	Loss: 0.061428	Acc: 16.4% (1642/10000)
[Test]  Epoch: 14	Loss: 0.060821	Acc: 17.4% (1743/10000)
[Test]  Epoch: 15	Loss: 0.060393	Acc: 17.8% (1775/10000)
[Test]  Epoch: 16	Loss: 0.060053	Acc: 17.6% (1764/10000)
[Test]  Epoch: 17	Loss: 0.059941	Acc: 18.1% (1810/10000)
[Test]  Epoch: 18	Loss: 0.059628	Acc: 18.0% (1801/10000)
[Test]  Epoch: 19	Loss: 0.059458	Acc: 18.8% (1884/10000)
[Test]  Epoch: 20	Loss: 0.059251	Acc: 19.1% (1907/10000)
[Test]  Epoch: 21	Loss: 0.058810	Acc: 19.2% (1923/10000)
[Test]  Epoch: 22	Loss: 0.059129	Acc: 19.6% (1962/10000)
[Test]  Epoch: 23	Loss: 0.058413	Acc: 19.9% (1991/10000)
[Test]  Epoch: 24	Loss: 0.058678	Acc: 19.6% (1960/10000)
[Test]  Epoch: 25	Loss: 0.058233	Acc: 20.1% (2006/10000)
[Test]  Epoch: 26	Loss: 0.058445	Acc: 20.1% (2014/10000)
[Test]  Epoch: 27	Loss: 0.058414	Acc: 19.3% (1928/10000)
[Test]  Epoch: 28	Loss: 0.058114	Acc: 20.3% (2033/10000)
[Test]  Epoch: 29	Loss: 0.057857	Acc: 20.2% (2024/10000)
[Test]  Epoch: 30	Loss: 0.057734	Acc: 20.9% (2094/10000)
[Test]  Epoch: 31	Loss: 0.057842	Acc: 21.0% (2096/10000)
[Test]  Epoch: 32	Loss: 0.057843	Acc: 20.8% (2075/10000)
[Test]  Epoch: 33	Loss: 0.057530	Acc: 21.4% (2135/10000)
[Test]  Epoch: 34	Loss: 0.057611	Acc: 21.2% (2125/10000)
[Test]  Epoch: 35	Loss: 0.057513	Acc: 21.3% (2133/10000)
[Test]  Epoch: 36	Loss: 0.057408	Acc: 21.4% (2136/10000)
[Test]  Epoch: 37	Loss: 0.057178	Acc: 21.7% (2171/10000)
[Test]  Epoch: 38	Loss: 0.057398	Acc: 21.9% (2188/10000)
[Test]  Epoch: 39	Loss: 0.057417	Acc: 21.5% (2151/10000)
[Test]  Epoch: 40	Loss: 0.057370	Acc: 21.9% (2188/10000)
[Test]  Epoch: 41	Loss: 0.057114	Acc: 21.9% (2192/10000)
[Test]  Epoch: 42	Loss: 0.057136	Acc: 21.8% (2182/10000)
[Test]  Epoch: 43	Loss: 0.057180	Acc: 21.8% (2180/10000)
[Test]  Epoch: 44	Loss: 0.057438	Acc: 21.8% (2180/10000)
[Test]  Epoch: 45	Loss: 0.057140	Acc: 21.7% (2166/10000)
[Test]  Epoch: 46	Loss: 0.057084	Acc: 22.1% (2209/10000)
[Test]  Epoch: 47	Loss: 0.057173	Acc: 22.1% (2211/10000)
[Test]  Epoch: 48	Loss: 0.056727	Acc: 22.4% (2242/10000)
[Test]  Epoch: 49	Loss: 0.056946	Acc: 22.3% (2234/10000)
[Test]  Epoch: 50	Loss: 0.057128	Acc: 22.1% (2211/10000)
[Test]  Epoch: 51	Loss: 0.056829	Acc: 22.3% (2233/10000)
[Test]  Epoch: 52	Loss: 0.056760	Acc: 23.1% (2306/10000)
[Test]  Epoch: 53	Loss: 0.056818	Acc: 22.7% (2271/10000)
[Test]  Epoch: 54	Loss: 0.056521	Acc: 22.8% (2277/10000)
[Test]  Epoch: 55	Loss: 0.056660	Acc: 22.8% (2278/10000)
[Test]  Epoch: 56	Loss: 0.056507	Acc: 22.8% (2276/10000)
[Test]  Epoch: 57	Loss: 0.056693	Acc: 22.4% (2235/10000)
[Test]  Epoch: 58	Loss: 0.056452	Acc: 22.9% (2290/10000)
[Test]  Epoch: 59	Loss: 0.056758	Acc: 22.5% (2250/10000)
[Test]  Epoch: 60	Loss: 0.056635	Acc: 22.8% (2277/10000)
[Test]  Epoch: 61	Loss: 0.056546	Acc: 22.8% (2275/10000)
[Test]  Epoch: 62	Loss: 0.056530	Acc: 22.8% (2284/10000)
[Test]  Epoch: 63	Loss: 0.056329	Acc: 23.1% (2313/10000)
[Test]  Epoch: 64	Loss: 0.056285	Acc: 23.3% (2326/10000)
[Test]  Epoch: 65	Loss: 0.056385	Acc: 23.2% (2320/10000)
[Test]  Epoch: 66	Loss: 0.056395	Acc: 23.2% (2318/10000)
[Test]  Epoch: 67	Loss: 0.056455	Acc: 23.1% (2310/10000)
[Test]  Epoch: 68	Loss: 0.056566	Acc: 22.9% (2287/10000)
[Test]  Epoch: 69	Loss: 0.056421	Acc: 23.1% (2311/10000)
[Test]  Epoch: 70	Loss: 0.056408	Acc: 23.3% (2327/10000)
[Test]  Epoch: 71	Loss: 0.056311	Acc: 23.5% (2350/10000)
[Test]  Epoch: 72	Loss: 0.056471	Acc: 23.2% (2323/10000)
[Test]  Epoch: 73	Loss: 0.056387	Acc: 23.4% (2337/10000)
[Test]  Epoch: 74	Loss: 0.056341	Acc: 23.1% (2313/10000)
[Test]  Epoch: 75	Loss: 0.056447	Acc: 23.1% (2308/10000)
[Test]  Epoch: 76	Loss: 0.056393	Acc: 23.3% (2328/10000)
[Test]  Epoch: 77	Loss: 0.056436	Acc: 23.0% (2301/10000)
[Test]  Epoch: 78	Loss: 0.056455	Acc: 22.9% (2291/10000)
[Test]  Epoch: 79	Loss: 0.056468	Acc: 23.1% (2314/10000)
[Test]  Epoch: 80	Loss: 0.056382	Acc: 23.2% (2320/10000)
[Test]  Epoch: 81	Loss: 0.056395	Acc: 23.3% (2328/10000)
[Test]  Epoch: 82	Loss: 0.056359	Acc: 23.4% (2343/10000)
[Test]  Epoch: 83	Loss: 0.056425	Acc: 23.3% (2327/10000)
[Test]  Epoch: 84	Loss: 0.056437	Acc: 23.4% (2336/10000)
[Test]  Epoch: 85	Loss: 0.056409	Acc: 23.1% (2310/10000)
[Test]  Epoch: 86	Loss: 0.056413	Acc: 23.3% (2327/10000)
[Test]  Epoch: 87	Loss: 0.056380	Acc: 23.4% (2335/10000)
[Test]  Epoch: 88	Loss: 0.056393	Acc: 23.4% (2337/10000)
[Test]  Epoch: 89	Loss: 0.056296	Acc: 23.4% (2343/10000)
[Test]  Epoch: 90	Loss: 0.056437	Acc: 23.3% (2328/10000)
[Test]  Epoch: 91	Loss: 0.056419	Acc: 23.2% (2317/10000)
[Test]  Epoch: 92	Loss: 0.056352	Acc: 23.3% (2333/10000)
[Test]  Epoch: 93	Loss: 0.056358	Acc: 23.2% (2325/10000)
[Test]  Epoch: 94	Loss: 0.056423	Acc: 23.5% (2346/10000)
[Test]  Epoch: 95	Loss: 0.056409	Acc: 23.3% (2326/10000)
[Test]  Epoch: 96	Loss: 0.056365	Acc: 23.3% (2326/10000)
[Test]  Epoch: 97	Loss: 0.056394	Acc: 23.3% (2333/10000)
[Test]  Epoch: 98	Loss: 0.056430	Acc: 23.2% (2316/10000)
[Test]  Epoch: 99	Loss: 0.056360	Acc: 23.2% (2318/10000)
[Test]  Epoch: 100	Loss: 0.056365	Acc: 23.2% (2325/10000)
===========finish==========
['2024-08-18', '19:47:59.927984', '100', 'test', '0.056365002703666685', '23.25', '23.5']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.8 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=84
get_sample_layers not_random
84 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.1.conv.2.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.3.weight', '_features.14.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.17.conv.3.weight', '_features.11.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.0.1.weight', '_features.17.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.1.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.17.conv.1.1.weight', '_features.17.conv.0.1.weight', '_features.13.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.12.conv.2.weight', '_features.13.conv.2.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.0.0.weight', '_features.6.conv.0.0.weight', '_features.4.conv.3.weight', '_features.6.conv.1.0.weight', '_features.11.conv.3.weight', '_features.3.conv.1.1.weight', '_features.14.conv.3.weight', '_features.8.conv.2.weight', '_features.8.conv.0.0.weight', '_features.3.conv.3.weight', '_features.10.conv.0.0.weight', '_features.9.conv.2.weight', '_features.9.conv.0.0.weight', '_features.7.conv.3.weight', '_features.10.conv.2.weight', '_features.12.conv.1.0.weight', '_features.5.conv.0.1.weight', '_features.6.conv.3.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.5.conv.3.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.2.weight', '_features.13.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.0.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.087002	Acc: 0.9% (92/10000)
[Test]  Epoch: 2	Loss: 0.077242	Acc: 5.5% (550/10000)
[Test]  Epoch: 3	Loss: 0.074102	Acc: 7.4% (737/10000)
[Test]  Epoch: 4	Loss: 0.071469	Acc: 9.7% (973/10000)
[Test]  Epoch: 5	Loss: 0.069946	Acc: 11.0% (1102/10000)
[Test]  Epoch: 6	Loss: 0.067842	Acc: 12.4% (1239/10000)
[Test]  Epoch: 7	Loss: 0.066865	Acc: 12.8% (1278/10000)
[Test]  Epoch: 8	Loss: 0.065463	Acc: 13.5% (1348/10000)
[Test]  Epoch: 9	Loss: 0.064730	Acc: 14.7% (1474/10000)
[Test]  Epoch: 10	Loss: 0.063664	Acc: 15.0% (1496/10000)
[Test]  Epoch: 11	Loss: 0.062771	Acc: 15.6% (1561/10000)
[Test]  Epoch: 12	Loss: 0.062546	Acc: 16.1% (1606/10000)
[Test]  Epoch: 13	Loss: 0.061714	Acc: 16.6% (1658/10000)
[Test]  Epoch: 14	Loss: 0.061342	Acc: 16.8% (1684/10000)
[Test]  Epoch: 15	Loss: 0.061018	Acc: 17.6% (1762/10000)
[Test]  Epoch: 16	Loss: 0.060378	Acc: 18.0% (1799/10000)
[Test]  Epoch: 17	Loss: 0.060282	Acc: 18.0% (1798/10000)
[Test]  Epoch: 18	Loss: 0.059906	Acc: 18.4% (1835/10000)
[Test]  Epoch: 19	Loss: 0.059502	Acc: 18.9% (1886/10000)
[Test]  Epoch: 20	Loss: 0.059438	Acc: 19.2% (1920/10000)
[Test]  Epoch: 21	Loss: 0.059317	Acc: 18.9% (1886/10000)
[Test]  Epoch: 22	Loss: 0.059256	Acc: 19.5% (1949/10000)
[Test]  Epoch: 23	Loss: 0.058875	Acc: 19.5% (1947/10000)
[Test]  Epoch: 24	Loss: 0.059334	Acc: 19.4% (1940/10000)
[Test]  Epoch: 25	Loss: 0.058490	Acc: 19.8% (1982/10000)
[Test]  Epoch: 26	Loss: 0.058558	Acc: 20.5% (2054/10000)
[Test]  Epoch: 27	Loss: 0.058319	Acc: 19.7% (1973/10000)
[Test]  Epoch: 28	Loss: 0.058364	Acc: 20.6% (2062/10000)
[Test]  Epoch: 29	Loss: 0.058238	Acc: 20.3% (2027/10000)
[Test]  Epoch: 30	Loss: 0.058288	Acc: 20.5% (2053/10000)
[Test]  Epoch: 31	Loss: 0.058348	Acc: 20.5% (2046/10000)
[Test]  Epoch: 32	Loss: 0.057999	Acc: 20.6% (2064/10000)
[Test]  Epoch: 33	Loss: 0.057971	Acc: 20.9% (2085/10000)
[Test]  Epoch: 34	Loss: 0.057544	Acc: 21.2% (2123/10000)
[Test]  Epoch: 35	Loss: 0.057778	Acc: 21.1% (2112/10000)
[Test]  Epoch: 36	Loss: 0.057607	Acc: 21.3% (2134/10000)
[Test]  Epoch: 37	Loss: 0.057373	Acc: 21.4% (2142/10000)
[Test]  Epoch: 38	Loss: 0.057815	Acc: 21.2% (2120/10000)
[Test]  Epoch: 39	Loss: 0.057331	Acc: 22.0% (2201/10000)
[Test]  Epoch: 40	Loss: 0.057418	Acc: 21.7% (2169/10000)
[Test]  Epoch: 41	Loss: 0.057684	Acc: 21.0% (2098/10000)
[Test]  Epoch: 42	Loss: 0.057499	Acc: 21.3% (2133/10000)
[Test]  Epoch: 43	Loss: 0.057353	Acc: 22.4% (2244/10000)
[Test]  Epoch: 44	Loss: 0.057327	Acc: 21.7% (2167/10000)
[Test]  Epoch: 45	Loss: 0.057258	Acc: 22.1% (2215/10000)
[Test]  Epoch: 46	Loss: 0.057433	Acc: 21.6% (2155/10000)
[Test]  Epoch: 47	Loss: 0.057056	Acc: 22.1% (2207/10000)
[Test]  Epoch: 48	Loss: 0.056916	Acc: 22.4% (2239/10000)
[Test]  Epoch: 49	Loss: 0.057144	Acc: 22.1% (2206/10000)
[Test]  Epoch: 50	Loss: 0.057315	Acc: 22.3% (2227/10000)
[Test]  Epoch: 51	Loss: 0.056843	Acc: 22.4% (2237/10000)
[Test]  Epoch: 52	Loss: 0.056893	Acc: 22.7% (2267/10000)
[Test]  Epoch: 53	Loss: 0.056956	Acc: 22.4% (2241/10000)
[Test]  Epoch: 54	Loss: 0.057053	Acc: 22.2% (2225/10000)
[Test]  Epoch: 55	Loss: 0.056812	Acc: 22.6% (2256/10000)
[Test]  Epoch: 56	Loss: 0.056872	Acc: 22.6% (2265/10000)
[Test]  Epoch: 57	Loss: 0.056853	Acc: 22.7% (2271/10000)
[Test]  Epoch: 58	Loss: 0.056632	Acc: 22.8% (2282/10000)
[Test]  Epoch: 59	Loss: 0.056888	Acc: 22.3% (2229/10000)
[Test]  Epoch: 60	Loss: 0.057068	Acc: 22.3% (2234/10000)
[Test]  Epoch: 61	Loss: 0.056785	Acc: 22.8% (2279/10000)
[Test]  Epoch: 62	Loss: 0.056745	Acc: 22.9% (2288/10000)
[Test]  Epoch: 63	Loss: 0.056625	Acc: 23.1% (2308/10000)
[Test]  Epoch: 64	Loss: 0.056552	Acc: 23.2% (2319/10000)
[Test]  Epoch: 65	Loss: 0.056632	Acc: 23.1% (2308/10000)
[Test]  Epoch: 66	Loss: 0.056596	Acc: 23.2% (2317/10000)
[Test]  Epoch: 67	Loss: 0.056651	Acc: 23.2% (2321/10000)
[Test]  Epoch: 68	Loss: 0.056748	Acc: 23.0% (2296/10000)
[Test]  Epoch: 69	Loss: 0.056573	Acc: 23.2% (2318/10000)
[Test]  Epoch: 70	Loss: 0.056605	Acc: 23.1% (2310/10000)
[Test]  Epoch: 71	Loss: 0.056545	Acc: 23.1% (2307/10000)
[Test]  Epoch: 72	Loss: 0.056681	Acc: 22.9% (2294/10000)
[Test]  Epoch: 73	Loss: 0.056614	Acc: 23.1% (2307/10000)
[Test]  Epoch: 74	Loss: 0.056618	Acc: 23.1% (2315/10000)
[Test]  Epoch: 75	Loss: 0.056669	Acc: 22.9% (2293/10000)
[Test]  Epoch: 76	Loss: 0.056620	Acc: 23.0% (2304/10000)
[Test]  Epoch: 77	Loss: 0.056629	Acc: 23.1% (2306/10000)
[Test]  Epoch: 78	Loss: 0.056660	Acc: 22.9% (2290/10000)
[Test]  Epoch: 79	Loss: 0.056662	Acc: 23.1% (2308/10000)
[Test]  Epoch: 80	Loss: 0.056556	Acc: 23.0% (2302/10000)
[Test]  Epoch: 81	Loss: 0.056603	Acc: 23.1% (2305/10000)
[Test]  Epoch: 82	Loss: 0.056595	Acc: 23.0% (2304/10000)
[Test]  Epoch: 83	Loss: 0.056624	Acc: 23.1% (2311/10000)
[Test]  Epoch: 84	Loss: 0.056682	Acc: 23.1% (2314/10000)
[Test]  Epoch: 85	Loss: 0.056660	Acc: 23.1% (2309/10000)
[Test]  Epoch: 86	Loss: 0.056659	Acc: 22.9% (2291/10000)
[Test]  Epoch: 87	Loss: 0.056619	Acc: 22.8% (2284/10000)
[Test]  Epoch: 88	Loss: 0.056629	Acc: 23.1% (2306/10000)
[Test]  Epoch: 89	Loss: 0.056543	Acc: 23.1% (2309/10000)
[Test]  Epoch: 90	Loss: 0.056639	Acc: 23.1% (2305/10000)
[Test]  Epoch: 91	Loss: 0.056671	Acc: 22.9% (2289/10000)
[Test]  Epoch: 92	Loss: 0.056547	Acc: 23.2% (2317/10000)
[Test]  Epoch: 93	Loss: 0.056606	Acc: 23.1% (2306/10000)
[Test]  Epoch: 94	Loss: 0.056624	Acc: 22.9% (2288/10000)
[Test]  Epoch: 95	Loss: 0.056572	Acc: 23.0% (2298/10000)
[Test]  Epoch: 96	Loss: 0.056580	Acc: 23.1% (2305/10000)
[Test]  Epoch: 97	Loss: 0.056599	Acc: 22.9% (2294/10000)
[Test]  Epoch: 98	Loss: 0.056633	Acc: 22.9% (2291/10000)
[Test]  Epoch: 99	Loss: 0.056606	Acc: 22.9% (2289/10000)
[Test]  Epoch: 100	Loss: 0.056579	Acc: 23.1% (2308/10000)
===========finish==========
['2024-08-18', '19:50:18.581650', '100', 'test', '0.05657933382987976', '23.08', '23.21']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 0.9 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=94
get_sample_layers not_random
94 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.1.conv.2.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.3.weight', '_features.14.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.17.conv.3.weight', '_features.11.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.0.1.weight', '_features.17.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.1.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.17.conv.1.1.weight', '_features.17.conv.0.1.weight', '_features.13.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.12.conv.2.weight', '_features.13.conv.2.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.0.0.weight', '_features.6.conv.0.0.weight', '_features.4.conv.3.weight', '_features.6.conv.1.0.weight', '_features.11.conv.3.weight', '_features.3.conv.1.1.weight', '_features.14.conv.3.weight', '_features.8.conv.2.weight', '_features.8.conv.0.0.weight', '_features.3.conv.3.weight', '_features.10.conv.0.0.weight', '_features.9.conv.2.weight', '_features.9.conv.0.0.weight', '_features.7.conv.3.weight', '_features.10.conv.2.weight', '_features.12.conv.1.0.weight', '_features.5.conv.0.1.weight', '_features.6.conv.3.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.5.conv.3.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.2.weight', '_features.13.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.10.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.8.conv.3.weight', '_features.10.conv.3.weight', '_features.10.conv.0.1.weight', '_features.16.conv.3.weight', '_features.8.conv.0.1.weight', '_features.9.conv.3.weight', '_features.15.conv.3.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.094159	Acc: 0.7% (74/10000)
[Test]  Epoch: 2	Loss: 0.077445	Acc: 5.0% (495/10000)
[Test]  Epoch: 3	Loss: 0.074195	Acc: 6.1% (611/10000)
[Test]  Epoch: 4	Loss: 0.070990	Acc: 9.0% (897/10000)
[Test]  Epoch: 5	Loss: 0.070002	Acc: 9.9% (992/10000)
[Test]  Epoch: 6	Loss: 0.067944	Acc: 11.1% (1110/10000)
[Test]  Epoch: 7	Loss: 0.066835	Acc: 11.6% (1164/10000)
[Test]  Epoch: 8	Loss: 0.065681	Acc: 12.8% (1282/10000)
[Test]  Epoch: 9	Loss: 0.064636	Acc: 14.1% (1413/10000)
[Test]  Epoch: 10	Loss: 0.063522	Acc: 14.6% (1460/10000)
[Test]  Epoch: 11	Loss: 0.063532	Acc: 14.6% (1456/10000)
[Test]  Epoch: 12	Loss: 0.062564	Acc: 15.8% (1583/10000)
[Test]  Epoch: 13	Loss: 0.061813	Acc: 16.3% (1629/10000)
[Test]  Epoch: 14	Loss: 0.061480	Acc: 16.7% (1672/10000)
[Test]  Epoch: 15	Loss: 0.061232	Acc: 16.8% (1679/10000)
[Test]  Epoch: 16	Loss: 0.060670	Acc: 17.2% (1719/10000)
[Test]  Epoch: 17	Loss: 0.060624	Acc: 17.3% (1731/10000)
[Test]  Epoch: 18	Loss: 0.059861	Acc: 17.8% (1777/10000)
[Test]  Epoch: 19	Loss: 0.059890	Acc: 17.1% (1710/10000)
[Test]  Epoch: 20	Loss: 0.060050	Acc: 18.2% (1821/10000)
[Test]  Epoch: 21	Loss: 0.059374	Acc: 18.1% (1811/10000)
[Test]  Epoch: 22	Loss: 0.059261	Acc: 18.7% (1870/10000)
[Test]  Epoch: 23	Loss: 0.059179	Acc: 18.6% (1855/10000)
[Test]  Epoch: 24	Loss: 0.059247	Acc: 18.2% (1818/10000)
[Test]  Epoch: 25	Loss: 0.059035	Acc: 18.6% (1862/10000)
[Test]  Epoch: 26	Loss: 0.058831	Acc: 19.4% (1936/10000)
[Test]  Epoch: 27	Loss: 0.058870	Acc: 18.9% (1893/10000)
[Test]  Epoch: 28	Loss: 0.058376	Acc: 19.8% (1983/10000)
[Test]  Epoch: 29	Loss: 0.058569	Acc: 19.0% (1902/10000)
[Test]  Epoch: 30	Loss: 0.058179	Acc: 19.8% (1979/10000)
[Test]  Epoch: 31	Loss: 0.058180	Acc: 20.1% (2015/10000)
[Test]  Epoch: 32	Loss: 0.058207	Acc: 19.4% (1943/10000)
[Test]  Epoch: 33	Loss: 0.058324	Acc: 19.9% (1986/10000)
[Test]  Epoch: 34	Loss: 0.057991	Acc: 20.3% (2031/10000)
[Test]  Epoch: 35	Loss: 0.057831	Acc: 20.3% (2030/10000)
[Test]  Epoch: 36	Loss: 0.058112	Acc: 20.2% (2025/10000)
[Test]  Epoch: 37	Loss: 0.057780	Acc: 20.7% (2068/10000)
[Test]  Epoch: 38	Loss: 0.057752	Acc: 21.2% (2116/10000)
[Test]  Epoch: 39	Loss: 0.057668	Acc: 20.5% (2053/10000)
[Test]  Epoch: 40	Loss: 0.057433	Acc: 21.1% (2109/10000)
[Test]  Epoch: 41	Loss: 0.057501	Acc: 20.6% (2065/10000)
[Test]  Epoch: 42	Loss: 0.057391	Acc: 21.0% (2104/10000)
[Test]  Epoch: 43	Loss: 0.057410	Acc: 21.2% (2116/10000)
[Test]  Epoch: 44	Loss: 0.057600	Acc: 21.0% (2104/10000)
[Test]  Epoch: 45	Loss: 0.057578	Acc: 20.7% (2070/10000)
[Test]  Epoch: 46	Loss: 0.057383	Acc: 21.2% (2125/10000)
[Test]  Epoch: 47	Loss: 0.057292	Acc: 21.5% (2154/10000)
[Test]  Epoch: 48	Loss: 0.057371	Acc: 21.3% (2132/10000)
[Test]  Epoch: 49	Loss: 0.057158	Acc: 21.9% (2185/10000)
[Test]  Epoch: 50	Loss: 0.057356	Acc: 21.6% (2164/10000)
[Test]  Epoch: 51	Loss: 0.057138	Acc: 21.7% (2167/10000)
[Test]  Epoch: 52	Loss: 0.057102	Acc: 21.9% (2195/10000)
[Test]  Epoch: 53	Loss: 0.057159	Acc: 21.7% (2172/10000)
[Test]  Epoch: 54	Loss: 0.057221	Acc: 21.4% (2145/10000)
[Test]  Epoch: 55	Loss: 0.056780	Acc: 22.2% (2217/10000)
[Test]  Epoch: 56	Loss: 0.056989	Acc: 21.9% (2190/10000)
[Test]  Epoch: 57	Loss: 0.056767	Acc: 22.5% (2247/10000)
[Test]  Epoch: 58	Loss: 0.056732	Acc: 22.0% (2198/10000)
[Test]  Epoch: 59	Loss: 0.056928	Acc: 22.0% (2201/10000)
[Test]  Epoch: 60	Loss: 0.056892	Acc: 21.7% (2170/10000)
[Test]  Epoch: 61	Loss: 0.056753	Acc: 22.2% (2221/10000)
[Test]  Epoch: 62	Loss: 0.056798	Acc: 22.4% (2235/10000)
[Test]  Epoch: 63	Loss: 0.056615	Acc: 22.6% (2259/10000)
[Test]  Epoch: 64	Loss: 0.056574	Acc: 22.6% (2256/10000)
[Test]  Epoch: 65	Loss: 0.056638	Acc: 22.4% (2240/10000)
[Test]  Epoch: 66	Loss: 0.056667	Acc: 22.5% (2248/10000)
[Test]  Epoch: 67	Loss: 0.056716	Acc: 22.6% (2256/10000)
[Test]  Epoch: 68	Loss: 0.056783	Acc: 22.3% (2230/10000)
[Test]  Epoch: 69	Loss: 0.056637	Acc: 22.5% (2249/10000)
[Test]  Epoch: 70	Loss: 0.056691	Acc: 22.5% (2246/10000)
[Test]  Epoch: 71	Loss: 0.056616	Acc: 22.7% (2269/10000)
[Test]  Epoch: 72	Loss: 0.056747	Acc: 22.5% (2251/10000)
[Test]  Epoch: 73	Loss: 0.056642	Acc: 22.6% (2265/10000)
[Test]  Epoch: 74	Loss: 0.056641	Acc: 22.6% (2258/10000)
[Test]  Epoch: 75	Loss: 0.056737	Acc: 22.2% (2221/10000)
[Test]  Epoch: 76	Loss: 0.056722	Acc: 22.5% (2248/10000)
[Test]  Epoch: 77	Loss: 0.056676	Acc: 22.3% (2234/10000)
[Test]  Epoch: 78	Loss: 0.056718	Acc: 22.4% (2236/10000)
[Test]  Epoch: 79	Loss: 0.056752	Acc: 22.4% (2239/10000)
[Test]  Epoch: 80	Loss: 0.056661	Acc: 22.7% (2273/10000)
[Test]  Epoch: 81	Loss: 0.056638	Acc: 22.5% (2251/10000)
[Test]  Epoch: 82	Loss: 0.056671	Acc: 22.5% (2249/10000)
[Test]  Epoch: 83	Loss: 0.056728	Acc: 22.4% (2240/10000)
[Test]  Epoch: 84	Loss: 0.056755	Acc: 22.5% (2254/10000)
[Test]  Epoch: 85	Loss: 0.056744	Acc: 22.5% (2247/10000)
[Test]  Epoch: 86	Loss: 0.056753	Acc: 22.3% (2229/10000)
[Test]  Epoch: 87	Loss: 0.056685	Acc: 22.5% (2253/10000)
[Test]  Epoch: 88	Loss: 0.056680	Acc: 22.4% (2245/10000)
[Test]  Epoch: 89	Loss: 0.056579	Acc: 22.6% (2259/10000)
[Test]  Epoch: 90	Loss: 0.056661	Acc: 22.5% (2253/10000)
[Test]  Epoch: 91	Loss: 0.056689	Acc: 22.4% (2236/10000)
[Test]  Epoch: 92	Loss: 0.056622	Acc: 22.7% (2267/10000)
[Test]  Epoch: 93	Loss: 0.056654	Acc: 22.6% (2259/10000)
[Test]  Epoch: 94	Loss: 0.056729	Acc: 22.5% (2247/10000)
[Test]  Epoch: 95	Loss: 0.056663	Acc: 22.5% (2254/10000)
[Test]  Epoch: 96	Loss: 0.056648	Acc: 22.5% (2246/10000)
[Test]  Epoch: 97	Loss: 0.056640	Acc: 22.6% (2264/10000)
[Test]  Epoch: 98	Loss: 0.056711	Acc: 22.5% (2250/10000)
[Test]  Epoch: 99	Loss: 0.056685	Acc: 22.5% (2254/10000)
[Test]  Epoch: 100	Loss: 0.056667	Acc: 22.6% (2259/10000)
===========finish==========
['2024-08-18', '19:52:37.725816', '100', 'test', '0.056666955494880676', '22.59', '22.73']
===========start==========
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/bin/python3 ./knockoff/adversary/train.py models/adversary/tinyimagenet200-mobilenetv2-channel mobilenetv2 TinyImageNet200 --budgets 1000 -d 1 --pretrained imagenet --log-interval 100 --epochs 100 --lr 0.1 --victim_dir models/victim/tinyimagenet200-mobilenetv2 --protect_percent 1 --channel_percent -1
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
=> found transfer set with 5000 samples, 200 classes
=> done loading TinyImageNet200 (test) with 10000 examples
[INFO] channel_percent = -1.0
=> done loading TinyImageNet200 (train) with 100000 examples
[DEBUG] knockoff.models.cifar.mobilenetv2
Importance info exists. Loading...
Importance info loaded.
get_sample_layers n=105  num_select=105
get_sample_layers not_random
105 ['_features.17.conv.0.0.weight', '_features.17.conv.2.weight', '_features.2.conv.2.weight', '_features.14.conv.2.weight', '_features.7.conv.2.weight', '_features.14.conv.0.0.weight', '_features.4.conv.2.weight', '_features.11.conv.2.weight', '_features.18.0.weight', '_features.4.conv.0.0.weight', 'last_linear.weight', '_features.11.conv.0.0.weight', '_features.2.conv.1.0.weight', '_features.1.conv.1.weight', '_features.7.conv.0.0.weight', '_features.1.conv.0.0.weight', '_features.0.0.weight', '_features.2.conv.0.1.weight', '_features.2.conv.0.0.weight', '_features.3.conv.2.weight', '_features.4.conv.0.1.weight', '_features.4.conv.1.0.weight', '_features.2.conv.1.1.weight', '_features.11.conv.1.0.weight', '_features.1.conv.0.1.weight', '_features.3.conv.1.0.weight', '_features.1.conv.2.weight', '_features.7.conv.0.1.weight', '_features.7.conv.1.0.weight', '_features.2.conv.3.weight', '_features.14.conv.0.1.weight', '_features.4.conv.1.1.weight', '_features.17.conv.3.weight', '_features.11.conv.0.1.weight', '_features.3.conv.0.0.weight', '_features.14.conv.1.0.weight', '_features.0.1.weight', '_features.17.conv.1.0.weight', '_features.7.conv.1.1.weight', '_features.11.conv.1.1.weight', '_features.14.conv.1.1.weight', '_features.6.conv.2.weight', '_features.5.conv.2.weight', '_features.17.conv.1.1.weight', '_features.17.conv.0.1.weight', '_features.13.conv.0.0.weight', '_features.12.conv.0.0.weight', '_features.12.conv.2.weight', '_features.13.conv.2.weight', '_features.5.conv.1.0.weight', '_features.3.conv.0.1.weight', '_features.5.conv.0.0.weight', '_features.6.conv.0.0.weight', '_features.4.conv.3.weight', '_features.6.conv.1.0.weight', '_features.11.conv.3.weight', '_features.3.conv.1.1.weight', '_features.14.conv.3.weight', '_features.8.conv.2.weight', '_features.8.conv.0.0.weight', '_features.3.conv.3.weight', '_features.10.conv.0.0.weight', '_features.9.conv.2.weight', '_features.9.conv.0.0.weight', '_features.7.conv.3.weight', '_features.10.conv.2.weight', '_features.12.conv.1.0.weight', '_features.5.conv.0.1.weight', '_features.6.conv.3.weight', '_features.16.conv.2.weight', '_features.16.conv.0.0.weight', '_features.6.conv.0.1.weight', '_features.13.conv.1.0.weight', '_features.5.conv.3.weight', '_features.8.conv.1.0.weight', '_features.5.conv.1.1.weight', '_features.12.conv.0.1.weight', '_features.13.conv.3.weight', '_features.15.conv.0.0.weight', '_features.15.conv.2.weight', '_features.13.conv.0.1.weight', '_features.12.conv.3.weight', '_features.6.conv.1.1.weight', '_features.9.conv.1.0.weight', '_features.10.conv.1.0.weight', '_features.12.conv.1.1.weight', '_features.13.conv.1.1.weight', '_features.8.conv.3.weight', '_features.10.conv.3.weight', '_features.10.conv.0.1.weight', '_features.16.conv.3.weight', '_features.8.conv.0.1.weight', '_features.9.conv.3.weight', '_features.15.conv.3.weight', '_features.9.conv.0.1.weight', '_features.8.conv.1.1.weight', '_features.10.conv.1.1.weight', '_features.9.conv.1.1.weight', '_features.16.conv.0.1.weight', '_features.15.conv.0.1.weight', '_features.15.conv.1.0.weight', '_features.16.conv.1.0.weight', '_features.15.conv.1.1.weight', '_features.16.conv.1.1.weight', '_features.18.1.weight']

=> Training at budget = 1000
/home/gpu2/miniconda3/envs/sunt_torch2.1.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[Test]  Epoch: 1	Loss: 0.101303	Acc: 1.1% (111/10000)
[Test]  Epoch: 2	Loss: 0.068663	Acc: 7.8% (778/10000)
[Test]  Epoch: 3	Loss: 0.064290	Acc: 10.5% (1046/10000)
[Test]  Epoch: 4	Loss: 0.062240	Acc: 13.7% (1367/10000)
[Test]  Epoch: 5	Loss: 0.060046	Acc: 16.1% (1607/10000)
[Test]  Epoch: 6	Loss: 0.059083	Acc: 16.6% (1663/10000)
[Test]  Epoch: 7	Loss: 0.058881	Acc: 17.0% (1697/10000)
[Test]  Epoch: 8	Loss: 0.058404	Acc: 18.4% (1838/10000)
[Test]  Epoch: 9	Loss: 0.057793	Acc: 19.6% (1958/10000)
[Test]  Epoch: 10	Loss: 0.057815	Acc: 19.3% (1926/10000)
[Test]  Epoch: 11	Loss: 0.057933	Acc: 19.0% (1896/10000)
[Test]  Epoch: 12	Loss: 0.057541	Acc: 20.0% (2000/10000)
[Test]  Epoch: 13	Loss: 0.057067	Acc: 20.5% (2048/10000)
[Test]  Epoch: 14	Loss: 0.057455	Acc: 19.7% (1969/10000)
[Test]  Epoch: 15	Loss: 0.056991	Acc: 20.5% (2049/10000)
[Test]  Epoch: 16	Loss: 0.056807	Acc: 21.3% (2126/10000)
[Test]  Epoch: 17	Loss: 0.056679	Acc: 20.8% (2082/10000)
[Test]  Epoch: 18	Loss: 0.056666	Acc: 20.9% (2091/10000)
[Test]  Epoch: 19	Loss: 0.056852	Acc: 21.0% (2102/10000)
[Test]  Epoch: 20	Loss: 0.056627	Acc: 21.3% (2130/10000)
[Test]  Epoch: 21	Loss: 0.056285	Acc: 21.7% (2169/10000)
[Test]  Epoch: 22	Loss: 0.056645	Acc: 21.6% (2161/10000)
[Test]  Epoch: 23	Loss: 0.056275	Acc: 21.9% (2189/10000)
[Test]  Epoch: 24	Loss: 0.056279	Acc: 22.4% (2238/10000)
[Test]  Epoch: 25	Loss: 0.056331	Acc: 22.2% (2225/10000)
[Test]  Epoch: 26	Loss: 0.056382	Acc: 22.3% (2226/10000)
[Test]  Epoch: 27	Loss: 0.056131	Acc: 22.7% (2266/10000)
[Test]  Epoch: 28	Loss: 0.056239	Acc: 21.9% (2195/10000)
[Test]  Epoch: 29	Loss: 0.056077	Acc: 22.4% (2238/10000)
[Test]  Epoch: 30	Loss: 0.056126	Acc: 22.4% (2236/10000)
[Test]  Epoch: 31	Loss: 0.055950	Acc: 21.9% (2185/10000)
[Test]  Epoch: 32	Loss: 0.055992	Acc: 23.1% (2310/10000)
[Test]  Epoch: 33	Loss: 0.056014	Acc: 22.9% (2294/10000)
[Test]  Epoch: 34	Loss: 0.055858	Acc: 23.0% (2300/10000)
[Test]  Epoch: 35	Loss: 0.055965	Acc: 22.9% (2287/10000)
[Test]  Epoch: 36	Loss: 0.056026	Acc: 22.8% (2277/10000)
[Test]  Epoch: 37	Loss: 0.055971	Acc: 23.2% (2316/10000)
[Test]  Epoch: 38	Loss: 0.055967	Acc: 23.5% (2348/10000)
[Test]  Epoch: 39	Loss: 0.055946	Acc: 23.4% (2344/10000)
[Test]  Epoch: 40	Loss: 0.056204	Acc: 22.7% (2272/10000)
[Test]  Epoch: 41	Loss: 0.055762	Acc: 23.0% (2303/10000)
[Test]  Epoch: 42	Loss: 0.055870	Acc: 23.0% (2304/10000)
[Test]  Epoch: 43	Loss: 0.055902	Acc: 22.8% (2283/10000)
[Test]  Epoch: 44	Loss: 0.056004	Acc: 23.6% (2364/10000)
[Test]  Epoch: 45	Loss: 0.055905	Acc: 22.9% (2294/10000)
[Test]  Epoch: 46	Loss: 0.055837	Acc: 23.6% (2357/10000)
[Test]  Epoch: 47	Loss: 0.055865	Acc: 23.6% (2364/10000)
[Test]  Epoch: 48	Loss: 0.056159	Acc: 22.6% (2259/10000)
[Test]  Epoch: 49	Loss: 0.056026	Acc: 23.0% (2300/10000)
[Test]  Epoch: 50	Loss: 0.055906	Acc: 23.3% (2329/10000)
[Test]  Epoch: 51	Loss: 0.055913	Acc: 23.6% (2363/10000)
[Test]  Epoch: 52	Loss: 0.055801	Acc: 23.7% (2372/10000)
[Test]  Epoch: 53	Loss: 0.055884	Acc: 23.8% (2381/10000)
[Test]  Epoch: 54	Loss: 0.055961	Acc: 23.6% (2357/10000)
[Test]  Epoch: 55	Loss: 0.055831	Acc: 23.6% (2361/10000)
[Test]  Epoch: 56	Loss: 0.055803	Acc: 23.9% (2394/10000)
[Test]  Epoch: 57	Loss: 0.055597	Acc: 23.7% (2374/10000)
[Test]  Epoch: 58	Loss: 0.055873	Acc: 23.0% (2304/10000)
[Test]  Epoch: 59	Loss: 0.055818	Acc: 23.5% (2347/10000)
[Test]  Epoch: 60	Loss: 0.055830	Acc: 23.6% (2363/10000)
[Test]  Epoch: 61	Loss: 0.055724	Acc: 24.2% (2418/10000)
[Test]  Epoch: 62	Loss: 0.055784	Acc: 24.1% (2413/10000)
[Test]  Epoch: 63	Loss: 0.055636	Acc: 24.2% (2419/10000)
[Test]  Epoch: 64	Loss: 0.055586	Acc: 24.2% (2422/10000)
[Test]  Epoch: 65	Loss: 0.055657	Acc: 24.1% (2405/10000)
[Test]  Epoch: 66	Loss: 0.055620	Acc: 24.5% (2447/10000)
[Test]  Epoch: 67	Loss: 0.055649	Acc: 24.2% (2417/10000)
[Test]  Epoch: 68	Loss: 0.055780	Acc: 24.1% (2412/10000)
[Test]  Epoch: 69	Loss: 0.055613	Acc: 24.3% (2432/10000)
[Test]  Epoch: 70	Loss: 0.055627	Acc: 24.4% (2437/10000)
[Test]  Epoch: 71	Loss: 0.055548	Acc: 24.6% (2455/10000)
[Test]  Epoch: 72	Loss: 0.055714	Acc: 24.2% (2421/10000)
[Test]  Epoch: 73	Loss: 0.055630	Acc: 24.3% (2434/10000)
[Test]  Epoch: 74	Loss: 0.055589	Acc: 24.3% (2433/10000)
[Test]  Epoch: 75	Loss: 0.055726	Acc: 24.0% (2404/10000)
[Test]  Epoch: 76	Loss: 0.055651	Acc: 24.3% (2432/10000)
[Test]  Epoch: 77	Loss: 0.055653	Acc: 24.2% (2423/10000)
[Test]  Epoch: 78	Loss: 0.055637	Acc: 24.4% (2435/10000)
[Test]  Epoch: 79	Loss: 0.055638	Acc: 24.2% (2424/10000)
[Test]  Epoch: 80	Loss: 0.055629	Acc: 24.2% (2421/10000)
[Test]  Epoch: 81	Loss: 0.055611	Acc: 24.2% (2424/10000)
[Test]  Epoch: 82	Loss: 0.055629	Acc: 24.4% (2439/10000)
[Test]  Epoch: 83	Loss: 0.055702	Acc: 24.2% (2416/10000)
[Test]  Epoch: 84	Loss: 0.055706	Acc: 24.2% (2419/10000)
[Test]  Epoch: 85	Loss: 0.055701	Acc: 24.0% (2404/10000)
[Test]  Epoch: 86	Loss: 0.055687	Acc: 24.1% (2407/10000)
[Test]  Epoch: 87	Loss: 0.055671	Acc: 24.0% (2399/10000)
[Test]  Epoch: 88	Loss: 0.055588	Acc: 24.4% (2442/10000)
[Test]  Epoch: 89	Loss: 0.055497	Acc: 24.6% (2456/10000)
[Test]  Epoch: 90	Loss: 0.055633	Acc: 24.4% (2441/10000)
[Test]  Epoch: 91	Loss: 0.055597	Acc: 24.4% (2436/10000)
[Test]  Epoch: 92	Loss: 0.055581	Acc: 24.4% (2443/10000)
[Test]  Epoch: 93	Loss: 0.055591	Acc: 24.5% (2449/10000)
[Test]  Epoch: 94	Loss: 0.055614	Acc: 24.3% (2433/10000)
[Test]  Epoch: 95	Loss: 0.055608	Acc: 24.3% (2432/10000)
[Test]  Epoch: 96	Loss: 0.055532	Acc: 24.3% (2428/10000)
[Test]  Epoch: 97	Loss: 0.055617	Acc: 24.3% (2429/10000)
[Test]  Epoch: 98	Loss: 0.055597	Acc: 24.4% (2437/10000)
[Test]  Epoch: 99	Loss: 0.055667	Acc: 24.3% (2432/10000)
[Test]  Epoch: 100	Loss: 0.055576	Acc: 24.6% (2456/10000)
===========finish==========
['2024-08-18', '19:54:56.029429', '100', 'test', '0.05557589936256409', '24.56', '24.56']
